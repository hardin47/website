[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Statistics",
    "section": "",
    "text": "Welcome to Computational Statistics\nDaily class notes for Computational Statistics, by Jo Hardin, Math 154 at Pomona College. Built on Modern Data Science with R by Baumer, Kaplan, and Horton, 3rd ed. and An Introduction to Statistical Learning with Applications in R by James, Witten, Hastie, and Tibshirani, 2nd ed.\nYou are responsible for reading your texts. They are both free, excellent, and readable, so you should use them. That said, you should also make sure you are coming to class and asking lots of questions.\nMore information and course details can be found at the Math 154 website.\n\nCopyright © 2024.\nVersion date: August 25, 2024.\nThe notes are available under a Creative Commons Attribution 4.0 United States License. License details are available at the Creative Commons website: https://creativecommons.org/licenses/by/4.0/.\nSource files for these notes can be found on GitHub at: https://github.com/hardin47/website/tree/gh-pages/Math154.",
    "crumbs": [
      "Welcome to Computational Statistics"
    ]
  },
  {
    "objectID": "11-compstats.html",
    "href": "11-compstats.html",
    "title": "11  Computational Statistics",
    "section": "",
    "text": "11.1 Wearing a Statistics Hat\nIn this class, we’ve learned a number of ways of dealing with data in order to communicate the information provided in the data. We have talked about data wrangling, visualization, inference, and classification techniques. Many of the ideas fall within the paradigm of “data science.” But what makes it statistics? What do statisticians bring to the table?\nPrimarily, statisticians are good at thinking about variability. We are also very skeptical. You should try not to be too skeptical. But be skeptical enough.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Computational Statistics</span>"
    ]
  },
  {
    "objectID": "11-compstats.html#wearing-a-statistics-hat",
    "href": "11-compstats.html#wearing-a-statistics-hat",
    "title": "11  Computational Statistics",
    "section": "",
    "text": "Important Adage #1: The perfect is the enemy of the good enough. (Voltaire?)\n\n\nImportant Adage #2: All models are wrong, but some are useful. (G.E.P. Box 1987)\n\n\nSome good thoughts: http://simplystatistics.org/2014/05/22/10-things-statistics-taught-us-about-big-data-analysis/\n\n\nThe statistics profession is caught at a confusing moment: the activities which preoccupied it over centuries are now in the limelight, but those activities are claimed to be bright shiny new, and carried out by (although not actually invented by) upstarts and strangers. [http://simplystatistics.org/2015/11/24/20-years-of-data-science-and-data-driven-discovery-from-music-to-genomics/]\n\nSamples\nThe data is a sample of the population. Is it a good representation of the population? Who got left out? Were any of the variables missing? Missing at random? (Probably not.)\n\nFor O’Hagan’s fire department, RAND built computer models that replicated when, where, and how often fires broke out in the city, and then predicted how quickly fire companies could respond to them. By showing which areas received faster and slower responses, RAND determined which companies could be closed with the least impact. In 1972, RAND recommended closing 13 companies, oddly including some of the busiest in the fire-prone South Bronx, and opening seven new ones, including units in suburban neighborhoods of Staten Island and the North Bronx.\n\n\nRAND’s first mistake was assuming that response time – a mediocre measure of firefighting operations as a whole, but the only aspect that can be easily quantified – was the only factor necessary for determining where companies should be opened and closed. To calculate these theoretical response times, RAND needed to gather real ones. But their sample was so small, unrepresentative and poorly compiled that the data indicated that traffic played no role in how quickly a fire company responded.\n\n\nThe models themselves were also full of mistakes and omissions. One assumed that fire companies were always available to respond to fires from their firehouse – true enough on Staten Island, but a rarity in places like The Bronx, where every company in a neighborhood, sometimes in the entire borough, could be out fighting fires at the same time. Numerous corners were cut, with RAND reports routinely dismissing crucial legwork as “too laborious,” and analysts writing that data discrepancies could “be ignored for many planning purposes.”\n\nhttp://nypost.com/2010/05/16/why-the-bronx-burned/ http://fivethirtyeight.com/datalab/why-the-bronx-really-burned/\nSample Size\nTiny samples show huge variability; huge samples will always give significance (tweets from first day of class).\nSampling distribution of \\(\\overline{x}\\) … http://www.rossmanchance.com/applets/OneSample.html?showBoth=1\nKnow your real sample size!!! One group project is using grocery store data with sales measured daily over the last 3 years. With 10 stores… is your sample size \\(10\\cdot 365\\cdot 3 = 10,950\\)? Or is the sample size 10 with \\(365\\cdot3 = 1095\\) variables?\nAlso: know whether your result is on an average or an individual and whether the “significance” is statistical only or whether it is also practical.\nCorrelation v. Causation\n\nAn article about handwriting appeared in the October 11, 2006 issue of the Washington Post. The article mentioned that among students who took the essay portion of the SAT exam in 2005-06, those who wrote in cursive style scored significantly higher on the essay, on average, than students who used printed block letters. Researchers wanted to know whether simply writing in cursive would be a way to increase scores.\n\n\nThe article also mentioned a different study in which the same one essay was given to all graders. But some graders were shown a cursive version of the essay and the other graders were shown a version with printed block letters. Researchers randomly decided which version the grader would receive. The average score assigned to the essay with the cursive style was significantly higher than the average score assigned to the essay with the printed block letters. (Chance and Rossman 2018)\n\nUnless you are running a randomized experiment, you should always try to think of as many possible confounding variables as you can.\nEnsemble Learners\nWe’ve seen ideas of ensembles in bagging, in random forests, and on the first take home exam (average of the bootstrap confidence intervals). If the goal is prediction accuracy, average many predictions together. If different models use/provide different pieces of information, then the average predictors will balance the information and reduce the variability of the prediction.\nNote that you wouldn’t want to average a set of ensemble learners if one of them was bad. (E.g., if the relationship was quadratic and you fit one model to be quadratic and another to be linear… your average will be worse.)\nAnd of course… the world isn’t always about prediction. Sometimes it is about describing! Simpler models (e.g., regression) get more to the heart of the impact of a specific variable on a response of interest.\nSupervised vs. Unsupervised\nThe classification models we’ve discussed are all supervised learning techniques. The word supervised refers to the fact that we know the response variable of all of the training observations. Next up, we’ll discuss clustering which is an unsupervised technique – none of the observations have a given response variable. For example, we might want to cluster a few hundred melanoma patients based on their genetic data. We are looking for patterns in who groups together, but we don’t have a preconceived idea of which patients belong to which group.\nThere are also semi-supervised techniques applied to data which have some observations that are labeled and some that are not.\nRegression to the mean\nRegression to the mean is the phenomenon that extreme effects even out over time. That is, if a measurement is extreme on the first measurement, it is likely to be closer to the mean on the second measurement.\n\n\nSports Sports Illustrated Jinx (you have to be good and lucky)\n\nDrugs New pharmaceuticals are likely to be less effective than they seem at first.\n\nTesting Best scores get worse, worst scores get better (be wary of interventions)\nMeasuring Accuracy\nNote that for any set of data, the observations are closer to the model built from them than they are to the model which fits the entire population. But the more we adjust the model, the more we run the risk of overfitting it to the data. (Draw a polynomial which way overfits the data.)\n\nTest / training data\nCross validation – do it without cheating\n\nChoosing variables (standardizing!) can make you overfit… same with subsetting or removing points from your analysis.\nDefine a metric for success and stick with it!\nChoose the algorithm (or algorithms!) that work, but do it up front.\nChoose hypotheses before looking at the data\nExploratory Data Analysis\nIf you want to understand a dataset, you have to play around with it. Graph it. Look at summary statistics. Look at bivariate relationships. Plot with colors and other markers.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Computational Statistics</span>"
    ]
  },
  {
    "objectID": "11-compstats.html#computational-statistics",
    "href": "11-compstats.html#computational-statistics",
    "title": "11  Computational Statistics",
    "section": "\n11.2 Computational Statistics",
    "text": "11.2 Computational Statistics\nThe course has been a mix of computational statistics and data science procedures. There are myriad other topics we could have covered. Indeed, many of the most basic and important statistical ideas have computational counterparts that allow us to perform analyses when the calculus doesn’t provide neat clean solutions. Some that we’ve seen and some that we haven’t seen include:\n\n\nHypothesis Testing: Permutation tests\n\nConfidence Intervals: Bootstrapping\n\nParameter Estimation: The EM algorithm\n\nBayesian Analysis: Gibbs sampler, Metropolis-Hasting algorithm\n\nPolynomial regression: Smoothing methods (e.g., loess)\n\nPrediction: Supervised learning methods",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Computational Statistics</span>"
    ]
  },
  {
    "objectID": "11-compstats.html#always-consider-impact.",
    "href": "11-compstats.html#always-consider-impact.",
    "title": "11  Computational Statistics",
    "section": "\n11.3 Always consider impact.",
    "text": "11.3 Always consider impact.\n\n\nKeeping asking yourself:\n\nHow do I stay accountable for my work?\nHow might others be impacted by what I’ve created?\nWhere did the data come from, and what biases might be inherent?\nWhat population is appropriate for any of the inferential claims I’m making?\nHow might individual’s privacy or anonymity be impacted by what I’ve created?\nIs it possible that my work could be misinterpreted or misused?\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Everyday Information Architecture by Lisa Maria Martin\n\n\n\n\nChance, Beth, and Allan Rossman. 2018. Investigating Statistical Concepts, Applications, and Methods. http://www.rossmanchance.com/iscam3/.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Computational Statistics</span>"
    ]
  },
  {
    "objectID": "09-clustering.html",
    "href": "09-clustering.html",
    "title": "9  Unsupervised Methods",
    "section": "",
    "text": "9.1 Latent Dirichlet Allocation\nLDA views each document as a mixture of a small (predefined) number of topics that describe a set of documents. Each word (typically very common and extremely rare words are removed before modeling) represents a an occurrence generated by one of the document’s topics (where the document is modeled as a mixture of topics). Through LDA, the model learns both the composition of each topic and the topic mixture of each document.\nFrom Wikipedia:\nHere is an algorithm for finding words that represent \\(K\\) topics (where \\(K\\) is chosen in advance). [Explained in more detail at http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/.]\nhttps://ziqixiong.shinyapps.io/TopicModeling/",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#latent-dirichlet-allocation",
    "href": "09-clustering.html#latent-dirichlet-allocation",
    "title": "9  Unsupervised Methods",
    "section": "",
    "text": "In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word’s creation is attributable to one of the document’s topics.\n\n\n\nFor each document in the training data, randomly assign each word to one of the \\(K\\) topics.\nEach of the \\(K\\) topics now has a set of words associated with it (albeit, that set of words is meaningless). To improve on the set of words associated with each topic. For each word \\(w\\) in document \\(d\\):\n\nFor each topic \\(t\\), compute two things:\n\np(topic \\(t\\) \\(|\\) document \\(d\\)) = the proportion of words in document \\(d\\) that are currently assigned to topic \\(t\\)\n\np(word \\(w\\) \\(|\\) topic \\(t\\)) = the proportion of assignments to topic \\(t\\) over all documents that come from this word \\(w\\).\n\n\nReassign \\(w\\) to a new topic, where we choose topic \\(t\\) with probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according to our generative model, this is essentially the probability that topic \\(t\\) generated word \\(w\\), so it makes sense that we resample the current word’s topic with this probability).\nIn other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.\n\n\nAfter repeating the previous steps a large number of times, the list of words in each topic will reach a steady state. The resulting assignments estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#dissimilarities",
    "href": "09-clustering.html#dissimilarities",
    "title": "9  Unsupervised Methods",
    "section": "\n9.2 Dissimilarities",
    "text": "9.2 Dissimilarities\nMany, though not all, clustering algorithms are based on distances between the objects being clustered. Mathematical properties of a distance function are the following. Consider two vectors \\({\\bf x}\\) and \\({\\bf y}\\) (\\({\\bf x}, {\\bf y} \\in \\mathbb{R}^p\\)), and the distance between them: \\(d({\\bf x}, {\\bf y})\\).\n\n\\(d({\\bf x}, {\\bf y}) \\geq 0\\)\n\\(d({\\bf x}, {\\bf y}) = d({\\bf y}, {\\bf x})\\)\n\n\\(d({\\bf x}, {\\bf y}) = 0\\) iff \\({\\bf x} = {\\bf y}\\)\n\n\n\\(d({\\bf x}, {\\bf y}) \\leq d({\\bf x}, {\\bf z}) + d({\\bf z}, {\\bf y})\\) for all other vectors \\({\\bf z}\\).\n\nTriangle Inequality\nThe key to proving the triangle inequality for most of the distances relies on the Cauchy-Schwarz inequality. \\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} &= || {\\bf x} ||  ||{\\bf y}|| \\cos(\\theta) \\\\\n|{\\bf x} \\cdot {\\bf y}| &\\leq || {\\bf x} ||  ||{\\bf y}||\n\\end{align}\\]\nEuclidean Distance\n\\[d_E({\\bf x}, {\\bf y}) = \\sqrt{\\sum_{i=1}^p (x_i - y_i)^2}\\]\nDistance properties all check out.\n\nCauchy-Schwarz: \\[\\begin{align}\n\\sum_{i=1}^p(x_i - y_i)^2 = \\sum_{i=1}^p ( (x_i - z_i) + (z_i - y_i))^2 &\\leq \\Bigg( \\sqrt{\\sum_{i=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{i=1}^p(z_i - y_i)^2} \\Bigg)^2\\\\\n\\sqrt{\\sum_{i=1}^p(x_i - y_i)^2} &\\leq \\sqrt{\\sum_{i=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{i=1}^p(z_i - y_i)^2}\\\\\nd_E({\\bf x}, {\\bf y}) &\\leq d_E({\\bf x}, {\\bf z}) + d_E({\\bf z}, {\\bf y})\n\\end{align}\\]\n\n\nShortcomings:\n\n\n\\(d_E\\) is not scale invariant.\n\n\\(d_E\\) measures magnitude differences, not pattern differences.\n\n\\(d_E\\) is sensitive to outliers.\n\nStrengths:\n\nDirectly measures what is commonly considered to be “distance.”\nPearson Correlation Distance\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n\\mbox{ or } &= 1 - |r_P ({\\bf x}, {\\bf y})|\\\\\n\\mbox{ or }   &= 1 - (r_P ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]\nNotice that Euclidean distance and Pearson correlation distance are similar if the original observations are scaled. Assume that the sample mean for \\({\\bf x}\\) (that is, \\(\\frac{1}{p} \\sum x_i = \\overline{x} = 0\\)) is zero and the sample standard deviation is 1.\n\\[\\begin{align}\nr_P ({\\bf x}, {\\bf y}) &=  \\frac{\\sum x_i y_i - p \\ \\overline{x} \\ \\overline{y}}{(p-1)s_x s_y}\\\\\n&=  \\frac{1}{(p-1)} \\sum x_i y_i\\\\\n& \\ \\ & \\\\\nd_E({\\bf x}, {\\bf y}) &= \\sqrt{\\sum(x_i - y_i)^2}\\\\\n&=  \\sqrt{ \\sum x_i^2 + \\sum y_i^2 - 2 \\sum x_i y_i}\\\\\nd_E^2 &= 2[(p-1) - \\sum x_i y_i]\\\\\n&= 2(p-1)*[1 - r_P({\\bf x}, {\\bf y})]\n\\end{align}\\]\nDistance properties don’t hold for Pearson correlation\n\n\\({\\bf y}=a{\\bf x}\\) \\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n&= 1 - r_P ({\\bf x}, a{\\bf x})\\\\\n&= 1 - 1 = 0\n\\end{align}\\]\n\\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\) \\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\)\n\n\\(d_P({\\bf x}, {\\bf y}) + d_P({\\bf y}, {\\bf z})  &lt;  d_P({\\bf z}, {\\bf x})\\) \\(\\rightarrow\\leftarrow\\)\nRegular Pearson distance\n\nx1 &lt;- c(1,2,3)\nx2 &lt;- c(1, 4, 10)\nx3 &lt;- c(9, 2, 2)\n\n# d(1,2)\n1 - cor(x1, x2)\n\n[1] 0.01801949\n\n# d(1,3)\n1 - cor(x1, x3)\n\n[1] 1.866025\n\n# d(2,3)\n1 - cor(x2, x3)\n\n[1] 1.755929\n\n# d(1,3) &gt; d(1,2) + d(2,3)\n1 - cor(x1, x2) + 1 - cor(x2, x3)\n\n[1] 1.773948\n\n\nAbsolute Pearson distance\nUsing absolute distance doesn’t fix things.\n\n# d(1,2)\n1 - abs(cor(x1, x2))\n\n[1] 0.01801949\n\n# d(1,3)\n1 - abs(cor(x1, x3))\n\n[1] 0.1339746\n\n# d(2,3)\n1 - abs(cor(x2, x3))\n\n[1] 0.2440711\n\n# d(2,3) &gt; d(1,2) + d(1,3)\n1 - abs(cor(x1, x2)) + 1 - abs(cor(x1, x3))\n\n[1] 0.1519941\n\n\n\n\n\n\n\n\n\n\nShortcomings:\n\n\n\\(d_P\\) does not satisfy the triangle inequality.\n\n\\(d_P\\) is sensitive to outliers.\n\nStrengths:\n\nCan measure the distance between variables on different scales (although will still be sensitive to extreme values).\nSpearman Correlation Distance\nSpearman correlation distance uses the Spearman correlation instead of the Pearson correlation. The Spearman correlation is simply the Pearson correlation applied to the ranks of the observations. The ranking allows the Spearman distance to be resistant to outlying observations.\n\\[\\begin{align}\nd_S({\\bf x}, {\\bf y}) &= 1 - r_S ({\\bf x}, {\\bf y})\\\\\n\\mbox{ or } &= 1 - |r_S ({\\bf x}, {\\bf y})|\\\\\n\\mbox{ or }   &= 1 - (r_S ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]\nShortcomings:\n\n\n\\(d_S\\) also does not satisfy the triangle inequality.\n\n\\(d_S\\) loses information about the shape of the relationship.\n\nStrengths:\n\nIs resistant to outlying values\nCosine Distance\n\\[\\begin{align}\nd_C({\\bf x}, {\\bf y}) &=  \\frac{{\\bf x} \\cdot {\\bf y}}{|| {\\bf x} ||  ||{\\bf y}||}\\\\\n&= \\frac{\\sum_{i=1}^p x_i y_i}{\\sqrt{\\sum_{i=1}^p x_i^2 \\sum_{i=1}^p y_i^2}}\\\\\n&= 1 - r_P ({\\bf x}, {\\bf y})  \\ \\ \\ \\ \\mbox{if } \\overline{\\bf x} = \\overline{\\bf y} = 0\n\\end{align}\\]\nSaid differently,\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) = d_C({\\bf x} -  \\overline{\\bf x}, {\\bf y} -  \\overline{\\bf y})\n\\end{align}\\]\nHaversine Distance\nHaversine distance is the great-circle distance (i.e., the distance between two points on a sphere) which is used to measure distance between two locations on the Earth. Let \\(R\\) be the radius of the Earth, and (lat1,long1) and (lat2, long2) be the two locations between which to calculate a distance.\n\\[d_{HV} = 2 R \\arcsin \\sqrt{\\sin^2 \\bigg( \\frac{lat2-lat1}{2} \\bigg) + \\cos(lat1) \\cos(lat2) \\sin^2 \\bigg(\\frac{long2 - long1}{2} \\bigg)} \\]\n\n\n\n\n\n\n\n\nShortcomings:\n\nEarth is not a perfect sphere\nDepending on how the distance is used, typically getting from one point to the next is not done by the shortest distance\n\nStrengths:\n\nAllows calculations, for example, between two cities.\nHamming Distance\nHamming distance is the number of coordinates across two vectors whose values differ. If the vectors are binary, the Hamming distance is equivalent to the \\(L_1\\) norm of the difference. (Hamming distance does satisfy the properties of a distance metric.) Some methods, equivalently, calculate the proportion of coordinates that differ.\n\\[\\begin{align}\nd_H({\\bf x}, {\\bf y}) = \\sum_{i=1}^p I(x_i \\ne y_i)\n\\end{align}\\]\n\n\n\n\n\n\n\nThe Hamming distance across the two DNA strands is 7.\n\n\n\nShortcomings:\n\nCan’t measure degree of difference between categorical variables.\n\nStrengths:\n\nIt is a distance metric. Gives a more direct “distance” between categorical variables.\n\n\n\n\n\n\n\n\nThe function dist in R calculates the distances given above.\n\n\n\nDistance on strings\nConsider the following infographic which compares different methods for computing distances between strings.\n\n\n\n\n\n\n\nComparison of string distance metrics from https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#hier",
    "href": "09-clustering.html#hier",
    "title": "9  Unsupervised Methods",
    "section": "\n9.3 Hierarchical Clustering",
    "text": "9.3 Hierarchical Clustering\nHierarchical Clustering is a set of nested clusters that are organized as a tree. Note that objects that belong to a child cluster also belong to the parent cluster.\nExample: Consider the following images / data (from Laura Hoopes, personal communication; Molecular characterisation of soft tissue tumours: a gene expression study by Nielsen et al., The Lancet 2002). The first represents a microarray sample from aging yeast. The second is a set of 41 samples of soft-tissue tumors (columns) and a subset of 5520 genes (rows) used to characterize their molecular signatures.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: the ordering of the variables (or samples) does not affect the clustering of the samples (or variables). That is: we can clustering the variables / samples either sequentially or in parallel to see trends in both relationships simultaneously. Clustering both the observations and the variables is called biclustering.\n\nAlgorithm: Agglomerative Hierarchical Clustering Algorithm\n\n\nBegin with \\(n\\) observations and a measure (such as Euclidean distance) of all the \\({n \\choose 2} = n(n-1)/2\\) pairwise dissimilarities. Treat each observation as its own cluster.\nFor \\(i = n, n - 1, \\ldots , 2\\):\n\n\nExamine all pairwise inter-cluster dissimilarities among the \\(i\\) clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\nCompute the new pairwise inter-cluster dissimilarities among the \\(i - 1\\) remaining clusters.\n\n\nAgglomerative methods start with each object (e.g., gene) in its own group. Groups are merged until all objects are together in one group.\nDivisive methods start with all objects in one group and break up the groups sequentially until all objects are individuals.\nSingle Linkage algorithm defines the distance between groups as that of the closest pair of individuals.\nComplete Linkage algorithm defines the distance between groups as that of the farthest pair of individuals.\nAverage Linkage algorithm defines the distance between groups as the average of the distances between all pairs of individuals across the groups.\nToy Example of Single Linkage Agglomerative Hierarchical Clustering\n\n\n\nA\nB\nC\nD\nE\n\n\n\nA\n0\n\n\n\n\n\n\nB\n0.2\n0\n\n\n\n\n\nC\n0.6\n0.5\n0\n\n\n\n\nD\n1\n0.9\n0.4\n0\n\n\n\nE\n0.9\n0.8\n0.5\n0.3\n0\n\n\n\nLink A and B! \\[\\begin{align}\nd_{(AB)C} &= \\min(d_{AC}, d_{BC}) = 0.5\\\\\nd_{(AB)D} &= \\min(d_{AD}, d_{BD}) = 0.9\\\\\nd_{(AB)E} &= \\min(d_{AE}, d_{BE}) = 0.8\\\\\n\\end{align}\\]\n\n\n\nAB\nC\nD\nE\n\n\n\nAB\n0\n\n\n\n\n\nC\n0.5\n0\n\n\n\n\nD\n0.9\n0.4\n0\n\n\n\nE\n0.8\n0.5\n0.3\n0\n\n\n\nLink D and E! \\[\\begin{align}\nd_{(AB)C} &=  0.5\\\\\nd_{(AB)(DE)} &= \\min(d_{AD}, d_{BD}, d_{AE}, d_{BE}) = 0.8\\\\\nd_{(DE)C} &= \\min(d_{CD}, d_{CE}) = 0.4\\\\\n\\end{align}\\]\n\n\n\nAB\nC\nDE\n\n\n\nAB\n0\n\n\n\n\nC\n0.5\n0\n\n\n\nDE\n0.8\n0.4\n0\n\n\n\nLink C with (DE)! \\[\\begin{align}\nd_{(AB)(CDE)} = d_{BC} = 0.5\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n9.3.1 R hierarchical Example\nAgain, using the penguins dataset, hierarchical clustering will be run. We can look at the dendrogram, in particular its alignment to the categorical variables like species and island.\nNotice that we are only using the numerical variables. And the numerical variables have been scaled (subtract the mean and divide by the standard deviation).\nFull example has been adapted from https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_h &lt;- penguins |&gt;\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_hclust &lt;- penguins_h |&gt;\n  dist() |&gt;\n  hclust(method = \"complete\")\n\npenguin_hclust\n\n\nCall:\nhclust(d = dist(penguins_h), method = \"complete\")\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 342 \n\npenguin_dend &lt;- as.dendrogram(penguin_hclust)\n\nPlotting\nThe basic dendrogram:\n\nplot(penguin_hclust)\n\n\n\n\n\n\n\nZooming in on a cluster:\n\nplot(penguin_dend[[1]])\n\n\n\n\n\n\n\nAdding color to branches and nodes\n\nlibrary(colorspace) # get nice colors\nspecies_col &lt;- rev(rainbow_hcl(3))[as.numeric(penguins$species)]\n\n\nlibrary(dendextend)\n# order the branches as closely as possible to the original order\npenguin_dend &lt;- rotate(penguin_dend, 1:342)\n\n\n# color branches based on the clusters\npenguin_dend &lt;- color_branches(penguin_dend, k = 3)\n\n# Manually match the labels, as much as possible, to the real classification of the flowers:\nlabels_colors(penguin_dend) &lt;-\n   rainbow_hcl(3)[sort_levels_values(\n      as.numeric(penguins$species)[order.dendrogram(penguin_dend)]\n   )]\n\n# We shall add the flower type to the labels:\nlabels(penguin_dend) &lt;- paste(as.character(penguins$species)[order.dendrogram(penguin_dend)],\n                           \"(\",labels(penguin_dend),\")\", \n                           sep = \"\")\n# We hang the dendrogram a bit:\npenguin_dend &lt;- hang.dendrogram(penguin_dend,hang_height=0.1)\n\n# reduce the size of the labels:\npenguin_dend &lt;- assign_values_to_leaves_nodePar(penguin_dend, 0.5, \"lab.cex\")\npenguin_dend &lt;- set(penguin_dend, \"labels_cex\", 0.5)\n# And plot:\npar(mar = c(3,3,3,7))\nplot(penguin_dend, \n     main = \"Clustered penguin data set\n     (the labels give the true penguin species)\", \n     horiz =  TRUE,  nodePar = list(cex = .007))\npenguin_species &lt;- rev(levels(penguins$species))\nlegend(\"topleft\", legend = penguin_species, fill = rainbow_hcl(3))",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#part",
    "href": "09-clustering.html#part",
    "title": "9  Unsupervised Methods",
    "section": "\n9.4 Partitioning Clustering",
    "text": "9.4 Partitioning Clustering\nPartition Clustering is a division of the set of data objects into \\(K\\) non-overlapping subsets (clusters) with each observation falling into exactly one cluster.\nIn contrast to hierarchical clustering where results were given for any (and all!) number of clusters, partitioning methods typically start with a given \\(k\\) value and a set of distances. The goal is to partition the observations into \\(k\\) groups such that an objective function is optimized. The number of possible partitions is roughly to \\(n^k / k!\\) (note: \\(100^{5} / 5! = 83\\) million). [The exact number can be computed using Sterling numbers.] So instead of looking through all of the partitions, we step through a recursive algorithm.\n\n9.4.1 \\(k\\)-means Clustering\nA fun applet!!\nhttps://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n\n\\(k\\)-means clustering is an unsupervised partitioning algorithm designed to find a partition of the observations such that the following objective function is minimized (find the smallest within cluster sum of squares):\n\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K 2 \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\overline{x}_{kj})^2 \\Bigg\\}\\]\nAs described in the algorithm below, reallocating observations can only improve the minimization criteria with the algorithm stopping when no changes of the observations will lower the objective function. The algorithm leads to a local optimum, with no confirmation that the global minimum has occurred. Often the \\(k\\)- means algorithm is run multiple times with different random starts, and the partition leading to the lowest objective criteria is chosen.\nNote that the following algorithm is simply one \\(k\\)-means algorithm. Other algorithms could include a different way to set the starting values, a different decision on when to recalculate the centers, what to do with ties, etc.\n\n\n\n\n\n\n\nFrom An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\n\n\n\n\n\n\n\n\nFrom An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\n\n\n\n\nAlgorithm: \\(k\\)-Means Clustering\n\n\nRandomly assign a number, from 1 to \\(k\\), to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\n\nFor each of the \\(k\\) clusters, compute the cluster centroid. The \\(k^{th}\\) cluster centroid is the vector of the \\(p\\) feature means for the observations in the \\(k^{th}\\) cluster.\nAssign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).\n\n\nTies? Do something consistent: for example, leave in the current cluster.\n\n\nWhy does the \\(k\\)-means algorithm converge / (local) minimize the objective function?\n\nIf a point is closer to a different center, moving it will lower the objective function.\nAverages minimize squared differences, so taking the new average will result in a lower objective function.\nIf a point is equidistant from two clusters, the point won’t move.\nThe algorithm must converge in finite number of steps because there are finitely many points.\n\nScaling matters\nNote that if the variables are on very different scales, whichever variable is larger in magnitude will dominate the distances (and therefore the clustering). Unless you explicitly want that to happen (which would be odd), you should scale the variables (subtract the mean and divide by the standard deviation) so that the distance is calculated on Z-scores instead of on the raw data.\nIn the example below, the \\(k=2\\) k-means algorithm is not able to see the cigar-shaped structure (on the raw data) because the distances are dominated by the x1 variable (which does not differentiate the clusters).\n\nset.seed(47)\nnorm_clust &lt;- data.frame(\n  x1 = rnorm(1000, 0, 15),\n  x2 = c(rnorm(500, 5, 1), rnorm(500, 0, 1)))\n\nnorm_clust |&gt;\n  kmeans(centers = 2) |&gt;\n  augment(norm_clust) |&gt;\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on raw data\")\n\n\n\n\n\n\nnorm_clust |&gt;\n  mutate(across(everything(), scale)) |&gt;\n  kmeans(centers = 2) |&gt;\n  augment(norm_clust) |&gt;\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on normalized / scaled data\")\n\n\n\n\n\n\n\nstrengths\n\nNo hierarchical structure / points can move from one cluster to another.\nCan run for a range of values of \\(k\\).\n\nshortcomings\n\n\n\\(k\\) has to be predefined to run the algorithm.\n\n\\(k\\)-means is based on Euclidean distance (only).\n\n9.4.2 R k-means Example\nAgain, using the penguins dataset, \\(k\\)-means clustering will be run. We try multiple different values of \\(k\\) to come up with a partition of the penguins. We can look at the clusterings on scatterplots of the numerical variables. We can also check to see if the clustering aligns with the categorical variables like species and island.\nNotice that we are only using the numerical variables. And the numerical variables have been scaled (subtract the mean and divide by the standard deviation).\nFull example has been adapted from https://www.tidymodels.org/learn/statistics/k-means/.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_km &lt;- penguins |&gt;\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_kclust &lt;- penguins_km |&gt;\n  kmeans(centers = 3)\n\npenguin_kclust\n\nK-means clustering with 3 clusters of sizes 132, 123, 87\n\nCluster means:\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1     -1.0465260     0.4858415        -0.8899121  -0.7694891\n2      0.6562677    -1.0983711         1.1571696   1.0901639\n3      0.6600059     0.8157307        -0.2857869  -0.3737654\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 3 1\n [75] 3 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 3 1 1 1 3 1 3 1 1 1 1 1 1 1 3 1 3\n[112] 1 3 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[149] 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3\n[297] 1 3 3 3 3 3 3 3 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3\n[334] 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1] 122.1477 143.1502 112.9852\n (between_SS / total_SS =  72.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nThe function augment works at the observation level:\n\npenguin_kclust |&gt; augment(penguins_km)\n\n# A tibble: 342 × 5\n   bill_length_mm[,1] bill_depth_mm[,1] flipper_length_mm[,1] body_mass_g[,1]\n                &lt;dbl&gt;             &lt;dbl&gt;                 &lt;dbl&gt;           &lt;dbl&gt;\n 1             -0.883            0.784                 -1.42          -0.563 \n 2             -0.810            0.126                 -1.06          -0.501 \n 3             -0.663            0.430                 -0.421         -1.19  \n 4             -1.32             1.09                  -0.563         -0.937 \n 5             -0.847            1.75                  -0.776         -0.688 \n 6             -0.920            0.329                 -1.42          -0.719 \n 7             -0.865            1.24                  -0.421          0.590 \n 8             -1.80             0.480                 -0.563         -0.906 \n 9             -0.352            1.54                  -0.776          0.0602\n10             -1.12            -0.0259                -1.06          -1.12  \n# ℹ 332 more rows\n# ℹ 1 more variable: .cluster &lt;fct&gt;\n\n\nThe function tidy() works at the per-cluster level:\n\npenguin_kclust |&gt; tidy()\n\n# A tibble: 3 × 7\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  size withinss\n           &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n1         -1.05          0.486            -0.890      -0.769   132     122.\n2          0.656        -1.10              1.16        1.09    123     143.\n3          0.660         0.816            -0.286      -0.374    87     113.\n# ℹ 1 more variable: cluster &lt;fct&gt;\n\n\nThe function glance() works at the per-model level:\n\npenguin_kclust |&gt; glance()\n\n# A tibble: 1 × 4\n  totss tot.withinss betweenss  iter\n  &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 1364.         378.      986.     2\n\n\nTrying various values of k\n\nkmax &lt;- 9\npenguin_kclusts &lt;- \n  tibble(k = 1:kmax) |&gt;\n  mutate(\n    penguin_kclust = map(k, ~kmeans(penguins_km, .x)),\n    tidied = map(penguin_kclust, tidy),\n    glanced = map(penguin_kclust, glance),\n    augmented = map(penguin_kclust, augment, penguins_km)\n  )\n\npenguin_kclusts\n\n# A tibble: 9 × 5\n      k penguin_kclust tidied           glanced          augmented         \n  &lt;int&gt; &lt;list&gt;         &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n1     1 &lt;kmeans&gt;       &lt;tibble [1 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n2     2 &lt;kmeans&gt;       &lt;tibble [2 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n3     3 &lt;kmeans&gt;       &lt;tibble [3 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n4     4 &lt;kmeans&gt;       &lt;tibble [4 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n5     5 &lt;kmeans&gt;       &lt;tibble [5 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n6     6 &lt;kmeans&gt;       &lt;tibble [6 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n7     7 &lt;kmeans&gt;       &lt;tibble [7 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n8     8 &lt;kmeans&gt;       &lt;tibble [8 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n9     9 &lt;kmeans&gt;       &lt;tibble [9 × 7]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [342 × 5]&gt;\n\n\nFor each of the values of \\(k\\) the augmented, tidyed, and glanced information can be calculated. Note that you might want to find how the total within sum of squares decreases as a function of \\(k\\). (Will the within sum of squares always decrease as a function of \\(k?\\) Does it here? Why not?) It seems as though \\(k=3\\) or \\(k=4\\) is probably sufficient (larger values of \\(k\\) do not reduce the sums of squares substantially).\n\nclusters &lt;- \n  penguin_kclusts |&gt;\n  unnest(cols = c(tidied))\n\nassignments &lt;- \n  penguin_kclusts |&gt; \n  unnest(cols = c(augmented))\n\nclusterings &lt;- \n  penguin_kclusts |&gt;\n  unnest(cols = c(glanced))\n\n\nclusterings |&gt;\n  ggplot(aes(x = k, y = tot.withinss)) + \n  geom_line() + \n  geom_point() + ylab(\"\") +\n  ggtitle(\"Total Within Sum of Squares\")\n\n\n\n\n\n\n\nAdding back the rest of the penguin information (species, sex, island, etc.) in order to determine if the unsupervised clusters align with any of the non-numeric information given in the dataframe.\n\nassignments &lt;- penguins |&gt;\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) |&gt;\n  select(species, island, sex, year) |&gt;\n  slice(rep(1:n(), times = kmax)) |&gt;\n  cbind(assignments)\n\nFrom the two variables flipper_length_mm and bill_lengh_mm it seems that two clusters is probably sufficient. However, it would probably make sense to look at the cluster colorings across all four of the variables and in higher dimensional space (e.g., 3-D projections).\n\nassignments |&gt;\n ggplot(aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\n\n\n\n\n\n\n\nBased on species, the clustering (\\(k=2\\)) seems to separate out Gentoo, but it can’t really differentiate Adelie and Chinstrap. If we let \\(k=4\\), we get an almost perfect partition of the three species.\n\nassignments |&gt;\n  group_by(k) |&gt;\n  select(.cluster, species) |&gt;\n  table() |&gt;\n  as.data.frame() |&gt;\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = species, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") + \n  scale_fill_gradient(low = \"white\", high = \"red\") + \n  ggtitle(\"Species vs cluster prediction across different values of k\")\n\n\n\n\n\n\n\nFor island, it seems like the clustering isn’t able (really, at all!) to distinguish the penguins from the three islands.\n\nassignments |&gt;\n  group_by(k) |&gt;\n  select(.cluster, island) |&gt;\n  table() |&gt;\n  as.data.frame() |&gt;\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = island, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  ggtitle(\"Island vs cluster prediction across different values of k\")\n\n\n\n\n\n\n\n\n9.4.3 Partitioning Around Medoids\nAs an alternative to \\(k\\)-means, Kaufman and Rousseeuw developed Partitioning around Medoids (Finding Groups in Data: an introduction to cluster analysis, 1990). The particular strength of PAM is that it allows for any dissimilarity metric. That is, a dissimilarity based on correlations is no problem, but the algorithm gets more complicated because the “center” is no longer defined in Euclidean terms.\nThe two main steps are to Build (akin to assigning points to clusters) and to Swap (akin to redefining cluster centers). The objective function the algorithm tries to minimize is the average dissimilarity of objects to the closest representative object. (The PAM algorithm is a good (not necessarily global optimum) solution to minimizing the objective function.)\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k}D_i \\Bigg\\} = \\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k}d(x_i, m_k) \\Bigg\\}\\] Where \\(D_i\\) represents this distance from observation \\(i\\) to the closest medoid, \\(m_k\\).\nstrengths\n\nNo hierarchical structure / points can move from one cluster to another.\nCan run for a range of values of \\(k\\).\nIt can use any distance measure\n\nshortcomings\n\n\n\\(k\\) has to be predefined to run the algorithm.\n\n\nAlgorithm: Partitioning Around Medoids (From The Elements of Statistical Learning (2001), by Hastie, Tibshirani, and Friedman, pg 469.)\n\n\nRandomly assign a number, from 1 to \\(K\\), to each of the observations. These serve as initial cluster assignments for the observations.\nIterate until the cluster assignments stop changing:\n\n\n(Repeat for \\(k \\in \\{1, 2, ...K\\}\\)) For a given cluster, \\(C_k\\), find the observation in the cluster minimizing total distance to other points in that cluster: \\[i^*_k = \\text{arg}\\,\\min\\limits_{i \\in C_k} \\sum_{i' \\in C_k} d(x_i, x_{i'})\\] Then \\(m_k = x_{i^*_k}, k=1, 2, \\ldots, K\\) are the current estimates of the cluster centers.\nGiven a current set of cluster centers \\(\\{m_1, m_2, \\ldots, m_K\\}\\), minimize the total error by assigning each observation to the closest (current) cluster center: \\[C_i = \\text{arg}\\,\\min\\limits_{1 \\leq k \\leq K} d(x_i, m_k)\\]",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#evaluation-metrics",
    "href": "09-clustering.html#evaluation-metrics",
    "title": "9  Unsupervised Methods",
    "section": "\n9.5 Evaluation Metrics",
    "text": "9.5 Evaluation Metrics\n\nSilhouette Width\n\nConsider observation \\(i \\in\\) cluster \\(Clus1\\). Let \\[\\begin{align}\nd(i, Clus2) &= \\mbox{average dissimilarity of } i \\mbox{ to all objects in cluster } Clus2\\\\\na(i) &=  \\mbox{average dissimilarity of } i \\mbox{ to all objects in } Clus1.\\\\\nb(i) &= \\min_{Clus2 \\ne Clus1} d(i,Clus2) = \\mbox{distance to the next closest neighbor cluster}\\\\\n\\mbox{ silhouette width} &= s(i) = \\frac{b(i) - a(i)}{\\max \\{ a(i), b(i) \\}}\\\\\n& &\\\\\n\\mbox{average}_{i \\in Clus1} s(i) &= \\mbox{average silhouette width for cluster $Clus1$}\n\\end{align}\\] Note that if \\(a(i) &lt; b(i)\\) then \\(i\\) is well classified with a maximum \\(s(i) = 1\\). If \\(a(i) &gt; b(i)\\) then \\(i\\) is not well classified with a maximum \\(s(i) = -1\\).\n\nDiameter of cluster \\(Clus1\\) (within cluster measure) \\[\\begin{align}\n\\mbox{diameter} = \\max_{i,j \\in Clus1} d(i,j)\n\\end{align}\\]\nSeparation of cluster \\(Clus1\\) (between cluster measure) \\[\\begin{align}\n\\mbox{separation} = \\min_{i \\in Clus1, j \\notin Clus1} d(i,j)\n\\end{align}\\]\n\\(L^*\\): a cluster with diameter \\(&lt;\\) separation; \\(L\\): a cluster with \\(\\max_{j \\in Clus1} d(i,j) &lt; \\min_{k \\notin Clus1} d(i,k)\\).\n\nPAM example\nBuilding the clusters\n\n\n\nA\nB\nC\nD\nE\n\n\n\nA\n0\n\n\n\n\n\n\nB\n0.2\n0\n\n\n\n\n\nC\n0.6\n0.5\n0\n\n\n\n\nD\n1\n0.9\n0.4\n0\n\n\n\nE\n0.9\n0.8\n5\n0.3\n0\n\n\n\n\nStart by considering the random allocation (AC) and (BDE)\nAs a second step, calculate the within cluster sums of distances:\n\nA: 0.6\nC: 0.6\nB: 0.9 + 0.8 = 1.7\nD: 0.9 + 0.3 = 1.2\nE: 0.8 + 0.3 = 1.1\nFor cluster 1, it doesn’t matter if we choose A or C (let’s choose C). For cluster 2, we should choose E (it is the most “central” as measured by its closer distance to both B and D).\n\nReallocate points:\n\nCluster1: C and all the points that are closer to C than E. A and B are both closer to C than to E. Cluster1 will be (A,B,C).\nCluster2: E and all the points that are closer to E than C. Only D is closer to E than C. Cluster2 will be (D,E)\n\nRedefine cluster centers:\n\nA: 0.2 + 0.6 = 0.8\nB: 0.2 + 0.5 = 0.7\nC: 0.6 + 0.5 = 1.1\nD: 0.3\nE: 0.3\nCluster1 now has a medoid of B. Cluster2 (we choose randomly) has a medoid of D.\n\nReallocate points:\n\nCluster1: B and A (A,B)\nCluster2: D and C, E (D, C, E)\n\nThe medoids are now A or B (randomly choose) and D. The iteration process has converged.\nEvaluating the clusters\n\n\n\nA\nB\nC\nD\n\n\n\nA\n0\n\n\n\n\n\nB\n0.2\n0\n\n\n\n\nC\n0.6\n0.5\n0\n\n\n\nD\n1\n0.9\n0.4\n0\n\n\n\n(Note: this is the same matrix as before, but with only 4 observations.)\nConsider the data with (AB)(CD) as the clusters, we can calculate the previous metrics:\n\n\nSilhouette Width \\[\\begin{align}\ns(i=A) = \\frac{b(A) - a(A)}{\\max \\{a(A), b(A)\\}} = \\frac{0.8 - 0.2}{0.8} = 0.75\\\\\ns(i=B) = \\frac{b(B) - a(B)}{\\max \\{a(B), b(B)\\}} = \\frac{0.7 - 0.2}{0.7} = 0.71\\\\\ns(i=C) = \\frac{b(C) - a(C)}{\\max \\{a(C), b(C)\\}} = \\frac{0.55 - 0.4}{0.55} = .27\\\\\ns(i=D) = \\frac{b(D) - a(D)}{\\max \\{a(D), b(D)\\}} = \\frac{0.95 - 0.4}{0.95} = .57\\\\\n\\mbox{Ave SW} = 0.575\\\\\n\\end{align}\\]\n\n\nDiameter \\[\\begin{align}\n\\mbox{diameter}(AB) = 0.2\\\\\n\\mbox{diameter}(CD) = 0.4\\\\\n\\end{align}\\]\n\n\nSeparation \\[\\begin{align}\n\\mbox{separation}(AB) = \\mbox{separation}(CD) = 0.5\\\\\n\\end{align}\\]\n\nRand Index / Adjusted Rand Index\nis based on a confusion matrix comparing either a known truth (labels) or comparing two different clusterings (e.g., comparing \\(k\\)-means and hierarchical clustering). Let the two different clusterings be called partition1 and partition2. * a is the number of pairs of observations put together in both partition1 and partition2 * b is the number of pairs of observations together in partition1 and apart in partition2 * c is the number of pairs of observations together in partition2 and apart in partition1 * d is the number of pairs of observations apart in both partitions\n\\[\\mbox{Rand index} = \\frac{a+d}{a+b+c+d}\\]\nThe cool thing about the Rand index is that the partitions don’t have to even have the same number of clusters. They can be absolutely any two clusterings (one might be known labels, for example). Details on the Adjusted Rand index are given at http://faculty.washington.edu/kayee/pca/supp.pdf (basic idea is to center and scale the Rand index so that the values are more meaningful).",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "09-clustering.html#em-algorithm",
    "href": "09-clustering.html#em-algorithm",
    "title": "9  Unsupervised Methods",
    "section": "\n9.6 EM algorithm",
    "text": "9.6 EM algorithm\nThe EM algorithm is an incredibly useful tool for solving complicated maximization procedures, particularly with respect to maximizing likelihoods (typically for parameter estimation). We will describe the procedure here in the context of estimating the parameters of a two-component mixture model.\nConsider the Old Faithful geyser Yellowstone National Park, Wyoming, USA with the following histogram of data on waiting times between each eruption:\n\n\n\n\n\n\n\n\n\\[\\begin{align}\nY_1 &\\sim N(\\mu_1, \\sigma_1^2)\\\\\nY_2 &\\sim N(\\mu_2, \\sigma_2^2)\\\\\nY &= (1-\\Delta) Y_1 + \\Delta Y_2\\\\\nP(\\Delta=1) &= \\pi\\\\\n\\end{align}\\] In the simple two component case, we can see that the representation above indicates that first we generate a \\(\\Delta \\in \\{0,1\\}\\), and then, depending on the result, we generate either \\(Y_1\\) or \\(Y_2\\). The likelihood associated with the above setting is:\n\\[\\begin{align}\ng_Y(y) = (1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)\n\\end{align}\\] where \\(\\phi_\\theta\\) represents the normal distribution with a vector \\(\\theta=(\\mu, \\sigma)\\) of parameters. Typically, in statistical theory, to find \\(\\theta\\), we would take the derivative of the log-likelihood to find the values which maximize. Here, however, the likelihood is too complicated to solve for \\(\\theta\\) in closed form.\n\\[\\begin{align}\nl(\\theta; {\\bf y}) = \\sum_{i=1}^N \\log [(1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)].\n\\end{align}\\]\nIf we know which point comes from which distribution, however, the maximization is straightforward in that we can use the points in group one to estimate the parameters from the first distribution, and the points in group two to estimate the parameters in the second distribution. The process of assigning points and estimating parameters can be thought of as two steps:\n\n\nExpectation: an assignment (soft here, because the points are weighted) of each observation to a group.\n\nMaximization: update the parameter estimates.\n\n\nAlgorithm: EM Algorithm for two-component Gaussian mixture. From The Elements of Statistical Learning (2001), by Hastie, Tibshirani, and Friedman, pg 238.\n\n\nTake initial guesses for the parameters \\(\\hat{\\mu}_1, \\hat{\\sigma}_1^2, \\hat{\\mu}_2, \\hat{\\sigma}_2^2, \\hat{\\pi}\\).\n\nExpectation Step: compute the responsibilities: \\[ \\hat{\\gamma}_i = \\frac{\\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}{(1-\\hat{\\pi}) \\phi_{\\hat{\\theta}_1} (y_i) + \\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}, i=1, 2, \\ldots, N.\\]\n\n\nMaximization Step: compute the weighted means and variances: \\[\\begin{align}\n\\hat{\\mu}_1 = \\frac{\\sum_{i=1}^N (1-\\hat{\\gamma_i})y_i}{\\sum_{i=1}^N (1-\\hat{\\gamma_i})} && \\hat{\\sigma}_1^2 = \\frac{\\sum_{i=1}^N (1-\\hat{\\gamma_i})(y_i - \\hat{\\mu}_1)^2}{\\sum_{i=1}^N (1-\\hat{\\gamma_i})}\\\\\n\\hat{\\mu}_2 = \\frac{\\sum_{i=1}^N \\hat{\\gamma_i}y_i}{\\sum_{i=1}^N \\hat{\\gamma_i}} && \\hat{\\sigma}_2^2 = \\frac{\\sum_{i=1}^N \\hat{\\gamma_i}(y_i - \\hat{\\mu}_2)^2}{\\sum_{i=1}^N \\hat{\\gamma_i}}\n\\end{align}\\] and the mixing probability \\(\\hat{\\pi} = \\sum_{i=1}^N \\hat{\\gamma}_i / N\\).\nIterate Steps 2. and 3. until convergence.\n\n\nThe algorithm shows that for a particular allocation of the points, we can maximize the given likelihood to estimate the parameter values (done in the Maximization Step). However, it is not obvious from the algorithm that the first allocation step leads to a maximization (local or global) of the likelihood. The proof of the EM algorithm converging to a local maximum likelihood (it does not necessarily converge to a global max) uses information on the marginal prior and posterior likelihoods of the parameter values and Jensen’s inequality to show that the likelihood does not decrease through the iterative steps.\nNote that in the previous \\(k\\)-means algorithm we iterated between two steps of assigning points to clusters and estimating the cluster centers (we thought of the space as scaled so that the Euclidean distance was appropriate in all dimensions). Two differences in the algorithms we covered are:\n\n\n\\(k\\)-means uses hard thresholding and EM uses soft thresholding\n\n\\(k\\)-means uses a fixed standard deviation of 1, EM allows the data/algorithm to find the standard deviation\n\nIndeed, although the EM-algorithm above is slightly different than the previous \\(k\\)-means algorithm, the two methods typically converge to the same result and are both considered to be different implementations of a \\(k\\)-means algorithm.\nSee the following applet for a visual representation of how the EM-algorithm converges: http://www.socr.ucla.edu/applets.dir/mixtureem.html.\n\n\n\nThe Hamming distance across the two DNA strands is 7.\nThe function dist in R calculates the distances given above.\nComparison of string distance metrics from https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html.\nFrom An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.\nFrom An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Unsupervised Methods</span>"
    ]
  },
  {
    "objectID": "08-classification.html",
    "href": "08-classification.html",
    "title": "8  Classification",
    "section": "",
    "text": "8.1 Model Building Process\nAll classification and prediction models have the same basic steps. The data is preprocessed, the model is trained, and then the model is validated.\nIf the variables and information used to train the model has not been fully tuned, processed, and considered for the model, it won’t matter how sophisticated or special the model is. Garbage in, garbage out.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#model-building-process",
    "href": "08-classification.html#model-building-process",
    "title": "8  Classification",
    "section": "",
    "text": "8.1.1 Cross Validation\nBias-variance trade-off\nExcellent resource\nfor explaining the bias-variance trade-off: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\nVariance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training set. Generally, the closer the model fits the data, the more variable it will be (it’ll be different for each data set!). A model with many many explanatory variables will often fit the data too closely.\nBias refers to the error that is introduced by approximating the “truth” by a model which is too simple. For example, we often use linear models to describe complex relationships, but it is unlikely that any real life situation actually has a true linear model. However, if the true relationship is close to linear, then the linear model will have a low bias.\n\nGenerally, the simpler the model, the lower the variance. The more complicated the model, the lower the bias. In this class, cross validation will be used to assess model fit. [If time permits, Receiver Operating Characteristic (ROC) curves will also be covered.]\n\\[\\begin{align}\n\\mbox{prediction error } = \\mbox{ irreducible error } + \\mbox{ bias } + \\mbox{ variance}\n\\end{align}\\]\n\n\nirreducible error The irreducible error is the natural variability that comes with observations. No matter how good the model is, we will never be able to predict perfectly.\n\nbias The bias of the model represents the difference between the true model and a model which is too simple. That is, the more complicated the model (e.g., smaller \\(k\\) in \\(k\\)NN), the closer the points are to the prediction. As the model gets more complicated (e.g., as \\(k\\) decreases), the bias goes down.\n\nvariance The variance represents the variability of the model from sample to sample. That is, a simple model (big \\(k\\) in \\(k\\)NN) would not change a lot from sample to sample. The variance decreases as the model becomes more simple (e.g., as \\(k\\) increases).\n\nNote the bias-variance trade-off. We want our prediction error to be small, so we choose a model that is medium with respect to both bias and variance. We cannot control the irreducible error.\n\n\n\n\n\n\n\nTest and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! (Hastie, Tibshirani, and Friedman 2001)\n\n\n\nThe following interactive visualization does an excellent job of communicating the trade-off between bias and variance as a function of a specific tuning parameter, here: minimum node size of a classification tree. http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n\n\n\n\n\n\n\nGreat interactive viz at http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n\n\n\nImplementing Cross Validation\n\n\n\n\n\n\n\n(Flach 2012)\n\n\n\nCross validation is typically used in two ways.\n\nTo assess a model’s accuracy (model assessment).\n\nTo build a model (model selection).\nDifferent ways to CV\nSuppose that we build a classifier on a given data set. We’d like to know how well the model classifies observations, but if we test on the samples at hand, the error rate will be much lower than the model’s inherent accuracy rate. Instead, we’d like to predict new observations that were not used to create the model. There are various ways of creating test or validation sets of data:\n\none training set, one test set [two drawbacks: estimate of error is highly variable because it depends on which points go into the training set; and because the training data set is smaller than the full data set, the error rate is biased in such a way that it overestimates the actual error rate of the modeling technique.]\nleave one out cross validation (LOOCV)\n\n\nremove one observation\nbuild the model using the remaining n-1 points\npredict class membership for the observation which was removed\nrepeat by removing each observation one at a time\n\n\n\n\\(V\\)-fold cross validation (\\(V\\)-fold CV)\n\nlike LOOCV except that the algorithm is run \\(V\\) times on each group (of approximately equal size) from a partition of the data set.]\nLOOCV is a special case of \\(V\\)-fold CV with \\(V=n\\)\n\nadvantage of \\(V\\)-fold is computational\n\n\\(V\\)-fold often has a better bias-variance trade-off [bias is lower with LOOCV. however, because LOOCV predicts \\(n\\) observations from \\(n\\) models which are basically the same, the variability will be higher (i.e., based on the \\(n\\) data values). with \\(V\\)-fold, prediction is on \\(n\\) values from \\(V\\) models which are much less correlated. the effect is to average out the predicted values in such a way that there will be less variability from data set to data set.]\n\n\nCV for Model assessment 10-fold\n\nassume \\(k\\) is given for \\(k\\)-NN\nremove 10% of the data\nbuild the model using the remaining 90%\npredict class membership / continuous response for the 10% of the observations which were removed\nrepeat by removing each decile one at a time\na good measure of the model’s ability to predict is the error rate associated with the predictions on the data which have been independently predicted\nCV for Model selection 10-fold\n\nset \\(k\\) in \\(k\\)-NN\nbuild the model using the \\(k\\) value set above:\n\nremove 10% of the data\nbuild the model using the remaining 90%\npredict class membership / continuous response for the 10% of the observations which were removed\nrepeat by removing each decile one at a time\n\n\nmeasure the CV prediction error for the \\(k\\) value at hand\nrepeat steps 1-3 and choose the \\(k\\) for which the prediction error is lowest\nCV for Model assessment and selection 10-fold\nTo do both, one approach is to use test/training data and CV in order to both model assessment and selection. Note that CV could be used in both steps, but the algorithm is slightly more complicated.\n\nsplit the data into training and test observations\nset \\(k\\) in \\(k\\)-NN\nbuild the model using the \\(k\\) value set above on only the training data:\n\nremove 10% of the training data\nbuild the model using the remaining 90% of the training data\npredict class membership / continuous response for the 10% of the training observations which were removed\nrepeat by removing each decile one at a time from the training data\n\n\nmeasure the CV prediction error for the \\(k\\) value at hand on the training data\nrepeat steps 2-4 and choose the \\(k\\) for which the prediction error is lowest for the training data\nusing the \\(k\\) value given in step 5, assess the prediction error on the test data\n\n\n\n\n\n\n\n\nNested cross-validation: two cross-validation loops are run one inside the other. (Varoquaux et al. 2017)\n\n\n\n\n8.1.2 tidymodels\n\nThe tidymodels framework provides a series of steps that allow for systematic model building. The steps are:\n\npartition the data\nbuild a recipe\nselect a model\ncreate a workflow\nfit the model\n\nvalidate the model\n\nThe process is synthesized in the following graphic from a course at Johns Hopkins, Tidyverse Skills for Data Science.\n\n\n\n\n\n\n\nImage credit: Wright et al., Chapter 5 of Tidyverse Skills for Data Science https://jhudatascience.org/tidyversecourse/\n\n\n\n1. Partition the data\nPut the testing data in your pocket (keep it secret from R!!)\n\n\n\n\n\n\n\nImage credit: Julia Silge\n\n\n\n2. build a recipe\n\nStart the recipe()\n\nDefine the variables involved\nDescribe preprocessing step-by-step\n\n\nfeature engineering or preprocessing:\n\nfeature engineering is the process of transforming raw data into features (variables) that are better predictors (for the model at hand).\n\nExamples include:\n\ncreate new variables (e.g., combine levels -&gt; from state to region)\ntransform variable (e.g., log, polar coordinates)\ncontinuous variables -&gt; discrete (e.g., binning)\nnumerical categorical data -&gt; factors / character strings (one hot encoding)\ntime -&gt; discretized time\nmissing values -&gt; imputed\nNA -&gt; level\ncontinuous variables -&gt; center & scale (“normalize”)\n\nstep_ functions\nFor more information: https://recipes.tidymodels.org/reference/index.html\n\nls(pattern = '^step_', env = as.environment('package:recipes'))\n\n [1] \"step_arrange\"            \"step_bagimpute\"         \n [3] \"step_bin2factor\"         \"step_BoxCox\"            \n [5] \"step_bs\"                 \"step_center\"            \n [7] \"step_classdist\"          \"step_classdist_shrunken\"\n [9] \"step_corr\"               \"step_count\"             \n[11] \"step_cut\"                \"step_date\"              \n[13] \"step_depth\"              \"step_discretize\"        \n[15] \"step_dummy\"              \"step_dummy_extract\"     \n[17] \"step_dummy_multi_choice\" \"step_factor2string\"     \n[19] \"step_filter\"             \"step_filter_missing\"    \n[21] \"step_geodist\"            \"step_harmonic\"          \n[23] \"step_holiday\"            \"step_hyperbolic\"        \n[25] \"step_ica\"                \"step_impute_bag\"        \n[27] \"step_impute_knn\"         \"step_impute_linear\"     \n[29] \"step_impute_lower\"       \"step_impute_mean\"       \n[31] \"step_impute_median\"      \"step_impute_mode\"       \n[33] \"step_impute_roll\"        \"step_indicate_na\"       \n[35] \"step_integer\"            \"step_interact\"          \n[37] \"step_intercept\"          \"step_inverse\"           \n[39] \"step_invlogit\"           \"step_isomap\"            \n[41] \"step_knnimpute\"          \"step_kpca\"              \n[43] \"step_kpca_poly\"          \"step_kpca_rbf\"          \n[45] \"step_lag\"                \"step_lincomb\"           \n[47] \"step_log\"                \"step_logit\"             \n[49] \"step_lowerimpute\"        \"step_meanimpute\"        \n[51] \"step_medianimpute\"       \"step_modeimpute\"        \n[53] \"step_mutate\"             \"step_mutate_at\"         \n[55] \"step_naomit\"             \"step_nnmf\"              \n[57] \"step_nnmf_sparse\"        \"step_normalize\"         \n[59] \"step_novel\"              \"step_ns\"                \n[61] \"step_num2factor\"         \"step_nzv\"               \n[63] \"step_ordinalscore\"       \"step_other\"             \n[65] \"step_pca\"                \"step_percentile\"        \n[67] \"step_pls\"                \"step_poly\"              \n[69] \"step_poly_bernstein\"     \"step_profile\"           \n[71] \"step_range\"              \"step_ratio\"             \n[73] \"step_regex\"              \"step_relevel\"           \n[75] \"step_relu\"               \"step_rename\"            \n[77] \"step_rename_at\"          \"step_rm\"                \n[79] \"step_rollimpute\"         \"step_sample\"            \n[81] \"step_scale\"              \"step_select\"            \n[83] \"step_shuffle\"            \"step_slice\"             \n[85] \"step_spatialsign\"        \"step_spline_b\"          \n[87] \"step_spline_convex\"      \"step_spline_monotone\"   \n[89] \"step_spline_natural\"     \"step_spline_nonnegative\"\n[91] \"step_sqrt\"               \"step_string2factor\"     \n[93] \"step_time\"               \"step_unknown\"           \n[95] \"step_unorder\"            \"step_window\"            \n[97] \"step_YeoJohnson\"         \"step_zv\"                \n\n\n3. select a model\nTo specify a model:\n\npick a model\n\nset the mode (regression vs classification, if needed)\nset the engine\n\n\nExamples of engines for some of the classification algorithms we will cover in class:\n\nshow_engines(\"nearest_neighbor\")\n\n# A tibble: 2 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 kknn   classification\n2 kknn   regression    \n\nshow_engines(\"decision_tree\")\n\n# A tibble: 5 × 2\n  engine mode          \n  &lt;chr&gt;  &lt;chr&gt;         \n1 rpart  classification\n2 rpart  regression    \n3 C5.0   classification\n4 spark  classification\n5 spark  regression    \n\nshow_engines(\"rand_forest\")\n\n# A tibble: 6 × 2\n  engine       mode          \n  &lt;chr&gt;        &lt;chr&gt;         \n1 ranger       classification\n2 ranger       regression    \n3 randomForest classification\n4 randomForest regression    \n5 spark        classification\n6 spark        regression    \n\nshow_engines(\"svm_poly\")\n\n# A tibble: 2 × 2\n  engine  mode          \n  &lt;chr&gt;   &lt;chr&gt;         \n1 kernlab classification\n2 kernlab regression    \n\nshow_engines(\"svm_rbf\")\n\n# A tibble: 4 × 2\n  engine    mode          \n  &lt;chr&gt;     &lt;chr&gt;         \n1 kernlab   classification\n2 kernlab   regression    \n3 liquidSVM classification\n4 liquidSVM regression    \n\nshow_engines(\"linear_reg\")\n\n# A tibble: 7 × 2\n  engine mode      \n  &lt;chr&gt;  &lt;chr&gt;     \n1 lm     regression\n2 glm    regression\n3 glmnet regression\n4 stan   regression\n5 spark  regression\n6 keras  regression\n7 brulee regression\n\n\n4. Create a workflow\nA workflow combines the model / engine with the recipe.\n5. Fit the model\nPutting it all together, the fit() will give the model specifications.\n6. Validate the model\nmodel parameters\n\n\nSome model parameters are tuned from the data (some aren’t).\n\nlinear model coefficients are optimized (not tuned)\nk-nn value of “k” is tuned\n\n\nIf the model is tuned using the data, the same data cannot be used to assess the model.\nWith Cross Validation, you iteratively put data in your pocket.\nFor example, keep 1/5 of the data in your pocket, build the model on the remaining 4/5 of the data.\n\nCross validation for tuning parameters. Note that all of the cross validation is done on the training data.\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\\[\\bigg\\Downarrow\\]\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\nReflecting on Model Building\nIn Tidy Modeling with R, Kuhn and Silge walk through an example of an entire model building process. Note that each of the stages is visited often before coming up with an appropriate model.\n\n\n\n\n\n\n\nImage credit: https://www.tmwr.org/\n\n\n\n\n\n\n\n\n\n\nImage credit: https://www.tmwr.org/\n\n\n\n\n\n\n\n\n\n\nImage credit: https://www.tmwr.org/\n\n\n\n\n8.1.3 R model: penguins\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n1. Partition the data\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nset.seed(47)\npenguin_split &lt;- initial_split(penguins)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\n2. build a recipe\n\npenguin_recipe &lt;-\n  recipe(body_mass_g ~ species + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm + sex + year,\n         data = penguin_train) |&gt;\n  step_mutate(year = as.factor(year)) |&gt;\n  step_unknown(sex, new_level = \"unknown\") |&gt;\n  step_relevel(sex, ref_level = \"female\") |&gt;\n  update_role(island, new_role = \"id variable\")\n\n\nsummary(penguin_recipe)\n\n# A tibble: 8 × 4\n  variable          type      role        source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n1 species           &lt;chr [3]&gt; predictor   original\n2 island            &lt;chr [3]&gt; id variable original\n3 bill_length_mm    &lt;chr [2]&gt; predictor   original\n4 bill_depth_mm     &lt;chr [2]&gt; predictor   original\n5 flipper_length_mm &lt;chr [2]&gt; predictor   original\n6 sex               &lt;chr [3]&gt; predictor   original\n7 year              &lt;chr [2]&gt; predictor   original\n8 body_mass_g       &lt;chr [2]&gt; outcome     original\n\n\n3. select a model\n\npenguin_lm &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\npenguin_lm\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n4. Create a workflow\n\npenguin_wflow &lt;- workflow() |&gt;\n  add_model(penguin_lm) |&gt;\n  add_recipe(penguin_recipe)\n\n\npenguin_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_mutate()\n• step_unknown()\n• step_relevel()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n5. Fit the model\n\npenguin_fit &lt;- penguin_wflow |&gt;\n  fit(data = penguin_train)\n\n\npenguin_fit |&gt; tidy()\n\n# A tibble: 10 × 5\n   term              estimate std.error statistic  p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)        -2417.     665.      -3.64  3.36e- 4\n 2 speciesChinstrap    -208.      92.9     -2.24  2.58e- 2\n 3 speciesGentoo        985.     152.       6.48  5.02e-10\n 4 bill_length_mm        13.5      8.29     1.63  1.04e- 1\n 5 bill_depth_mm         80.9     22.1      3.66  3.10e- 4\n 6 flipper_length_mm     20.8      3.62     5.74  2.81e- 8\n 7 sexmale              351.      52.6      6.67  1.72e-10\n 8 sexunknown            47.6    103.       0.460 6.46e- 1\n 9 year2008             -24.8     47.5     -0.521 6.03e- 1\n10 year2009             -61.9     46.0     -1.35  1.80e- 1\n\n\n6. Cross validation\n(See Section @ref(cv) and future R examples for a full description of cross validation.)",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#knn",
    "href": "08-classification.html#knn",
    "title": "8  Classification",
    "section": "\n8.2 \\(k\\)-Nearest Neighbors",
    "text": "8.2 \\(k\\)-Nearest Neighbors\nThe \\(k\\)-Nearest Neighbor algorithm does exactly what it sounds like it does.\n\nuser decides on the integer value for \\(k\\)\nuser decides on a distance metric (most \\(k\\)-NN algorithms default to Euclidean distance)\na point is classified to be in the same group as the majority of the \\(k\\) closest points in the training data.\n\n\n8.2.1 \\(k\\)-NN algorithm\n\nDecide on a distance metric (e.g., Euclidean distance, 1 - correlation, etc.) and find the distances from each point in the test set to each point in the training set. The distance is measured in the feature space, that is, with respect to the explanatory variables (not the response variable).\n\nn.b. In most machine learning algorithms that use “distance” as a measure, the “distance” is not required to be a mathematical distance metric. Indeed, 1-correlation is a very common distance measure, and it fails the triangle inequality.\n\nConsider a point in the test set. Find the \\(k\\) closest points in the training set to the one test observation.\nUsing majority vote, find the dominate class of the \\(k\\) closest points. Predict that class label to the test observation.\n\nNote: if the response variable is continuous (instead of categorical), find the average response variable of the \\(k\\) training point to be the predicted response for the one test observation.\nShortcomings of \\(k\\)-NN:\n\none class can dominate if it has a large majority\nEuclidean distance is dominated by scale\nit can be computationally unwieldy (and unneeded!!) to calculate all distances (there are algorithms to search smartly)\nthe output doesn’t provide any information about which explanatory variables are informative.\ndoesn’t work well with large datasets (the cost of prediction is high, and the model doesn’t always find the structure)\ndoesn’t work well in high dimensions (curse of dimensionality – distance becomes meaningless in high dimensions)\nwe need a lot of feature scaling\nsensitive to noise and outliers\n\nStrengths of \\(k\\)-NN:\n\nit can easily work for any number of categories (of the outcome variable)\nit can predict a quantitative response variable\nthe bias of 1-NN is often low (but the variance is high)\nany distance metric can be used (so the algorithm models the data appropriately)\nthe method is straightforward to implement / understand\nthere is no training period (i.e., no discrimination function is created)\nmodel is nonparametric (no distributional assumptions on the data)\ngreat model for imputing missing data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.2.2 R k-NN: penguins\nWe will fit a \\(k\\)-Nearest Neighbor algorithm to the penguins dataset. As previously (and as to come), we’ll use the entire tidymodels workflow including partitioning the data, build a recipe, select a model, create a workflow, fit a model, and validate the model\n\nlibrary(GGally) # for plotting\nlibrary(tidymodels)\ndata(penguins)\n\npenguin data\n\nggpairs(penguins, mapping = ggplot2::aes(color = species), alpha=.4)\n\n\n\n\n\n\n\n\n\\(k\\)-NN to predict penguin species\n1. Partition the data\n2. Build a recipe\n\npenguin_knn_recipe &lt;-\n  recipe(species ~ body_mass_g + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm,\n         data = penguin_train) |&gt;\n  update_role(island, new_role = \"id variable\") |&gt;\n  step_normalize(all_predictors())\n\nsummary(penguin_knn_recipe)\n\n# A tibble: 6 × 4\n  variable          type      role        source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;       &lt;chr&gt;   \n1 body_mass_g       &lt;chr [2]&gt; predictor   original\n2 island            &lt;chr [3]&gt; id variable original\n3 bill_length_mm    &lt;chr [2]&gt; predictor   original\n4 bill_depth_mm     &lt;chr [2]&gt; predictor   original\n5 flipper_length_mm &lt;chr [2]&gt; predictor   original\n6 species           &lt;chr [3]&gt; outcome     original\n\n\n3. Select a model\n(note that we’ve used the default number of neighbors (here \\(k=7\\)).)\n\npenguin_knn &lt;- nearest_neighbor() |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\")\n\npenguin_knn\n\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n4. Create a workflow\n\npenguin_knn_wflow &lt;- workflow() |&gt;\n  add_model(penguin_knn) |&gt;\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nComputational engine: kknn \n\n\n5. Fit (/ predict)\n\npenguin_knn_fit &lt;- penguin_knn_wflow |&gt;\n  fit(data = penguin_train)\n\nFor the next R code chunk break it down into pieces – that is, run each line one at a time.\n\npenguin_knn_fit |&gt; \n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  metrics(truth = species, estimate = .pred_class) |&gt;\n  filter(.metric == \"accuracy\")\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.988\n\n\nWhat is \\(k\\)?\nIt turns out that the default value for \\(k\\) in the kknn engine is 7. Is 7 best?\nCross Validation!!!\nThe red observations are used to fit the model, the black observations are used to assess the model.\n\n\n\n\n\n\n\nImage credit: Alison Hill\n\n\n\nAs we saw above, cross validation randomly splits the training data into V distinct blocks of roughly equal size.\n\nleave out the first block of analysis data and fit a model.\nthe model is used to predict the held-out block of assessment data.\ncontinue the process until all V assessment blocks have been predicted\n\nThe final performance is based on the hold-out predictions by averaging the statistics from the V blocks.\n1b. A new partition of the training data\n\nset.seed(470)\npenguin_vfold &lt;- vfold_cv(penguin_train,\n                          v = 3, strata = species)\n\n3. Select a model\nNow the knn model uses tune() to indicate that we actually don’t know how many neighbors to use.\n\npenguin_knn_tune &lt;- nearest_neighbor(neighbors = tune()) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\")\n\n4. Re-create a workflow\nThis time, use the model that has not set the number of neighbors.\n\npenguin_knn_wflow_tune &lt;- workflow() |&gt;\n  add_model(penguin_knn_tune) |&gt;\n  add_recipe(penguin_knn_recipe)\n\n5. Fit the model\nThe model is fit to all three of the folds created above for each value of \\(k\\) in k_grid.\n\nk_grid &lt;- data.frame(neighbors = seq(1, 15, by = 4))\nk_grid\n\n  neighbors\n1         1\n2         5\n3         9\n4        13\n\npenguin_knn_wflow_tune |&gt;\n  tune_grid(resamples = penguin_vfold, \n           grid = k_grid) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"accuracy\")\n\n# A tibble: 4 × 7\n  neighbors .metric  .estimator  mean     n   std_err .config             \n      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;               \n1         1 accuracy multiclass 0.971     2 0.00595   Preprocessor1_Model1\n2         5 accuracy multiclass 0.977     2 0.000134  Preprocessor1_Model2\n3         9 accuracy multiclass 0.988     2 0.0000668 Preprocessor1_Model3\n4        13 accuracy multiclass 0.983     2 0.00568   Preprocessor1_Model4\n\n\n6. Validate the model\nUsing \\(k\\) = 9, the model is re-trained on the training data and tested on the test data (to estimate overall model accuracy).\n3. select a model\n\npenguin_knn_final &lt;- nearest_neighbor(neighbors = 9) |&gt;\n  set_engine(\"kknn\") |&gt;\n  set_mode(\"classification\")\n\npenguin_knn_final\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 9\n\nComputational engine: kknn \n\n\n4. create a workflow\n\npenguin_knn_wflow_final &lt;- workflow() |&gt;\n  add_model(penguin_knn_final) |&gt;\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow_final\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: nearest_neighbor()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = 9\n\nComputational engine: kknn \n\n\n5. fit the model\n\npenguin_knn_fit_final &lt;- penguin_knn_wflow_final |&gt;\n  fit(data = penguin_train)\n\n6. validate the model\n\npenguin_knn_fit_final |&gt; \n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  metrics(truth = species, estimate = .pred_class) |&gt;\n  filter(.metric == \"accuracy\")\n\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy multiclass     0.977\n\n\nHuh. Seems like \\(k=9\\) didn’t do as well as \\(k=7\\) (the value we tried at the very beginning before cross validating).\nWell, it turns out, that’s the nature of variability, randomness, and model building.\nWe don’t know truth, and we won’t every find a perfect model.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#cart",
    "href": "08-classification.html#cart",
    "title": "8  Classification",
    "section": "\n8.3 Decision Trees",
    "text": "8.3 Decision Trees\nStephanie Yee and Tony Chu created the following (amazing!) demonstration for tree intuition. Step-by-step, they build a recursive binary tree in order to model the differences between homes in SF and homes in NYC.\n\n\n\n\n\n\n\nhttp://www.r2d3.us/visual-intro-to-machine-learning-part-1/ A visual introduction to machine learning.\n\n\n\nDecision trees are used for all sorts of predictive and descriptive models. The NYT created a recursive binary decision tree to show patterns in identity and political affiliation.\n\n\n\n\n\n\n\nhttps://www.nytimes.com/interactive/2019/08/08/opinion/sunday/party-polarization-quiz.html Quiz: Let Us Predict Whether You’re a Democrat or a Republican NYT, Aug 8, 2019. Note that race is the first and dominant node, followed by religion.\n\n\n\n\n8.3.1 CART algorithm\nBasic Classification and Regression Trees (CART) Algorithm:\n\nStart with all observations in one group.\nFind the variable/split that best separates the response variable (successive binary partitions based on the different predictors / explanatory variables).\n\nEvaluation “homogeneity” within each group\nDivide the data into two groups (“leaves”) on that split (“node”).\nWithin each split, find the best variable/split that separates the outcomes.\n\n\nContinue until the groups are too small or sufficiently “pure”.\nPrune tree.\n\nShortcomings of CART:\n\nStraight CART do not generally have the same predictive accuracy as other classification approaches. (we will improve the model - see Random Forests, boosting, bagging)\nDifficult to write down / consider the CART “model”\nWithout proper pruning, the model can easily lead to overfitting\nWith lots of predictors, (even greedy) partitioning can become computationally unwieldy\nOften, prediction performance is poor\n\nStrengths of CART:\n\nThey are easy to explain; trees are easy to display graphically (which make them easy to interpret). (They mirror the typical human decision-making process.)\nCan handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!).\nCan handle more than 2 groups for categorical predictions\nEasily ignore redundant variables.\nPerform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables.\nData transformations may be less important (monotone transformations on the explanatory variables won’t change anything).\n\nClassification Trees\nA classification tree is used to predict a categorical response variable (rather than a quantitative one). The end predicted value will be the one of the most commonly occurring class of training observations in the region to which it belongs. The goal is to create regions which are as homogeneous as possible with respect to the response variable - categories.\nmeasures of impurity\n\nCalculate the classification error rate as the fraction of the training observations in that region that do not belong to the most common class: \\[E_m = 1 - \\max_k(\\hat{p}_{mk})\\] where \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)th region that are from the \\(k\\)th class. However, the classification error rate is not particularly sensitive to node purity, and so two additional measures are typically used to partition the regions.\nFurther, the Gini index is defined by \\[G_m= \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})\\] a measure of total variance across the \\(K\\) classes. [Recall, the variance of a Bernoulli random variable with \\(\\pi\\) = P(success) is \\(\\pi(1-\\pi)\\).] Note that the Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\) values are close to zero or one. For this reason, the Gini index is referred to as a measure of node purity - a small value indicates that a node contains predominantly observations from a single class.\nLast, the cross-entropy is defined as \\[D_m = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}\\] Since \\(0 \\leq \\hat{p}_{mk} \\leq 1\\) it follows that \\(0 \\leq -\\hat{p}_{mk} \\log\\hat{p}_{mk}\\). One can show that the cross-entropy will take on a value near zero if the \\(\\hat{p}_{mk}\\) values are all near zero or all near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the \\(m\\)th node is pure.\nTo build the tree, typically the Gini index or the cross-entropy are used to evaluate a particular split.\nTo prune the tree, often classification error is used (if accuracy of the final pruned tree is the goal)\n\nComputationally, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a top down approach to the problem which is known as recursive binary splitting (greedy because we look only at the current split and not at the outcomes of the splits to come).\nRecursive Binary Splitting on Categories (for a given node)\n\nSelect the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j&lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) lead to the greatest reduction in Gini index or cross-entropy.\nFor any \\(j\\) and \\(s\\), define the pair of half-planes to be \\[R_1(j,s) = \\{X | X_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{X | X_j \\geq s\\}\\] and we seek the value of \\(j\\) and \\(s\\) that minimize the equation: \\[\\begin{align}\n& \\sum_{i:x_i \\in R_1(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + \\sum_{i:x_i \\in R_2(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\mbox{equivalently: } & n_{R_1} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + n_{R_2} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\end{align}\\]\n\nRepeat the process, looking for the best predictor and best cutpoint within one of the previously identified regions (producing three regions, now).\nKeep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations.\nRegression Trees\nThe goal of the algorithm in a regression tree is to split the set of possible value for the data into \\(|T|\\) distinct and non-overlapping regions, \\(R_1, R_2, \\ldots, R_{|T|}\\). For every observation that falls into the region \\(R_m\\), we make the same prediction - the mean of the response values for the training observations in \\(R_m\\). So how do we find the regions \\(R_1, \\ldots, R_{|T|}\\)?\n\\(\\Rightarrow\\) Minimize RSS, \\[RSS = \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2\\] where \\(\\overline{y}_{R_m}\\) is the mean response for the training observations within the \\(m\\)th region.\n(Note: in the chapter (James et al. 2021) they refer to MSE - mean squared error - in addition to RSS where MSE is simply RSS / n, see equation (2.5).)\n\nAgain, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a top down approach to the problem which is known as recursive binary splitting (greedy because we look only at the current split and not at the outcomes of the splits to come).\nRecursive Binary Splitting on Numerical Response (for a given node)\n\nSelect the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j&lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) lead to the greatest reduction in RSS.\nFor any \\(j\\) and \\(s\\), define the pair of half-planes to be \\[R_1(j,s) = \\{X | X_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{X | X_j \\geq s\\}\\] and we see the value of \\(j\\) and \\(s\\) that minimize the equation: \\[\\sum_{i:x_i \\in R_1(j,s)} (y_i - \\overline{y}_{R_1})^2 + \\sum_{i:x_i \\in R_2(j,s)} (y_i - \\overline{y}_{R_2})^2\\] where \\(\\overline{y}_{R_1}\\) is the mean response for the training observations in \\(R_1(j,s)\\) and \\(\\overline{y}_{R_2}\\) is the mean response for training observations in \\(R_2(j,s)\\).\nRepeat the process, looking for the best predictor and best cutpoint within one of the previously identified regions (producing three regions, now).\nKeep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations.\n(Avoiding) Overfitting\nIdeally, the tree would not overfit the training data. One could imagine how easy it would be to grow the tree over the training data so as to end up with terminal nodes which are completely homogeneous (but then don’t represent the test data).\nSee the following (amazing!) demonstration for intuition on model validation / overfitting: http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\nOne possible algorithm for building a tree is to split based on the reduction in RSS (or Gini index, etc.) exceeding some (presumably high) threshold. However, the strategy is known to be short sighted, as a split later down the tree may contain a large amount of information. A better strategy is to grow a very large tree \\(T_0\\) and then prune it back in order to obtain a subtree. Use cross validation to build the subtree so as to not overfit the data.\n\nAlgorithm: Building a Regression Tree\n\n\nUse recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\nApply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\).\nUse \\(V\\)-fold cross-validation to choose \\(\\alpha\\). That is, divide the training observations into \\(V\\) folds. For each \\(v=1, 2, \\ldots, V\\):\n\nRepeat Steps 1 and 2 on all but the \\(V\\)th fold of the training data.\nEvaluate the mean squared prediction error on the data in the left-out \\(k\\)th fold, as a function of \\(\\alpha\\). For each value of \\(\\alpha\\), average the prediction error (either misclassification or RSS), and pick \\(\\alpha\\) to minimize the average error.\n\n\nReturn the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\).\n\nCost Complexity Pruning\nAlso known as weakest link pruning, the idea is to consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\) (instead of considering every single subtree). Generally, the idea is that there is a cost to having a larger (more complex!) tree. We define the cost complexity criterion (\\(\\alpha &gt; 0\\)): \\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} I(y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\] where \\(k(m)\\) is the class with the majority of observations in node \\(m\\) and \\(|T|\\) is the number of terminal nodes in the tree.\n\n\n\\(\\alpha\\) small: If \\(\\alpha\\) is set to be small, we are saying that the risk is more worrisome than the complexity and larger trees are favored because they reduce the risk.\n\n\\(\\alpha\\) large: If \\(\\alpha\\) is set to be large, then the complexity of the tree is more worrisome and smaller trees are favored.\n\nThe way to think about cost complexity is to consider \\(\\alpha\\) increasing. As \\(\\alpha\\) gets bigger, the “best” tree will be smaller. But the test error will not be monotonically related to the size of the training tree.\n\n\n\n\n\n\n\n\nA note on \\(\\alpha\\)\nIn the text (Introduction to Statistical Learning) and almost everywhere else you might look, the cost complexity is defined as in previous slides.\nHowever, you might notice that in R the cost_complexity value is typically less than 1. From what I can tell, the value of the function that is being minimized in R is the average of the squared errors and the missclassification rate.\n\\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{i \\in R_m} I(y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\]\nVariations on a theme\nThe main ideas above are consistent throughout all CART algorithms. However, the exact details of implementation can change from function to function, and often times it is very difficult to decipher exactly which equation is being used. In the tree function in R, much of the decision making is done on deviance which is defined as:\n\\[\\mbox{numerical: deviance} = \\sum_{m=1}^{|T|}  \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2\\]\n\\[\\mbox{categorical: deviance} = -2\\sum_{m=1}^{|T|} \\sum_{k=1}^K n_{mk} \\log \\hat{p}_{mk}\\]\nFor the CART algorithm, minimize the deviance (for both types of variables). The categorical deviance will be small if most of the observations are in the majority group (with high proportion). Also, \\(\\lim_{\\epsilon \\rightarrow 0} \\epsilon \\log(\\epsilon) = 0\\). Additionally, methods of cross validation can also vary. In particular, if the number of variables is large, the tree algorithm can be slow and so the cross validation process - choice of \\(\\alpha\\) - needs to be efficient.\nCV for model building and model assessment\nNotice that CV is used for both model building and model assessment. It is possible (and practical, though quite computational!) to use both practices on the same classification model. The algorithm could be as follows.\n\nAlgorithm: CV for both \\(V_1\\)-fold CV building and \\(V_2\\)-fold CV assessment\n\n\nPartition the data in \\(V_1\\) groups.\nRemove the first group, and train the data on the remaining \\(V_1-1\\) groups.\nUse \\(V_2\\)-fold cross-validation (on the \\(V_1-1\\) groups) to choose \\(\\alpha\\). That is, divide the training observations into \\(V_2\\) folds and find \\(\\alpha\\) that minimizes the error.\nUsing the subtree that corresponds to the chosen value of \\(\\alpha\\), predict the first of the \\(V_1\\) hold out samples.\nRepeat steps 2-4 using the remaining \\(V_1 - 1\\) groups.\n\n\n8.3.2 R CART Example\nThe Census Bureau divides the country up into “tracts” of approximately equal population. For the 1990 Census, California was divided into 20640 tracts. One data sets (houses on http://lib.stat.cmu.edu/datasets/; http://lib.stat.cmu.edu/datasets/houses.zip) records the following for each tract in California: Median house price, median house age, total number of rooms, total number of bedrooms, total number of occupants, total number of houses, median income (in thousands of dollars), latitude and longitude. It appeared in Pace and Barry (1997), “Sparse Spatial Autoregressions”, Statistics and Probability Letters.\nClassification and Regression Trees\nClassification Trees are used to predict a response or class \\(Y\\) from input \\(X_1, X_2, \\ldots, X_n\\). If it is a continuous response it’s called a regression tree, if it is categorical, it’s called a classification tree. At each node of the tree, we check the value of one the input \\(X_i\\) and depending of the (binary) answer we continue to the left or to the right subbranch. When we reach a leaf we will find the prediction (usually it is a simple statistic of the dataset the leaf represents, like the most common value from the available classes).\nNote on maxdepth: as you might expect, maxdepth indicates the longest length from the root of the tree to a terminal node. However, for rpart (in particular, using rpart or rpart2 in caret), there are other default settings that keep the tree from growing all the way to singular nodes, even with a high maxdepth.\nRegression Trees\nFor technical reasons (e.g., see here), the step_log() on the outcome variable step gives problems with predictions at the end. Therefore, we mutate the outcome variable within the dataset before starting the model building process.\n\nreal.estate &lt;- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) |&gt;\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split &lt;- initial_split(real.estate)\nhouse_train &lt;- training(house_split)\nhouse_test &lt;- testing(house_split)\n\n# recipe\nhouse_cart_recipe &lt;-\n  recipe(logValue ~ Longitude + Latitude ,\n         data = house_train)\n# model\nhouse_cart &lt;- decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_wflow &lt;- workflow() |&gt;\n  add_model(house_cart) |&gt;\n  add_recipe(house_cart_recipe)\n\n# fit\nhouse_cart_fit &lt;- house_cart_wflow |&gt;\n  fit(data = house_train)\n\nModel Output\n\nhouse_cart_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 15480 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n  1) root 15480 5024.405000 12.08947  \n    2) Latitude&gt;=38.485 1541  283.738200 11.59436  \n      4) Latitude&gt;=39.355 506   48.267930 11.31530 *\n      5) Latitude&lt; 39.355 1035  176.803400 11.73079 *\n    3) Latitude&lt; 38.485 13939 4321.152000 12.14421  \n      6) Longitude&gt;=-121.645 10454 3320.946000 12.06198  \n       12) Latitude&gt;=34.635 2166  491.986400 11.52110  \n         24) Longitude&gt;=-120.265 1083  166.051200 11.28432 *\n         25) Longitude&lt; -120.265 1083  204.505800 11.75787 *\n       13) Latitude&lt; 34.635 8288 2029.685000 12.20333  \n         26) Longitude&gt;=-118.315 6240 1373.830000 12.09295  \n           52) Longitude&gt;=-117.575 2130  516.313400 11.87918  \n            104) Latitude&gt;=33.605 821  123.684300 11.64002 *\n            105) Latitude&lt; 33.605 1309  316.218800 12.02918  \n              210) Longitude&gt;=-116.33 97    8.931327 11.17127 *\n              211) Longitude&lt; -116.33 1212  230.181300 12.09784  \n                422) Longitude&gt;=-117.165 796  101.805300 11.94935 *\n                423) Longitude&lt; -117.165 416   77.245280 12.38196 *\n           53) Longitude&lt; -117.575 4110  709.740000 12.20373  \n            106) Latitude&gt;=33.735 3529  542.838300 12.14908  \n              212) Latitude&lt; 34.105 2931  379.526800 12.09154  \n                424) Longitude&lt; -118.165 1114  147.375800 11.91911 *\n                425) Longitude&gt;=-118.165 1817  178.722200 12.19726 *\n              213) Latitude&gt;=34.105 598  106.051400 12.43109 *\n            107) Latitude&lt; 33.735 581   92.340630 12.53568 *\n         27) Longitude&lt; -118.315 2048  348.149000 12.53967  \n           54) Latitude&gt;=34.165 949  106.791800 12.38022 *\n           55) Latitude&lt; 34.165 1099  196.395200 12.67735  \n            110) Longitude&gt;=-118.365 431   85.796770 12.38191 *\n            111) Longitude&lt; -118.365 668   48.703000 12.86798 *\n      7) Longitude&lt; -121.645 3485  717.479900 12.39087  \n       14) Latitude&gt;=37.925 796  133.300900 12.10055 *\n       15) Latitude&lt; 37.925 2689  497.226200 12.47681 *\n\n\nThe following scatter plot can only be made when the CART is built using two numerical predictor variables.\n\n#remotes::install_github(\"grantmcdermott/parttree\")\nlibrary(parttree)\nhouse_train |&gt;\n  ggplot(aes(y = Longitude, x = Latitude)) + \n  geom_parttree(data = house_cart_fit, alpha = 0.2) +\n  geom_point(aes(color = MedianHouseValue)) \n\n\n\n\n\n\n\nPredicting\nAs seen in the image above, there are only 12 region so there are only 12 predicted values. The plot below seems a little odd at first glance, but it should make sense after careful consideration of what is the outcome measurement and what is the predicted value.\n\nhouse_cart_fit |&gt;\n  predict(new_data = house_test) |&gt;\n  cbind(house_test) |&gt;\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1)\n\n\n\n\n\n\n\nFiner partition\nFrom above:\n       12) Latitude&gt;=34.675 2182  513.95640 11.52385  \nThe node that splits at latitude greater than 34.675 has 2182 houses. 513.9564 is the “deviance” which is the sum of squares value for that node. The predicted value is the average of the points in that node: 11.5. It is not a terminal node (no asterisk).\nMore variables\nIncluding all the variables, not only the latitude and longitude. Note the predictions are much better!\n\nreal.estate &lt;- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) |&gt;\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split &lt;- initial_split(real.estate)\nhouse_train &lt;- training(house_split)\nhouse_test &lt;- testing(house_split)\n\n# recipe\nhouse_cart_full_recipe &lt;-\n  recipe(logValue ~ . ,\n         data = house_train) |&gt;\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart &lt;- decision_tree() |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_full_wflow &lt;- workflow() |&gt;\n  add_model(house_cart) |&gt;\n  add_recipe(house_cart_full_recipe)\n\n# fit\nhouse_cart_full_fit &lt;- house_cart_full_wflow |&gt;\n  fit(data = house_train)\n\n\nhouse_cart_full_fit |&gt;\n  predict(new_data = house_test) |&gt;\n  cbind(house_test) |&gt;\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.01)\n\n\n\n\n\n\n\nCross Validation (model building!)\n\nreal.estate &lt;- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) |&gt;\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split &lt;- initial_split(real.estate)\nhouse_train &lt;- training(house_split)\nhouse_test &lt;- testing(house_split)\n\nset.seed(4321)\nhouse_vfold &lt;- vfold_cv(house_train, v = 10)\n\ncart_grid &lt;- expand.grid(tree_depth = seq(2, 20, by = 2))\n\n# recipe\nhouse_cart_tune_recipe &lt;-\n  recipe(logValue ~ .,\n         data = house_train) |&gt;\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_tune &lt;- decision_tree(tree_depth = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_tune_wflow &lt;- workflow() |&gt;\n  add_model(house_cart_tune) |&gt;\n  add_recipe(house_cart_tune_recipe)\n\n# tuning / fit\nhouse_tuned &lt;- house_cart_tune_wflow |&gt;\n  tune_grid(resamples = house_vfold, \n           grid = cart_grid) \n\nCV accuracy\n\nhouse_tuned |&gt; collect_metrics() |&gt;\n  filter()\n\n# A tibble: 20 × 7\n   tree_depth .metric .estimator  mean     n std_err .config              \n        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1          2 rmse    standard   0.428    10 0.00224 Preprocessor1_Model01\n 2          2 rsq     standard   0.436    10 0.00665 Preprocessor1_Model01\n 3          4 rmse    standard   0.383    10 0.00242 Preprocessor1_Model02\n 4          4 rsq     standard   0.547    10 0.00629 Preprocessor1_Model02\n 5          6 rmse    standard   0.366    10 0.00239 Preprocessor1_Model03\n 6          6 rsq     standard   0.588    10 0.00586 Preprocessor1_Model03\n 7          8 rmse    standard   0.366    10 0.00239 Preprocessor1_Model04\n 8          8 rsq     standard   0.588    10 0.00586 Preprocessor1_Model04\n 9         10 rmse    standard   0.366    10 0.00239 Preprocessor1_Model05\n10         10 rsq     standard   0.588    10 0.00586 Preprocessor1_Model05\n11         12 rmse    standard   0.366    10 0.00239 Preprocessor1_Model06\n12         12 rsq     standard   0.588    10 0.00586 Preprocessor1_Model06\n13         14 rmse    standard   0.366    10 0.00239 Preprocessor1_Model07\n14         14 rsq     standard   0.588    10 0.00586 Preprocessor1_Model07\n15         16 rmse    standard   0.366    10 0.00239 Preprocessor1_Model08\n16         16 rsq     standard   0.588    10 0.00586 Preprocessor1_Model08\n17         18 rmse    standard   0.366    10 0.00239 Preprocessor1_Model09\n18         18 rsq     standard   0.588    10 0.00586 Preprocessor1_Model09\n19         20 rmse    standard   0.366    10 0.00239 Preprocessor1_Model10\n20         20 rsq     standard   0.588    10 0.00586 Preprocessor1_Model10\n\nhouse_tuned |&gt;\n  autoplot(metric = \"rmse\")\n\n\n\n\n\n\nhouse_tuned |&gt; \n  select_best(metric = \"rmse\")\n\n# A tibble: 1 × 2\n  tree_depth .config              \n       &lt;dbl&gt; &lt;chr&gt;                \n1          6 Preprocessor1_Model03\n\n\nFinal model + prediction on test data\nTurns out that the tree does “better” by being more complex – why is that? The tree with 14 nodes (depth of 6) corresponds to the tree with the lowest deviance.\n\n# recipe\nhouse_cart_final_recipe &lt;-\n  recipe(logValue ~ .,\n         data = house_train) |&gt;\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_final &lt;- decision_tree(tree_depth = 6) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_final_wflow &lt;- workflow() |&gt;\n  add_model(house_cart_final) |&gt;\n  add_recipe(house_cart_final_recipe)\n\n# tuning / fit\nhouse_final &lt;- house_cart_final_wflow |&gt;\n  fit(data = house_train)\n\nPredicting the final model on test data\n\nhouse_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 15480 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 15480 5024.40500 12.08947  \n   2) MedianIncome&lt; 3.54635 7696 1992.69800 11.77343  \n     4) MedianIncome&lt; 2.5165 3632  904.76740 11.57590  \n       8) Latitude&gt;=34.445 1897  412.81950 11.38488  \n        16) Longitude&gt;=-120.265 549   63.97662 11.08633 *\n        17) Longitude&lt; -120.265 1348  279.98120 11.50647 *\n       9) Latitude&lt; 34.445 1735  347.04430 11.78476  \n        18) Longitude&gt;=-117.775 645  111.86670 11.52607 *\n        19) Longitude&lt; -117.775 1090  166.47070 11.93784 *\n     5) MedianIncome&gt;=2.5165 4064  819.58450 11.94995  \n      10) Latitude&gt;=37.925 809   91.49688 11.68589 *\n      11) Latitude&lt; 37.925 3255  657.65510 12.01558  \n        22) Longitude&gt;=-122.235 2992  563.13610 11.97426  \n          44) Latitude&gt;=34.455 940  203.99070 11.77685  \n            88) Longitude&gt;=-120.155 338   31.54079 11.36422 *\n            89) Longitude&lt; -120.155 602   82.59029 12.00852 *\n          45) Latitude&lt; 34.455 2052  305.72870 12.06470  \n            90) Longitude&gt;=-118.285 1476  171.16160 11.95681 *\n            91) Longitude&lt; -118.285 576   73.36843 12.34115 *\n        23) Longitude&lt; -122.235 263   31.29310 12.48567 *\n   3) MedianIncome&gt;=3.54635 7784 1502.97400 12.40194  \n     6) MedianIncome&lt; 5.59185 5526  876.96730 12.25670  \n      12) MedianHouseAge&lt; 38.5 4497  651.27750 12.20567  \n        24) MedianIncome&lt; 4.53095 2616  388.38650 12.11491 *\n        25) MedianIncome&gt;=4.53095 1881  211.37640 12.33189 *\n      13) MedianHouseAge&gt;=38.5 1029  162.80030 12.47972 *\n     7) MedianIncome&gt;=5.59185 2258  224.13060 12.75740  \n      14) MedianIncome&lt; 7.393 1527  134.00030 12.64684 *\n      15) MedianIncome&gt;=7.393 731   32.47344 12.98835 *\n\n\n\nhouse_final |&gt; \n  predict(new_data = house_test) |&gt;\n  cbind(house_test) |&gt;\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1) + \n  labs(x = \"log of the Median House Value\",\n       y = \"predicted value of log Median House\")",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#bagging",
    "href": "08-classification.html#bagging",
    "title": "8  Classification",
    "section": "\n8.4 Bagging",
    "text": "8.4 Bagging\nThe tree based models given by CART are easy to understand and implement, but they suffer from high variance. That is, if we split the training data into two parts at random and fit a decision tree to both halves, the results that we get could be quite different (you might have seen this in your homework assignment!). We’d like a model that produces low variance - one for which if we ran it on different datasets, we’d get (close to) the same model every time.\nBagging = Bootstrap Aggregating. The idea is that sometimes when you fit multiple models and aggregate those models together, you get a smoother model fit which will give you a better balance between bias in your fit and variance in your fit. Bagging can be applied to any classifier to reduce variability.\n\nRecall that the variance of the sample mean is variance of data / n. So we’ve seen the idea that averaging an outcome gives reduced variability.\n\n\n8.4.1 Bagging algorithm\n\nAlgorithm: Bagging Forest\n\n\nResample (bootstrap) cases (observational units, not variables).\n\nBuild a tree on each new set of (bootstrapped) training observations.\nAverage (regression) or majority vote (classification).\nNote that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen.\n\n\n\\[\\begin{align}\nP(\\mbox{observation $i$ is not in the bootstrap sample}) &= \\bigg(1 - \\frac{1}{n} \\bigg)^n\\\\\n\\lim_{n \\rightarrow \\infty} \\bigg(1 - \\frac{1}{n} \\bigg)^n = \\frac{1}{e} \\approx \\frac{1}{3}\n\\end{align}\\]\nShortcomings of Bagging:\n\nModel is even harder to “write-down” (than CART)\nWith lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! (because of the number of trees grown for each bootstrap sample)\n\nStrengths of Bagging:\n\nCan handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!).\nCan handle more than 2 groups for categorical predictions\nEasily ignore redundant variables.\nPerform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables.\nData transformations may be less important (monotone transformations on the explanatory variables won’t change anything).\n\nSimilar bias to CART, but reduced variance\n\n(can be proved).\n\nNotes on bagging:\n\nBagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped).\nNote that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand.\nBagging gives a smoother decision boundary\nBagging can be done on any decision method (not just trees).\nNo need to prune or CV trees. The reason is that averaging keeps us from overfitting a particular few observations (think of averages in other contexts: law of large numbers). Pruning wouldn’t be a bad thing to do in terms of fit, but it is unnecessary for good predictions (and would add a lot to the complexity of the algorithm).\n\n8.4.2 Out Of Bag (OOB) error rate\nAdditionally, with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:\n\nEach tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the \\(b^{th}\\) tree.\nPut each case left out in the construction of the \\(b^{th}\\) tree down the \\(b^{th}\\) tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees.\n\nAt the end of the run, take \\(j\\) to be the class that got most of the votes every time case \\(i\\) was oob. The proportion of times that \\(j\\) is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.\n\nHow does it work? Consider the following predictions for a silly toy data set of 9 observations. Recall that \\(\\sim 1/3\\) of the observations will be left out at each bootstrap sample. Those are the observations for which predictions will be made. In the table below, an X is given if there is a prediction made for that value.\n\n\n\n\n\n\n\n\n\n\n\n\nobs\ntree1\ntree2\ntree3\ntree4\n\\(\\cdots\\)\ntree100\naverage\n\n\n\n1\n\nX\nX\n\n\n\n\\(\\sum(pred)/38\\)\n\n\n2\nX\n\n\n\n\n\n\\(\\sum(pred)/30\\)\n\n\n3\n\n\n\nX\n\nX\n\\(\\sum(pred)/33\\)\n\n\n4\nX\n\n\n\n\n\n\\(\\sum(pred)/32\\)\n\n\n5\nX\n\n\n\n\n\n\\(\\sum(pred)/39\\)\n\n\n6\n\n\nX\n\n\nX\n\\(\\sum(pred)/29\\)\n\n\n7\n\n\n\n\n\nX\n\\(\\sum(pred)/29\\)\n\n\n8\n\n\nX\nX\n\nX\n\\(\\sum(pred)/31\\)\n\n\n9\n\n\n\nX\n\n\n\\(\\sum(pred)/36\\)\n\n\n\nLet the OOB prediction for the \\(i^{th}\\) observation to be \\(\\hat{y}_{(-i)}\\)\n\\[\\begin{align}\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{i=1}^n \\textrm{I} (y_i \\ne \\hat{y}_{(-i)}) \\ \\ \\ \\ \\ \\ \\ \\  \\mbox{classification}\\\\\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{i=1}^n  (y_i - \\hat{y}_{(-i)})^2  \\ \\ \\ \\ \\ \\ \\ \\ \\mbox{regression}\\\\\n\\end{align}\\]",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#rf",
    "href": "08-classification.html#rf",
    "title": "8  Classification",
    "section": "\n8.5 Random Forests",
    "text": "8.5 Random Forests\nRandom Forests are an extension to bagging for regression trees (note: bagging can be done on any prediction method). Again, with the idea of infusing extra variability and then averaging over that variability, RFs use a subset of predictor variables at every node in the tree.\n\n“Random forests does not overfit. You can run as many trees as you want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\n\n8.5.1 Random Forest algorithm\n\nAlgorithm: Random Forest\n\n\nBootstrap sample from the training set.\nGrow an un-pruned tree on the bootstrap sample.\n\n\n\nAt each split, select \\(m\\) variables and determine the best split using only the \\(m\\) predictors. Typically \\(m = \\sqrt{p}\\) or \\(\\log_2 p\\), where \\(p\\) is the number of features. Random Forests are not overly sensitive to the value of \\(m\\). [splits are chosen as with trees: according to either squared error or gini index / cross entropy / classification error.]\nDo not prune the tree. Save the tree as is!\n\n\nRepeat steps 1-2 for many many trees.\nFor each tree grown on a bootstrap sample, predict the OOB samples. For each tree grown, \\(~1/3\\) of the training samples won’t be in the bootstrap sample – those are called out of bootstrap (OOB) samples. OOB samples can be used as test data to estimate the error rate of the tree.\nCombine the OOB predictions to create the “out-of-bag” error rate (either majority vote or average of predictions / class probabilities).\nAll trees together represent the model that is used for new predictions (either majority vote or average).\n\n\n\n\n\n\n\n\n\nBuilding multiple trees and then combining the outputs (predictions). Note that this image makes the choice to average the tree probabilities instead of using majority vote. Both are valid methods for creating a Random Forest prediction model. http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf\n\n\n\nShortcomings of Random Forests:\n\nModel is even harder to “write-down” (than CART)\nWith lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! … bagging the observations and\n\nStrengths of Random Forests:\n\nrefinement of bagged trees; quite popular (Random Forests tries to improve on bagging by “de-correlating” the trees. Each tree has the same expectation, but the average will again reduce the variability.)\nsubset of predictors makes Random Forests much faster to search through than all predictors\ncreates a diverse set of trees that can be built. Note that by bootstrapping the samples and the predictor variables, we add another level of randomness over which we can average to again decrease the variability.\nRandom Forests are quite accurate\ngenerally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (\\(m\\), node size, max number of nodes, etc.).\n\nNotes on Random Forests:\n\nBagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped). Random Forests use a subset of predictors.\nNote that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand.\nBagging is a special case of Random Forest where \\(m=p\\).\ngenerally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (\\(m\\), node size, max number of nodes, etc.).\n\n\n“Random forests does not overfit. You can run as many trees as you want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n\nHow to choose parameters?\n\n\n\\(\\#\\) trees Build trees until the error no longer decreases\n\n\\(m\\) Try the recommended defaults, half of them, and twice of them - pick the best (use CV to avoid overfitting).\nVariable Importance\nAll learners are bad when there are too many noisy variables because the response is bound to correlate with some of them. We can measure the contribution of each additional variable in the model by how much the model accuracy decreased when the given variable was excluded from the model.\n\nimportance = decrease in node impurity resulting from splits over that variable, averaged over all trees\n\n(“impurity” is defined as RSS for regression trees and deviance for classification trees).\nVariable importance is measured by two different metrics (from R help on importance):\n\n(permutation) accuracy: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression).Permute the \\(j^{th}\\) variable and recalculate the prediction error. The difference between the two are then averaged over all trees (for the \\(j^{th}\\) variable) to give the importance for the \\(j^{th}\\) variable.\n\npurity: The decrease (or increase, depending on the plot) in node purity: root sum of squares (RSS) [deviance/gini for classification trees]. That is, the amount of total decrease in RSS from splitting on that variable, averaged over all trees.\n\nIf the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run.\n\n8.5.2 R RF Example\n(“impurity” is defined as RSS for regression trees and deviance for classification trees).\nmethod= 'ranger' is about a zillion times faster than method = 'randomForest' or method = 'rf', but they all do the work.\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins &lt;- penguins |&gt;\n  drop_na()\n\n# partition\nset.seed(47)\npenguin_split &lt;- initial_split(penguins)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\n# recipe\npenguin_rf_recipe &lt;-\n  recipe(body_mass_g ~ . ,\n         data = penguin_train) |&gt;\n  step_unknown(sex, new_level = \"unknown\") |&gt;\n  step_mutate(year = as.factor(year)) \n\n#model\npenguin_rf &lt;- rand_forest(mtry = tune(),\n                           trees = tune()) |&gt;\n  set_engine(\"ranger\", importance = \"permutation\") |&gt;\n  set_mode(\"regression\")\n\n# workflow\npenguin_rf_wflow &lt;- workflow() |&gt;\n  add_model(penguin_rf) |&gt;\n  add_recipe(penguin_rf_recipe)\n\n# CV\nset.seed(234)\npenguin_folds &lt;- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\npenguin_grid &lt;- grid_regular(mtry(range = c(2,7)),\n                             trees(range = c(1,500)),\n                             levels = 5)\n\n# tune\npenguin_rf_tune &lt;- \n  penguin_rf_wflow |&gt;\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\nselect_best(penguin_rf_tune, metric = \"rmse\")\n\n# A tibble: 1 × 3\n   mtry trees .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1     2   375 Preprocessor1_Model16\n\n\nWhich mtry and number of trees?\n\npenguin_rf_tune |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  ggplot() + \n  geom_line(aes(x = trees, y = mean, color = as.factor(mtry)))\n\n\n\n\n\n\n\nGet the final model:\n\npenguin_rf_best &lt;- finalize_model(\n  penguin_rf,\n  select_best(penguin_rf_tune, metric = \"rmse\"))\n\npenguin_rf_best\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = 2\n  trees = 375\n\nEngine-Specific Arguments:\n  importance = permutation\n\nComputational engine: ranger \n\npenguin_rf_final &lt;-\n  workflow() |&gt;\n  add_model(penguin_rf_best) |&gt;\n  add_recipe(penguin_rf_recipe) |&gt;\n  fit(data = penguin_train)\n\npenguin_rf_final\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_unknown()\n• step_mutate()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~2L,      x), num.trees = ~375L, importance = ~\"permutation\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n\nType:                             Regression \nNumber of trees:                  375 \nSample size:                      249 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         permutation \nSplitrule:                        variance \nOOB prediction error (MSE):       84149.09 \nR squared (OOB):                  0.8634591 \n\n\nPredict the test data:\n\npenguin_rf_final |&gt;\n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  ggplot() +\n  geom_point(aes(x = body_mass_g, y = .pred)) + \n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\nVariable Importance\nIn order to get the variable importance, you need to specify importance within the model of the forest.\n\nlibrary(vip)\n\npenguin_rf_final |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(geom = \"point\")",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#model-choices",
    "href": "08-classification.html#model-choices",
    "title": "8  Classification",
    "section": "\n8.6 Model Choices",
    "text": "8.6 Model Choices\nThere are soooooo many choices we’ve made along the way. The following list should make you realize that there is no truth with respect to any given model. Every choice will (could) lead to a different model.\n\n\n\n\n\n\n\\(\\mbox{  }\\)\n\\(\\mbox{  }\\)\n\n\n\n* explanatory variable choice\n* \\(k\\) (\\(k\\)-NN)\n\n\n* number of explanatory variables\n* distance measure\n\n\n* functions/transformation of explanatory\n* V (CV)\n\n\n* transformation of response\n* CV set.seed\n\n\n\n* response:continuous vs. categorical\n* \\(\\alpha\\) prune\n\n\n* how missing data is dealt with\n* maxdepth prune\n\n\n* train/test split (set.seed)\n* prune or not\n\n\n* train/test proportion\n* gini / entropy (split)\n\n\n* type of classification model\n* # trees / # BS samples\n\n\n* use of cost complexity / parameter\n* grid search etc. for tuning\n\n\n* majority / average prob (tree error rate)\n* value(s) of mtry\n\n\n\n* accuracy vs sensitivity vs specificity\n* OOB vs CV for tuning",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "08-classification.html#support-vector-machines",
    "href": "08-classification.html#support-vector-machines",
    "title": "8  Classification",
    "section": "\n8.7 Support Vector Machines",
    "text": "8.7 Support Vector Machines\n\nSupport Vector Machines are one more algorithm for classification. As you’ll see, they have some excellent properties, but one important aspect to note is that they use only numeric predictor variables and only binary response variables (classify two groups).\nVladimir Vapnik (b. 1936) created SVMs in the late 1990s. History: he actually did the work as his PhD in the early 60s in the Soviet Union. Someone from Bell Labs asked him to visit, and he ended up immigrating to the US. No one actually thought that SVMs would work, but he eventually (1995 - took 30 years between the idea and the implementation) bet a dinner on classifying handwriting via SVM (using a very simple kernel) versus neural networks and the rest is history.\nThe basic idea of SVMs is to figure out a way to create really complicated decision boundaries. We want to put in a straight line with the widest possible street (draw street with gutters and 4 points, two positive and two negative). The decision rule has to do with a dot product of the new sample with a vector \\({\\bf w}\\) which is perpendicular to the median of the “street.”\nNote: the standard formulation of SVM requires the computer to find dot products between each of the observations. In order to do so, the explanatory variables must be numeric. In order for the dot products to be meaningful, the data must be on the same scale.\n\n8.7.0.1 Linear Separator\nRecall ideas of kNN and trees:\n\n\n\n\n\n\n\n\nBut today’s decision boundary is going to be based on a hyperplane which separates the values in the “best” way. Certainly, if the data are linearly separable, then there are infinitely many hyperplanes which will partition the data perfectly. For SVM, the idea is to find the “street” which separates the positive and negative samples to give the widest margin.\n\n\n\n\n\n\n\nThe correct project of the observations can often produce a perfect one dimensional (i.e., linear) classifier. http://www.rmki.kfki.hu/~banmi/elte/Bishop - Pattern Recognition and Machine Learning.pdf\n\n\n\nAside: what is a dot product?\nLet \\({\\bf x} = (x_1, x_2, \\ldots, x_p)^t\\) and \\({\\bf y} = (y_1, y_2, \\ldots, y_p)^t\\) be two vectors which live in \\(R^p\\). Then their dot product is defined as: \\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} = {\\bf x}^t {\\bf y} = \\sum_{i=1}^p x_i y_i\n\\end{align}\\]\n\n\n\n\n\n\n\nIf w is known, then the projection of any new observation onto w will lead to a linear partition of the space.\n\n\n\nHow can the street be used to get a decision rule? All that is known is that \\({\\bf w}\\) is perpendicular to the street. We don’t yet know \\({\\bf w}\\) or \\(b\\).\nThe “width” of the street will be a vector which is perpendicular to the street (median). We don’t know the width yet, but we know know that we can use that perpendicular vector (\\({\\bf w}\\)) to figure out how to classify the points. Project an unknown point (\\({\\bf u}\\)) onto \\({\\bf w}\\) to see which side of the street the unknown value lands. That is, if the projection is large enough, we would classify the point as positive: \\[{\\bf w} \\cdot {\\bf u} \\geq c?\\]\n[Keep in mind that \\({\\bf u} \\cdot {\\bf w} = ||{\\bf w}|| \\times\\)(the length of the shadow). That is, the projection will only be the length of the shadow if \\({\\bf w}\\) is a unit vector. And we aren’t going to constrain \\({\\bf w}\\) to be unit vector (though we could!). But regardless, \\({\\bf u} \\cdot {\\bf w}\\) still gives the ability to classify because it is proportional to the length of the shadow.]\n\nDecision rule: if \\({\\bf w} \\cdot {\\bf u} + b \\geq 0\\) then label the new sample “positive”\n\nwhere \\({\\bf w}\\) is created in such a way that it is perpendicular to the median of the street. Then the unknown (\\({\\bf u}\\)) vector is projected onto \\({\\bf w}\\) to see if it is on the left or the right side of the street.\nBut we don’t know the values in the decision rule! We need more constraints. Assuming that the data are linearly separable, as an initial step to find \\({\\bf w}\\) and \\(b\\), for all positive samples (\\(x_+\\)) and all negative samples (\\(x_-\\)) force: \\[\\begin{align}\n{\\bf w} \\cdot {\\bf x}_+ + b &\\geq 1 (\\#eq:posconstr)\\\\\n{\\bf w} \\cdot {\\bf x}_- + b &\\leq -1 (\\#eq:negconstr)\n\\end{align}\\]\nFor mathematical convenience (so that we don’t have 2 equations hanging around), introduce \\(y_i\\) such that \\[\\begin{align}\ny_i &= 1 \\mbox{ for positive samples}\\\\\ny_i &= -1 \\mbox{ for negative samples}\n\\end{align}\\]\nWhich simplifies the criteria for finding \\({\\bf w}\\) and \\(b\\) to be: \\[ y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1\\] (Multiplying through by -1 on equation (@ref(eq:negconstr) switches the signs, and both equation (@ref(eq:posconstr)) and (@ref(eq:negconstr) end up as the same for both types of points.)\nAgain, working toward solving for \\({\\bf w}\\) and \\(b\\), add the additional constraint that for the points in the gutter (on the margin lines):\n\nFor \\(x_i\\) in the gutter (by definition): \\[y_i({\\bf w} \\cdot {\\bf x}_i + b) - 1 = 0\\]\n\nNow consider two particular positive and negative values that live on the margin (gutter). The difference is almost the width of the street (we want to find the street that is as wide as possible), but it is at the wrong angle (see street picture again). Remember, our goal here is to find the street separating the pluses and the minuses that is as wide as possible. If we had a unit vector, we could dot it with \\((x_+ - x_-)\\) to get the width of the street!\n\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||}\n\\end{align}\\] which doesn’t do us much good yet.\n\n\n\n\n\n\n\n\n\nGoal: Try to find as wide a street as possible.\n\nBut remember, the gutter points are constrained: it turns out that \\(x_+ \\cdot {\\bf w} = 1 - b\\) and \\(x_- \\cdot {\\bf w} = -1 - b\\). Therefore:\n\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||} = \\frac{(1-b) - (-1-b)}{|| {\\bf w} ||} = \\frac{2}{||w||}\n\\end{align}\\]\nIn order to maximize \\(\\frac{2}{||w||}\\), minimize \\(||w||\\), or\n\n\nminimize \\((1/2)*||w||^2\\)\n\n\n(to make it mathematically easier). We have all the pieces of making the decision rules as an optimization problem. That is, minimize some quantity subject to the constraints given by the problem.\nLagrange multipliers\nRecall, with Lagrange multipliers, the first part is the optimization, the second part is the constraint. The point of Lagrange multipliers is to put together the constraint and the optimization into one equation where you don’t worry about the constraints any longer.\n\\(L\\) consists of two parts. The first is the thing to minimize. The second is the set of constraints (here, the summation over all the constraints). Each constraint has a multiplier \\(\\alpha_i\\), the non-zero \\(\\alpha_i\\) will be the ones connected to the values on the gutter.\n\\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1]\n\\end{align}\\]\nFind derivatives, set them equal to zero. Note that we can differentiate with respect to the vector component wise, but we’ll skip that notation, but you could do it one element at a time.\n\\[\\begin{align}\n\\frac{\\partial L}{\\partial {\\bf w}} &= {\\bf w} - \\sum \\alpha_i  y_i  {\\bf x}_i = 0 \\rightarrow {\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i \\\\\n\\frac{\\partial L}{\\partial b} &= -\\sum \\alpha_i y_i = 0\\\\\n\\end{align}\\]\n\nIt turns out that \\({\\bf w}\\) is a linear sum of data vectors, either all of them or some of them (it turns out that for some \\(i\\), \\(\\alpha_i=0\\)): \\[{\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i\\]\n\nUse the value of \\({\\bf w}\\) to plug back into \\(L\\) to minimize\n\\[\\begin{align}\nL &= \\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum_i \\alpha_i [ y_i ((\\sum_j \\alpha_j y_j {\\bf x}_j) \\cdot{\\bf x}_i + b ) - 1]\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum \\alpha_i y_i b + \\sum \\alpha_i\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - 0 + \\sum \\alpha_i\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\n\\end{align}\\]\n\nFind the minimum of this expression: \\[L = \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\\]\n\nThe computer / numerical analyst is going to solve \\(L\\) for the \\(\\alpha_i\\), so why did we go to all the work? We need to understand the dependencies of sample vectors. That is,\n\n\nthe optimization depends only on the dot product of pairs of samples.\n\n\nAnd the decision rule also depends only on the dot product of the new observation with the original samples. [Note, the points on the margin / gutter can be used to solve for \\(b\\): \\(b =y_i -  {\\bf w} \\cdot {\\bf x}_i\\), because \\(y_i = 1/y_i\\).]\n\nDecision Rule, call positive if: \\[\\sum \\alpha_i y_i {\\bf x}_i \\cdot {\\bf u} + b \\geq 0\\]\n\nNote that we have a convex space (can be proved), and so we can’t get stuck in a local maximum.\n\n\n8.7.1 Not Linearly Separable\nTransformations\nSimultaneously,\n\n\nthe data can be transformed into a new space where the data are linearly separable.\n\n\nIf we can transform the data into a different space (where they are linearly separable), then we can transform the data into the new space and then do the same thing! That is, consider the function \\(\\phi\\) such that our new space consists of vectors \\(\\phi({\\bf x})\\).\nConsider the case with a circle on the plane. The class boundary should segment the space by considering the points within that circle to belong to one class, and the points outside that circle to another one. The space is not linearly separable, but mapping it into a third dimension will make it separable. Two great videos: https://www.youtube.com/watch?v=3liCbRZPrZA and https://www.youtube.com/watch?v=9NrALgHFwTo .\nWithin the transformed space, the minimization procedure will amount to minimizing the following:\n\nWe want the minimum of this expression: \\[\\begin{align}\nL &= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j \\phi({\\bf x}_i) \\cdot  \\phi({\\bf x}_j)\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j K({\\bf x}_i, {\\bf x}_j)\n\\end{align}\\]\n\n\nDecision Rule, call positive if: \\[\\begin{align}\n\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b &\\geq& 0\\\\\n\\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b &\\geq& 0\n\\end{align}\\]\n\n\nKernel Examples:\n\nKernel 1\n\nConsider the following transformation, \\(\\phi: R^2 \\rightarrow R^3\\): \\[\\begin{align}\n\\phi({\\bf x}) &= (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\\\\\nK({\\bf x}, {\\bf y}) &= \\phi({\\bf x}) \\cdot \\phi({\\bf y}) = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2\\\\\n&= (x_1y_1 + x_2y_2)^2\\\\\nK({\\bf x}, {\\bf y}) &= ({\\bf x} \\cdot {\\bf y})^2\n\\end{align}\\] Which is to say, as long as we know the dot product of the original data, then we can recover the dot product in the transformed space using the quadratic kernel.\n\n\nKernel 2 Writing the polynomial kernel out (for \\(d=2\\)), we can find the exact \\(\\phi\\) function. Consider the following polynomial kernel for \\(d=2\\). \\[K({\\bf x}, {\\bf y}) = ({\\bf x} \\cdot {\\bf y} + c)^2\\] By writing down the dot product and then considering the square of each of the components separately, we get \\[\\begin{align}\n({\\bf x} \\cdot {\\bf y} + c)^2 &= (c + \\sum_{i=1}^p x_i y_i)^2\\\\\n&= c^2 + \\sum_{i=1}^p x_i^2 y_i^2 + \\sum_{i=1}^{p-1} \\sum_{j={i+1}}^{p} 2x_i y_i x_j y_j + \\sum_{i=1}^p 2 cx_i y_i\n\\end{align}\\] By pulling the sum apart into all the components of the \\({\\bf x}\\) and \\({\\bf y}\\) vectors separately, we find that \\[\\begin{align}\n\\phi({\\bf x}) = (c, x_1^2, \\ldots, x_p^2, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_1x_p, \\sqrt{2}x_2x_3, \\ldots, \\sqrt{2}x_{p-1}x_p, \\sqrt{2c}x_1, \\ldots, \\sqrt{2c}x_p)\n\\end{align}\\]\n\n\n\n\nKernel 3 Using the radial kernel (see below) it is possible to map the observations into an infinite dimensional space yet still only consider the kernel associated with the dot product of the original data. Consider the following example for \\(x\\) in one dimension mapped to infinite dimensions.\n\n\\[\\begin{align}\n\\phi_{RBF}(x) &= e^{-\\gamma x} \\bigg(1, \\sqrt{\\frac{2\\gamma}{1!}} x, \\sqrt{\\frac{(2\\gamma)^2}{2!}} x^2, \\sqrt{\\frac{(2\\gamma)^3}{3!}} x^3, \\ldots \\bigg)^t\\\\\nK_{RBF} (x,y) &= \\exp( -\\gamma ||x-y||^2)\n\\end{align}\\] where cross validation is used to find the tuning value \\(\\gamma\\) as well as the penalty parameter \\(C\\).\nConsider the following example from http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex8/ex8.html.\nWhat if the boundary is wiggly?\nThe take home message here is that a wiggly boundary is really best, and the value of \\(\\gamma\\) should be high to represent the high model complexity.\n\n\n\n\n\n\n\nExtremely complicated decision boundary\n\n\n\n\n\n\n\n\nExtremely complicated decision boundary\n\n\n\nWhat if the boundary isn’t wiggly?\nBut if the boundary has low complexity, then the best value of \\(\\gamma\\) is probably much lower.\n\n\n\n\n\n\n\nSimple decision boundary\n\n\n\n\n\n\n\n\n\n\nSimple decision boundary – reasonable gamma\n\n\n\n\n\n\n\n\nSimple decision boundary – reasonable gamma\n\n\n\n\n\n\n\n\n\n\nSimple decision boundary – gamma too big\n\n\n\n\n\n\n\n\nSimple decision boundary – gamma too big\n\n\n\n\n8.7.2 What is a Kernel?\nWhat is a kernel: A kernel function is a function that obeys certain mathematical properties. I won’t go into these properties right now, but for now think of a kernel as a function as a function of the dot product between two vectors, (e.g., a measure of “similarity” between the two vectors). If \\(K\\) is a function of two vectors \\({\\bf x}\\) and \\({\\bf y}\\), then it is a kernel function if \\(K\\) is the dot product of \\(\\phi()\\) applied to those vectors. We know that \\(\\phi()\\) exists if \\(K\\) is symmetric and if when \\(K_{ij} = K({\\bf x}_i, {\\bf x}_j)\\), the matrix \\({\\bf K} = [K_{ij}]\\) is positive definite.\nA helpful website about kernels: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\n\\[\\begin{align}\nK({\\bf x},{\\bf y}) = \\phi({\\bf x}) \\cdot \\phi({\\bf y})\n\\end{align}\\]\nExamples of kernels:\n\nlinear \\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\] Note, the only tuning parameter is the penalty/cost parameter \\(C\\)).\npolynomial \\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma &gt; 0\\] Note, here \\(\\gamma, r, d\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)).\nRBF The radial basis function is also called the Gaussian kernel because of its similarity to the Gaussian distribution (aka the normal distribution). Because the RBF maps to infinite dimensional space, it can easily over fit the training data. Care must be taken to estimate \\(\\gamma\\). \\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} -  {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\] Note, here \\(\\gamma\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)).\nsigmoid The sigmoid kernel is not a valid kernel method for all values of \\(\\gamma\\) and \\(r\\) [which means that for certain parameter values, the \\(\\phi()\\) function may not exist]. \\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\] Note, here \\(\\gamma, r\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)). One benefit of the sigmoid kernel is that it has equivalence to a two-layer perceptron neural network.\nSoft Margins\nBut what if the data aren’t linearly separable? The optimization problem can be changed to allow for points to be on the other side of the margin. The optimization problem is slightly more complicated, but basically the same idea: \\[y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i  \\ \\ \\ \\ \\ \\ 1 \\leq i \\leq n, \\ \\  \\xi_i \\geq 0\\]\n\n\n\n\n\n\n\nNote that now the problem is set up such that points are allowed to cross the boundary. Slack variables (the xi_i) allow for every point to be classified correctly up to the slack. Note that xi_i=0 for any point that is actually calculated correctly.\n\n\n\nThe optimization problem gets slightly more complicated in two ways, first, the minimization piece includes a penalty parameter, \\(C\\) (how much misclassification is allowed - the value of \\(C\\) is set/tuned not optimized), and second, the constraint now allows for points to be misclassified.\n\nMinimize (for \\({\\bf w}\\), \\(\\xi_i\\), \\(b\\)): \\[\\frac{1}{2} ||{\\bf w}||^2 + C \\sum_{i=1}^n \\xi_i\\] Subject to: \\[y_i ({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i \\ \\ \\ \\ \\xi_i \\geq 0\\]\n\nWhich leads to the following Lagrangian equation: \\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 + C \\sum_{i=1}^n \\xi_i - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^n \\beta_i \\xi_i \\ \\ \\ \\ \\alpha_i, \\beta_i \\geq 0\n\\end{align}\\]\nThat is, the objective function now allows for a trade-off between a large margin and a small error penalty. Again, Lagrange multipliers can be shown to give classification rule that is based only on the dot product of the observations. The key here is that although quadratic programming can be used to solve for most of the parameters,\n\n\n\\(C\\) is now a tuning parameter that needs to be set by the user or by cross validation.\n\n\nHow does \\(C\\) relate to margins?\nNotice that the minimization is now over many more variables (with \\(C\\) set/tuned - not optimized). If we are allowing for misclassification and \\(C=0\\), that implies that \\(\\xi_i\\) can be as large as possible. Which means the algorithm will choose the widest possible street. The widest possible street will be the one that hits at the two most extreme data points (the “support vectors” will now be the ones on the edge, not the ones near the separating hyperplane). \\(C\\) small allows the constraints (on points crossing the line) to be ignored.\n\\[C=0 \\rightarrow \\mbox{ can lead to large training error}\\]\nIf \\(C\\) is quite large, then the algorithm will try very hard to classify exactly perfectly. That is, it will want \\(\\xi_i\\) to be as close to zero as possible. When projecting into high dimensions, we can always perfectly classify, so a large \\(C\\) will tend to overfit the training data and give a very small margin. \\[C&gt;&gt;&gt; \\rightarrow \\mbox{ can lead to classification rule which does not generalize to test data}\\]\n\n\n\n\n\n\n\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n\n\n\n\n\n\n\n\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n\n\n\n\n\n\n\n\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n\n\n\n\n8.7.3 Support Vector Machine algorithm\n\nAlgorithm: Support Vector Machine\n\n\nUsing cross validation, find values of \\(C, \\gamma, d, r\\), etc. (and the kernel function!)\nUsing Lagrange multipliers (read: the computer), solve for \\(\\alpha_i\\) and \\(b\\).\nClassify an unknown observation (\\({\\bf u}\\)) as “positive” if: \\[\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b  = \\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b \\geq 0\\]\n\n\n\n\nShortcomings of Support Vector Machines:\nCan only classify binary categories (response variable).\n\nAll predictor variables must be numeric.\n\nA great differential in range will allow variables with large range to dominate the predictions. Either linearly scale each attribute to some range [ e.g., (-1, +1) or (0,1)] or divide by the standard deviation.\nCategorical variables can be used if formatted as binary factor variables.\nWhatever is done to the training data must also be done to the test data!\n\n\n\nAnother problem is the kernel function itself.\n\nWith primitive data (e.g., 2d data points), good kernels are easy to come by.\nWith harder data (e.g., MRI scans), finding a sensible kernel function may be much harder.\n\n\nWith really large data, it doesn’t perform well because of the large amount of required training time\nIt also doesn’t perform very well when the data set has a lot of noise i.e., target classes are overlapping\nSVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\nStrengths of Support Vector Machines:\nCan always fit a linear separating hyper plane in a high enough dimensional space.\nThe kernel trick makes it possible to not know the transformation functions, \\(\\phi\\).\nBecause the optimization is on a convex function, the numerical process for finding solutions are extremely efficient.\nIt works really well with clear margin of separation\nIt is effective in high dimensional spaces.\nIt is effective in cases where number of dimensions is greater than the number of samples.\nIt uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n8.7.4 Classifying more than one group\nWhen there are more than two classes, the problem needs to be reduced into a binary classification problem. Consider the groups associated with Red, Green, and Blue. In order to figure out which points get classified as Red, two different methods can be applied.\n\n\nOne vs All Each category can be compared to the rest of the groups. This will create \\(K\\) different classifiers (here \\(K=\\) the number of classes the response variable can take on). Each test value would then be classified according to each classifier, and the group assignment would be given by the group giving the highest value of \\({\\bf w}_K \\cdot {\\bf u} + b\\), as the projection would represent the classification farthest into the group center. In the end, there will be \\(K\\) classifiers.\n\nOne vs One Alternatively, each group can be compared with each other group (e.g., Red vs. Green, Red vs. Blue, Green vs. Blue). Class membership will be determine by the group to which the unknown point is most often classified. In the end, there will be \\(K(K-1)/2\\) classifiers.\n\n8.7.5 R SVM Example\nWe’ll go back to the penguin data. As a first pass, let’s use SVM to distinguish between male and female penguins. I removed the missing data from the dataset to make predictions easier.\n\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\npenguins &lt;- penguins |&gt;\n  drop_na()\n\nset.seed(47)\npenguin_split &lt;- initial_split(penguins)\npenguin_train &lt;- training(penguin_split)\npenguin_test &lt;- testing(penguin_split)\n\nLinear SVM (no tuning)\n\n# recipe\npenguin_svm_recipe &lt;-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) |&gt;\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)\n\n# A tibble: 5 × 4\n  variable          type      role      source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 bill_length_mm    &lt;chr [2]&gt; predictor original\n2 bill_depth_mm     &lt;chr [2]&gt; predictor original\n3 flipper_length_mm &lt;chr [2]&gt; predictor original\n4 body_mass_g       &lt;chr [2]&gt; predictor original\n5 sex               &lt;chr [3]&gt; outcome   original\n\n# model\npenguin_svm_lin &lt;- svm_linear() |&gt;\n  set_engine(\"LiblineaR\") |&gt;\n  set_mode(\"classification\")\n\npenguin_svm_lin\n\nLinear Support Vector Machine Model Specification (classification)\n\nComputational engine: LiblineaR \n\n# workflow\npenguin_svm_lin_wflow &lt;- workflow() |&gt;\n  add_model(penguin_svm_lin) |&gt;\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_lin_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Support Vector Machine Model Specification (classification)\n\nComputational engine: LiblineaR \n\n# fit\npenguin_svm_lin_fit &lt;- \n  penguin_svm_lin_wflow |&gt;\n  fit(data = penguin_train)\n\npenguin_svm_lin_fit \n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_linear()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\n$TypeDetail\n[1] \"L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)\"\n\n$Type\n[1] 1\n\n$W\n     bill_length_mm bill_depth_mm flipper_length_mm body_mass_g       Bias\n[1,]       0.248908      1.080195        -0.2256375    1.328448 0.06992734\n\n$Bias\n[1] 1\n\n$ClassNames\n[1] male   female\nLevels: female male\n\n$NbClass\n[1] 2\n\nattr(,\"class\")\n[1] \"LiblineaR\"\n\n\nRBF SVM (with tuning)\n\n# recipe\npenguin_svm_recipe &lt;-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) |&gt;\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)\n\n# A tibble: 5 × 4\n  variable          type      role      source  \n  &lt;chr&gt;             &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n1 bill_length_mm    &lt;chr [2]&gt; predictor original\n2 bill_depth_mm     &lt;chr [2]&gt; predictor original\n3 flipper_length_mm &lt;chr [2]&gt; predictor original\n4 body_mass_g       &lt;chr [2]&gt; predictor original\n5 sex               &lt;chr [3]&gt; outcome   original\n\n# model\npenguin_svm_rbf &lt;- svm_rbf(cost = tune(),\n                           rbf_sigma = tune()) |&gt;\n  set_engine(\"kernlab\") |&gt;\n  set_mode(\"classification\")\n\npenguin_svm_rbf\n\nRadial Basis Function Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n  rbf_sigma = tune()\n\nComputational engine: kernlab \n\n# workflow\npenguin_svm_rbf_wflow &lt;- workflow() |&gt;\n  add_model(penguin_svm_rbf) |&gt;\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_rbf_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: svm_rbf()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRadial Basis Function Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = tune()\n  rbf_sigma = tune()\n\nComputational engine: kernlab \n\n# CV\nset.seed(234)\npenguin_folds &lt;- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\n# the tuned parameters also have default values you can use\npenguin_grid &lt;- grid_regular(cost(),\n                             rbf_sigma(),\n                             levels = 8)\n\npenguin_grid\n\n# A tibble: 64 × 2\n        cost     rbf_sigma\n       &lt;dbl&gt;         &lt;dbl&gt;\n 1  0.000977 0.0000000001 \n 2  0.00431  0.0000000001 \n 3  0.0190   0.0000000001 \n 4  0.0841   0.0000000001 \n 5  0.371    0.0000000001 \n 6  1.64     0.0000000001 \n 7  7.25     0.0000000001 \n 8 32        0.0000000001 \n 9  0.000977 0.00000000268\n10  0.00431  0.00000000268\n# ℹ 54 more rows\n\n# tune\n# this takes a few minutes\npenguin_svm_rbf_tune &lt;- \n  penguin_svm_rbf_wflow |&gt;\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\npenguin_svm_rbf_tune \n\n# Tuning results\n# 4-fold cross-validation \n# A tibble: 4 × 4\n  splits           id    .metrics           .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;             &lt;list&gt;          \n1 &lt;split [186/63]&gt; Fold1 &lt;tibble [192 × 6]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [187/62]&gt; Fold2 &lt;tibble [192 × 6]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [187/62]&gt; Fold3 &lt;tibble [192 × 6]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [187/62]&gt; Fold4 &lt;tibble [192 × 6]&gt; &lt;tibble [0 × 3]&gt;\n\n\nWhat is best?\n\npenguin_svm_rbf_tune |&gt;\n  collect_metrics() |&gt;\n  filter(.metric == \"accuracy\") |&gt;\n  ggplot() + \n  geom_line(aes(color = as.factor(cost), y = mean, x = rbf_sigma)) +\n  labs(color = \"Cost\")\n\n\n\n\n\n\n\n\npenguin_svm_rbf_tune |&gt;\n  autoplot()\n\n\n\n\n\n\n\nRBF SVM final model\n\npenguin_svm_rbf_best &lt;- finalize_model(\n  penguin_svm_rbf,\n  select_best(penguin_svm_rbf_tune, metric = \"accuracy\"))\n\npenguin_svm_rbf_best\n\nRadial Basis Function Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = 0.371498572284237\n  rbf_sigma = 1\n\nComputational engine: kernlab \n\npenguin_svm_rbf_final &lt;-\n  workflow() |&gt;\n  add_model(penguin_svm_rbf_best) |&gt;\n  add_recipe(penguin_svm_recipe) |&gt;\n  fit(data = penguin_train)\n\nTest predictions\n\nlibrary(yardstick)\npenguin_svm_rbf_final |&gt;\n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  select(sex, .pred_class) |&gt;\n  table()\n\n        .pred_class\nsex      female male\n  female     39    5\n  male        4   36\n\npenguin_svm_rbf_final |&gt;\n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     39    4\n    male        5   36\n\n# https://yardstick.tidymodels.org/articles/metric-types.html\nclass_metrics &lt;- yardstick::metric_set(accuracy,sens,\n                                       spec, f_meas)\n\npenguin_svm_rbf_final |&gt;\n  predict(new_data = penguin_test) |&gt;\n  cbind(penguin_test) |&gt;\n  class_metrics(truth = sex, estimate = .pred_class)\n\n# A tibble: 4 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.893\n2 sens     binary         0.886\n3 spec     binary         0.9  \n4 f_meas   binary         0.897\n\n\n\n\n\nTest and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! (Hastie, Tibshirani, and Friedman 2001)\nGreat interactive viz at http://www.r2d3.us/visual-intro-to-machine-learning-part-2/\n(Flach 2012)\nNested cross-validation: two cross-validation loops are run one inside the other. (Varoquaux et al. 2017)\nImage credit: Wright et al., Chapter 5 of Tidyverse Skills for Data Science https://jhudatascience.org/tidyversecourse/\nImage credit: Julia Silge\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: Alison Hill\nImage credit: https://www.tmwr.org/\nImage credit: https://www.tmwr.org/\nImage credit: https://www.tmwr.org/\nImage credit: Alison Hill\nImage credit: Alison Hill\nhttp://www.r2d3.us/visual-intro-to-machine-learning-part-1/ A visual introduction to machine learning.\nhttps://www.nytimes.com/interactive/2019/08/08/opinion/sunday/party-polarization-quiz.html Quiz: Let Us Predict Whether You’re a Democrat or a Republican NYT, Aug 8, 2019. Note that race is the first and dominant node, followed by religion.\nBuilding multiple trees and then combining the outputs (predictions). Note that this image makes the choice to average the tree probabilities instead of using majority vote. Both are valid methods for creating a Random Forest prediction model. http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf\nThe correct project of the observations can often produce a perfect one dimensional (i.e., linear) classifier. http://www.rmki.kfki.hu/~banmi/elte/Bishop - Pattern Recognition and Machine Learning.pdf\nIf w is known, then the projection of any new observation onto w will lead to a linear partition of the space.\nExtremely complicated decision boundary\nExtremely complicated decision boundary\nSimple decision boundary\nSimple decision boundary – reasonable gamma\nSimple decision boundary – reasonable gamma\nSimple decision boundary – gamma too big\nSimple decision boundary – gamma too big\nNote that now the problem is set up such that points are allowed to cross the boundary. Slack variables (the xi_i) allow for every point to be classified correctly up to the slack. Note that xi_i=0 for any point that is actually calculated correctly.\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\nIn the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel\n\n\n\nBaumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” The American Statistician.\n\n\nBreiman, L. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–215.\n\n\nFielding, Alan. 2007. Cluster and Classification Techniques for the Biosciences. Cambridge.\n\n\nFlach, P. 2012. Machine Learning. Cambridge University Press.\n\n\nHastie, T., R. Tibshirani, and J. Friedman. 2001. The Elements of Statistical Learning. Springer.\n\n\nJames, Witten, Hastie, and Tibshirani. 2021. An Introduction to Statistical Learning. Springer. https://www.statlearning.com/.\n\n\nVaroquaux, G., P. Reddy Raamana, D. Engemann, A. Hoyos-Idrobo, Y. Schwartz, and B. Thirion. 2017. “Assessing and Tuning Brain Decoders: Cross-Validation, Caveats, and Guidelines.” NeuroImage 145: 166–79.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Classification</span>"
    ]
  },
  {
    "objectID": "07-ethics.html",
    "href": "07-ethics.html",
    "title": "7  Ethics",
    "section": "",
    "text": "7.1 Doing Data Science\nQuestions to ask yourself in every single data analysis you perform (taken from Data Science for Social Good at the University of Chicago https://dssg.uchicago.edu/):",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#doing-data-science",
    "href": "07-ethics.html#doing-data-science",
    "title": "7  Ethics",
    "section": "",
    "text": "What biases may exist in the data you’ve been given? How can you find out?\nHow will your choices with tuning parameters affect different populations represented in the data?\nHow do you know you aren’t getting the right answer to the wrong question?\nHow would you justify what you’d built to someone whose welfare is made worse off by the implementation of your algorithm?\nSee the section on bias in modeling (@ref(biasmodels)) for times when there are no inherent biases but the structure of the data create unequal model results.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#graphics",
    "href": "07-ethics.html#graphics",
    "title": "7  Ethics",
    "section": "\n7.2 Graphics",
    "text": "7.2 Graphics\nThere are so many ways to lie with graphics. You may already be familiar with the book How to Lie with Statistics. Many of the ideas are replicated here: http://www.rci.rutgers.edu/~roos/Courses/grstat502/wainer.pdf [just the plots are provided here: http://www.stat.berkeley.edu/~nolan/stat133/Fall05/lectures/DirtyDozen.pdf]\nFor a recent and relevant example, consider the following image. What do you think is wrong? (Hint: examine the y-axis carefully)\n\n\n\n\n\n\n\nReproduction of a data graphic reporting the number of gun deaths in Florida over time. The original image was published by Reuters. (Baumer, Kaplan, and Horton 2021)\n\n\n\n\nOr another plot that has gotten a lot of press is the following. What is wrong with this plot? (Hint: again, think about the y-axis)\n\n\n\n\n\n\n\nA tweet by National Review on December 14, 2015 showing the change in global temperature over time. (Baumer, Kaplan, and Horton 2021)\n\n\n\nThe Georgia Department of Health came out with a grouped barplot showing the number of COVID-19 cases by day in 5 populous counties in GA. The bars were arranged in some kind of decreasing order, but at first glance, the typical reader will think that time is increasing along the x-axis.\n\n\n\n\n\n\n\nMay 10, 2020, Georgia Department of Health, COVID-19 cases for 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\n\n\n\n\n\n\n\n\n\n\nMay 17, 2020, Georgia Department of Health, COVID-19 cases for 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\n\n\n\nA few weeks later, the Georgia Department of Health came out with the following two plots where, despite cases skyrocketing, they display images where the visual doesn’t really change.\n\n\n\n\n\n\n\nJuly 2, 2020 (left) and July 17, 2020 (right), Georgia Department of Health, COVID-19 cases per 100K https://dph.georgia.gov/covid-19-daily-status-report\n\n\n\n\n\n\n\n\nJuly 2, 2020 (left) and July 17, 2020 (right), Georgia Department of Health, COVID-19 cases per 100K https://dph.georgia.gov/covid-19-daily-status-report\n\n\n\nSeems odd that a linear model was fit to these data. In particular, the trend is absolutely non-linear! Note that the screen shot has a slider to indicate the number of days modeled and displayed. There is some information in 14, 28, and 42 days related to policy decisions based on whether cases are trending positive or negative. However, even if you do need to know about positive or negative trends, even over a short time frame, the linear model fit to the data seems to be inappropriate.\n\n\n\n\n\n\n\nFrom the Kent County COVID dashboard, screen shot 8/25/2021. https://www.accesskent.com/Health/covid-19-data.htm",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#p-hacking",
    "href": "07-ethics.html#p-hacking",
    "title": "7  Ethics",
    "section": "\n7.3 p-hacking",
    "text": "7.3 p-hacking\n\nGreat applet from 538 on how to get significance by just trying enough things: https://projects.fivethirtyeight.com/p-hacking/\n\nJ. Ioannidis, Why most published research findings are false. PLoS Medicine, 2. e124 2005. Ioannidis focuses on multiple testing with specific understanding of the effect of testing in three different contexts:\n\nWhen looking for as many possible significant findings as possible (publish or perish)\nWhen bias exists in our work\nWhen the researchers study the same effect\n\n\n7.3.1 Multiple Studies\n\n\n\\(\\alpha\\)\n\nIf a study is null, the probability of seeing null is \\((1-\\alpha)\\)\n\nIf 3 of us test the same thing, the probability that we will all see null is \\((1-\\alpha)^3\\)\n\n\nand the probability that at least one of use will see significance goes from \\(\\alpha\\) to \\(1 - (1-\\alpha)^3\\)\n\nAs \\(n \\uparrow\\) someone will definitely see significance (bad!)\n\n\n\n\\(\\beta\\)\n\nIf a study is significant, the probability of seeing null is \\(\\beta\\)\n\nIf 3 of us test the same thing, the probability that we’ll all see null is \\(\\beta^3\\)\n\n\nand the probability that at least one of us will see significance goes from \\((1-\\beta)\\) to \\((1-\\beta^3)\\)\n\nAs \\(n \\uparrow\\), someone will definitely see significance (good!)\n\n\n\n7.3.2 p-values\n\nIn 1929 RA Fisher said the following (and thus 0.05 was born):\n\n\n…An observation is judged significant, if it would rarely have been produced, in the absence of a real cause of the kind we are seeking.\n\nIt is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained.\n\nHe should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation.\n\nNote the Fisher is telling us that studies with p-values above 0.05 are not worth pursuing. He is not saying the studies with p-values less than 0.05 establish any sort of truth.\n\nIn 2014 George Cobb (Amherst College) posed the following questions:\n\nQ: Why do so many colleges and grad schools teach p = .05?\nA: Because that’s still what the scientific community and journal editors use.\nQ: Why do so many people still use p = 0.05?\nA: Because that’s what they were taught in college or grad school.\n\nIn 2015, Basic and Applied Social Psychology banned all NHSTP (null hypothesis significance testing procedures) from scientific articles.\n\nWhat are the implications for authors? http://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991\nQuestion 3. Are any inferential statistical procedures required?\nAnswer to Question 3. No, because the state of the art remains uncertain. However, BASP will require strong descriptive statistics, including effect sizes. We also encourage the presentation of frequency or distributional data when this is feasible. Finally, we encourage the use of larger sample sizes than is typical in much psychology research, because as the sample size increases, descriptive statistics become increasingly stable and sampling error is less of a problem. However, we will stop short of requiring particular sample sizes, because it is possible to imagine circumstances where more typical sample sizes might be justifiable.\n\nMany people think CIs are far superior to p-values. Not only can you assess significance, but you can also assess effect size. Here is a video that makes clear the comparison between a p-value and a confidence interval:\n\nhttps://www.youtube.com/watch?v=5OL1RqHrZQ8\n\nIn 2016, the American Statistical Association came out with a statement on p-values http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108. The 6 main tenants of the statement are:\n\n\nP-values can indicate how incompatible the data are with a specified statistical model.\nP-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.\nScientific conclusions and business or policy decisions should not be based only on whether a p- value passes a specific threshold.\nProper inference requires full reporting and transparency.\nA p-value, or statistical significance, does not measure the size of an effect or the importance of a result.\nBy itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.\n\n\n“Statisticians issue warning over misuse of P values” (Nature, March 7, 2016) http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503\nMany people think CIs are far superior to p-values. Not only can you assess significance, but you can also assess effect size. Here is a video that makes clear the comparison between a p-value and a confidence interval, it also addresses how p-values can be misinterpreted. The Dance of the P-values (think about power when you watch it): https://www.youtube.com/watch?v=5OL1RqHrZQ8\nLast, you can discover your own level of significance through this activity: https://www.openintro.org/stat/why05.php?stat_book=os",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#human-subjects-research",
    "href": "07-ethics.html#human-subjects-research",
    "title": "7  Ethics",
    "section": "\n7.4 Human Subjects Research",
    "text": "7.4 Human Subjects Research\nThere are many ways to learn about federal regulations on studying human subjects. Any study that goes through an academic institution must be approved by the institution’s Institutional Review Board (IRB); each of the Claremont Colleges has an IRB. Some of you may be familiar with HIPPA (Health Insurance Portability and Accountability Act of 1996) which is United States legislation that provides data privacy and security provisions for safeguarding medical information. HIPPA is also the legislation that keeps your academic records private.\nTraining in IRB policies can be found by Citi Program here: https://about.citiprogram.org/en/series/human-subjects-research-hsr/\nConsider the following article: Cami Gearhart, “IRB Review of the Use of Social Media in Research”, Monitor, 2012. https://www.quorumreview.com/wp-content/uploads/2012/12/IRB-Review-of-the-Use-of-Social-Media-in-Research_Gearhart_Quorum-Review_Monitor_2012_12_01.pdf\n\nThe use of interactive social media to collect information during study recruitment raises additional issues under the HIPAA Privacy Rule. The Privacy Rule prohibits the collection of an individual?s personal health information (or PHI) by a covered entity without prior written authorization from that individual.18 As PHI includes an individual?s contact information, including name, age, e-mail address, and mailing address, the Privacy Rule prohibits the collection of contact information via a website without prior authorization.\n\n\nThis rule creates a conundrum when using social media, as it may be impractical to obtain a written authorization prior to collecting contact information during the recruitment process. To get around this restriction, a researcher generally must obtain a partial waiver of the HIPAA authorization requirement. (A waiver in this situation is considered “partial” because it is needed only for the recruitment phase of the clinical study.) A researcher can apply to either an IRB or a privacy board for such a waiver; the researcher will be asked to explain why it is impractical to obtain written authorizations, the plan for collecting information, and the planned safeguards for the data.\n\nOKCupid\nConsider a study done on a dataset of nearly 70,000 users of the online dating site OkCupid (including usernames, age, gender, location, relationship interests, personality traits, and many other profile variables). The authors did not violate any technical policies such as breaking passwords. However, their work indicates a violation of privacy ethics as indicated by HIPPA and use of IRBs. [Kirkegaard and Bjerrekaer, “The OKCupid dataset: A very large public dataset of dating site users,” Open Differential Pyschology, 2016.] https://openpsych.net/paper/46\n\n\n\n\n\n\n\nNot only is it worth discussing the ethics of how the data were collected, but it also seems like maybe the study did some p-hacking.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#authorship",
    "href": "07-ethics.html#authorship",
    "title": "7  Ethics",
    "section": "\n7.5 Authorship",
    "text": "7.5 Authorship\nFrom the International Committee of Medical Journal Editors, http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html. Many other organizations have suggested guidelines for authorship, but such guidelines generally follow the same criteria.\nAuthors\n\nSubstantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND\nDrafting the work or revising it critically for important intellectual content; AND\nFinal approval of the version to be published; AND\nAgreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved.\nNon-authors\n\nContributors who meet fewer than all 4 of the above criteria for authorship should not be listed as authors, but they should be acknowledged. Examples of activities that alone (without other contributions) do not qualify a contributor for authorship are acquisition of funding; general supervision of a research group or general administrative support; and writing assistance, technical editing, language editing, and proofreading.\n\n\n\n\n\n\n\n\nThe paper was retracted because the authors could not agree on the order of authorship.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#algorithms",
    "href": "07-ethics.html#algorithms",
    "title": "7  Ethics",
    "section": "\n7.6 Algorithms",
    "text": "7.6 Algorithms\nWe could spend days talking about bias in algorithms. The take away from the examples below is that the data that is used to train the model can have huge effects on the creation of the model. A fantastic book on the issue is Weapons of Math Destruction by Cathy O’Neil. A podcast about the book is at: https://99percentinvisible.org/episode/the-age-of-the-algorithm/.\nAlso keep in mind the various laws which are designed to protect privacy and civil liberties. Just because you didn’t try to build an algorithm that is biased against a protected group does not mean that you are off the hook. There are two ways that laws are enforced (both equally important):\n\ndisparate treatment \\(\\rightarrow\\) means that the differential treatment is intentional\ndisparate impact \\(\\rightarrow\\) means that the differential treatment is unintentional or implicit (some examples include advancing mortgage credit, employment selection, predictive policing)\n\n\nAlexandria Ocasio-Cortez, Jan 22, 2019 MLK event with Ta-Nehisi Coates http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html\nS. Barocas and A. Selbst, “Big Data’s Disparate Impact”, California Law Review, 671, 2016.\n\nAnti-discrimination Laws\n\nCivil Rights Acts of 1964 and 1991\nAmericans with Disabilities Act\nGenetic Information Nondiscrimination Act\nEqual Credit Opportunity Act\nFair Housing Act\nSentencing\n“Machine Bias” in Pro Publica by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, May 23, 2016 https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing/\n\n\n\n\n\n\n\nDylan Fugett had three subsequent arrests for drug possession. Bernard Parker had no subsequent offenses.\n\n\n\n\n\nDYLAN FUGETT\nBERNARD PARKER\n\n\n\nPrior Offense\nPrior Offense\n\n\n1 attempted burglary\n1 resisting arrest without violence\n\n\nLOW RISK3\nHIGH RISK10\n\n\nSubsequent Offenses\nSubsequent Offenses\n\n\n3 drug possessions\nNone\n\n\n\n\n\n\n\n\n\n\nFalse positive and false negative rates broken down by race.\n\n\n\nAlgorithmic Justice League\nhttps://www.ajlunited.org/\nThe Algorithmic Justice League is a collective that aims to:\n\nHighlight algorithmic bias through media, art, and science\nProvide space for people to voice concerns and experiences with coded bias\nDevelop practices for accountability during design, development, and deployment of coded systems\nJoy Buolamwini – AI, Ain’t I A Woman? https://www.youtube.com/embed/QxuyfWoVV98\nSentiment Analysis\nhttps://blog.dominodatalab.com/video-how-machine-learning-amplifies-societal-privilege/\nIn this talk, Mike Williams, Research Engineer at Fast Forward Labs, looks at how supervised machine learning has the potential to amplify power and privilege in society. Using sentiment analysis, he demonstrates how text analytics often favors the voices of men. Mike discusses how bias can inadvertently be introduced into any model, and how to recognize and mitigate these harms.\n\nThere isn’t an option which is objective and fair. That option doesn’t exist… The whole premise is based on bias in your training set. If there are no biases, there are no patterns in your training set. Then the system is not going to work… Supervised machine learning, when it goes really well, when you do a really good job, reproduces the biases in the training data.\n\n\nWilliams’ full (biased!) sentiment analysis on GitHub: https://github.com/williamsmj/sentiment/blob/master/sentiment.ipynb\n\nR packages\n\nwru Who are You? Bayesian Prediction of Racial Category Using Surname and Geolocation https://cran.r-project.org/web/packages/wru/index.html\ntm A framework for text mining applications within R. https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf\nRSentiment Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, Neutral. For a vector of sentences, it counts the number of sentences in each category of sentiment.In calculating the score, negation and various degrees of adjectives are taken into consideration. It deals only with English sentences. https://cran.r-project.org/web/packages/RSentiment/index.html\nSentimentAnalysis Performs a sentiment analysis of textual contents in R. This implementation utilizes various existing dictionaries, such as Harvard IV, or finance-specific dictionaries. Furthermore, it can also create customized dictionaries. The latter uses LASSO regularization as a statistical approach to select relevant terms based on an exogenous response variable. https://cran.r-project.org/web/packages/SentimentAnalysis/index.html",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "07-ethics.html#guiding-ethical-principles",
    "href": "07-ethics.html#guiding-ethical-principles",
    "title": "7  Ethics",
    "section": "\n7.7 Guiding Ethical Principles",
    "text": "7.7 Guiding Ethical Principles\nFrom Baumer, Kaplan, and Horton (2021): 1. Do your work well by your own standards and by the standards of your profession (avoid using your skills in a way that is effectively lying - leading others to believe one thing when in fact something different is true).\n\nRecognize the parties to whom you have a special professional obligation (you should worry about conflict of interest!).\nReport results and methods honestly and respect your responsibility to identify and report flaws and shortcomings in your work (don’t be over-confident, do use reproducible methods).\n\nASA Ethical Guidelines for Statistical Practice\nhttp://www.amstat.org/ASA/Your-Career/Ethical-Guidelines-for-Statistical-Practice.aspx\nProfessional Integrity and Accountability\nThe ethical statistician uses methodology and data that are relevant and appropriate, without favoritism or prejudice, and in a manner intended to produce valid, interpretable, and reproducible results. The ethical statistician does not knowingly accept work for which they are not sufficiently qualified, is honest with the client about any limitation of expertise, and consults other statisticians when necessary or in doubt.\nIntegrity of Data and Methods\nThe ethical statistician is candid about any known or suspected limitations, defects, or biases in the data that may impact the integrity or reliability of the statistical analysis. Objective and valid interpretation of the results requires that the underlying analysis recognizes and acknowledges the degree of reliability and integrity of the data.\nResponsibilities to Science/Public/Funder/Client\nThe ethical statistician supports valid inferences, transparency, and good science in general, keeping the interests of the public, funder, client, or customer in mind (as well as professional colleagues, patients, the public, and the scientific community).\nResponsibilities to Research Subjects\nThe ethical statistician protects and respects the rights and interests of human and animal subjects at all stages of their involvement in a project. This includes respondents to the census or to surveys, those whose data are contained in administrative records, and subjects of physically or psychologically invasive research.\nResponsibilities to Research Team Colleagues\nScience and statistical practice are often conducted in teams made up of professionals with different professional standards. The statistician must know how to work ethically in this environment.\nResponsibilities to Other Statisticians or Statistics Practitioners\nThe practice of statistics requires consideration of the entire range of possible explanations for observed phenomena, and distinct observers drawing on their own unique sets of experiences can arrive at different and potentially diverging judgments about the plausibility of different explanations. Even in adversarial settings, discourse tends to be most successful when statisticians treat one another with mutual respect and focus on scientific principles, methodology and the substance of data interpretations.\nResponsibilities Regarding Allegations of Misconduct\nThe ethical statistician understands the difference between questionable scientific practices and practices that constitute misconduct, avoids both, but knows how each should be handled.\nResponsibilities of Employers, Including Organizations, Individuals, Attorneys, or Other Clients Employing Statistical Practitioners\nThose employing any person to analyze data are implicitly relying on the profession’s reputation for objectivity. However, this creates an obligation on the part of the employer to understand and respect statisticians’ obligation of objectivity.\n\n\n\nReproduction of a data graphic reporting the number of gun deaths in Florida over time. The original image was published by Reuters. (Baumer, Kaplan, and Horton 2021)\nA tweet by National Review on December 14, 2015 showing the change in global temperature over time. (Baumer, Kaplan, and Horton 2021)\nMay 10, 2020, Georgia Department of Health, COVID-19 cases for 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\nMay 17, 2020, Georgia Department of Health, COVID-19 cases for 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\nJuly 2, 2020 (left) and July 17, 2020 (right), Georgia Department of Health, COVID-19 cases per 100K https://dph.georgia.gov/covid-19-daily-status-report\nJuly 2, 2020 (left) and July 17, 2020 (right), Georgia Department of Health, COVID-19 cases per 100K https://dph.georgia.gov/covid-19-daily-status-report\nFrom the Kent County COVID dashboard, screen shot 8/25/2021. https://www.accesskent.com/Health/covid-19-data.htm\nNot only is it worth discussing the ethics of how the data were collected, but it also seems like maybe the study did some p-hacking.\nThe paper was retracted because the authors could not agree on the order of authorship.\nDylan Fugett had three subsequent arrests for drug possession. Bernard Parker had no subsequent offenses.\nFalse positive and false negative rates broken down by race.\n\n\n\nBaumer, Ben, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. CRC Press. https://mdsr-book.github.io/mdsr2e/.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Ethics</span>"
    ]
  },
  {
    "objectID": "06-bootstrap.html",
    "href": "06-bootstrap.html",
    "title": "6  Bootstrapping",
    "section": "",
    "text": "6.1 Introduction\nAs we did with permutation tests, we are going to use random samples to describe the population (assuming we have a simple random sample).\nMain idea: we will be able to estimate the variability of the estimator (difference in medians, ordinary least square with non-normal errors, etc.).\nThe following applets may be helpful:",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "06-bootstrap.html#introduction",
    "href": "06-bootstrap.html#introduction",
    "title": "6  Bootstrapping",
    "section": "",
    "text": "It’s not so strange to get \\(\\hat{\\theta}\\) and SE(\\(\\hat{\\theta}\\)) from the data (consider \\(\\hat{p}\\) & \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) and \\(\\overline{X}\\) & \\(s/\\sqrt{n}\\)).\nWe’ll only consider confidence intervals for now.\nBootstrapping doesn’t help get around small samples.\n\n\n\nThe logic of confidence intervals https://www.rossmanchance.com/applets/2021/confsim/ConfSim.html\nBootstrapping from actual datasets https://www.lock5stat.com/StatKey/index.html",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "06-bootstrap.html#BSnotation",
    "href": "06-bootstrap.html#BSnotation",
    "title": "6  Bootstrapping",
    "section": "\n6.2 Basics & Notation",
    "text": "6.2 Basics & Notation\nLet \\(\\theta\\) be the parameter of interest, and let \\(\\hat{\\theta}\\) be the estimate of \\(\\theta\\). If we could, we’d take lots of samples of size \\(n\\) from the population to create a sampling distribution for \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples from \\(F\\):\n\\[\\begin{align}\n\\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}_b\n\\end{align}\\] is the best guess for \\(\\theta\\). If \\(\\hat{\\theta}\\) is very different from \\(\\theta\\), we would call it biased. \\[\\begin{align}\nSE(\\hat{\\theta}) &= \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B(\\hat{\\theta}_b - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\\nq_1 &= [0.25 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\\nq_3 &= [0.75 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\\n\\end{align}\\]\nIf we could, we would completely characterize the sampling distribution (as a function of \\(\\theta\\)) which would allow us to make inference on \\(\\theta\\) when we only had \\(\\hat{\\theta}\\).\n\n\n\n\n\n\n\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig\n\n\n\n\n6.2.1 The Plug-in Principle\nRecall \\[\\begin{align}\nF(x) &= P(X \\leq x)\\\\\n\\hat{F}(x) &= S(x) = \\frac{\\# \\{X_i \\leq x\\} }{n}\n\\end{align}\\] \\(\\hat{F}(x)\\) is a sufficient statistic for \\(F(x)\\). That is, all the information about \\(F\\) that is in the data is contained in \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) is the MLE of \\(F(x)\\) (they are both probabilities, so it’s a binomial argument).\nNote that, in general, we are interested in a parameter, \\(\\theta\\). \\[\\begin{align}\n\\theta = t(F) \\ \\ \\ \\ [\\mbox{e.g., } \\mu = \\int x f(x) dx ]\n\\end{align}\\]\nThe plug-in estimate of \\(\\theta\\) is: \\[\\begin{align}\n\\hat{\\theta} = t(\\hat{F}) \\ \\ \\ \\ [\\mbox{e.g., } \\overline{X} = \\frac{1}{n} \\sum X_i ]\n\\end{align}\\]\nThat is: to estimate a parameter, use the statistic that is the corresponding quantity for the sample.\n\n\n\n\n\n\n\nIdeal Real World\n\nBootstrap World\n\n\n\n\\(F \\rightarrow x\\)\n\\(\\Rightarrow\\)\n\\(\\hat{F} \\rightarrow x^*\\)\n\n\n\\(\\downarrow\\)\n\n\\(\\downarrow\\)\n\n\n\\(\\hat{\\theta}\\)\n\n\\(\\hat{\\theta}^*\\)\n\n\n\nThe idea of boostrapping (and in fact, the bootstrap samples themselves), depends on the double arrow. We must have a random sample: that is, \\(\\hat{F}\\) must do a good job of estimating \\(F\\) in order for bootstrap concepts to be meaningful.\nNote that you’ve seen the plug-in-principle before: \\[\\begin{align}\n\\sqrt{\\frac{p(1-p)}{n}} &\\approx& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\end{align}\\] \n\n6.2.2 The Bootstrap Idea\nWe can resample from the sample to represent samples from the actual population! The boostrap distribution of a statistic, based on many resamples, represents the sampling distribution of the statistic based on many samples. Is this okay?? What are we assuming?\n\nAs \\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\nAs \\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (with large \\(n\\)). Or really, what we typically see if \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) or \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)\n\n6.2.3 Bootstrap Procedure\n\nResample data with replacement from the original sample.\nCalculate the statistic of interest for each resample.\nRepeat 1. and 2. \\(B\\) times.\nUse the bootstrap distribution for inference.\n\n6.2.4 Bootstrap Notation\nTake many (\\(B\\)) resamples of size \\(n\\) from the sample, \\(\\hat{F}(x)\\) (instead of from the population, \\(F(x)\\) ) to create a bootstrap distribution for \\(\\hat{\\theta}^*\\) (instead of the sampling distribution for \\(\\hat{\\theta}\\)).\nLet \\(\\hat{\\theta}^*(b)\\) be the calculated statistic of interest for the \\(b^{th}\\) bootstrap sample. The best guess for \\(\\theta\\) is: \\[\\begin{align}\n\\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*(b)\n\\end{align}\\] (if \\(\\hat{\\theta}^*\\) is very different from \\(\\hat{\\theta}\\), we call it biased.) And the estimated value for the standard error of the estimate is \\[\\begin{align}\n\\hat{SE}^* = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*(b) - \\hat{\\theta}^*)^2 \\bigg]^{1/2}\n\\end{align}\\]\nJust like repeatedly taking samples from the population, taking resamples from the sample allows us to characterize the bootstrap distribution which approximates the sampling distribution. The bootstrap distribution approximates the shape, spread, & bias of the actual sampling distribution.\n\n\n\n\n\n\n\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\n\n\n\n\n\n\n\n\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\n\n\n\n\n\n\n\n\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\n\n\n\n\nThe StatKey applets which demonstrate bootstrapping are here: https://www.lock5stat.com/StatKey/index.html",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "06-bootstrap.html#BSCI",
    "href": "06-bootstrap.html#BSCI",
    "title": "6  Bootstrapping",
    "section": "\n6.3 Bootstrap Confidence Intervals",
    "text": "6.3 Bootstrap Confidence Intervals\n\n6.3.1 Normal (standard) CI with BootSE: type=\"norm\"\n\nKeep in mind that what we are trying to do is approximate the sampling distribution of \\(\\hat{\\theta}\\). In fact, what we are really able to do here is to estimate the sampling distribution of \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). We hope that:\n\\[\\begin{align}\n\\hat{F}\\Big(\\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)} \\Big) \\rightarrow F\\Big(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\Big)\n\\end{align}\\]\nRecall the derivation of conventional confidence intervals (based on the assumption that the sampling distribution of the test statistic is normal or close):\n\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\]\nThat is, it’s the endpoints that are random, and we have a 0.95 probability that we’ll get a random sample which will produce endpoints which will capture the true parameter.\nA 95% CI for \\(\\theta\\) would then be: \\[\\hat{\\theta} \\pm z_{(\\alpha/2)} \\hat{SE}^*\\]\n\n6.3.2 Bootstrap-t Confidence Intervals: type=\"stud\"\n\n(The idea here is that we are calculating the “t-multiplier” that is used in the CI. It was William Gosset who went my the pseudonym of “Student” who originally figured out the distribution of the t-multiplier, so the following intervals are called either “studentized” or “t” bootstrap confidence intervals.)\nRecall the derivation of conventional confidence intervals:\n\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\]\nThat is, it’s the endpoints that are random, and we have a 0.95 probability that we’ll get a random sample which will produce endpoints which will capture the true parameter.\n\nWe could use the Boot SE within the CI formula (and did for the interval above). The problem is that such an interval will only be accurate if the distribution for \\(\\hat{\\theta}\\) is reasonably normal. If there is any bias or skew, the CI will not have desired coverage levels (Efron and Tibshirani (1993), pg 161 and chapter 22).\nNow consider using the bootstrap to estimate the distribution for \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). \\[\\begin{align}\nT^*(b) &= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{align}\\]\n\nwhere \\(\\hat{\\theta}^*(b)\\) is the value of \\(\\hat{\\theta}\\) for the \\(b^{th}\\) bootstrap sample, and \\(\\hat{SE}^*(b)\\) is the estimated standard error of \\(\\hat{\\theta}^*(b)\\) for the \\(b^{th}\\) bootstrap sample. The \\(\\alpha^{th}\\) percentile of \\(T^*(b)\\) is estimated by the value of \\(\\hat{t}^*_\\alpha\\) such that\n\\[\\begin{align}\n\\frac{\\# \\{T^*(b) \\leq \\hat{t}^*_{\\alpha/2} \\} }{B} = \\alpha/2\n\\end{align}\\]\nFor example, if \\(B=1000\\), the estimate of the 5% point is the \\(50^{th}\\) smallest value of the \\(T^*(b)\\)s, and the estimate of the 95% point is the \\(950^{th}\\) smallest value of the \\(T^*(b)\\)s.\nFinally, the boostrap-t confidence interval is: \\[\n(\\hat{\\theta} - \\hat{t}^*_{1-\\alpha/2}\\hat{SE}^*,  \\hat{\\theta} - \\hat{t}^*_{\\alpha/2}\\hat{SE}^*)\n\\tag{6.1}\\]\nTo find a bootstrap-t interval, we have to bootstrap twice. The algorithm is as follows:\n\nGenerate \\(B_1\\) bootstrap samples, and for each sample \\(\\underline{X}^{*b}\\) compute the bootstrap estimate \\(\\hat{\\theta}^*(b)\\).\nTake \\(B_2\\) bootstrap samples from \\(\\underline{X}^{*b}\\), and estimate the standard error, \\(\\hat{SE}^*(b)\\).\nFind \\(B_1\\) values for \\(T^*(b)\\). Calculate \\(\\hat{t}^*_\\alpha/2\\) and \\(\\hat{t}^*_{1-\\alpha/2}\\).\nCalculate the CI as in Equation 6.1.\n\n\nIf \\(B\\cdot \\alpha\\) is not an integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) and \\(B+1-k\\).\nBootstrap-t intervals are somewhat erratic and can be influenced by a few outliers. Percentile methods can be more reliable. (The balance of which is best when is an open question depending a lot on the data distribution and statistic of interest.)\n\\(B=100\\) or 200 is probably not enough for a bootstrap-t CI (500 or 1000 is better). However, \\(B=25\\) may be enough to estimate the SE in the inner-Bootprocedure. (\\(B=1000\\) is needed for computing percentiles.)\n\nIn choosing the appropriate multiplier:\n\nWhen it is the correct multiplier to use, the normal multiplier (\\(z\\)) is good for all \\(n\\) and all samples.\nWhen it is the correct multiplier to use, the t multiplier is good for all samples but a specified \\(n\\).\nWhen it is the correct multiplier to use, the bootstrap-t multiplier is good for this sample only.\n\n\nThe resulting intervals will typically not be symmetric (that is \\(\\hat{t}^*_\\alpha \\ne - \\hat{t}^*_{1-\\alpha}\\)). This is part of the improvement over \\(z\\) or \\(t\\) intervals.\nBootstrap-t intervals are good for location statistics (mean, quantiles, trimmed means) but cannot be trusted for other statistics like the correlation (which do not necessarily vary based on ideas of shift).\n\n6.3.3 Percentile Confidence Intervals: type=\"perc\"\n\nThe interval between the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the bootstrap distribution of a statistic is a \\((1-\\alpha)100\\%\\) bootstrap percentile confidence interval for the corresponding parameter:\n\\[\\begin{align}\n[\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}]\n\\end{align}\\]\n\nYou do not need to know why the percentile interval works… but isn’t it so cool to see how it works???\n\nWhy does it work? It isn’t immediately obvious that the interval above will capture the true parameter, \\(\\theta\\), at a rate or 95%. Consider a skewed sampling distribution. If the observed \\(\\hat{\\theta}\\) comes from the long tail, is it obvious that the short tail side of the CI will get up to the true parameter value at the correct rate? (Hall (1992) (and earlier papers) refers to these as Efron’s “backwards” intervals.) Or, if the sampling distribution is biased, the percentiles of the bootstrap interval won’t capture the parameter with the correct rate.\nTo see how / why percentiles intervals work, we first start by considering normal sampling distributions for a function of the statistic. Let \\(\\phi = g(\\theta), \\hat{\\phi} = g(\\hat{\\theta}), \\hat{\\phi}^* = g(\\hat{\\theta}^*)\\), where \\(g\\) is a monotonic function (assume wlog that \\(g\\) is increasing). The point is to choose, if possible, \\(g(\\cdot)\\) such that\n\\[\n\\hat{\\phi}^* - \\hat{\\phi} \\sim \\hat{\\phi} - \\phi \\sim N(0, \\sigma^2).\n\\tag{6.2}\\]\nAgain, consider the logic for the conventional confidence interval. Because \\(\\hat{\\phi} - \\phi \\sim N(0, \\sigma^2)\\), the interval for \\(\\theta\\) is derived by:\n\\[\n\\begin{align}\nP(z_{0.05} \\leq \\frac{\\hat{\\phi} - \\phi}{\\sigma}  ) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} - z_{0.05} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} + z_{0.95} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\theta \\leq g^{-1}(\\hat{\\phi} + z_{0.95} \\sigma)) = 0.95  \\nonumber \\\\\n\\Rightarrow \\mbox{CI for } \\theta: \\ \\ \\ (-\\infty, g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha}))\n\\end{align}\n\\tag{6.3}\\]\nwhere \\(z_{1-\\alpha}\\) is the \\(100(1-\\alpha)\\) percent point of the standard normal distribution. Ideally, if we knew \\(g\\) and \\(\\sigma\\), we’d be able to do the transformation and find \\(g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})\\) (which would give the endpoint of the confidence interval).\nGoing back to Equation 6.2 indicates that \\(\\hat{\\phi} + \\sigma z_{1-\\alpha} = F^{-1}_{\\hat{\\phi}^*}(1-\\alpha)\\) (because \\(\\hat{\\phi} ^* \\sim N(\\hat{\\phi}, \\sigma^2)\\)). Further, since \\(g\\) is monotonically increasing, \\(F^{-1}_{\\hat{\\phi}^*}(1-\\alpha) = g(F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\\) Substituting in Equation 6.3, gives the percentile interval for \\(\\theta\\),\n\\[\\begin{align}\n(-\\infty, F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\n\\end{align}\\]\n(A similar argument gives the same derivation of the two sided confidence interval. Proof from Carpenter and Bithell (2000)) In order for a percentile interval to be appropriate, the technical condition is only that a normalizing transformation exists. We do not need to actually find the transformation!\n\nThe transformation respecting property\nA CI is transformation respecting if, for any monotone transformation, the CI for the transformed parameter is (exactly) the transformed CI for the unstransformed parameter. Let \\(\\phi = m(\\theta)\\).\n\\[\\begin{align}\n[\\phi_{lo}, \\phi_{up}] = [m(\\theta_{lo}), m(\\theta_{up})]\n\\end{align}\\]\nNote that the idea has to do with the process of creating the CI. That is, if we create the confidence interval using \\(\\phi\\), we’ll get the same thing as if we created the CI using \\(\\theta\\) and then transformed it. It is straightforward to see that the percentile CI is transformation respecting. That is, for any monotone transformation of the statistic and parameter, the CI will be transformed appropriately.\nLet \\[\\begin{align}\n\\hat{\\phi} &= 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg)\\\\\nr &=\\frac{e^{2\\phi}+1}{e^{2\\phi}-1}\\\\\n\\end{align}\\]\nWe know that \\(\\hat{\\phi}\\) does have an approximated normal distribution. So, the percentile CI for \\(\\phi\\) will approximate the normal theory CI which we know to be correct (for a given \\(\\alpha\\)). But once we have a CI for \\(\\phi\\) we can find the CI for \\(\\rho\\) by taking the inverse monotonic transformation; or rather… we can just use the r percentile CI to start with!\n\n\n6.3.3.1 The range preserving property\nAnother advantage of the percentile interval is that it is range preserving. That is, the CI always produces endpoints that fall within the allowable range of the parameter.\n\n6.3.3.2 Bias\nThe percentile interval is not, however, perfect. If the statistic is a biased estimator of the parameter, there will not exist a transformation such that the distribution is centered around the correct function of the parameter. Formally, if \\[\\begin{align}\n\\hat{\\theta} \\sim N(\\theta + bias, \\hat{SE}^2)\n\\end{align}\\] no transformation \\(\\phi = m(\\theta)\\) can fix things up. Keep in mind that standard intervals can fail in a variety of ways, and the percentile method has only fixed the specific situation of the sampling distribution being is non-normal.\n\n6.3.4 What makes a CI procedure good?\nThe following qualities that may or may not result from a confidence interval procedure are what determines the choice of method for the researcher.\n\nSymmetry (??): the interval is symmetric, pivotal around some value. Not necessarily a good thing. Maybe a bad thing to force?\nResistant: BS-t is particularly not resistant to outliers or crazy sampling distributions of the statistic (can make it more robust with a variance stabilizing transformation)\nRange preserving: the CI always contains only values that fall within an allowable range (\\(p, \\rho\\),…)\nTransformation respecting: for any monotone transformation, \\(\\phi = m(\\theta)\\), the interval for \\(\\theta\\) is mapped directly by \\(m(\\theta)\\). If \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) is a \\((1-\\alpha)100\\)% interval for \\(\\theta\\), then\n\n\\[\n\\begin{align}\n[\\hat{\\phi}_{(lo)},\\hat{\\phi}_{(hi)}] = [m(\\hat{\\theta}_{(lo)}),m(\\hat{\\theta}_{(hi)})]\n\\end{align} \\] are exactly the same interval.\n\nLevel of confidence: A central (not symmetric) confidence interval, \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) should have probability \\(\\alpha/2\\) of not covering \\(\\theta\\) from above or below:\n\n\\[\\begin{align}\nP(\\theta &lt; \\hat{\\theta}_{(lo)})&=\\alpha/2\\\\\nP(\\theta &gt; \\hat{\\theta}_{(hi)})&=\\alpha/2\\\\\n\\end{align}\\]\n\n\nNote: all of the intervals are approximate. We judge them based on how accurately they cover \\(\\theta\\).\n\nA CI is first order accurate if: \\[\\begin{align}\nP(\\theta &lt; \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta &gt; \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]\nA CI is second order accurate if: \\[\\begin{align}\nP(\\theta &lt; \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta &gt; \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]\n\n\n\nBS-t is \\(2^{nd}\\) order accurate for a large general class of functions. However, in practice, the coverage rate doesn’t kick in for small/med sample sizes unless appropriate transformations make the distribution more bell-shaped. (Tibshirani 1988)\n\n\n\n\n\n\n\n\n\n\n\nCI\nSymmetric\nRange Resp\nTrans Resp\nAccuracy\nNormal Samp Dist?\nOther\n\n\n\nBootSE\nYes\nNo\nNo\n\n\\(1^{st}\\) order\nYes\nparam assump \\(F(\\hat{\\theta})\\)\n\n\n\nBS-t\nNo\nNo\nNo\n\n\\(2^{nd}\\) order\nYes/No\ncomputer intensive\n\n\nperc\nNo\nYes\nYes\n\n\\(1^{st}\\) order\nNo\nsmall \\(n \\rightarrow\\) low accuracy\n\n\nBCa\nNo\nYes\nYes\n\n\\(2^{nd}\\) order\nNo\nlimited param assump\n\n\n\nAll of the above criteria speak to the coverage rates of the parameters. But note that they must be taken in context. Much also depends on: the choice of statistic itself; the original data distribution; any outlying observations; etc.\n\n6.3.4.1 Advantages and Disadvantages\n\nNormal Approximation\n\n\nAdvantages similar to the familiar parametric approach; useful with a normally distributed \\(\\hat{\\theta}\\); requires the least computation (\\(B=50-200\\))\n\nDisadvantages fails to use the entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); only works if \\(\\hat{\\theta}\\) is reasonably normal to start with\n\n\nBootstrap-t Confidence Interval\n\n\nAdvantages highly accurate CI in many cases; handles skewed \\(F(\\hat{\\theta})\\) better than the percentile method\n\nDisadvantages not invariant to transformations; computationally expensive with the double bootstrap; coverage probabilities are best if the distribution of \\(\\hat{\\theta}\\) is nice (e.g., normal)\n\n\nPercentile\n\n\nAdvantages uses the entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) to be asymmetrical; invariant to transformations; range respecting; simple to execute\n\nDisadvantages small samples may result in low accuracy (because of the dependence on the tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) to be unbiased\n\n\nBCa\n\n\nAdvantages all of those of the percentile method; allows for bias in \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can be calculated easily from \\(\\hat{F}^*(\\hat{\\theta}^*)\\)\n\n\nDisadvantages requires a limited parametric assumption; more computational than other intervals\n\n\n\n6.3.4.2 Bootstrap CI and Hypothesis Testing\nIf a null value for a parameter is not contained in a CI, we reject the null hypothesis; similarly, we do not reject a null value if it does lie inside the CI. Using BootCIs, we can apply the same logic, and test any hypothesis of interest (note: we can always create one-sided intervals as well!). But using CIs leaves out the p-value information. How do we get a p-value from a CI? Consider an alternative definition for the p-value:\np-value: The smallest level of significance at which you would reject \\(H_0\\).\nSo, what we want is for the null value (\\(\\theta_0\\)) to be one of the endpoints of the confidence interval with some level of confidence \\(1-2\\alpha_0\\). \\(\\alpha_0\\) will then be the one-sided p-value, \\(2\\alpha_0\\) will be the two-sided p-value.\nFor percentile intervals, \\[\\begin{align}\np-value = \\alpha_0 = \\frac{\\# \\hat{\\theta}^*(b) &lt; \\theta_0}{B}\n\\end{align}\\] (without loss of generality, assuming we set \\(\\hat{\\theta}^*_{lo} = \\theta_0\\)).\n\n\nBCa CI: type=\"bca\"\n\n\nAnother cool bootstrap CI method that we won’t have time to cover. You are not responsible for the remainder of the bootstrap material in these notes.\n\nIn the percentile method, we’ve assumed that there exists a transformation of \\(\\theta\\), \\(\\phi(\\theta)\\), such that \\[\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(0,1)\n\\end{align}\\] The transformation assumes that neither \\(\\theta\\) nor \\(\\phi\\) are biased, and it assumes that the variance is constant for all values of the parameter. That is, in the percentage intervals, we assume the normalizing transformation creates a sampling distribution that is unbiased and variance stabilizing. Consider a monotone transformation that *normalizes} the sampling distribution (we no longer assume unbiased or constant variance).\nWe now consider the case where \\(\\theta\\) is a biased estimator. That is: \\[\\begin{align}\n\\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{c} \\sim N(-z_0,1)\n\\end{align}\\] We’ve corrected for the bias, but if there is non-constant variance, we need a further adjustment to stabilize the variance:\n\\[\n\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(-z_0 \\sigma_\\phi,\\sigma_\\phi), \\ \\ \\ \\ \\ \\ \\sigma_\\phi = 1 + a \\phi\n\\end{align}\\] That is, there must exist a monotone transformation \\(\\phi\\) such that \\(\\phi(\\hat{\\theta}) \\sim N\\) where \\[ \\begin{align}\nE(\\phi(\\hat{\\theta})) = \\phi(\\theta) - z_0 [1 + a \\phi(\\theta)] \\mbox{  and  } SE(\\phi(\\hat{\\theta})) = 1 + a \\phi(\\theta)\n\\end{align} \\] (Note: in the expected value and SE we’ve assumed that \\(c=1\\). If \\(c\\ne1\\), then we can always choose a different transformation, \\(\\phi'\\) so that \\(c=1\\).) Then \\[ \\begin{align}\nP(z_{\\alpha/2} \\leq \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{1 + a \\phi(\\theta)} + z_0 \\leq z_{1-\\alpha/2}) = 1 - \\alpha\n\\end{align} \\] A \\((1-\\alpha)\\) 100% CI for \\(\\phi(\\theta)\\) is \\[\\begin{align}\n\\bigg[ \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{1 + a (z_{1-\\alpha/2} - z_0)}, \\frac{\\phi(\\hat{\\theta}) - (z_{\\alpha/2} - z_0)}{1 + a (z_{\\alpha/2} - z_0)} \\bigg]\n\\end{align} \\] Let’s consider an interesting probability question: \\[ \\begin{align}\nP\\bigg( \\phi(\\hat{\\theta}^*) &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))} \\bigg) = ?\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta}) - \\phi(\\hat{\\theta})a(z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta})a(z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &\\leq \\frac{ -(1+a \\phi(\\hat{\\theta})) (z_{1-\\alpha/2} - z_0) }{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) }{(1 + a (z_{1-\\alpha/2} - z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} + z_0 &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n= P\\bigg( Z &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg) = \\gamma_1\\\\\n\\mbox{where } \\gamma_1 &= \\Phi \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n&= \\verb;pnorm; \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\n\\end{align} \\]\nWhat we’ve shown is that the \\(\\gamma_1\\) quantile of the \\(\\phi(\\hat{\\theta}^*)\\) sampling distribution will be a good estimate for the lower bound of the confidence interval for \\(\\phi(\\theta)\\). Using the same argument on the upper bound, we find a \\((1-\\alpha)\\) 100% confidence interval for \\(\\phi(\\theta)\\) to be:\n\\[ \\begin{align}\n&[\\phi(\\hat{\\theta}^*)_{\\gamma_1}, \\phi(\\hat{\\theta}^*)_{\\gamma_2}]\\\\\n& \\\\\n\\mbox{where } \\gamma_1 &= \\Phi\\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\gamma_2 &= \\Phi \\bigg(\\frac{ (z_{1-\\alpha/2} + z_0) }{(1 - a (z_{1-\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\end{align} \\]\nUsing the transformation respecting property of percentile intervals, we know that a \\((1-\\alpha)\\) 100% confidence interval for \\(\\theta\\) is:\n\\[ \\begin{align}\n&[\\hat{\\theta}^*_{\\gamma_1}, \\hat{\\theta}^*_{\\gamma_2}]\n\\end{align} \\]\nHow do we estimate \\(a\\) and \\(z_0\\)?\n\n\nbias: \\(z_0\\) is a measure of the bias. Recall:\n\n\\[ \\begin{align}\nbias &= E(\\hat{\\theta}) - \\theta\\\\\n\\hat{bias} &= \\hat{\\theta}^* - \\hat{\\theta}\\\\\n\\end{align} \\]\nBut remember that \\(z_0\\) represents the bias for \\(\\phi(\\hat{\\theta})\\), not for \\(\\hat{\\theta}\\) (and we don’t know \\(\\phi\\)!). So, we use \\(\\theta\\) to see what proportion of \\(\\theta\\) values are too low, and we can map it back to the \\(\\phi\\) space using the normal distribution:\n\\[ \\begin{align}\n\\hat{z}_0 &= \\Phi^{-1} \\bigg( \\frac{ \\# \\hat{\\theta}^*(b) &lt; \\hat{\\theta}}{B} \\bigg)\n\\end{align} \\] That is, if \\(\\hat{\\theta}^*\\) underestimates \\(\\hat{\\theta}\\), then \\(\\hat{\\theta}\\) likely underestimates \\(\\theta\\); \\(z_0 &gt; 0\\). We think of \\(z_0\\) and the normal quantile associated with the proportion of Bootreplicates less than \\(\\hat{\\theta}\\).\n\n\nskew: \\(a\\) is a measure of skew. \\[\\begin{align}\nbias&= E(\\hat{\\theta} - \\theta)\\\\\nvar &= E(\\hat{\\theta} - \\theta)^2 = \\sigma^2\\\\\nskew &= E(\\hat{\\theta} - \\theta)^3 / \\sigma^3\\\\\n\\end{align}\\] We can think of the skew as the rate of chance of the standard error on a normalized scale. If there is no skew, we will estimate \\(a=0\\). The estimate of \\(a\\) comes from a procedure known as the jackknife.\n\n\\[\\begin{align}\n\\hat{a} = \\frac{\\sum_{i=1}^n (\\hat{\\theta} - \\hat{\\theta}_{(i)})^3}{6 [ \\sum_{i=1}^n (\\hat{\\theta} - \\hat{\\theta}_{(i)})^2 ] ^{3/2}}\n\\end{align}\\]",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "06-bootstrap.html#r-example-heroin",
    "href": "06-bootstrap.html#r-example-heroin",
    "title": "6  Bootstrapping",
    "section": "\n6.4 R example: Heroin",
    "text": "6.4 R example: Heroin\nEveritt and Rabe-Hesketh (2006) report on a study by Caplehorn and Bell (1991) that investigated the times (in days) spent a clinic for methadone maintenance treatment for people addicted to heroin. The data in heroin.txt include the amount of time that the subjects stayed in the facility until treatment was terminated (column 4). For about 37% of the subjects, the study ended while they were still the in clinic (status=0). Thus, their survival time has been truncated. For this reason we might not want to estimate the mean survival time, but rather some other measure of typical survival time. Below we explore using the median as well as the 25% trimmed mean. We treat the group of 238 patients as representative of the population. (From Chance and Rossman 2018, Investigation 4.5.3)\nWhy bootstrap?\nMotivation: to estimate the variability of a statistic (not dependent on \\(H_0\\) being true).\nReading in the data\n\nheroin &lt;- read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nheroin |&gt;\n  select(-prison)\n\n# A tibble: 238 × 5\n      id clinic status times  dose\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1      1      1   428    50\n 2     2      1      1   275    55\n 3     3      1      1   262    55\n 4     4      1      1   183    30\n 5     5      1      1   259    65\n 6     6      1      1   714    55\n 7     7      1      1   438    65\n 8     8      1      0   796    60\n 9     9      1      1   892    50\n10    10      1      1   393    65\n# ℹ 228 more rows\n\n\nObserved Test Statistic(s)\n\nheroin |&gt;\n  summarize(obs_med = median(times), \n            obs_tr_mean = mean(times, trim = 0.25))\n\n# A tibble: 1 × 2\n  obs_med obs_tr_mean\n    &lt;dbl&gt;       &lt;dbl&gt;\n1    368.        378.\n\n\nBootstrapped data!\n\nset.seed(4747)\n\nheroin |&gt; \n  sample_frac(size=1, replace=TRUE) |&gt;\n  summarize(boot_med = median(times), \n            boot_tr_mean = mean(times, trim = 0.25))\n\n# A tibble: 1 × 2\n  boot_med boot_tr_mean\n     &lt;dbl&gt;        &lt;dbl&gt;\n1      368         372.\n\n\nNeed to bootstrap a lot of times…\nBelow is the code showing how to bootstrap using for loops (nested to create the t multipliers needed for the BS-t intervals). (There is a package boot which will bootstrap for you, but you need to write functions to use it.)\nset variables\n\nn_rep1 &lt;- 100\nn_rep2 &lt;- 20\nset.seed(4747)\n\nboot stat function\n\nboot_stat_func &lt;- function(df){ \n    df |&gt; \n    mutate(obs_med = median(times),\n           obs_tr_mean = mean(times, trim = 0.25)) |&gt;\n    sample_frac(size=1, replace=TRUE) |&gt;\n    summarize(boot_med = median(times), \n              boot_tr_mean = mean(times, trim = 0.25),\n              obs_med = mean(obs_med),\n              obs_tr_mean = mean(obs_tr_mean))}\n\nresample function\n\nboot_1_func &lt;- function(df){\n  df |&gt; \n    sample_frac(size=1, replace=TRUE)\n}\n\nbootstrapping\n\nmap_df(1:n_rep1, ~boot_stat_func(df = heroin))\n\n# A tibble: 100 × 4\n   boot_med boot_tr_mean obs_med obs_tr_mean\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1     368          372.    368.        378.\n 2     358          363.    368.        378.\n 3     431          421.    368.        378.\n 4     332.         350.    368.        378.\n 5     310.         331.    368.        378.\n 6     376          382.    368.        378.\n 7     366          365.    368.        378.\n 8     378.         382.    368.        378.\n 9     394          386.    368.        378.\n10     392.         402.    368.        378.\n# ℹ 90 more rows\n\n\nWhat do the data distributions look like?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do the sampling distributions look like?\nThe distributions of both the median and the trimmed mean are symmetric and bell-shaped. However, the trimmed mean has a more normal distribution (as evidenced by the points of the qq plot falling on the line y=x).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does the boot output look like?\n\nboot_stats &lt;- map_df(1:n_rep1, ~boot_stat_func(df = heroin))\n\nboot_stats\n\n# A tibble: 100 × 4\n   boot_med boot_tr_mean obs_med obs_tr_mean\n      &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n 1     362          373.    368.        378.\n 2     342.         345.    368.        378.\n 3     388.         393.    368.        378.\n 4     452          428.    368.        378.\n 5     400.         400.    368.        378.\n 6     348          363.    368.        378.\n 7     399          405.    368.        378.\n 8     394          398.    368.        378.\n 9     358          359.    368.        378.\n10     299          332.    368.        378.\n# ℹ 90 more rows\n\n\n95% normal CI with Boot SE\n\nboot_stats |&gt;\n  summarize(\n    low_med = mean(obs_med) + qnorm(0.025) * sd(boot_med),\n    up_med = mean(obs_med) + qnorm(0.975) * sd(boot_med),\n    low_tr_mean = mean(obs_tr_mean) + qnorm(0.025) * sd(boot_tr_mean),\n    up_tr_mean = mean(obs_tr_mean) + qnorm(0.975) * sd(boot_tr_mean))\n\n# A tibble: 1 × 4\n  low_med up_med low_tr_mean up_tr_mean\n    &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1    301.   434.        332.       425.\n\n\n95% Percentile CI\n\nboot_stats |&gt;\n  summarize(\n    perc_CI_med = quantile(boot_med, c(0.025, 0.975)),\n    perc_CI_tr_mean = quantile(boot_tr_mean, c(0.025, 0.975)),\n    q = c(0.025, 0.975))\n\n# A tibble: 2 × 3\n  perc_CI_med perc_CI_tr_mean     q\n        &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1        319.            329. 0.025\n2        448.            423. 0.975\n\n\n95% Bootstrap-t CI\nNote that the t-value is needed (which requires a different SE for each bootstrap sample). It is necessary to bootstrap twice.\nre-resample function\n\nboot_2_func &lt;- function(df, reps){\n  resample2 &lt;- 1:reps\n  df |&gt;\n    summarize(boot_med = median(times), boot_tr_mean = mean(times, trim = 0.25)) |&gt;\n    cbind(resample2, \n          map_df(resample2, \n                 ~df |&gt; \n                   sample_frac(size=1, replace=TRUE) |&gt;\n                   summarize(boot_2_med = median(times), \n                             boot_2_tr_mean = mean(times, \n                                                   trim = 0.25)))) |&gt;\n    select(resample2, everything())\n}\n\ndouble bootstrap!\n\nboot_2_stats &lt;- data.frame(resample1 = 1:n_rep1) |&gt;\n  mutate(first_boot = map(1:n_rep1, ~boot_1_func(df = heroin))) |&gt;\n  mutate(second_boot = map(first_boot, boot_2_func, reps = n_rep2)) \n\n\n6.4.0.1 Summarizing the double bootstrap\nresults\n\nboot_2_stats |&gt;\n  unnest(second_boot) |&gt;\n  unnest(first_boot) \n\n# A tibble: 476,000 × 12\n   resample1    id clinic status times prison  dose resample2 boot_med\n       &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;\n 1         1   137      2      0   563      0    70         1      372\n 2         1    91      1      0   840      0    80         1      372\n 3         1   250      1      1   117      0    40         1      372\n 4         1   168      2      0   788      0    70         1      372\n 5         1    67      1      1   386      0    60         1      372\n 6         1     3      1      1   262      0    55         1      372\n 7         1   104      2      0   713      0    50         1      372\n 8         1   251      1      1   175      1    60         1      372\n 9         1    68      1      0   439      0    80         1      372\n10         1   118      2      0   532      0    70         1      372\n# ℹ 475,990 more rows\n# ℹ 3 more variables: boot_tr_mean &lt;dbl&gt;, boot_2_med &lt;dbl&gt;,\n#   boot_2_tr_mean &lt;dbl&gt;\n\n\nsummary for resample 1\n\nboot_2_stats |&gt;\n  unnest(second_boot) |&gt;\n  unnest(first_boot) |&gt;\n  select(resample1, resample2, everything() ) |&gt;\n  filter(resample1 == 1) |&gt;\n  select(boot_med, boot_tr_mean, boot_2_med, boot_2_tr_mean) |&gt;\n  skimr::skim_without_charts() |&gt; as_tibble() |&gt; \n  select(skim_variable, numeric.mean, numeric.sd, numeric.p50)\n\n# A tibble: 4 × 4\n  skim_variable  numeric.mean numeric.sd numeric.p50\n  &lt;chr&gt;                 &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1 boot_med               372         0          372 \n2 boot_tr_mean           378.        0          378.\n3 boot_2_med             370.       46.6        365.\n4 boot_2_tr_mean         372.       24.7        370.\n\n\nsummary for all resamples\n\nboot_t_stats &lt;- boot_2_stats |&gt;\n  unnest(second_boot) |&gt;\n  unnest(first_boot) |&gt;\n  group_by(resample1) |&gt;\n  summarize(boot_se_med = sd(boot_2_med),\n            boot_se_tr_mean = sd(boot_2_tr_mean),\n            boot_med = mean(boot_med),  # doesn't do anything, just copies over\n            boot_tr_mean = mean(boot_tr_mean))  |&gt; # the variables into the output\n  mutate(boot_t_med = (boot_med - mean(boot_med)) / boot_se_med,\n         boot_t_tr_mean = (boot_tr_mean - mean(boot_tr_mean)) / boot_se_tr_mean)\n\n  \nboot_t_stats\n\n# A tibble: 100 × 7\n   resample1 boot_se_med boot_se_tr_mean boot_med boot_tr_mean boot_t_med\n       &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n 1         1        46.6            24.7     372          378.     0.0222\n 2         2        24.8            25.3     344          370.    -1.09  \n 3         3        36.9            24.2     372.         380.     0.0145\n 4         4        20.8            19.7     354          359.    -0.817 \n 5         5        30.1            22.0     308          331.    -2.09  \n 6         6        25.5            24.5     318          336.    -2.08  \n 7         7        24.9            20.3     367          378.    -0.159 \n 8         8        46.3            25.0     402          393.     0.670 \n 9         9        32.3            20.8     388.         380.     0.512 \n10        10        18.1            15.1     381          384.     0.555 \n# ℹ 90 more rows\n# ℹ 1 more variable: boot_t_tr_mean &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n95% Bootstrap-t CI\nNote that the t-value is needed (which requires a different SE for each bootstrap sample).\nt-values\n\nboot_t_stats |&gt;\n  select(boot_t_med, boot_t_tr_mean)\n\n# A tibble: 100 × 2\n   boot_t_med boot_t_tr_mean\n        &lt;dbl&gt;          &lt;dbl&gt;\n 1     0.0222          0.140\n 2    -1.09           -0.160\n 3     0.0145          0.257\n 4    -0.817          -0.767\n 5    -2.09           -1.98 \n 6    -2.08           -1.56 \n 7    -0.159           0.202\n 8     0.670           0.764\n 9     0.512           0.280\n10     0.555           0.625\n# ℹ 90 more rows\n\n\nmultipliers\n\nboot_q &lt;- boot_t_stats |&gt;\n  select(boot_t_med, boot_t_tr_mean) |&gt;\n  summarize(q_t_med = quantile(boot_t_med, c(0.025, 0.975)), \n            q_t_tr_mean = quantile(boot_t_tr_mean, c(0.025, 0.975)),\n            q = c(0.025, 0.975))\n\nboot_q\n\n# A tibble: 2 × 3\n  q_t_med q_t_tr_mean     q\n    &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1   -2.08       -2.01 0.025\n2    2.25        2.03 0.975\n\n\npull numbers\n\nboot_q_med &lt;- boot_q |&gt; select(q_t_med) |&gt; pull()\nboot_q_med\n\n 2.5% 97.5% \n-2.08  2.25 \n\nboot_q_tr_mean &lt;- boot_q |&gt; select(q_t_tr_mean) |&gt; pull()\nboot_q_tr_mean\n\n 2.5% 97.5% \n-2.01  2.03 \n\n\nBS-t CI\n\nboot_t_stats |&gt;\n  summarize(boot_t_CI_med = mean(boot_med) + \n                                  boot_q_med*sd(boot_med),\n            boot_t_CI_tr_mean = mean(boot_tr_mean) + \n                                  boot_q_tr_mean * sd(boot_tr_mean),\n            q = c(0.025, 0.975))\n\n# A tibble: 2 × 3\n  boot_t_CI_med boot_t_CI_tr_mean     q\n          &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1          306.              330. 0.025\n2          441.              418. 0.975\n\n\n\n6.4.0.2 Comparison of intervals\nThe first three columns correspond to the CIs for the true median of the survival times. The second three columns correspond to the CIs for the true trimmed mean of the survival times.\n\n\n\n\n\n\n\n\n\n\n\nCI\nLower\nObs Med\nUpper\nLower\nObs Tr Mean\nUpper\n\n\n\nPercentile\n321\n367.50\n434.58\n334.86\n378.30\n419.77\n\n\nw BS SE\n309.99\n367.50\n425.01\n336.87\n378.30\n419.73\n\n\nBS-t\n309.30\n367.50\n425.31\n331.03\n378.30\n421.17\n\n\n\n(Can’t know what the Truth is…)\n\n\n\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\nFrom Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15.\n\n\n\nCaplehorn, JR, and J Bell. 1991. “Methadone Dosage and Retention of Patients in Maintenance Treatment.” The Medical Journal of Australia 154 (3): 195–99.\n\n\nCarpenter, James, and John Bithell. 2000. “Bootstrap Confidence Intervals: When, Which, What? A Practical Guide for Medical Statisticians.” Statistics in Medicine 19: 1141–64.\n\n\nChance, Beth, and Allan Rossman. 2018. Investigating Statistical Concepts, Applications, and Methods. http://www.rossmanchance.com/iscam3/.\n\n\nEfron, Bradley, and Robert Tibshirani. 1993. An Introduction to the Bootstrap. Edited by Chapman and Hall / CRC.\n\n\nEveritt, Brian S., and Sophia Rabe-Hesketh. 2006. Handbook of Statistical Analyses Using Stata. Chapman & Hall.\n\n\nHall, Peter. 1992. The Bootstrap and Edgeworth Expansion. Springer.\n\n\nTibshirani, Robert. 1988. “Variance Stabilization and the Bootstrap.” Biometrika 75: 433–44.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "05-permutation.html",
    "href": "05-permutation.html",
    "title": "5  Permutation Tests",
    "section": "",
    "text": "Motivation:\nRauser says that the in order to follow a statistical argument that uses simulation, you need three things:",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "05-permutation.html#algs",
    "href": "05-permutation.html#algs",
    "title": "5  Permutation Tests",
    "section": "\n5.1 Inference Algorithms",
    "text": "5.1 Inference Algorithms\n\n5.1.1 Hypothesis Test Algorithm\nBefore working out the nitty gritty details, recall the structure of hypothesis testing. Consider the applet on Simulating ANOVA Tables (Chance & Rossman) http://www.rossmanchance.com/applets/AnovaSim.html\n\nChoose a statistic that measures the effect you are looking for. For example, the ANOVA F statistic is:\n\n\\[\\begin{align}\nF &= \\frac{\\text{between-group variability}}{\\text{within-group variability}}\\\\\n&= \\frac{\\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{i\\cdot})^2/(N-K)}\n\\end{align}\\]\n\nConstruct the sampling distribution that this statistic would have if the effect were not present in the population (the null sampling distribution). [The sampling distributions for t statistics and F statistics are based on the Central Limit Theorem and derived in Math 152.]\nLocate the observed statistic in the null distribution. A value in the main body of the distribution could easily occur just by chance. A value in the tail would rarely occur by chance and so is evidence that something other than chance is operating. [This piece is going to happen in permutation tests as well as in analytic tests – the point is to see if the observed data is consistent with the null distribution.]\np-value is the probability of the observed data or more extreme if the null hypothesis is true. [Same definition for analytic, permutation, and randomization tests!]\nTo estimate the p-value for a test of significance, estimate the sampling distribution of the test statistic when the null hypothesis is true by simulating in a manner that is consistent with the null hypothesis. Alternatively, there are analytic / mathematical formulas for many of the common statistical hypothesis tests.\n\n5.1.2 Permutation Tests Algorithm\nTo evaluate the p-value for a permutation test, estimate the sampling distribution of the test statistic when the null hypothesis is true by resampling in a manner that is consistent with the null hypothesis (the number of resamples is finite but can be large!).\n\nChoose a test statistic\nShuffle the data (force the null hypothesis to be true)\nCreate a null sampling distribution of the test statistic (under \\(H_0\\))\nFind the observed test statistic on the null sampling distribution and compute the p-value (observed data or more extreme). The p-value can be one or two-sided.\n\nTechnical Conditions\nPermutation tests fall into a broad class of tests called “non-parametric” tests. The label indicates that there are no distributional conditions required about the data (i.e., no condition that the data come from a normal or binomial distribution). However, a test which is “non-parametric” does not meant that there are no conditions on the data, simply that there are no distributional or parametric conditions on the data. The parameters are at the heart of almost all parametric tests.\nFor permutation tests, we are not basing the test on population parameters, so we don’t need to make any claims about them (i.e., that they are the mean of a particular distribution).\n\n\nPermutation The different treatments have the same effect. [Note: exchangeability, same population, etc.] If the null hypothesis is true, the labels assigning groups are interchangeable with respect to the probability distribution.\n\nNote that it is our choice of statistic which makes the test more sensitive to some kinds of difference (e.g., difference in mean) than other kinds (e.g., difference in variance).\n\n\n\nParametric For example, the different populations have the same mean.\n\nIMPORTANT KEY IDEA the point of technical conditions for parametric or permutation tests is to create a sampling distribution that accurately reflects the null sampling distribution for the statistic of interest (the statistic which captures the relevant research question information).",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "05-permutation.html#perms",
    "href": "05-permutation.html#perms",
    "title": "5  Permutation Tests",
    "section": "\n5.2 Permutation tests in practice",
    "text": "5.2 Permutation tests in practice\nHow is the test interpreted given the different types of sampling which are possibly used to collect the data?\n\nRandom Sample The concept of a p-value usually comes from the idea of taking a sample from a population and comparing it to a sampling distribution (from many many random samples).\n\nRandom Experiment In the context of a randomized experiment, the p-value represents the observed data compared to “happening by chance.”\n\nThe interpretation is direct: if there is only a very small chance that the observed statistic would take such an extreme value, as a result only of the randomization of cases: we reject the null treatment effect hypothesis. CAUSAL!\n\n\n\nObservational Study In the context of observational studies the results are less strong, but it is reasonable to conclude that the effect observed in the sample reflects an effect present in the population.\n\nIn a sample, consider the difference (or ratio) and ask “Is this difference so large it would rarely occur by chance in a particular sample constructed under the null setting?”\nIf the data come from a random sample, then the sample (or results from the sample) are probably consistent with the population [i.e., we can infer the results back to the larger population].\n\n\n\n\n5.2.0.1 Other Test Statistics\nThe example in class used a modification of the ANOVA F-statistic to compare the observed data with the permuted data test statistics. Depending on the data and question, the permuted test statistic can take on any of a variety of forms.\n\n\n\n\n\n\n\nData\nHypothesis Question\nStatistic\n\n\n\n2 categorical\ndiff in prop\n\n\\(\\hat{p}_1 - \\hat{p}_2\\) or \\(\\chi^2\\)\n\n\n\nvariables\nratio of prop\n\\(\\hat{p}_1 /  \\hat{p}_2\\)\n\n\n1 numeric\ndiff in means\n\\(\\overline{X}_1 - \\overline{X}_2\\)\n\n\n1 binary\nratio of means\n\\(\\overline{X}_1 / \\overline{X}_2\\)\n\n\n\ndiff in medians\n\\(\\mbox{median}_1 - \\mbox{median}_2\\)\n\n\n\nratio of medians\n\\(\\mbox{median}_1 / \\mbox{median}_2\\)\n\n\n\ndiff in SD\n\\(s_1 - s_2\\)\n\n\n\ndiff in var\n\\(s^2_1 - s^2_2\\)\n\n\n\nratio of SD or VAR\n\\(s_1 / s_2\\)\n\n\n1 numeric\ndiff in means\n\n\\(\\sum n_i (\\overline{X}_i - \\overline{X})^2\\) or\n\n\nk groups\n\nF stat\n\n\npaired or\n(permute within row)\n\\(\\overline{X}_1 - \\overline{X}_2\\)\n\n\nrepeated measures\n\n\n\n\nregression\ncorrelation\nleast sq slope\n\n\ntime series\nno serial corr\nlag 1 autocross\n\n\n\nDepending on the data, hypotheses, and original data collection structure (e.g., random sampling vs random allocation), the choice of statistic for the permutation test will vary.\n\n5.2.1 Permutation vs. Randomization Tests\nWe will call randomization tests those that enumerate all possible data permutations. permutation tests, on the other hand, will permute the data \\(B\\) (\\(&lt; &lt;\\) all) times. [Some authors call a permutation test applied to a randomized experiment a randomization test, but we will use the term randomization to indicate that all possible permutations have been considered.]\n\nMain difference: randomization tests consider every possible permutation of the labels, permutation tests take a random sample of permutations of the labels.\nBoth can only be applied to a comparison situation (e.g., no one sample t-tests).\nBoth permute labels under \\(H_0\\), for example, \\(H_0: F(x) = G(x)\\).\nBoth can be used in situations where sampling distributions are unknown (e.g., differences in medians).\nBoth can be used in situations where sampling distributions are based on population distributions (e.g., ratio of variances).\nRandomization tests were the first nonparametric tests conceived (R.A. Fisher, 1935).\n\nRandomization p-value\nLet \\(t^*\\) be the observed test statistic. For a two sample test with \\(N\\) total observations and \\(n\\) observations in group 1, there are \\({N \\choose n}\\) randomizations, all of which are equally likely under \\(H_0\\). The p-value then becomes: \\[\\begin{align}\np_R &= P(T \\leq t^* | H_0) = \\frac{\\sum_{i=1}^{{N \\choose n}} I(t_i \\leq t*)}{{N \\choose n}}\n\\end{align}\\] If we choose a significance level of \\(\\alpha = k/{N \\choose n}\\), then the type I error rate is: \\[\\begin{align}\nP(\\text{type I error}) &= P(p_R \\leq \\alpha | H_0)\\\\\n&= P\\bigg(\\sum_{i=1}^{{N \\choose n}} I(t_i \\leq t*) \\leq k | H_0 \\bigg)\\\\\n&= \\frac{k}{{N \\choose n}}= \\alpha\\\\\n\\text{alternatively }  k&= \\alpha {N \\choose n}\n\\end{align}\\] The point of which is to say that the randomization test controls the probability of a Type I error under the very minimal conditions that the subjects are randomized to treatments (a minimal condition, but hard to do in practice!!)\nPermutation p-value\nNow consider a permutation test that randomly permutes the data \\(B\\) times (instead of all \\({N \\choose n}\\) times). A permutation test approximates a randomization test. In fact, the permutation test can be analyzed using the following binomial random variable: \\[\\begin{align}\nX_P &= \\# \\ \\mbox{permutations out of B that give a more extreme value than the observed test statistic}\\\\\nX_P &\\sim Bin(p_R, B)\\\\\nSE(X_P) &= \\sqrt{\\frac{p_R (1-p_R)}{B}} \\approx \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\n\\end{align}\\]\nConsider a situation where interest is in a small effect, say p-value\\(\\approx 0.01\\). The SE should be less than 0.001. \\[\\begin{align}\n0.001 &= \\sqrt{ (0.01)\\cdot(0.99) / B}\\\\\nB &= (0.01) \\cdot (0.99) / (0.001)^2\\\\\n&= 9900\n\\end{align}\\]\nAnother way to look at the same problem is to use the estimated p-value = \\(\\hat{p}_P = \\frac{X_P}{B}\\) to come up with a confidence interval for \\(p_R\\).\nCI for \\(p_R \\approx \\hat{p}_P \\pm 1.96 \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\\)\n\n5.2.2 CI from Permutation Tests\nUse shifts or rescaling to create a CI for a parameter value using permutation tests. That is, consider a situation with data from \\(X\\) and \\(Y\\) Use one of the following transformation (depending on the study): \\[\\begin{align}\nW &= Y + a\\\\\n\\mbox{or } U &= Y / b\n\\end{align}\\] and run the permutation test of interest on \\(X\\) vs. \\(W\\) or \\(X\\) vs. \\(U\\). For a series of \\(a\\) or \\(b\\) values we can find which we don’t reject at a particular level of significance (\\(\\alpha\\)) to create a \\((1-\\alpha)100\\%\\) confidence interval.\nUsually, however, we use bootstrapping for confidence intervals and permutation tests for hypothesis testing.\n\n5.2.3 Randomization Example\n\n5.2.3.1 Fisher’s Exact Test – computationally efficient randomization test\n\nN observations are classified into a 2x2 table.\nEach observation is classified into exactly one cell.\nRow and column totals are fixed.\n\nGiven fixed row and column totals, we can easily calculate the interior distribution using the hypergeometric. Note that once a single cell is filled, all other cells are determined.\n\n\n\nCol 1\nCol 2\nTotal\n\n\n\nRow 1\nX\nr-X\nr\n\n\nRow 2\nc-X\nN-r-c+X\nN-r\n\n\nTotal\nc\nN-c\nN\n\n\n\n\\[\\begin{align}\nP(X=x) &= \\frac{{r \\choose x}{{N-r} \\choose{c-x}}}{{N \\choose c}}\\\\\n& \\mbox{out of those in col 1, how many are in row 1?}\\\\\nP(X \\leq x) &= \\sum_{i=0}^x \\frac{{r \\choose i}{{N-r} \\choose {c-i}}}{{N \\choose c}}\\\\\n&= \\mbox{p-value}\n\\end{align}\\]\nNot common for both row and column totals to be fixed. (More likely for just column totals to be fixed, e.g., men and women.) Instead, consider all subsets of the sample space with \\(N\\) observations. For any particular combination of row and column totals (\\(rc\\)):\n\\[\\begin{align}\nP(\\mbox{rejecting } H_0 | rc, H_0) &\\leq& \\alpha\\\\\nP(\\mbox{rejecting } H_0 \\ \\ \\forall \\mbox{ subsets } | H_0) &\\leq& \\sum_{rc \\ combos} P(\\mbox{rejecting } H_0 | rc, H_0) P(rc | H_0)\\\\\n&\\leq& \\alpha\n\\end{align}\\]\n(Note: assume $P(rc | H_0) = 1 / # rc $ combos.) The test will be valid at any \\(\\alpha\\) level, but it won’t be as powerful as one in which fixed columns/rows is actually meaningful.\n\n5.2.3.2 \\(r \\times c\\) tables\n\npermute data in a new way\nnew test stat \\[\\begin{align}\nT = \\sum_{i,j} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\n\\end{align}\\]\n\n2-sided p-value. what do we expect? \\[\\begin{align}\nE_{i,j} = \\frac{R_i C_j}{N}\n\\end{align}\\]\n\n\n\n5.2.3.2.1 Example: Observer\nIn a study published in the Journal of Personality and Social Psychology (Butler and Baumeister, 1998), researchers investigated a conjecture that having an observer with a vested interest would decrease subjects’ performance on a skill-based task. Subjects were given time to practice playing a video game that required them to navigate an obstacle course as quickly as possible. They were then told to play the game on final time with an observer present. Subjects were randomly assigned to one of two groups:\n\nGroup A was told that the participant and the observer would each win $3 if the participant beat a certain threshold.\nGroup B was told that only that the participant would win the prize if the threshold was beaten.\n\nThe goal of this data analysis is to determine whether or not there is an effect from the observer on the performance. That is, like the \\(\\chi^2\\) test, our hypotheses are:\n\\(H_0:\\) there is no association between the two variables \\(H_a:\\) there is an association between the two variables\nThe data from the 24 subjects is given below:\n\n\n\nA: shares prize\nB: no sharing\nTotal\n\n\n\nBeat threshold\n3\n8\n11\n\n\nDid not beat threshold\n9\n4\n13\n\n\nTotal\n12\n12\n24\n\n\n\n\nCard simulation (to demonstrate how the permutation test works)\nPermutation Test (see Chance and Rossman applet for automated permutation test, http://www.rossmanchance.com/applets/ChisqShuffle.htm?FET=1)\n\n\\[\\begin{align}\nSE(\\mbox{p-value}) = \\sqrt{\\frac{\\hat{p}_r (1-\\hat{p}_r)}{100}} = 0.02\n\\end{align}\\]\n\nRandomization Test \\[\\begin{align}\nP(X \\leq 3) = \\sum_{i=0}^3 \\frac{{11 \\choose i}{12 \\choose {12-i}}}{{24 \\choose 12}} = 0.0436\n\\end{align}\\]",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "05-permutation.html#r-examples",
    "href": "05-permutation.html#r-examples",
    "title": "5  Permutation Tests",
    "section": "\n5.3 R examples",
    "text": "5.3 R examples\n\n5.3.1 Cloud Seeding (Two sample test – computationally very difficult to do a randomization test)\nCloud seeding data: seeding or not seeding was randomly allocated to 52 days when seeding was appropriate. The pilot did not know whether or not the plane was seeding. Rain is measured in acre-feet.\nAfter running tests to compare means and variances we obtain the following p-values:\n\n\n\n\n\n\n\n\n\n\ncomparison of means\n\ncomparison of variances\n\n\n\n\n\nPermutation\nt-test\nPermutation\nF-test\n\n\nRaw Data\n0.031\n0.054\n0.068\n0.000067\n\n\nLogged Data\n0.010\n0.014\n0.535\n0.897\n\n\n\n\n5.3.1.1 R code\nBefore doing anything, let’s look at the data. Here, we visualize with both boxplots and histograms. Also, we visualize on the raw scale as well as the log scale. Certainly, the log10 scale indicates that a transformation makes the data more symmetric.\n\nclouds &lt;- read_delim(\"figs/cloud-seeding.txt\", \n     \"\\t\", escape_double = FALSE, trim_ws = TRUE) \n\nnames(clouds) &lt;- c(\"unseeded\", \"seeded\")\nclouds &lt;- tidyr::pivot_longer(clouds, cols = 1:2, names_to = \"seeding\", values_to = \"rainfall\") |&gt;\n  mutate(seeding = as.factor(seeding))\n\nclouds |&gt;\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot()\n\n\n\n\n\n\nclouds |&gt;\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding)\n\n\n\n\n\n\n\n\nclouds |&gt;\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() + scale_y_log10()\n\n\n\n\n\n\nclouds |&gt;\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) + scale_x_log10()\n\n\n\n\n\n\n\nunlogged data:\n\nclouds |&gt;\n  mutate(lnrain = log(rainfall)) |&gt;\n  group_by(seeding) |&gt;\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain))\n\n# A tibble: 2 × 3\n  seeding  meanrain meanlnrain\n  &lt;fct&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 seeded       442.       5.13\n2 unseeded     165.       3.99\n\nclouds |&gt;\n  mutate(lnrain = log(rainfall)) |&gt;\n  group_by(seeding) |&gt;\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) |&gt;\n  summarize(diff(meanrain), diff(meanlnrain))\n\n# A tibble: 1 × 2\n  `diff(meanrain)` `diff(meanlnrain)`\n             &lt;dbl&gt;              &lt;dbl&gt;\n1            -277.              -1.14\n\nraindiffs &lt;- clouds |&gt;\n  mutate(lnrain = log(rainfall)) |&gt;\n  group_by(seeding) |&gt;\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) |&gt;\n  summarize(diffrain = diff(meanrain), difflnrain = diff(meanlnrain))\n\nraindiffs\n\n# A tibble: 1 × 2\n  diffrain difflnrain\n     &lt;dbl&gt;      &lt;dbl&gt;\n1    -277.      -1.14\n\n\nBelow, we’ve formally gone through a permutation. here, the resampling is not coded in a particularly tidy way, but there is a tidy way to code loops! Generally, loops are not the fasted way to code in R, so if you need to quickly run code that seems like it should go in a loop, it is very likely that purrr is the direction you want to go, https://purrr.tidyverse.org/.\n\n5.3.1.1.1 Difference in means after permuting\n\nreps &lt;- 1000\npermdiffs &lt;- c()\n\nfor(i in 1:reps){\n  onediff &lt;- clouds |&gt;\n    mutate(permseed = sample(seeding)) |&gt;\n    group_by(permseed) |&gt;\n    summarize(meanrain = mean(rainfall)) |&gt;\n    summarize(diff(meanrain)) |&gt; pull()\n  \npermdiffs &lt;- c(permdiffs, onediff)\n}\n\npermdiffs |&gt; data.frame() |&gt;\n  ggplot(aes(x = permdiffs)) + geom_histogram(bins=30) + geom_vline(xintercept = raindiffs$diffrain, color = \"red\")\n\n\n\n\n\n\n\n\n5.3.1.1.2 Ratio of variances after permuting\n\nrainvarratio &lt;- clouds |&gt;\n    group_by(seeding) |&gt;\n    summarize(varrain = var(rainfall)) |&gt;\n    summarize(rainratio = varrain[1] / varrain[2])\n\n\nreps &lt;- 1000\npermvars &lt;- c()\n\nfor(i in 1:reps){\n  oneratio &lt;- clouds |&gt;\n    mutate(permseed = sample(seeding)) |&gt;\n    group_by(permseed) |&gt;\n    summarize(varrain = var(rainfall)) |&gt;\n    summarize(varrain[1] / varrain[2]) |&gt; pull()\n  \npermvars &lt;- c(permvars, oneratio)\n}\n\npermvars |&gt; data.frame() |&gt;\n  ggplot(aes(x = permvars)) + geom_histogram(bins=30) + geom_vline(xintercept = rainvarratio$rainratio , color = \"red\")\n\n\n\n\n\n\n\n\n5.3.1.1.3 Testing differences in means or ratios of variances\nAs evidenced in the histograms above,\n\nthe permutation test (one-sided) for the difference in means will count the number of permuted differences that are less than or equal to the observed difference in means, just over 1%.\nthe permutation test (one-sided) for the ratio of variances will count the number of permuted ratios that are greater than or equal to the observed ratio of variances, about 7%.\n\n\n(sum(raindiffs$diffrain &gt;= permdiffs) + 1) /1000\n\n[1] 0.036\n\n(sum(rainvarratio$rainratio &lt;= permvars)+1)/1000\n\n[1] 0.085\n\n\n\n\n5.3.2 MacNell Teaching Evaluations (Stratified two-sample t-test)\nBoring et al. (2016) reanalyze data from MacNell et al. (2014). Students were randomized to 4 online sections of a course. In two sections, the instructors swapped identities. Was the instructor who identified as female rated lower on average? (https://www.math.upenn.edu/~pemantle/active-papers/Evals/stark2016.pdf)\n\n\n\n\n\n\n\nKraj (2017)\n\n\n\n\n\n\n\n\nKraj (2017)\n\n\n\n\n\n\n\n\n\n\nMengel, Sauermann, and Zölitz (2019)\n\n\n\n\n\n\n\n\n\n\nMacNell, Driscoll, and Hunt (2015)\n\n\n\n\n\n\n\n\nMacNell, Driscoll, and Hunt (2015)\n\n\n\n\n5.3.2.0.1 R code\n\n# The data come from `permuter` which is no longer kept up as a package\nmacnell &lt;- readr::read_csv(\"https://raw.githubusercontent.com/statlab/permuter/master/data-raw/macnell.csv\")\n#library(permuter)\n#data(macnell)\n\n\nlibrary(ggridges)\nmacnell |&gt; \n  mutate(TAID = ifelse(taidgender==1, \"male\", \"female\")) |&gt;\n  mutate(TAGend = ifelse(tagender==1, \"male\", \"female\")) |&gt;\nggplot(aes(y=TAGend, x=overall, \n           group = interaction(TAGend, TAID), \n           fill=TAID)) +\n  geom_point(position=position_jitterdodge(jitter.height=0.3, jitter.width = 0, dodge.width = 0.4), \n             aes(color = TAID)) +\n  stat_summary(fun=\"mean\", geom=\"crossbar\", \n               size=.3, width = 1,\n               aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  stat_summary(fun=\"mean\", geom=\"point\", shape = \"X\",\n               size=5, aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  coord_flip() +\n  labs(title = \"Overall teaching effectiveness score\",\n       x = \"\",\n       y = \"TA gender\",\n       color = \"TA identifier\",\n       fill = \"TA identifier\")\n\n\n\n\n\n\n\n\n5.3.2.1 Analysis goal\nWant to know if the score for the perceived gender is different.\n\\[H_0:  \\mu_{ID.Female} = \\mu_{ID.Male}\\] &gt; Although for the permutation test, under the null hypothesis not only are the means of the population distributions the same, but the variance and all other aspects of the distributions across perceived gender.\n\n5.3.2.2 MacNell Data without permutation\n\nmacnell |&gt;\n  select(overall, tagender, taidgender) |&gt; head(15)\n\n# A tibble: 15 × 3\n   overall tagender taidgender\n     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1       4        0          1\n 2       4        0          1\n 3       5        0          1\n 4       5        0          1\n 5       5        0          1\n 6       4        0          1\n 7       4        0          1\n 8       5        0          1\n 9       4        0          1\n10       3        0          1\n11       5        0          1\n12       4        0          1\n13       5        1          1\n14       5        1          1\n15       4        1          1\n\n\n\n5.3.2.3 Permuting MacNell data\nConceptually, there are two levels of randomization:\n\n\\(N_m\\) students are randomly assigned to the male instructor and \\(N_f\\) are assigned to the female instructor.\nOf the \\(N_j\\) assigned to instructor \\(j\\), \\(N_{jm}\\) are told that the instructor is male, and \\(N_{jf}\\) are told that the instructor is female for \\(j=m,f\\).\n\n\nmacnell |&gt;\n  group_by(tagender, taidgender) |&gt;\n  summarize(n())\n\n# A tibble: 4 × 3\n# Groups:   tagender [2]\n  tagender taidgender `n()`\n     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1        0          0    11\n2        0          1    12\n3        1          0    13\n4        1          1    11\n\n\nStratified two-sample test:\n\n\nFor each instructor, permute perceived gender assignments.\nUse difference in mean ratings for female-identified vs male-identified instructors.\n\n\nmacnell |&gt; \n  group_by(tagender) |&gt;\n  mutate(permTAID = sample(taidgender, replace=FALSE)) |&gt;\n  select(overall, tagender, taidgender, permTAID) \n\n# A tibble: 47 × 4\n# Groups:   tagender [2]\n   overall tagender taidgender permTAID\n     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n 1       4        0          1        0\n 2       4        0          1        0\n 3       5        0          1        1\n 4       5        0          1        1\n 5       5        0          1        1\n 6       4        0          1        0\n 7       4        0          1        0\n 8       5        0          1        0\n 9       4        0          1        1\n10       3        0          1        1\n# ℹ 37 more rows\n\n\n\nmacnell |&gt; \n  group_by(tagender) |&gt;\n  mutate(permTAID = sample(taidgender, replace=FALSE)) |&gt;\n  ungroup(tagender) |&gt;\n  group_by(permTAID) |&gt;\n  summarize(pmeans = mean(overall, na.rm=TRUE)) |&gt;\n  summarize(diff(pmeans))\n\n# A tibble: 1 × 1\n  `diff(pmeans)`\n           &lt;dbl&gt;\n1         0.0909\n\n\n\ndiff_means_func &lt;- function(.x){\n  macnell |&gt; group_by(tagender) |&gt;\n  mutate(permTAID = sample(taidgender, replace=FALSE)) |&gt;\n  ungroup(tagender) |&gt;\n  group_by(permTAID) |&gt;\n  summarize(pmeans = mean(overall, na.rm=TRUE)) |&gt;\n  summarize(diff_mean = diff(pmeans))\n  }\n\nmap(1:5, diff_means_func) |&gt; \n  list_rbind()\n\n# A tibble: 5 × 1\n  diff_mean\n      &lt;dbl&gt;\n1    0.374 \n2   -0.0909\n3    0.0952\n4    0.281 \n5   -0.287 \n\n\n\n5.3.2.4 Observed vs. Permuted statistic\n\n# observed\nmacnell |&gt; \n  group_by(taidgender) |&gt;\n  summarize(pmeans = mean(overall, na.rm=TRUE)) |&gt;\n  summarize(diff_mean = diff(pmeans))\n\n# A tibble: 1 × 1\n  diff_mean\n      &lt;dbl&gt;\n1     0.474\n\n\n\n# permuted\nset.seed(47)\nreps = 1000\nperm_diff_means &lt;- map(1:reps, diff_means_func) |&gt; list_rbind()\n\n\n5.3.2.5 permutation sampling distribution:\n.pull-left[\n\n\n\n\n\n\n\n\n]\n.pull-right[\n\n# permutation p-value\nperm_diff_means |&gt;\n  summarize(p_val = \n      sum(diff_mean &gt; 0.474) / \n      reps)\n\n# A tibble: 1 × 1\n  p_val\n  &lt;dbl&gt;\n1 0.048\n\n\n]\n\n5.3.2.6 Actual MacNell results\n\n\n\n\n\n\n\n\n\n5.3.3 Income and Health (F-like test)\nConsider the NHANES dataset.\n\nIncome\n\n(HHIncomeMid - Numerical version of HHIncome derived from the middle income in each category)\n\n\nHealth\n\n(HealthGen - Self-reported rating of participant’s health in general Reported for participants aged 12 years or older. One of Excellent, Vgood, Good, Fair, or Poor.)\n\n\n\n\n5.3.3.1 Summary of the variables of interest\n\nNHANES |&gt; select(HealthGen) |&gt; table()\n\nHealthGen\nExcellent     Vgood      Good      Fair      Poor \n      878      2508      2956      1010       187 \n\nNHANES |&gt; select(HHIncomeMid) |&gt; summary()\n\n  HHIncomeMid    \n Min.   :  2500  \n 1st Qu.: 30000  \n Median : 50000  \n Mean   : 57206  \n 3rd Qu.: 87500  \n Max.   :100000  \n NA's   :811     \n\n\n\n5.3.3.2 Mean Income broken down by Health\n\nNH.means &lt;- NHANES |&gt; \n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) |&gt; \n  group_by(HealthGen) |&gt; \n  summarize(IncMean = mean(HHIncomeMid, na.rm=TRUE), count=n())\nNH.means\n\n# A tibble: 5 × 3\n  HealthGen IncMean count\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Excellent  69354.   817\n2 Vgood      65011.  2342\n3 Good       55662.  2744\n4 Fair       44194.   899\n5 Poor       37027.   164\n\n\nAre the differences in means simply due to random chance??\n\nNHANES |&gt; filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) |&gt; \nggplot(aes(x=HealthGen, y=HHIncomeMid)) + \n  geom_boxplot() + \n  geom_jitter(width=0.1, alpha=.2)\n\n\n\n\n\n\n\nThe differences in health, can be calculated directly, but we still don’t know if the differences are due to randome chance or some other larger structure.\n\n\n          Excellent  Vgood   Good  Fair  Poor\nExcellent         0   4344  13692 25161 32327\nVgood         -4344      0   9348 20817 27983\nGood         -13692  -9348      0 11469 18635\nFair         -25161 -20817 -11469     0  7166\nPoor         -32327 -27983 -18635 -7166     0\n\n\n\n5.3.3.3 Overall difference\nWe can measure the overall differences as the amount of variability between each of the means and the overall mean:\n\\[F = \\frac{\\text{between-group variability}}{\\text{within-group variability}}\\] \\[F = \\frac{\\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{i\\cdot})^2/(N-K)}\\] \\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2\\]\n\n5.3.3.4 Creating a test statistic\n\nNHANES |&gt; select(HHIncomeMid, HealthGen) |&gt; \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid))\n\n# A tibble: 6,966 × 2\n   HHIncomeMid HealthGen\n         &lt;int&gt; &lt;fct&gt;    \n 1       30000 Good     \n 2       30000 Good     \n 3       30000 Good     \n 4       40000 Good     \n 5       87500 Vgood    \n 6       87500 Vgood    \n 7       87500 Vgood    \n 8       30000 Vgood    \n 9      100000 Vgood    \n10       70000 Fair     \n# ℹ 6,956 more rows\n\n\n\nGM &lt;- mean(NHANES$HHIncomeMid, na.rm=TRUE)\n\nGM\n\n[1] 57206\n\nNH.means\n\n# A tibble: 5 × 3\n  HealthGen IncMean count\n  &lt;fct&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Excellent  69354.   817\n2 Vgood      65011.  2342\n3 Good       55662.  2744\n4 Fair       44194.   899\n5 Poor       37027.   164\n\n\n\nNH.means$IncMean - GM\n\n[1]  12148   7805  -1544 -13013 -20179\n\n(NH.means$IncMean - GM)^2\n\n[1] 1.48e+08 6.09e+07 2.38e+06 1.69e+08 4.07e+08\n\nNH.means$count\n\n[1]  817 2342 2744  899  164\n\nNH.means$count * (NH.means$IncMean - GM)^2\n\n[1] 1.21e+11 1.43e+11 6.54e+09 1.52e+11 6.68e+10\n\n\n\\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2\\]\n\nsum(NH.means |&gt; select(count) |&gt; pull() * \n      (NH.means |&gt; select(IncMean) |&gt; pull() - GM)^2)\n\n[1] 4.89e+11\n\n\n\n5.3.3.5 Permuting the data\n\nNHANES |&gt; \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) |&gt;\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) |&gt;\n  select(HealthGen, HHIncomeMid, IncomePerm) \n\n# A tibble: 6,966 × 3\n   HealthGen HHIncomeMid IncomePerm\n   &lt;fct&gt;           &lt;int&gt;      &lt;int&gt;\n 1 Good            30000      87500\n 2 Good            30000      70000\n 3 Good            30000     100000\n 4 Good            40000      40000\n 5 Vgood           87500      87500\n 6 Vgood           87500     100000\n 7 Vgood           87500      50000\n 8 Vgood           30000      50000\n 9 Vgood          100000      70000\n10 Fair            70000      22500\n# ℹ 6,956 more rows\n\n\n\n5.3.3.6 Permuting the data & a new test statistic\n\nNHANES |&gt; \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) |&gt;\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) |&gt;\n  group_by(HealthGen) |&gt; \n  summarize(IncMeanP = mean(IncomePerm), count=n()) |&gt;\n  summarize(teststat = sum(count*(IncMeanP - GM)^2))\n\n# A tibble: 1 × 1\n      teststat\n         &lt;dbl&gt;\n1 14268151317.\n\n\n\n5.3.3.7 Lots of times…\n\nreps &lt;- 1000\n\nSSB_perm_func &lt;- function(.x){\n  NHANES |&gt; \n        filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) |&gt;\n        mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) |&gt;\n        group_by(HealthGen) |&gt; \n        summarize(IncMeanP = mean(IncomePerm), count=n()) |&gt;\n        summarize(teststat = sum(count*(IncMeanP - GM)^2)) \n}\n\nSSB_perm_val &lt;- map(1:reps, SSB_perm_func) |&gt; list_rbind()\n\nSSB_perm_val\n\n# A tibble: 1,000 × 1\n       teststat\n          &lt;dbl&gt;\n 1 11839838857.\n 2 14805138617.\n 3 12238328218.\n 4 14493898296.\n 5 19052560418.\n 6 14099580957.\n 7 19808304723.\n 8 14972708855.\n 9 15543404291.\n10 18334398022.\n# ℹ 990 more rows\n\n\n\n5.3.3.8 Compared to the real data\n\nSSB_obs &lt;- NHANES |&gt;\n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) |&gt; \n  group_by(HealthGen) |&gt; \n  summarize(IncMean = mean(HHIncomeMid), count=n()) |&gt;\n  summarize(obs_teststat = sum(count*(IncMean - GM)^2)) \n\nSSB_obs \n\n# A tibble: 1 × 1\n   obs_teststat\n          &lt;dbl&gt;\n1 488767088754.\n\nsum(SSB_perm_val |&gt; pull() &gt; SSB_obs |&gt; pull() ) / reps\n\n[1] 0\n\n\n\nSSB_perm_val |&gt;\n  ggplot(aes(x = teststat)) +\n  geom_histogram() + \n  geom_vline(data = SSB_obs, aes(xintercept = obs_teststat), color = \"red\") +\n  labs(y = \"\",\n       x = \"Permuted SSB Test Statistics\")\n\n\n\n\n\n\n\n\n\n\nKraj (2017)\nKraj (2017)\nMengel, Sauermann, and Zölitz (2019)\nMacNell, Driscoll, and Hunt (2015)\nMacNell, Driscoll, and Hunt (2015)\n\n\n\nKraj, Tori. 2017. “Research Suggests Students Are Biased Against Female Lecturers.” The Economist.\n\n\nMacNell, Lillian, Adam Driscoll, and Andrea Hunt. 2015. “What’s in a Name: Exposing Gender Bias in Student Ratings of Teaching.” Innovative Higher Education 40: 291–303.\n\n\nMengel, Friederike, Jan Sauermann, and Ulf Zölitz. 2019. “Gender Bias in Teaching Evaluations.” Journal of the European Economic Association 17: 535–66.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Permutation Tests</span>"
    ]
  },
  {
    "objectID": "04-simulating.html",
    "href": "04-simulating.html",
    "title": "4  Simulating",
    "section": "",
    "text": "4.1 Approximating probabilities\nSimulation is done to model a scenario which allows us to understand random behavior without actually replicating the entire study multiple times or trying to model the process analytically.\nFor example, what if you have a keen interest in understanding the probability of getting a single during room draw? Or getting a single on north campus? You wouldn’t actually run room draw thousands of times to find your probability of getting a single room. Similarly, the situation (e.g., room draw) may have too much information (e.g., all the different permutations of integers assigned to groups of 3 or 4 people) to model (easily) in a closed form solution. With a few moderate assumptions (proportion of students in groups of 1, 2, 3, 4; probability of choosing dorm X over dorm Y; random allocation of integers to students; etc.) it is straightforward to simulate the scenario thousands of time and measure the proportion of times your rank (47) will give you the room you want (single in Sontag).\nExample Consider the following problem from probability. Two points are selected randomly on a line of length \\(L\\) so as to be on opposite sides of the midpoint of the line. [In other words, the two points \\(X\\) and \\(Y\\) are independent random variables such that \\(X\\) is uniformly distributed over \\((0,L/2)\\) and \\(Y\\) is uniformly distributed over \\((L/2, 1)\\).] Find the probability that the 3 line segments from \\(0\\) to \\(X\\), from \\(X\\) to \\(Y\\), and from \\(Y\\) to \\(L\\) could be made to form the three sides of a triangle. (Note that three line segments can be made to form a triangle if the length of each of them is less than the sum of the lengths of the others.)\nThe joint density is: \\[ f(x,y) = \\begin{cases} \\frac{4}{L^2} & 0 \\le x \\le L/2, \\, L/2 \\le y \\le L \\\\ 0 & else \\end{cases} \\]\nThe three pieces have lengths: \\(X\\), \\(Y- X\\) and \\(L - Y\\). Three conditions need to be satisfied in order that the three pieces form a triangle:\n\\[\\begin{align}\nX + (Y- X) &&gt; (L - Y) \\Rightarrow Y &gt; L - Y \\Rightarrow 2 Y &gt; L \\Rightarrow Y &gt; L/2 \\\\\nX + (L-Y ) &&gt; Y - X \\Rightarrow 2X + L &gt; 2Y \\Rightarrow X + \\frac{L}{2} &gt; Y \\\\\nY + (L - Y) &&gt; X \\Rightarrow L &gt; X\n\\end{align}\\]\nThe first and third conditions are always satisfied, so we just need to find the probability that \\(Y\\) is below the line \\(X + \\frac{L}{2}\\). The density is the same as in the previous problem, so, as before, we just need to find the area of the region below the line that is within the square \\([0, L/2] \\times [L/2, L]\\), and then multiply it by \\(\\frac{4}{L^2}\\).\nArea = \\(\\displaystyle{ \\frac{1}{2}\\frac{L}{2}\\frac{L}{2} = \\frac{L^2}{8} }\\).\nThus, the probability is\n\\[\\begin{align}\n\\int_{area} f(x,y)dxdy = \\frac{4}{L^2} \\frac{L^2}{8} =  \\frac{1}{2}.\n\\end{align}\\]\nWhat happens for different values of \\(f(x,y)\\)? For example, if \\(x\\) and \\(y\\) have Beta(3,47) distributions on [0,.5] and [.5,1]? Simulating the probability in R is quite straightforward. What is the confidence bounds on the point estimates for the probabilities?? [n.b., we could simulate repeatedly to get a sense for the variability of our estimate!]\nsticks &lt;- function() {\n    pointx &lt;- runif(1,0,.5)  # runif is \"random uniform\", not \"run if\"\n    pointy &lt;- runif(1,.5,1)\n    l1 &lt;- pointx\n    l2 &lt;- pointy-pointx\n    l3 &lt;- 1 - pointy\n    max(l1,l2,l3) &gt; 1-max(l1,l2,l3)}\n    \nsum(replicate(100000, sticks())) / 100000\n\n[1] 0.5\nsticks_beta &lt;- function() {\n  pointx &lt;- rbeta(1,3, 47) / 2  # rbeta is random beta\n  pointy &lt;- (rbeta(1, 3, 47) +  1)/2\n  l1 &lt;- pointx\n  l2 &lt;- pointy-pointx\n  l3 &lt;- 1 - pointy\n  max(l1,l2,l3) &gt; 1-max(l1,l2,l3)}\n\nsum(replicate(100000, sticks_beta())) / 100000\n\n[1] 0.498\nExample Or consider the problem where the goal is to estimate \\(E(X)\\) where \\(X=\\max \\{ k: \\sum_{i=1}^k U_i &lt; 1 \\}\\) and \\(U_i\\) are uniform(0,1). The simulation problem is quite straightforward. Look carefully at the pieces. How are they broken down into steps? Notice that the steps go from inside out.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  },
  {
    "objectID": "04-simulating.html#simmodels",
    "href": "04-simulating.html#simmodels",
    "title": "4  Simulating",
    "section": "\n4.2 Understanding complicated models",
    "text": "4.2 Understanding complicated models\nConsider the following simulation where the top 10 GOP candidates get to participate in the debate, and the remaining 6 are kept out (example taken for debate on August 6, 2015). The write-up (and example) is a few years old, but the process is identical to the process used for deciding who is eligible for the 2020 Democratic debates for president. http://www.nytimes.com/interactive/2015/07/21/upshot/election-2015-the-first-gop-debate-and-the-role-of-chance.html?_r=0\n\nA candidate needed to get at least two percent support in four different polls published from a list of approved pollsters between June 28 and August 28, 2019, which cannot be based on open-ended questions and may cover either the national level or one of the first four primary/caucus states (Iowa, New Hampshire, Nevada, and South Carolina). Only one poll from each approved pollster counted towards meeting the criterion in each region. Wikipedia\n\n\n\n\n\n\n\n\nFor the 2016 election the Republican primary debates allowed only the top 10 candidates, ranked by national polls NYT\n\n\n\n\n4.2.1 Goals of simulating complicated models\nThe goal of simulating a complicated model is not only to create a program which will provide the desired results. We also hope to be able to code such that:\n\nThe problem is broken down into small pieces\nThe problem has checks in it to see what works (run the lines inside the if statements!)\nSimple code is best\n\n4.2.2 Simulating to assess sensitivity\nAs a second use of simulations, we can assess the sensitivity of parameters, model assumptions, sample size, etc. Ideally, the results will be summarized graphically, instead of as a table. A graphical representation can often provide insight into how parameters are related, whereas a table can be very hard to read.\n\n4.2.3 Simulating to assess bias in models\nThe example below is taken directly (and mostly verbatim) from a blog by Aaron Roth Algorithmic Unfairness Without Any Bias Baked In.\n\nBias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is “unfair”. Here is the (toy) scenario – the specifics aren’t important. High school students are applying to college, and each student has some innate “talent” \\(I\\), which we will imagine is normally distributed, with mean 100 and standard deviation 15: \\(I \\sim N(100,15)\\). The college would like to admit students who are sufficiently talented — say one standard deviation above the mean (so, it would like to admit students with \\(I \\geq 115\\)). The problem is that talent isn’t directly observable. Instead, the college can observe grades \\(g\\) and SAT scores \\(s\\), which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student’s talent level, and also with standard deviation 15: \\(g \\sim N(I, 15)\\), \\(s \\sim N(I, 15)\\).\n\n\nIn this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose predicted talent is at least 115. This is indeed “driven by math” – since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college.\n\nThe data\n\nOk. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population – the Blues only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above.\n\n\nBut there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.\n\n\n\n\n\n\n\n\n\nTwo separate models\n\nSo what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don’t see much difference:\n\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   33.1     0.153        217.       0\n2 SAT            0.335   0.00150      223.       0\n3 grades         0.334   0.00150      223.       0\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   24.3      1.65        14.7 1.95e- 44\n2 SAT            0.407    0.0168      24.2 3.36e-102\n3 grades         0.316    0.0151      21.0 3.05e- 81\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   33.0     0.152        218.       0\n2 SAT            0.335   0.00149      224.       0\n3 grades         0.335   0.00149      224.       0\n\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 6\n  color   tpr    fpr   fnr   fdr error\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 blue  0.518 0.0477 0.482 0.360 0.109\n2 red   0.505 0.0379 0.495 0.285 0.110\n\n\n\nThe Red classifier makes errors approximately 11.018% of the time. The Blue classifier does about the same — it makes errors about 10.9% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate!\n\n\nAnd since we are interested in fairness, lets think about the false negative rate of our classifiers. “False Negatives” in this setting are the people who are qualified to attend the college (\\(I &gt; 115\\)), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier’s mistakes. And the False Negative Rate is the probability that a randomly selected qualified person is mistakenly rejected from college — i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier’s mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness — sometimes referred to as “equal opportunity.”\n\n\nSo how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 48.227% on the blues, and the Red model has a false negative rate of 49.455% on the reds — so the difference between these two is a satisfyingly small 1.228%.\n\nOne global model\n\nBut you might reasonably object: because we have learned separate models for the Blues and the Reds, we are explicitly making admissions decisions as a function of a student’s color! This might sound like a form of discrimination, baked in by the algorithm designer — and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending.\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 6\n  color   tpr    fpr   fnr   fdr error\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 blue  0.617 0.0675 0.383 0.4   0.112\n2 red   0.504 0.0376 0.496 0.284 0.110\n\n\n\nSo what happens if we don’t allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 11.252%. Additionally, the Blues now have a higher false positive rate (people who don’t have talent about 115 are let in accidentally) and the Reds now have a higher false negative rate (people who do have talent are mistakenly kept out). This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population.\n\nWhat happened????\n\nWhat happened? There wasn’t any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population – which contributed much more to the average. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them – all in their favor.\n\n\nThis is the kind of thing that happens all the time: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, it is exacerbated if we artificially force the algorithm to be group blind. Well intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  },
  {
    "objectID": "04-simulating.html#simsens",
    "href": "04-simulating.html#simsens",
    "title": "4  Simulating",
    "section": "\n4.2 Simulating to Assess Sensitivity",
    "text": "4.2 Simulating to Assess Sensitivity\nAs a second use of simulations, we can assess the sensitivity of parameters, model assumptions, sample size, etc. Ideally, the results will be summarized graphically, instead of as a table. A graphical representation can often provide insight into how parameters are related, whereas a table can be very hard to read.\n\n4.2.1 Bias in Models\nThe example below is taken directly (and mostly verbatim) from a blog by Aaron Roth Algorithmic Unfairness Without Any Bias Baked In.\n\nBias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is “unfair”. Here is the (toy) scenario – the specifics aren’t important. High school students are applying to college, and each student has some innate “talent” \\(I\\), which we will imagine is normally distributed, with mean 100 and standard deviation 15: \\(I \\sim N(100,15)\\). The college would like to admit students who are sufficiently talented — say one standard deviation above the mean (so, it would like to admit students with \\(I \\geq 115\\)). The problem is that talent isn’t directly observable. Instead, the college can observe grades \\(g\\) and SAT scores \\(s\\), which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student’s talent level, and also with standard deviation 15: \\(g \\sim N(I, 15)\\), \\(s \\sim N(I, 15)\\).\n\n\nIn this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose predicted talent is at least 115. This is indeed “driven by math” – since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college.\n\nThe data\n\nOk. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population – the Blues only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above.\n\n\nBut there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds.\n\n\n\n\n\n\n\n\n\nTwo separate models\n\nSo what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don’t see much difference:\n\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   33.2     0.153        216.       0\n2 SAT            0.333   0.00151      220.       0\n3 grades         0.335   0.00151      222.       0\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   25.6      1.64        15.6 1.85e- 49\n2 SAT            0.429    0.0169      25.3 1.12e-109\n3 grades         0.280    0.0157      17.9 3.50e- 62\n\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   33.2     0.153        217.       0\n2 SAT            0.333   0.00151      221.       0\n3 grades         0.335   0.00150      223.       0\n\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 6\n  color   tpr    fpr   fnr   fdr error\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 blue  0.548 0.0367 0.452 0.267 0.101\n2 red   0.506 0.0379 0.494 0.284 0.111\n\n\n\nThe Red classifier makes errors approximately 11.053% of the time. The Blue classifier does about the same — it makes errors about 10.1% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate!\n\n\nAnd since we are interested in fairness, lets think about the false negative rate of our classifiers. “False Negatives” in this setting are the people who are qualified to attend the college (\\(I &gt; 115\\)), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier’s mistakes. And the False Negative Rate is the probability that a randomly selected qualified person is mistakenly rejected from college — i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier’s mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness — sometimes referred to as “equal opportunity.”\n\n\nSo how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 45.161% on the blues, and the Red model has a false negative rate of 49.413% on the reds — so the difference between these two is a satisfyingly small 4.252%.\n\nOne global model\n\nBut you might reasonably object: because we have learned separate models for the Blues and the Reds, we are explicitly making admissions decisions as a function of a student’s color! This might sound like a form of discrimination, baked in by the algorithm designer — and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending.\n\n\n\n\n\n\n\n\n\n# A tibble: 2 × 6\n  color   tpr    fpr   fnr   fdr error\n  &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 blue  0.632 0.0627 0.368 0.351 0.11 \n2 red   0.504 0.0377 0.496 0.283 0.111\n\n\n\nSo what happens if we don’t allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.791%. Additionally, the Blues now have a higher false positive rate (people who don’t have talent about 115 are let in accidentally) and the Reds now have a higher false negative rate (people who do have talent are mistakenly kept out). This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population.\n\nWhat happened????\n\nWhat happened? There wasn’t any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population – which contributed much more to the average. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them – all in their favor.\n\n\nThis is the kind of thing that happens all the time: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, it is exacerbated if we artificially force the algorithm to be group blind. Well intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time.\n\n\n4.2.2 Technical Conditions\nDefinitions\np-value is the probability of obtaining the observed data or more extreme given the null hypothesis is true.\n\\((1-\\alpha)100\\)% confidence interval is a range of values collected in such a way that repeated samples of data (using the same mechanism) would capture the parameter of interest in \\((1-\\alpha)100\\)% of the intervals.\nExamples\nEqual variance in the t-test Recall that one of the technical conditions for the t-test is that the two samples come from populations where the variance is equal (at least when var.equal=TRUE is specified). What happens if the null hypothesis is true (i.e., the means are equal!) but the technical conditions are violated (i.e., the variances are unequal)?\n\n4.2.2.0.1 t-test function (for use in map())\n\nt_test_pval &lt;- function(df){\n  t.test(y ~ x1, data = df, var.equal = TRUE) |&gt;\n    tidy() |&gt;\n    select(estimate, p.value) \n}\n\n\n4.2.2.0.2 generating data (equal variance)\n\nset.seed(470)\nreps &lt;- 1000\nn_obs &lt;- 20\nnull_data_equal &lt;- \n  data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,1), each = n()/2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\n\n4.2.2.0.3 summarize p-values\n(Note: we rejected 4.5% of the null tests, close to 5%.)\n\nnull_data_equal |&gt; \n  mutate(t_vals = map(data,t_test_pval)) |&gt;\n  select(t_vals) |&gt; \n  unnest(t_vals) |&gt;\n  ungroup(sim_id) |&gt;\n  summarize(type1error_rate = sum(p.value &lt; 0.05)/reps)\n\n# A tibble: 1 × 1\n  type1error_rate\n            &lt;dbl&gt;\n1           0.045\n\n\nUnequal variance in the t-test\n\n4.2.2.0.4 generating data (unequal variance)\n\nset.seed(470)\nreps &lt;- 1000\nn_obs &lt;- 20\nnull_data_unequal &lt;- \n  data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,100), each = n()/2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\n\n4.2.2.0.5 summarize p-values\n(Note, we rejected 5.7% of the null tests, not too bad!)\n\nnull_data_unequal |&gt; \n  mutate(t_vals = map(data,t_test_pval)) |&gt;\n  select(t_vals) |&gt; \n  unnest(t_vals) |&gt;\n  ungroup(sim_id) |&gt;\n  summarize(type1error_rate = sum(p.value &lt; 0.05)/reps)\n\n# A tibble: 1 × 1\n  type1error_rate\n            &lt;dbl&gt;\n1           0.057\n\n\nEqual variance in the linear model\nThe ISCAM applet by Beth Chance and Allan Rossman (Chance and Rossman 2018) demonstrates ideas of confidence intervals and what the analyst should expect with inferential assessment.\nConsider the following linear model with the points normally distributed with equal variance around the line. [Spoiler: when the technical conditions are met, the theory works out well. It turns out that the confidence interval will capture the true parameter in 95% of samples!]\n\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1)\\]\n\n\n\n\n\n\n\n\nDid we capture the true parameter in the CI? YES!\n\nCI &lt;- lm(y~x1+x2) |&gt; tidy(conf.int=TRUE) |&gt; data.frame()\nCI\n\n         term estimate std.error statistic  p.value conf.low conf.high\n1 (Intercept)   -0.950     0.148     -6.41 5.39e-09   -1.244    -0.656\n2          x1    0.259     0.210      1.23 2.20e-01   -0.158     0.677\n3          x2    1.401     0.194      7.21 1.24e-10    1.015     1.787\n\nCI |&gt;\n  filter(term == \"x2\") |&gt;\n  select(term, estimate, conf.low, conf.high) |&gt;\n  mutate(inside = between(beta2, conf.low, conf.high))\n\n  term estimate conf.low conf.high inside\n1   x2      1.4     1.02      1.79   TRUE\n\n\nWhat if we want to repeat lots of times…\nFUNCTION\n\nbeta_coef &lt;- function(df){\n  lm(y ~ x1 + x2, data = df) |&gt;\n    tidy(conf.int = TRUE) |&gt;\n    filter(term == \"x2\") |&gt;\n    select(estimate, conf.low, conf.high, p.value) \n}\n\nDATA\n\neqvar_data &lt;- data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, sd = 1)\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\neqvar_data\n\n# A tibble: 1,000 × 2\n# Groups:   sim_id [1,000]\n   sim_id data              \n    &lt;int&gt; &lt;list&gt;            \n 1      1 &lt;tibble [100 × 4]&gt;\n 2      2 &lt;tibble [100 × 4]&gt;\n 3      3 &lt;tibble [100 × 4]&gt;\n 4      4 &lt;tibble [100 × 4]&gt;\n 5      5 &lt;tibble [100 × 4]&gt;\n 6      6 &lt;tibble [100 × 4]&gt;\n 7      7 &lt;tibble [100 × 4]&gt;\n 8      8 &lt;tibble [100 × 4]&gt;\n 9      9 &lt;tibble [100 × 4]&gt;\n10     10 &lt;tibble [100 × 4]&gt;\n# ℹ 990 more rows\n\n\nMAPPING\nWe captured the true slope parameter in 95.5% of the confidence intervals (i.e., 95.5% of the datasets created confidence intervals that captured the true parameter).\n\neqvar_data |&gt; \n  mutate(b2_vals = map(data, beta_coef)) |&gt;\n  select(b2_vals) |&gt; \n  unnest(b2_vals) |&gt;\n  summarize(capture = between(beta2, conf.low, conf.high)) |&gt;\n  summarize(capture_rate = sum(capture)/reps)\n\n# A tibble: 1 × 1\n  capture_rate\n         &lt;dbl&gt;\n1        0.955\n\n\nUnequal variance in the linear model\nConsider the following linear model with the points normally distributed with unequal variance around the line. [Spoiler: when the technical conditions are met, the theory does not work out as well. It turns out that the confidence interval will not capture the true parameter in 95% of samples!]\n\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1+ X_1 + 10 \\cdot |X_2|)\\]\n\n\n\n\n\n\n\n\nWhat if we want to repeat lots of times…\nFUNCTION\n\nbeta_coef &lt;- function(df){\n  lm(y ~ x1 + x2, data = df) |&gt;\n    tidy(conf.int = TRUE) |&gt;\n    filter(term == \"x2\") |&gt;\n    select(estimate, conf.low, conf.high, p.value) \n}\n\nDATA\n\nuneqvar_data &lt;- data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, \n                                            sd = 1 + x1 + 10*abs(x2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\nuneqvar_data\n\n# A tibble: 1,000 × 2\n# Groups:   sim_id [1,000]\n   sim_id data              \n    &lt;int&gt; &lt;list&gt;            \n 1      1 &lt;tibble [100 × 4]&gt;\n 2      2 &lt;tibble [100 × 4]&gt;\n 3      3 &lt;tibble [100 × 4]&gt;\n 4      4 &lt;tibble [100 × 4]&gt;\n 5      5 &lt;tibble [100 × 4]&gt;\n 6      6 &lt;tibble [100 × 4]&gt;\n 7      7 &lt;tibble [100 × 4]&gt;\n 8      8 &lt;tibble [100 × 4]&gt;\n 9      9 &lt;tibble [100 × 4]&gt;\n10     10 &lt;tibble [100 × 4]&gt;\n# ℹ 990 more rows\n\n\nMAPPING\nUsing the data with unequal variability, we only captured the slope parameter about 88% of the time.\n\nuneqvar_data |&gt; \n  mutate(b2_vals = map(data, beta_coef)) |&gt;\n  select(b2_vals) |&gt; \n  unnest(b2_vals) |&gt;\n  summarize(capture = between(beta2, conf.low, conf.high)) |&gt;\n  summarize(capture_rate = sum(capture)/reps)\n\n# A tibble: 1 × 1\n  capture_rate\n         &lt;dbl&gt;\n1        0.861\n\n\n\n4.2.3 Generating random numbers\n\nYou are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation.\n\n\n4.2.3.1 How do we generate uniform[0,1] numbers?\nLCG - linear congruence generators. Set \\(a,b,m\\) to be large integers. The sequence of numbers \\(X_i / m\\) will pass all tests for uniformly distributed variables. \\[ X_{n+1} = (aX_n + b) \\mod m \\]\nwhere\n\n\n\\(m\\) and \\(b\\) are relatively prime,\n\n\\(a - 1\\) is divisible by all prime factors of \\(m\\),\n\n\\(a - 1\\) is divisible by 4 if \\(m\\) is divisible by 4.\n\n\na &lt;- 31541435235\nb &lt;- 23462146143 \nm &lt;- 423514351351\n\nxval &lt;- 47 \nreps &lt;- 10\nunif.val &lt;- c()\n\nfor(i in 1:reps){\n  xval &lt;- (a*xval + b) %% m\n  unif.val &lt;- c(unif.val, xval/m)   }\n\nupdate_rv &lt;- function(x){(a*x + b) %% m }\n\nrep(xval, reps) |&gt;\n  map(~ accumulate(., ~ ((a*.x + b) %% m))/m )\n\n[[1]]\n[1] 0.0584\n\n[[2]]\n[1] 0.0584\n\n[[3]]\n[1] 0.0584\n\n[[4]]\n[1] 0.0584\n\n[[5]]\n[1] 0.0584\n\n[[6]]\n[1] 0.0584\n\n[[7]]\n[1] 0.0584\n\n[[8]]\n[1] 0.0584\n\n[[9]]\n[1] 0.0584\n\n[[10]]\n[1] 0.0584\n\ndata.frame(uniformRVs = unif.val) |&gt;\n  ggplot(aes(x = uniformRVs)) + geom_histogram(bins = 25)\n\n\n\n\n\n\n\n\n4.2.4 Generating other RVs: The Inverse Transform Method\n\n\n\nYou are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation.\n\nContinuous RVs\nUse the inverse of the cumulative distribution function to generate data that come from a particular continuous distribution. For example, generate 100 random normal deviates. Start by assuming that \\(F\\) is a continuous and increasing function. Also assume that \\(F^{-1}\\) exists.\n\\[F(x) = P(X \\leq x)\\] Note that \\(F\\) is just the area function describing the density (histogram) of the data.\n\nAlgorithm: Generate Continuous RV\n\n\nGenerate a uniform random variable \\(U\\)\n\nSet \\(X = F^{-1}(U)\\)\n\n\n\nProof: that the algorithm above generates variables that come from the probability distribution represented by \\(F\\).\n\\[\\begin{align}\nP(X \\leq x) &= P(F^{-1}(U) \\leq x)\\\\\n&= P(U \\leq F(x))\\\\\n&= F(x)\\\\\n\\end{align}\\]\nExample: \\[ f(x) = \\begin{cases}\n2x e^{-x^2} & 0 &lt; x \\\\ 0 & x &lt; 0 \\end{cases}\\]\nNote: This is known as a Weibull(\\(\\lambda=1\\), \\(k=2\\)) distribution.\n\n\n\n\n\n\n\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\n\n\n\n\n\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\n\nWhat is \\(F(x)\\)? \\[ F(x) = \\int_0^x 2w e^{-w^2} dw = 1 - e^{-x^2}\\]\n\nWhat is \\(F^{-1}(u)\\)?\n\n\\[\\begin{align}\nu &= F(x)\\\\\n&= 1 - e^{-x^2}\\\\\n1-u &= e^{-x^2}\\\\\n-\\ln(1-u) &= x^2\\\\\n\\sqrt{-\\ln(1-u)} &= x\\\\\nF^{-1}(u) &= \\sqrt{-\\ln(1-u)}\n\\end{align}\\]\n\nSuppose you could simulate uniform random variables, \\(U_1, U_2, \\dots\\). How could you use these to simulate RV’s with the Weibull density, \\(f(x)\\), given above? \\[ \\mbox{Let: } X_i = \\sqrt{-\\ln(1-U_i)}\\]\n\n\n\nunifdata = runif(10000,0,1)\nweib1data = sqrt(-log(1-unifdata))\nweib2data = rweibull(10000,2,1)\n\nweibdata &lt;- data.frame(weibull = c(weib1data, weib2data),\n                       sim.method = c(rep(\"InvTrans\", 10000), \n                                      rep(\"rweibull\", 10000)))\n\nggplot(weibdata, aes(x = weibull)) + geom_histogram(bins = 25) + \n  facet_grid(~sim.method)\n\n\n\n\n\n\n\nDiscrete RVs\nA similar algorithm is used to generate data that come from a particular discrete distribution. For example, generate 100 random normal deviates. We start by assuming the probability mass function of \\(X\\) is \\[ P(X = x_i) = p_i, i=1, \\ldots, m\\]\n\nAlgorithm: Generate Discrete RV\n\n\nGenerate a uniform random variable \\(U\\)\n\nTransform \\(U\\) into \\(X\\) as follows, \\[X = x_j \\mbox{ if } \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\]\n\n\n\nProof: that the algorithm above generates variables that come from the probability mass function \\(\\{p_1, p_2, \\ldots, p_m\\}\\).\n\\[\\begin{align}\nP(X = x_j) &= \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\\\\n&= \\sum_{i=1}^j p_i - \\sum_{i=1}^{j-1} p_i\\\\\n&= p_j\\\\\n\\end{align}\\]\nWhat if you don’t know \\(F\\)? Or can’t calculate \\(F^{-1}\\)?\nIn the case that the CDF cannot be calculated explicitly (the normal for example), one could still use this methodology by estimating F at a collection of points \\(x_i, u_i = F(x_i)\\). Now we temporarily mimic the discrete inverse transform, as we generate a \\(U\\) and see which subinterval it falls in, i.e. \\(u_i \\leq U \\leq u_{i+1}\\). Assuming the \\(x_i\\) are close enough, we expect the CDF to be approximately linear on this subinterval, so then we take a linear interpolation of the CDF on the subinterval to get \\(X\\) via\n\\[\\begin{align}\nX = \\frac{u_{i+1} -  U}{u_{i+1} - u_i} x_i + \\frac{U - u_i}{u_{i+1} - u_i} x_j\n\\end{align}\\]\nHowever, the linear interpolation requires a complete approximation of \\(F(x)\\), regardless of the sample size desired, and doesn’t generalize to higher dimensions, and of course only gives you something with the approximate distribution back, even if you have your hands on real uniform random variables.\n\n\n\nFor the 2016 election the Republican primary debates allowed only the top 10 candidates, ranked by national polls NYT\nFrom Advanced R by Wickham. https://adv-r.hadley.nz/functionals.html\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\nChance, Beth, and Allan Rossman. 2018. Investigating Statistics, Concepts, Applications, and Methods. 3rd ed. http://www.rossmanchance.com/iscam3/.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html",
    "href": "03-wrangling.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "3.1 Coding Style\nAn important aspect of the data wrangling tools in this chapter includes best practices for writing code. The tidyverse style of coding allows you to implement code that is clean and communicates ideas easily. The more your code conforms to a style, the easier it will be to debug, and the fewer mistakes you will make. Understanding the rules of programming discourse is part of what makes excellent coders. (Soloway and Ehrlich 1984)",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#datastruc",
    "href": "03-wrangling.html#datastruc",
    "title": "3  Data Wrangling",
    "section": "\n3.2 Structure of Data",
    "text": "3.2 Structure of Data\nFor plotting, analyses, model building, etc., it’s important that the data be structured in a very particular way. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham (2014).\n\n\nTidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and *every} column is a variable. No exceptions.\nCreating tidy data is not trivial. We work with objects (often data tables), functions, and arguments (often variables).\n\nThe Active Duty data are not tidy! What are the cases? How are the data not tidy? What might the data look like in tidy form? Suppose that the case was “an individual in the armed forces.” What variables would you use to capture the information in the following table?\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794\n\nProblem: totals and different sheets\nBetter for R: longer format with columns - grade, gender, status, service, count (case is still the total pay grade)\nCase is individual (?): grade, gender, status, service (no count because each row does the counting)\n\n3.2.1 Building Tidy Data\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), we need to understand how to build data using the patterns of the language. Some things to consider:\n\n\nobject_name &lt;- function_name(arguments) is a way of using a function to create a new object.\n\nobject_name &lt;- data_table |&gt; function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of |&gt; becomes the first argument to the function on the right side.\n\n\nobject_name &lt;- data_table |&gt;\n  function_name(arguments) |&gt; \n  function_name(arguments)\n\nis extended chaining. |&gt; is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of |&gt; is always a data table. * The pipe syntax should be read as then, |&gt;.\n\n3.2.2 Examples of Chaining\nThe pipe syntax (|&gt;) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example:\nx |&gt; f() is the same as f(x)\nx |&gt; f(y) is the same as f(x, y)\n\n3.2.2.1 Little Bunny Foo Foo\nFrom Hadley Wickham, how to think about tidy data.\n\nLittle bunny Foo Foo Went hopping through the forest Scooping up the field mice And bopping them on the head\n\nThe nursery rhyme could be created by a series of steps where the output from each step is saved as an object along the way.\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\nAnother approach is to concatenate the functions so that there is only one output.\nbop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)\nOr even worse, as one line:\nbop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))\nInstead, the code can be written using the pipe in the order in which the function is evaluated:\nfoo_foo |&gt;\n   hop(through = forest) |&gt;\n       scoop(up = field_mice) |&gt;\n           bop(on = head)\nbabynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. From 2014 until 2017, Social Security Administration’s website showed the names Emma and Olivia leading for girls, Noah and Liam for boys.\nThe babynames data table in the babynames package comes from the Social Security Administration’s listing of the names givens to babies in each year, and the number of babies of each sex given that name. (Only names with 5 or more babies are published by the SSA.)\n\n3.2.3 Data Verbs (on single data frames)\n\n\n\n\n\n\nImportant\n\n\n\nSuper great resource: The Posit dplyr cheat sheet\n\n\nData verbs take data tables as input and give data tables as output (that’s how we can use the chaining syntax!). We will use the R package dplyr to do much of our data wrangling. Below is a list of verbs which will be helpful in wrangling many different types of data. See the Data Wrangling cheat from Posit for additional help.\n\nsample_n() take a random row(s)\nhead() grab the first few rows\ntail() grab the last few rows\nfilter() removes unwanted cases\narrange() reorders the cases\nselect() removes unwanted variables (and rename() )\ndistinct() returns the unique values in a table\nmutate() transforms the variable (and transmute() like mutate, returns only new variables)\ngroup_by() tells R that SUCCESSIVE functions keep in mind that there are groups of items. So group_by() only makes sense with verbs later on (like summarize()).\n\nsummarize() collapses a data frame to a single row. Some functions that are used within summarize() include:\n\n\nmin(), max(), mean(), sum(), sd(), median(), and IQR()\n\n\nn(): number of observations in the current group\n\nn_distinct(x): count the number of unique values in x\n\n\nfirst_value(x), last_value(x) and nth_value(x, n): work similarly to x[1], x[length(x)], and x[n]",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#r-examples-basic-verbs",
    "href": "03-wrangling.html#r-examples-basic-verbs",
    "title": "3  Data Wrangling",
    "section": "\n3.3 R examples, basic verbs",
    "text": "3.3 R examples, basic verbs\n\n3.3.1 Datasets\nThe starwars dataframe is from dplyr , although the data are originally from SWAPI, the Star Wars API.\nNHANES From ?NHANES: NHANES is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC).\nbabynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. From 2014 until 2017, Social Security Administration’s website showed the names Emma and Olivia leading for girls, Noah and Liam for boys. (Only names with 5 or more babies are published by the SSA.)\n\n3.3.2 Examples of Chaining\n\nlibrary(babynames)\nbabynames |&gt; nrow()\n\n[1] 1924665\n\nbabynames |&gt; names()\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\nbabynames |&gt; glimpse()\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\nbabynames |&gt; head()\n\n# A tibble: 6 × 5\n   year sex   name          n   prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n1  1880 F     Mary       7065 0.0724\n2  1880 F     Anna       2604 0.0267\n3  1880 F     Emma       2003 0.0205\n4  1880 F     Elizabeth  1939 0.0199\n5  1880 F     Minnie     1746 0.0179\n6  1880 F     Margaret   1578 0.0162\n\nbabynames |&gt; tail()\n\n# A tibble: 6 × 5\n   year sex   name       n       prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1  2017 M     Zyhier     5 0.00000255\n2  2017 M     Zykai      5 0.00000255\n3  2017 M     Zykeem     5 0.00000255\n4  2017 M     Zylin      5 0.00000255\n5  2017 M     Zylis      5 0.00000255\n6  2017 M     Zyrie      5 0.00000255\n\nbabynames |&gt; sample_n(size=5)\n\n# A tibble: 5 × 5\n   year sex   name        n      prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;     &lt;dbl&gt;\n1  2002 F     Diya       92 0.0000466\n2  1916 M     Obediah     6 0.0000065\n3  1918 M     Clare      86 0.0000820\n4  2012 F     Nahiara     6 0.0000031\n5  2007 F     Rylynn     57 0.0000270\n\n\n\n3.3.3 Data Verbs\nTaken from the dplyr tutorial\n\n3.3.3.1 Starwars\n\nlibrary(dplyr)\n\nstarwars |&gt; dim()\n\n[1] 87 14\n\nstarwars |&gt; names()\n\n [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n[11] \"species\"    \"films\"      \"vehicles\"   \"starships\" \n\nstarwars |&gt; head()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n4 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n5 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n6 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars |&gt; \n  dplyr::filter(species == \"Droid\")\n\n# A tibble: 6 × 14\n  name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n1 C-3PO     167    75 &lt;NA&gt;       gold        yellow           112 none  masculi…\n2 R2-D2      96    32 &lt;NA&gt;       white, blue red               33 none  masculi…\n3 R5-D4      97    32 &lt;NA&gt;       white, red  red               NA none  masculi…\n4 IG-88     200   140 none       metal       red               15 none  masculi…\n5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n6 BB8        NA    NA none       none        black             NA none  masculi…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars |&gt; \n  dplyr::select(name, ends_with(\"color\"))\n\n# A tibble: 87 × 4\n   name               hair_color    skin_color  eye_color\n   &lt;chr&gt;              &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 Luke Skywalker     blond         fair        blue     \n 2 C-3PO              &lt;NA&gt;          gold        yellow   \n 3 R2-D2              &lt;NA&gt;          white, blue red      \n 4 Darth Vader        none          white       yellow   \n 5 Leia Organa        brown         light       brown    \n 6 Owen Lars          brown, grey   light       blue     \n 7 Beru Whitesun Lars brown         light       blue     \n 8 R5-D4              &lt;NA&gt;          white, red  red      \n 9 Biggs Darklighter  black         light       brown    \n10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n# ℹ 77 more rows\n\nstarwars |&gt; \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) |&gt;\n  dplyr::select(name:mass, bmi)\n\n# A tibble: 87 × 4\n   name               height  mass   bmi\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Luke Skywalker        172    77  26.0\n 2 C-3PO                 167    75  26.9\n 3 R2-D2                  96    32  34.7\n 4 Darth Vader           202   136  33.3\n 5 Leia Organa           150    49  21.8\n 6 Owen Lars             178   120  37.9\n 7 Beru Whitesun Lars    165    75  27.5\n 8 R5-D4                  97    32  34.0\n 9 Biggs Darklighter     183    84  25.1\n10 Obi-Wan Kenobi        182    77  23.2\n# ℹ 77 more rows\n\nstarwars |&gt; \n  dplyr::arrange(desc(mass))\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Jabba D…    175  1358 &lt;NA&gt;       green-tan… orange         600   herm… mascu…\n 2 Grievous    216   159 none       brown, wh… green, y…       NA   male  mascu…\n 3 IG-88       200   140 none       metal      red             15   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Tarfful     234   136 brown      brown      blue            NA   male  mascu…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Bossk       190   113 none       green      red             53   male  mascu…\n 8 Chewbac…    228   112 brown      unknown    blue           200   male  mascu…\n 9 Jek Ton…    180   110 brown      fair       blue            NA   &lt;NA&gt;  &lt;NA&gt;  \n10 Dexter …    198   102 none       brown      yellow          NA   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars |&gt;\n  dplyr::group_by(species) |&gt;\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) |&gt;\n  dplyr::filter(num &gt; 1)\n\n# A tibble: 9 × 3\n  species    num  mass\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Droid        6  69.8\n2 Gungan       3  74  \n3 Human       35  81.3\n4 Kaminoan     2  88  \n5 Mirialan     2  53.1\n6 Twi'lek      2  55  \n7 Wookiee      2 124  \n8 Zabrak       2  80  \n9 &lt;NA&gt;         4  81  \n\n\n\n3.3.3.2 NHANES\n\nlibrary(NHANES)\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n# find the sleep variables\nNHANESsleep &lt;- NHANES |&gt; \n  select(Gender, Age, Weight, Race1, Race3,\n         Education, SleepTrouble, SleepHrsNight,\n         TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)\n\n [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"   \n\ndim(NHANESsleep)\n\n[1] 10000    11\n\n# subset for college students\nNHANESsleep &lt;- NHANESsleep |&gt; \n  filter(Age %in% c(18:22)) |&gt; \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)\n\n [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"     \n\ndim(NHANESsleep)\n\n[1] 655  12\n\nNHANESsleep |&gt; \n  ggplot(aes(x = Age, y = SleepHrsNight, color = Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) \n\n\n\n\n\n\n\n\n3.3.4 summarize and group_by\n\n\n# number of people (cases) in NHANES\nNHANES |&gt; summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1 10000\n\n# total weight of all the people in NHANES (silly)\nNHANES |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(sum(Weightlb, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `sum(Weightlb, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                      1549419.\n\n# mean weight of all the people in NHANES\nNHANES |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(Weightlb, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                           156.\n\n# repeat the above but for groups\n\n# males versus females\nNHANES |&gt; \n  group_by(Gender) |&gt; \n  summarize(n())\n\n# A tibble: 2 × 2\n  Gender `n()`\n  &lt;fct&gt;  &lt;int&gt;\n1 female  5020\n2 male    4980\n\nNHANES |&gt; \n  group_by(Gender) |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  Gender `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                           &lt;dbl&gt;\n1 female                           146.\n2 male                             167.\n\n# smokers and non-smokers\nNHANES |&gt; \n  group_by(SmokeNow) |&gt; \n  summarize(n())\n\n# A tibble: 3 × 2\n  SmokeNow `n()`\n  &lt;fct&gt;    &lt;int&gt;\n1 No        1745\n2 Yes       1466\n3 &lt;NA&gt;      6789\n\nNHANES |&gt; \n  group_by(SmokeNow) |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  SmokeNow `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 No                                 186.\n2 Yes                                177.\n3 &lt;NA&gt;                               144.\n\n# people with and without diabetes\nNHANES |&gt; \n  group_by(Diabetes) |&gt; \n  summarize(n())\n\n# A tibble: 3 × 2\n  Diabetes `n()`\n  &lt;fct&gt;    &lt;int&gt;\n1 No        9098\n2 Yes        760\n3 &lt;NA&gt;       142\n\nNHANES |&gt; \n  group_by(Diabetes) |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  Diabetes `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 No                                155. \n2 Yes                               202. \n3 &lt;NA&gt;                               21.6\n\n# break down the smokers versus non-smokers further, by sex\nNHANES |&gt; \n  group_by(SmokeNow, Gender) |&gt; \n  summarize(n())\n\n# A tibble: 6 × 3\n# Groups:   SmokeNow [3]\n  SmokeNow Gender `n()`\n  &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt;\n1 No       female   764\n2 No       male     981\n3 Yes      female   638\n4 Yes      male     828\n5 &lt;NA&gt;     female  3618\n6 &lt;NA&gt;     male    3171\n\nNHANES |&gt; \n  group_by(SmokeNow, Gender) |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 6 × 3\n# Groups:   SmokeNow [3]\n  SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;    &lt;fct&gt;                           &lt;dbl&gt;\n1 No       female                           167.\n2 No       male                             201.\n3 Yes      female                           167.\n4 Yes      male                             185.\n5 &lt;NA&gt;     female                           138.\n6 &lt;NA&gt;     male                             151.\n\n# break down the people with diabetes further, by smoking\nNHANES |&gt; \n  group_by(Diabetes, SmokeNow) |&gt; \n  summarize(n())\n\n# A tibble: 8 × 3\n# Groups:   Diabetes [3]\n  Diabetes SmokeNow `n()`\n  &lt;fct&gt;    &lt;fct&gt;    &lt;int&gt;\n1 No       No        1476\n2 No       Yes       1360\n3 No       &lt;NA&gt;      6262\n4 Yes      No         267\n5 Yes      Yes        106\n6 Yes      &lt;NA&gt;       387\n7 &lt;NA&gt;     No           2\n8 &lt;NA&gt;     &lt;NA&gt;       140\n\nNHANES |&gt; \n  group_by(Diabetes, SmokeNow) |&gt; \n  mutate(Weightlb = Weight*2.2) |&gt; \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 8 × 3\n# Groups:   Diabetes [3]\n  Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;    &lt;fct&gt;                             &lt;dbl&gt;\n1 No       No                                183. \n2 No       Yes                               175. \n3 No       &lt;NA&gt;                              143. \n4 Yes      No                                204. \n5 Yes      Yes                               204. \n6 Yes      &lt;NA&gt;                              199. \n7 &lt;NA&gt;     No                                193. \n8 &lt;NA&gt;     &lt;NA&gt;                               19.1\n\n\n\n3.3.5 babynames\n\nbabynames |&gt; group_by(sex) |&gt;\n  summarize(total=sum(n))\n\n# A tibble: 2 × 2\n  sex       total\n  &lt;chr&gt;     &lt;int&gt;\n1 F     172371079\n2 M     175749438\n\nbabynames |&gt; \n  group_by(year, sex) |&gt;\n  summarize(name_count = n_distinct(name)) |&gt; \n  head()\n\n# A tibble: 6 × 3\n# Groups:   year [3]\n   year sex   name_count\n  &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;\n1  1880 F            942\n2  1880 M           1058\n3  1881 F            938\n4  1881 M            997\n5  1882 F           1028\n6  1882 M           1099\n\nbabynames |&gt; \n  group_by(year, sex) |&gt;\n  summarize(name_count = n_distinct(name)) |&gt; \n  tail()\n\n# A tibble: 6 × 3\n# Groups:   year [3]\n   year sex   name_count\n  &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;\n1  2015 F          19074\n2  2015 M          14024\n3  2016 F          18817\n4  2016 M          14162\n5  2017 F          18309\n6  2017 M          14160\n\nbabysamp &lt;- babynames |&gt; \n  sample_n(size=50)\nbabysamp |&gt; \n  select(year) |&gt; \n  distinct() |&gt; \n  table()\n\nyear\n1884 1885 1887 1893 1896 1905 1926 1928 1935 1936 1943 1944 1946 1950 1957 1964 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1967 1970 1971 1973 1974 1975 1978 1980 1982 1984 1987 1988 1990 1993 2001 2002 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n2003 2005 2008 2009 2010 2011 2012 2013 \n   1    1    1    1    1    1    1    1 \n\nbabysamp |&gt; \n  distinct() |&gt; \n  select(year) |&gt; \n  table()\n\nyear\n1884 1885 1887 1893 1896 1905 1926 1928 1935 1936 1943 1944 1946 1950 1957 1964 \n   1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    2 \n1967 1970 1971 1973 1974 1975 1978 1980 1982 1984 1987 1988 1990 1993 2001 2002 \n   1    1    1    1    2    1    1    2    2    1    1    2    1    1    1    1 \n2003 2005 2008 2009 2010 2011 2012 2013 \n   1    1    2    1    2    1    3    1 \n\nFrances &lt;- babynames |&gt;\n  filter(name== \"Frances\") |&gt;\n  group_by(year, sex) |&gt;\n  summarize(yrTot = sum(n))\n\nFrances |&gt; \n  ggplot(aes(x = year, y = yrTot)) +\n  geom_point(aes(color = sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  labs(y = \"Yearly total on log10 scale\",\n       title = \"Number of babies named Frances, over time\")",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#highverb",
    "href": "03-wrangling.html#highverb",
    "title": "3  Data Wrangling",
    "section": "\n3.4 Higher Level Data Verbs",
    "text": "3.4 Higher Level Data Verbs\nThere are more complicated verbs which may be important for more sophisticated analyses. See the RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.\n\n\npivot_longer makes many columns into 2 columns: pivot_longer(data, cols,  names_to = , value_to = )\n\n\npivot_wider makes one column into multiple columns: pivot_wider(data, names_from = , values_from = )\n\n\nleft_join returns all rows from the left table, and any rows with matching keys from the right table.\n\ninner_join returns only the rows in which the left table have matching keys in the right table (i.e., matching rows in both sets).\n\nfull_join returns all rows from both tables, join records from the left which have matching keys in the right table.\n\nGood practice: always specify the by argument when joining data frames.\n\n\nIf you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#r-examples-higher-level-verbs",
    "href": "03-wrangling.html#r-examples-higher-level-verbs",
    "title": "3  Data Wrangling",
    "section": "\n3.5 R examples, higher level verbs",
    "text": "3.5 R examples, higher level verbs\n\ntidyr 1.0.0 has just been released! The new release means that you need to update tidyr. You will know if you have the latest version if the following command works in the console (window below):\n?tidyr::pivot_longer\nIf you are familiar with spread and gather, you should acquaint yourself with pivot_longer() and pivot_wider(). The idea is to go from very wide dataframes to very long dataframes and vice versa.\n\n3.5.1 pivot_longer()\n\npivot the military pay grade to become longer?\n\n\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794\n\n\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\n\n\nglimpse(navy_gs)\n\nRows: 38\nColumns: 17\n$ ...1                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Active Duty Family` &lt;chr&gt; NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n$ ...3                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n$ ...4                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n$ ...5                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n$ ...6                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n$ ...7                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n$ ...8                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n$ ...9                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n$ ...10                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n$ ...11                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n$ ...12                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n$ ...13                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n$ ...14                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n$ ...15                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n$ ...16                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n$ ...17                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\n\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)\n\nRows: 30\nColumns: 16\n$ pay.grade       &lt;chr&gt; \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n$ male.sing.wo    &lt;dbl&gt; 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n$ female.sing.wo  &lt;dbl&gt; 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n$ tot.sing.wo     &lt;dbl&gt; 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n$ male.sing.w     &lt;dbl&gt; 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n$ female.sing.w   &lt;dbl&gt; 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n$ tot.sing.w      &lt;dbl&gt; 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n$ male.joint.NA   &lt;dbl&gt; 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n$ female.joint.NA &lt;dbl&gt; 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n$ tot.joint.NA    &lt;dbl&gt; 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n$ male.civ.NA     &lt;dbl&gt; 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n$ female.civ.NA   &lt;dbl&gt; 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n$ tot.civ.NA      &lt;dbl&gt; 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n$ male.tot.NA     &lt;dbl&gt; 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n$ female.tot.NA   &lt;dbl&gt; 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n$ tot.tot.NA      &lt;dbl&gt; 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n\n# get rid of total columns & rows:\n\nnavyWR = navy |&gt; \n  select(-contains(\"tot\")) |&gt;\n  filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n            substr(pay.grade, 1, 5) != \"GRAND\" ) |&gt;\n  pivot_longer(-pay.grade,\n                values_to = \"numPeople\",\n                names_to = \"status\") |&gt;\n  separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR |&gt; head()\n\n# A tibble: 6 × 5\n  pay.grade sex    marital kids  numPeople\n  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 E-1       male   sing    wo        31229\n2 E-1       female sing    wo         5717\n3 E-1       male   sing    w           563\n4 E-1       female sing    w           122\n5 E-1       male   joint   NA          139\n6 E-1       female joint   NA          141\n\n\nDoes a graph tell us if we did it right? what if we had done it wrong…?\n\nnavyWR |&gt;\n  ggplot(aes(x = pay.grade, y = numPeople, color = sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))\n\n\n\n\n\n\n\n\n3.5.2 pivot_wider\n\n\nlibrary(babynames)\nbabynames |&gt; \n  dplyr::select(-prop) |&gt;\n  tidyr::pivot_wider(names_from = sex, values_from = n) \n\n# A tibble: 1,756,284 × 4\n    year name          F     M\n   &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1  1880 Mary       7065    27\n 2  1880 Anna       2604    12\n 3  1880 Emma       2003    10\n 4  1880 Elizabeth  1939     9\n 5  1880 Minnie     1746     9\n 6  1880 Margaret   1578    NA\n 7  1880 Ida        1472     8\n 8  1880 Alice      1414    NA\n 9  1880 Bertha     1320    NA\n10  1880 Sarah      1288    NA\n# ℹ 1,756,274 more rows\n\n\n\nbabynames |&gt; \n  select(-prop) |&gt; \n  pivot_wider(names_from = sex, values_from = n) |&gt;\n  filter(!is.na(F) & !is.na(M)) |&gt;\n  arrange(desc(year), desc(M))\n\n# A tibble: 168,381 × 4\n    year name         F     M\n   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt;\n 1  2017 Liam        36 18728\n 2  2017 Noah       170 18326\n 3  2017 William     18 14904\n 4  2017 James       77 14232\n 5  2017 Logan     1103 13974\n 6  2017 Benjamin     8 13733\n 7  2017 Mason       58 13502\n 8  2017 Elijah      26 13268\n 9  2017 Oliver      15 13141\n10  2017 Jacob       16 13106\n# ℹ 168,371 more rows\n\n\n\nbabynames |&gt; \n  pivot_wider(names_from = sex, values_from = n) |&gt;\n  filter(!is.na(F) & !is.na(M)) |&gt;\n  arrange(desc(prop))\n\n# A tibble: 12 × 5\n    year name            prop     F     M\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1  1986 Marquette 0.0000130     24    25\n 2  1996 Dariel    0.0000115     22    23\n 3  2014 Laramie   0.0000108     21    22\n 4  1939 Earnie    0.00000882    10    10\n 5  1939 Vertis    0.00000882    10    10\n 6  1921 Vernis    0.00000703     9     8\n 7  1939 Alvia     0.00000529     6     6\n 8  1939 Eudell    0.00000529     6     6\n 9  1939 Ladell    0.00000529     6     6\n10  1939 Lory      0.00000529     6     6\n11  1939 Maitland  0.00000529     6     6\n12  1939 Delaney   0.00000441     5     5\n\n\n\n3.5.3 join (use join to merge two datasets)\n\n3.5.3.1 First get the data (GapMinder)\nBoth of the following datasets come from GapMinder. The first represents country, year, and female literacy rate. The second represents country, year, and GDP (in fixed 2000 US$).\n\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF &lt;- litF |&gt; \n  select(country = starts_with(\"Adult\"),\n         starts_with(\"1\"), starts_with(\"2\")) |&gt;\n  pivot_longer(-country,\n               names_to = \"year\",\n               values_to = \"litRateF\") |&gt;\n  filter(!is.na(litRateF))\n\n\ngs4_deauth()\nGDP &lt;- read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP &lt;- GDP |&gt; \n  select(country = starts_with(\"Income\"),\n         starts_with(\"1\"), starts_with(\"2\")) |&gt;\n  pivot_longer(-country,\n               names_to = \"year\",\n               values_to = \"gdp\") |&gt;\n  filter(!is.na(gdp))\n\n\nhead(litF)\n\n# A tibble: 6 × 3\n  country     year  litRateF\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n1 Afghanistan 1979      4.99\n2 Afghanistan 2011     13   \n3 Albania     2001     98.3 \n4 Albania     2008     94.7 \n5 Albania     2011     95.7 \n6 Algeria     1987     35.8 \n\nhead(GDP)\n\n# A tibble: 6 × 3\n  country year    gdp\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n1 Albania 1980  1061.\n2 Albania 1981  1100.\n3 Albania 1982  1111.\n4 Albania 1983  1101.\n5 Albania 1984  1065.\n6 Albania 1985  1060.\n\n# left\nlitGDPleft = left_join(litF, GDP, by = c(\"country\", \"year\"))\ndim(litGDPleft)\n\n[1] 571   4\n\nsum(is.na(litGDPleft$gdp))\n\n[1] 66\n\nhead(litGDPleft)\n\n# A tibble: 6 × 4\n  country     year  litRateF   gdp\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan 1979      4.99   NA \n2 Afghanistan 2011     13      NA \n3 Albania     2001     98.3  1282.\n4 Albania     2008     94.7  1804.\n5 Albania     2011     95.7  1966.\n6 Algeria     1987     35.8  1902.\n\n# right\nlitGDPright = right_join(litF, GDP, by = c(\"country\", \"year\"))\ndim(litGDPright)\n\n[1] 7988    4\n\nsum(is.na(litGDPright$gdp))\n\n[1] 0\n\nhead(litGDPright)\n\n# A tibble: 6 × 4\n  country year  litRateF   gdp\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Albania 2001      98.3 1282.\n2 Albania 2008      94.7 1804.\n3 Albania 2011      95.7 1966.\n4 Algeria 1987      35.8 1902.\n5 Algeria 2002      60.1 1872.\n6 Algeria 2006      63.9 2125.\n\n# inner\nlitGDPinner = inner_join(litF, GDP, by = c(\"country\", \"year\"))\ndim(litGDPinner)\n\n[1] 505   4\n\nsum(is.na(litGDPinner$gdp))\n\n[1] 0\n\nhead(litGDPinner)\n\n# A tibble: 6 × 4\n  country year  litRateF   gdp\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Albania 2001      98.3 1282.\n2 Albania 2008      94.7 1804.\n3 Albania 2011      95.7 1966.\n4 Algeria 1987      35.8 1902.\n5 Algeria 2002      60.1 1872.\n6 Algeria 2006      63.9 2125.\n\n# full\nlitGDPfull = full_join(litF, GDP, by = c(\"country\", \"year\"))\ndim(litGDPfull)\n\n[1] 8054    4\n\nsum(is.na(litGDPfull$gdp))\n\n[1] 66\n\nhead(litGDPfull)\n\n# A tibble: 6 × 4\n  country     year  litRateF   gdp\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan 1979      4.99   NA \n2 Afghanistan 2011     13      NA \n3 Albania     2001     98.3  1282.\n4 Albania     2008     94.7  1804.\n5 Albania     2011     95.7  1966.\n6 Algeria     1987     35.8  1902.\n\n\n\n3.5.4 lubridate\n\nlubridate is a another R package meant for data wrangling (Grolemund and Wickham 2011). In particular, lubridate makes it very easy to work with days, times, and dates. The base idea is to start with dates in a ymd (year month day) format and transform the information into whatever you want. The lubridate cheatsheet provides many of the basic functionality.\nExample from https://lubridate.tidyverse.org/reference/lubridate-package.html\n\n3.5.4.1 If anyone drove a time machine, they would crash\nThe length of months and years change so often that doing arithmetic with them can be unintuitive. Consider a simple operation, January 31st + one month. Should the answer be:\n\nFebruary 31st (which doesn’t exist)\nMarch 4th (31 days after January 31), or\nFebruary 28th (assuming its not a leap year)\n\nA basic property of arithmetic is that a + b - b = a. Only solution 1 obeys the mathematical property, but it is an invalid date. Wickham wants to make lubridate as consistent as possible by invoking the following rule: if adding or subtracting a month or a year creates an invalid date, lubridate will return an NA.\nIf you thought solution 2 or 3 was more useful, no problem. You can still get those results with clever arithmetic, or by using the special %m+% and %m-% operators. %m+% and %m-% automatically roll dates back to the last day of the month, should that be necessary.\n\n3.5.4.2 R examples, lubridate\n\nSome basics in lubridate\n\n\nlibrary(lubridate)\nrightnow &lt;- now()\n\nday(rightnow)\n\n[1] 29\n\nweek(rightnow)\n\n[1] 39\n\nmonth(rightnow, label=FALSE)\n\n[1] 9\n\nmonth(rightnow, label=TRUE)\n\n[1] Sep\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nyear(rightnow)\n\n[1] 2024\n\nminute(rightnow)\n\n[1] 18\n\nhour(rightnow)\n\n[1] 15\n\nyday(rightnow)\n\n[1] 273\n\nmday(rightnow)\n\n[1] 29\n\nwday(rightnow, label=FALSE)\n\n[1] 1\n\nwday(rightnow, label=TRUE)\n\n[1] Sun\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\nBut how do I create a date object?\n\njan31 &lt;- ymd(\"2021-01-31\")\njan31 + months(0:11)\n\n [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n[11] NA           \"2021-12-31\"\n\nfloor_date(jan31, \"month\") + months(0:11) + days(31)\n\n [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n[11] \"2021-12-02\" \"2022-01-01\"\n\njan31 + months(0:11) + days(31)\n\n [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n[11] NA           \"2022-01-31\"\n\njan31 %m+% months(0:11)\n\n [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n[11] \"2021-11-30\" \"2021-12-31\"\n\n\nNYC flights\n\nlibrary(nycflights13)\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\nflightsWK &lt;- flights |&gt; \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) |&gt;\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)\n\n# A tibble: 6 × 22\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ymdday &lt;date&gt;, weekdy &lt;ord&gt;,\n#   whichweek &lt;dbl&gt;\n\nflightsWK &lt;- flights |&gt; \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) |&gt;\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK |&gt; \n  select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n         arr_time, air_time) |&gt;  \n   head()\n\n# A tibble: 6 × 9\n   year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;    &lt;dbl&gt;\n1  2013     1     1 2013-01-01 Tue            1      517      830      227\n2  2013     1     1 2013-01-01 Tue            1      533      850      227\n3  2013     1     1 2013-01-01 Tue            1      542      923      160\n4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n5  2013     1     1 2013-01-01 Tue            1      554      812      116\n6  2013     1     1 2013-01-01 Tue            1      554      740      150",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#purrr-for-functional-programming",
    "href": "03-wrangling.html#purrr-for-functional-programming",
    "title": "3  Data Wrangling",
    "section": "\n3.6 purrr for functional programming",
    "text": "3.6 purrr for functional programming\nWe will see the R package purrr in greater detail as we go, but for now, let’s get a hint for how it works.\nWe are going to focus on the map family of functions which will just get us started. Lots of other good purrr functions like pluck() and accumulate().\nMuch of below is taken from a tutorial by Rebecca Barter.\nThe map functions are named by the output the produce. For example:\n\nmap(.x, .f) is the main mapping function and returns a list\nmap_df(.x, .f) returns a data frame\nmap_dbl(.x, .f) returns a numeric (double) vector\nmap_chr(.x, .f) returns a character vector\nmap_lgl(.x, .f) returns a logical vector\n\nNote that the first argument is always the data object and the second object is always the function you want to iteratively apply to each element in the input object.\nThe input to a map function is always either a vector (like a column), a list (which can be non-rectangular), or a dataframe (like a rectangle).\nA list is a way to hold things which might be very different in shape:\n\na_list &lt;- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)\n\n$a_number\n[1] 5\n\n$a_vector\n[1] \"a\" \"b\" \"c\"\n\n$a_dataframe\n  a b             c\n1 1 q       bananas\n2 2 b           are\n3 3 z so very great\n\n\nConsider the following function:\n\nadd_ten &lt;- function(x) {\n  return(x + 10)\n  }\n\nWe can map() the add_ten() function across a vector. Note that the output is a list (the default).\n\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 20\n\n\nWhat if we use a different type of input? The default behavior is to still return a list!\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map(add_ten)\n\n$a\n[1] 12\n\n$b\n[1] 15\n\n$c\n[1] 20\n\n\nWhat if we want a different type of output? We use a different map() function, map_df(), for example.\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map_df(add_ten)\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nShorthand lets us get away from pre-defining the function (which will be useful). Use the tilde ~ to indicate that you have a function:\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map_df(~{.x + 10})\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nMostly, the tilde will be used for functions we already know but want to modify (if we don’t modify, and it has a simple name, we don’t use the tilde):\n\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins_split &lt;- split(penguins, penguins$species)\npenguins_split |&gt;\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) |&gt;\n  map_df(tidy)  # map(tidy)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\n\npenguins |&gt;\n  group_by(species) |&gt;\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) |&gt;\n  map(tidy)  # map_df(tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 0.00948      \n2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n\n[[2]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -3037.     997.       -3.05 0.00333      \n2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n\n[[3]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n2 flipper_length_mm     54.6      5.03     10.9  1.33e-19",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#reprex",
    "href": "03-wrangling.html#reprex",
    "title": "3  Data Wrangling",
    "section": "\n3.7 reprex()\n",
    "text": "3.7 reprex()\n\n\nHelp me help you\n\nIn order to create a reproducible example …\nStep 1. Copy code onto the clipboard\nStep 2. Type reprex() into the Console\nStep 3. Look at the Viewer to the right. Copy the Viewer output into GitHub, Piazza, an email, stackexchange, etc.\nSome places to learn more about reprex include\n\nA blog about it: https://teachdatascience.com/reprex/\nThe reprex vignette: https://reprex.tidyverse.org/index.html\n\nreprex dos and donts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nJenny Bryan webinar on reprex: “Help me help you. Creating reproducible examples” https://resources.rstudio.com/webinars/help-me-help-you-creating-reproducible-examples-jenny-bryan\nSome advice: https://stackoverflow.com/help/minimal-reproducible-example\n\n\n3.7.0.1 reprex demo\nreprex(\n  jan31 + months(0:11) + days(31)\n)\nmultiple lines of code:\nreprex({\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})\nreprex({\n  library(lubridate)\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})\n\n\n\nIf you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794\n\n\n\nGobet, Fernand, and Herbert A. Simon. 1996. “Templates in Chess Memory: A Mechanism for Recalling Several Boards.” Cognitive Psychology 31 (1): 1–40. https://doi.org/https://doi.org/10.1006/cogp.1996.0011.\n\n\nGrolemund, G., and H. Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3). http://www.jstatsoft.org/v40/i03/paper.\n\n\nKaplan, Daniel. 2015. Data Computing: An Introduction to Wrangling and Visualization with r. Project Mosaic Books.\n\n\nSoloway, Elliot, and Kate Ehrlich. 1984. “Empirical Studies of Programming Knowledge.” IEEE Transactions on Software Engineering SE-10 (5).\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "02-viz.html",
    "href": "02-viz.html",
    "title": "\n2  Visualization\n",
    "section": "",
    "text": "2.1 Examples\nThe first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#examples",
    "href": "02-viz.html#examples",
    "title": "\n2  Visualization\n",
    "section": "",
    "text": "An aside\nGenerally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi\nAs mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (Tufte 1997).\n\n“An essential analytic task in making decisions based on evidence is to understand how things work.”\nMaking decisions based on evidence requires the appropriate display of that evidence.”\n\nBack to the examples…\n\n2.1.1 Cholera via Tufte\nIn September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after.\nWhy was John Snow successful at solving the problem? Some thoughts to consider (as reported in Tufte (1997)):\n\nThe bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means.\nData in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area.\nComparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease.\nAlternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case.\nDid removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline?\nAssessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told?\n\n2.1.2 Challenger via Tufte\nJohn Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F).\nUnlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated!\nThe biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what??\n\n\nThe engineers who understood the problem created tables and engineering graphs which were\n\nNot visually appealing.\nNot decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”)\nThere was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility.\n\n\n\nThe information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure.\nMissing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.)\nAnecdotal evidence was misconstrued: SRM-15 at 57F had the most damage, but SRM-22 at 75F had the second most damage.\nIn the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F.\nTufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order.\n\nAs with the cholera outbreak, a persuasive argument could have been made if the visualizations had\n\nbeen in context plot data versus temperature not time!,\nused appropriate comparisons: as compared with what?,\nconsider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and\nthe graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?.\n\nTufte (Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms.\n\n\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#thoughts",
    "href": "02-viz.html#thoughts",
    "title": "\n2  Visualization\n",
    "section": "\n2.2 Thoughts on Plotting",
    "text": "2.2 Thoughts on Plotting\n\n2.2.1 Advice\n(I think that the advice below is a compilation of important ideas from many amazing graphical and visual scientists and what I’ve come to know from my own work. For example see Yau (2013), Cleveland and McGill (1984), Tufte (1997). The advice, however, may have come more directly from another single source (which I am currently unable to find), and I am appreciative of such good advice.)\n\nThink carefully about the basic plot\n\nAvoid having other graph elements interfere with data\nUse visually prominent symbols\nAvoid over-plotting (One way to avoid over plotting: Jitter the values)\nDifferent values of data may obscure each other\nInclude all or nearly all of the data\nFill data region\n\n\nEliminate superfluous material\n\nChart junk & stuff that adds no meaning, e.g. butterflies on top of barplots, background images\nExtra tick marks and grid lines\nUnnecessary text and arrows\nDecimal places beyond the measurement error or the level of difference\n\n\nFacilitate Comparisons\n\nPut juxtaposed plots on same scale\nMake it easy to distinguish elements of superposed plots (e.g. color)\nEmphasizes the important difference\nComparison: volume, area, height (be careful, volume can seem bigger than you mean it to)\n\n\nChoosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.)\n\nKeep scales on x and y axes the same for both plots to facilitate the comparison\nZoom in to focus on the region that contains the bulk of the data\nKeep the scale the same throughout the plot (i.e. don’t change it mid-axis)\nOrigin need not be on the scale\nChoose a scale that improves resolution\nAvoid jiggling the baseline\n\n\nHow to make a plot information rich\n\nDescribe what you see in the caption\nAdd context with reference markers (lines and points) including text\nAdd legends and labels\nUse color and plotting symbols to add more information\nPlot the same thing more than once in different ways/scales\nReduce clutter\n\n\nCaptions should\n\nBe comprehensive\nSelf-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graph\n\n\nGood Plot Making Practice\n\nPut major conclusions in graphical form\nProvide reference information\nProof read for clarity and consistency\nGraphing is an iterative process\nMultiplicity is OK, i.e. two plots of the same variable may provide different messages\nMake plots data rich\n\n\n\nCreating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data.\n\n2.2.2 An example from Information is Beautiful\nConsider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph.\n\n\n\n\n\n\n\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\n\n\n\nI have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations.\n\n\n\n\n\n\n\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nData retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0\n\n2.2.2.1 Fonts Matter\nAt RStudio::conf 2020, The Glamour of Graphics, Will Chase makes some very important points about how and why making good graphics matters. The talk might be summarized by the plot below: fonts matter.\n\n\n\n\n\n\n\n\n\n2.2.3 Assessing Graphics (and Other Analyses)\n\n\n\n\n\n\n\n\nCritical Task\nNeeds Improvement\nBasic\nSurpassed\n\n\n\n\nComputation Perform computations\nComputations contain errors and extraneous code\nComputations are correct but contain extraneous / unnecessary computations\nComputations are correct and properly identified and labeled\n\n\n\nAnalysis Choose and carry out analysis appropriate for data and content(s)\nChoice of analysis is overly simplistic, irrelevant, or missing key component\nAnalysis appropriate, but incomplete, or not important features and assumptions not made explicit\nAnalysis appropriate, complete, advanced, relevant, and informative\n\n\n\nSynthesis Identify key features of the analysis, and interpret results (including context)\nConclusions are missing, incorrect, or not made based on results of analysis\nConclusions reasonable, but is partially correct or partially complete\nMake relevant conclusions explicitly connect to analysis and to context\n\n\n\nVisual presentation Communicate findings graphically clearly, precisely, and concisely\nInappropriate choice of plots; poorly labeled plots; plots missing\nPlots convey information correctly but lack context for interpretation\nPlots convey information correctly with adequate / appropriate reference information\n\n\n\nWritten Communicate findings clearly, precisely, and concisely\nExplanation is illogical, incorrect, or incoherent\nExplanation is partially correct but incomplete or unconvincing\nExplanation is correct, complete, and convincing\n\n\n\nA rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016).",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#deconstruct",
    "href": "02-viz.html#deconstruct",
    "title": "\n2  Visualization\n",
    "section": "\n2.3 Deconstructing a graph",
    "text": "2.3 Deconstructing a graph\n\n2.3.1 The Grammar of Graphics (gg)\nYau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence.\nOne great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?)\nThe most important questions you can ask with respect to creating figures are:\n\nWhat do we want R to do? (What is the goal?)\nWhat does R need to know?\n\nYau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2021), chapter 2.)\n\nVisual Cues: the aspects of the figure where we should focus.Position (numerical) where in relation to other things?Length (numerical) how big (in one dimension)?Angle (numerical) how wide? parallel to something else?Direction (numerical) at what slope? In a time series, going up or down?Shape (categorical) belonging to what group?Area (numerical) how big (in two dimensions)? Beware of improper scaling!Volume (numerical) how big (in three dimensions)? Beware of improper scaling!Shade (either) to what extent? how severely?Color (either) to what extent? how severely? Beware of red/green color blindness.\nCoordinate System: rectangular, polar, geographic, etc.\nScale: numeric (linear? logarithmic?), categorical (ordered?), time\nContext: in comparison to what (think back to ideas from Tufte)\n\n\n\n\n\n\n\n\nFigure 3.3 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013).\n\n\n\nOrder Matters\n\n\n\n\n\n\n\nFigure 3.12 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013). Original work by Cleveland and McGill (1984).\n\n\n\nCues Together\n\n\n\n\n\n\n\nFigure 3.25 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013).\n\n\n\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\n\n2.3.1.1 The grammar of graphics in ggplot2\n\ngeom: the geometric “shape” used to display data\n\nbar, point, line, ribbon, text, etc.\n\naesthetic: an attribute controlling how geom is displayed with respect to variables\n\nx position, y position, color, fill, shape, size, etc.\n\nscale: adjust information in the aesthetic to map onto the plot\n\n\nparticular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values.\n\nguide: helps user convert visual data back into raw data (legends, axes)\nstat: a transformation applied to data before geom gets it\n\nexample: histograms work on binned data\n\n2.3.2 ggplot2\n\nIn ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2021). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births.\n\nGoals\nWhat I will try to do\n\ngive a tour of ggplot2\nexplain how to think about plots the ggplot2 way\nprepare/encourage you to learn more later\n\nWhat I can’t do in one session\n\nshow every bell and whistle\nmake you an expert at using ggplot2\nGetting help\n\nOne of the best ways to get started with ggplot is to Google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/\nggplot2 cheat sheet: https://rstudio.github.io/cheatsheets/html/data-visualization.html\nLook at the end of the presentation. More help options there.\n\n\n\n\n\n\n\n\n\n\nlibrary(mosaic)    # where the data lives\ndata(Births78)     # restore fresh version of Births78\n\n\nhead(Births78, 3)\n\n\n\n\n\n\n\n\ndate\nbirths\nwday\nyear\nmonth\nday_of_year\nday_of_month\nday_of_week\n\n\n\n1978-01-01\n7701\nSun\n1978\n1\n1\n1\n1\n\n\n1978-01-02\n7527\nMon\n1978\n1\n2\n2\n2\n\n\n1978-01-03\n8825\nTue\n1978\n1\n3\n3\n3\n\n\n\n\n\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nTwo Questions:\n\nWhat do we want R to do? (What is the goal?)\n\nWhat does R need to know?\n\ndata source: Births78\n\naesthetics:\n\ndate -&gt; x\nbirths -&gt; y\npoints (!)\n\n\n\n\n\nGoal: scatterplot = a plot with points\n\nggplot() + geom_point()\n\n\n\nWhat does R need to know?\n\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nWhat has changed?\n\nnew aesthetic: mapping color to day of week\nAdding day of week to the data set\n\nggplot(data = Births78) +\n  geom_point(aes(x = date, y = births, color = wday))+\n  labs(title = \"US Births in 1978\")\n\n\n\n\n\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we use lines instead of dots\n\nggplot(data = Births78) +\n  geom_line(aes(x = date, y = births, color = wday)) +\n  labs(title = \"US Births in 1978\")\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we have two layers, one with points and one with lines\n\nggplot(data = Births78, \n       aes(x = date, y = births, color = wday)) + \n  geom_point() +  \n  geom_line()+\n  labs(title = \"US Births in 1978\")\n\n\nThe layers are placed one on top of the other: the points are below and the lines are above.\ndata and aes specified in ggplot() affect all geoms\nAlternative Syntax\n\nggplot(data = Births78, \n       aes(x = date, y = births, color = wday)) + \n  geom_point() + \n  geom_line()+\n  labs(title = \"US Births in 1978\")\n\n\n\n\n\n\n\nWhat does adding the color argument do?\n\nggplot(data = Births78, \n       aes(x = date, y = births, color = \"navy\")) + \n  geom_point()  +\n  labs(title = \"US Births in 1978\")\n\n\n\n\n\n\n\n\n\nBecause there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy.\nSetting vs. Mapping\nIf we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable:\n\nggplot(data = Births78, \n       aes(x = date, y = births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  labs(title = \"US Births in 1978\")\n\n\n\n\n\n\n\n\nNote that color = \"navy\" is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting.\nHow can we make the plot?\n\n\n\n\n\n\n\n\n\nggplot(data = Births78, \n       aes(x = date, y = births)) + \n  geom_line(aes(color = wday)) +       # map color here\n  geom_point(color = \"navy\") +          # set color here\n  labs(title = \"US Births in 1978\")\n\n\nggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults.\ngood practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah()\nSetting vs. Mapping (again)\nInformation gets passed to the plot via:\n\nmap the variable information inside the aes (aesthetic) command\nset the non-variable information outside the aes (aesthetic) command\nOther geoms\n\napropos(\"^geom_\")\n\n [1] \"geom_abline\"            \"geom_area\"              \"geom_ash\"              \n [4] \"geom_bar\"               \"geom_bin_2d\"            \"geom_bin2d\"            \n [7] \"geom_blank\"             \"geom_boxplot\"           \"geom_col\"              \n[10] \"geom_contour\"           \"geom_contour_filled\"    \"geom_count\"            \n[13] \"geom_crossbar\"          \"geom_curve\"             \"geom_density\"          \n[16] \"geom_density_2d\"        \"geom_density_2d_filled\" \"geom_density2d\"        \n[19] \"geom_density2d_filled\"  \"geom_dotplot\"           \"geom_errorbar\"         \n[22] \"geom_errorbarh\"         \"geom_freqpoly\"          \"geom_function\"         \n[25] \"geom_hex\"               \"geom_histogram\"         \"geom_hline\"            \n[28] \"geom_jitter\"            \"geom_label\"             \"geom_line\"             \n[31] \"geom_linerange\"         \"geom_lm\"                \"geom_map\"              \n[34] \"geom_path\"              \"geom_point\"             \"geom_pointrange\"       \n[37] \"geom_polygon\"           \"geom_qq\"                \"geom_qq_line\"          \n[40] \"geom_quantile\"          \"geom_raster\"            \"geom_rect\"             \n[43] \"geom_ribbon\"            \"geom_rug\"               \"geom_segment\"          \n[46] \"geom_sf\"                \"geom_sf_label\"          \"geom_sf_text\"          \n[49] \"geom_smooth\"            \"geom_spline\"            \"geom_spoke\"            \n[52] \"geom_step\"              \"geom_text\"              \"geom_tile\"             \n[55] \"geom_violin\"            \"geom_vline\"            \n\n\nhelp pages will tell you their aesthetics, default stats, etc.\n\n?geom_area             # for example\n\nLet’s try geom_area\n\n\nggplot(data = Births78, \n       aes(x = date, y = births, fill = wday)) + \n  geom_area()+\n  labs(title = \"US Births in 1978\")\n\n\n\n\n\n\n\nUsing area does not produce a good plot\n\nover plotting is hiding much of the data\nextending y-axis to 0 may or may not be desirable.\nSide note: what makes a plot good?\nMost (all?) graphics are intended to help us make comparisons\n\nHow does something change over time?\nDo my treatments matter? How much?\nDo men and women respond the same way?\n\nKey plot metric: Does my plot make the comparisons I am interested in\n\neasily, and\naccurately?\nTime for some different data\nHELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial\n\nhead(HELPrct)\n\n\n\n\n\n\n\n\nage\nanysubstatus\nanysub\ncesd\nd1\ndaysanysub\ndayslink\ndrugrisk\ne2b\nfemale\nsex\ng1b\nhomeless\ni1\ni2\nid\nindtot\nlinkstatus\nlink\nmcs\npcs\npss_fr\nracegrp\nsatreat\nsexrisk\nsubstance\ntreat\navg_drinks\nmax_drinks\nhospitalizations\n\n\n\n37\n1\nyes\n49\n3\n177\n225\n0\nNA\n0\nmale\nyes\nhoused\n13\n26\n1\n39\n1\nyes\n25.111990\n58.41369\n0\nblack\nno\n4\ncocaine\nyes\n13\n26\n3\n\n\n37\n1\nyes\n30\n22\n2\nNA\n0\nNA\n0\nmale\nyes\nhomeless\n56\n62\n2\n43\nNA\nNA\n26.670307\n36.03694\n1\nwhite\nno\n7\nalcohol\nyes\n56\n62\n22\n\n\n26\n1\nyes\n39\n0\n3\n365\n20\nNA\n0\nmale\nno\nhoused\n0\n0\n3\n41\n0\nno\n6.762923\n74.80633\n13\nblack\nno\n2\nheroin\nno\n0\n0\n0\n\n\n39\n1\nyes\n15\n2\n189\n343\n0\n1\n1\nfemale\nno\nhoused\n5\n5\n4\n28\n0\nno\n43.967880\n61.93168\n11\nwhite\nyes\n4\nheroin\nno\n5\n5\n2\n\n\n32\n1\nyes\n39\n12\n2\n57\n0\n1\n0\nmale\nno\nhomeless\n10\n13\n5\n38\n1\nyes\n21.675755\n37.34558\n10\nblack\nno\n6\ncocaine\nno\n10\n13\n12\n\n\n47\n1\nyes\n6\n1\n31\n365\n0\nNA\n1\nfemale\nno\nhoused\n4\n4\n6\n29\n0\nno\n55.508991\n46.47521\n5\nblack\nno\n5\ncocaine\nyes\n4\n4\n1\n\n\n\n\n\n\n\nSubjects admitted for treatment for addiction to one of three substances.\nWho are the people in the study?\n\nggplot(data = HELPrct, \n       aes(x = substance)) + \n  geom_bar()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\nHmm. What’s up with y?\n\n\nstat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values.\n\n\nWho are the people in the study?\n\nggplot(data = HELPrct, \n       aes(x = substance, fill = sex)) + \n  geom_bar()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nlibrary(scales)\nggplot(data = HELPrct, \n       aes(x = substance, fill = sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nggplot(data = HELPrct, \n       aes(x = substance, fill = sex)) + \n  geom_bar(position = \"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study?\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_histogram()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice the messages\n\nstat_bin: Histograms are not mapping the raw data but binned data.stat_bin() performs the data transformation.\nbinwidth: a default binwidth has been selected, but we should really choose our own.\nSetting the binwidth manually\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_histogram(binwidth = 2)+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study? – Other geoms\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_freqpoly(binwidth = 2)+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_density()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery geom comes with a default stat\n\nfor simple cases, the stat is stat_identity() which does nothing\nwe can mix and match geoms and stats however we like\n\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_line(stat = \"density\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery stat comes with a default geom, every geom with a default stat\n\nwe can specify stats instead of geom, if we prefer\nwe can mix and match geoms and stats however we like\n\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  stat_density(geom = \"line\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMore combinations\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_point(stat = \"bin\", binwidth = 3) + \n  geom_line(stat = \"bin\", binwidth = 3)  +\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_area(stat = \"bin\", binwidth = 3) +\n  labs(title = \"HELP clinical trial at detoxification unit\") \n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = age)) + \n  geom_point(stat = \"bin\", binwidth = 3, \n             aes(size=..count..)) +\n  geom_line(stat = \"bin\", binwidth = 3) +\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow much do they drink? (i1)\n\nggplot(data = HELPrct, \n       aes(x = i1)) + \n  geom_histogram()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = i1)) + \n  geom_density()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = i1)) + \n  geom_area(stat = \"density\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nCovariates: Adding in more variables\nUsing color and linetype:\n\nggplot(data = HELPrct, \n       aes(x = i1, color = substance, linetype = sex)) + \n  geom_line(stat = \"density\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nUsing color and facets\n\nggplot(data = HELPrct, \n       aes(x = i1, color = substance)) + \n  geom_line(stat = \"density\") + \n  facet_grid( . ~ sex )+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = i1, color = substance)) + \n  geom_line(stat = \"density\") + \n  facet_grid( sex ~ . )+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nBoxplots\nBoxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”.\nThe quantitative variable must be y, and there must be an additional x variable.\n\nggplot(data = HELPrct, \n       aes(x = substance, y = age, color = sex)) + \n  geom_boxplot()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHorizontal boxplots\nHorizontal boxplots are obtained by flipping the coordinate system:\n\nggplot(data = HELPrct, \n       aes(x = substance, y = age, color = sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\ncoord_flip() may be used with other plots as well to reverse the roles of x and y on the plot.\nAxes scaling with boxplots\nWe can scale the continuous axis\n\nggplot(data = HELPrct, \n       aes(x = substance, y = age, color = sex)) + \n  geom_boxplot() +\n  coord_trans(y = \"log\")+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nGive me some space\nWe’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually.\n\nggplot(data = HELPrct, \n       aes(x = substance, y = age, color = sex)) + \n  geom_boxplot(position = position_dodge(width = 1)) +\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nIssues with bigger data\n\nlibrary(NHANES)\ndim(NHANES)\n\n[1] 10000    76\n\nggplot(data = NHANES, \n       aes(x = Height, y = Weight)) +\n  geom_point() + \n  facet_grid( Gender ~ PregnantNow ) +\n  labs(title = \"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\n\nAlthough we can see a generally positive association (as we would expect), the over plotting may be hiding information.\nUsing alpha (opacity)\nOne way to deal with over plotting is to set the opacity low.\n\nggplot(data = NHANES, aes(x = Height, y = Weight)) +\n  geom_point(alpha = 0.01) + \n  facet_grid( Gender ~ PregnantNow ) +\n  labs(title = \"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\ngeom_density2d\nAlternatively (or simultaneously) we might prefer a different geom altogether.\n\nggplot(data = NHANES, aes(x = Height, y = Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  labs(title = \"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data = HELPrct, aes(x = sex, y = age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha = .6) +\n  coord_flip()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data = HELPrct, aes(x = sex, y = age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha = .6, position = position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nThings I haven’t mentioned (much)\n\ncoords (coord_flip() is good to know about)\nthemes (for customizing appearance)\nposition (position_dodge(), position_jitterdodge() (for use with points on top of dodged boxplots), position_stack(), etc.)\ntransforming axes\n\n\nlibrary(ggthemes)\nggplot(Births78, aes(x = date, y = births)) +\n  geom_point() + \n  theme_wsj()\n\n\n\n\n\n\n\n\nggplot(data = HELPrct, \n       aes(x = substance, y = age, color = sex)) +\n  geom_boxplot(position = position_dodge(width = 1)) +\n  geom_point(aes(color = sex, fill = sex), \n             position = position_jitterdodge(dodge.width = 1, jitter.width = 0.1), \n             size = 0.5) +\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nA little bit of everything\n\nggplot( data = HELPrct, \n        aes(x = substance, y = age, color = sex)) +\n  geom_boxplot(coef = 10, \n               position = position_dodge(width=1)) +\n  geom_point(aes(fill = sex), alpha = .5, \n             position = position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  labs(title = \"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWhat else can we do?\nshiny\n\ninteractive graphics / modeling\nhttps://shiny.posit.co/\n\nplotly\n\nPlotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function.\n\n\nhttps://plot.ly/ggplot2/getting-started/\n\nDynamic documents\n\ncombination of Quarto, ggvis, and shiny\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\nFigure 3.3 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013).\nFigure 3.12 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013). Original work by Cleveland and McGill (1984).\nFigure 3.25 of Yau’s Data Points: Visualization That Means Something, https://flowingdata.com/data-points/DataPoints-Ch3.pdf, (Yau 2013).\n\n\n\nBaumer, Ben, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. CRC Press. https://mdsr-book.github.io/mdsr2e/.\n\n\nCleveland, William S., and Robert McGill. 1984. “Graphical Perception: Theory, Experimentation, and Application to the Development of Graphical Methods.” Journal of the American Statistical Association 79 (387): 531–54. https://doi.org/10.1080/01621459.1984.10478080.\n\n\nGelman, Andrew. 2011. “Rejoinder.” Journal of Computational and Graphical Statistics 20: 36–40. http://arxiv.org/abs/1503.00781.\n\n\nNolan, Deborah, and Jamis Perrett. 2016. “Teaching and Learning Data Visualization: Ideas and Assignments.” The American Statistician.\n\n\nTufte, Edward. 1997. “Visual Explanations: Images and Quantities, Evidence and Narrative.” In, 27–54. Graphics Press, LLC. www.edwardtufte.com.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.\n\n\nYau, Nathan. 2013. Data Points: Visualization That Means Something. Wiley.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction1",
    "section": "",
    "text": "1.1 Course Logistics\nWhat is Statistics?\nGenerally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nProbability vs. Statistics\nWhat is the content of Math 154?\nThis class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years.\nWho should take Math 154?\nComputational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science.\nWhat are the prerequisites for Math 154?\nComputational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended.\nIt is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nIs there overlap with other classes?\nThere are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability?\nWhen should I take Math 154?\nWhile the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors.\nWhat is the workload for Math 154?\nThere is one homework assignment per week, regular quizzes, and a final end of the semester project. Many students report working about 8-10 hours per week on this class.\nWhat software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS?\nAll of the work will be done in R (using RStudio as a front end, called an integrated development environment, IDE). You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing.\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-logistics",
    "href": "01-intro.html#course-logistics",
    "title": "1  Introduction1",
    "section": "",
    "text": "descriptive statistics describe the sample at hand with no intent on making generalizations.\ninferential statistics use a sample to make claims about a population\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou may use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.)\nIf you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required https://posit.co/downloads/.\nAll assignments should be turned in using Quarto compiled to pdf.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-content",
    "href": "01-intro.html#course-content",
    "title": "1  Introduction1",
    "section": "1.2 Course Content",
    "text": "1.2 Course Content\n\n1.2.1 Topics\nComputational Statistics can be a very large umbrella for many ideas. Indeed, sometimes the topics can seem somewhat disjointed. Below, I’ve categorized the topics we will cover into four groups. The four different broad topics all play different roles and can be more or less important depending on the problem at hand. None of the topics should exist on their own, because only with the bigger focus on all topics will any sort of data analysis / interpretation be accurate and compelling.\nLetting the computer help: R, RStudio, Git, GitHub, Reproducibility, Data Viz, Data Wrangling\nStatistics: Simulating, Randomization / Permutation Tests, Bootstrapping, Ethics\nMachine Learning: Classification, Clustering, Regular Expressions\nMathematics: Support Vector Machines\n\n\n1.2.2 Vocabulary\n\nA statistic is a numerical measurement we get from the sample, a function of the data.\nA parameter is a numerical measurement of the population. We never know the true value of the parameter.\nAn estimator is a function of the unobserved data that tries to approximate the unknown parameter value.\nAn estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.]\n\n\nOne of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015)\n\nWith thanks to Ben Baumer for perspective and sharing course materials.\n\n\n1.2.3 The Workflow\n\n\n\n\n\n\n\n\n\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\n\n\n\n\n\n\n\n\n\n\n\n\n\nStitch Fix Algorithms Tour\n\n\n\n\n\n\n1.2.4 Principles for the Data Science Process tl;dr\n(Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.)\nDuncan Temple Lang, University of California, Davis\nDuncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.]\nDuncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science.\nTemple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process:\n\nAsking a general question.\nRefining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood.\nAccess to data. This is unrelated to the science but does require computational skill.\nTransforming to data structures.\nExploratory data analyses to understand the data and determine whether the results will scale.\n\nThis is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results.\nTemple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student:\n\nMapping the general question to a statistical framework.\nUnderstanding the scope of inference, sampling, biases, and limitations.\nExploratory data analyses, including missing values, data quality, cleaning, matching, and fusing.\nUnderstanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability.\nConditional dependence and heterogeneity.\nDimension reduction, variable selection, and sparsity.\nSpurious relationships and multiple testing.\nParameter estimation versus “black box” prediction and classification.\nDiagnostics, residuals, and comparing models.\nQuantifying the uncertainty of a model.\nSampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed.\nStatistical accuracy versus computational complexity and efficiency.\n\nTemple Lang then briefly discussed some of the practical aspects of computing, including the following:\n\nAccessing data.\nManipulating raw data.\nData structures and storage, including correlated data.\nVisualization at all stages (particularly in exploratory data analyses and conveying the results).\nParallel computing, which can be challenging for a new student.\nTranslating high-level descriptions to optimal programs.\n\nDuring the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).]\nA workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "01-intro.html#repro",
    "href": "01-intro.html#repro",
    "title": "1  Introduction1",
    "section": "1.3 Reproducibility",
    "text": "1.3 Reproducibility\nReproducibility has long been considered an important topic for consideration in any research project. However, recently there has been increased press and available examples for understanding the impact that non-reproducible science can have.\nKitzes, Turek, and Deniz (2018) provide a full textbook on the structure of reproducible research as well as dozens of case studies to help hone skills and consider different aspects of the reproducible pipeline. Below are a handful of examples to get us started.\n\n1.3.1 Need for Reproducibility\n\n\n\n\n\n\n\n\n\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\n\n\n\n\n\n\nExample 1\nScience retracts gay marriage paper without agreement of lead author LaCour\n\n\n\n\n\n\n\n\n\n\nIn May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior.\nScience Editor-in-Chief Marcia McNutt:\n\nOriginal survey data not made available for independent reproduction of results.\nSurvey incentives misrepresented.\nSponsorship statement false.\n\nTwo Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked.\nMethods we’ll discuss can’t prevent this, but they can make it easier to discover issues.\nSource: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent\n\n\n\nExample 2\nSeizure study retracted after authors realize data got “terribly mixed”\n\n\n\n\n\n\n\n\n\n\nFrom the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates:\n\n\nThe article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness.\n\n\nSource: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/\n\n\n\nExample 3\nBad spreadsheet merge kills depression paper, quick fix resurrects it\n\n\n\n\n\n\n\n\n\n\nThe authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct.\n\n\nOriginal conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression …\n\n\nRevised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression …\n\n\nSource: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/\n\n\n\nExample 4\nPNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/\n\n\n\n\n\n\n\n\n\n\n\n1.3.2 The reproducible data analysis process\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\n\nScripting and literate programming\nDonald Knuth “Literate Programming” (1983)\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do.\n\n\nThe ideas of literate programming have been around for many years!\nand tools for putting them to practice have also been around\nbut they have never been as accessible as the current tools\n\n\n\nReproducibility checklist\n\nAre the tables and figures reproducible from the code and data?\nDoes the code actually do what you think it does?\nIn addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?)\nCan the code be used for other data?\nCan you extend the code to do other things?\n\n\n\nTools: R & R Studio\nSee this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.be\n\nYou must use both R and RStudio software programs\nR does the programming\nR Studio brings everything together\nYou may use Pomona’s server: https://rstudio.pomona.edu/\nSee course website for getting started: http://research.pomona.edu/johardin/math154f19/\n\n\n\n\n\n\n\n\n\n\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\n\n\n\n\n\n\n\n\n\n\n\n\n\nJessica Ward, PhD student at Newcastle University\n\n\n\n\n\n\nTools: Git & GitHub\n\nYou must submit your assignments via GitHub\nFollow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/\nClass specific instructions at https://m154-comp-stats.netlify.app/github.html\n\nAdmittedly, there is a steep learning curve with Git. However, it is among the tools which you are most likely to use in your future endeavors, so spending a little time focusing on the concepts now may pay off big time in the future. Beyond practicing and working through http://happygitwithr.com/, you may want to read a little bit about waht Git is doing behind the scences. This reference: Learn git concepts, not commands is very good and accessible.\n\n\nTools: a GitHub merge conflict (demo)\n\nOn GitHub (on the web) edit the README document and Commit it with a message describing what you did.\nThen, in RStudio also edit the README document with a different change.\n\nCommit your changes\nTry to push \\(\\rightarrow\\) you’ll get an error!\nTry pulling\nResolve the merge conflict and then commit and push\n\nAs you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important.\n\n\n\n\n\n\n\n\n\n\nhttps://xkcd.com/1597/\n\n\n\n\n\n\nSteps for weekly homework\n\nYou will get a link to the new assignment (clicking on the link will create a new private repo)\n\nUse R (within R Studio)\n\nNew Project, version control, Git\n\nClone the repo using SSH\n\n\nIf it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd\n\nDo the assignment\n\ncommit and push after every problem\n\n\nAll necessary files must be in the same folder (e.g., data)",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-examples",
    "href": "01-intro.html#data-examples",
    "title": "1  Introduction1",
    "section": "1.4 Data Examples",
    "text": "1.4 Data Examples\n\nWhat can/can’t Data Science Do?\n\nCan model the data at hand!\nCan find patterns & visualizations in large datasets.\nCan’t establish causation.\nCan’t represent data if it isn’t there.\n\n\n\nStats / Data Science / Math are not apolitical/agnostic\n\n“Today, America has record energy production and we are energy independent.” (Kamala Harris, 7/18/24)\n“Due to the arrangements that I’ve reached with (Mexico) President Obrador, the number of migrants coming … to our shared border unlawfully in recent months has dropped dramatically.” (Joe Biden 6/4/24)\n“The average commute time for people who drive to work in (New York City) is 43 minutes, the longest in the nation.” (Carl Weisbrod 3/3/24)\n“The Trump administration `achieved the lowest African American unemployment rate and the lowest African American poverty rate ever recorded.’” (Donald Trump 6/15/24)\n“If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16)\n“Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16)\n\nSource: http://www.politifact.com/truth-o-meter/statements/\n\n\n1.4.1 College Rankings Systems\nCheating\n\nBucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html)\n\nGaming the system\n\nPoint by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/)\n\nNo way to measure “quality of education”\nWhat is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!).\n\n\n1.4.2 Trump and Twitter\nAnalysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone.\n\nAug 9, 2016 http://varianceexplained.org/r/trump-tweets/\n\n\nMy analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures.\n\n\nAug 9, 2017 http://varianceexplained.org/r/trump-followup/\n\n\nThere is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post.\n\n\nI’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it.\n\n\nI’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement.\n\n\n\n1.4.3 Can Twitter Predict Election Results?\n\nIn 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013)\nWhat makes using Tweets to predict elections relevant to our class? (See Baumer (2015).)\n\nThe data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet.\nThe analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets).\nThe dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed.\nThe researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses.\n\n\nActivity\nSpend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions.\n\nworking paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423\npublished in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449.\neditorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html\neditorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html\neditorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Hat\n\nWrite a sentence summarizing the findings of the paper.\nDiscuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently?\n\nshould be proportion for the response variable. The bizarre scaling could dramatically change the results\ndots could then be scaled in proportion to the number of tweets\nlinear fit may be questionable.\nHow would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement?\n\nInterpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units.\nDiscuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election?\n\n\\(R^2\\) is way higher after control variables are included, but duh!\nthe full model will likely do a better job of predicting\n\nWhy do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election?\n\nAfter controlling for how many Republicans are in the district, most of the effect disappears\nWhile the coefficient of the main term is still statistically significant, the size of the coefficient\n(155 +/- 43 votes) is of little practical significance\n\nDo you think the study holds water? Why or why not? What are the shortcomings of this study?\n\nNot really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model?\n\n\n\n\nData Scientist Hat\nImagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor.\n\nWhat steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform.\nWhat computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities & knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in?\n\nHow much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\nCheap\nCan measure any political race (not just the wealthy ones).\n\n\n\nDisadvantages\n\nIs it really reflective of the voting populace? Who would it bias toward?\nDoes simple mention of a candidate always reflect voting patterns? When wouldn’t it?\nMargin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\).\nTweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling?\nCan’t measure any demographic information.\n\n\n\nWhat could be done differently?\n\nGelman: look only at close races\nGelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.”\nGelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.”\nGelman: Make the data publicly available so that others can try to reproduce the results\n\n\n\nTweeting and R\nThe twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions.\n\n\n\nProbability vs. Statistics\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\nStitch Fix Algorithms Tour\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nhttps://xkcd.com/1597/\n\n\n\nBaumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” The American Statistician.\n\n\nDiGrazia, Joseph, Karissa McKelvey, Johan Bollen, and Fabio Rojas. 2013. “More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior.” PLoS ONE 8 (11): e79449.\n\n\nKitzes, Justin, Daniel Turek, and Fatma Deniz, eds. 2018. In The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. University of California Press.\n\n\nRojas, Fabio. 2013. “How Twitter Can Predict and Election.” The Washington Post.",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "p2-inference.html",
    "href": "p2-inference.html",
    "title": "Data Inference",
    "section": "",
    "text": "more soon",
    "crumbs": [
      "Data Inference"
    ]
  },
  {
    "objectID": "p3-machinelearn.html",
    "href": "p3-machinelearn.html",
    "title": "Data Modeling",
    "section": "",
    "text": "more soon",
    "crumbs": [
      "Data Modeling"
    ]
  },
  {
    "objectID": "10-misc.html",
    "href": "10-misc.html",
    "title": "10  Misc",
    "section": "",
    "text": "10.1 11/26/19 Agenda",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#Nov26",
    "href": "10-misc.html#Nov26",
    "title": "10  Misc",
    "section": "",
    "text": "API / authenticating\nparallel computing\ncloud computing\n\nreticulate (Python in R!)\nSQL",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#api",
    "href": "10-misc.html#api",
    "title": "10  Misc",
    "section": "\n10.2 API",
    "text": "10.2 API\n\n\n\n\n\n\n\n\nxkcd, https://xkcd.com/1481/\n\n\n\n\nWhat is an API? (Application Programming Interface)\nThink of an API as a restaurant menu. The menu provides a list of what the restaurant has to offer, and you order off the menu by choosing the dish that you want. After you order, the restaurant figures out how to bring the food from the kitchen to your table in the way that you’ve specified.\nAn API is an intermediary that allows two applications to talk to one another. It is not the database or the server, instead it is the code that allows communication.\nExamples of APIs\n\nWhen you use an app on your phone, the app connects to the internet and sends information to a server somewhere. The server retrieves the data, interprets it, does what it does, and sends it back to you. The application which takes the data from the server and presents it to you in a readable way is an API.\nLet’s say you are booking a flight on United. You choose all the details, you interact with the airline’s website. BUT INSTEAD, what if you are interacting with a software like Expedia? Then Expedia has to talk to United’s API to get all the information about available flights, costs, seats, etc.\nIf you’ve ever been to a third party site and clicked on “Share on Facebook” or “Share on Twitter” your third party site is communicating with the Facebook API or the Twitter API.\nYou sign up to go to a concert, and StubHub asks whether you want to add the concert to your Google calendar. StubHub needs to talk to Google via Google’s API.\nWhat if you want some Twitter data? How might you get it? Well, you could email Twitter and ask someone for it. Instead Twitter provides information about how their data is stored, and allows you to query their data in an automated way.\n\n\n\n\n\n\n\n\nImage taken from https://rigor.com/blog/what-is-an-api-a-brief-intro\n\n\n\n\n10.2.1 Authenticating\n\n\nAuthenticating is stating who you are\n\nAuthorization is asking for access to a resources (and happens after authentication)\n\nDO NOT post your credentials and keys to a public GitHub repo!!\nIn almost all cases, in order to communicate with an API, you must tell the API who you are and that you should have access to the information the API is providing.\n\n\n\n\n\n\n\nImage taken from https://blog.restcase.com/restful-api-authentication-basics/",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#parallel-computing",
    "href": "10-misc.html#parallel-computing",
    "title": "10  Misc",
    "section": "\n10.3 Parallel Computing",
    "text": "10.3 Parallel Computing\n(Taken from the Teach Data Science blog: https://teachdatascience.com/parallel/)\nTo demonstrate what parallel computing is, we’ll perform tasks that are embarrassingly parallel which means there is no dependency or communication between the parallel tasks. Again, parallel computing can be powerful in ways that link computational tasks in complicated ways. But we believe that as a first pass at teaching parallel computing, we should teach the parallel structure before bringing in dependence across the parallel tasks. Examples of embarrassingly parallel algorithms include: Monte Carlo analysis, bootstrapping, growing trees for Random Forests, group_by analyses, and cross-validation. Additionally, data science methods increasingly use randomized algorithms which can often be written in parallel.\nIndeed, it isn’t always easy to know when to use a parallel construction. Because of existing overhead processes (e.g., copying data across many threads, bring results together, etc.) an algorithm run on 10 parallel strands will not reduce an original (non-parallel) run time by 10-fold. Figuring out when a parallel implementation is appropriate is beyond the scope of this blog but should be carefully considered before embarking on large projects.\nSome parallel examples\nBefore running code in parallel, it is valuable to know how many cores your computer has to work with. Note that the detectCores function will provide information about the specific device you are using (logical = FALSE tells you only the physical cores which is likely what you want). Note that after makeCluster the separate threads have information. [The argument setup_strategy = \"sequential\" is a fix for working with R 4.0+.] After stopCluster, the code is no longer connecting to the cluster structure.\n\nlibrary(parallel)\nP &lt;- detectCores(logical=FALSE)\nP\n\n[1] 11\n\ncl &lt;- makeCluster(P, setup_strategy = \"sequential\")\ncl[[1]]\n\nnode of a socket cluster on host 'localhost' with pid 25520\n\nstopCluster(cl)\ncl[[1]]\n\nError in summary.connection(connection): invalid connection\n\n\nEmbarrassingly embarrassing example\nIn the example below, we generate some Cauchy data and find the max of each sample. Note that for the current device there are 11 cores, so the process will happen 100/P = 9.0909091 times on each core. The second argument of clusterApply is a sequence of numbers that gets passed to each worker as the (first) argument of func1. Below, I’ve specified that the value 50 (number of reps) should be passed separately to 100 different workers.\n\nW &lt;- 100\nP &lt;- parallel::detectCores(logical=FALSE)\ncl &lt;- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\nfunc1 &lt;- function(reps){\n  max(rcauchy(reps))\n}\n\n\nclusterApply(cl, rep(50,W), fun = func1) |&gt; head(3)\n\n[[1]]\n[1] 78.81132\n\n[[2]]\n[1] 504.0628\n\n[[3]]\n[1] 43.64679\n\nstopCluster(cl)\n\nThere are many R functions which implement parallel processing. For example, the same code from above can be processed using foreach.\n\nlibrary(doParallel)\ncl &lt;- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\ndoParallel::registerDoParallel(cl)\nforeach(reps = rep(50, 100), .combine = 'c') %dopar% {\n  max(rcauchy(reps))\n       } |&gt; head(3)\n\n[1] 11.64233 28.13474 35.99419\n\nstopCluster(cl)\n\nExample bootstrapping\nA slightly less embarrassingly parallel example comes with bootstrapping. Below we have used parallel implementation to bootstrap the mean of the iris data petal length (Virginica only).\n\ndata(iris)\n\niris_bs &lt;- iris |&gt;\n  filter(Species == \"virginica\") |&gt;\n  select(Petal.Length)\n\n\ncl &lt;- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\ndoParallel::registerDoParallel(cl)\nbsmean_PL &lt;- foreach(i = 1:100, .combine = 'c') %dopar% {\n  mean(sample(iris_bs$Petal.Length, replace = TRUE))\n}\nbootstrap &lt;- tibble(bsmean_PL)\nstopCluster(cl)\n\nggplot(bootstrap, aes(x = bsmean_PL)) + geom_histogram(bins = 25) + labs(title = \"Histogram of 100 Bootstrapped Means using foreach\")\n\n\n\n\n\n\n\n\n10.3.1 Spark and sparklyr\n\nSome of you may be familiar with Apache Spark which is an open-source product for distributed cluster-computing. You may want to learn more about its capabilities, including scheduling workflow, dispatching tasks, and consolidating end results. While incredibly powerful, there has historically been a steep learning curve to getting R to work smoothly with a Spark connection. Recently, RStudio has come out with a new package sparklyr which integrates R and Spark seamlessly. Note that in the example below, we’ve set up a local connection just for the purposes of the example. For your work, you may want to connect to a cluster or cloud space with many cores.\nThe RStudio sparklyr webpage provides a plethora of good examples demonstrating the sophistication and power of the technology. sparklyr has particularly strong connections to the suite of tidyverse functions. Indeed, the power of sparklyr is more about distributing the computing than about parallelizing it. For example, with sparklyr the computations are delayed until you need the results. Additionally, Spark is doing the heavy lifting and only at the very end (when your results are called) do you need to worry about the size of the table, results, or computational space. The example below repeats the bootstrapping work that was done previously.\nNote, it is important to look at your data structures and variables names. For example, when copying the local dataframe iris_samps to the remote data source called iris_samps_tbl, the variable Petal.Length was changed to Petal_Length.\n\nlibrary(sparklyr)\nsparklyr::spark_install()\n\nsc &lt;- spark_connect(master = \"local\")\n\nn_sim = 100\niris_samps &lt;- iris |&gt; dplyr::filter(Species == \"virginica\") |&gt;\n  sapply(rep.int, times=n_sim) |&gt; cbind(replicate = rep(1:n_sim, each = 50)) |&gt; \n  data.frame() |&gt;\n  dplyr::group_by(replicate) |&gt;\n  dplyr::sample_n(50, replace = TRUE)\n\niris_samps_tbl &lt;- copy_to(sc, iris_samps)\n\niris_samps_tbl |&gt; \n  spark_apply(function(x) {mean(x$Petal_Length)}, \n    group_by = \"replicate\") |&gt;\n  ggplot(aes(x = result)) + geom_histogram(bins = 20) + labs(title = \"Histogram of 100 Bootstrapped Means using sparklyr\")\n\nspark_disconnect(sc)\n\nFor our particular application, the adept reader has probably noticed that the average of a variable using group_by is a very quick and easy task for dplyr. Indeed, the use of sparklyr above is overkill and is presented only as a way to demonstrate using sparklyr. If you are working with big datasets that require large computing infrastructure, the RStudio help pages on sparklyr are fantastic. Additionally, there are many instances of working with Spark in the wild, and you might consider working through someone else’s Spark analysis like this fantastic example on splitting up large amounts of raw DNA sequencing to get data for a given genetic location.\n\niris_samps |&gt; dplyr::group_by(replicate) |&gt;\n  dplyr::summarize(result = mean(Petal.Length)) |&gt;\n  ggplot(aes(x = result)) + geom_histogram(bins = 25) + labs(title = \"Histogram of 100 Bootstrapped Means using dplyr\")\n\nWhile an introduction to parallel and cloud computing will help you become more adept and less apprehensive about using the tools, there is also a recognition that sufficient background in computer science is needed to be able to fully engage with principles of high performance computing.\nLearn more\n\nHana Sevcikova Introduction to parallel computing with R useR 2017 in Brussels, tutorial here\n\n\nsparklyr to do parallel cross-validation\n\nhttps://www.rstudio.com/resources/cheatsheets/\n\nhttps://www.rstudio.com/resources/cheatsheets/#sparklyr\n\nhttps://github.com/rstudio/cheatsheets/raw/master/parallel_computation.pdf\n\n\nGreat blog Two Flavors of Parallel Simulation by Mark LeBoeuf comparing different ways to process code in parallel.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#cloud-computing",
    "href": "10-misc.html#cloud-computing",
    "title": "10  Misc",
    "section": "\n10.4 Cloud Computing",
    "text": "10.4 Cloud Computing\nA great overview on high performance computing (HPC) what it is and what it isn’t is given here: https://www.slideshare.net/raamana/high-performance-computing-with-checklist-and-tips-optimal-cluster-usage\n\n\n\n\n\n\n\nImage from Pradeep Redddy Raamana High performance computing tutorial, with checklist and tips to optimize cluster usage\n\n\n\n\n\n\n\n\nImage from Pradeep Redddy Raamana High performance computing tutorial, with checklist and tips to optimize cluster usage\n\n\n\n(The rest, below, is taken from the Teach Data Science blog: https://teachdatascience.com/cloud2/, this entry written by Nick Horton)\nThe R package parallel is designed to send tasks to each of multiple cores. Today’s computers (even small laptops!) typically have multiple cores, and any server or cloud computing infrastructure can easily handle dozens or hundreds of parallel tasks. The structure of the R parallel implementation sends tasks to workers that don’t talk to one another until compiling their results at the end. In her 2017 UseR! tutorial, Hana Sevcikova describes the function of workers which run code/functions/iterations separately before results are subsequently combined.\n\n\n\n\n\n\n\nImage from Sevcikova UseR! 2017 tutorial on parallel computing\n\n\n\nAs computing infrastructure becomes more sophisticated, it is important to have the language to describe how connected components work. Parallel processing allows for a conversation on the differences between distributed computing, cluster computing, and grid computing, and generally, the framework of high performance computing. The benefit of parallel computing as an introduction to the larger infrastructure is that the task of each worker is clear, important, and easy to describe.\nThis discussion is motivated by several recent papers and blog posts that describe how complex, real-world data science computation can be structured in ways that would not have been feasible in past years without herculean efforts. It is worth noting the fantastic example that described multiple iterations needed to parse huge amounts of raw DNA sequencing data to undertake analyses for a given set of genetic locations. In “Ambitious data science can be painless” Monajemi et al. describe workflows that take advantage of new software stacks to undertake massive cloud-based experiments. While a few years older, Chamandy et al.’s Teaching statistics at Google scale described three examples where modern data challenges were overcome with creative statistical thinking (see companion report on Estimating uncertainty for massive data streams ). Finally, the NSF-funded workshop report on “Enabling computer and information science and engineering research and education in the cloud” highlights opportunities as university computing migrates to cloud solutions more and more.\nAnd last, you may enjoy reading the recent Three strategies for working with Big Data in R blog post.\nHow can we prepare for cloud computing in an undergraduate course?\nGetting started\nWhat are the steps to exploring cloud-based systems? Each of the main cloud providers have active educational outreach programs.\n\nGoogle Compute Platform allows faculty to apply to receive $100 in GCP credits and $50 per student. Credits can be used in courses, student clubs, and other faculty-sponsored events. (To replicate our example later in this blog, you’ll want to set up an account and request credits.)\nAzure for Education provides access for educators to open source content for classes and $200 in Azure credits, plus free services.\nAmazon Web Services Educate provides between $75 and $200 in AWS annual credits per educator (depending on membership status) and between $30 and $100 for students.\n\nYou should sign up and start to explore! The world of cloud computing is quickly changing. By gaining experience through investment in time in learning these tools will help instructors provide guidance to their students in use of these modern computational tools.\nAn example: BigQuery in Google’s GCP\nConsider an example using GCP (kudos to Shukry Zablah for his assistance).\nBigQuery is Google’s serverless, highly-scalable, cloud data warehouse. A quickstart document is available which discusses use of the web user interface and the GCP console as well as access through an API interface. The bigrquery package in R makes it easy to work with data stored in Google BigQuery through queries to BigQuery tables.\nThe first step is to request GCP credits (see above) and use the online interface to create a project (below called “Test Project for Blog”).\n\nlibrary(dplyr)\nlibrary(bigrquery)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(purrr)\nlibrary(readr)\n\n\nprojectId &lt;- \"bigquery-public-data\"  # replace with your own project\nbillingId &lt;- \"test-project-for-blog\" # replace with your own billing ID\ndatasetName &lt;- \"samples\"\ntableName &lt;- \"wikipedia\"\n\nBigQuery includes a number of public datasets. Below is an analysis of the public dataset of the revisions of Wikipedia articles up to April 2010, hosted in GCP BigQuery. The size of the table is 35.69GB. The queries take only seconds to run.\n\nquery &lt;- \"SELECT  title, COUNT(title) as n\n          FROM `bigquery-public-data.samples.wikipedia` \n          GROUP BY title\n          ORDER BY n DESC\n          LIMIT 500\"\n\nFor safety, always try to make sure that your queries have the LIMIT set on your queries.\n\nmostRevisions_tb &lt;- \n  bigrquery::bq_project_query(x = billingId, \n    query = query) #creates temporary table\n\nWhen the previous bq_project_query() function is run within RStudio, a connection is made to Google (GCP) and an authentication window will open up in a local browser.\nAll the heavy lifting we perform is done on the database end (note that we are billed for it, though the first 1TB of accesses are free). The local machine only receives the data once we try to display it. Right now mostRevisions_tb is just a reference to a temporary table online. The query accessed 7GB of data.\nWe can get a copy of the data on our local machine once we are confident that it is what we want.\n\nmostRevisions &lt;- bq_table_download(mostRevisions_tb) \n\n\nglimpse(mostRevisions)\n\nRows: 500\nColumns: 2\n$ title &lt;chr&gt; \"Wikipedia:Administrator intervention against vandalism\", \"Wikip…\n$ n     &lt;int&gt; 643271, 419695, 326337, 257893, 226802, 204469, 191679, 186715, …\n\n\n\nclean &lt;- mostRevisions |&gt; \n  filter(!grepl(\"Wikipedia|User|Template|Talk\", title)) |&gt;\n  mutate(title = fct_reorder(title, n)) |&gt; #to sort levels\n  glimpse()\n\nRows: 272\nColumns: 2\n$ title &lt;fct&gt; \"George W. Bush\", \"List of World Wrestling Entertainment employe…\n$ n     &lt;int&gt; 43652, 30572, 27433, 23245, 21768, 20814, 20546, 20529, 20225, 2…\n\n\nLet’s plot the top 10 entries.\n\nggplot(clean |&gt; head(10), aes(x = title, y = n, fill = n)) + \n  geom_bar(stat = \"identity\") + \n  labs(x = \"Article Title\",\n       y = \"Number of Revisions\",\n       title = \"Most Revised Wikipedia Articles (Up to April 2010)\") +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkred\", guide = FALSE) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 20, hjust = 1)) \n\n\n\n\n\n\n\nWe’ve obviously just scratched the surface here. There are lots of other examples out there to consider replicating in your classroom (e.g., returning tweets on a schedule). Hopefully you are intrigued enough to request some credits for you and your students and start to explore. Not sure where to begin? Check out the GCP Essentials Videos series.",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#reticulate",
    "href": "10-misc.html#reticulate",
    "title": "10  Misc",
    "section": "\n10.5 reticulate\n",
    "text": "10.5 reticulate\n\n(Taken from the Teach Data Science blog: https://teachdatascience.com/reticulate/)\nConnect to Python within RStudio\nFor many statisticians, the go-to software language is R. However, there is no doubt that Python is a very important language in data science. Why not do both??\n\nlibrary(tidyverse)\nlibrary(reticulate)\nuse_virtualenv(\"r-reticulate\")\nreticulate::import(\"statsmodels\")\n\nI can run Python inside Quarto??\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npandas for data wrangling.\n\nIn R, the chunk is specified to be a Python chunk (RStudio is now running Python).\n\n```{python}\nimport pandas\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[flights[\"dest\"] == \"ORD\"]\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights = flights.dropna()\n```\nA view of the Python chunk which is actually run:\n\nimport pandas\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[flights[\"dest\"] == \"ORD\"]\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights = flights.dropna()\n\nLearn about the dataset\n```{python}\nflights.shape\nflights.head(3)\nflights.describe()\n```\n\nflights.shape\nflights.head(3)\nflights.describe()\n\nComputations using pandas\n\n```{python}\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights.groupby(\"carrier\").mean()\n```\n\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights.groupby(\"carrier\").mean()\n\nFrom Python chunk to R chunk\n\n\npy$x accesses an x variable created within Python from R\n\nr.x accesses an x variable created within R from Python\n\n\nlibrary(ggplot2)\nggplot(py$flights, \n       aes(x=carrier, \n           y=arr_delay)) + \n  geom_point() + \n  geom_jitter()\n\nFrom R chunk to Python chunk\n\ndata(diamonds)\nhead(diamonds)\n\nPython chunks\nNote that we’re calling Python code on an R object.\n\nprint(r.diamonds.describe())\n\n\nimport statsmodels.formula.api as smf\nmodel = smf.ols('price ~ carat', data = r.diamonds).fit()\nprint(model.summary())\n\nRunning just Python\n\n\n\n\n\n\n\n\nFull disclosure\nreticulate is not always trivial to set up. Indeed, I’ve had trouble figuring out which Python version is talking to R and where different module versions live.\nLearn more\n\nRStudio R Interface to Python\n\nhttps://rstudio.github.io/reticulate/\n\nRStudio blog on Reticulated Python\n\nhttps://blog.rstudio.com/2018/10/09/rstudio-1-2-preview-reticulated-python",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#sql-in-r",
    "href": "10-misc.html#sql-in-r",
    "title": "10  Misc",
    "section": "\n10.6 SQL (in R)",
    "text": "10.6 SQL (in R)\nNote that there exists an R interface to work with SQL commands from within an R Markdown file. For consistency with the class notes, we’ve continued to use the R Markdown structure to demonstrate the course material.\n(Taken from the Teach Data Science blog: https://teachdatascience.com/sql/, this entry written by Nick Horton)\nSQL (pronounced sequel) stands for Structured Query Language; it is a language designed to manage data in a relational database system.\nWe will use a public facing MySQL database containing wideband acoustic immittance (WAI) measures made on normal ears of adults. (The project is funded by the National Institutes of Health, NIDCD, and hosted on a server at Smith College, PI Susan Voss, R15 DC014129-01.) The database was created to enable auditory researchers to share WAI measurements and combine analyses over multiple datasets.\nWe begin by demonstrating how SQL queries can be sent to a database. It is necessary to set up a connection using the dbConnect() function.\n\nlibrary(mosaic)\nlibrary(RMySQL)  \ncon &lt;- dbConnect(\n  RMySQL::MySQL(), host = \"scidb.smith.edu\", user = \"waiuser\", \n  password = \"smith_waiDB\", dbname = \"wai\")\n\nNext a series of SQL queries can be sent to the database using the DBI::dbGetQuery() function: each query returns an R dataframe.\n\nclass(dbGetQuery(con, \"SHOW TABLES\"))\n\nThere are multiple tables within the wai database.\n\ndbGetQuery(con, \"SHOW TABLES\")\n\nThe EXPLAIN command describes the ten field names (variables) in the PI_Info table.\n\ndbGetQuery(con, \"EXPLAIN PI_Info\")\n\nThe SELECT statement can be used to select all fields for eight observations in the Measurements table.\n\neightobs &lt;- dbGetQuery(con, \"SELECT * FROM Measurements LIMIT 8\")\neightobs\n\nMore interesting and complicated SELECT calls can be used to undertake grouping and aggregation. Here we calculate the sample size for each study\n\ndbGetQuery(con, \n  \"SELECT Identifier, count(*) AS NUM FROM Measurements GROUP BY Identifier ORDER BY NUM\")\n\nAccessing a database using dplyr commands\nAlternatively, a connection can be made to the server by creating a series of dplyr tbl objects. Connecting with familiar dplyr syntax is attractive because, as Hadley Wickham has noted, SQL and R have similar syntax (but sufficiently different to be confusing).\nThe setup process looks similar.\n\nMeasurements &lt;- tbl(con, \"Measurements\")\nclass(Measurements)\nPI_Info &lt;- tbl(con, \"PI_Info\")\nSubject &lt;- tbl(con, \"Subjects\")\n\nWe explore the PI_Info table using the collect() function used to force computation on the database (and return the results). One attractive aspect of database systems is that they feature lazy evaluation, where computation is optimized and postponed as long as possible.\n\nPI_Info |&gt; collect() |&gt; data.frame()   \n# be careful with collect() when dealing with large tables!\n\nNote how the number of rows is unknown (?? at the top of the output above) for the lazy query.\nSimilarly, we can explore the Subjects table.\n\n#Subject  |&gt; summarise(total = n())\nSubject |&gt; collect()  # be careful with collect() with large tables!\n\nLet’s explore the Measurements table.\n\n#Measurements |&gt; summarise(total = n())\nMeasurements |&gt; collect()\n\nThere are more than a quarter million observations.\nIn the next step, we will download the data from a given subject for a specific study, in this case a paper by Rosowski et al. (2012) entitled “Ear-canal reflectance, umbo velocity, and tympanometry in normal-hearing adults”.\nArbitrarily we choose to collect data from subject number three.\n\nonesubj &lt;- \n  Measurements |&gt; \n  filter(Identifier == \"Rosowski_2012\", Sub_Number == 3) |&gt;\n  collect |&gt;\n  mutate(SessionNum = as.factor(Session))\nhead(onesubj)\n\nFinally we can display the results of the measurements as a function of frequency and which ear (left or right) that was used.\n\nonesubj &lt;- mutate(onesubj, Ear = ifelse(Left_Ear == 1, \"Left\", \"Right\"))\nggplot(onesubj, aes(x = Freq, y = Absorbance)) + geom_point() +\n  aes(colour = Ear) + scale_x_log10() + labs(title=\"Absorbance by ear Rosowski subject 3\")\n\nAlways a good idea to terminate the SQL connection.\n\ndbDisconnect(con)\n\nWe note that a number of relational database systems exist, including MySQL (illustrated here), PostgreSQL, and SQLite. More information about databases within R can be found in the CRAN Databases with R Task View.\nSetting up and managing a database is a topic for a different day: here we focused on how SQL can be used within R to access data in a flexible and powerful manner.\nLearn more\n\nhttps://chance.amstat.org/2015/04/setting-the-stage/ (Setting the stage for data technologies)\nhttps://www.w3schools.com/sql/sql_intro.asp (Intro to SQL)\nhttp://www.science.smith.edu/wai-database/home/about/ (WAI SQL Database)\nhttps://cran.r-project.org/web/views/Databases.html (CRAN Task View on Databases with R)\nhttps://db.rstudio.com (RStudio Database resources)\nhttps://dbplyr.tidyverse.org/articles/dbplyr.html (dbplyr package)",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "10-misc.html#regexpr",
    "href": "10-misc.html#regexpr",
    "title": "10  Misc",
    "section": "\n10.7 Regular Expressions",
    "text": "10.7 Regular Expressions\n\nA regular expression … is a sequence of characters that define a search pattern. Usually such patterns are used by string searching algorithms for “find” or “find and replace” operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory. [From https://en.wikipedia.org/wiki/Regular_expression]\n\nMain tasks in character matching:\n\nbasic string operations\npattern matching (regular expressions)\nsentiment analysis\n\nThe ideas below are mostly taken from Jenny Bryan’s STAT545 class: https://github.com/STAT545-UBC/STAT545-UBC-original-website/blob/master/block022_regular-expression.rmd\nR packages to make your life easier\n\n\nstringr package A core package in the tidyverse. It is installed via install.packages(\"tidyverse\") and also loaded via library(tidyverse). Of course, you can also install or load it individually.\n\nMany of the main functions start with str_. Auto-complete is your friend.\nReplacements for base functions re: string manipulation and regular expressions (see below).\nMain advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task.\n\nstringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf\n\n\n\ntidyr package Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract().\nBase functions: nchar(), strsplit(), substr(), paste(), paste0().\nThe glue package is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package.\nString functions related to regular expression\nRegular expression is a pattern that describes a specific set of strings with a common structure. It is heavily used for string matching / replacing in all programming languages, although specific syntax may differ a bit. It is truly the heart and soul for string operations. In R, many string functions in base R as well as in stringr package use regular expressions, even Rstudio’s search and replace allows regular expression:\n\nidentify match to a pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()\n\nextract match to a pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()\n\nlocate pattern within a string, i.e. give the start position of matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()\n\nreplace a pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()\n\nsplit a string using a pattern: strsplit(), stringr::str_split()\n\n\nRegular expressions typically specify characters (or character classes) to seek out, possibly with information about repeats and location within the string. This is accomplished with the help of metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. We will use some small examples to introduce regular expression syntax and what these metacharacters mean.\nEscape sequences\nThere are some special characters in R that cannot be directly coded in a string. For example, let’s say you specify your pattern with single quotes and you want to find countries with the single quote '. You would have to “escape” the single quote in the pattern, by preceding it with \\, so it is clear that it is not part of the string-specifying machinery.\nThere are other characters in R that require escaping, and this rule applies to all string functions in R, including regular expressions. See here for a complete list of R escape sequences.\n\n\n\\': single quote. You don’t need to escape single quote inside a double-quoted string, so we can also use \"'\" in the previous example.\n\n\n\\\": double quote. Similarly, double quotes can be used inside a single-quoted string, i.e. '\"'.\n\n\n\\n: newline.\n\n\n\\r: carriage return.\n\n\n\\t: tab character.\n\n\nNote: cat() and print() handle escape sequences differently, if you want to print a string out with these sequences interpreted, use cat().\n\n\nprint(\"a\\nb\")\n\n[1] \"a\\nb\"\n\ncat(\"a\\nb\")\n\na\nb\n\n\nQuantifiers\nQuantifiers specify how many repetitions of the pattern.\n\n\n*: matches at least 0 times.\n\n\n+: matches at least 1 times.\n\n\n?: matches at most 1 times.\n\n\n{n}: matches exactly n times.\n\n\n{n,}: matches at least n times.\n\n\n{n,m}: matches between n and m times.\n\n\n(strings &lt;- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\"))\n\n[1] \"a\"      \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = TRUE)\n\n[1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = FALSE)\n\n[1] 2 3 4 5 6\n\ngrep(\"ac+b\", strings, value = TRUE)\n\n[1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac?b\", strings, value = TRUE)\n\n[1] \"ab\"  \"acb\"\n\ngrep(\"ac{2}b\", strings, value = TRUE)\n\n[1] \"accb\"\n\ngrep(\"ac{2}b\", strings, value = FALSE)\n\n[1] 4\n\ngrep(\"ac{2,}b\", strings, value = TRUE)\n\n[1] \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac{2,3}b\", strings, value = TRUE)\n\n[1] \"accb\"  \"acccb\"\n\n\nPosition of pattern within the string\n\n\n^: matches the start of the string.\n\n\n$: matches the end of the string.\n\n\n\\b: matches the empty string at either edge of a word. Don’t confuse it with ^ $ which marks the edge of a string.\n\n\n\\B: matches the empty string provided it is not at an edge of a word.\n\n\n(strings &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\"))\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"ab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"abcd\"\n\ngrep(\"ab$\", strings, value = TRUE)\n\n[1] \"cdab\"\n\ngrep(\"\\\\bab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"c abd\"\n\n\nOperators\n\n\n.: matches any single character, as shown in the first example.\n\n[...]: a character list, matches any one of the characters inside the square brackets. We can also use - inside the brackets to specify a range of characters.\n\n\n[^...]: an inverted character list, similar to [...], but matches any characters except those inside the square brackets.\n\n\n\\: suppress the special meaning of metacharacters in regular expression, i.e. $ * + . ? [ ] ^ { } | ( ) \\, similar to its usage in escape sequences. Since \\ itself needs to be escaped in R, we need to escape these metacharacters with double backslash like \\\\$.\n\n\n|: an “or” operator, matches patterns on either side of the |.\n\n\n(...): grouping in regular expressions. This allows you to retrieve the bits that matched various parts of your regular expression so you can alter them or use them for building up a new string. Each group can than be refer using \\\\N, with N being the No. of (...) used. This is called backreference.\n\n\n(strings &lt;- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\"))\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab.\", strings, value = TRUE)\n\n[1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab[c-e]\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\" \"abe\"\n\ngrep(\"ab[^c]\", strings, value = TRUE)\n\n[1] \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"\\\\^ab\", strings, value = TRUE)\n\n[1] \"^ab\"\n\ngrep(\"abc|abd\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\"\n\ngsub(\"(ab) 12\", \"\\\\1 34\", strings)\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 34\"\n\n\nCharacter classes\nCharacter classes allow specifying entire classes of characters, such as numbers, letters, etc. There are two flavors of character classes, one uses [: and :] around a predefined name inside square brackets and the other uses \\ and a special character. They are sometimes interchangeable.\n\n\n[:digit:] or \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent to [0-9].\n\n\n\\D: non-digits, equivalent to [^0-9].\n\n\n[:lower:]: lower-case letters, equivalent to [a-z].\n\n\n[:upper:]: upper-case letters, equivalent to [A-Z].\n\n\n[:alpha:]: alphabetic characters, equivalent to [[:lower:][:upper:]] or [A-z].\n\n\n[:alnum:]: alphanumeric characters, equivalent to [[:alpha:][:digit:]] or [A-z0-9].\n\n\n\\w: word characters, equivalent to [[:alnum:]_] or [A-z0-9_].\n\n\n\\W: not word, equivalent to [^A-z0-9_].\n\n\n[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f, equivalent to [0-9A-Fa-f].\n\n[:blank:]: blank characters, i.e. space and tab.\n\n\n[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\n\n\\s: space, .\n\n\n\\S: not space.\n\n\n[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~.\n\n[:graph:]: graphical (human readable) characters: equivalent to [[:alnum:][:punct:]].\n\n[:print:]: printable characters, equivalent to [[:alnum:][:punct:]\\\\s].\n\n[:cntrl:]: control characters, like \\n or \\r, [\\x00-\\x1F\\x7F].\n\nNote:\n* [:...:] has to be used inside square brackets, e.g. [[:digit:]].\n* \\ itself is a special character that needs escape, e.g. \\\\d. Do not confuse these regular expressions with R escape sequences such as \\t.\nstringr\nIn many cases, you will want to use the incredibly useful and tidy set of functions available in the stringr package. (stringr is a core package in the tidyverse.) For example, below we’ve extracted the first (and then last) word as a character string from the StreetName variable.\n\n\nstringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf\n\n\nlibrary(Stat2Data)\ndata(RailsTrails)\nRailsTrails &lt;- RailsTrails |&gt; \n  select(HouseNum, Bedrooms, Price2014, StreetName) \nRailsTrails |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName\n1        1        3   210.729 Acrebrook Drive\n2        2        3   204.171       Autumn Dr\n3        3        3   338.662     Bridge Road\n4        4        3   276.250     Bridge Road\n5        5        4   169.173     Bridge Road\n6        6        3   211.487 Brierwood Drive\n\nRailsTrails |&gt;\n  mutate(first_piece = stringr::word(StreetName, start = 1)) |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName first_piece\n1        1        3   210.729 Acrebrook Drive   Acrebrook\n2        2        3   204.171       Autumn Dr      Autumn\n3        3        3   338.662     Bridge Road      Bridge\n4        4        3   276.250     Bridge Road      Bridge\n5        5        4   169.173     Bridge Road      Bridge\n6        6        3   211.487 Brierwood Drive   Brierwood\n\nRailsTrails |&gt;\n  mutate(last_piece = stringr::word(StreetName, start = -1)) |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName last_piece\n1        1        3   210.729 Acrebrook Drive      Drive\n2        2        3   204.171       Autumn Dr         Dr\n3        3        3   338.662     Bridge Road       Road\n4        4        3   276.250     Bridge Road       Road\n5        5        4   169.173     Bridge Road       Road\n6        6        3   211.487 Brierwood Drive      Drive\n\n\nAn example from my work\nBelow are a handful of string characters that represent genomic sequences which were measured in an RNA Sequencing dataset. The task below is to find intergenic regions (IGR) and identify which coding sequences (CDS) bookend the intergenic regions. Note that IGRs do not code for proteins while CDSs do. Additionally, AS refers to anti-sense which identifies the genomic sequence in the opposite orientation (e.g., CGGATCC vs CCTAGGC). [The code below was written by Madison Hobbs, Scripps ’19.]\nThe names of the genomic pieces\n\nallCounts &lt;- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\n\nFirst, it is important to identify which are IGR, CDS, and anti-sense.\n\nallCounts &lt;- allCounts |&gt; tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts\n\n  feature\n1     CDS\n2     CDS\n3     IGR\n4  AS_IGR\n5     IGR\n                                                                                                                                                                                       rest\n1                                                                                                                                                                                     b2743\n2                                                                                                                                                                                     b2764\n3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                                                                                                                     GeneidBackup\n1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nWe keep only the IGR and AS_IGR strings, and we separate the two bookends. Note, the separation comes at the backslash.\n\nigr &lt;- allCounts |&gt; filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr &lt;- igr |&gt; tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)\n\n[1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nFor each of the two bookend Genes, we need to separate out the feature from the rest. Note that we write over feature1 in the second line of code below. Both of the bookends for all sequences are CDS elements.\n\nigr$feature1 &lt;- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 &lt;- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 &lt;- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)\n\n[1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2\n1      CDS      CDS\n2      CDS      CDS\n3      CDS      CDS\n\n\nAs CDS, it is now important to find the actual genenames for each of the IGR sequences. We also keep each element’s bnum which represents a unique gene identifier in E. coli.\nbnum, genename, rna.name act as place holders for the types of elements that we will need to identify the bookends of the IGRs.\n\nbnum = \"b[0-9]{4}\"\nbnum\n\n[1] \"b[0-9]{4}\"\n\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\n\n\nigr$start.gene &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr &lt;- igr |&gt; tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma) |&gt; \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma)\nnames(igr)\n\n [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"  \n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2 start.gene end.gene start.bnum end.bnum\n1      CDS      CDS        mlc     ynfL      b1594    b1595\n2      CDS      CDS       talB      mog      b0008    b0009\n3      CDS      CDS       yoaA     yoaB      b1808    b1809\n\n\nHelpful tutorials/files\n\n\nstringr vignette: https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html\nstringr package\nJenny Bryan’s STAT 545 notes: https://stat545.com/character-vectors.html\nJenny Bryan’s STAT 545 lab: http://stat545.com/block022_regular-expression.html\nHadley Wickham’s book R for Data Science\n\n\nregexpal\n\n\nRegExr\n\nRegular expression in R official document.\nFun examples\n\nThe name Hilary: https://hilaryparker.com/2013/01/30/hilary-the-most-poisoned-baby-name-in-us-history/\nTrump’s tweets: http://varianceexplained.org/r/trump-tweets/\nTrump’s tweets, take two: http://varianceexplained.org/r/trump-followup/\n\n\n\n\nxkcd, https://xkcd.com/1481/\nImage taken from https://rigor.com/blog/what-is-an-api-a-brief-intro\nImage taken from https://blog.restcase.com/restful-api-authentication-basics/\nImage from Pradeep Redddy Raamana High performance computing tutorial, with checklist and tips to optimize cluster usage\nImage from Pradeep Redddy Raamana High performance computing tutorial, with checklist and tips to optimize cluster usage\nImage from Sevcikova UseR! 2017 tutorial on parallel computing",
    "crumbs": [
      "Data Modeling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Misc</span>"
    ]
  },
  {
    "objectID": "01-intro.html#footnotes",
    "href": "01-intro.html#footnotes",
    "title": "1  Introduction1",
    "section": "",
    "text": "Much of this content was inspired by great educators who provide open source materials for educational use. Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.↩︎",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction^[Much of this content was inspired by great educators who provide open source materials for educational use.  Many thanks to Mine Çetinkaya-Rundel (Duke), Ben Baumer (Smith), Brianna Heggeseth (Macalester), Leslie Myint (Macalester), Paul Roback (St Olaf) for sharing their materials.]</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#coding-style",
    "href": "03-wrangling.html#coding-style",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread. The tidyverse style guide\n\n\n3.1.1 Why do style and format matter?\nConsistency and communication are very important aspects of coding. While the computer does not usually care about code style, the computer is not the only entity working in the coding space. Humans play a huge role in making code work, and they absolutely do better in situations where style is consistent and clear.\nIn chess, experienced players have excellent memories for recall of specific games and positions they have seen previously. However, recall is substantially better for situations with regular patterns as opposed to remembering random positions. (Gobet and Simon 1996) The related psychological idea, chunking, states that small pieces of information improve long-term retention of the material. When code is random or unwieldy, the coder has to use more brainpower to process the intent of the code.\nIn particular, good code style:\n\nhighlights where the errors are.\nmakes it easier to fix errors.\nfocuses the reader’s eye on what the code is doing (bad style obscures the information into the code itself).\nhelps the future reader of the code (which is likely you in the future!) understand what the code does.\nfollows the rules that specify conventions in programming.\n\n3.1.2 Best practices for writing good code.\nA style guide is a set of rules and best practices that help coders write code. There are no absolute truths when it comes to style, but consistency is important for being able to communicate your work.\n\nUse a style guide. For example, try The tidyverse style guide\n\nUse the same guide that your collaborators are using – remember, communication is key!",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#sec-purrr",
    "href": "03-wrangling.html#sec-purrr",
    "title": "3  Data Wrangling",
    "section": "\n3.6 purrr for functional programming",
    "text": "3.6 purrr for functional programming\nWe will see the R package purrr in greater detail as we go, but for now, let’s get a hint for how it works.\nWe are going to focus on the map family of functions which will just get us started. Lots of other good purrr functions like pluck() and accumulate().\nMuch of below is taken from a tutorial by Rebecca Barter.\nThe map functions are named by the output the produce. For example:\n\nmap(.x, .f) is the main mapping function and returns a list\nmap_df(.x, .f) returns a data frame\nmap_dbl(.x, .f) returns a numeric (double) vector\nmap_chr(.x, .f) returns a character vector\nmap_lgl(.x, .f) returns a logical vector\n\nNote that the first argument is always the data object and the second object is always the function you want to iteratively apply to each element in the input object.\nThe input to a map function is always either a vector (like a column), a list (which can be non-rectangular), or a dataframe (like a rectangle).\nA list is a way to hold things which might be very different in shape:\n\na_list &lt;- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)\n\n$a_number\n[1] 5\n\n$a_vector\n[1] \"a\" \"b\" \"c\"\n\n$a_dataframe\n  a b             c\n1 1 q       bananas\n2 2 b           are\n3 3 z so very great\n\n\nConsider the following function:\n\nadd_ten &lt;- function(x) {\n  return(x + 10)\n  }\n\nWe can map() the add_ten() function across a vector. Note that the output is a list (the default).\n\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 20\n\n\nWhat if we use a different type of input? The default behavior is to still return a list!\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map(add_ten)\n\n$a\n[1] 12\n\n$b\n[1] 15\n\n$c\n[1] 20\n\n\nWhat if we want a different type of output? We use a different map() function, map_df(), for example.\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map_df(add_ten)\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nShorthand lets us get away from pre-defining the function (which will be useful). Use the tilde ~ to indicate that you have a function:\n\ndata.frame(a = 2, b = 5, c = 10) |&gt;\n  map_df(~{.x + 10})\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nMostly, the tilde will be used for functions we already know but want to modify (if we don’t modify, and it has a simple name, we don’t use the tilde):\n\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins_split &lt;- split(penguins, penguins$species)\npenguins_split |&gt;\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) |&gt;\n  map_df(tidy)  # map(tidy)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\n\npenguins |&gt;\n  group_by(species) |&gt;\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) |&gt;\n  map(tidy)  # map_df(tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 0.00948      \n2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n\n[[2]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -3037.     997.       -3.05 0.00333      \n2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n\n[[3]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n2 flipper_length_mm     54.6      5.03     10.9  1.33e-19",
    "crumbs": [
      "Data communication",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "04-simulating.html#approximating-probabilities",
    "href": "04-simulating.html#approximating-probabilities",
    "title": "4  Simulating",
    "section": "",
    "text": "Set k (the number of random numbers) equal to zero. And the running sum to zero.\nGenerate a uniform random variable.\nAdd the random variable to the running sum. Repeat steps 1 and 2 until the sum is larger than 1.\nFigure out how many random variables were needed to get the sum larger than 1.\nRepeat the entire process many times so as to account for variability in the simulation.\nUse the law of large numbers to conclude that the average of the simulation approximates the expected value.\nUse the law of large numbers to conclude that the average of the simulation approximates the expected value.\n\nUsing functional programming and the map() function\nSee Section 3.6 for more information about map().\nFunctional programming is typically much faster than for loops are, and they also fit more cleanly into a tidy pipeline. The map functions (in the purrr package) are named by the output the produce. Some of the map() functions include:\n\nmap(.x, .f) is the main mapping function and returns a list\nmap_df(.x, .f) returns a data frame\nmap_dbl(.x, .f) returns a numeric (double) vector\nmap_chr(.x, .f) returns a character vector\nmap_lgl(.x, .f) returns a logical vector\n\n\n\n\n\n\n\n\nFrom Advanced R by Wickham. https://adv-r.hadley.nz/functionals.html\n\n\n\nNote that the first argument is always the data object and the second object is always the function you want to iteratively apply to each element in the input object.\nTo use functional programming on expected value problem, the first step is to write a function (here called sum_unif()) which will select uniform random variables until they add up to more than one. Note that the function itself doesn’t have any arguments.\n\nsum_unif &lt;- function(.x){\n  sumU &lt;- 0\n  k &lt;- 0\n  while(sumU &lt; 1) {\n    sumU &lt;- sumU + runif(1)\n    k &lt;- k+1\n}\n  return(c(k-1, sumU))\n}\n\nUsing map(), the sum_unif() function is run reps number of times. Note that sum_unif() doesn’t have any arguments, so it doesn’t really matter what the form of the input is for sum_unif().\n\nset.seed(4747)\nreps &lt;- 1000\n \nsim_k_max &lt;- data.frame(row_id = seq(1, reps, 1)) |&gt;\n  mutate(max_for_EX = map(row_id, sum_unif)) |&gt;\n  unnest(max_for_EX) |&gt;\n  mutate(output = rep(c(\"k\", \"sum\"), reps)) |&gt;\n  pivot_wider(id_cols = row_id, names_from = output, \n              values_from = max_for_EX) \n\nsim_k_max\n\n# A tibble: 1,000 × 3\n   row_id     k   sum\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1     2  1.05\n 2      2     2  1.68\n 3      3     1  1.39\n 4      4     1  1.47\n 5      5     2  1.03\n 6      6     1  1.45\n 7      7     1  1.41\n 8      8     1  1.83\n 9      9     1  1.17\n10     10     2  1.42\n# ℹ 990 more rows\n\n\nLast, approximate the expected value of \\(X\\) using the law of large numbers… the sample average converges in probability to the expected value.\n\nsim_k_max |&gt;\n  summarize(EX = mean(k))\n\n# A tibble: 1 × 1\n     EX\n  &lt;dbl&gt;\n1  1.74\n\n\n\n4.1.1 Aside: the R function sample()\n\nThe word “simulate” can mean a variety of things. In this course, we will simulate under various settings: sampling, shuffling, and resampling. All of the simultion methods can be done using the same R function sample()\n\nalph &lt;- letters[1:10]\n\nalph\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\nsample(alph, 5, replace = FALSE) # sample (from a population)\n\n[1] \"g\" \"i\" \"h\" \"b\" \"a\"\n\nsample(alph, 15, replace = TRUE) # sample (from a population)\n\n [1] \"i\" \"e\" \"d\" \"i\" \"d\" \"a\" \"c\" \"h\" \"a\" \"a\" \"b\" \"f\" \"i\" \"e\" \"h\"\n\nsample(alph, 10, replace = FALSE)  # shuffle\n\n [1] \"h\" \"a\" \"e\" \"g\" \"i\" \"c\" \"j\" \"d\" \"f\" \"b\"\n\nsample(alph, 10, replace = TRUE)  # resample\n\n [1] \"c\" \"h\" \"i\" \"j\" \"i\" \"b\" \"e\" \"j\" \"g\" \"c\"\n\n\n\n4.1.2 Examples of Pigs & Blackjack & Roulette\n\n4.1.2.1 Pass the Pigs\n\nFamiliarize yourself with how to play Pass the Pigs at http://www.hasbro.com/common/instruct/passthepigs.pdf and https://en.wikipedia.org/wiki/Pass_the_Pigs.\nFor more information on how to play Pass the Pigs, google online resources and see the following manuscript, http://pubsonline.informs.org/doi/pdf/10.1287/ited.1120.0088 Analytics, Pedagogy and the Pass the Pigs Game, (2012), Gorman, INFORMS Transactions on Education 1.\nMore sophisticated modeling: http://www.amstat.org/publications/jse/v14n3/datasets.kern.html\nSome strategies for playing: http://passpigs.tripod.com/strat.html (The link has other stuff, too.)\n\n4.1.2.2 Blackjack\n\nExample and code come from Data Science in R: a case studies approach to computational reasoning and problem solving, by Nolan and Temple Lang, Chapter 9 Simulating Blackjack, by Hadley Wickham\nAll R code is online at http://rdatasciencecases.org/\n\nMore about the game of blackjack, there are many online resources that you can use to learn about the came. Two resources that Nolan and Temple Lang recommend are http://wizardofodds.com/games/blackjack/ and http://hitorstand.net/.\n\nBasic Blackjack\n\nCard game, goal: sum cards as close to 21 without going over\nA few nuances to card value (e.g., Ace can be 1 or 11)\nStart with 2 cards, build up one card at a time\nLots of different strategies (also based on dealer’s cards)\n\n\n\n\n\n\n\n\n\nWhat do we need to simulate poker?\n\n\nset-up of cards, dealing, hands\n“score” (both sum of cards and payout)\nstrategies\nresult of strategies (summary of outcomes)\n\n\nSource\n\nExample and code come from Data Science in R: a case studies approach to computational reasoning and problem solving by Nolan and Temple Lang.\nChapter 9 Simulating Blackjack by Hadley Wickham\nAll R code is online at http://rdatasciencecases.org/\n\nLink is also on the HW4 assignment\nSetting up the Game in R\n\ndeck = rep(c(1:10, 10, 10, 10), 4)\n\nshuffle_decks = function(ndecks){sample(rep(deck, ndecks))}\n\nshuffle_decks(4)\n\n  [1]  6  5 10  4  2  1  2 10  1 10 10  7  9  5 10  4 10  2  6  8  8 10  7  9  3\n [26]  1 10 10 10  5  3 10  2 10 10 10  8  3  3 10 10 10  5 10  7  8 10 10  5 10\n [51]  9  6 10  8 10  9 10  2  1  4 10  7  3 10 10  8  3  6  6  3  5  5  6  9 10\n [76]  9  4 10  6  3  9 10  1 10  8  1  5 10  7  8 10  4 10  1  2  2  8  4  5  8\n[101]  4  6  8  7  7 10 10  5  3  4 10 10  3 10  1 10 10  4  6  7  8  9 10  4  2\n[126]  5  6  1 10  4 10  5 10  1  3  7  3  4 10  9 10  6  2 10  2  5  2  2  9 10\n[151]  1 10  3  8  9  3 10  1 10  5  4  2  5  4  8 10  2  9 10  8  7  7  9  2 10\n[176]  6  3  4  1 10  1  6  7  9  5 10  1  8  2  7  3  7  1 10  6  4  6  6 10 10\n[201] 10 10  7  9  9  7 10 10\n\n\nOutcome of cards in hand\n\nhandValue = function(cards) {\n  value = sum(cards)\n  \n       # Check for an Ace and change value if it doesn't bust\n  if (any(cards == 1) && value &lt;= 11) \n    value = value + 10\n    value\n  \n       # Check bust (set to 0); check Blackjack (set to 21.5)\n  if(value &gt; 21)  \n    0 \n  else if (value == 21 && length(cards) == 2)  \n    21.5 # Blackjack\n  else \n    value\n}\nhandValue(c(10,4))\n\n[1] 14\n\nhandValue(c(10, 4, 9))\n\n[1] 0\n\n\n$ of cards in hand\n\nwinnings = function(dealer, players) {\n  if (dealer &gt; 21) {  # Dealer=Blackjack, ties players with Blackjack\n    -1 * (players &lt;= 21)\n  } else if (dealer == 0) { # Dealer busts - all non-busted players win\n    1.5 * (players &gt; 21) +\n      1 * (players &lt;= 21 & players &gt; 0) +\n     -1 * (players == 0) \n  } else {            # Dealer 21 or below, all players &gt; dealer win\n    1.5 * (players &gt; 21) +  \n      1 * (players &lt;= 21 & players &gt; dealer) +\n     -1 * (players &lt;= 21 & players &lt; dealer) \n  }\n}\nwinnings(17,c(20, 21.5, 14, 0, 21))\n\n[1]  1.0  1.5 -1.0 -1.0  1.0\n\n\nBetter $ of cards in hand\n\nwinnings = function(dealer, players){\n  (players &gt; dealer & players &gt; 21) * 1.5 + # Blackjack\n  (players &gt; dealer & players &lt;= 21) * 1 +  # win\n  (players &lt; dealer | players == 0) * -1    # lose\n}\n\nwinnings(17,c(20, 21.5, 14, 0, 21))\n\n[1]  1.0  1.5 -1.0 -1.0  1.0\n\nwinnings(21.5,c(20, 21.5, 14, 0, 21))\n\n[1] -1  0 -1 -1 -1\n\n\nHow well does handValue work?\n\ntest_cards = list( c(10, 1), c(10, 5, 6), c(10, 1, 1), \n                   c(7, 6, 1, 5), c(3, 6, 1, 1), \n                   c(2, 3, 4, 10), c(5, 1, 9, 1, 1),\n                   c(5, 10, 7), c(10, 9, 1, 1, 1)) \n\ntest_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0)\nmap_dbl(test_cards, handValue)  # apply the function handValue to test_cards\n\n[1] 21.5 21.0 12.0 19.0 21.0 19.0 17.0  0.0  0.0\n\nidentical(test_cards_val, map_dbl(test_cards, handValue))\n\n[1] TRUE\n\n\nTesting winnings (create known)\n\ntest_vals = c(0, 16, 19, 20, 21, 21.5)\n\ntestWinnings =\n  matrix(c( -1,  1,  1,  1,  1, 1.5,\n            -1,  0,  1,  1,  1, 1.5,\n            -1, -1,  0,  1,  1, 1.5,\n            -1, -1, -1,  0,  1, 1.5,\n            -1, -1, -1, -1,  0, 1.5,\n            -1, -1, -1, -1, -1, 0), \n         nrow = length(test_vals), byrow = TRUE)\ndimnames(testWinnings) = list(dealer = test_vals, \n                              player = test_vals)\n\ntestWinnings\n\n      player\ndealer  0 16 19 20 21 21.5\n  0    -1  1  1  1  1  1.5\n  16   -1  0  1  1  1  1.5\n  19   -1 -1  0  1  1  1.5\n  20   -1 -1 -1  0  1  1.5\n  21   -1 -1 -1 -1  0  1.5\n  21.5 -1 -1 -1 -1 -1  0.0\n\n\nDoes winnings work?\n\ncheck = testWinnings  # make the matrix the right size\ncheck[] = NA  # make all entries NA\n \nfor(i in seq_along(test_vals)) {\n  for(j in seq_along(test_vals)) {\n    check[i, j] = winnings(test_vals[i], test_vals[j])\n  }\n}\n\nidentical(check, testWinnings)\n\n[1] TRUE\n\n\nFunction for getting more cards\n\nshoe = function(m = 1) sample(deck, m, replace = TRUE)\n\nnew_hand = function(shoe, cards = shoe(2), bet = 1) {\n  list(bet = bet, shoe = shoe, cards = cards)\n}\nmyCards = new_hand(shoe, bet = 7)\nmyCards\n\n$bet\n[1] 7\n\n$shoe\nfunction(m = 1) sample(deck, m, replace = TRUE)\n\n$cards\n[1] 10  3\n\n\nFirst action: hit\nreceive another card\n\nhit = function(hand) {\n  hand$cards = c(hand$cards, hand$shoe(1))\n  hand\n}\n\nhit(myCards)$cards\n\n[1] 10  3 10\n\n\nSecond action: stand\nstay with current cards\n\nstand = function(hand) hand\n\nstand(myCards)$cards\n\n[1] 10  3\n\n\nThird action: double down\ndouble the bet after receiving exactly one more card\n\ndd =  function(hand) {\n  hand$bet = hand$bet * 2\n  hand = hit(hand)\n  stand(hand)\n}\n\ndd(myCards)$cards\n\n[1] 10  3 10\n\n\nFourth action: split a pair\ncreate two different hands from initial hand with two cards of the same value\n\nsplitPair = function(hand) {\n  list( new_hand(hand$shoe, \n             cards = c(hand$cards[1], hand$shoe(1)),\n             bet = hand$bet),\n        new_hand(hand$shoe, \n             cards = c(hand$cards[2], hand$shoe(1)),\n             bet = hand$bet))   }\nsplitHand = splitPair(myCards)\n\nResults of splitting\n(can we always split?)\n\nsplitHand[[1]]$cards\n\n[1] 10  1\n\nsplitHand[[2]]$cards\n\n[1] 3 3\n\n\nLet’s play! Not yet automated…\n\nset.seed(470); dealer = new_hand(shoe); player = new_hand(shoe); \ndealer$cards[1]\n\n[1] 2\n\nplayer$cards; player = hit(player); player$cards\n\n[1]  5 10\n\n\n[1]  5 10  9\n\ndealer$cards; dealer = hit(dealer); dealer$cards\n\n[1] 2 3\n\n\n[1] 2 3 3\n\n\nWho won?\n\ndealer$cards; player$cards\n\n[1] 2 3 3\n\n\n[1]  5 10  9\n\nhandValue(dealer$cards); handValue(player$cards)\n\n[1] 8\n\n\n[1] 0\n\nwinnings(handValue(dealer$cards), handValue(player$cards))\n\n[1] -1\n\n\nSimply strategy\nrecall the handValue function – what if player busts?\n\nstrategy_simple = function(mine, dealerFaceUp) {\n  if (handValue(dealerFaceUp) &gt; 6 && handValue(mine) &lt; 17) \n     \"H\" \n  else \n     \"S\"\n}\n\nBetter simple strategy\n\nstrategy_simple = function(mine, dealerFaceUp) {\n  if (handValue(mine) == 0) return(\"S\")\n  if (handValue(dealerFaceUp) &gt; 6 && handValue(mine) &lt; 17) \n     \"H\" \n  else \n     \"S\"\n}\n\nDealer\nThe dealer gets cards regardless of what the player does\n\ndealer_cards = function(shoe) {\n  cards = shoe(2)\n  while(handValue(cards) &lt; 17 && handValue(cards) &gt; 0) {\n    cards = c(cards, shoe(1))\n  }\n  cards\n}\n\ndealer_cards(shoe)\n\n[1] 5 9 1 8\n\ndealer_cards(shoe)\n\n[1]  4  4  7 10\n\n\nPlaying a hand\n\nplay_hand = function(shoe, strategy, \n                      hand = new_hand(shoe), \n                      dealer = dealer_cards(shoe)) {\n  \n  face_up_card = dealer[1]\n  \n  action = strategy(hand$cards, face_up_card)\n  while(action != \"S\" && handValue(hand$cards) != 0) {\n    if (action == \"H\") {\n      hand = hit(hand)\n      action = strategy(hand$cards, face_up_card)\n    } else {\n      stop(\"Unknown action: should be one of S, H\")\n    }\n  }  \n\n  winnings(handValue(dealer), handValue(hand$cards)) * hand$bet\n}\n\nPlay a few hands\n\nplay_hand(shoe, strategy_simple)\n\n[1] 0\n\nplay_hand(shoe, strategy_simple)\n\n[1] -1\n\nplay_hand(shoe, strategy_simple, new_hand(shoe, bet=7))\n\n[1] 0\n\nplay_hand(shoe, strategy_simple, new_hand(shoe, bet=7))\n\n[1] 7\n\n\nRepeated games\nTo repeat the game, we simply repeat the play_hand function and keep track of the dollars gained or lost.\n\nreps=10\nmoney=20\nfor(i in 1:reps){\n  money &lt;- money + play_hand(shoe, strategy_simple)\n  print(money)}\n\n[1] 19\n[1] 20\n[1] 19\n[1] 18\n[1] 19.5\n[1] 18.5\n[1] 20\n[1] 19\n[1] 18\n[1] 19.5\n\n\n\n4.1.2.3 Roulette\n\nA roulette wheel has 38 slots numbered 00, 0, and 1–36. Two are green, 18 are red, and 18 are black.\nIf a gambler bets based on color, the return on a $1 bet is $2\nA gambler has $20, and will continuously bet $1 on red until they double their money (have $40) or lose the money they came with\nWhat is the probability the gambler doubles their money?\n\nQuestion: Without calculating probabilities, how could you design an experiment to estimate this probability?\nThe example is taken directly from Stat 279, Fall 2023 with Ciaran Evans.\nDesign the simulation\nStep 1: Need a Roulette wheel! … and money\nStep 2: Will spin the wheel\nStep 3: Depending on the wheel, update the money.\n* if spin is red: money + 1\n* if spin is not red: money - 1\nStep 4: Keep spinning until money = 40 or money = 0\nStep 5: Repeat many times\nStep 1: create a roulette wheel\n\nwheel &lt;- c(rep(\"green\", 2), rep(\"black\", 18), rep(\"red\", 18))\n\nwheel\n\n [1] \"green\" \"green\" \"black\" \"black\" \"black\" \"black\" \"black\" \"black\" \"black\"\n[10] \"black\" \"black\" \"black\" \"black\" \"black\" \"black\" \"black\" \"black\" \"black\"\n[19] \"black\" \"black\" \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"  \n[28] \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"   \"red\"  \n[37] \"red\"   \"red\"  \n\n\n\n\nrep() repeats a value the specified number of times\n\nc() combines vectors (or scalars) into a single vector\nStep 2: spin the wheel\n\nspin &lt;- sample(wheel, size = 1)\n\nspin\n\n[1] \"black\"\n\n\nStep 3: update the money\n\nmoney &lt;- 20\n\nspin &lt;- sample(wheel, size = 1)\nmoney &lt;- ifelse(spin == \"red\", money + 1, money  - 1)\n\nspin\n\n[1] \"red\"\n\nmoney\n\n[1] 21\n\n\nStep 4: keep spinning (function first)\n\nspin_func &lt;- function(money = 20, i){\n  wheel &lt;- c(rep(\"green\", 2), rep(\"black\", 18), rep(\"red\", 18))\n  spin &lt;- sample(wheel, size = 1)\n  money &lt;- ifelse(spin == \"red\", money + 1, money  - 1)\n  money\n}\n\nspin_func(10)\n\n[1] 9\n\nspin_func(spin_func(10))\n\n[1] 10\n\n\nAside: the accumulate() function\nIn R, the accumulate() function will run f(f(f(f(x)))) as many times as asked.\n\n1:10 |&gt; \n  accumulate(`+`)\n\n [1]  1  3  6 10 15 21 28 36 45 55\n\n1:10 |&gt; \n  accumulate(`+`, .init = 47)\n\n [1]  47  48  50  53  57  62  68  75  83  92 102\n\n\nStep 4: run multiple times\n\nset.seed(4747)\n1:20 |&gt; accumulate(spin_func, .init = 20)\n\n [1] 20 21 20 19 20 19 20 19 18 19 18 17 16 15 16 17 18 19 18 19 18\n\n\nStep 4: another function\n\ngame_func &lt;- function(rep, money, iter){\n  data.frame(interim_money = 1:iter |&gt; accumulate(spin_func, .init = money),\n             game = 0:iter,\n             rep = rep)\n}\n\ngame_func(1, 20, 3)\n\n  interim_money game rep\n1            20    0   1\n2            19    1   1\n3            18    2   1\n4            17    3   1\n\n\nStep 5: Repeat many times\nNote that map() is specifically designed to work in vectorized ways, so it doesn’t have an easy while() companion. Which means that we need to make sure to iterate long enough to either lose or double our money.\n\noutput &lt;- 1:20 |&gt; map(game_func, money = 20, iter = 100000) |&gt; \n  list_rbind()\n\noutput |&gt; \n  head()\n\n  interim_money game rep\n1            20    0   1\n2            19    1   1\n3            18    2   1\n4            19    3   1\n5            18    4   1\n6            17    5   1\n\n\nStep 6: Wrangle the output\n\nend &lt;- output |&gt; \n  group_by(rep) |&gt; \n  filter(interim_money &gt;= 40 | interim_money &lt;= 0) |&gt; \n  group_by(rep) |&gt; \n  slice_min(game) |&gt; \n  ungroup()\n\nend\n\n# A tibble: 20 × 3\n   interim_money  game   rep\n           &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1             0   104     1\n 2             0   286     2\n 3             0   410     3\n 4             0   136     4\n 5             0   156     5\n 6             0   442     6\n 7             0   110     7\n 8             0   170     8\n 9             0   150     9\n10             0   318    10\n11             0   210    11\n12             0   268    12\n13             0   750    13\n14             0   116    14\n15             0   204    15\n16             0    92    16\n17             0   354    17\n18            40   264    18\n19             0   124    19\n20            40   472    20\n\n\nStep 6: Wrangle the output\n\nend |&gt; \n  summarize(prop_double = mean(interim_money == 40))\n\n# A tibble: 1 × 1\n  prop_double\n        &lt;dbl&gt;\n1         0.1\n\n\nWithout magic numbers\n\nset.seed(4747)\nmoney &lt;- 10\ngames &lt;- 20\niterations &lt;- 10000\n\n\n1:games |&gt; map(game_func, money = money, iter = iterations) |&gt; \n  list_rbind() |&gt; \n  group_by(rep) |&gt; \n  filter(interim_money &gt;= 2 * money | interim_money &lt;= 0) |&gt; \n  group_by(rep) |&gt; \n  slice_min(game) |&gt; \n  ungroup() |&gt; \n  summarize(prop_double = mean(interim_money == 2 * money))\n\n# A tibble: 1 × 1\n  prop_double\n        &lt;dbl&gt;\n1        0.25\n\n\n\n4.1.3 Why do we simulate differently?\nThree simulating methods are used for different purposes:\n\nMonte Carlo methods - use sampling repeated sampling from populations with known (either via data or via populations) characteristics. [Note, another very common tool for sampling from a population is to use a probability model. Some of the distribution functions we will use include rnorm(), runif(), rbinom(), etc.]\nRandomization / Permutation methods - use shuffling (sampling without replacement) to test hypotheses of “no effect”.\nBootstrap methods - use resampling (sampling with replacement) to establish confidence intervals.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  },
  {
    "objectID": "04-simulating.html#assessing-sensitivity-of-inferential-procedures",
    "href": "04-simulating.html#assessing-sensitivity-of-inferential-procedures",
    "title": "4  Simulating",
    "section": "\n4.3 Assessing sensitivity of inferential procedures",
    "text": "4.3 Assessing sensitivity of inferential procedures\n\n4.3.1 Technical Conditions\nDefinitions\np-value is the probability of obtaining the observed data or more extreme given the null hypothesis is true.\n\\((1-\\alpha)100\\)% confidence interval is a range of values collected in such a way that repeated samples of data (using the same mechanism) would capture the parameter of interest in \\((1-\\alpha)100\\)% of the intervals.\nExamples\nEqual variance in the t-test Recall that one of the technical conditions for the t-test is that the two samples come from populations where the variance is equal (at least when var.equal=TRUE is specified). What happens if the null hypothesis is true (i.e., the means are equal!) but the technical conditions are violated (i.e., the variances are unequal)?\nt-test function (for use in map())\n\nt_test_pval &lt;- function(df){\n  t.test(y ~ x1, data = df, var.equal = TRUE) |&gt;\n    tidy() |&gt;\n    select(estimate, p.value) \n}\n\ngenerating data (equal variance)\n\nset.seed(470)\nreps &lt;- 1000\nn_obs &lt;- 20\nnull_data_equal &lt;- \n  data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,1), each = n()/2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\nsummarize p-values\n(Note: we rejected 4.5% of the null tests, close to 5%.)\n\nnull_data_equal |&gt; \n  mutate(t_vals = map(data,t_test_pval)) |&gt;\n  select(t_vals) |&gt; \n  unnest(t_vals) |&gt;\n  ungroup(sim_id) |&gt;\n  summarize(type1error_rate = sum(p.value &lt; 0.05)/reps)\n\n# A tibble: 1 × 1\n  type1error_rate\n            &lt;dbl&gt;\n1           0.045\n\n\nUnequal variance in the t-test\ngenerating data (unequal variance)\n\nset.seed(470)\nreps &lt;- 1000\nn_obs &lt;- 20\nnull_data_unequal &lt;- \n  data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,100), each = n()/2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\nsummarize p-values\n(Note, we rejected 5.7% of the null tests, not too bad!)\n\nnull_data_unequal |&gt; \n  mutate(t_vals = map(data,t_test_pval)) |&gt;\n  select(t_vals) |&gt; \n  unnest(t_vals) |&gt;\n  ungroup(sim_id) |&gt;\n  summarize(type1error_rate = sum(p.value &lt; 0.05)/reps)\n\n# A tibble: 1 × 1\n  type1error_rate\n            &lt;dbl&gt;\n1           0.057\n\n\nEqual variance in the linear model\nThe ISCAM applet by Beth Chance and Allan Rossman (Chance and Rossman 2018) demonstrates ideas of confidence intervals and what the analyst should expect with inferential assessment.\nConsider the following linear model with the points normally distributed with equal variance around the line. [Spoiler: when the technical conditions are met, the theory works out well. It turns out that the confidence interval will capture the true parameter in 95% of samples!]\n\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1)\\]\n\n\n\n\n\n\n\n\nDid we capture the true parameter in the CI? YES!\n\nCI &lt;- lm(y~x1+x2) |&gt; tidy(conf.int=TRUE) |&gt; data.frame()\nCI\n\n         term estimate std.error statistic  p.value conf.low conf.high\n1 (Intercept)   -0.950     0.148     -6.41 5.39e-09   -1.244    -0.656\n2          x1    0.259     0.210      1.23 2.20e-01   -0.158     0.677\n3          x2    1.401     0.194      7.21 1.24e-10    1.015     1.787\n\nCI |&gt;\n  filter(term == \"x2\") |&gt;\n  select(term, estimate, conf.low, conf.high) |&gt;\n  mutate(inside = between(beta2, conf.low, conf.high))\n\n  term estimate conf.low conf.high inside\n1   x2      1.4     1.02      1.79   TRUE\n\n\nWhat if we want to repeat lots of times…\nFUNCTION\n\nbeta_coef &lt;- function(df){\n  lm(y ~ x1 + x2, data = df) |&gt;\n    tidy(conf.int = TRUE) |&gt;\n    filter(term == \"x2\") |&gt;\n    select(estimate, conf.low, conf.high, p.value) \n}\n\nDATA\n\neqvar_data &lt;- data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, sd = 1)\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\neqvar_data\n\n# A tibble: 1,000 × 2\n# Groups:   sim_id [1,000]\n   sim_id data              \n    &lt;int&gt; &lt;list&gt;            \n 1      1 &lt;tibble [100 × 4]&gt;\n 2      2 &lt;tibble [100 × 4]&gt;\n 3      3 &lt;tibble [100 × 4]&gt;\n 4      4 &lt;tibble [100 × 4]&gt;\n 5      5 &lt;tibble [100 × 4]&gt;\n 6      6 &lt;tibble [100 × 4]&gt;\n 7      7 &lt;tibble [100 × 4]&gt;\n 8      8 &lt;tibble [100 × 4]&gt;\n 9      9 &lt;tibble [100 × 4]&gt;\n10     10 &lt;tibble [100 × 4]&gt;\n# ℹ 990 more rows\n\n\nMAPPING\nWe captured the true slope parameter in 95.5% of the confidence intervals (i.e., 95.5% of the datasets created confidence intervals that captured the true parameter).\n\neqvar_data |&gt; \n  mutate(b2_vals = map(data, beta_coef)) |&gt;\n  select(b2_vals) |&gt; \n  unnest(b2_vals) |&gt;\n  summarize(capture = between(beta2, conf.low, conf.high)) |&gt;\n  summarize(capture_rate = sum(capture)/reps)\n\n# A tibble: 1 × 1\n  capture_rate\n         &lt;dbl&gt;\n1        0.955\n\n\nUnequal variance in the linear model\nConsider the following linear model with the points normally distributed with unequal variance around the line. [Spoiler: when the technical conditions are met, the theory does not work out as well. It turns out that the confidence interval will not capture the true parameter in 95% of samples!]\n\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1+ X_1 + 10 \\cdot |X_2|)\\]\n\n\n\n\n\n\n\n\nWhat if we want to repeat lots of times…\nFUNCTION\n\nbeta_coef &lt;- function(df){\n  lm(y ~ x1 + x2, data = df) |&gt;\n    tidy(conf.int = TRUE) |&gt;\n    filter(term == \"x2\") |&gt;\n    select(estimate, conf.low, conf.high, p.value) \n}\n\nDATA\n\nuneqvar_data &lt;- data.frame(row_id = seq(1, n_obs, 1)) |&gt;\n  slice(rep(row_id, each = reps)) |&gt;\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, \n                                            sd = 1 + x1 + 10*abs(x2))\n  ) |&gt;\n  arrange(sim_id, row_id) |&gt;\n  group_by(sim_id) |&gt;\n  nest()\n\nuneqvar_data\n\n# A tibble: 1,000 × 2\n# Groups:   sim_id [1,000]\n   sim_id data              \n    &lt;int&gt; &lt;list&gt;            \n 1      1 &lt;tibble [100 × 4]&gt;\n 2      2 &lt;tibble [100 × 4]&gt;\n 3      3 &lt;tibble [100 × 4]&gt;\n 4      4 &lt;tibble [100 × 4]&gt;\n 5      5 &lt;tibble [100 × 4]&gt;\n 6      6 &lt;tibble [100 × 4]&gt;\n 7      7 &lt;tibble [100 × 4]&gt;\n 8      8 &lt;tibble [100 × 4]&gt;\n 9      9 &lt;tibble [100 × 4]&gt;\n10     10 &lt;tibble [100 × 4]&gt;\n# ℹ 990 more rows\n\n\nMAPPING\nUsing the data with unequal variability, we only captured the slope parameter about 88% of the time.\n\nuneqvar_data |&gt; \n  mutate(b2_vals = map(data, beta_coef)) |&gt;\n  select(b2_vals) |&gt; \n  unnest(b2_vals) |&gt;\n  summarize(capture = between(beta2, conf.low, conf.high)) |&gt;\n  summarize(capture_rate = sum(capture)/reps)\n\n# A tibble: 1 × 1\n  capture_rate\n         &lt;dbl&gt;\n1        0.861",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  },
  {
    "objectID": "04-simulating.html#generating-random-numbers",
    "href": "04-simulating.html#generating-random-numbers",
    "title": "4  Simulating",
    "section": "\n4.4 Generating random numbers",
    "text": "4.4 Generating random numbers\n\nYou are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation.\n\n\n4.4.1 How do we generate uniform[0,1] numbers?\nLCG - linear congruence generators. Set \\(a,b,m\\) to be large integers. The sequence of numbers \\(X_i / m\\) will pass all tests for uniformly distributed variables. \\[ X_{n+1} = (aX_n + b) \\mod m \\]\nwhere\n\n\n\\(m\\) and \\(b\\) are relatively prime,\n\n\\(a - 1\\) is divisible by all prime factors of \\(m\\),\n\n\\(a - 1\\) is divisible by 4 if \\(m\\) is divisible by 4.\n\n\na &lt;- 31541435235\nb &lt;- 23462146143 \nm &lt;- 423514351351\n\nxval &lt;- 47 \nreps &lt;- 10\nunif.val &lt;- c()\n\nfor(i in 1:reps){\n  xval &lt;- (a*xval + b) %% m\n  unif.val &lt;- c(unif.val, xval/m)   }\n\nupdate_rv &lt;- function(x){(a*x + b) %% m }\n\nrep(xval, reps) |&gt;\n  map(~ accumulate(., ~ ((a*.x + b) %% m))/m )\n\n[[1]]\n[1] 0.0584\n\n[[2]]\n[1] 0.0584\n\n[[3]]\n[1] 0.0584\n\n[[4]]\n[1] 0.0584\n\n[[5]]\n[1] 0.0584\n\n[[6]]\n[1] 0.0584\n\n[[7]]\n[1] 0.0584\n\n[[8]]\n[1] 0.0584\n\n[[9]]\n[1] 0.0584\n\n[[10]]\n[1] 0.0584\n\ndata.frame(uniformRVs = unif.val) |&gt;\n  ggplot(aes(x = uniformRVs)) + geom_histogram(bins = 25)\n\n\n\n\n\n\n\n\n4.4.2 Generating other RVs: The Inverse Transform Method\n\n\n\nYou are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation.\n\nContinuous RVs\nUse the inverse of the cumulative distribution function to generate data that come from a particular continuous distribution. For example, generate 100 random normal deviates. Start by assuming that \\(F\\) is a continuous and increasing function. Also assume that \\(F^{-1}\\) exists.\n\\[F(x) = P(X \\leq x)\\] Note that \\(F\\) is just the area function describing the density (histogram) of the data.\n\nAlgorithm: Generate Continuous RV\n\n\nGenerate a uniform random variable \\(U\\)\n\nSet \\(X = F^{-1}(U)\\)\n\n\n\nProof: that the algorithm above generates variables that come from the probability distribution represented by \\(F\\).\n\\[\\begin{align}\nP(X \\leq x) &= P(F^{-1}(U) \\leq x)\\\\\n&= P(U \\leq F(x))\\\\\n&= F(x)\\\\\n\\end{align}\\]\nExample: \\[ f(x) = \\begin{cases}\n2x e^{-x^2} & 0 &lt; x \\\\ 0 & x &lt; 0 \\end{cases}\\]\nNote: This is known as a Weibull(\\(\\lambda=1\\), \\(k=2\\)) distribution.\n\n\n\n\n\n\n\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\n\n\n\n\n\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\n\nWhat is \\(F(x)\\)? \\[ F(x) = \\int_0^x 2w e^{-w^2} dw = 1 - e^{-x^2}\\]\n\nWhat is \\(F^{-1}(u)\\)?\n\n\\[\\begin{align}\nu &= F(x)\\\\\n&= 1 - e^{-x^2}\\\\\n1-u &= e^{-x^2}\\\\\n-\\ln(1-u) &= x^2\\\\\n\\sqrt{-\\ln(1-u)} &= x\\\\\nF^{-1}(u) &= \\sqrt{-\\ln(1-u)}\n\\end{align}\\]\n\nSuppose you could simulate uniform random variables, \\(U_1, U_2, \\dots\\). How could you use these to simulate RV’s with the Weibull density, \\(f(x)\\), given above? \\[ \\mbox{Let: } X_i = \\sqrt{-\\ln(1-U_i)}\\]\n\n\n\nunifdata = runif(10000,0,1)\nweib1data = sqrt(-log(1-unifdata))\nweib2data = rweibull(10000,2,1)\n\nweibdata &lt;- data.frame(weibull = c(weib1data, weib2data),\n                       sim.method = c(rep(\"InvTrans\", 10000), \n                                      rep(\"rweibull\", 10000)))\n\nggplot(weibdata, aes(x = weibull)) + geom_histogram(bins = 25) + \n  facet_grid(~sim.method)\n\n\n\n\n\n\n\nDiscrete RVs\nA similar algorithm is used to generate data that come from a particular discrete distribution. For example, generate 100 random normal deviates. We start by assuming the probability mass function of \\(X\\) is \\[ P(X = x_i) = p_i, i=1, \\ldots, m\\]\n\nAlgorithm: Generate Discrete RV\n\n\nGenerate a uniform random variable \\(U\\)\n\nTransform \\(U\\) into \\(X\\) as follows, \\[X = x_j \\mbox{ if } \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\]\n\n\n\nProof: that the algorithm above generates variables that come from the probability mass function \\(\\{p_1, p_2, \\ldots, p_m\\}\\).\n\\[\\begin{align}\nP(X = x_j) &= \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\\\\n&= \\sum_{i=1}^j p_i - \\sum_{i=1}^{j-1} p_i\\\\\n&= p_j\\\\\n\\end{align}\\]\nWhat if you don’t know \\(F\\)? Or can’t calculate \\(F^{-1}\\)?\nIn the case that the CDF cannot be calculated explicitly (the normal for example), one could still use this methodology by estimating F at a collection of points \\(x_i, u_i = F(x_i)\\). Now we temporarily mimic the discrete inverse transform, as we generate a \\(U\\) and see which subinterval it falls in, i.e. \\(u_i \\leq U \\leq u_{i+1}\\). Assuming the \\(x_i\\) are close enough, we expect the CDF to be approximately linear on this subinterval, so then we take a linear interpolation of the CDF on the subinterval to get \\(X\\) via\n\\[\\begin{align}\nX = \\frac{u_{i+1} -  U}{u_{i+1} - u_i} x_i + \\frac{U - u_i}{u_{i+1} - u_i} x_j\n\\end{align}\\]\nHowever, the linear interpolation requires a complete approximation of \\(F(x)\\), regardless of the sample size desired, and doesn’t generalize to higher dimensions, and of course only gives you something with the approximate distribution back, even if you have your hands on real uniform random variables.\n\n\n\nFrom Advanced R by Wickham. https://adv-r.hadley.nz/functionals.html\nFor the 2016 election the Republican primary debates allowed only the top 10 candidates, ranked by national polls NYT\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\nWeibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons\n\n\n\nChance, Beth, and Allan Rossman. 2018. Investigating Statistics, Concepts, Applications, and Methods. 3rd ed. http://www.rossmanchance.com/iscam3/.",
    "crumbs": [
      "Data Inference",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Simulating</span>"
    ]
  }
]