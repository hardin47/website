[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 154 Pomona College: Computational Statistics. notes based extensively Introduction Statistical Learning (James et al. 2021) James, Witten, Hastie, Tibshiran; Modern Data Science (Baumer, Kaplan, Horton 2021) R Baumer, Kaplan, Horton; Data Science R: Case Studies Approach Computational Reasoning Problem Solving (Nolan Temple Lang 2015) Nolan Temple Lang; Visual Statistical Thinking: Displays Evidence Making Decisions (Tufte 1997) Tufte. computing part class taken R Data Science (Wickham Grolemund 2017) Wickham Grolemund well Tidy Modeling R (Kuhn Silge 2021) Kuhn Silge.responsible reading relevant chapters text. texts good & readable, use . make sure coming class also reading materials associated activities.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims populationWhat content Math 154?\nclass introduction statistical methods rely heavily use computers. course generally three parts. first section include communicating working data modern era. includes data wrangling, data visualization, data ethics, collaborative research (via GitHub). second part course focus traditional statistical inference done computational methods (e.g., permutation tests, bootstrapping, regression smoothers). last part course focus machine learning ideas classification, clustering, dimension reduction techniques. methods invented ubiquitous use personal computers, calculus used solve problem relatively straightforward (method wasn’t actually every used). methods developed within last years.take Math 154?\nComputational Statistics cover many concepts tools modern data analysis, therefore ideas important people like modern data analysis. individuals may want go graduate school statistics data science, may hope become data scientists without additional graduate work, may hope use modern techniques disciplines (e.g., computational biology, environmental analysis, political science). groups individuals get lot Computational Statistics turn analyzing data. Computational Statistics , however, course necessary entry graduate school statistics, mathematics, data science, computer science.prerequisites Math 154?\nComputational Statistics requires strong background statistics well algorithmic thinking. formal prerequisite introductory statistics course, AP Statistics, may find working hard first weeks class catch . taken lot mathematics, parts course come easily . However, mathematics degree substitute introductory statistics, taken introductory statistics, majority course work intuitive . must taken prior statistics course pre-requisite Math 154; computer science course also recommended.worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.overlap classes?\nmany machine learning data science courses 5Cs overlap Math 154. courses continue developed change, speak . Generally, Data Science courses taught 5C math departments focus slightly mathematics tools (e.g., mathematically breaking sparse matrices) Machine Learning courses taught 5C CS departments focus programming aspects tools (e.g., code Random Forest). focus inferential aspect tools, , results say larger problem trying solve? can know results accurate? sources variability?take Math 154?\nprerequisite Computational Statistics Introduction Statistics, course moves quickly covers tremendous amount material. ideally suited first year student coming straight AP Statistics. Instead, student focus taking mathematics, CS, interdisciplinary science, statistics courses. students taking Computational Statistics juniors seniors.workload Math 154?\none homework assignment per week, two -class midterm exams, two take-home midterm exams, final end semester project. Many students report working 8-10 hours per week class.software use? real world applications? mathematics? CS?\nwork done R (using RStudio front end, called integrated development environment, IDE). need either download R RStudio (free) onto computer use Pomona’s server. assignments posted private repositories GitHub. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.assignments turned using R Markdown compiled pdf.\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"course-content","chapter":"1 Introduction","heading":"1.2 Course Content","text":"","code":""},{"path":"intro.html","id":"topics","chapter":"1 Introduction","heading":"1.2.1 Topics","text":"Computational Statistics can large umbrella many ideas. Indeed, sometimes topics can seem somewhat disjointed. , ’ve categorized topics cover four groups. four different broad topics play different roles can less important depending problem hand. None topics exist , bigger focus topics sort data analysis / interpretation accurate compelling.Letting computer help: R, RStudio, Git, GitHub, Reproducibility, Data Viz, Data WranglingStatistics: Simulating, Randomization / Permutation Tests, Bootstrapping, EthicsMachine Learning: Classification, Clustering, Regular ExpressionsMathematics: Support Vector Machines","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.2 Vocabulary","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]One goals course convince students two major kinds skills one must order successful data scientist: technical skills actually analyses; communication skills order present one’s findings presumably non-technical audience. (Baumer 2015)thanks Ben Baumer perspective sharing course materials.","code":""},{"path":"intro.html","id":"the-workflow","chapter":"1 Introduction","heading":"1.2.3 The Workflow","text":"\nFigure 1.4: schematic typical workflow used data analysis. statistics classes focus left side. work address aspects (including right side). (Baumer 2015)\n\nFigure 1.5: Stitch Fix Algorithms Tour\n","code":""},{"path":"intro.html","id":"principles-for-the-data-science-process-tldr","chapter":"1 Introduction","heading":"1.2.4 Principles for the Data Science Process tl;dr","text":"(good thoughts DS Process, responsible content section.)Duncan Temple Lang, University California, DavisDuncan Temple-Lang leader area combining computer science research concepts within context statistics science generally. Recently, invited participate workshop, Training Students Extract Value Big Data. workshop subsequently summarized manuscript name provided free charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students Extract Value Big Data: Summary Workshop. Washington, DC: National Academies Press, 2014.]Duncan Temple Lang began listing core concepts data science - items need taught: statistics machine learning, computing technologies, domain knowledge problem. stressed importance interpretation reasoning - methods - addressing data. Students work data science broad set skills - including knowledge randomness uncertainty, statistical methods, programming, technology - practical experience . Students tend computing statistics classes entering graduate school domain science.Temple Lang described data analysis pipeline, outlining steps one example data analysis exploration process:Asking general question.Refining question, identifying data, understanding data metadata. Temple Lang noted data used usually collected specific question hand, original experiment data set understood.Access data. unrelated science require computational skill.Transforming data structures.Exploratory data analyses understand data determine whether results scale.critical step; Temple Lang noted 80 percent data scientist’s time can spent cleaning preparing data.\n6. Dimension reduction. Temple Lang stressed can difficult impossible automate step.\n7. Modeling estimation. Temple Lang noted computer machine learning scientists tend focus predictive models modeling physical behavior characteristics.\n8. Diagnostics. helps understand well model fits data identifies\nanomalies aspects study. step similarities exploratory data analysis.\n9. Quantifying uncertainty. Temple Lang indicated quantifying uncertainty statistical techniques important understanding interpreting models results.\n10. Conveying results.Temple Lang stressed data analysis process highly interactive iterative requires presence human loop. next step data processing often clear results current step clear, often something unexpected uncovered. also emphasized importance abstract skills concepts said people need exposed authentic data analyses, methods used. Data scientists also need statistical understanding, Temple Lang described statistical concepts taught student:Mapping general question statistical framework.Understanding scope inference, sampling, biases, limitations.Exploratory data analyses, including missing values, data quality, cleaning, matching, fusing.Understanding randomness, variability, uncertainty. Temple Lang noted many\nstudents understand sampling variability.Conditional dependence heterogeneity.Dimension reduction, variable selection, sparsity.Spurious relationships multiple testing.Parameter estimation versus “black box” prediction classification.Diagnostics, residuals, comparing models.Quantifying uncertainty model.Sampling structure dependence data reduction. Temple Lang noted modeling data becomes complicated variables independent, identically distributed.Statistical accuracy versus computational complexity efficiency.Temple Lang briefly discussed practical aspects computing, including following:Accessing data.Manipulating raw data.Data structures storage, including correlated data.Visualization stages (particularly exploratory data analyses conveying results).Parallel computing, can challenging new student.Translating high-level descriptions optimal programs.discussion, Temple Lang proposed computing statistics visualizations examine data rigorously statistical automated way. explained “scagnostics” (scatter plot diagnostics) data analysis technique graphically exploring relationships among variables. small set statistical measures can characterize scatter plots, exploratory data analysis can conducted residuals. [information scagnostics can found (Wilkinson et al., 2005, 2006).]workshop participant noted difference data error data blunder. blunder large, easily noticeable mistake. participant gave example shipboard observations cloud cover; blunders, case, occur location ship observation given land rather sea. Another blunder case ship’s changing location quickly. participant speculated blunders generalized detect problematic observations, although tools need scalable applied large data sets.","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.3 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.3.1 Need for Reproducibility","text":"\nFigure 1.6: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent , can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based data set error occurred. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.3.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/See course website getting started: http://research.pomona.edu/johardin/math154f19/\nFigure 1.7: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.8: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"tools-git-github","chapter":"1 Introduction","heading":"Tools: Git & GitHub","text":"must submit assignments via GitHubFollow Jenny Bryan’s advice get set-: http://happygitwithr.com/Class specific instructions https://m154-comp-stats.netlify.app/github.htmlAdmittedly, steep learning curve Git. However, among tools likely use future endeavors, spending little time focusing concepts now may pay big time future. Beyond practicing working http://happygitwithr.com/, may want read little bit waht Git behind scences. reference: Learn git concepts, commands good accessible.","code":""},{"path":"intro.html","id":"tools-a-github-merge-conflict-demo","chapter":"1 Introduction","heading":"Tools: a GitHub merge conflict (demo)","text":"GitHub (web) edit README document Commit message describing ., RStudio also edit README document different change.\nCommit changes\nTry push \\(\\rightarrow\\) ’ll get error!\nTry pulling\nResolve merge conflict commit push\nCommit changesTry push \\(\\rightarrow\\) ’ll get error!Try pullingResolve merge conflict commit pushAs work teams run merge conflicts, learning resolve properly important.\nFigure 1.9: https://xkcd.com/1597/\n","code":""},{"path":"intro.html","id":"steps-for-weekly-homework","chapter":"1 Introduction","heading":"Steps for weekly homework","text":"get link new assignment (clicking link create new private repo)Use R (within R Studio)\nNew Project, version control, Git\nClone repo using SSH\nNew Project, version control, GitClone repo using SSHIf exists, rename Rmd file ma154-hw#-lname-fname.RmdDo assignment\ncommit push every problem\ncommit push every problemAll necessary files must folder (e.g., data)","code":""},{"path":"intro.html","id":"data-examples","chapter":"1 Introduction","heading":"1.4 Data Examples","text":"","code":""},{"path":"intro.html","id":"what-cancant-data-science-do","chapter":"1 Introduction","heading":"What can/can’t Data Science Do?","text":"Can model data hand!Can find patterns & visualizations large datasets.Can’t establish causation.Can’t represent data isn’t .","code":""},{"path":"intro.html","id":"stats-data-science-math-are-not-apoliticalagnostic","chapter":"1 Introduction","heading":"Stats / Data Science / Math are not apolitical/agnostic","text":"“Inner city crime reaching record levels” (Donald Trump, 8/30/16)“unemployment rate African-American youth 59 percent” (Donald Trump 6/20/16)“Two million Latinos poverty today President Obama took oath office less eight years ago” (Donald Trump 8/25/16)“now, first time ever, energy independent” (Hillary Clinton 8/10/16)“look worldwide, number terrorist incidents substantially increased” (Barack Obama 10/13/16)“Illegal immigration lower ’s 40 years” (Barack Obama, 3/17/16)Source: http://www.politifact.com/truth-o-meter/statements/","code":""},{"path":"intro.html","id":"college-rankings-systems","chapter":"1 Introduction","heading":"1.4.1 College Rankings Systems","text":"CheatingBucknell University lied SAT averages 2006 2012, Emory University sent biased SAT scores class ranks least 11 years, starting 2000. Iona College admitted fudging SAT scores, graduation rates, retention rates, acceptance rates, student--faculty ratios order move 50th place 30th nine years discovered. ( Weapons Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html)Gaming systemPoint point, senior staff members tackled different criteria, always eye U.S. News’s methodology. Freeland added faculty, instance, reduce class size. “play kinds games,” says. “get credit number classes 20 [students], lowered caps lot classes 19 just make sure.” 1996 2003 edition (released 2002), Northeastern rose 20 spots. ( 14 Reasons US News College Rankings Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/)way measure “quality education”“best?” big part ranking system peer-assessed reputation (feedback loop!).","code":""},{"path":"intro.html","id":"trump-and-twitter","chapter":"1 Introduction","heading":"1.4.2 Trump and Twitter","text":"Analysis Trump’s tweets evidence someone else tweets account using iPhone.Aug 9, 2016 http://varianceexplained.org/r/trump-tweets/analysis, shown , concludes Android iPhone tweets clearly different people, posting different times day using hashtags, links, retweets distinct ways. ’s , can see Android tweets angrier negative, iPhone tweets tend benign announcements pictures.Aug 9, 2017 http://varianceexplained.org/r/trump-followup/year new data, 2700 tweets. quite notably, Trump stopped using Android March 2017. machine learning approaches like http://didtrumptweetit.com/ useful, since can still distinguish Trump’s tweets campaign’s training kinds features used original post.’ve found better dataset: original analysis, working quickly used twitteR package (https://cran.r-project.org/web/packages/twitteR/) query Trump’s tweets. since learned ’s bug package caused retrieve half tweets retrieved, case able go back January 2016. ’ve since found truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), contains Trump’s tweets going back 2009. show R code querying .’ve heard interesting questions wanted follow : come comments original post conversations ’ve since. Two questions included device Trump tended use campaign, types tweets tended lead high engagement.","code":""},{"path":"intro.html","id":"can-twitter-predict-election-results","chapter":"1 Introduction","heading":"1.4.3 Can Twitter Predict Election Results?","text":"2013, DiGrazia et al. (2013) published provocative paper suggesting polling now replaced analyzing social media data. analyzed 406 competitive US congressional races using 3.5 billion tweets. article Washington Post one co-authors, Rojas, writes: “Anyone programming skills can write program harvest tweets, sort content analyze results. can done nothing laptop computer.” (Rojas 2013)makes using Tweets predict elections relevant class? (See Baumer (2015).)data come neither experiment random sample - must careful thought applied question analysis can generalized. data also scraped internet.analysis done combining domain knowledge (congressional races) data source seems completely irrelevant outset (tweets).dataset quite large! 3.5 billion tweets collected random sample 500,000 tweets analyzed.researchers sociology computer science - truly collaborative endeavor, one often quite efficient producing high quality analyses.","code":""},{"path":"intro.html","id":"activity","chapter":"1 Introduction","heading":"Activity","text":"Spend minutes reading Rojas editorial skimming actual paper. sure consider Figure 1 Table 1 carefully, address following questions.working paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423published PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) Tweets, Votes: Social Media Quantitative Indicator Political Behavior. PLoS ONE 8 (11): e79449.editorial Washington Post Rojas: http://www.washingtonpost.com/opinions/-twitter-can-predict--election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.htmleditorial Huffington Post Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.htmleditorial blog Gelman: http://andrewgelman.com/2013/04/24/-tweets-votes-curve/","code":""},{"path":"intro.html","id":"statistics-hat","chapter":"1 Introduction","heading":"Statistics Hat","text":"Write sentence summarizing findings paper.Discuss Figure 1 neighbor. purpose? convey? Think critically data visualization. differently?\nproportion response variable. bizarre scaling dramatically change results\ndots scaled proportion number tweets\nlinear fit may questionable.\nimprove plot? .e., annotate make convincing / communicative? need enhancement?\nproportion response variable. bizarre scaling dramatically change resultsdots scaled proportion number tweetslinear fit may questionable.improve plot? .e., annotate make convincing / communicative? need enhancement?Interpret coefficient Republican Tweet Share models shown Table 1. sure include units.Discuss neighbor differences Bivariate model Full Model. one think better job predicting outcome election? one think best addresses influence tweets election?\n\\(R^2\\) way higher control variables included, duh!\nfull model likely better job predicting\n\\(R^2\\) way higher control variables included, duh!full model likely better job predictingWhy suppose coefficient Republican Tweet Share much larger Bivariate model? reflect influence tweets election?\ncontrolling many Republicans district, effect disappears\ncoefficient main term still statistically significant, size coefficient\n(155 +/- 43 votes) little practical significance\ncontrolling many Republicans district, effect disappearsWhile coefficient main term still statistically significant, size coefficient(155 +/- 43 votes) little practical significanceDo think study holds water? ? shortcomings study?\nreally. First , many races actually competitive? ’s 406, ’s probably fewer 100. redid study sample, tweet share still statistically significant full model?\nreally. First , many races actually competitive? ’s 406, ’s probably fewer 100. redid study sample, tweet share still statistically significant full model?","code":""},{"path":"intro.html","id":"data-scientist-hat","chapter":"1 Introduction","heading":"Data Scientist Hat","text":"Imagine boss, advanced technical skills knowledge, asked reproduce study just read. Discuss following neighbor.steps necessary reproduce study? specific can! Try list subtasks perform.computational tools use task?\nIdentify steps necessary conduct study. given current abilities & knowledge? practical considerations? (1) download Twitter? (2) API (Application Programming Interface), R interface APIs? (3) hard store 3.5 billion tweets? (4) big tweet? (5) know congressional district person tweeted ?much storage take download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (hard drive likely 1Tb, unless small computer). Can explain billions tweets stored Indiana University? randomly sample database? One tweet 2/3 Kb.","code":""},{"path":"intro.html","id":"advantages","chapter":"1 Introduction","heading":"Advantages","text":"CheapCan measure political race (just wealthy ones).","code":""},{"path":"intro.html","id":"disadvantages","chapter":"1 Introduction","heading":"Disadvantages","text":"really reflective voting populace? bias toward?simple mention candidate always reflect voting patterns? wouldn’t ?Margin error 2.7%. number typically calculated poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\).Tweets feel free terms able say - good thing bad thing respect polling?Can’t measure demographic information.","code":""},{"path":"intro.html","id":"what-could-be-done-differently","chapter":"1 Introduction","heading":"What could be done differently?","text":"Gelman: look close racesGelman: “might make sense flip around predict twitter mentions given candidate popularity. , rotate graph 90 degrees, see much variation tweet shares elections different degrees closeness.”Gelman: “scale size dot total number tweets two candidates election.”Gelman: Make data publicly available others can try reproduce results","code":""},{"path":"intro.html","id":"tweeting-and-r","chapter":"1 Introduction","heading":"Tweeting and R","text":"twitter analysis requires twitter password, sorry, won’t give mine. want download tweets, follow instructions http://stats.seandolinar.com/collecting-twitter-data-introduction/ maybe one : https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ http://davetang.org/muse/2013/04/06/using--r_twitter-package/ ask questions.","code":""},{"path":"visualization.html","id":"visualization","chapter":"2 Visualization","heading":"2 Visualization","text":"Data visualization integral understanding data models. Computational statistics data science sometimes focus models resulting predictions models. doubt structure format data key whether model appropriate good. good data analyst always spend lot time effort exploratory data analysis, much includes making many visualizations data possible.Depending introductory () statistics classes ’ve , instructor may focused less visualizations class. () may even said something like making visualizations incredibly important entire data analysis process. even buy perspective, don’t see good graphics analyses? Andrew Gelman (Gelman 2011) responds stating, “Good statistical graphics hard , much harder running regressions making tables.” goal create graphics visualizations convey statistical information.Nolan (D. Nolan Perrett 2016) describes three important ways graphics can used convey statistical information. “guiding principles” used way evaluating others’ figures well metric creating visualizations help statistical analysis.Make data stand outThe important idea find anything unusual data. patterns? Outliers? bounds variables? axes scaled? transformations warranted?Facilitate comparisonThe second item allows us consider research questions hand. important variables? emphasize ? variables plotted together? Can super-imposed? color, plotting character, size plot character help bring important relationships? aware plotting issues color blindness. http://colorbrewer2.org/Add informationPlots also add context comparison. Figure legends, axes scales, reference markers (e.g., line \\(y=x\\)) go long way toward helping reader understand message. Captions self-contained (assume user also read text) descriptive; summarize content figure conclusion related message want convey.Randy Pruim asks following question decide whether plot good: plot make comparisons interested …easily? andaccurately?Consider adding alt text allow screen readers parse image. DataViz Society/Nightingale way Amy Cesal article writing good alt text plots/graphs, Writing Alt Text Data Visualization.","code":""},{"path":"visualization.html","id":"examples","chapter":"2 Visualization","heading":"2.1 Examples","text":"first two examples taken book Edward Tufte arguably master visualizations. book Visual Statistical Thinking: Displays Evidence Making decisions. book can purchased http://www.edwardtufte.com/tufte/books_textb, though may online versions can download.","code":""},{"path":"visualization.html","id":"an-aside","chapter":"2 Visualization","heading":"An aside","text":"Generally, better graphics , better able communicate ideas broadly (’s become rich famous). graphics mean figures associated analyses, also power point presentations, posters, information website provided scientists might interested work. Tufte master understanding convey information visually, strongly recommend look work. Start Wikipedia main ideas provided (e.g., “data-ink ratio”) check incredible texts. many office happy let peruse . http://www.edwardtufte.com/tufte/books_vdqiAs mentioned booklet using, two main motivational steps working graphics part argument (E. Tufte 1997).“essential analytic task making decisions based evidence understand things work.”Making decisions based evidence requires appropriate display evidence.”Back examples…","code":""},{"path":"visualization.html","id":"cholera-via-tufte","chapter":"2 Visualization","heading":"2.1.1 Cholera via Tufte","text":"September 1854, worst outbreak cholera London occurred block radius - within 10 days, 500 fatalities. John Snow recognized clumping deaths, hypothesized due contamination Broad Street water pump. Despite testing water pump finding suspicious impurities, notice water quality varies data day. importantly, seemed possible causal mechanism outbreak. Eight days outbreak began, Snow described findings authorities, Board Guardians St. James’s Parish ordered Broad Street pump handle removed. epidemic ended soon .John Snow successful solving problem? thoughts consider (reported E. Tufte (1997)):bacterium Vibrio cholerae discovered 1886, however Snow myriad experience medical doctor looking patterns outbreaks. first realized cholera transmitted water instead air means.bacterium Vibrio cholerae discovered 1886, however Snow myriad experience medical doctor looking patterns outbreaks. first realized cholera transmitted water instead air means.Data Context Snow thought carefully present data. Instead simply looking data counts frequencies, looked death spatially - map area.Data Context Snow thought carefully present data. Instead simply looking data counts frequencies, looked death spatially - map area.Comparisons order isolate pump cause outbreak, Snow needed understand individuals died different individuals survived. Snow found two groups individuals (brewers drank beer, employees work house -site pump) succumbed disease.Comparisons order isolate pump cause outbreak, Snow needed understand individuals died different individuals survived. Snow found two groups individuals (brewers drank beer, employees work house -site pump) succumbed disease.Alternatives Whenever theory present, vitally important contrast theory possible alternative possibilities. Snow’s case, needed consider individuals regularly use Broad Street pump - able understand exceptions every case.Alternatives Whenever theory present, vitally important contrast theory possible alternative possibilities. Snow’s case, needed consider individuals regularly use Broad Street pump - able understand exceptions every case.removing pump handle really cause outbreak cease? Wasn’t already decline?removing pump handle really cause outbreak cease? Wasn’t already decline?Assessment Graphic individuals die place map? Live place map? (types ) individuals missing graph? Missing random? decisions make creating graph (axes, binning histogram bars, time data plotted, etc.) change story needing told?Assessment Graphic individuals die place map? Live place map? (types ) individuals missing graph? Missing random? decisions make creating graph (axes, binning histogram bars, time data plotted, etc.) change story needing told?","code":""},{"path":"visualization.html","id":"challenger-via-tufte","chapter":"2 Visualization","heading":"2.1.2 Challenger via Tufte","text":"John Snow’s story successful graphical intervention cholera outbreak contrasted fateful poor-graphical non-intervention Challenger disaster. January 28, 1986, space shuttle Challenger took Cape Canaveral, FL immediately exploded, killing seven astronauts aboard. now know reason explosion due failure two rubber O-rings malfunctioned due cold temperature day (\\(\\sim 29^\\circ\\) F).Unlike cholera epidemic, understood liability shuttle launch cold conditions unable convince powers postpone launch (much political momentum going forward get shuttle ground, including first teacher space, Christa McAuliffe). seen Tufte chapter, evidence clear communicated!biggest problem (existing many bullet points ) engineers failed important question data: relation ??engineers understood problem created tables engineering graphs \nvisually appealing.\ndecipherable layman (e.g., “\\(50^\\circ\\) F blow-experienced case joints”)\nalso authorship (reproducibility!). Figures always accountability reproducibility.\nengineers understood problem created tables engineering graphs wereNot visually appealing.decipherable layman (e.g., “\\(50^\\circ\\) F blow-experienced case joints”)also authorship (reproducibility!). Figures always accountability reproducibility.information provided included relevant points (temperature) superfluous information unrelated temperature. univariate analysis insufficient story data trying tell bivariate relationship temperature o-ring failure.information provided included relevant points (temperature) superfluous information unrelated temperature. univariate analysis insufficient story data trying tell bivariate relationship temperature o-ring failure.Missing data created illusion lack evidence, fact, true story quite strong given full set information. (92% temperature data missing vital tables.)Missing data created illusion lack evidence, fact, true story quite strong given full set information. (92% temperature data missing vital tables.)Anecdotal evidence misconstrued: SRM-15 57F damage, SRM-22 75F second damage.Anecdotal evidence misconstrued: SRM-15 57F damage, SRM-22 75F second damage.end, shuttle launched day extrapolation model suggested data. never launched shuttle temperatures \\(26^\\circ-29^\\circ\\)F.end, shuttle launched day extrapolation model suggested data. never launched shuttle temperatures \\(26^\\circ-29^\\circ\\)F.Tufte goes describe many ways final presentation engineers administrators inadequate: disappearing legend (labels), chartjunk, lack clarity depicting cause effect, wrong order.Tufte goes describe many ways final presentation engineers administrators inadequate: disappearing legend (labels), chartjunk, lack clarity depicting cause effect, wrong order.cholera outbreak, persuasive argument made visualizations hadbeen context plot data versus temperature time!,used appropriate comparisons: compared ?,consider alternative scenarios else O-rings fail? science behind O-ring failure?, andthe graphics assessed extra noise? words used accessible non-engineers?.Tufte (E. Tufte 1997) created graphic used launch convince others postpone. can see, graphic extremely convincing. aside: O-ring data well suited analysis using logistic regression. Today, scientists believe temperature caused O-ring failure, however, data speak causal relationship collected using randomized experiment. , confounding variables (e.g., humidity) possible causal mechanisms.\nFigure 1.2: graphic engineers led trying persuade administrators launch. evident number O-ring failures quite highly associated ambient temperature. Note vital information x-axis associated large number launches warm temperatures zero O-ring failures. (E. Tufte 1997)\n","code":""},{"path":"visualization.html","id":"thoughts","chapter":"2 Visualization","heading":"2.2 Thoughts on Plotting","text":"","code":""},{"path":"visualization.html","id":"advice","chapter":"2 Visualization","heading":"2.2.1 Advice","text":"Basic plotting\nAvoid graph elements interfere data\nUse visually prominent symbols\nAvoid -plotting (One way avoid plotting: Jitter values)\nDifferent values data may obscure \nInclude nearly data\nFill data region\nAvoid graph elements interfere dataUse visually prominent symbolsAvoid -plotting (One way avoid plotting: Jitter values)Different values data may obscure otherInclude nearly dataFill data regionEliminate superfluous material\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background images\nExtra tick marks grid lines\nUnnecessary text arrows\nDecimal places beyond measurement error level difference\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background imagesExtra tick marks grid linesUnnecessary text arrowsDecimal places beyond measurement error level differenceFacilitate Comparisons\nPut juxtaposed plots scale\nMake easy distinguish elements superposed plots (e.g. color)\nEmphasizes important difference\nComparison: volume, area, height (careful, volume can seem bigger mean )\nPut juxtaposed plots scaleMake easy distinguish elements superposed plots (e.g. color)Emphasizes important differenceComparison: volume, area, height (careful, volume can seem bigger mean )Choosing Scale (n.b., principles may go counter one another, use judgment.)\nKeep scales x y axes plots facilitate comparison\nZoom focus region contains bulk data\nKeep scale throughout plot (.e. don’t change mid-axis)\nOrigin need scale\nChoose scale improves resolution\nAvoid jiggling baseline\nKeep scales x y axes plots facilitate comparisonZoom focus region contains bulk dataKeep scale throughout plot (.e. don’t change mid-axis)Origin need scaleChoose scale improves resolutionAvoid jiggling baselineHow make plot information rich\nDescribe see caption\nAdd context reference markers (lines points) including text\nAdd legends labels\nUse color plotting symbols add information\nPlot thing different ways/scales\nReduce clutter\nDescribe see captionAdd context reference markers (lines points) including textAdd legends labelsUse color plotting symbols add informationPlot thing different ways/scalesReduce clutterCaptions \ncomprehensive\nSelf-contained\nDescribe graphed\nDraw attention important features\nDescribe conclusions drawn graph\ncomprehensiveSelf-containedDescribe graphedDraw attention important featuresDescribe conclusions drawn graphGood Plot Making Practice\nPut major conclusions graphical form\nProvide reference information\nProof read clarity consistency\nGraphing iterative process\nMultiplicity OK, .e. two plots variable may provide different messages\nMake plots data rich\nPut major conclusions graphical formProvide reference informationProof read clarity consistencyGraphing iterative processMultiplicity OK, .e. two plots variable may provide different messagesMake plots data richCreating statistical graphic iterative process discovery fine tuning. try model process creating visualizations course dedicating class time iterative creation plot. begin either plot screams correction, transform step--step, always thinking goal graph data rich presents clear vision important features data.","code":""},{"path":"visualization.html","id":"an-example-from-information-is-beautiful","chapter":"2 Visualization","heading":"2.2.2 An example from Information is Beautiful","text":"(See HW2 details R code)Consider plot http://www.informationisbeautiful.net/visualizations/caffeine--calories/. Note origin point (150,150). can get hurdle, expected looking graph.\nFigure 1.3: http://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\nremoved vertical horizontal lines detracted idea origin. also added additional information (color) describe chain drink comes . Notice additional difference plot original plot many observations.\nFigure 1.4: Calories Caffeine drinks various drinks items. Data source : World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, observational units (rows) random sample anything. , careful summarizing data way - ‘average’ calories even mean? Note, entire dataset give, average calories 179.8 average caffeine 134.43. numbers compare original plot?\nData retrieved :\nhttps://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0","code":""},{"path":"visualization.html","id":"fonts-matter","chapter":"2 Visualization","heading":"2.2.2.1 Fonts Matter","text":"RStudio::conf 2020, Glamour Graphics, Chase makes important points making good graphics matters. talk might summarized plot : fonts matter.","code":""},{"path":"visualization.html","id":"assessing-graphics-and-other-analyses","chapter":"2 Visualization","heading":"2.2.3 Assessing Graphics (and Other Analyses)","text":"rubric assessing analysis corresponding visualization. Note can large amount information gained moving basic competency surpassed competency. Table taken D. Nolan Perrett (2016).","code":""},{"path":"visualization.html","id":"deconstruct","chapter":"2 Visualization","heading":"2.3 Deconstructing a graph","text":"","code":""},{"path":"visualization.html","id":"gg","chapter":"2 Visualization","heading":"2.3.1 The Grammar of Graphics (gg)","text":"Yau (2013) Wickham (2014) come taxonomy grammar thinking parts figure just like conceptualize parts body parts sentence.One great way thinking new process: longer necessary talk name graph (e.g., boxplot). Instead now think glyphs (geoms), can put whatever want plot. Note also transition leads passive consumer (need make plot XXX everyone else , just plug data) active participant (want data say? can put information onto graphic?)important questions can ask respect creating figures :want R ? (goal?)R need know?Yau (2013) gives us nine visual cues, Wickham (2014) translates language using ggplot2. (items Baumer, Kaplan, Horton (2021), chapter 2.)Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Coordinate System: rectangular, polar, geographic, etc.Coordinate System: rectangular, polar, geographic, etc.Scale: numeric (linear? logarithmic?), categorical (ordered?), timeScale: numeric (linear? logarithmic?), categorical (ordered?), timeContext: comparison (think back ideas Tufte)Context: comparison (think back ideas Tufte)","code":""},{"path":"visualization.html","id":"order-matters","chapter":"2 Visualization","heading":"Order Matters","text":"","code":""},{"path":"visualization.html","id":"cues-together","chapter":"2 Visualization","heading":"Cues Together","text":"","code":""},{"path":"visualization.html","id":"what-are-the-visual-cues-on-the-plot","chapter":"2 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"visualization.html","id":"what-are-the-visual-cues-on-the-plot-1","chapter":"2 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"visualization.html","id":"what-are-the-visual-cues-on-the-plot-2","chapter":"2 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"visualization.html","id":"the-grammar-of-graphics-in-ggplot2","chapter":"2 Visualization","heading":"2.3.1.1 The grammar of graphics in ggplot2","text":"geom: geometric “shape” used display databar, point, line, ribbon, text, etc.aesthetic: attribute controlling geom displayed respect variablesx position, y position, color, fill, shape, size, etc.scale: adjust information aesthetic map onto plotparticular assignment colors, shapes, sizes, etc.; making axes continuous constrained particular range values.guide: helps user convert visual data back raw data (legends, axes)stat: transformation applied data geom gets itexample: histograms work binned data","code":""},{"path":"visualization.html","id":"ggplot2","chapter":"2 Visualization","heading":"2.3.2 ggplot2","text":"ggplot2, aesthetic refers mapping variable information conveys plot. information plotting visualizing information given chapter 2 (Data visualization) Baumer, Kaplan, Horton (2021). Much data presentation represents births 1978 US: date, day year, number births.","code":""},{"path":"visualization.html","id":"goals","chapter":"2 Visualization","heading":"Goals","text":"try dogive tour ggplot2give tour ggplot2explain think plots ggplot2 wayexplain think plots ggplot2 wayprepare/encourage learn laterprepare/encourage learn laterWhat can’t one sessionshow every bell whistleshow every bell whistlemake expert using ggplot2make expert using ggplot2","code":""},{"path":"visualization.html","id":"getting-help","chapter":"2 Visualization","heading":"Getting help","text":"One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfLook end presentation. help options .Look end presentation. help options .","code":"\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)##         date births wday year month day_of_year day_of_month day_of_week\n## 1 1978-01-01   7701  Sun 1978     1           1            1           1\n## 2 1978-01-02   7527  Mon 1978     1           2            2           2\n## 3 1978-01-03   8825  Tue 1978     1           3            3           3"},{"path":"visualization.html","id":"how-can-we-make-the-plot","chapter":"2 Visualization","heading":"How can we make the plot?","text":"Two Questions:want R ? (goal?)want R ? (goal?)R need know?\ndata source: Births78\naesthetics:\ndate -> x\nbirths -> y\npoints (!)\n\nR need know?data source: Births78data source: Births78aesthetics:\ndate -> x\nbirths -> y\npoints (!)\naesthetics:date -> xbirths -> ypoints (!)Goal: scatterplot = plot points\nggplot() + geom_point()\nGoal: scatterplot = plot pointsggplot() + geom_point()R need know?\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\nR need know?data source: data = Births78data source: data = Births78aesthetics: aes(x = date, y = births)aesthetics: aes(x = date, y = births)","code":""},{"path":"visualization.html","id":"how-can-we-make-the-plot-1","chapter":"2 Visualization","heading":"How can we make the plot?","text":"changed?new aesthetic: mapping color day week","code":""},{"path":"visualization.html","id":"adding-day-of-week-to-the-data-set","chapter":"2 Visualization","heading":"Adding day of week to the data set","text":"wday() function lubridate package computes day week date.","code":"\nBirths78 <-  \n  Births78 %>% \n  mutate(wday = lubridate::wday(date, label=TRUE))\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"how-can-we-make-the-plot-2","chapter":"2 Visualization","heading":"How can we make the plot?","text":"Now use lines instead dots","code":"\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"how-can-we-make-the-plot-3","chapter":"2 Visualization","heading":"How can we make the plot?","text":"Now two layers, one points one \nlinesThe layers placed one top : points\nlines .layers placed one top : points\nlines .data aes specified ggplot() affect geomsdata aes specified ggplot() affect geoms","code":"\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"alternative-syntax","chapter":"2 Visualization","heading":"Alternative Syntax","text":"","code":"\nBirths78 %>% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"what-does-adding-the-color-argument-do","chapter":"2 Visualization","heading":"What does adding the color argument do?","text":"variable, mapped color aesthetic new variable one value (“navy”). dots get set color, ’s navy.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"setting-vs.-mapping","chapter":"2 Visualization","heading":"Setting vs. Mapping","text":"want set color navy dots, outside aesthetic, without dataset variable:Note color = \"navy\" now outside aesthetics list. ’s ggplot2 distinguishes mapping setting.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"how-can-we-make-the-plot-4","chapter":"2 Visualization","heading":"How can we make the plot?","text":"ggplot() establishes default data aesthetics geoms, geom may change defaults.ggplot() establishes default data aesthetics geoms, geom may change defaults.good practice: put ggplot() things affect () layers; rest geom_blah()good practice: put ggplot() things affect () layers; rest geom_blah()","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"setting-vs.-mapping-again","chapter":"2 Visualization","heading":"Setting vs. Mapping (again)","text":"Information gets passed plot via:map variable information inside aes (aesthetic) commandmap variable information inside aes (aesthetic) commandset non-variable information outside aes (aesthetic) commandset non-variable information outside aes (aesthetic) command","code":""},{"path":"visualization.html","id":"other-geoms","chapter":"2 Visualization","heading":"Other geoms","text":"help pages tell aesthetics, default stats, etc.","code":"\napropos(\"^geom_\") [1] \"geom_abline\"                  \"geom_area\"                   \n [3] \"geom_ash\"                     \"geom_bar\"                    \n [5] \"geom_barh\"                    \"geom_bin_2d\"                 \n [7] \"geom_bin2d\"                   \"geom_blank\"                  \n [9] \"geom_boxplot\"                 \"geom_boxploth\"               \n[11] \"geom_col\"                     \"geom_colh\"                   \n[13] \"geom_contour\"                 \"geom_contour_filled\"         \n[15] \"geom_count\"                   \"geom_crossbar\"               \n[17] \"geom_crossbarh\"               \"geom_curve\"                  \n[19] \"geom_density\"                 \"geom_density_2d\"             \n[21] \"geom_density_2d_filled\"       \"geom_density_line\"           \n[23] \"geom_density_ridges\"          \"geom_density_ridges_gradient\"\n[25] \"geom_density_ridges2\"         \"geom_density2d\"              \n[27] \"geom_density2d_filled\"        \"geom_dotplot\"                \n[29] \"geom_errorbar\"                \"geom_errorbarh\"              \n[31] \"geom_errorbarh\"               \"geom_freqpoly\"               \n[33] \"geom_function\"                \"geom_hex\"                    \n[35] \"geom_histogram\"               \"geom_histogramh\"             \n[37] \"geom_hline\"                   \"geom_jitter\"                 \n[39] \"geom_label\"                   \"geom_line\"                   \n[41] \"geom_linerange\"               \"geom_linerangeh\"             \n[43] \"geom_lm\"                      \"geom_map\"                    \n[45] \"geom_path\"                    \"geom_point\"                  \n[47] \"geom_pointrange\"              \"geom_pointrangeh\"            \n[49] \"geom_polygon\"                 \"geom_qq\"                     \n[51] \"geom_qq_line\"                 \"geom_quantile\"               \n[53] \"geom_rangeframe\"              \"geom_raster\"                 \n[55] \"geom_rect\"                    \"geom_ribbon\"                 \n[57] \"geom_ridgeline\"               \"geom_ridgeline_gradient\"     \n[59] \"geom_rug\"                     \"geom_segment\"                \n[61] \"geom_sf\"                      \"geom_sf_label\"               \n[63] \"geom_sf_text\"                 \"geom_sina\"                   \n[65] \"geom_smooth\"                  \"geom_spline\"                 \n[67] \"geom_spoke\"                   \"geom_step\"                   \n[69] \"geom_text\"                    \"geom_tile\"                   \n[71] \"geom_tufteboxplot\"            \"geom_violin\"                 \n[73] \"geom_violinh\"                 \"geom_vline\"                  \n[75] \"geom_vridgeline\"             \n?geom_area             # for example"},{"path":"visualization.html","id":"lets-try-geom_area","chapter":"2 Visualization","heading":"Let’s try geom_area","text":"Using area produce good plotover plotting hiding much dataextending y-axis 0 may may desirable.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")"},{"path":"visualization.html","id":"side-note-what-makes-a-plot-good","chapter":"2 Visualization","heading":"Side note: what makes a plot good?","text":"(?) graphics intended help us make comparisonsHow something change time?treatments matter? much?men women respond way?Key plot metric: plot make comparisons interested ineasily, andaccurately?","code":""},{"path":"visualization.html","id":"time-for-some-different-data","chapter":"2 Visualization","heading":"Time for some different data","text":"HELPrct: Health Evaluation Linkage Primary care randomized clinical trialSubjects admitted treatment addiction one three substances.","code":"\nhead(HELPrct)##   age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n## 1  37            1    yes   49  3        177      225        0  NA      0\n## 2  37            1    yes   30 22          2       NA        0  NA      0\n## 3  26            1    yes   39  0          3      365       20  NA      0\n## 4  39            1    yes   15  2        189      343        0   1      1\n## 5  32            1    yes   39 12          2       57        0   1      0\n## 6  47            1    yes    6  1         31      365        0  NA      1\n##      sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n## 1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n## 2   male yes homeless 56 62  2     43         NA <NA> 26.670307 36.03694      1\n## 3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n## 4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n## 5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n## 6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n##   racegrp satreat sexrisk substance treat avg_drinks max_drinks\n## 1   black      no       4   cocaine   yes         13         26\n## 2   white      no       7   alcohol   yes         56         62\n## 3   black      no       2    heroin    no          0          0\n## 4   white     yes       4    heroin    no          5          5\n## 5   black      no       6   cocaine    no         10         13\n## 6   black      no       5   cocaine   yes          4          4\n##   hospitalizations\n## 1                3\n## 2               22\n## 3                0\n## 4                2\n## 5               12\n## 6                1"},{"path":"visualization.html","id":"who-are-the-people-in-the-study","chapter":"2 Visualization","heading":"Who are the people in the study?","text":"Hmm. ’s y?\nstat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.\nHmm. ’s y?stat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"who-are-the-people-in-the-study-1","chapter":"2 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"who-are-the-people-in-the-study-2","chapter":"2 Visualization","heading":"Who are the people in the study?","text":"","code":"\nlibrary(scales)\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"who-are-the-people-in-the-study-3","chapter":"2 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"how-old-are-people-in-the-help-study","chapter":"2 Visualization","heading":"How old are people in the HELP study?","text":"Notice messagesstat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.stat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.binwidth: default binwidth selected, really choose .binwidth: default binwidth selected, really choose .","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"visualization.html","id":"setting-the-binwidth-manually","chapter":"2 Visualization","heading":"Setting the binwidth manually","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"how-old-are-people-in-the-help-study-other-geoms","chapter":"2 Visualization","heading":"How old are people in the HELP study? – Other geoms","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"selecting-stat-and-geom-manually","chapter":"2 Visualization","heading":"Selecting stat and geom manually","text":"Every geom comes default statfor simple cases, stat stat_identity() nothingwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"selecting-stat-and-geom-manually-1","chapter":"2 Visualization","heading":"Selecting stat and geom manually","text":"Every stat comes default geom, every geom default statwe can specify stats instead geom, preferwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"more-combinations","chapter":"2 Visualization","heading":"More combinations","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"how-much-do-they-drink-i1","chapter":"2 Visualization","heading":"How much do they drink? (i1)","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"covariates-adding-in-more-variables","chapter":"2 Visualization","heading":"Covariates: Adding in more variables","text":"Using color linetype:Using color facets","code":"\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"boxplots","chapter":"2 Visualization","heading":"Boxplots","text":"Boxplots use stat_quantile() computes five-number summary (roughly five quartiles data) uses define “box” “whiskers.”quantitative variable must y, must additional x variable.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"horizontal-boxplots","chapter":"2 Visualization","heading":"Horizontal boxplots","text":"Horizontal boxplots obtained flipping coordinate system:coord_flip() may used plots well reverse roles\nx y plot.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"axes-scaling-with-boxplots","chapter":"2 Visualization","heading":"Axes scaling with boxplots","text":"can scale continuous axis","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"give-me-some-space","chapter":"2 Visualization","heading":"Give me some space","text":"’ve triggered new feature: dodge (dodging things left/right). can control much set dodge manually.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"issues-with-bigger-data","chapter":"2 Visualization","heading":"Issues with bigger data","text":"Although can see generally positive association (expect), plotting may hiding information.","code":"\nrequire(NHANES)\ndim(NHANES)## [1] 10000    76\nNHANES %>%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"visualization.html","id":"using-alpha-opacity","chapter":"2 Visualization","heading":"Using alpha (opacity)","text":"One way deal plotting set opacity low.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"visualization.html","id":"geom_density2d","chapter":"2 Visualization","heading":"geom_density2d","text":"Alternatively (simultaneously) might prefer different geom altogether.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"visualization.html","id":"multiple-layers","chapter":"2 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"multiple-layers-1","chapter":"2 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"things-i-havent-mentioned-much","chapter":"2 Visualization","heading":"Things I haven’t mentioned (much)","text":"coords (coord_flip() good know )coords (coord_flip() good know )themes (customizing appearance)themes (customizing appearance)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)transforming axestransforming axes","code":"\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"a-little-bit-of-everything","chapter":"2 Visualization","heading":"A little bit of everything","text":"","code":"\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"visualization.html","id":"want-to-learn-more","chapter":"2 Visualization","heading":"Want to learn more?","text":"docs.ggplot2.org/docs.ggplot2.org/Winston Chang’s: R Graphics CookbookWinston Chang’s: R Graphics Cookbook","code":""},{"path":"visualization.html","id":"what-else-can-we-do","chapter":"2 Visualization","heading":"What else can we do?","text":"shinyinteractive graphics / modelinginteractive graphics / modelinghttps://shiny.rstudio.com/https://shiny.rstudio.com/plotlyPlotly R package creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. plotly R library contains ggplotly function , convert ggplot2 figures Plotly object. Furthermore, option manipulating Plotly object style function.https://plot.ly/ggplot2/getting-started/Dynamic documentscombination RMarkdown, ggvis, shiny","code":""},{"path":"wrang.html","id":"wrang","chapter":"3 Data Wrangling","heading":"3 Data Wrangling","text":"data visualization, data wrangling fundamental part able accurately, reproducibly, efficiently work data. approach taken following chapter based philosophy tidy data takes many precepts database theory. done much work SQL, functionality approach tidy data feel familiar. adept data wrangling, effective data analysis.Information want, data ’ve got. (Kaplan 2015)Embrace ways get help!cheat sheets: https://www.rstudio.com/resources/cheatsheets/tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/pivoting: https://tidyr.tidyverse.org/articles/pivot.htmlgoogle need include R tidy tidyverse","code":""},{"path":"wrang.html","id":"datastruc","chapter":"3 Data Wrangling","heading":"3.1 Structure of Data","text":"plotting, analyses, model building, etc., ’s important data structured particular way. Hadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy Data: rows (cases/observational units) columns (variables). key every row case *every} column variable. exceptions.Creating tidy data trivial. work objects (often data tables), functions, arguments (often variables).Active Duty data tidy! cases? data tidy? might data look like tidy form? Suppose case “individual armed forces.” variables use capture information following table?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794Problem: totals different sheetsBetter R: longer format columns - grade, gender, status, service, count (case still total pay grade)Case individual (?): grade, gender, status, service (count row counting)","code":""},{"path":"wrang.html","id":"building-tidy-data","chapter":"3 Data Wrangling","heading":"3.1.1 Building Tidy Data","text":"Within R (really within type computing language, Python, SQL, Java, etc.), need understand build data using patterns language. things consider:object_name = function_name(arguments) way using function create new object.object_name = data_table %>% function_name(arguments) uses chaining syntax extension ideas functions. chaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.\n* R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.\n* pipe syntax read , %>%.","code":"object_name = data_table %>%\nfunction_name(arguments) %>% \nfunction_name(arguments)"},{"path":"wrang.html","id":"examples-of-chaining","chapter":"3 Data Wrangling","heading":"3.1.2 Examples of Chaining","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)","code":""},{"path":"wrang.html","id":"little-bunny-foo-foo","chapter":"3 Data Wrangling","heading":"3.1.2.1 Little Bunny Foo Foo","text":"Hadley Wickham, think tidy data.Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThe nursery rhyme created series steps output step saved object along way.Another approach concatenate functions one output.even worse, one line:Instead, code can written using pipe order function evaluated:babynames year, US Social Security Administration publishes list popular names given babies. 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys.babynames data table babynames package comes Social Security Administration’s listing names givens babies year, number babies sex given name. (names 5 babies published SSA.)","code":"foo_foo <- little_bunny()\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)bop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))foo_foo %>%\n   hop(through = forest) %>%\n       scoop(up = field_mice) %>%\n           bop(on = head)"},{"path":"wrang.html","id":"data-verbs-on-single-data-frames","chapter":"3 Data Wrangling","heading":"3.1.3 Data Verbs (on single data frames)","text":"Super important resource: RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). use R package dplyr much data wrangling. list verbs helpful wrangling many different types data. See Data Wrangling cheat sheet RStudio additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfsample_n() take random row(s)sample_n() take random row(s)head() grab first rowshead() grab first rowstail() grab last rowstail() grab last rowsfilter() removes unwanted casesfilter() removes unwanted casesarrange() reorders casesarrange() reorders casesselect() removes unwanted variables (rename() )select() removes unwanted variables (rename() )distinct() returns unique values tabledistinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)mutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nsummarize() collapses data frame single row. functions used within summarize() include:min(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]","code":""},{"path":"wrang.html","id":"r-examples-basic-verbs","chapter":"3 Data Wrangling","heading":"3.2 R examples, basic verbs","text":"","code":""},{"path":"wrang.html","id":"datasets","chapter":"3 Data Wrangling","heading":"3.2.1 Datasets","text":"starwars dplyr , although originally SWAPI, Star Wars API, http://swapi.co/.NHANES ?NHANES: NHANES survey data collected US National Center Health Statistics (NCHS) conducted series health nutrition surveys since early 1960’s. Since 1999 approximately 5,000 individuals ages interviewed homes every year complete health examination component survey. health examination conducted mobile examination center (MEC).babynames year, US Social Security Administration publishes list popular names given babies. 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys. (names 5 babies published SSA.)","code":""},{"path":"wrang.html","id":"examples-of-chaining-1","chapter":"3 Data Wrangling","heading":"3.2.2 Examples of Chaining","text":"","code":"\nlibrary(babynames)\nbabynames %>% nrow()## [1] 1924665\nbabynames %>% names()## [1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\nbabynames %>% glimpse()## Rows: 1,924,665\n## Columns: 5\n## $ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n## $ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n## $ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n## $ n    <int> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n## $ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\nbabynames %>% head()## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\nbabynames %>% tail()## # A tibble: 6 × 5\n##    year sex   name       n       prop\n##   <dbl> <chr> <chr>  <int>      <dbl>\n## 1  2017 M     Zyhier     5 0.00000255\n## 2  2017 M     Zykai      5 0.00000255\n## 3  2017 M     Zykeem     5 0.00000255\n## 4  2017 M     Zylin      5 0.00000255\n## 5  2017 M     Zylis      5 0.00000255\n## 6  2017 M     Zyrie      5 0.00000255\nbabynames %>% sample_n(size=5)## # A tibble: 5 × 5\n##    year sex   name        n       prop\n##   <dbl> <chr> <chr>   <int>      <dbl>\n## 1  1964 M     Myles     112 0.0000552 \n## 2  1981 F     Saima      12 0.00000671\n## 3  2003 M     Nykolas    13 0.00000619\n## 4  1997 F     Yobana      6 0.00000314\n## 5  2006 M     Brilee      6 0.00000274\nbabynames %>% mosaic::favstats(n ~ sex, data = .)##   sex min Q1 median Q3   max     mean       sd       n missing\n## 1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n## 2   M   5  7     12 33 94756 223.4940 1932.338  786372       0"},{"path":"wrang.html","id":"data-verbs","chapter":"3 Data Wrangling","heading":"3.2.3 Data Verbs","text":"Taken dplyr tutorial: http://dplyr.tidyverse.org/","code":""},{"path":"wrang.html","id":"starwars","chapter":"3 Data Wrangling","heading":"3.2.3.1 Starwars","text":"","code":"\nlibrary(dplyr)\n\nstarwars %>% dim()## [1] 87 14\nstarwars %>% names()##  [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n##  [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n## [11] \"species\"    \"films\"      \"vehicles\"   \"starships\"\nstarwars %>% head()## # A tibble: 6 × 14\n##   name     height  mass hair_color  skin_color eye_color birth_year sex   gender\n##   <chr>     <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Luke Sk…    172    77 blond       fair       blue            19   male  mascu…\n## 2 C-3PO       167    75 <NA>        gold       yellow         112   none  mascu…\n## 3 R2-D2        96    32 <NA>        white, bl… red             33   none  mascu…\n## 4 Darth V…    202   136 none        white      yellow          41.9 male  mascu…\n## 5 Leia Or…    150    49 brown       light      brown           19   fema… femin…\n## 6 Owen La…    178   120 brown, grey light      blue            52   male  mascu…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       8\n## 2 masculine  15 75     80 88.0 1358 106.14694 184.972677 49      17\nstarwars %>% \n  dplyr::filter(species == \"Droid\")## # A tibble: 6 × 14\n##   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n##   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n## 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculi…\n## 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculi…\n## 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculi…\n## 4 IG-88     200   140 none       metal       red               15 none  masculi…\n## 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n## 6 BB8        NA    NA none       none        black             NA none  masculi…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  dplyr::filter(species != \"Droid\") %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n## 2 masculine  15 77     80 88.0 1358 109.38222 192.397084 45      16\nstarwars %>% \n  dplyr::select(name, ends_with(\"color\"))## # A tibble: 87 × 4\n##    name               hair_color    skin_color  eye_color\n##    <chr>              <chr>         <chr>       <chr>    \n##  1 Luke Skywalker     blond         fair        blue     \n##  2 C-3PO              <NA>          gold        yellow   \n##  3 R2-D2              <NA>          white, blue red      \n##  4 Darth Vader        none          white       yellow   \n##  5 Leia Organa        brown         light       brown    \n##  6 Owen Lars          brown, grey   light       blue     \n##  7 Beru Whitesun lars brown         light       blue     \n##  8 R5-D4              <NA>          white, red  red      \n##  9 Biggs Darklighter  black         light       brown    \n## 10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n## # … with 77 more rows\nstarwars %>% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %>%\n  dplyr::select(name:mass, bmi)## # A tibble: 87 × 4\n##    name               height  mass   bmi\n##    <chr>               <int> <dbl> <dbl>\n##  1 Luke Skywalker        172    77  26.0\n##  2 C-3PO                 167    75  26.9\n##  3 R2-D2                  96    32  34.7\n##  4 Darth Vader           202   136  33.3\n##  5 Leia Organa           150    49  21.8\n##  6 Owen Lars             178   120  37.9\n##  7 Beru Whitesun lars    165    75  27.5\n##  8 R5-D4                  97    32  34.0\n##  9 Biggs Darklighter     183    84  25.1\n## 10 Obi-Wan Kenobi        182    77  23.2\n## # … with 77 more rows\nstarwars %>% \n  dplyr::arrange(desc(mass))## # A tibble: 87 × 14\n##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Jabba …    175  1358 <NA>        green-tan… orange         600   herm… mascu…\n##  2 Grievo…    216   159 none        brown, wh… green, y…       NA   male  mascu…\n##  3 IG-88      200   140 none        metal      red             15   none  mascu…\n##  4 Darth …    202   136 none        white      yellow          41.9 male  mascu…\n##  5 Tarfful    234   136 brown       brown      blue            NA   male  mascu…\n##  6 Owen L…    178   120 brown, grey light      blue            52   male  mascu…\n##  7 Bossk      190   113 none        green      red             53   male  mascu…\n##  8 Chewba…    228   112 brown       unknown    blue           200   male  mascu…\n##  9 Jek To…    180   110 brown       fair       blue            NA   male  mascu…\n## 10 Dexter…    198   102 none        brown      yellow          NA   male  mascu…\n## # … with 77 more rows, and 5 more variables: homeworld <chr>, species <chr>,\n## #   films <list>, vehicles <list>, starships <list>\nstarwars %>%\n  dplyr::group_by(species) %>%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  dplyr::filter(num > 1)## # A tibble: 9 × 3\n##   species    num  mass\n##   <chr>    <int> <dbl>\n## 1 Droid        6  69.8\n## 2 Gungan       3  74  \n## 3 Human       35  82.8\n## 4 Kaminoan     2  88  \n## 5 Mirialan     2  53.1\n## 6 Twi'lek      2  55  \n## 7 Wookiee      2 124  \n## 8 Zabrak       2  80  \n## 9 <NA>         4  48"},{"path":"wrang.html","id":"nhanes","chapter":"3 Data Wrangling","heading":"3.2.3.2 NHANES","text":"","code":"\nrequire(NHANES)\nnames(NHANES)##  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n##  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n##  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n## [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n## [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n## [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n## [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n## [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n## [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n## [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n## [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n## [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n## [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n## [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n## [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n## [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n## [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n## [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n## [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n# find the sleep variables\nNHANESsleep <- NHANES %>% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"\ndim(NHANESsleep)## [1] 10000    11\n# subset for college students\nNHANESsleep <- NHANESsleep %>% filter(Age %in% c(18:22)) %>% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"\ndim(NHANESsleep)## [1] 655  12\nNHANESsleep %>% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) "},{"path":"wrang.html","id":"summarize-and-group_by","chapter":"3 Data Wrangling","heading":"3.2.4 summarize and group_by","text":"","code":"\n# number of people (cases) in NHANES\nNHANES %>% summarize(n())## # A tibble: 1 × 1\n##   `n()`\n##   <int>\n## 1 10000\n# total weight of all the people in NHANES (silly)\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(sum(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `sum(Weightlb, na.rm = TRUE)`\n##                           <dbl>\n## 1                      1549419.\n# mean weight of all the people in NHANES\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `mean(Weightlb, na.rm = TRUE)`\n##                            <dbl>\n## 1                           156.\n# repeat the above but for groups\n\n# males versus females\nNHANES %>% group_by(Gender) %>% summarize(n())## # A tibble: 2 × 2\n##   Gender `n()`\n##   <fct>  <int>\n## 1 female  5020\n## 2 male    4980\nNHANES %>% group_by(Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 2 × 2\n##   Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                           <dbl>\n## 1 female                           146.\n## 2 male                             167.\n# smokers and non-smokers\nNHANES %>% group_by(SmokeNow) %>% summarize(n())## # A tibble: 3 × 2\n##   SmokeNow `n()`\n##   <fct>    <int>\n## 1 No        1745\n## 2 Yes       1466\n## 3 <NA>      6789\nNHANES %>% group_by(SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                 186.\n## 2 Yes                                177.\n## 3 <NA>                               144.\n# people with and without diabetes\nNHANES %>% group_by(Diabetes) %>% summarize(n())## # A tibble: 3 × 2\n##   Diabetes `n()`\n##   <fct>    <int>\n## 1 No        9098\n## 2 Yes        760\n## 3 <NA>       142\nNHANES %>% group_by(Diabetes) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   Diabetes `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                155. \n## 2 Yes                               202. \n## 3 <NA>                               21.6\n# break down the smokers versus non-smokers further, by sex\nNHANES %>% group_by(SmokeNow, Gender) %>% summarize(n())## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `n()`\n##   <fct>    <fct>  <int>\n## 1 No       female   764\n## 2 No       male     981\n## 3 Yes      female   638\n## 4 Yes      male     828\n## 5 <NA>     female  3618\n## 6 <NA>     male    3171\nNHANES %>% group_by(SmokeNow, Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                           <dbl>\n## 1 No       female                           167.\n## 2 No       male                             201.\n## 3 Yes      female                           167.\n## 4 Yes      male                             185.\n## 5 <NA>     female                           138.\n## 6 <NA>     male                             151.\n# break down the people with diabetes further, by smoking\nNHANES %>% group_by(Diabetes, SmokeNow) %>% summarize(n())## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `n()`\n##   <fct>    <fct>    <int>\n## 1 No       No        1476\n## 2 No       Yes       1360\n## 3 No       <NA>      6262\n## 4 Yes      No         267\n## 5 Yes      Yes        106\n## 6 Yes      <NA>       387\n## 7 <NA>     No           2\n## 8 <NA>     <NA>       140\nNHANES %>% group_by(Diabetes, SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                             <dbl>\n## 1 No       No                                183. \n## 2 No       Yes                               175. \n## 3 No       <NA>                              143. \n## 4 Yes      No                                204. \n## 5 Yes      Yes                               204. \n## 6 Yes      <NA>                              199. \n## 7 <NA>     No                                193. \n## 8 <NA>     <NA>                               19.1"},{"path":"wrang.html","id":"babynames","chapter":"3 Data Wrangling","heading":"3.2.5 babynames","text":"","code":"\nbabynames %>% group_by(sex) %>%\n  summarize(total=sum(n))## # A tibble: 2 × 2\n##   sex       total\n##   <chr>     <int>\n## 1 F     172371079\n## 2 M     175749438\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% head()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  1880 F            942\n## 2  1880 M           1058\n## 3  1881 F            938\n## 4  1881 M            997\n## 5  1882 F           1028\n## 6  1882 M           1099\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% tail()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  2015 F          19074\n## 2  2015 M          14024\n## 3  2016 F          18817\n## 4  2016 M          14162\n## 5  2017 F          18309\n## 6  2017 M          14160\nbabysamp <- babynames %>% sample_n(size=50)\nbabysamp %>% select(year) %>% distinct() %>% table()## .\n## 1907 1915 1921 1922 1925 1928 1930 1946 1950 1951 1952 1953 1954 1965 1966 1969 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1971 1973 1974 1975 1978 1979 1980 1981 1985 1987 1993 1998 1999 2000 2001 2002 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 2004 2005 2007 2008 2010 2012 2014 2016 2017 \n##    1    1    1    1    1    1    1    1    1\nbabysamp %>% distinct() %>% select(year) %>% table()## .\n## 1907 1915 1921 1922 1925 1928 1930 1946 1950 1951 1952 1953 1954 1965 1966 1969 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1971 1973 1974 1975 1978 1979 1980 1981 1985 1987 1993 1998 1999 2000 2001 2002 \n##    2    1    3    1    1    1    1    1    1    2    2    1    2    1    1    1 \n## 2004 2005 2007 2008 2010 2012 2014 2016 2017 \n##    3    1    1    2    1    1    1    1    1\nFrances <- babynames %>%\n  filter(name== \"Frances\") %>%\n  group_by(year, sex) %>%\n  summarize(yrTot = sum(n))\n\nFrances %>% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")"},{"path":"wrang.html","id":"highverb","chapter":"3 Data Wrangling","heading":"3.3 Higher Level Data Verbs","text":"complicated verbs may important sophisticated analyses. See RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.pivot_longer makes many columns 2 columns: pivot_longer(data, cols,  names_to = , value_to = )pivot_wider makes one column multiple columns: pivot_wider(data, names_from = , values_from = )left_join returns rows left table, rows matching keys right table.inner_join returns rows left table matching keys right table (.e., matching rows sets).full_join returns rows tables, join records left matching keys right table.Good practice: always specify argument joining data frames.ever need understand join right join , try find image lay function . found one quite good taken Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti","code":""},{"path":"wrang.html","id":"r-examples-higher-level-verbs","chapter":"3 Data Wrangling","heading":"3.4 R examples, higher level verbs","text":"tidyr 1.0.0 just released! new release means need update tidyr. know latest version following command works console (window ):familiar spread gather, acquaint pivot_longer() pivot_wider(). idea go wide dataframes long dataframes vice versa.","code":"?tidyr::pivot_longer"},{"path":"wrang.html","id":"pivot_longer","chapter":"3 Data Wrangling","heading":"3.4.1 pivot_longer()","text":"pivot military pay grade become longer?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#\ngid=1811988794Does graph tell us right? done wrong…?","code":"\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\nglimpse(navy_gs)## Rows: 38\n## Columns: 17\n## $ ...1                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ `Active Duty Family` <chr> NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n## $ ...3                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n## $ ...4                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n## $ ...5                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n## $ ...6                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n## $ ...7                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n## $ ...8                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n## $ ...9                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n## $ ...10                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n## $ ...11                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n## $ ...12                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n## $ ...13                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n## $ ...14                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n## $ ...15                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n## $ ...16                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n## $ ...17                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)## Rows: 30\n## Columns: 16\n## $ pay.grade       <chr> \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n## $ male.sing.wo    <dbl> 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n## $ female.sing.wo  <dbl> 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n## $ tot.sing.wo     <dbl> 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n## $ male.sing.w     <dbl> 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n## $ female.sing.w   <dbl> 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n## $ tot.sing.w      <dbl> 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n## $ male.joint.NA   <dbl> 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n## $ female.joint.NA <dbl> 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n## $ tot.joint.NA    <dbl> 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n## $ male.civ.NA     <dbl> 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n## $ female.civ.NA   <dbl> 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n## $ tot.civ.NA      <dbl> 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n## $ male.tot.NA     <dbl> 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n## $ female.tot.NA   <dbl> 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n## $ tot.tot.NA      <dbl> 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n# get rid of total columns & rows:\n\nnavyWR = navy %>% select(-contains(\"tot\")) %>%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %>%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %>%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %>% head()## # A tibble: 6 × 5\n##   pay.grade sex    marital kids  numPeople\n##   <chr>     <chr>  <chr>   <chr>     <dbl>\n## 1 E-1       male   sing    wo        31229\n## 2 E-1       female sing    wo         5717\n## 3 E-1       male   sing    w           563\n## 4 E-1       female sing    w           122\n## 5 E-1       male   joint   NA          139\n## 6 E-1       female joint   NA          141\nnavyWR %>% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))"},{"path":"wrang.html","id":"pivot_wider","chapter":"3 Data Wrangling","heading":"3.4.2 pivot_wider","text":"","code":"\nlibrary(babynames)\nbabynames %>% dplyr::select(-prop) %>%\n   tidyr::pivot_wider(names_from = sex, values_from = n) ## # A tibble: 1,756,284 × 4\n##     year name          F     M\n##    <dbl> <chr>     <int> <int>\n##  1  1880 Mary       7065    27\n##  2  1880 Anna       2604    12\n##  3  1880 Emma       2003    10\n##  4  1880 Elizabeth  1939     9\n##  5  1880 Minnie     1746     9\n##  6  1880 Margaret   1578    NA\n##  7  1880 Ida        1472     8\n##  8  1880 Alice      1414    NA\n##  9  1880 Bertha     1320    NA\n## 10  1880 Sarah      1288    NA\n## # … with 1,756,274 more rows\nbabynames %>% \n  select(-prop) %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(year), desc(M))## # A tibble: 168,381 × 4\n##     year name         F     M\n##    <dbl> <chr>    <int> <int>\n##  1  2017 Liam        36 18728\n##  2  2017 Noah       170 18326\n##  3  2017 William     18 14904\n##  4  2017 James       77 14232\n##  5  2017 Logan     1103 13974\n##  6  2017 Benjamin     8 13733\n##  7  2017 Mason       58 13502\n##  8  2017 Elijah      26 13268\n##  9  2017 Oliver      15 13141\n## 10  2017 Jacob       16 13106\n## # … with 168,371 more rows\nbabynames %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(prop))## # A tibble: 12 × 5\n##     year name            prop     F     M\n##    <dbl> <chr>          <dbl> <int> <int>\n##  1  1986 Marquette 0.0000130     24    25\n##  2  1996 Dariel    0.0000115     22    23\n##  3  2014 Laramie   0.0000108     21    22\n##  4  1939 Earnie    0.00000882    10    10\n##  5  1939 Vertis    0.00000882    10    10\n##  6  1921 Vernis    0.00000703     9     8\n##  7  1939 Alvia     0.00000529     6     6\n##  8  1939 Eudell    0.00000529     6     6\n##  9  1939 Ladell    0.00000529     6     6\n## 10  1939 Lory      0.00000529     6     6\n## 11  1939 Maitland  0.00000529     6     6\n## 12  1939 Delaney   0.00000441     5     5"},{"path":"wrang.html","id":"join-use-join-to-merge-two-datasets","chapter":"3 Data Wrangling","heading":"3.4.3 join (use join to merge two datasets)","text":"","code":""},{"path":"wrang.html","id":"first-get-the-data-gapminder","chapter":"3 Data Wrangling","heading":"3.4.3.1 First get the data (GapMinder)","text":"following datasets come GapMinder. first represents country, year, female literacy rate. second represents country, year, GDP (fixed 2000 US$).","code":"\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %>% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %>%\n  filter(!is.na(litRateF))\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %>% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %>%\n  filter(!is.na(gdp))\nhead(litF)## # A tibble: 6 × 3\n##   country     year  litRateF\n##   <chr>       <chr>    <dbl>\n## 1 Afghanistan 1979      4.99\n## 2 Afghanistan 2011     13   \n## 3 Albania     2001     98.3 \n## 4 Albania     2008     94.7 \n## 5 Albania     2011     95.7 \n## 6 Algeria     1987     35.8\nhead(GDP)## # A tibble: 6 × 3\n##   country year    gdp\n##   <chr>   <chr> <dbl>\n## 1 Albania 1980  1061.\n## 2 Albania 1981  1100.\n## 3 Albania 1982  1111.\n## 4 Albania 1983  1101.\n## 5 Albania 1984  1065.\n## 6 Albania 1985  1060.\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)## [1] 571   4\nsum(is.na(litGDPleft$gdp))## [1] 66\nhead(litGDPleft)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902.\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)## [1] 7988    4\nsum(is.na(litGDPright$gdp))## [1] 0\nhead(litGDPright)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)## [1] 505   4\nsum(is.na(litGDPinner$gdp))## [1] 0\nhead(litGDPinner)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)## [1] 8054    4\nsum(is.na(litGDPfull$gdp))## [1] 66\nhead(litGDPfull)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902."},{"path":"wrang.html","id":"lubridate","chapter":"3 Data Wrangling","heading":"3.4.4 lubridate","text":"lubridate another R package meant data wrangling (Grolemund Wickham 2011). particular, lubridate makes easy work days, times, dates. base idea start dates ymd (year month day) format transform information whatever want. linked table original paper provides many basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.Example https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html","code":""},{"path":"wrang.html","id":"if-anyone-drove-a-time-machine-they-would-crash","chapter":"3 Data Wrangling","heading":"3.4.4.1 If anyone drove a time machine, they would crash","text":"length months years change often arithmetic can unintuitive. Consider simple operation, January 31st + one month. answer :February 31st (doesn’t exist)March 4th (31 days January 31), orFebruary 28th (assuming leap year)basic property arithmetic + b - b = . solution 1 obeys mathematical property, invalid date. Wickham wants make lubridate consistent possible invoking following rule: adding subtracting month year creates invalid date, lubridate return NA.thought solution 2 3 useful, problem. can still get results clever arithmetic, using special %m+% %m-% operators. %m+% %m-% automatically roll dates back last day month, necessary.","code":""},{"path":"wrang.html","id":"r-examples-lubridate","chapter":"3 Data Wrangling","heading":"3.4.4.2 R examples, lubridate()","text":"","code":""},{"path":"wrang.html","id":"some-basics-in-lubridate","chapter":"3 Data Wrangling","heading":"Some basics in lubridate","text":"","code":"\nrequire(lubridate)\nrightnow <- now()\n\nday(rightnow)## [1] 4\nweek(rightnow)## [1] 44\nmonth(rightnow, label=FALSE)## [1] 11\nmonth(rightnow, label=TRUE)## [1] Nov\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nyear(rightnow)## [1] 2021\nminute(rightnow)## [1] 2\nhour(rightnow)## [1] 9\nyday(rightnow)## [1] 308\nmday(rightnow)## [1] 4\nwday(rightnow, label=FALSE)## [1] 5\nwday(rightnow, label=TRUE)## [1] Thu\n## Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"},{"path":"wrang.html","id":"but-how-do-i-create-a-date-object","chapter":"3 Data Wrangling","heading":"But how do I create a date object?","text":"","code":"\njan31 <- ymd(\"2021-01-31\")\njan31 + months(0:11)##  [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n##  [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n## [11] NA           \"2021-12-31\"\nfloor_date(jan31, \"month\") + months(0:11) + days(31)##  [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n##  [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n## [11] \"2021-12-02\" \"2022-01-01\"\njan31 + months(0:11) + days(31)##  [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n##  [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n## [11] NA           \"2022-01-31\"\njan31 %m+% months(0:11)##  [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n##  [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n## [11] \"2021-11-30\" \"2021-12-31\""},{"path":"wrang.html","id":"nyc-flights","chapter":"3 Data Wrangling","heading":"NYC flights","text":"","code":"\nlibrary(nycflights13)\nnames(flights)##  [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n##  [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n##  [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n## [13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n## [17] \"hour\"           \"minute\"         \"time_hour\"\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)## # A tibble: 6 × 22\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      517            515         2      830            819\n## 2  2013     1     1      533            529         4      850            830\n## 3  2013     1     1      542            540         2      923            850\n## 4  2013     1     1      544            545        -1     1004           1022\n## 5  2013     1     1      554            600        -6      812            837\n## 6  2013     1     1      554            558        -4      740            728\n## # … with 14 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, ymdday <date>, weekdy <ord>,\n## #   whichweek <dbl>\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %>% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %>%  \n   head()## # A tibble: 6 × 9\n##    year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n##   <int> <int> <int> <date>     <ord>      <dbl>    <int>    <int>    <dbl>\n## 1  2013     1     1 2013-01-01 Tue            1      517      830      227\n## 2  2013     1     1 2013-01-01 Tue            1      533      850      227\n## 3  2013     1     1 2013-01-01 Tue            1      542      923      160\n## 4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n## 5  2013     1     1 2013-01-01 Tue            1      554      812      116\n## 6  2013     1     1 2013-01-01 Tue            1      554      740      150"},{"path":"wrang.html","id":"purrr-for-functional-programming","chapter":"3 Data Wrangling","heading":"3.5 purrr for functional programming","text":"see R package purrr greater detail go, now, let’s get hint works.going focus map family functions just get us started. Lots good purrr functions like pluck() accumulate().Much taken tutorial Rebecca Barter.map functions named output produce. example:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vectorNote first argument always data object second object always function want iteratively apply element input object.input map function always either vector (like column), list (can non-rectangular), dataframe (like rectangle).list way hold things might different shape:Consider following function:can map() add_ten() function across vector. Note output list (default).use different type input? default behavior still return list!want different type output? use different map() function, map_df(), example.Shorthand lets us get away pre-defining function (useful). Use tilde ~ indicate function:Mostly, tilde used functions already know:","code":"\na_list <- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)## $a_number\n## [1] 5\n## \n## $a_vector\n## [1] \"a\" \"b\" \"c\"\n## \n## $a_dataframe\n##   a b             c\n## 1 1 q       bananas\n## 2 2 b           are\n## 3 3 z so very great\nadd_ten <- function(x) {\n  return(x + 10)\n  }\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 15\n## \n## [[3]]\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map(add_ten)## $a\n## [1] 12\n## \n## $b\n## [1] 15\n## \n## $c\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(add_ten)## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(~{.x + 10})## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %>%\n  split(.$species) %>%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map_df(tidy)  # map(tidy)## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n## 2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n## 3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n## 4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n## 5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\npenguins %>%\n  group_by(species) %>%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map(tidy)  # map_df(tidy)## [[1]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 0.00948      \n## 2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n## \n## [[2]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -3037.     997.       -3.05 0.00333      \n## 2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n## \n## [[3]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 2 flipper_length_mm     54.6      5.03     10.9  1.33e-19"},{"path":"wrang.html","id":"reprex","chapter":"3 Data Wrangling","heading":"3.6 reprex()","text":"Help help youIn order create reproducible example …Step 1. Copy code onto clipboardStep 2. Type reprex() ConsoleStep 3. Look Viewer right. Copy Viewer output GitHub, Piazza, email, stackexchange, etc.places learn reprex includeA blog : https://teachdatascience.com/reprex/reprex vignette: https://reprex.tidyverse.org/index.htmlreprex dos donts: https://reprex.tidyverse.org/articles/reprex-dos--donts.htmlJenny Bryan webinar reprex: “Help help . Creating reproducible examples” https://resources.rstudio.com/webinars/help--help--creating-reproducible-examples-jenny-bryanSome advice: https://stackoverflow.com/help/minimal-reproducible-example","code":""},{"path":"wrang.html","id":"reprex-demo","chapter":"3 Data Wrangling","heading":"3.6.0.1 reprex demo","text":"multiple lines code:","code":"reprex(\n  jan31 + months(0:11) + days(31)\n)reprex({\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})reprex({\n  library(lubridate)\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})"},{"path":"sims.html","id":"sims","chapter":"4 Simulating","heading":"4 Simulating","text":", computer simulations used two main objectives:understand complicated modelsTo assess sensitivity proceduresWe can use simulation studies understand complex estimators, stochastic processes, etc. Often times, analytic solutions exist theory, extremely complicated solve. particular, slight variations model added, simulation often trivial change whereas analytic solution often becomes intractable. Similarly, repeated applications procedure (e.g., linear regression) scenario (e.g., dataset, set parameters, etc.) can provide important insight procedure varies / behaves.","code":""},{"path":"sims.html","id":"simmodels","chapter":"4 Simulating","heading":"4.1 Simulating Complicated Models","text":"Simulation done model scenario allows us understand random behavior without actually replicating entire study multiple times trying model process analytically.example, keen interest understanding probability getting single room draw? getting single north campus? wouldn’t actually run room draw thousands times find probability getting single room. Similarly, situation (e.g., room draw) may much information (e.g., different permutations integers assigned groups 3 4 people) model (easily) closed form solution. moderate assumptions (proportion students groups 1, 2, 3, 4; probability choosing dorm X dorm Y; random allocation integers students; etc.) straightforward simulate scenario thousands time measure proportion times rank (47) give room want (single Sontag).Consider following simulation top 10 GOP candidates get participate debate, remaining 6 kept (example taken debate August 6, 2015). write-(example) years old, process identical process used deciding eligible 2020 Democtratic debates president. http://www.nytimes.com/interactive/2015/07/21/upshot/election-2015--first-gop-debate---role--chance.html?_r=0A candidate needed get least two percent support four different polls published list approved pollsters June 28 August 28, 2019, based open-ended questions may cover either national level one first four primary/caucus states (Iowa, New Hampshire, Nevada, South Carolina). one poll approved pollster counted towards meeting criterion region. Wikipedia\nFigure 1.2: 2016 election Republican primary debates allowed top 10 candidates, ranked national polls NYT\nExample Consider following problem probability. Two points selected randomly line length \\(L\\) opposite sides midpoint line. [words, two points \\(X\\) \\(Y\\) independent random variables \\(X\\) uniformly distributed \\((0,L/2)\\) \\(Y\\) uniformly distributed \\((L/2, 1)\\).] Find probability 3 line segments \\(0\\) \\(X\\), \\(X\\) \\(Y\\), \\(Y\\) \\(L\\) made form three sides triangle. (Note three line segments can made form triangle length less sum lengths others.)joint density :\n\\[ f(x,y) = \\begin{cases} \\frac{4}{L^2} & 0 \\le x \\le L/2, \\, L/2 \\le y \\le L \\\\ 0 & else \\end{cases} \\]three pieces lengths: \\(X\\), \\(Y- X\\) \\(L - Y\\). Three conditions need satisfied order three pieces form triangle:\\[\\begin{align}\nX + (Y- X) &> (L - Y) \\Rightarrow Y > L - Y \\Rightarrow 2 Y > L \\Rightarrow Y > L/2 \\\\\nX + (L-Y ) &> Y - X \\Rightarrow 2X + L > 2Y \\Rightarrow X + \\frac{L}{2} > Y \\\\\nY + (L - Y) &> X \\Rightarrow L > X\n\\end{align}\\]first third conditions always satisfied, just need find probability \\(Y\\) line \\(X + \\frac{L}{2}\\). density previous problem, , , just need find area region line within square \\([0, L/2] \\times [L/2, L]\\), multiply \\(\\frac{4}{L^2}\\).Area = \\(\\displaystyle{ \\frac{1}{2}\\frac{L}{2}\\frac{L}{2} = \\frac{L^2}{8} }\\).Thus, probability \\[\\begin{align}\n\\int_{area} f(x,y)dxdy = \\frac{4}{L^2} \\frac{L^2}{8} =  \\frac{1}{2}.\n\\end{align}\\]happens different values \\(f(x,y)\\)? example, \\(x\\) \\(y\\) Beta(3,47) distributions [0,.5] [.5,1]? Simulating probability R quite straightforward. confidence bounds point estimates probabilities?? [n.b., simulate repeatedly get sense variability estimate!]Example consider problem goal estimate \\(E(X)\\) \\(X=\\max \\{ k: \\sum_{=1}^k U_i < 1 \\}\\) \\(U_i\\) uniform(0,1). simulation problem quite straightforward. Look carefully pieces. broken steps? Notice steps go inside .Set k (number random numbers) equal zero. running sum zero.Generate uniform random variable.Add random variable running sum. Repeat steps 1 2 sum larger 1.Figure many random variables needed get sum larger 1.Repeat entire process many times account variability simulation.Use law large numbers conclude average simulation approximates expected value.","code":"\nsticks <- function() {\n    pointx <- runif(1,0,.5)  # runif is \"random uniform\", not \"run if\"\n    pointy <- runif(1,.5,1)\n    l1 <- pointx\n    l2 <- pointy-pointx\n    l3 <- 1 - pointy\n    max(l1,l2,l3) > 1-max(l1,l2,l3)}\n    \nsum(replicate(100000, sticks())) / 100000## [1] 0.498\nsticks_beta <- function() {\n  pointx <- rbeta(1,3, 47) / 2  # rbeta is random beta\n  pointy <- (rbeta(1, 3, 47) +  1)/2\n  l1 <- pointx\n  l2 <- pointy-pointx\n  l3 <- 1 - pointy\n  max(l1,l2,l3) > 1-max(l1,l2,l3)}\n\nsum(replicate(100000, sticks_beta())) / 100000## [1] 0.499"},{"path":"sims.html","id":"using-a-for-loop","chapter":"4 Simulating","heading":"4.1.0.0.1 Using a for-loop","text":"","code":"\nallk <- c()\nfor(i in 1:1000){\n  k <- 0; sumU <- 0  \n  while(sumU < 1) {\n    sumU <- sumU + runif(1)\n    k <- k+1 }\n  allk <- c(allk, k-1) }\nmean(allk)## [1] 1.73"},{"path":"sims.html","id":"using-functional-programming-and-the-map-function","chapter":"4 Simulating","heading":"4.1.0.1 Using functional programming and the map() function","text":"Functional programming typically much faster loops , also fit cleanly tidy pipeline. map functions (purrr package) named output produce. map() functions include:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vector\nFigure 1.6: Advanced R Wickham. https://adv-r.hadley.nz/functionals.html\nNote first argument always data object second object always function want iteratively apply element input object.use functional programming expected value problem, first step write function (called sum_unif()) select uniform random variables add one. Note function doesn’t arguments.Using map(), sum_unif() function run reps number times. Note sum_unif() doesn’t arguments, doesn’t really matter form input sum_unif().Last, approximate expected value \\(X\\) using law large numbers… sample average converges probability expected value.","code":"\nsum_unif <- function(.x){\n  sumU <- 0\n  k <- 0\n  while(sumU < 1) {\n    sumU <- sumU + runif(1)\n    k <- k+1\n}\n  return(c(k-1, sumU))\n}\nset.seed(4747)\nreps <- 1000\n \nsim_k_max <- data.frame(row_id = seq(1, reps, 1)) %>%\n  mutate(max_for_EX = map(row_id, sum_unif)) %>%\n  unnest(max_for_EX) %>%\n  mutate(output = rep(c(\"k\", \"sum\"), reps)) %>%\n  pivot_wider(id_cols = row_id, names_from = output, \n              values_from = max_for_EX) \n\nsim_k_max## # A tibble: 1,000 × 3\n##    row_id     k   sum\n##     <dbl> <dbl> <dbl>\n##  1      1     2  1.05\n##  2      2     2  1.68\n##  3      3     1  1.39\n##  4      4     1  1.47\n##  5      5     2  1.03\n##  6      6     1  1.45\n##  7      7     1  1.41\n##  8      8     1  1.83\n##  9      9     1  1.17\n## 10     10     2  1.42\n## # … with 990 more rows\nsim_k_max %>%\n  summarize(EX = mean(k))## # A tibble: 1 × 1\n##      EX\n##   <dbl>\n## 1  1.74"},{"path":"sims.html","id":"aside-the-r-function-sample","chapter":"4 Simulating","heading":"4.1.1 Aside: the R function sample()","text":"word “simulate” can mean variety things. course, simulate various settings: sampling, shuffling, resampling. simultion methods can done using R function sample()","code":"\nalph <- letters[1:10]\n\nalph##  [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\nsample(alph, 5, replace = FALSE) # sample (from a population)## [1] \"g\" \"i\" \"h\" \"b\" \"a\"\nsample(alph, 15, replace = TRUE) # sample (from a population)##  [1] \"i\" \"e\" \"d\" \"i\" \"d\" \"a\" \"c\" \"h\" \"a\" \"a\" \"b\" \"f\" \"i\" \"e\" \"h\"\nsample(alph, 10, replace = FALSE)  # shuffle##  [1] \"h\" \"a\" \"e\" \"g\" \"i\" \"c\" \"j\" \"d\" \"f\" \"b\"\nsample(alph, 10, replace = TRUE)  # resample##  [1] \"c\" \"h\" \"i\" \"j\" \"i\" \"b\" \"e\" \"j\" \"g\" \"c\""},{"path":"sims.html","id":"why-do-we-simulate-differently","chapter":"4 Simulating","heading":"4.1.2 Why do we simulate differently?","text":"Three simulating methods used different purposes:Monte Carlo methods - use sampling repeated sampling populations known (either via data via populations) characteristics. [Note, another common tool sampling population use probability model. distribution functions use include rnorm(), runif(), rbinom(), etc.]Monte Carlo methods - use sampling repeated sampling populations known (either via data via populations) characteristics. [Note, another common tool sampling population use probability model. distribution functions use include rnorm(), runif(), rbinom(), etc.]Randomization / Permutation methods - use shuffling (sampling without replacement) test hypotheses “effect.”Randomization / Permutation methods - use shuffling (sampling without replacement) test hypotheses “effect.”Bootstrap methods - use resampling (sampling replacement) establish confidence intervals.Bootstrap methods - use resampling (sampling replacement) establish confidence intervals.","code":""},{"path":"sims.html","id":"goals-of-simulating-complicated-models","chapter":"4 Simulating","heading":"4.1.3 Goals of Simulating Complicated Models","text":"goal simulating complicated model create program provide desired results. also hope able code :problem broken small piecesThe problem checks see works (run lines inside statements!)Simple code best","code":""},{"path":"sims.html","id":"examples-of-pigs-and-blackjack","chapter":"4 Simulating","heading":"4.1.4 Examples of Pigs and Blackjack","text":"","code":""},{"path":"sims.html","id":"pass-the-pigs","chapter":"4 Simulating","heading":"4.1.4.1 Pass the Pigs","text":"Familiarize play Pass Pigs http://www.hasbro.com/common/instruct/passthepigs.pdf https://en.wikipedia.org/wiki/Pass_the_Pigs.information play Pass Pigs, google online resources see following manuscript, http://pubsonline.informs.org/doi/pdf/10.1287/ited.1120.0088 Analytics, Pedagogy Pass Pigs Game, (2012), Gorman, INFORMS Transactions Education 1.sophisticated modeling: http://www.amstat.org/publications/jse/v14n3/datasets.kern.htmlSome strategies playing: http://passpigs.tripod.com/strat.html (link stuff, .)","code":""},{"path":"sims.html","id":"blackjack","chapter":"4 Simulating","heading":"4.1.4.2 Blackjack","text":"Example code come Data Science R: case studies approach computational reasoning problem solving, Nolan Temple Lang, Chapter 9 Simulating Blackjack, Hadley WickhamAll R code online http://rdatasciencecases.org/game blackjack, many online resources can use learn came. Two resources Nolan Temple Lang recommend http://wizardofodds.com/games/blackjack/ http://hitorstand.net/.","code":""},{"path":"sims.html","id":"basic-blackjack","chapter":"4 Simulating","heading":"Basic Blackjack","text":"Card game, goal: sum cards close 21 without going overA nuances card value (e.g., Ace can 1 11)Start 2 cards, build one card timeLots different strategies (also based dealer’s cards)","code":""},{"path":"sims.html","id":"what-do-we-need-to-simulate-poker","chapter":"4 Simulating","heading":"What do we need to simulate poker?","text":"set-cards, dealing, hands“score” (sum cards payout)strategiesresult strategies (summary outcomes)","code":""},{"path":"sims.html","id":"source","chapter":"4 Simulating","heading":"Source","text":"Example code come Data Science R: case studies approach computational reasoning problem solving Nolan Temple Lang.Chapter 9 Simulating Blackjack Hadley WickhamAll R code online http://rdatasciencecases.org/Link also HW4 assignment","code":""},{"path":"sims.html","id":"setting-up-the-game-in-r","chapter":"4 Simulating","heading":"Setting up the Game in R","text":"","code":"\ndeck = rep(c(1:10, 10, 10, 10), 4)\n\nshuffle_decks = function(ndecks){sample(rep(deck, ndecks))}\n\nshuffle_decks(4)##   [1]  6  5 10  4  2  1  2 10  1 10 10  7  9  5 10  4 10  2  6  8  8 10  7  9  3\n##  [26]  1 10 10 10  5  3 10  2 10 10 10  8  3  3 10 10 10  5 10  7  8 10 10  5 10\n##  [51]  9  6 10  8 10  9 10  2  1  4 10  7  3 10 10  8  3  6  6  3  5  5  6  9 10\n##  [76]  9  4 10  6  3  9 10  1 10  8  1  5 10  7  8 10  4 10  1  2  2  8  4  5  8\n## [101]  4  6  8  7  7 10 10  5  3  4 10 10  3 10  1 10 10  4  6  7  8  9 10  4  2\n## [126]  5  6  1 10  4 10  5 10  1  3  7  3  4 10  9 10  6  2 10  2  5  2  2  9 10\n## [151]  1 10  3  8  9  3 10  1 10  5  4  2  5  4  8 10  2  9 10  8  7  7  9  2 10\n## [176]  6  3  4  1 10  1  6  7  9  5 10  1  8  2  7  3  7  1 10  6  4  6  6 10 10\n## [201] 10 10  7  9  9  7 10 10"},{"path":"sims.html","id":"outcome-of-cards-in-hand","chapter":"4 Simulating","heading":"Outcome of cards in hand","text":"","code":"\nhandValue = function(cards) {\n  value = sum(cards)\n  \n       # Check for an Ace and change value if it doesn't bust\n  if (any(cards == 1) && value <= 11) \n    value = value + 10\n    value\n  \n       # Check bust (set to 0); check Blackjack (set to 21.5)\n  if(value > 21)  \n    0 \n  else if (value == 21 && length(cards) == 2)  \n    21.5 # Blackjack\n  else \n    value\n}\nhandValue(c(10,4))## [1] 14\nhandValue(c(10, 4, 9))## [1] 0"},{"path":"sims.html","id":"of-cards-in-hand","chapter":"4 Simulating","heading":"$ of cards in hand","text":"","code":"\nwinnings = function(dealer, players) {\n  if (dealer > 21) {  # Dealer=Blackjack, ties players with Blackjack\n    -1 * (players <= 21)\n  } else if (dealer == 0) { # Dealer busts - all non-busted players win\n    1.5 * (players > 21) +\n      1 * (players <= 21 & players > 0) +\n     -1 * (players == 0) \n  } else {            # Dealer 21 or below, all players > dealer win\n    1.5 * (players > 21) +  \n      1 * (players <= 21 & players > dealer) +\n     -1 * (players <= 21 & players < dealer) \n  }\n}\nwinnings(17,c(20, 21.5, 14, 0, 21))## [1]  1.0  1.5 -1.0 -1.0  1.0"},{"path":"sims.html","id":"better-of-cards-in-hand","chapter":"4 Simulating","heading":"Better $ of cards in hand","text":"","code":"\nwinnings = function(dealer, players){\n  (players > dealer & players > 21) * 1.5 + # Blackjack\n  (players > dealer & players <= 21) * 1 +  # win\n  (players < dealer | players == 0) * -1    # lose\n}\n\nwinnings(17,c(20, 21.5, 14, 0, 21))## [1]  1.0  1.5 -1.0 -1.0  1.0\nwinnings(21.5,c(20, 21.5, 14, 0, 21))## [1] -1  0 -1 -1 -1"},{"path":"sims.html","id":"how-well-does-handvalue-work","chapter":"4 Simulating","heading":"How well does handValue work?","text":"","code":"\ntest_cards = list( c(10, 1), c(10, 5, 6), c(10, 1, 1), \n                   c(7, 6, 1, 5), c(3, 6, 1, 1), \n                   c(2, 3, 4, 10), c(5, 1, 9, 1, 1),\n                   c(5, 10, 7), c(10, 9, 1, 1, 1)) \n\ntest_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0)\nmap_dbl(test_cards, handValue)  # apply the function handValue to test_cards## [1] 21.5 21.0 12.0 19.0 21.0 19.0 17.0  0.0  0.0\nidentical(test_cards_val, map_dbl(test_cards, handValue))## [1] TRUE"},{"path":"sims.html","id":"testing-winnings-create-known","chapter":"4 Simulating","heading":"Testing winnings (create known)","text":"","code":"\ntest_vals = c(0, 16, 19, 20, 21, 21.5)\n\ntestWinnings =\n  matrix(c( -1,  1,  1,  1,  1, 1.5,\n            -1,  0,  1,  1,  1, 1.5,\n            -1, -1,  0,  1,  1, 1.5,\n            -1, -1, -1,  0,  1, 1.5,\n            -1, -1, -1, -1,  0, 1.5,\n            -1, -1, -1, -1, -1, 0), \n         nrow = length(test_vals), byrow = TRUE)\ndimnames(testWinnings) = list(dealer = test_vals, \n                              player = test_vals)\n\ntestWinnings##       player\n## dealer  0 16 19 20 21 21.5\n##   0    -1  1  1  1  1  1.5\n##   16   -1  0  1  1  1  1.5\n##   19   -1 -1  0  1  1  1.5\n##   20   -1 -1 -1  0  1  1.5\n##   21   -1 -1 -1 -1  0  1.5\n##   21.5 -1 -1 -1 -1 -1  0.0"},{"path":"sims.html","id":"does-winnings-work","chapter":"4 Simulating","heading":"Does winnings work?","text":"","code":"\ncheck = testWinnings  # make the matrix the right size\ncheck[] = NA  # make all entries NA\n \nfor(i in seq_along(test_vals)) {\n  for(j in seq_along(test_vals)) {\n    check[i, j] = winnings(test_vals[i], test_vals[j])\n  }\n}\n\nidentical(check, testWinnings)## [1] TRUE"},{"path":"sims.html","id":"function-for-getting-more-cards","chapter":"4 Simulating","heading":"Function for getting more cards","text":"","code":"\nshoe = function(m = 1) sample(deck, m, replace = TRUE)\n\nnew_hand = function(shoe, cards = shoe(2), bet = 1) {\n  list(bet = bet, shoe = shoe, cards = cards)\n}\nmyCards = new_hand(shoe, bet = 7)\nmyCards## $bet\n## [1] 7\n## \n## $shoe\n## function(m = 1) sample(deck, m, replace = TRUE)\n## \n## $cards\n## [1] 10  3"},{"path":"sims.html","id":"first-action-hit","chapter":"4 Simulating","heading":"First action: hit","text":"receive another card","code":"\nhit = function(hand) {\n  hand$cards = c(hand$cards, hand$shoe(1))\n  hand\n}\n\nhit(myCards)$cards## [1] 10  3 10"},{"path":"sims.html","id":"second-action-stand","chapter":"4 Simulating","heading":"Second action: stand","text":"stay current cards","code":"\nstand = function(hand) hand\n\nstand(myCards)$cards## [1] 10  3"},{"path":"sims.html","id":"third-action-double-down","chapter":"4 Simulating","heading":"Third action: double down","text":"double bet receiving exactly one card","code":"\ndd =  function(hand) {\n  hand$bet = hand$bet * 2\n  hand = hit(hand)\n  stand(hand)\n}\n\ndd(myCards)$cards## [1] 10  3 10"},{"path":"sims.html","id":"fourth-action-split-a-pair","chapter":"4 Simulating","heading":"Fourth action: split a pair","text":"create two different hands initial hand two cards value","code":"\nsplitPair = function(hand) {\n  list( new_hand(hand$shoe, \n             cards = c(hand$cards[1], hand$shoe(1)),\n             bet = hand$bet),\n        new_hand(hand$shoe, \n             cards = c(hand$cards[2], hand$shoe(1)),\n             bet = hand$bet))   }\nsplitHand = splitPair(myCards)"},{"path":"sims.html","id":"results-of-splitting","chapter":"4 Simulating","heading":"Results of splitting","text":"(can always split?)","code":"\nsplitHand[[1]]$cards## [1] 10  1\nsplitHand[[2]]$cards## [1] 3 3"},{"path":"sims.html","id":"lets-play-not-yet-automated","chapter":"4 Simulating","heading":"Let’s play! Not yet automated…","text":"","code":"\nset.seed(470); dealer = new_hand(shoe); player = new_hand(shoe); \ndealer$cards[1]## [1] 2\nplayer$cards; player = hit(player); player$cards## [1]  5 10## [1]  5 10  9\ndealer$cards; dealer = hit(dealer); dealer$cards## [1] 2 3## [1] 2 3 3"},{"path":"sims.html","id":"who-won","chapter":"4 Simulating","heading":"Who won?","text":"","code":"\ndealer$cards; player$cards## [1] 2 3 3## [1]  5 10  9\nhandValue(dealer$cards); handValue(player$cards)## [1] 8## [1] 0\nwinnings(handValue(dealer$cards), handValue(player$cards))## [1] -1"},{"path":"sims.html","id":"simply-strategy","chapter":"4 Simulating","heading":"Simply strategy","text":"recall handValue function – player busts?","code":"\nstrategy_simple = function(mine, dealerFaceUp) {\n  if (handValue(dealerFaceUp) > 6 && handValue(mine) < 17) \n     \"H\" \n  else \n     \"S\"\n}"},{"path":"sims.html","id":"better-simple-strategy","chapter":"4 Simulating","heading":"Better simple strategy","text":"","code":"\nstrategy_simple = function(mine, dealerFaceUp) {\n  if (handValue(mine) == 0) return(\"S\")\n  if (handValue(dealerFaceUp) > 6 && handValue(mine) < 17) \n     \"H\" \n  else \n     \"S\"\n}"},{"path":"sims.html","id":"dealer","chapter":"4 Simulating","heading":"Dealer","text":"dealer gets cards regardless player ","code":"\ndealer_cards = function(shoe) {\n  cards = shoe(2)\n  while(handValue(cards) < 17 && handValue(cards) > 0) {\n    cards = c(cards, shoe(1))\n  }\n  cards\n}\n\ndealer_cards(shoe)## [1] 5 9 1 8\ndealer_cards(shoe)## [1]  4  4  7 10"},{"path":"sims.html","id":"playing-a-hand","chapter":"4 Simulating","heading":"Playing a hand","text":"","code":"\nplay_hand = function(shoe, strategy, \n                      hand = new_hand(shoe), \n                      dealer = dealer_cards(shoe)) {\n  \n  face_up_card = dealer[1]\n  \n  action = strategy(hand$cards, face_up_card)\n  while(action != \"S\" && handValue(hand$cards) != 0) {\n    if (action == \"H\") {\n      hand = hit(hand)\n      action = strategy(hand$cards, face_up_card)\n    } else {\n      stop(\"Unknown action: should be one of S, H\")\n    }\n  }  \n\n  winnings(handValue(dealer), handValue(hand$cards)) * hand$bet\n}"},{"path":"sims.html","id":"play-a-few-hands","chapter":"4 Simulating","heading":"Play a few hands","text":"","code":"\nplay_hand(shoe, strategy_simple)## [1] 0\nplay_hand(shoe, strategy_simple)## [1] -1\nplay_hand(shoe, strategy_simple, new_hand(shoe, bet=7))## [1] 0\nplay_hand(shoe, strategy_simple, new_hand(shoe, bet=7))## [1] 7"},{"path":"sims.html","id":"repeated-games","chapter":"4 Simulating","heading":"Repeated games","text":"repeat game, simply repeat play_hand function keep track dollars gained lost.","code":"\nreps=10\nmoney=20\nfor(i in 1:reps){\n  money <- money + play_hand(shoe, strategy_simple)\n  print(money)}## [1] 19\n## [1] 20\n## [1] 19\n## [1] 18\n## [1] 19.5\n## [1] 18.5\n## [1] 20\n## [1] 19\n## [1] 18\n## [1] 19.5"},{"path":"sims.html","id":"simsens","chapter":"4 Simulating","heading":"4.2 Simulating to Assess Sensitivity","text":"second use simulations, can assess sensitivity parameters, model assumptions, sample size, etc. Ideally, results summarized graphically, instead table. graphical representation can often provide insight parameters related, whereas table can hard read.","code":""},{"path":"sims.html","id":"biasmodels","chapter":"4 Simulating","heading":"4.2.1 Bias in Models","text":"example taken directly (mostly verbatim) blog Aaron Roth Algorithmic Unfairness Without Bias Baked .Bias data certainly problem, especially labels gathered human beings. far problem. post, want walk simple example algorithm designer entirely reasonable, human beings injecting bias labels, yet resulting outcome “unfair.” (toy) scenario – specifics aren’t important. High school students applying college, student innate “talent” \\(\\), imagine normally distributed, mean 100 standard deviation 15: \\(\\sim N(100,15)\\). college like admit students sufficiently talented — say one standard deviation mean (, like admit students \\(\\geq 115\\)). problem talent isn’t directly observable. Instead, college can observe grades \\(g\\) SAT scores \\(s\\), noisy estimate talent. simplicity, lets imagine grades SAT scores independently normally distributed, centered student’s talent level, also standard deviation 15: \\(g \\sim N(, 15)\\), \\(s \\sim N(, 15)\\).scenario, college simple, optimal decision rule: run linear regression try predict student talent grades SAT scores, admit students whose predicted talent least 115. indeed “driven math” – since assumed everything normally distributed , turns correspond Bayesian optimal decision rule college.","code":""},{"path":"sims.html","id":"the-data","chapter":"4 Simulating","heading":"The data","text":"Ok. Now lets suppose two populations students, call Reds Blues. Reds majority population, Blues small minority population – Blues make 1% student body. Reds Blues different comes talent: talent distribution, described . bias baked grading exams: Reds Blues also exactly grade exam score distributions, described .one difference: Blues bit money Reds, take SAT twice, report highest two scores college. results small noticeable bump average SAT scores, compared Reds.","code":"\nn_obs <- 100000\nn.red <- n_obs*0.99\nn.blue <- n_obs*0.01\nreds <- rnorm(n.red, mean = 100, sd = 15)\nblues <- rnorm(n.blue, mean = 100, sd = 15)\n\nred.sat <- reds + rnorm(n.red, mean = 0, sd = 15)\nblue.sat <- blues + \n    pmax(rnorm(n.blue, mean = 0, sd = 15),\n         rnorm(n.blue, mean = 0, sd = 15))\n\nred.grade <- reds + rnorm(n.red, mean = 0, sd = 15)\nblue.grade <- blues + rnorm(n.blue, mean = 0, sd = 15)\n\ncollege.data <- data.frame(talent = c(reds, blues),\n                           SAT = c(red.sat, blue.sat),\n                           grades = c(red.grade, blue.grade),\n                           color = c(rep(\"red\", n.red), rep(\"blue\", n.blue)))\n\nggplot(college.data, aes(x = grades, y = SAT, color = color)) +\n  geom_point(size = 0.5) +\n  scale_color_identity(name = \"Color Group\",\n                       guide = \"legend\") +\n  geom_abline(intercept = 0, slope = 1)"},{"path":"sims.html","id":"two-separate-models","chapter":"4 Simulating","heading":"Two separate models","text":"effect use reasonable inference procedure? First, lets consider happens learn two different regression models: one Blues, different one Reds. don’t see much difference:Red classifier makes errors approximately 11.053% time. Blue classifier — makes errors 10.1% time. makes sense: Blues artificially inflated SAT score distribution without increasing talent, classifier picked corrected . fact, even little accurate!since interested fairness, lets think false negative rate classifiers. “False Negatives” setting people qualified attend college (\\(> 115\\)), college mistakenly rejects. really people come harm result classifier’s mistakes. False Negative Rate probability randomly selected qualified person mistakenly rejected college — .e. probability randomly selected student harmed classifier. want false negative rates approximately equal across two populations: mean burden harm caused classifier’s mistakes disproportionately borne one population . one reason difference false negative rates across different populations become standard fairness metric algorithmic fairness — sometimes referred “equal opportunity.”fare metric? badly! Blue model false negative rate 45.161% blues, Red model false negative rate 49.413% reds — difference two satisfyingly small 4.252%.","code":"\nred.lm = college.data %>%\n  dplyr::filter(color == \"red\") %>%\n  lm(talent ~ SAT + grades, data = .)\n\nblue.lm = college.data %>%\n  dplyr::filter(color == \"blue\") %>%\n  lm(talent ~ SAT + grades, data = .)\n\nglobal.lm = college.data %>%\n  lm(talent ~ SAT + grades, data = .)\n\nred.lm %>% broom::tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)   33.2     0.153        216.       0\n## 2 SAT            0.333   0.00151      220.       0\n## 3 grades         0.335   0.00151      222.       0\nblue.lm %>% broom::tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   25.6      1.64        15.6 1.85e- 49\n## 2 SAT            0.429    0.0169      25.3 1.12e-109\n## 3 grades         0.280    0.0157      17.9 3.50e- 62\nglobal.lm %>% broom::tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)   33.2     0.153        217.       0\n## 2 SAT            0.333   0.00151      221.       0\n## 3 grades         0.335   0.00150      223.       0\nnew.reds <- rnorm(n.red, mean = 100, sd = 15)\nnew.blues <- rnorm(n.blue, mean = 100, sd = 15)\n\nnew.red.sat <- new.reds + rnorm(n.red, mean = 0, sd = 15)\nnew.blue.sat <- new.blues + \n    pmax(rnorm(n.blue, mean = 0, sd = 15),\n         rnorm(n.blue, mean = 0, sd = 15))\n\nnew.red.grade <- new.reds + rnorm(n.red, mean = 0, sd = 15)\nnew.blue.grade <- new.blues + rnorm(n.blue, mean = 0, sd = 15)\n\nnew.college.data <- data.frame(talent = c(new.reds, new.blues),\n                           SAT = c(new.red.sat, new.blue.sat),\n                           grades = c(new.red.grade, new.blue.grade),\n                           color = c(rep(\"red\", n.red), rep(\"blue\", n.blue)))\n\n\nnew.red.pred <- new.college.data %>%\n  filter(color == \"red\") %>%\n  predict.lm(red.lm, newdata = .)\n\nnew.blue.pred <- new.college.data %>%\n  filter(color == \"blue\") %>%\n  predict.lm(blue.lm, newdata = .)\n\nnew.college.data <- new.college.data %>% \n  cbind(predicted = c(new.red.pred, new.blue.pred))\n\nggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + \n  geom_point(size = 0.5) + \n  geom_hline(yintercept = 115) + \n  geom_vline(xintercept = 115) +\n  scale_color_identity(name = \"Color Group\",\n                       guide = \"legend\")\nnew.college.data <- new.college.data %>% \n  mutate(fp = ifelse(talent < 115 & predicted > 115, 1, 0),\n         tp = ifelse(talent > 115 & predicted > 115, 1, 0),\n         fn = ifelse(talent > 115 & predicted < 115, 1, 0),\n         tn = ifelse(talent < 115 & predicted < 115, 1, 0))\n\nerror.rates <- new.college.data %>% group_by(color) %>%\n  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),\n            fpr = sum(fp) / (sum(fp) + sum(tn)),\n            fnr = sum(fn) / (sum(fn) + sum(tp)),\n            fdr = sum(fp) / (sum(fp) + sum(tp)),\n            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))\n\nerror.rates## # A tibble: 2 × 6\n##   color   tpr    fpr   fnr   fdr error\n##   <chr> <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 blue  0.548 0.0367 0.452 0.267 0.101\n## 2 red   0.506 0.0379 0.494 0.284 0.111"},{"path":"sims.html","id":"one-global-model","chapter":"4 Simulating","heading":"One global model","text":"might reasonably object: learned separate models Blues Reds, explicitly making admissions decisions function student’s color! might sound like form discrimination, baked algorithm designer — two populations represent e.g. racial groups, explicitly illegal number settings, including lending.happens don’t allow classifier see group membership, just train one classifier whole student body? gap false negative rates two populations balloons 12.791%. Additionally, Blues now higher false positive rate (people don’t talent 115 let accidentally) Reds now higher false negative rate (people talent mistakenly kept ). means qualified member Red population, substantially likely mistakenly rejected classifier qualified member Blue population.","code":"\nnew.pred <- new.college.data %>%\n  predict.lm(global.lm, newdata = .)\n\nnew.college.data <- new.college.data %>% \n  cbind(global.predicted = new.pred)\n\nggplot(new.college.data, aes(x = talent, \n                             y = global.predicted, \n                             color = color)) + \n  geom_point(size = 0.5) + \n  geom_hline(yintercept = 115) + \n  geom_vline(xintercept = 115) +\n  scale_color_identity(name = \"Color Group\",\n                       guide = \"legend\")\nnew.college.data <- new.college.data %>% \n  mutate(fp = ifelse(talent < 115 & global.predicted > 115, 1, 0),\n         tp = ifelse(talent > 115 & global.predicted > 115, 1, 0),\n         fn = ifelse(talent > 115 & global.predicted < 115, 1, 0),\n         tn = ifelse(talent < 115 & global.predicted < 115, 1, 0))\n\nerror.rates <- new.college.data %>% group_by(color) %>%\n  summarize(tpr = sum(tp) / (sum(tp) + sum(fn)),\n            fpr = sum(fp) / (sum(fp) + sum(tn)),\n            fnr = sum(fn) / (sum(fn) + sum(tp)),\n            fdr = sum(fp) / (sum(fp) + sum(tp)),\n            error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) ))\n\nerror.rates## # A tibble: 2 × 6\n##   color   tpr    fpr   fnr   fdr error\n##   <chr> <dbl>  <dbl> <dbl> <dbl> <dbl>\n## 1 blue  0.632 0.0627 0.368 0.351 0.11 \n## 2 red   0.504 0.0377 0.496 0.283 0.111"},{"path":"sims.html","id":"what-happened","chapter":"4 Simulating","heading":"What happened????","text":"happened? wasn’t malice anywhere data pipeline. just Red population much larger Blue population, trained classifier minimize average error entire student body, naturally fit Red population – contributed much average. means classifier longer compensating artificially inflated SAT scores Blues, making disproportionate number errors – favor.kind thing happens time: whenever two populations different feature distributions, learning single classifier (prohibited discriminating based population) fit bigger two populations, simply contribute average error. Depending nature distribution difference, can either benefit detriment minority population. involve explicit human bias, either part algorithm designer data gathering process, exacerbated artificially force algorithm group blind. Well intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes account can actually make things less fair less accurate time.","code":""},{"path":"sims.html","id":"technical-conditions","chapter":"4 Simulating","heading":"4.2.2 Technical Conditions","text":"","code":""},{"path":"sims.html","id":"definitions","chapter":"4 Simulating","heading":"Definitions","text":"p-value probability obtaining observed data extreme given null hypothesis true.\\((1-\\alpha)100\\)% confidence interval range values collected way repeated samples data (using mechanism) capture parameter interest \\((1-\\alpha)100\\)% intervals.","code":""},{"path":"sims.html","id":"examples-1","chapter":"4 Simulating","heading":"Examples","text":"Equal variance t-test Recall one technical conditions t-test two samples come populations variance equal (least var.equal=TRUE specified). happens null hypothesis true (.e., means equal!) technical conditions violated (.e., variances unequal)?","code":""},{"path":"sims.html","id":"t-test-function-for-use-in-map","chapter":"4 Simulating","heading":"4.2.2.0.1 t-test function (for use in map())","text":"","code":"\nt_test_pval <- function(df){\n  t.test(y ~ x1, data = df, var.equal = TRUE) %>%\n    tidy() %>%\n    select(estimate, p.value) \n}"},{"path":"sims.html","id":"generating-data-equal-variance","chapter":"4 Simulating","heading":"4.2.2.0.2 generating data (equal variance)","text":"","code":"\nset.seed(470)\nreps <- 1000\nn_obs <- 20\nnull_data_equal <- \n  data.frame(row_id = seq(1, n_obs, 1)) %>%\n  slice(rep(row_id, each = reps)) %>%\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,1), each = n()/2))\n  ) %>%\n  arrange(sim_id, row_id) %>%\n  group_by(sim_id) %>%\n  nest()"},{"path":"sims.html","id":"summarize-p-values","chapter":"4 Simulating","heading":"4.2.2.0.3 summarize p-values","text":"(Note: rejected 4.5% null tests, close 5%.)Unequal variance t-test","code":"\nnull_data_equal %>% \n  mutate(t_vals = map(data,t_test_pval)) %>%\n  select(t_vals) %>% \n  unnest(t_vals) %>%\n  ungroup(sim_id) %>%\n  summarize(type1error_rate = sum(p.value < 0.05)/reps)## # A tibble: 1 × 1\n##   type1error_rate\n##             <dbl>\n## 1           0.045"},{"path":"sims.html","id":"generating-data-unequal-variance","chapter":"4 Simulating","heading":"4.2.2.0.4 generating data (unequal variance)","text":"","code":"\nset.seed(470)\nreps <- 1000\nn_obs <- 20\nnull_data_unequal <- \n  data.frame(row_id = seq(1, n_obs, 1)) %>%\n  slice(rep(row_id, each = reps)) %>%\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(\"group1\", \"group2\"), each = n()/2),\n    y = rnorm(n(), mean = 10, \n               sd = rep(c(1,100), each = n()/2))\n  ) %>%\n  arrange(sim_id, row_id) %>%\n  group_by(sim_id) %>%\n  nest()"},{"path":"sims.html","id":"summarize-p-values-1","chapter":"4 Simulating","heading":"4.2.2.0.5 summarize p-values","text":"(Note, rejected 5.7% null tests, bad!)Equal variance linear modelThe ISCAM applet Beth Chance Allan Rossman (Chance Rossman 2018b) demonstrates ideas confidence intervals analyst expect inferential assessment.Consider following linear model points normally distributed equal variance around line. [Spoiler: technical conditions met, theory works well. turns confidence interval capture true parameter 95% samples!]\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1)\\]capture true parameter CI? YES!want repeat lots times…FUNCTIONDATAMAPPINGWe captured true slope parameter 95.5% confidence intervals (.e., 95.5% datasets created confidence intervals captured true parameter).Unequal variance linear modelConsider following linear model points normally distributed unequal variance around line. [Spoiler: technical conditions met, theory work well. turns confidence interval capture true parameter 95% samples!]\\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1+ X_1 + 10 \\cdot |X_2|)\\]want repeat lots times…FUNCTIONDATAMAPPINGUsing data unequal variability, captured slope parameter 88% time.","code":"\nnull_data_unequal %>% \n  mutate(t_vals = map(data,t_test_pval)) %>%\n  select(t_vals) %>% \n  unnest(t_vals) %>%\n  ungroup(sim_id) %>%\n  summarize(type1error_rate = sum(p.value < 0.05)/reps)## # A tibble: 1 × 1\n##   type1error_rate\n##             <dbl>\n## 1           0.057\nCI <- lm(y~x1+x2) %>% tidy(conf.int=TRUE) %>% data.frame()\nCI##          term estimate std.error statistic  p.value conf.low conf.high\n## 1 (Intercept)   -0.950     0.148     -6.41 5.39e-09   -1.244    -0.656\n## 2          x1    0.259     0.210      1.23 2.20e-01   -0.158     0.677\n## 3          x2    1.401     0.194      7.21 1.24e-10    1.015     1.787\nCI %>%\n  filter(term == \"x2\") %>%\n  select(term, estimate, conf.low, conf.high) %>%\n  mutate(inside = between(beta2, conf.low, conf.high))##   term estimate conf.low conf.high inside\n## 1   x2      1.4     1.02      1.79   TRUE\nbeta_coef <- function(df){\n  lm(y ~ x1 + x2, data = df) %>%\n    tidy(conf.int = TRUE) %>%\n    filter(term == \"x2\") %>%\n    select(estimate, conf.low, conf.high, p.value) \n}\neqvar_data <- data.frame(row_id = seq(1, n_obs, 1)) %>%\n  slice(rep(row_id, each = reps)) %>%\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, sd = 1)\n  ) %>%\n  arrange(sim_id, row_id) %>%\n  group_by(sim_id) %>%\n  nest()\n\neqvar_data## # A tibble: 1,000 × 2\n## # Groups:   sim_id [1,000]\n##    sim_id data              \n##     <int> <list>            \n##  1      1 <tibble [100 × 4]>\n##  2      2 <tibble [100 × 4]>\n##  3      3 <tibble [100 × 4]>\n##  4      4 <tibble [100 × 4]>\n##  5      5 <tibble [100 × 4]>\n##  6      6 <tibble [100 × 4]>\n##  7      7 <tibble [100 × 4]>\n##  8      8 <tibble [100 × 4]>\n##  9      9 <tibble [100 × 4]>\n## 10     10 <tibble [100 × 4]>\n## # … with 990 more rows\neqvar_data %>% \n  mutate(b2_vals = map(data, beta_coef)) %>%\n  select(b2_vals) %>% \n  unnest(b2_vals) %>%\n  summarize(capture = between(beta2, conf.low, conf.high)) %>%\n  summarize(capture_rate = sum(capture)/reps)## # A tibble: 1 × 1\n##   capture_rate\n##          <dbl>\n## 1        0.955\nbeta_coef <- function(df){\n  lm(y ~ x1 + x2, data = df) %>%\n    tidy(conf.int = TRUE) %>%\n    filter(term == \"x2\") %>%\n    select(estimate, conf.low, conf.high, p.value) \n}\nuneqvar_data <- data.frame(row_id = seq(1, n_obs, 1)) %>%\n  slice(rep(row_id, each = reps)) %>%\n  mutate(\n    sim_id = rep(1:reps, n_obs),\n    x1 = rep(c(0,1), each = n()/2),\n    x2 = runif(n(), min = -1, max = 1),\n    y = beta0 + beta1*x1 + beta2*x2 + rnorm(n(), mean = 0, \n                                            sd = 1 + x1 + 10*abs(x2))\n  ) %>%\n  arrange(sim_id, row_id) %>%\n  group_by(sim_id) %>%\n  nest()\n\nuneqvar_data## # A tibble: 1,000 × 2\n## # Groups:   sim_id [1,000]\n##    sim_id data              \n##     <int> <list>            \n##  1      1 <tibble [100 × 4]>\n##  2      2 <tibble [100 × 4]>\n##  3      3 <tibble [100 × 4]>\n##  4      4 <tibble [100 × 4]>\n##  5      5 <tibble [100 × 4]>\n##  6      6 <tibble [100 × 4]>\n##  7      7 <tibble [100 × 4]>\n##  8      8 <tibble [100 × 4]>\n##  9      9 <tibble [100 × 4]>\n## 10     10 <tibble [100 × 4]>\n## # … with 990 more rows\nuneqvar_data %>% \n  mutate(b2_vals = map(data, beta_coef)) %>%\n  select(b2_vals) %>% \n  unnest(b2_vals) %>%\n  summarize(capture = between(beta2, conf.low, conf.high)) %>%\n  summarize(capture_rate = sum(capture)/reps)## # A tibble: 1 × 1\n##   capture_rate\n##          <dbl>\n## 1        0.861"},{"path":"sims.html","id":"generating-random-numbers","chapter":"4 Simulating","heading":"4.2.3 Generating random numbers","text":"\nresponsible material generating random numbers, ’s pretty cool stuff relies heavily simulation.\n","code":""},{"path":"sims.html","id":"how-do-we-generate-uniform01-numbers","chapter":"4 Simulating","heading":"4.2.3.1 How do we generate uniform[0,1] numbers?","text":"LCG - linear congruence generators. Set \\(,b,m\\) large integers. sequence numbers \\(X_i / m\\) pass tests uniformly distributed variables.\n\\[ X_{n+1} = (aX_n + b) \\mod m \\]\\(m\\) \\(b\\) relatively prime,\\(- 1\\) divisible prime factors \\(m\\),\\(- 1\\) divisible 4 \\(m\\) divisible 4.","code":"\na <- 31541435235\nb <- 23462146143 \nm <- 423514351351\n\nxval <- 47 \nreps <- 10\nunif.val <- c()\n\nfor(i in 1:reps){\n  xval <- (a*xval + b) %% m\n  unif.val <- c(unif.val, xval/m)   }\n\nupdate_rv <- function(x){(a*x + b) %% m }\n\nrep(xval, reps) %>%\n  map(~ accumulate(., ~ ((a*.x + b) %% m))/m )## [[1]]\n## [1] 0.28\n## \n## [[2]]\n## [1] 0.28\n## \n## [[3]]\n## [1] 0.28\n## \n## [[4]]\n## [1] 0.28\n## \n## [[5]]\n## [1] 0.28\n## \n## [[6]]\n## [1] 0.28\n## \n## [[7]]\n## [1] 0.28\n## \n## [[8]]\n## [1] 0.28\n## \n## [[9]]\n## [1] 0.28\n## \n## [[10]]\n## [1] 0.28\ndata.frame(uniformRVs = unif.val) %>%\n  ggplot(aes(x = uniformRVs)) + geom_histogram(bins = 25)"},{"path":"sims.html","id":"generating-other-rvs-the-inverse-transform-method","chapter":"4 Simulating","heading":"4.2.4 Generating other RVs: The Inverse Transform Method","text":"\nresponsible material generating random numbers, ’s pretty cool stuff relies heavily simulation.\n","code":""},{"path":"sims.html","id":"continuous-rvs","chapter":"4 Simulating","heading":"Continuous RVs","text":"Use inverse cumulative distribution function generate data come particular continuous distribution. example, generate 100 random normal deviates. Start assuming \\(F\\) continuous increasing function. Also assume \\(F^{-1}\\) exists.\\[F(x) = P(X \\leq x)\\]\nNote \\(F\\) just area function describing density (histogram) data.Algorithm: Generate Continuous RVGenerate uniform random variable \\(U\\)Set \\(X = F^{-1}(U)\\)Proof: algorithm generates variables come probability distribution represented \\(F\\).\\[\\begin{align}\nP(X \\leq x) &= P(F^{-1}(U) \\leq x)\\\\\n&= P(U \\leq F(x))\\\\\n&= F(x)\\\\\n\\end{align}\\]Example:\n\\[ f(x) = \\begin{cases}\n 2x e^{-x^2} & 0 < x \\\\ 0 & x < 0 \\end{cases}\\]Note: known Weibull(\\(\\lambda=1\\), \\(k=2\\)) distribution.\nFigure 4.1: Weibull PDF Calimo - work, Philip Leitch.. Licensed CC -SA 3.0 via Commons\n\\(F(x)\\)?\n\\[ F(x) = \\int_0^x 2w e^{-w^2} dw = 1 - e^{-x^2}\\]\\(F^{-1}(u)\\)?\\[\\begin{align}\nu &= F(x)\\\\\n&= 1 - e^{-x^2}\\\\\n1-u &= e^{-x^2}\\\\\n-\\ln(1-u) &= x^2\\\\\n\\sqrt{-\\ln(1-u)} &= x\\\\\nF^{-1}(u) &= \\sqrt{-\\ln(1-u)}\n\\end{align}\\]Suppose simulate uniform random variables, \\(U_1, U_2, \\dots\\). \nuse simulate RV’s Weibull density, \\(f(x)\\), given ?\n\\[ \\mbox{Let: } X_i = \\sqrt{-\\ln(1-U_i)}\\]","code":"\nunifdata = runif(10000,0,1)\nweib1data = sqrt(-log(1-unifdata))\nweib2data = rweibull(10000,2,1)\n\nweibdata <- data.frame(weibull = c(weib1data, weib2data),\n                       sim.method = c(rep(\"InvTrans\", 10000), \n                                      rep(\"rweibull\", 10000)))\n\nggplot(weibdata, aes(x = weibull)) + geom_histogram(bins = 25) + \n  facet_grid(~sim.method)"},{"path":"sims.html","id":"discrete-rvs","chapter":"4 Simulating","heading":"Discrete RVs","text":"similar algorithm used generate data come particular discrete distribution. example, generate 100 random normal deviates. start assuming probability mass function \\(X\\) \n\\[ P(X = x_i) = p_i, =1, \\ldots, m\\]Algorithm: Generate Discrete RVGenerate uniform random variable \\(U\\)Transform \\(U\\) \\(X\\) follows,\n\\[X = x_j \\mbox{ } \\sum_{=1}^{j-1} p_i \\leq U \\leq \\sum_{=1}^j p_i\\]Proof: algorithm generates variables come probability mass function \\(\\{p_1, p_2, \\ldots, p_m\\}\\).\\[\\begin{align}\nP(X = x_j) &= \\sum_{=1}^{j-1} p_i \\leq U \\leq \\sum_{=1}^j p_i\\\\\n&= \\sum_{=1}^j p_i - \\sum_{=1}^{j-1} p_i\\\\\n&= p_j\\\\\n\\end{align}\\]","code":""},{"path":"sims.html","id":"what-if-you-dont-know-f-or-cant-calculate-f-1","chapter":"4 Simulating","heading":"What if you don’t know \\(F\\)? Or can’t calculate \\(F^{-1}\\)?","text":"case CDF calculated explicitly (normal example), one still use methodology estimating F collection points \\(x_i, u_i = F(x_i)\\). Now temporarily mimic discrete inverse transform, generate \\(U\\) see subinterval falls , .e. \\(u_i \\leq U \\leq u_{+1}\\). Assuming \\(x_i\\) close enough, expect CDF approximately linear subinterval, take linear interpolation CDF subinterval get \\(X\\) via\\[\\begin{align}\nX = \\frac{u_{+1} -  U}{u_{+1} - u_i} x_i + \\frac{U - u_i}{u_{+1} - u_i} x_j\n\\end{align}\\]However, linear interpolation requires complete approximation \\(F(x)\\), regardless sample size desired, doesn’t generalize higher dimensions, course gives something approximate distribution back, even hands real uniform random variables.","code":""},{"path":"permschp.html","id":"permschp","chapter":"5 Permutation Tests","heading":"5 Permutation Tests","text":"","code":""},{"path":"permschp.html","id":"motivation","chapter":"5 Permutation Tests","heading":"Motivation:","text":"Great video /computational statistical methods can extremely useful. ’s beer mosquitoes! John Rauser Pintrest gives keynote address Strata + Hadoop World Conference October 16, 2014. David Smith, Revolution Analytics blog, October 17, 2014. http://blog.revolutionanalytics.com/2014/10/statistics-doesnt-----hard.htmlA complicated scenario tools applied. point understand gerrymandering. https://www.youtube.com/watch?v=gRCZR_BbjTo&t=125sThe big lesson , IMO, many statistical problems can seem complex, can actually get lot insight recognizing data just one possible instance random process. hypothesis process , can simulate , get intuitive sense surprising data . R excellent tools simulating data, couple hours spent writing code simulate data can often give insights valuable formal data analysis come. (David Smith)Rauser says order follow statistical argument uses simulation, need three things:Ability follow simple logical argument.Random number generation.Iteration","code":""},{"path":"permschp.html","id":"algs","chapter":"5 Permutation Tests","heading":"5.1 Inference Algorithms","text":"","code":""},{"path":"permschp.html","id":"hypothesis-test-algorithm","chapter":"5 Permutation Tests","heading":"5.1.1 Hypothesis Test Algorithm","text":"working nitty gritty details, recall structure hypothesis testing. Consider applet Simulating ANOVA Tables (Chance & Rossman) http://www.rossmanchance.com/applets/AnovaSim.htmlChoose statistic measures effect looking . example, ANOVA F statistic :\\[\\begin{align}\nF &= \\frac{\\text{-group variability}}{\\text{within-group variability}}\\\\\n&= \\frac{\\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{\\cdot})^2/(N-K)}\n\\end{align}\\]Construct sampling distribution statistic effect present population (null sampling distribution). [sampling distributions t statistics F statistics based Central Limit Theorem derived Math 152.]Construct sampling distribution statistic effect present population (null sampling distribution). [sampling distributions t statistics F statistics based Central Limit Theorem derived Math 152.]Locate observed statistic null distribution. value main body distribution easily occur just chance. value tail rarely occur chance evidence something chance operating. [piece going happen permutation tests well analytic tests – point see observed data consistent null distribution.]Locate observed statistic null distribution. value main body distribution easily occur just chance. value tail rarely occur chance evidence something chance operating. [piece going happen permutation tests well analytic tests – point see observed data consistent null distribution.]p-value probability observed data extreme null hypothesis true. [definition analytic, permutation, randomization tests!]\nestimate p-value test significance, estimate sampling distribution test statistic null hypothesis true simulating manner consistent null hypothesis. Alternatively, analytic / mathematical formulas many common statistical hypothesis tests.p-value probability observed data extreme null hypothesis true. [definition analytic, permutation, randomization tests!]\nestimate p-value test significance, estimate sampling distribution test statistic null hypothesis true simulating manner consistent null hypothesis. Alternatively, analytic / mathematical formulas many common statistical hypothesis tests.","code":""},{"path":"permschp.html","id":"permutation-tests-algorithm","chapter":"5 Permutation Tests","heading":"5.1.2 Permutation Tests Algorithm","text":"evaluate p-value permutation test, estimate sampling distribution test statistic null hypothesis true resampling manner consistent null hypothesis (number resamples finite can large!).Choose test statisticChoose test statisticShuffle data (force null hypothesis true)Shuffle data (force null hypothesis true)Create null sampling distribution test statistic (\\(H_0\\))Create null sampling distribution test statistic (\\(H_0\\))Find observed test statistic null sampling distribution compute p-value (observed data extreme). p-value can one two-sided.Find observed test statistic null sampling distribution compute p-value (observed data extreme). p-value can one two-sided.","code":""},{"path":"permschp.html","id":"technical-conditions-1","chapter":"5 Permutation Tests","heading":"Technical Conditions","text":"Permutation tests fall broad class tests called “non-parametric” tests. label indicates distributional conditions required data (.e., condition data come normal binomial distribution). However, test “non-parametric” meant conditions data, simply distributional parametric conditions data. parameters heart almost parametric tests.permutation tests, basing test population parameters, don’t need make claims (.e., mean particular distribution).Permutation different treatments effect. [Note: exchangeability, population, etc.] null hypothesis true, labels assigning groups interchangeable respect probability distribution.\nNote choice statistic makes test sensitive kinds difference (e.g., difference mean) kinds (e.g., difference variance).\nNote choice statistic makes test sensitive kinds difference (e.g., difference mean) kinds (e.g., difference variance).Parametric example, different populations mean.IMPORTANT KEY IDEA point technical conditions parametric permutation tests create sampling distribution accurately reflects null sampling distribution statistic interest (statistic captures relevant research question information).","code":""},{"path":"permschp.html","id":"perms","chapter":"5 Permutation Tests","heading":"5.2 Permutation tests in practice","text":"test interpreted given different types sampling possibly used collect data?Random Sample concept p-value usually comes idea taking sample population comparing sampling distribution (many many random samples).Random Sample concept p-value usually comes idea taking sample population comparing sampling distribution (many many random samples).Random Experiment context randomized experiment, p-value represents observed data compared “happening chance.”\ninterpretation direct: small chance observed statistic take extreme value, result randomization cases: reject null treatment effect hypothesis. CAUSAL!\nRandom Experiment context randomized experiment, p-value represents observed data compared “happening chance.”interpretation direct: small chance observed statistic take extreme value, result randomization cases: reject null treatment effect hypothesis. CAUSAL!Observational Study context observational studies results less strong, reasonable conclude effect observed sample reflects effect present population.\nsample, consider difference (ratio) ask “difference large rarely occur chance particular sample constructed null setting?”\ndata come random sample, sample (results sample) probably consistent population [.e., can infer results back larger population].\nObservational Study context observational studies results less strong, reasonable conclude effect observed sample reflects effect present population.sample, consider difference (ratio) ask “difference large rarely occur chance particular sample constructed null setting?”data come random sample, sample (results sample) probably consistent population [.e., can infer results back larger population].","code":""},{"path":"permschp.html","id":"other-test-statistics","chapter":"5 Permutation Tests","heading":"5.2.0.1 Other Test Statistics","text":"example class used modification ANOVA F-statistic compare observed data permuted data test statistics. Depending data question, permuted test statistic can take variety forms.Depending data, hypotheses, original data collection structure (e.g., random sampling vs random allocation), choice statistic permutation test vary.","code":""},{"path":"permschp.html","id":"permutation-vs.-randomization-tests","chapter":"5 Permutation Tests","heading":"5.2.1 Permutation vs. Randomization Tests","text":"call randomization tests enumerate possible data permutations. permutation tests, hand, permute data \\(B\\) (\\(< <\\) ) times. [authors call permutation test applied randomized experiment randomization test, use term randomization indicate possible permutations considered.]Main difference: randomization tests consider every possible permutation labels, permutation tests take random sample permutations labels.can applied comparison situation (e.g., one sample t-tests).permute labels \\(H_0\\), example, \\(H_0: F(x) = G(x)\\).can used situations sampling distributions unknown (e.g., differences medians).can used situations sampling distributions based population distributions (e.g., ratio variances).Randomization tests first nonparametric tests conceived (R.. Fisher, 1935).","code":""},{"path":"permschp.html","id":"randomization-p-value","chapter":"5 Permutation Tests","heading":"Randomization p-value","text":"Let \\(t^*\\) observed test statistic. two sample test \\(N\\) total observations \\(n\\) observations group 1, \\({N \\choose n}\\) randomizations, equally likely \\(H_0\\). p-value becomes:\n\\[\\begin{align}\np_R &= P(T \\leq t^* | H_0) = \\frac{\\sum_{=1}^{{N \\choose n}} (t_i \\leq t*)}{{N \\choose n}}\n\\end{align}\\]\nchoose significance level \\(\\alpha = k/{N \\choose n}\\), type error rate :\n\\[\\begin{align}\nP(\\text{type error}) &= P(p_R \\leq \\alpha | H_0)\\\\\n&= P\\bigg(\\sum_{=1}^{{N \\choose n}} (t_i \\leq t*) \\leq k | H_0 \\bigg)\\\\\n&= \\frac{k}{{N \\choose n}}= \\alpha\\\\\n\\text{alternatively }  k&= \\alpha {N \\choose n}\n\\end{align}\\]\npoint say randomization test controls probability Type error minimal conditions subjects randomized treatments (minimal condition, hard practice!!)","code":""},{"path":"permschp.html","id":"permutation-p-value","chapter":"5 Permutation Tests","heading":"Permutation p-value","text":"Now consider permutation test randomly permutes data \\(B\\) times (instead \\({N \\choose n}\\) times). permutation test approximates randomization test. fact, permutation test can analyzed using following binomial random variable:\n\\[\\begin{align}\nX_P &= \\# \\ \\mbox{permutations B give extreme value observed test statistic}\\\\\nX_P &\\sim Bin(p_R, B)\\\\\nSE(X_P) &= \\sqrt{\\frac{p_R (1-p_R)}{B}} \\approx \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\n\\end{align}\\]Consider situation interest small effect, say p-value\\(\\approx 0.01\\). SE less 0.001.\n\\[\\begin{align}\n0.001 &= \\sqrt{ (0.01)\\cdot(0.99) / B}\\\\\nB &= (0.01) \\cdot (0.99) / (0.001)^2\\\\\n&= 9900\n\\end{align}\\]Another way look problem use estimated p-value = \\(\\hat{p}_P = \\frac{X_P}{B}\\) come confidence interval \\(p_R\\).CI \\(p_R \\approx \\hat{p}_P \\pm 1.96 \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\\)","code":""},{"path":"permschp.html","id":"ci-from-permutation-tests","chapter":"5 Permutation Tests","heading":"5.2.2 CI from Permutation Tests","text":"Use shifts rescaling create CI parameter value using permutation tests. , consider situation data \\(X\\) \\(Y\\) Use one following transformation (depending study):\n\\[\\begin{align}\nW &= Y + \\\\\n\\mbox{} U &= Y / b\n\\end{align}\\]\nrun permutation test interest \\(X\\) vs. \\(W\\) \\(X\\) vs. \\(U\\). series \\(\\) \\(b\\) values can find don’t reject particular level significance (\\(\\alpha\\)) create \\((1-\\alpha)100\\%\\) confidence interval.Usually, however, use bootstrapping confidence intervals permutation tests hypothesis testing.","code":""},{"path":"permschp.html","id":"randomization-example","chapter":"5 Permutation Tests","heading":"5.2.3 Randomization Example","text":"","code":""},{"path":"permschp.html","id":"fishers-exact-test-computationally-efficient-randomization-test","chapter":"5 Permutation Tests","heading":"5.2.3.1 Fisher’s Exact Test – computationally efficient randomization test","text":"N observations classified 2x2 table.observation classified exactly one cell.Row column totals fixed.Given fixed row column totals, can easily calculate interior distribution using hypergeometric. Note single cell filled, cells determined.\\[\\begin{align}\nP(X=x) &= \\frac{{r \\choose x}{{N-r} \\choose{c-x}}}{{N \\choose c}}\\\\\n& \\mbox{col 1, many row 1?}\\\\\nP(X \\leq x) &= \\sum_{=0}^x \\frac{{r \\choose }{{N-r} \\choose {c-}}}{{N \\choose c}}\\\\\n&= \\mbox{p-value}\n\\end{align}\\]common row column totals fixed. (likely just column totals fixed, e.g., men women.) Instead, consider subsets sample space \\(N\\) observations. particular combination row column totals (\\(rc\\)):\\[\\begin{align}\nP(\\mbox{rejecting } H_0 | rc, H_0) &\\leq& \\alpha\\\\\nP(\\mbox{rejecting } H_0 \\ \\ \\forall \\mbox{ subsets } | H_0) &\\leq& \\sum_{rc \\ combos} P(\\mbox{rejecting } H_0 | rc, H_0) P(rc | H_0)\\\\\n&\\leq& \\alpha\n\\end{align}\\](Note: assume $P(rc | H_0) = 1 / # rc $ combos.) test valid \\(\\alpha\\) level, won’t powerful one fixed columns/rows actually meaningful.","code":""},{"path":"permschp.html","id":"r-times-c-tables","chapter":"5 Permutation Tests","heading":"5.2.3.2 \\(r \\times c\\) tables","text":"permute data new waynew test stat\n\\[\\begin{align}\nT = \\sum_{,j} \\frac{(O_{,j} - E_{,j})^2}{E_{,j}}\n\\end{align}\\]2-sided p-value. expect?\n\\[\\begin{align}\nE_{,j} = \\frac{R_i C_j}{N}\n\\end{align}\\]","code":""},{"path":"permschp.html","id":"example-observer","chapter":"5 Permutation Tests","heading":"5.2.3.2.1 Example: Observer","text":"study published Journal Personality Social Psychology (Butler Baumeister, 1998), researchers investigated conjecture observer vested interest decrease subjects’ performance skill-based task. Subjects given time practice playing video game required navigate obstacle course quickly possible. told play game final time observer present. Subjects randomly assigned one two groups:Group told participant observer win $3 participant beat certain threshold.Group B told participant win prize threshold beaten.goal data analysis determine whether effect observer performance. , like \\(\\chi^2\\) test, hypotheses :\\(H_0:\\) association two variables\n\\(H_a:\\) association two variablesThe data 24 subjects given :Card simulation (demonstrate permutation test works)Permutation Test (see Chance Rossman applet automated permutation test, http://www.rossmanchance.com/applets/ChisqShuffle.htm?FET=1)\\[\\begin{align}\nSE(\\mbox{p-value}) = \\sqrt{\\frac{\\hat{p}_r (1-\\hat{p}_r)}{100}} = 0.02\n\\end{align}\\]Randomization Test\n\\[\\begin{align}\nP(X \\leq 3) = \\sum_{=0}^3 \\frac{{11 \\choose }{12 \\choose {12-}}}{{24 \\choose 12}} = 0.0436\n\\end{align}\\]","code":""},{"path":"permschp.html","id":"r-examples","chapter":"5 Permutation Tests","heading":"5.3 R examples","text":"","code":""},{"path":"permschp.html","id":"cloud-seeding-two-sample-test-computationally-very-difficult-to-do-a-randomization-test","chapter":"5 Permutation Tests","heading":"5.3.1 Cloud Seeding (Two sample test – computationally very difficult to do a randomization test)","text":"Cloud seeding data: seeding seeding randomly allocated 52 days seeding appropriate. pilot know whether plane seeding. Rain measured acre-feet.running tests compare means variances obtain following p-values:","code":""},{"path":"permschp.html","id":"r-code","chapter":"5 Permutation Tests","heading":"5.3.1.1 R code","text":"anything, let’s look data. , visualize boxplots histograms. Also, visualize raw scale well log scale. Certainly, log10 scale indicates transformation makes data symmetric.unlogged data:, ’ve formally gone permutation. , resampling coded particularly tidy way, tidy way code loops! Generally, loops fasted way code R, need quickly run code seems like go loop, likely purrr direction want go, https://purrr.tidyverse.org/.","code":"\nclouds <- read_delim(\"figs/cloud-seeding.txt\", \n     \"\\t\", escape_double = FALSE, trim_ws = TRUE) \n\nnames(clouds) <- c(\"unseeded\", \"seeded\")\nclouds <- tidyr::pivot_longer(clouds, cols = 1:2, names_to = \"seeding\", values_to = \"rainfall\") %>%\n  mutate(seeding = as.factor(seeding))\n\nclouds %>%\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot()\nclouds %>%\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding)\nclouds %>%\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() + scale_y_log10()\nclouds %>%\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) + scale_x_log10()\nclouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain))## # A tibble: 2 × 3\n##   seeding  meanrain meanlnrain\n##   <fct>       <dbl>      <dbl>\n## 1 seeded       442.       5.13\n## 2 unseeded     165.       3.99\nclouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %>%\n  summarize(diff(meanrain), diff(meanlnrain))## # A tibble: 1 × 2\n##   `diff(meanrain)` `diff(meanlnrain)`\n##              <dbl>              <dbl>\n## 1            -277.              -1.14\nraindiffs <- clouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %>%\n  summarize(diffrain = diff(meanrain), difflnrain = diff(meanlnrain))\n\nraindiffs## # A tibble: 1 × 2\n##   diffrain difflnrain\n##      <dbl>      <dbl>\n## 1    -277.      -1.14"},{"path":"permschp.html","id":"difference-in-means-after-permuting","chapter":"5 Permutation Tests","heading":"5.3.1.1.1 Difference in means after permuting","text":"","code":"\nreps <- 1000\npermdiffs <- c()\n\nfor(i in 1:reps){\n  onediff <- clouds %>%\n    mutate(permseed = sample(seeding)) %>%\n    group_by(permseed) %>%\n    summarize(meanrain = mean(rainfall)) %>%\n    summarize(diff(meanrain)) %>% pull()\n  \npermdiffs <- c(permdiffs, onediff)\n}\n\npermdiffs %>% data.frame() %>%\n  ggplot(aes(x = permdiffs)) + geom_histogram(bins=30) + geom_vline(xintercept = raindiffs$diffrain, color = \"red\")"},{"path":"permschp.html","id":"ratio-of-variances-after-permuting","chapter":"5 Permutation Tests","heading":"5.3.1.1.2 Ratio of variances after permuting","text":"","code":"\nrainvarratio <- clouds %>%\n    group_by(seeding) %>%\n    summarize(varrain = var(rainfall)) %>%\n    summarize(rainratio = varrain[1] / varrain[2])\n\n\nreps <- 1000\npermvars <- c()\n\nfor(i in 1:reps){\n  oneratio <- clouds %>%\n    mutate(permseed = sample(seeding)) %>%\n    group_by(permseed) %>%\n    summarize(varrain = var(rainfall)) %>%\n    summarize(varrain[1] / varrain[2]) %>% pull()\n  \npermvars <- c(permvars, oneratio)\n}\n\npermvars %>% data.frame() %>%\n  ggplot(aes(x = permvars)) + geom_histogram(bins=30) + geom_vline(xintercept = rainvarratio$rainratio , color = \"red\")"},{"path":"permschp.html","id":"testing-differences-in-means-or-ratios-of-variances","chapter":"5 Permutation Tests","heading":"5.3.1.1.3 Testing differences in means or ratios of variances","text":"evidenced histograms ,permutation test (one-sided) difference means count number permuted differences less equal observed difference means, just 1%.permutation test (one-sided) difference means count number permuted differences less equal observed difference means, just 1%.permutation test (one-sided) ratio variances count number permuted ratios greater equal observed ratio variances, 7%.permutation test (one-sided) ratio variances count number permuted ratios greater equal observed ratio variances, 7%.","code":"\n(sum(raindiffs$diffrain >= permdiffs) + 1) /1000## [1] 0.02\n(sum(rainvarratio$rainratio <= permvars)+1)/1000## [1] 0.075"},{"path":"permschp.html","id":"macnell-teaching-evaluations-stratified-two-sample-t-test","chapter":"5 Permutation Tests","heading":"5.3.2 MacNell Teaching Evaluations (Stratified two-sample t-test)","text":"Boring et al. (2016) reanalyze data MacNell et al. (2014). Students randomized 4 online sections course. two sections, instructors swapped identities. instructor identified female rated lower average? (https://www.math.upenn.edu/~pemantle/active-papers/Evals/stark2016.pdf)\nFigure 5.1: Kraj (2017)\n\nFigure 5.2: Kraj (2017)\n\nFigure 5.3: Mengel, Sauermann, Zölitz (2019)\n\nFigure 5.4: MacNell, Driscoll, Hunt (2015)\n\nFigure 5.5: MacNell, Driscoll, Hunt (2015)\n","code":""},{"path":"permschp.html","id":"r-code-1","chapter":"5 Permutation Tests","heading":"5.3.2.0.1 R code","text":"","code":"\n#macnell <- readr::read_csv(\"https://raw.githubusercontent.com/statlab/permuter/master/data-raw/macnell.csv\")\nlibrary(permuter)\ndata(macnell)\nlibrary(ggridges)\nmacnell %>% \n  mutate(TAID = ifelse(taidgender==1, \"male\", \"female\")) %>%\n  mutate(TAGend = ifelse(tagender==1, \"male\", \"female\")) %>%\nggplot(aes(y=TAGend, x=overall, group = interaction(TAGend, TAID), \n           fill=TAID)) +\n  geom_point(position=position_jitterdodge(jitter.height=0.3, jitter.width = 0, dodge.width = 0.4), aes(color = TAID)) +\n  stat_summary(fun=\"mean\", geom=\"crossbar\", size=.3, width = 1,\n               aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  stat_summary(fun=\"mean\", geom=\"point\", shape = \"X\",\n               size=5, aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  coord_flip() +\n  xlab(\"\") + ggtitle(\"Overall teaching effectiveness score\")"},{"path":"permschp.html","id":"analysis-goal","chapter":"5 Permutation Tests","heading":"5.3.2.1 Analysis goal","text":"Want know score perceived gender different.\\[H_0:  \\mu_{ID.Female} = \\mu_{ID.Male}\\]\n> Although permutation test, null hypothesis means population distributions , variance aspects distributions across perceived gender.","code":""},{"path":"permschp.html","id":"macnell-data-without-permutation","chapter":"5 Permutation Tests","heading":"5.3.2.2 MacNell Data without permutation","text":"","code":"\nmacnell %>%\n  select(overall, tagender, taidgender) %>% head(15)##    overall tagender taidgender\n## 1        4        0          1\n## 2        4        0          1\n## 3        5        0          1\n## 4        5        0          1\n## 5        5        0          1\n## 6        4        0          1\n## 7        4        0          1\n## 8        5        0          1\n## 9        4        0          1\n## 10       3        0          1\n## 11       5        0          1\n## 12       4        0          1\n## 13       5        1          1\n## 14       5        1          1\n## 15       4        1          1"},{"path":"permschp.html","id":"permuting-macnell-data","chapter":"5 Permutation Tests","heading":"5.3.2.3 Permuting MacNell data","text":"Conceptually, two levels randomization:\\(N_m\\) students randomly assigned male instructor \\(N_f\\) assigned female instructor.\\(N_m\\) students randomly assigned male instructor \\(N_f\\) assigned female instructor.\\(N_j\\) assigned instructor \\(j\\), \\(N_{jm}\\) told instructor male, \\(N_{jf}\\) told instructor female \\(j=m,f\\).\\(N_j\\) assigned instructor \\(j\\), \\(N_{jm}\\) told instructor male, \\(N_{jf}\\) told instructor female \\(j=m,f\\).Stratified two-sample test:instructor, permute perceived gender assignments.Use difference mean ratings female-identified vs male-identified instructors.","code":"\nmacnell %>%\n  group_by(tagender, taidgender) %>%\n  summarize(n())## # A tibble: 4 × 3\n## # Groups:   tagender [2]\n##   tagender taidgender `n()`\n##      <int>      <int> <int>\n## 1        0          0    11\n## 2        0          1    12\n## 3        1          0    13\n## 4        1          1    11macnell %>%   group_by(tagender) %>%  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%  select(overall, tagender, taidgender, permTAID)\n## # A tibble: 47 × 4\n## # Groups:   tagender [2]\n##    overall tagender taidgender permTAID\n##      <dbl>    <int>      <int>    <int>\n##  1       4        0          1        1\n##  2       4        0          1        1\n##  3       5        0          1        1\n##  4       5        0          1        1\n##  5       5        0          1        1\n##  6       4        0          1        1\n##  7       4        0          1        0\n##  8       5        0          1        0\n##  9       4        0          1        0\n## 10       3        0          1        0\n## # … with 37 more rows\nmacnell %>%   group_by(tagender) %>%  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%  ungroup(tagender) %>%  group_by(permTAID) %>%  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%  summarize(diff(pmeans))\n## # A tibble: 1 × 1\n##   `diff(pmeans)`\n##            <dbl>\n## 1        -0.0870\n\ndiff_means_func <- function(.x){\n  macnell %>% group_by(tagender) %>%\n  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%\n  ungroup(tagender) %>%\n  group_by(permTAID) %>%\n  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%\n  summarize(diff_mean = diff(pmeans))\n  }\n\nmap_df(1:5, diff_means_func)## # A tibble: 5 × 1\n##   diff_mean\n##       <dbl>\n## 1   -0.188 \n## 2   -0.0909\n## 3   -0.474 \n## 4   -0.0909\n## 5    0.277"},{"path":"permschp.html","id":"observed-vs.-permuted-statistic","chapter":"5 Permutation Tests","heading":"5.3.2.4 Observed vs. Permuted statistic","text":"","code":"\n# observed\nmacnell %>% \n  group_by(taidgender) %>%\n  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%\n  summarize(diff_mean = diff(pmeans))## # A tibble: 1 × 1\n##   diff_mean\n##       <dbl>\n## 1     0.474\n# permuted\nset.seed(47)\nreps = 1000\nperm_diff_means <- map_df(1:reps, diff_means_func)"},{"path":"permschp.html","id":"permutation-sampling-distribution","chapter":"5 Permutation Tests","heading":"5.3.2.5 permutation sampling distribution:","text":".pull-right[]","code":""},{"path":"permschp.html","id":"actual-macnell-results","chapter":"5 Permutation Tests","heading":"5.3.2.6 Actual MacNell results","text":"","code":""},{"path":"permschp.html","id":"income-and-health-f-like-test","chapter":"5 Permutation Tests","heading":"5.3.3 Income and Health (F-like test)","text":"Consider NHANES dataset.Income\n(HHIncomeMid - Numerical version HHIncome derived middle income category)\n(HHIncomeMid - Numerical version HHIncome derived middle income category)Health\n(HealthGen - Self-reported rating participant’s health general Reported participants aged 12 years older. One Excellent, Vgood, Good, Fair, Poor.)\n(HealthGen - Self-reported rating participant’s health general Reported participants aged 12 years older. One Excellent, Vgood, Good, Fair, Poor.)","code":""},{"path":"permschp.html","id":"summary-of-the-variables-of-interest","chapter":"5 Permutation Tests","heading":"5.3.3.1 Summary of the variables of interest","text":"","code":"\nNHANES %>% select(HealthGen) %>% table()## .\n## Excellent     Vgood      Good      Fair      Poor \n##       878      2508      2956      1010       187\nNHANES %>% select(HHIncomeMid) %>% summary()##   HHIncomeMid    \n##  Min.   :  2500  \n##  1st Qu.: 30000  \n##  Median : 50000  \n##  Mean   : 57206  \n##  3rd Qu.: 87500  \n##  Max.   :100000  \n##  NA's   :811"},{"path":"permschp.html","id":"mean-income-broken-down-by-health","chapter":"5 Permutation Tests","heading":"5.3.3.2 Mean Income broken down by Health","text":"differences means simply due random chance??differences health, can calculated directly, still don’t know differences due randome chance larger structure.","code":"\nNH.means <- NHANES %>% \n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) %>% \n  group_by(HealthGen) %>% \n  summarize(IncMean = mean(HHIncomeMid, na.rm=TRUE), count=n())\nNH.means## # A tibble: 5 × 3\n##   HealthGen IncMean count\n##   <fct>       <dbl> <int>\n## 1 Excellent  69354.   817\n## 2 Vgood      65011.  2342\n## 3 Good       55662.  2744\n## 4 Fair       44194.   899\n## 5 Poor       37027.   164\nNHANES %>% filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>% \nggplot(aes(x=HealthGen, y=HHIncomeMid)) + \n  geom_boxplot() + \n  geom_jitter(width=0.1, alpha=.2)##           Excellent  Vgood   Good  Fair  Poor\n## Excellent         0   4344  13692 25161 32327\n## Vgood         -4344      0   9348 20817 27983\n## Good         -13692  -9348      0 11469 18635\n## Fair         -25161 -20817 -11469     0  7166\n## Poor         -32327 -27983 -18635 -7166     0"},{"path":"permschp.html","id":"overall-difference","chapter":"5 Permutation Tests","heading":"5.3.3.3 Overall difference","text":"can measure overall differences amount variability means overall mean:\\[F = \\frac{\\text{-group variability}}{\\text{within-group variability}}\\]\n\\[F = \\frac{\\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{\\cdot})^2/(N-K)}\\]\n\\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2\\]","code":""},{"path":"permschp.html","id":"creating-a-test-statistic","chapter":"5 Permutation Tests","heading":"5.3.3.4 Creating a test statistic","text":"\\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2\\]","code":"\nNHANES %>% select(HHIncomeMid, HealthGen) %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid))## # A tibble: 6,966 × 2\n##    HHIncomeMid HealthGen\n##          <int> <fct>    \n##  1       30000 Good     \n##  2       30000 Good     \n##  3       30000 Good     \n##  4       40000 Good     \n##  5       87500 Vgood    \n##  6       87500 Vgood    \n##  7       87500 Vgood    \n##  8       30000 Vgood    \n##  9      100000 Vgood    \n## 10       70000 Fair     \n## # … with 6,956 more rows\nGM <- mean(NHANES$HHIncomeMid, na.rm=TRUE)\n\nGM## [1] 57206\nNH.means## # A tibble: 5 × 3\n##   HealthGen IncMean count\n##   <fct>       <dbl> <int>\n## 1 Excellent  69354.   817\n## 2 Vgood      65011.  2342\n## 3 Good       55662.  2744\n## 4 Fair       44194.   899\n## 5 Poor       37027.   164\nNH.means$IncMean - GM## [1]  12148   7805  -1544 -13013 -20179\n(NH.means$IncMean - GM)^2## [1] 1.48e+08 6.09e+07 2.38e+06 1.69e+08 4.07e+08\nNH.means$count## [1]  817 2342 2744  899  164\nNH.means$count * (NH.means$IncMean - GM)^2## [1] 1.21e+11 1.43e+11 6.54e+09 1.52e+11 6.68e+10\nsum(NH.means %>% select(count) %>% pull() * \n      (NH.means %>% select(IncMean) %>% pull() - GM)^2)## [1] 4.89e+11"},{"path":"permschp.html","id":"permuting-the-data","chapter":"5 Permutation Tests","heading":"5.3.3.5 Permuting the data","text":"","code":"\nNHANES %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n  select(HealthGen, HHIncomeMid, IncomePerm) ## # A tibble: 6,966 × 3\n##    HealthGen HHIncomeMid IncomePerm\n##    <fct>           <int>      <int>\n##  1 Good            30000      87500\n##  2 Good            30000      70000\n##  3 Good            30000     100000\n##  4 Good            40000      40000\n##  5 Vgood           87500      87500\n##  6 Vgood           87500     100000\n##  7 Vgood           87500      50000\n##  8 Vgood           30000      50000\n##  9 Vgood          100000      70000\n## 10 Fair            70000      22500\n## # … with 6,956 more rows"},{"path":"permschp.html","id":"permuting-the-data-a-new-test-statistic","chapter":"5 Permutation Tests","heading":"5.3.3.6 Permuting the data & a new test statistic","text":"","code":"\nNHANES %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n  group_by(HealthGen) %>% \n  summarize(IncMeanP = mean(IncomePerm), count=n()) %>%\n  summarize(teststat = sum(count*(IncMeanP - GM)^2))## # A tibble: 1 × 1\n##       teststat\n##          <dbl>\n## 1 14268151317."},{"path":"permschp.html","id":"lots-of-times","chapter":"5 Permutation Tests","heading":"5.3.3.7 Lots of times…","text":"","code":"\nreps <- 1000\n\nSSB_perm_func <- function(.x){\n  NHANES %>% \n        filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n        mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n        group_by(HealthGen) %>% \n        summarize(IncMeanP = mean(IncomePerm), count=n()) %>%\n        summarize(teststat = sum(count*(IncMeanP - GM)^2)) \n}\n\nSSB_perm_val <- map_dfr(1:reps, SSB_perm_func)\n\nSSB_perm_val## # A tibble: 1,000 × 1\n##        teststat\n##           <dbl>\n##  1 11839838857.\n##  2 14805138617.\n##  3 12238328218.\n##  4 14493898296.\n##  5 19052560418.\n##  6 14099580957.\n##  7 19808304723.\n##  8 14972708855.\n##  9 15543404291.\n## 10 18334398022.\n## # … with 990 more rows"},{"path":"permschp.html","id":"compared-to-the-real-data","chapter":"5 Permutation Tests","heading":"5.3.3.8 Compared to the real data","text":"","code":"\nSSB_obs <- NHANES %>%\n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) %>% \n  group_by(HealthGen) %>% \n  summarize(IncMean = mean(HHIncomeMid), count=n()) %>%\n  summarize(obs_teststat = sum(count*(IncMean - GM)^2)) \n\nSSB_obs ## # A tibble: 1 × 1\n##    obs_teststat\n##           <dbl>\n## 1 488767088754.\nsum(SSB_perm_val %>% pull() > SSB_obs %>% pull() ) / reps## [1] 0\nSSB_perm_val %>%\n  ggplot(aes(x = teststat)) +\n  geom_histogram() + \n  geom_vline(data = SSB_obs, aes(xintercept = obs_teststat), color = \"red\") +\n  ylab(\"\") + xlab(\"Permuted SSB Test Statistics\")"},{"path":"boot.html","id":"boot","chapter":"6 Bootstrapping","heading":"6 Bootstrapping","text":"","code":""},{"path":"boot.html","id":"introduction","chapter":"6 Bootstrapping","heading":"6.1 Introduction","text":"permutation tests, going use random samples describe population (assuming simple random sample).Main idea: able estimate variability estimator (difference medians, ordinary least square non-normal errors, etc.).’s strange get \\(\\hat{\\theta}\\) SE(\\(\\hat{\\theta}\\)) data (consider \\(\\hat{p}\\) & \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) \\(\\overline{X}\\) & \\(s/\\sqrt{n}\\)).’ll consider confidence intervals now.Bootstrapping doesn’t help get around small samples.following applets may helpful:logic confidence intervals http://www.rossmanchance.com/applets/ConfSim.htmlBootstrapping actual datasets http://lock5stat.com/statkey/index.html","code":""},{"path":"boot.html","id":"BSnotation","chapter":"6 Bootstrapping","heading":"6.2 Basics & Notation","text":"Let \\(\\theta\\) parameter interest, let \\(\\hat{\\theta}\\) estimate \\(\\theta\\). , ’d take lots samples size \\(n\\) population create sampling distribution \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples \\(F\\):\\[\\begin{align}\n\\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{=1}^B \\hat{\\theta}_i\n\\end{align}\\]\nbest guess \\(\\theta\\). \\(\\hat{\\theta}\\) different \\(\\theta\\), call biased.\n\\[\\begin{align}\nSE(\\hat{\\theta}) &= \\bigg[ \\frac{1}{B-1} \\sum_{=1}^B(\\hat{\\theta}_i - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\\nq_1 &= [0.25 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\\nq_3 &= [0.75 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\\n\\end{align}\\], completely characterize sampling distribution (function \\(\\theta\\)) allow us make inference \\(\\theta\\) \\(\\hat{\\theta}\\).\nFigure 1.2: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig\n","code":""},{"path":"boot.html","id":"the-plug-in-principle","chapter":"6 Bootstrapping","heading":"6.2.1 The Plug-in Principle","text":"Recall\n\\[\\begin{align}\nF(x) &= P(X \\leq x)\\\\\n\\hat{F}(x) &= S(x) = \\frac{\\# \\{X_i \\leq x\\} }{n}\n\\end{align}\\]\n\\(\\hat{F}(x)\\) sufficient statistic \\(F(x)\\). , information \\(F\\) data contained \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) MLE \\(F(x)\\) (probabilities, ’s binomial argument).Note , general, interested parameter, \\(\\theta\\).\n\\[\\begin{align}\n\\theta = t(F) \\ \\ \\ \\ [\\mbox{e.g., } \\mu = \\int x f(x) dx ]\n\\end{align}\\]plug-estimate \\(\\theta\\) :\n\\[\\begin{align}\n\\hat{\\theta} = t(\\hat{F}) \\ \\ \\ \\ [\\mbox{e.g., } \\overline{X} = \\frac{1}{n} \\sum X_i ]\n\\end{align}\\]: estimate parameter, use statistic corresponding quantity sample.\\[\\begin{align}\n\\mbox{Ideal Real World} & \\mbox{Boostrap World}\\\\\nF \\rightarrow x &\\Rightarrow \\hat{F} \\rightarrow x^*\\\\\n\\downarrow &  \\downarrow\\\\\n\\hat{\\theta}  & \\hat{\\theta}^*\n\\end{align}\\]idea boostrapping (fact, bootstrap samples ), depends double arrow. must random sample: , \\(\\hat{F}\\) must good job estimating \\(F\\) order bootstrap concepts meaningful.Note ’ve seen plug--principle :\n\\[\\begin{align}\n\\sqrt{\\frac{p(1-p)}{n}} &\\approx& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\end{align}\\]\n","code":""},{"path":"boot.html","id":"the-bootstrap-idea","chapter":"6 Bootstrapping","heading":"6.2.2 The Bootstrap Idea","text":"can resample sample represent samples actual population! boostrap distribution statistic, based many resamples, represents sampling distribution statistic based many samples. okay?? assuming?\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)","code":""},{"path":"boot.html","id":"bootstrap-procedure","chapter":"6 Bootstrapping","heading":"6.2.3 Bootstrap Procedure","text":"Resample data replacement original sample.Calculate statistic interest resample.Repeat 1. 2. \\(B\\) times.Use bootstrap distribution inference.","code":""},{"path":"boot.html","id":"bootstrap-notation","chapter":"6 Bootstrapping","heading":"6.2.4 Bootstrap Notation","text":"Take many (\\(B\\)) resamples size \\(n\\) sample, \\(\\hat{F}(x)\\) (instead population, \\(F(x)\\) ) create bootstrap distribution \\(\\hat{\\theta}^*\\) (instead sampling distribution \\(\\hat{\\theta}\\)).Let \\(\\hat{\\theta}^*(b)\\) calculated statistic interest \\(b^{th}\\) bootstrap sample. best guess \\(\\theta\\) :\n\\[\\begin{align}\n\\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*(b)\n\\end{align}\\]\n(\\(\\hat{\\theta}^*\\) different \\(\\hat{\\theta}\\), call biased.) estimated value standard error estimate \n\\[\\begin{align}\n\\hat{SE}^* = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*(b) - \\hat{\\theta}^*)^2 \\bigg]^{1/2}\n\\end{align}\\]Just like repeatedly taking samples population, taking resamples sample allows us characterize bootstrap distribution approximates sampling distribution. bootstrap distribution approximates shape, spread, & bias actual sampling distribution.\nFigure 1.3: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig. left image represents mean n=50. center image represents mean n=9. right image represents median n=15.\nStatKey applets demonstrate bootstrapping : http://www.lock5stat.com/StatKey/","code":""},{"path":"boot.html","id":"BSCI","chapter":"6 Bootstrapping","heading":"6.3 Bootstrap Confidence Intervals","text":"","code":""},{"path":"boot.html","id":"normal-standard-ci-with-bootse-typenorm","chapter":"6 Bootstrapping","heading":"6.3.1 Normal (standard) CI with BootSE: type=\"norm\"","text":"Keep mind trying approximate sampling distribution \\(\\hat{\\theta}\\). fact, really able estimate sampling distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). hope :\\[\\begin{align}\n\\hat{F}\\Big(\\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)} \\Big) \\rightarrow F\\Big(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\Big)\n\\end{align}\\]Recall derivation conventional confidence intervals (based assumption sampling distribution test statistic normal close):\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\], ’s endpoints random, 0.95 probability ’ll get random sample produce endpoints capture true parameter.95% CI \\(\\theta\\) : \\[\\hat{\\theta} \\pm z_{(\\alpha/2)} \\hat{SE}^*\\]","code":""},{"path":"boot.html","id":"bootstrap-t-confidence-intervals-typestud","chapter":"6 Bootstrapping","heading":"6.3.2 Bootstrap-t Confidence Intervals: type=\"stud\"","text":"(idea calculating “t-multiplier” used CI. William Gosset went pseudonym “Student” originally figured distribution t-multiplier, following intervals called either “studentized” “t” bootstrap confidence intervals.)Recall derivation conventional confidence intervals:\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\], ’s endpoints random, 0.95 probability ’ll get random sample produce endpoints capture true parameter.use Boot SE within CI formula (interval ). problem interval accurate distribution \\(\\hat{\\theta}\\) reasonably normal. bias skew, CI desired coverage levels (Efron Tibshirani (1993), pg 161 chapter 22).use Boot SE within CI formula (interval ). problem interval accurate distribution \\(\\hat{\\theta}\\) reasonably normal. bias skew, CI desired coverage levels (Efron Tibshirani (1993), pg 161 chapter 22).Now consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{align}\nT^*(b) &= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{align}\\]Now consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{align}\nT^*(b) &= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{align}\\]\\(\\hat{\\theta}^*(b)\\) value \\(\\hat{\\theta}\\) \\(b^{th}\\) bootstrap sample, \\(\\hat{SE}^*(b)\\) estimated standard error \\(\\hat{\\theta}^*(b)\\) \\(b^{th}\\) bootstrap sample. \\(\\alpha^{th}\\) percentile \\(T^*(b)\\) estimated value \\(\\hat{t}^*_\\alpha\\) \\[\\begin{align}\n\\frac{\\# \\{T^*(b) \\leq \\hat{t}^*_{\\alpha/2} \\} }{B} = \\alpha/2\n\\end{align}\\]example, \\(B=1000\\), estimate 5% point \\(50^{th}\\) smallest value \\(T^*(b)\\)s, estimate 95% point \\(950^{th}\\) smallest value \\(T^*(b)\\)s.Finally, boostrap-t confidence interval :\n\\[\\begin{equation} \n(\\hat{\\theta} - \\hat{t}^*_{1-\\alpha/2}\\hat{SE}^*,  \\hat{\\theta} - \\hat{t}^*_{\\alpha/2}\\hat{SE}^*) \\tag{6.1}\n\\end{equation}\\]find bootstrap-t interval, bootstrap twice. algorithm follows:Generate \\(B_1\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*(b)\\).Generate \\(B_1\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*(b)\\).Take \\(B_2\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Take \\(B_2\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Find \\(B_1\\) values \\(T^*(b)\\). Calculate \\(\\hat{t}^*_\\alpha/2\\) \\(\\hat{t}^*_{1-\\alpha/2}\\).Find \\(B_1\\) values \\(T^*(b)\\). Calculate \\(\\hat{t}^*_\\alpha/2\\) \\(\\hat{t}^*_{1-\\alpha/2}\\).Calculate CI equation ((6.1)).Calculate CI equation ((6.1)).\\(B\\cdot \\alpha\\) integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) \\(B+1-k\\).\\(B\\cdot \\alpha\\) integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) \\(B+1-k\\).Bootstrap-t intervals somewhat erratic can influenced outliers. Percentile methods can reliable. [balance best open question depending lot data distribution statistic interest.]Bootstrap-t intervals somewhat erratic can influenced outliers. Percentile methods can reliable. [balance best open question depending lot data distribution statistic interest.]\\(B=100\\) 200 probably enough bootstrap-t CI (500 1000 better). However, \\(B=25\\) may enough estimate SE inner-Bootprocedure. (\\(B=1000\\) needed computing percentiles.)\\(B=100\\) 200 probably enough bootstrap-t CI (500 1000 better). However, \\(B=25\\) may enough estimate SE inner-Bootprocedure. (\\(B=1000\\) needed computing percentiles.)choosing appropriate multiplier:\ncorrect multiplier use, normal multiplier (\\(z\\)) good \\(n\\) samples.\ncorrect multiplier use, t multiplier good samples specified \\(n\\).\ncorrect multiplier use, bootstrap-t multiplier good sample .\nchoosing appropriate multiplier:correct multiplier use, normal multiplier (\\(z\\)) good \\(n\\) samples.correct multiplier use, t multiplier good samples specified \\(n\\).correct multiplier use, bootstrap-t multiplier good sample .resulting intervals typically symmetric (\\(\\hat{t}^*_\\alpha \\ne - \\hat{t}^*_{1-\\alpha}\\)). part improvement \\(z\\) \\(t\\) intervals.resulting intervals typically symmetric (\\(\\hat{t}^*_\\alpha \\ne - \\hat{t}^*_{1-\\alpha}\\)). part improvement \\(z\\) \\(t\\) intervals.Bootstrap-t intervals good location statistics (mean, quantiles, trimmed means) trusted statistics like correlation (necessarily vary based ideas shift).Bootstrap-t intervals good location statistics (mean, quantiles, trimmed means) trusted statistics like correlation (necessarily vary based ideas shift).","code":""},{"path":"boot.html","id":"percentile-confidence-intervals-typeperc","chapter":"6 Bootstrapping","heading":"6.3.3 Percentile Confidence Intervals: type=\"perc\"","text":"interval \\(\\alpha/2\\) \\(1-\\alpha/2\\) quantiles bootstrap distribution statistic \\((1-\\alpha)100\\%\\) bootstrap percentile confidence interval corresponding parameter:\\[\\begin{align}\n[\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}]\n\\end{align}\\]\nneed know percentile interval works… isn’t cool see works???\nwork? isn’t immediately obvious interval capture true parameter, \\(\\theta\\), rate 95%. Consider skewed sampling distribution. observed \\(\\hat{\\theta}\\) comes long tail, obvious short tail side CI get true parameter value correct rate? (Hall (*Bootstrap Edgeworth Expansion}, Springer, 1992, earlier papers) refers Efron’s “backwards” intervals.) , sampling distribution biased, percentiles bootstrap interval won’t capture parameter correct rate.see / percentiles intervals work, first start considering normal sampling distributions function statistic. Let \\(\\phi = g(\\theta), \\hat{\\phi} = g(\\hat{\\theta}), \\hat{\\phi}^* = g(\\hat{\\theta}^*)\\), g monotonic function (assume wlog g increasing). point choose (possible) \\(g(\\cdot)\\) \\[\\begin{equation}\n \\hat{\\phi}^* - \\hat{\\phi} \\sim \\hat{\\phi} - \\phi \\sim N(0, \\sigma^2) \\tag{6.2}.\n \\end{equation}\\]\n, consider logic conventional confidence interval. \\(\\hat{\\phi} - \\phi \\sim N(0, \\sigma^2)\\), interval \\(\\theta\\) derived :\\[\\begin{align}\nP(z_{0.05} \\leq \\frac{\\hat{\\phi} - \\phi}{\\sigma}  ) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} - z_{0.05} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} + z_{0.95} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\theta \\leq g^{-1}(\\hat{\\phi} + z_{0.95} \\sigma)) = 0.95  \\nonumber \\\\\n\\Rightarrow \\mbox{CI } \\theta: \\ \\ \\ (-\\infty, g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})) \\tag{6.3}\n\\end{align}\\]\\(z_{1-\\alpha}\\) \\(100(1-\\alpha)\\) percent point standard normal distribution. Ideally, knew \\(g\\) \\(\\sigma\\), ’d able transformation find \\(g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})\\) (give endpoint confidence interval).Going back ((6.2)) indicates \\(\\hat{\\phi} + \\sigma z_{1-\\alpha} = F^{-1}_{\\hat{\\phi}^*}(1-\\alpha)\\) (\\(\\hat{\\phi} ^* \\sim N(\\hat{\\phi}, \\sigma^2)\\)). , since \\(g\\) monotonically increasing, \\(F^{-1}_{\\hat{\\phi}^*}(1-\\alpha) = g(F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\\)\nSubstituting ((6.3)), gives percentile interval \\(\\theta\\),\\[\\begin{align}\n(-\\infty, F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\n\\end{align}\\](similar argument gives derivation two sided confidence interval. Proof Carpenter Bithell (2000)) order percentile interval appropriate, technical condition normalizing transformation exists. need actually find transformation!transformation respecting property CI transformation respecting , monotone transformation, CI transformed parameter (exactly) transformed CI unstransformed parameter. Let \\(\\phi = m(\\theta)\\).\\[\\begin{align}\n[\\phi_{lo}, \\phi_{}] = [m(\\theta_{lo}), m(\\theta_{})]\n\\end{align}\\]Note idea process creating CI. , create confidence interval using \\(\\phi\\), ’ll get thing created CI using \\(\\theta\\) transformed . straightforward see percentile CI transformation respecting. , monotone transformation statistic parameter, CI transformed appropriately.Let\n\\[\\begin{align}\n\\hat{\\phi} &= 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg)\\\\\nr &=\\frac{e^{2\\phi}+1}{e^{2\\phi}-1}\\\\\n\\end{align}\\]know \\(\\hat{\\phi}\\) approximated normal distribution. , percentile CI \\(\\phi\\) approximate normal theory CI know correct (given \\(\\alpha\\)). CI \\(\\phi\\) can find CI \\(\\rho\\) taking inverse monotonic transformation; rather… can just use r percentile CI start !range preserving property Another advantage percentile interval range preserving. , CI always produces endpoints fall within allowable range parameter.Bias percentile interval , however, perfect. statistic biased estimator parameter, exist transformation distribution centered around correct function parameter. Formally, \n\\[\\begin{align}\n\\hat{\\theta} \\sim N(\\theta + bias, \\hat{SE}^2)\n\\end{align}\\]\ntransformation \\(\\phi = m(\\theta)\\) can fix things . Keep mind standard intervals can fail variety ways, percentile method fixed specific situation sampling distribution non-normal.","code":""},{"path":"boot.html","id":"what-makes-a-ci-procedure-good","chapter":"6 Bootstrapping","heading":"6.3.4 What makes a CI procedure good?","text":"following qualities may may result confidence interval procedure determines choice method researcher.Symmetry (??): interval symmetric, pivotal around value. necessarily good thing. Maybe bad thing force?Resistant: BS-t particularly resistant outliers crazy sampling distributions statistic (can make robust variance stabilizing transformation)Range preserving: CI always contains values fall within allowable range (\\(p, \\rho\\),…)Transformation respecting: monotone transformation, \\(\\phi = m(\\theta)\\), interval \\(\\theta\\) mapped directly \\(m(\\theta)\\). \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) \\((1-\\alpha)100\\)% interval \\(\\theta\\), \\[\\begin{align}\n[\\hat{\\phi}_{(lo)},\\hat{\\phi}_{(hi)}] = [m(\\hat{\\theta}_{(lo)}),m(\\hat{\\theta}_{(hi)})]\n\\end{align}\\]\nexactly interval.Level confidence: central (symmetric) confidence interval, \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) probability \\(\\alpha/2\\) covering \\(\\theta\\) :\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2\\\\\n\\end{align}\\]Note: intervals approximate. judge based accurately cover \\(\\theta\\).\nCI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]\nCI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]\nNote: intervals approximate. judge based accurately cover \\(\\theta\\).CI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]CI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]CI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]CI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]BS-t \\(2^{nd}\\) order accurate large general class functions. However, practice, coverage rate doesn’t kick small/med sample sizes unless appropriate transformations make distribution bell-shaped. (Tibshirani 1988)criteria speak coverage rates parameters. note must taken context. Much also depends :\nchoice statistic ; original data distribution; outlying observations; etc.","code":""},{"path":"boot.html","id":"advantages-and-disadvantages","chapter":"6 Bootstrapping","heading":"6.3.4.1 Advantages and Disadvantages","text":"Normal Approximation\nAdvantages similar familiar parametric approach; useful normally distributed \\(\\hat{\\theta}\\); requires least computation (\\(B=50-200\\))\nDisadvantages fails use entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); works \\(\\hat{\\theta}\\) reasonably normal start \nAdvantages similar familiar parametric approach; useful normally distributed \\(\\hat{\\theta}\\); requires least computation (\\(B=50-200\\))Disadvantages fails use entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); works \\(\\hat{\\theta}\\) reasonably normal start withBootstrap-t Confidence Interval\nAdvantages highly accurate CI many cases; handles skewed \\(F(\\hat{\\theta})\\) better percentile method\nDisadvantages invariant transformations; computationally expensive double bootstrap; coverage probabilities best distribution \\(\\hat{\\theta}\\) nice (e.g., normal)\nAdvantages highly accurate CI many cases; handles skewed \\(F(\\hat{\\theta})\\) better percentile methodDisadvantages invariant transformations; computationally expensive double bootstrap; coverage probabilities best distribution \\(\\hat{\\theta}\\) nice (e.g., normal)Percentile\nAdvantages uses entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) asymmetrical; invariant transformations; range respecting; simple execute\nDisadvantages small samples may result low accuracy (dependence tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) unbiased\nAdvantages uses entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) asymmetrical; invariant transformations; range respecting; simple executeDisadvantages small samples may result low accuracy (dependence tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) unbiasedBCa\nAdvantages\npercentile method; allows bias \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can calculated easily \\(\\hat{F}^*(\\hat{\\theta}^*)\\)\nDisadvantages requires limited parametric assumption; computational intervals\nAdvantages\npercentile method; allows bias \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can calculated easily \\(\\hat{F}^*(\\hat{\\theta}^*)\\)Disadvantages requires limited parametric assumption; computational intervals","code":""},{"path":"boot.html","id":"bootstrap-ci-and-hypothesis-testing","chapter":"6 Bootstrapping","heading":"6.3.4.2 Bootstrap CI and Hypothesis Testing","text":"null value parameter contained CI, reject null hypothesis; similarly, reject null value lie inside CI. Using BootCIs, can apply logic, test hypothesis interest (note: can always create one-sided intervals well!). using CIs leaves p-value information. get p-value CI? Consider alternative definition p-value:p-value: smallest level significance reject \\(H_0\\)., want null value (\\(\\theta_0\\)) one endpoints confidence interval level confidence \\(1-2\\alpha_0\\). \\(\\alpha_0\\) one-sided p-value, \\(2\\alpha_0\\) two-sided p-value.percentile intervals,\n\\[\\begin{align}\np-value = \\alpha_0 = \\frac{\\# \\hat{\\theta}^*(b) < \\theta_0}{B}\n\\end{align}\\]\n(without loss generality, assuming set \\(\\hat{\\theta}^*_{lo} = \\theta_0\\)).\nAnother cool bootstrap CI method won’t time cover. responsible remainder bootstrap material notes.\npercentile method, ’ve assumed exists transformation \\(\\theta\\), \\(\\phi(\\theta)\\), \n\\[\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(0,1)\n\\end{align}\\]\ntransformation assumes neither \\(\\theta\\) \\(\\phi\\) biased, assumes variance constant values parameter. , percentage intervals, assume normalizing transformation creates sampling distribution unbiased variance stabilizing. Consider monotone transformation *normalizes} sampling distribution (longer assume unbiased constant variance).now consider case \\(\\theta\\) biased estimator. :\n\\[\\begin{align}\n\\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{c} \\sim N(-z_0,1)\n\\end{align}\\]\n’ve corrected bias, non-constant variance, need adjustment stabilize variance:\\[\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(-z_0 \\sigma_\\phi,\\sigma_\\phi), \\ \\ \\ \\ \\ \\ \\sigma_\\phi = 1 + \\phi\n\\end{align}\\]\n, must exist monotone transformation \\(\\phi\\) \\(\\phi(\\hat{\\theta}) \\sim N\\) \n\\[\\begin{align}\nE(\\phi(\\hat{\\theta})) = \\phi(\\theta) - z_0 [1 + \\phi(\\theta)] & SE(\\phi(\\hat{\\theta})) = 1 + \\phi(\\theta)\n\\end{align}\\]\n(Note: expected value SE ’ve assumed \\(c=1\\). \\(c\\ne1\\), can always choose different transformation, \\(\\phi'\\) \\(c=1\\).) \n\\[\\begin{align}\nP(z_{\\alpha/2} \\leq \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{1 + \\phi(\\theta)} + z_0 \\leq z_{1-\\alpha/2}) = 1 - \\alpha\n\\end{align}\\]\n$(1-)$100% CI \\(\\phi(\\theta)\\) \n\\[\\begin{align}\n\\bigg[ \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{1 + (z_{1-\\alpha/2} - z_0)}, \\frac{\\phi(\\hat{\\theta}) - (z_{\\alpha/2} - z_0)}{1 + (z_{\\alpha/2} - z_0)} \\bigg]\n\\end{align}\\]\nLet’s consider interesting probability question:\n\\[\\begin{align}\nP\\bigg( \\phi(\\hat{\\theta}^*) &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg) = ?\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta}) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ -(1+\\phi(\\hat{\\theta})) (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} + z_0 &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n= P\\bigg( Z &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg) = \\gamma_1\\\\\n\\mbox{} \\gamma_1 &= \\Phi \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n &= \\verb;pnorm; \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\n\\end{align}\\]’ve shown \\(\\gamma_1\\) quantile \\(\\phi(\\hat{\\theta}^*)\\) sampling distribution good estimate lower bound confidence interval \\(\\phi(\\theta)\\). Using argument upper bound, find $(1-)$100% confidence interval \\(\\phi(\\theta)\\) :\\[\\begin{align}\n&[\\phi(\\hat{\\theta}^*)_{\\gamma_1}, \\phi(\\hat{\\theta}^*)_{\\gamma_2}]\\\\\n& \\\\\n\\mbox{} \\gamma_1 &= \\Phi\\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n \\gamma_2 &= \\Phi \\bigg(\\frac{ (z_{1-\\alpha/2} + z_0) }{(1 - (z_{1-\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\end{align}\\]Using transformation respecting property percentile intervals, know $(1-)$100% confidence interval \\(\\theta\\) :\\[\\begin{align}\n&[\\hat{\\theta}^*_{\\gamma_1}, \\hat{\\theta}^*_{\\gamma_2}]\n\\end{align}\\]estimate \\(\\) \\(z_0\\)?bias:\n\\(z_0\\) measure bias. Recall:\\[\\begin{align}\nbias &= E(\\hat{\\theta}) - \\theta\\\\\n\\hat{bias} &= \\hat{\\theta}^* - \\hat{\\theta}\\\\\n\\end{align}\\]remember \\(z_0\\) represents bias \\(\\phi(\\hat{\\theta})\\), \\(\\hat{\\theta}\\) (don’t know \\(\\phi\\)!). , use \\(\\theta\\) see proportion \\(\\theta\\) values low, can map back \\(\\phi\\) space using normal distribution:\\[\\begin{align}\n\\hat{z}_0 &= \\Phi^{-1} \\bigg( \\frac{ \\# \\hat{\\theta}^*(b) < \\hat{\\theta}}{B} \\bigg)\n\\end{align}\\]\n, \\(\\hat{\\theta}^*\\) underestimates \\(\\hat{\\theta}\\), \\(\\hat{\\theta}\\) likely underestimates \\(\\theta\\); \\(z_0 > 0\\). think \\(z_0\\) normal quantile associated proportion Bootreplicates less \\(\\hat{\\theta}\\).skew:\n\\(\\) measure skew.\n\\[\\begin{align}\nbias&= E(\\hat{\\theta} - \\theta)\\\\\nvar &= E(\\hat{\\theta} - \\theta)^2 = \\sigma^2\\\\\nskew &= E(\\hat{\\theta} - \\theta)^3 / \\sigma^3\\\\\n\\end{align}\\]\ncan think skew rate chance standard error normalized scale. skew, estimate \\(=0\\). estimate \\(\\) comes procedure known jackknife.\\[\\begin{align}\n\\hat{} = \\frac{\\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^3}{6 [ \\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^2 ] ^{3/2}}\n\\end{align}\\]","code":""},{"path":"boot.html","id":"r-example-heroin","chapter":"6 Bootstrapping","heading":"6.4 R example: Heroin","text":"Hesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times (days) spent clinic methadone maintenance treatment people addicted heroin. data heroin.txt include amount time subjects stayed facility treatment terminated (column 4). 37% subjects, study ended still clinic (status=0). Thus, survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using median well 25% trimmed mean. treat group 238 patients representative population. (Chance Rossman 2018a, Investigation 4.5.3)","code":""},{"path":"boot.html","id":"why-bootstrap","chapter":"6 Bootstrapping","heading":"Why bootstrap?","text":"Motivation: estimate variability statistic (dependent \\(H_0\\) true).","code":""},{"path":"boot.html","id":"reading-in-the-data","chapter":"6 Bootstrapping","heading":"Reading in the data","text":"","code":"\nheroin <- read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nheroin %>%\n  select(-prison)## # A tibble: 238 × 5\n##       id clinic status times  dose\n##    <dbl>  <dbl>  <dbl> <dbl> <dbl>\n##  1     1      1      1   428    50\n##  2     2      1      1   275    55\n##  3     3      1      1   262    55\n##  4     4      1      1   183    30\n##  5     5      1      1   259    65\n##  6     6      1      1   714    55\n##  7     7      1      1   438    65\n##  8     8      1      0   796    60\n##  9     9      1      1   892    50\n## 10    10      1      1   393    65\n## # … with 228 more rows"},{"path":"boot.html","id":"observed-test-statistics","chapter":"6 Bootstrapping","heading":"Observed Test Statistic(s)","text":"","code":"\nheroin %>%\n  summarize(obs_med = median(times), \n            obs_tr_mean = mean(times, trim = 0.25))## # A tibble: 1 × 2\n##   obs_med obs_tr_mean\n##     <dbl>       <dbl>\n## 1    368.        378."},{"path":"boot.html","id":"bootstrapped-data","chapter":"6 Bootstrapping","heading":"Bootstrapped data!","text":"","code":"\nset.seed(4747)\n\nheroin %>% \n  sample_frac(size=1, replace=TRUE) %>%\n  summarize(boot_med = median(times), \n            boot_tr_mean = mean(times, trim = 0.25))## # A tibble: 1 × 2\n##   boot_med boot_tr_mean\n##      <dbl>        <dbl>\n## 1      368         372."},{"path":"boot.html","id":"need-to-bootstrap-a-lot-of-times","chapter":"6 Bootstrapping","heading":"Need to bootstrap a lot of times…","text":"code showing bootstrap using loops (nested create t multipliers needed BS-t intervals). (package boot bootstrap , need write functions use .)set variablesboot stat functionresample functionbootstrapping","code":"\nn_rep1 <- 100\nn_rep2 <- 20\nset.seed(4747)\nboot_stat_func <- function(df){ \n    df %>% \n    mutate(obs_med = median(times),\n           obs_tr_mean = mean(times, trim = 0.25)) %>%\n    sample_frac(size=1, replace=TRUE) %>%\n    summarize(boot_med = median(times), \n              boot_tr_mean = mean(times, trim = 0.25),\n              obs_med = mean(obs_med),\n              obs_tr_mean = mean(obs_tr_mean))}\nboot_1_func <- function(df){\n  df %>% \n    sample_frac(size=1, replace=TRUE)\n}\nmap_df(1:n_rep1, ~boot_stat_func(df = heroin))## # A tibble: 100 × 4\n##    boot_med boot_tr_mean obs_med obs_tr_mean\n##       <dbl>        <dbl>   <dbl>       <dbl>\n##  1     368          372.    368.        378.\n##  2     358          363.    368.        378.\n##  3     431          421.    368.        378.\n##  4     332.         350.    368.        378.\n##  5     310.         331.    368.        378.\n##  6     376          382.    368.        378.\n##  7     366          365.    368.        378.\n##  8     378.         382.    368.        378.\n##  9     394          386.    368.        378.\n## 10     392.         402.    368.        378.\n## # … with 90 more rows"},{"path":"boot.html","id":"what-do-the-data-distributions-look-like","chapter":"6 Bootstrapping","heading":"What do the data distributions look like?","text":"","code":""},{"path":"boot.html","id":"what-do-the-sampling-distributions-look-like","chapter":"6 Bootstrapping","heading":"What do the sampling distributions look like?","text":"distributions median trimmed mean symmetric bell-shaped. However, trimmed mean normal distribution (evidenced points qq plot falling line y=x).","code":""},{"path":"boot.html","id":"what-does-the-boot-output-look-like","chapter":"6 Bootstrapping","heading":"What does the boot output look like?","text":"","code":"\nboot_stats <- map_df(1:n_rep1, ~boot_stat_func(df = heroin))\n\nboot_stats## # A tibble: 100 × 4\n##    boot_med boot_tr_mean obs_med obs_tr_mean\n##       <dbl>        <dbl>   <dbl>       <dbl>\n##  1     362          373.    368.        378.\n##  2     342.         345.    368.        378.\n##  3     388.         393.    368.        378.\n##  4     452          428.    368.        378.\n##  5     400.         400.    368.        378.\n##  6     348          363.    368.        378.\n##  7     399          405.    368.        378.\n##  8     394          398.    368.        378.\n##  9     358          359.    368.        378.\n## 10     299          332.    368.        378.\n## # … with 90 more rows"},{"path":"boot.html","id":"normal-ci-with-boot-se","chapter":"6 Bootstrapping","heading":"95% normal CI with Boot SE","text":"","code":"\nboot_stats %>%\n  summarize(low_med = mean(obs_med) + qnorm(0.025) * sd(boot_med), \n         up_med = mean(obs_med) + qnorm(0.975) * sd(boot_med),\n         low_tr_mean = mean(obs_tr_mean) + qnorm(0.025) * sd(boot_tr_mean), \n         up_tr_mean = mean(obs_tr_mean) + qnorm(0.975) * sd(boot_tr_mean))## # A tibble: 1 × 4\n##   low_med up_med low_tr_mean up_tr_mean\n##     <dbl>  <dbl>       <dbl>      <dbl>\n## 1    301.   434.        332.       425."},{"path":"boot.html","id":"percentile-ci","chapter":"6 Bootstrapping","heading":"95% Percentile CI","text":"","code":"\nboot_stats %>%\n  summarize(perc_CI_med = quantile(boot_med, c(0.025, 0.975)), \n            perc_CI_tr_mean = quantile(boot_tr_mean, c(0.025, 0.975)), \n            q = c(0.025, 0.975))## # A tibble: 2 × 3\n##   perc_CI_med perc_CI_tr_mean     q\n##         <dbl>           <dbl> <dbl>\n## 1        319.            329. 0.025\n## 2        448.            423. 0.975"},{"path":"boot.html","id":"bootstrap-t-ci","chapter":"6 Bootstrapping","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample). necessary bootstrap twice.re-resample function]double bootstrap!","code":"\nboot_2_func <- function(df, reps){\n  resample2 <- 1:reps\n  df %>%\n    summarize(boot_med = median(times), boot_tr_mean = mean(times, trim = 0.25)) %>%\n    cbind(resample2, map_df(resample2, ~df %>% \n             sample_frac(size=1, replace=TRUE) %>%\n             summarize(boot_2_med = median(times), \n                       boot_2_tr_mean = mean(times, trim = 0.25)))) %>%\n    select(resample2, everything())\n}\nboot_2_stats <- data.frame(resample1 = 1:n_rep1) %>%\n  mutate(first_boot = map(1:n_rep1, ~boot_1_func(df = heroin))) %>%\n  mutate(second_boot = map(first_boot, boot_2_func, reps = n_rep2)) "},{"path":"boot.html","id":"summarizing-the-double-bootstrap","chapter":"6 Bootstrapping","heading":"6.4.0.1 Summarizing the double bootstrap","text":"resultssummary resample 1summary resamples","code":"\nboot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) ## # A tibble: 476,000 × 12\n##    resample1    id clinic status times prison  dose resample2 boot_med\n##        <int> <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>     <int>    <dbl>\n##  1         1   137      2      0   563      0    70         1      372\n##  2         1    91      1      0   840      0    80         1      372\n##  3         1   250      1      1   117      0    40         1      372\n##  4         1   168      2      0   788      0    70         1      372\n##  5         1    67      1      1   386      0    60         1      372\n##  6         1     3      1      1   262      0    55         1      372\n##  7         1   104      2      0   713      0    50         1      372\n##  8         1   251      1      1   175      1    60         1      372\n##  9         1    68      1      0   439      0    80         1      372\n## 10         1   118      2      0   532      0    70         1      372\n## # … with 475,990 more rows, and 3 more variables: boot_tr_mean <dbl>,\n## #   boot_2_med <dbl>, boot_2_tr_mean <dbl>\nboot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) %>%\n  select(resample1, resample2, everything() ) %>%\n  filter(resample1 == 1) %>%\n  select(boot_med, boot_tr_mean, boot_2_med, boot_2_tr_mean) %>%\n  skim_without_charts() %>% as_tibble() %>% \n  select(skim_variable, numeric.mean, numeric.sd, numeric.p50)## # A tibble: 4 × 4\n##   skim_variable  numeric.mean numeric.sd numeric.p50\n##   <chr>                 <dbl>      <dbl>       <dbl>\n## 1 boot_med               372         0          372 \n## 2 boot_tr_mean           378.        0          378.\n## 3 boot_2_med             370.       46.6        365.\n## 4 boot_2_tr_mean         372.       24.7        370.\nboot_t_stats <- boot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) %>%\n  group_by(resample1) %>%\n  summarize(boot_se_med = sd(boot_2_med),\n            boot_se_tr_mean = sd(boot_2_tr_mean),\n            boot_med = mean(boot_med),  # doesn't do anything, just copies over\n            boot_tr_mean = mean(boot_tr_mean))  %>% # the variables into the output\n  mutate(boot_t_med = (boot_med - mean(boot_med)) / boot_se_med,\n            boot_t_tr_mean = (boot_tr_mean - mean(boot_tr_mean)) / boot_se_tr_mean)\n\n  \nboot_t_stats## # A tibble: 100 × 7\n##    resample1 boot_se_med boot_se_tr_mean boot_med boot_tr_mean boot_t_med\n##        <int>       <dbl>           <dbl>    <dbl>        <dbl>      <dbl>\n##  1         1        46.6            24.7     372          378.     0.0222\n##  2         2        24.8            25.3     344          370.    -1.09  \n##  3         3        36.9            24.2     372.         380.     0.0145\n##  4         4        20.8            19.7     354          359.    -0.817 \n##  5         5        30.1            22.0     308          331.    -2.09  \n##  6         6        25.5            24.5     318          336.    -2.08  \n##  7         7        24.9            20.3     367          378.    -0.159 \n##  8         8        46.3            25.0     402          393.     0.670 \n##  9         9        32.3            20.8     388.         380.     0.512 \n## 10        10        18.1            15.1     381          384.     0.555 \n## # … with 90 more rows, and 1 more variable: boot_t_tr_mean <dbl>"},{"path":"boot.html","id":"bootstrap-t-ci-1","chapter":"6 Bootstrapping","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample).t-values]multiplierspull numbersBS-t CI","code":"\nboot_t_stats %>%\n  select(boot_t_med, boot_t_tr_mean)## # A tibble: 100 × 2\n##    boot_t_med boot_t_tr_mean\n##         <dbl>          <dbl>\n##  1     0.0222          0.140\n##  2    -1.09           -0.160\n##  3     0.0145          0.257\n##  4    -0.817          -0.767\n##  5    -2.09           -1.98 \n##  6    -2.08           -1.56 \n##  7    -0.159           0.202\n##  8     0.670           0.764\n##  9     0.512           0.280\n## 10     0.555           0.625\n## # … with 90 more rows\nboot_q <- boot_t_stats %>%\n  select(boot_t_med, boot_t_tr_mean) %>%\n  summarize(q_t_med = quantile(boot_t_med, c(0.025, 0.975)), \n            q_t_tr_mean = quantile(boot_t_tr_mean, c(0.025, 0.975)),\n            q = c(0.025, 0.975))\n\nboot_q## # A tibble: 2 × 3\n##   q_t_med q_t_tr_mean     q\n##     <dbl>       <dbl> <dbl>\n## 1   -2.08       -2.01 0.025\n## 2    2.25        2.03 0.975\nboot_q_med <- boot_q %>% select(q_t_med) %>% pull()\nboot_q_med##  2.5% 97.5% \n## -2.08  2.25\nboot_q_tr_mean <- boot_q %>% select(q_t_tr_mean) %>% pull()\nboot_q_tr_mean##  2.5% 97.5% \n## -2.01  2.03\nboot_t_stats %>%\n  summarize(boot_t_CI_med = mean(boot_med) + \n                                  boot_q_med*sd(boot_med),\n            boot_t_CI_tr_mean = mean(boot_tr_mean) + \n                                  boot_q_tr_mean * sd(boot_tr_mean),\n            q = c(0.025, 0.975))## # A tibble: 2 × 3\n##   boot_t_CI_med boot_t_CI_tr_mean     q\n##           <dbl>             <dbl> <dbl>\n## 1          306.              330. 0.025\n## 2          441.              418. 0.975"},{"path":"boot.html","id":"comparison-of-intervals","chapter":"6 Bootstrapping","heading":"6.4.0.2 Comparison of intervals","text":"first three columns correspond CIs true median survival times. second three columns correspond CIs true trimmed mean survival times.","code":""},{"path":"ethics.html","id":"ethics","chapter":"7 Ethics","heading":"7 Ethics","text":"","code":""},{"path":"ethics.html","id":"doing-data-science","chapter":"7 Ethics","heading":"7.1 Doing Data Science","text":"Questions ask every single data analysis perform (taken Data Science Social Good University Chicago https://dssg.uchicago.edu/):biases may exist data ’ve given? can find ?choices tuning parameters affect different populations represented data?know aren’t getting right answer wrong question?justify ’d built someone whose welfare made worse implementation algorithm?See section bias modeling (4.2.1) times inherent biases structure data create unequal model results.","code":""},{"path":"ethics.html","id":"graphics","chapter":"7 Ethics","heading":"7.2 Graphics","text":"many ways lie graphics. may already familiar book Lie Statistics. Many ideas replicated : http://www.rci.rutgers.edu/~roos/Courses/grstat502/wainer.pdf [just plots provided : http://www.stat.berkeley.edu/~nolan/stat133/Fall05/lectures/DirtyDozen.pdf]recent relevant example, consider following image. think wrong? (Hint: examine y-axis carefully)\nFigure 1.3: Reproduction data graphic reporting number gun deaths Florida time. original image published Reuters. (Baumer, Kaplan, Horton 2021)\nanother plot gotten lot press following. wrong plot? (Hint: , think y-axis)\nFigure 1.4: tweet National Review December 14, 2015 showing change global temperature time. (Baumer, Kaplan, Horton 2021)\nGeorgia Department Health came grouped barplot showing number COVID-19 cases day 5 populous counties GA. bars arranged kind decreasing order, first glance, typical reader think time increasing along x-axis.\nFigure 1.6: May 10, 2020, Georgia Department Health, COVID-19 cases 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\n\nFigure 7.1: May 17, 2020, Georgia Department Health, COVID-19 cases 5 counties across time. https://dph.georgia.gov/covid-19-daily-status-report\nweeks later, Georgia Department Health came following two plots , despite cases skyrocketing, display images visual doesn’t really change.\nFigure 7.2: July 2, 2020 (left) July 17, 2020 (right), Georgia Department Health, COVID-19 cases per 100K https://dph.georgia.gov/covid-19-daily-status-report\nSeems odd linear model fit data.\nparticular, trend absolutely non-linear!\nNote screen shot slider indicate number days modeled displayed.\ninformation 14, 28, 42 days related policy decisions based whether cases trending positive negative.\nHowever, even need know positive negative trends, even short time frame, linear model fit data seems inappropriate.\nFigure 7.3: Kent County COVID dashboard, screen shot 8/25/2021. https://www.accesskent.com/Health/covid-19-data.htm\n","code":""},{"path":"ethics.html","id":"p-hacking","chapter":"7 Ethics","heading":"7.3 p-hacking","text":"Great applet 538 get significance just trying enough things: https://projects.fivethirtyeight.com/p-hacking/J. Ioannidis, published research findings false. PLoS Medicine, 2. e124 2005. Ioannidis focuses multiple testing specific understanding effect testing three different contexts:looking many possible significant findings possible (publish perish)bias exists workWhen researchers study effect","code":""},{"path":"ethics.html","id":"multiple-studies","chapter":"7 Ethics","heading":"7.3.1 Multiple Studies","text":"\\(\\alpha\\)\nstudy null, probability seeing null \\((1-\\alpha)\\)\n3 us test thing, probability see null \\((1-\\alpha)^3\\)\nprobability least one use see significance goes \\(\\alpha\\) \\(1 - (1-\\alpha)^3\\)\n\\(n \\uparrow\\) someone definitely see significance (bad!)\nstudy null, probability seeing null \\((1-\\alpha)\\)3 us test thing, probability see null \\((1-\\alpha)^3\\)probability least one use see significance goes \\(\\alpha\\) \\(1 - (1-\\alpha)^3\\)\\(n \\uparrow\\) someone definitely see significance (bad!)\\(\\beta\\)\nstudy significant, probability seeing null \\(\\beta\\)\n3 us test thing, probability ’ll see null \\(\\beta^3\\)\nprobability least one us see significance goes \\((1-\\beta)\\) \\((1-\\beta^3)\\)\n\\(n \\uparrow\\), someone definitely see significance (good!)\nstudy significant, probability seeing null \\(\\beta\\)3 us test thing, probability ’ll see null \\(\\beta^3\\)probability least one us see significance goes \\((1-\\beta)\\) \\((1-\\beta^3)\\)\\(n \\uparrow\\), someone definitely see significance (good!)","code":""},{"path":"ethics.html","id":"p-values","chapter":"7 Ethics","heading":"7.3.2 p-values","text":"1929 RA Fisher said following (thus 0.05 born):\ncommon practice judge result significant, magnitude produced chance frequently twenty trials. arbitrary, convenient, level significance practical investigator, mean allows deceived every twenty experiments. test significance tells ignore, namely experiments significant results obtained.\nclaim phenomenon experimentally demonstrable knows design experiment rarely fail give significant result. Consequently, isolated significant results know reproduce left suspense pending investigation.Note Fisher telling us studies p-values 0.05 worth pursuing. saying studies p-values less 0.05 establish sort truth.2014 George Cobb (Amherst College) posed following questions:Q: many colleges grad schools teach p = .05?: ’s still scientific community journal editors use.Q: many people still use p = 0.05?: ’s taught college grad school.2015, Basic Applied Social Psychology banned NHSTP (null hypothesis significance testing procedures) scientific articles.implications authors?\nhttp://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991Question 3. inferential statistical procedures required?Answer Question 3. , state art remains uncertain. However, BASP require strong descriptive statistics, including effect sizes. also encourage presentation frequency distributional data feasible. Finally, encourage use larger sample sizes typical much psychology research, sample size increases, descriptive statistics become increasingly stable sampling error less problem. However, stop short requiring particular sample sizes, possible imagine circumstances typical sample sizes might justifiable.Many people think CIs far superior p-values. can assess significance, can also assess effect size. video makes clear comparison p-value confidence interval:https://www.youtube.com/watch?v=5OL1RqHrZQ8In 2016, American Statistical Association came statement p-values http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108. 6 main tenants statement :P-values can indicate incompatible data specified statistical model.P-values can indicate incompatible data specified statistical model.P-values measure probability studied hypothesis true, probability data produced random chance alone.P-values measure probability studied hypothesis true, probability data produced random chance alone.Scientific conclusions business policy decisions based whether p- value passes specific threshold.Scientific conclusions business policy decisions based whether p- value passes specific threshold.Proper inference requires full reporting transparency.Proper inference requires full reporting transparency.p-value, statistical significance, measure size effect importance result.p-value, statistical significance, measure size effect importance result., p-value provide good measure evidence regarding model hypothesis., p-value provide good measure evidence regarding model hypothesis.“Statisticians issue warning misuse P values” (Nature, March 7, 2016) http://www.nature.com/news/statisticians-issue-warning--misuse--p-values-1.19503“Statisticians issue warning misuse P values” (Nature, March 7, 2016) http://www.nature.com/news/statisticians-issue-warning--misuse--p-values-1.19503Many people think CIs far superior p-values. can assess significance, can also assess effect size. video makes clear comparison p-value confidence interval, also addresses p-values can misinterpreted. Dance P-values (think power watch ): https://www.youtube.com/watch?v=5OL1RqHrZQ8Many people think CIs far superior p-values. can assess significance, can also assess effect size. video makes clear comparison p-value confidence interval, also addresses p-values can misinterpreted. Dance P-values (think power watch ): https://www.youtube.com/watch?v=5OL1RqHrZQ8Last, can discover level significance activity: https://www.openintro.org/stat/why05.php?stat_book=osLast, can discover level significance activity: https://www.openintro.org/stat/why05.php?stat_book=os","code":""},{"path":"ethics.html","id":"human-subjects-research","chapter":"7 Ethics","heading":"7.4 Human Subjects Research","text":"many ways learn federal regulations studying human subjects. study goes academic institution must approved institution’s Institutional Review Board (IRB); Claremont Colleges IRB. may familiar HIPPA (Health Insurance Portability Accountability Act 1996) United States legislation provides data privacy security provisions safeguarding medical information. HIPPA also legislation keeps academic records private.Training IRB policies can found Citi Program : https://.citiprogram.org/en/series/human-subjects-research-hsr/Consider following article: Cami Gearhart, “IRB Review Use \nSocial Media Research,” Monitor, 2012. https://www.quorumreview.com/wp-content/uploads/2012/12/IRB-Review---Use--Social-Media--Research_Gearhart_Quorum-Review_Monitor_2012_12_01.pdfThe use interactive social media collect information study recruitment raises additional issues HIPAA Privacy Rule. Privacy Rule prohibits collection individual?s personal health information (PHI) covered entity without prior written authorization individual.18 PHI includes individual?s contact information, including name, age, e-mail address, mailing address, Privacy Rule prohibits collection contact information via website without prior authorization.rule creates conundrum using social media, may impractical obtain written authorization prior collecting contact information recruitment process. get around restriction, researcher generally must obtain partial waiver HIPAA authorization requirement. (waiver situation considered “partial” needed recruitment phase clinical study.) researcher can apply either IRB privacy board waiver; researcher asked explain impractical obtain written authorizations, plan collecting information, planned safeguards data.","code":""},{"path":"ethics.html","id":"okcupid","chapter":"7 Ethics","heading":"OKCupid","text":"Consider study done dataset nearly 70,000 users online dating site OkCupid (including usernames, age, gender, location, relationship interests, personality traits, many profile variables). authors violate technical policies breaking passwords. However, work indicates violation privacy ethics indicated HIPPA use IRBs. [Kirkegaard Bjerrekaer, “OKCupid dataset: large public dataset dating site users,” Open Differential Pyschology, 2016.] https://openpsych.net/paper/46\nFigure 5.3: worth discussing ethics data collected, also seems like maybe study p-hacking.\n","code":""},{"path":"ethics.html","id":"authorship","chapter":"7 Ethics","heading":"7.5 Authorship","text":"International Committee Medical Journal Editors, http://www.icmje.org/recommendations/browse/roles--responsibilities/defining--role--authors--contributors.html. Many organizations suggested guidelines authorship, guidelines generally follow criteria.","code":""},{"path":"ethics.html","id":"authors","chapter":"7 Ethics","heading":"Authors","text":"Substantial contributions conception design work; acquisition, analysis, interpretation data work; ANDDrafting work revising critically important intellectual content; ANDFinal approval version published; ANDAgreement accountable aspects work ensuring questions related accuracy integrity part work appropriately investigated resolved.","code":""},{"path":"ethics.html","id":"non-authors","chapter":"7 Ethics","heading":"Non-authors","text":"Contributors meet fewer 4 criteria authorship listed authors, acknowledged. Examples activities alone (without contributions) qualify contributor authorship acquisition funding; general supervision research group general administrative support; writing assistance, technical editing, language editing, proofreading.\nFigure 1.7: paper retracted authors agree order authorship.\n","code":""},{"path":"ethics.html","id":"algorithms","chapter":"7 Ethics","heading":"7.6 Algorithms","text":"spend days talking bias algorithms. take away examples data used train model can huge effects creation model. fantastic book issue Weapons Math Destruction Cathy O’Neil. podcast book : https://99percentinvisible.org/episode/-age---algorithm/.Also keep mind various laws designed protect privacy civil liberties. Just didn’t try build algorithm biased protected group mean hook. two ways laws enforced (equally important):disparate treatment \\(\\rightarrow\\) means differential treatment intentionaldisparate treatment \\(\\rightarrow\\) means differential treatment intentionaldisparate impact \\(\\rightarrow\\) means differential treatment unintentional implicit (examples include advancing mortgage credit, employment selection, predictive policing)disparate impact \\(\\rightarrow\\) means differential treatment unintentional implicit (examples include advancing mortgage credit, employment selection, predictive policing)Alexandria Ocasio-Cortez, Jan 22, 2019 MLK event Ta-Nehisi Coates http://aaronsadventures.blogspot.com/2019/01/discussion--unfairness--machine.htmlS. Barocas . Selbst, “Big Data’s Disparate Impact,” California Law Review, 671, 2016.","code":""},{"path":"ethics.html","id":"anti-discrimination-laws","chapter":"7 Ethics","heading":"Anti-discrimination Laws","text":"Civil Rights Acts 1964 1991Americans Disabilities ActGenetic Information Nondiscrimination ActEqual Credit Opportunity ActFair Housing Act","code":""},{"path":"ethics.html","id":"sentencing","chapter":"7 Ethics","heading":"Sentencing","text":"“Machine Bias” Pro Publica Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner, May 23, 2016 https://www.propublica.org/article/machine-bias-risk-assessments--criminal-sentencing/\nFigure 1.8: Dylan Fugett three subsequent arrests drug possession. Bernard Parker subsequent offenses.\n\nFigure 1.9: False positive false negative rates broken race.\n","code":""},{"path":"ethics.html","id":"algorithmic-justice-league","chapter":"7 Ethics","heading":"Algorithmic Justice League","text":"https://www.ajlunited.org/Algorithmic Justice League collective aims :Highlight algorithmic bias media, art, scienceHighlight algorithmic bias media, art, scienceProvide space people voice concerns experiences coded biasProvide space people voice concerns experiences coded biasDevelop practices accountability design, development, deployment coded systemsDevelop practices accountability design, development, deployment coded systemsJoy Buolamwini – AI, Ain’t Woman? https://www.youtube.com/embed/QxuyfWoVV98Joy Buolamwini – AI, Ain’t Woman? https://www.youtube.com/embed/QxuyfWoVV98","code":""},{"path":"ethics.html","id":"sentiment-analysis","chapter":"7 Ethics","heading":"Sentiment Analysis","text":"https://blog.dominodatalab.com/video--machine-learning-amplifies-societal-privilege/talk, Mike Williams, Research Engineer Fast Forward Labs, looks supervised machine learning potential amplify power privilege society. Using sentiment analysis, demonstrates text analytics often favors voices men. Mike discusses bias can inadvertently introduced model, recognize mitigate harms.isn’t option objective fair. option doesn’t exist… whole premise based bias training set. biases, patterns training set. system going work… Supervised machine learning, goes really well, really good job, reproduces biases training data.Williams’ full (biased!) sentiment analysis GitHub: https://github.com/williamsmj/sentiment/blob/master/sentiment.ipynb","code":""},{"path":"ethics.html","id":"r-packages","chapter":"7 Ethics","heading":"R packages","text":"wru\n? Bayesian Prediction Racial Category Using Surname Geolocation https://cran.r-project.org/web/packages/wru/index.htmlwru\n? Bayesian Prediction Racial Category Using Surname Geolocation https://cran.r-project.org/web/packages/wru/index.htmltm\nframework text mining applications within R. https://cran.r-project.org/web/packages/tm/vignettes/tm.pdftm\nframework text mining applications within R. https://cran.r-project.org/web/packages/tm/vignettes/tm.pdfRSentiment\nAnalyses sentiment sentence English assigns score . can classify sentences following categories sentiments:- Positive, Negative, Positive, negative, Neutral. vector sentences, counts number sentences category sentiment.calculating score, negation various degrees adjectives taken consideration. deals English sentences. https://cran.r-project.org/web/packages/RSentiment/index.htmlRSentiment\nAnalyses sentiment sentence English assigns score . can classify sentences following categories sentiments:- Positive, Negative, Positive, negative, Neutral. vector sentences, counts number sentences category sentiment.calculating score, negation various degrees adjectives taken consideration. deals English sentences. https://cran.r-project.org/web/packages/RSentiment/index.htmlSentimentAnalysis\nPerforms sentiment analysis textual contents R. implementation utilizes various existing dictionaries, Harvard IV, finance-specific dictionaries. Furthermore, can also create customized dictionaries. latter uses LASSO regularization statistical approach select relevant terms based exogenous response variable. https://cran.r-project.org/web/packages/SentimentAnalysis/index.htmlSentimentAnalysis\nPerforms sentiment analysis textual contents R. implementation utilizes various existing dictionaries, Harvard IV, finance-specific dictionaries. Furthermore, can also create customized dictionaries. latter uses LASSO regularization statistical approach select relevant terms based exogenous response variable. https://cran.r-project.org/web/packages/SentimentAnalysis/index.html","code":""},{"path":"ethics.html","id":"guiding-ethical-principles","chapter":"7 Ethics","heading":"7.7 Guiding Ethical Principles","text":"Baumer, Kaplan, Horton (2021):\n1. work well standards standards profession (avoid using skills way effectively lying - leading others believe one thing fact something different true).Recognize parties special professional obligation (worry conflict interest!).Recognize parties special professional obligation (worry conflict interest!).Report results methods honestly respect responsibility identify report flaws shortcomings work (don’t -confident, use reproducible methods).Report results methods honestly respect responsibility identify report flaws shortcomings work (don’t -confident, use reproducible methods).","code":""},{"path":"ethics.html","id":"asa-ethical-guidelines-for-statistical-practice","chapter":"7 Ethics","heading":"ASA Ethical Guidelines for Statistical Practice","text":"http://www.amstat.org/ASA/-Career/Ethical-Guidelines--Statistical-Practice.aspx","code":""},{"path":"ethics.html","id":"professional-integrity-and-accountability","chapter":"7 Ethics","heading":"Professional Integrity and Accountability","text":"ethical statistician uses methodology data relevant appropriate, without favoritism prejudice, manner intended produce valid, interpretable, reproducible results. ethical statistician knowingly accept work sufficiently qualified, honest client limitation expertise, consults statisticians necessary doubt.","code":""},{"path":"ethics.html","id":"integrity-of-data-and-methods","chapter":"7 Ethics","heading":"Integrity of Data and Methods","text":"ethical statistician candid known suspected limitations, defects, biases data may impact integrity reliability statistical analysis. Objective valid interpretation results requires underlying analysis recognizes acknowledges degree reliability integrity data.","code":""},{"path":"ethics.html","id":"responsibilities-to-sciencepublicfunderclient","chapter":"7 Ethics","heading":"Responsibilities to Science/Public/Funder/Client","text":"ethical statistician supports valid inferences, transparency, good science general, keeping interests public, funder, client, customer mind (well professional colleagues, patients, public, scientific community).","code":""},{"path":"ethics.html","id":"responsibilities-to-research-subjects","chapter":"7 Ethics","heading":"Responsibilities to Research Subjects","text":"ethical statistician protects respects rights interests human animal subjects stages involvement project. includes respondents census surveys, whose data contained administrative records, subjects physically psychologically invasive research.","code":""},{"path":"ethics.html","id":"responsibilities-to-research-team-colleagues","chapter":"7 Ethics","heading":"Responsibilities to Research Team Colleagues","text":"Science statistical practice often conducted teams made professionals different professional standards. statistician must know work ethically environment.","code":""},{"path":"ethics.html","id":"responsibilities-to-other-statisticians-or-statistics-practitioners","chapter":"7 Ethics","heading":"Responsibilities to Other Statisticians or Statistics Practitioners","text":"practice statistics requires consideration entire range possible explanations observed phenomena, distinct observers drawing unique sets experiences can arrive different potentially diverging judgments plausibility different explanations. Even adversarial settings, discourse tends successful statisticians treat one another mutual respect focus scientific principles, methodology substance data interpretations.","code":""},{"path":"ethics.html","id":"responsibilities-regarding-allegations-of-misconduct","chapter":"7 Ethics","heading":"Responsibilities Regarding Allegations of Misconduct","text":"ethical statistician understands difference questionable scientific practices practices constitute misconduct, avoids , knows handled.","code":""},{"path":"ethics.html","id":"responsibilities-of-employers-including-organizations-individuals-attorneys-or-other-clients-employing-statistical-practitioners","chapter":"7 Ethics","heading":"Responsibilities of Employers, Including Organizations, Individuals, Attorneys, or Other Clients Employing Statistical Practitioners","text":"employing person analyze data implicitly relying profession’s reputation objectivity. However, creates obligation part employer understand respect statisticians’ obligation objectivity.","code":""},{"path":"class.html","id":"class","chapter":"8 Classification","heading":"8 Classification","text":"Baumer (2015) provides concise explanation statistics data science work enhance ideas machine learning, one aspect classification:order understand machine learning, one must recognize differences mindset data miner statistician, notably characterized Breiman (2001), distinguished two types models f y, response variable, x, vector explanatory variables. One might consider data model f y \\(\\sim\\) f(x), assess whether f reasonably process generated y x, make inferences f. goal learn real process generated y x.Alternatively, one might construct algorithmic model f, \\(y \\sim f(x)\\), use f predict unobserved values y. can determined f fact good job predicting values y, one might care learn much f. former case, since want learn f, simpler model may preferred. Conversely, latter case, since want predict new values y, may indifferent model complexity (concerns overfitting scalability).Classification supervised learning technique extract general patterns data order build predictor new test validation data set. , model classify new points groups (numerical response values) based model built set data provides known group membership value. consider classifying categories (often one two categories) well predicting numeric variable (e.g., support vector machines linear regression).examples classification techniques include: linear regression, logistic regression, neural networks, classification trees, Random Forests, k-nearest neighbors, support vector machines, näive Bayes, linear discriminant analysis. cover methods bold.Simple Better (Fielding (2007), p. 87)want avoid -fitting model (certainly, bad idea model noise!)Future prediction performance goes many predictors.Simple models provide better insight causality specific associations.Fewer predictors implies fewer variables collect later studies.said, model still represent complexity data! describe trade-“bias-variance” trade-. order fully understand trade-, let’s first cover structure model building also classification method known \\(k\\)-Nearest Neighbors.","code":""},{"path":"class.html","id":"model-building-process","chapter":"8 Classification","heading":"8.1 Model Building Process","text":"classification prediction models basic steps. data preprocessed, model trained, model validated.variables information used train model fully tuned, processed, considered model, won’t matter sophisticated special model . Garbage , garbage .","code":""},{"path":"class.html","id":"cv","chapter":"8 Classification","heading":"8.1.1 Cross Validation","text":"","code":""},{"path":"class.html","id":"bias-variance-trade-off","chapter":"8 Classification","heading":"Bias-variance trade-off","text":"Excellent resourcefor explaining bias-variance trade-: http://scott.fortmann-roe.com/docs/BiasVariance.htmlVariance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Variance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Generally, simpler model, lower variance. complicated model, lower bias. class, cross validation used assess model fit. [time permits, Receiver Operating Characteristic (ROC) curves also covered.]\\[\\begin{align}\n\\mbox{prediction error } = \\mbox{ irreducible error } + \\mbox{ bias } + \\mbox{ variance}\n\\end{align}\\]irreducible error irreducible error natural variability comes observations. matter good model , never able predict perfectly.bias bias model represents difference true model model simple. , complicated model (e.g., smaller \\(k\\) \\(k\\)NN), closer points prediction. model gets complicated (e.g., \\(k\\) decreases), bias goes .variance variance represents variability model sample sample. , simple model (big \\(k\\) \\(k\\)NN) change lot sample sample. variance decreases model becomes simple (e.g., \\(k\\) increases).Note bias-variance trade-. want prediction error small, choose model medium respect bias variance. control irreducible error.\nFigure 1.4: Test training error function model complexity. Note error goes monotonically training data. careful overfit!! (Hastie, Tibshirani, Friedman 2001)\nfollowing interactive visualization excellent job communicating trade-bias variance function specific tuning parameter, : minimum node size classification tree. http://www.r2d3.us/visual-intro--machine-learning-part-2/\nFigure 1.5: Great interactive viz http://www.r2d3.us/visual-intro--machine-learning-part-2/\n","code":""},{"path":"class.html","id":"implementing-cross-validation","chapter":"8 Classification","heading":"Implementing Cross Validation","text":"\nFigure 1.6: (Flach 2012)\nCross validation typically used two ways.assess model’s accuracy (model assessment).build model (model selection).","code":""},{"path":"class.html","id":"different-ways-to-cv","chapter":"8 Classification","heading":"Different ways to CV","text":"Suppose build classifier given data set. ’d like know well model classifies observations, test samples hand, error rate much lower model’s inherent accuracy rate. Instead, ’d like predict new observations used create model. various ways creating test validation sets data:one training set, one test set [two drawbacks: estimate error highly variable depends points go training set; training data set smaller full data set, error rate biased way overestimates actual error rate modeling technique.]leave one cross validation (LOOCV)remove one observationbuild model using remaining n-1 pointspredict class membership observation removedrepeat removing observation one time\\(V\\)-fold cross validation (\\(V\\)-fold CV)\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]\nLOOCV special case \\(V\\)-fold CV \\(V=n\\)\nadvantage \\(V\\)-fold computational\n\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]LOOCV special case \\(V\\)-fold CV \\(V=n\\)advantage \\(V\\)-fold computational\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]","code":""},{"path":"class.html","id":"cv-for-model-assessment-10-fold","chapter":"8 Classification","heading":"CV for Model assessment 10-fold","text":"assume \\(k\\) given \\(k\\)-NNremove 10% databuild model using remaining 90%predict class membership / continuous response 10% observations removedrepeat removing decile one timea good measure model’s ability predict error rate associated predictions data independently predicted","code":""},{"path":"class.html","id":"cv-for-model-selection-10-fold","chapter":"8 Classification","heading":"CV for Model selection 10-fold","text":"set \\(k\\) \\(k\\)-NNbuild model using \\(k\\) value set :\nremove 10% data\nbuild model using remaining 90%\npredict class membership / continuous response 10% observations removed\nrepeat removing decile one time\nremove 10% databuild model using remaining 90%predict class membership / continuous response 10% observations removedrepeat removing decile one timemeasure CV prediction error \\(k\\) value handrepeat steps 1-3 choose \\(k\\) prediction error lowest","code":""},{"path":"class.html","id":"cv-for-model-assessment-and-selection-10-fold","chapter":"8 Classification","heading":"CV for Model assessment and selection 10-fold","text":", one approach use test/training data CV order model assessment selection. Note CV used steps, algorithm slightly complicated.split data training test observationsset \\(k\\) \\(k\\)-NNbuild model using \\(k\\) value set training data:\nremove 10% training data\nbuild model using remaining 90% training data\npredict class membership / continuous response 10% training observations removed\nrepeat removing decile one time training data\nremove 10% training databuild model using remaining 90% training datapredict class membership / continuous response 10% training observations removedrepeat removing decile one time training datameasure CV prediction error \\(k\\) value hand training datarepeat steps 2-4 choose \\(k\\) prediction error lowest training datausing \\(k\\) value given step 5, assess prediction error test data\nFigure 7.1: Nested cross-validation: two cross-validation loops run one inside . (Varoquaux et al. 2017)\n","code":""},{"path":"class.html","id":"tidymodels","chapter":"8 Classification","heading":"8.1.2 tidymodels","text":"tidymodels framework provides series steps allow systematic model building. steps :partition databuild recipeselect modelcreate workflowfit modelvalidate model","code":""},{"path":"class.html","id":"partition-the-data","chapter":"8 Classification","heading":"1. Partition the data","text":"Put testing data pocket (keep secret R!!)\nFigure 7.2: Image credit: Julia Silge\n","code":""},{"path":"class.html","id":"build-a-recipe","chapter":"8 Classification","heading":"2. build a recipe","text":"Start recipe()Define variables involvedDescribe preprocessing step--stepfeature engineering preprocessing:feature engineering process transforming raw data features (variables) better predictors (model hand).Examples include:create new variables (e.g., combine levels -> state region)transform variable (e.g., log, polar coordinates)continuous variables -> discrete (e.g., binning)numerical categorical data -> factors / character strings (one hot encoding)time -> discretized timemissing values -> imputedNA -> levelcontinuous variables -> center & scale (“normalize”)step_ functionsFor information: https://recipes.tidymodels.org/reference/index.html","code":"\nls(pattern = '^step_', env = as.environment('package:recipes'))##  [1] \"step_arrange\"       \"step_bagimpute\"     \"step_bin2factor\"   \n##  [4] \"step_BoxCox\"        \"step_bs\"            \"step_center\"       \n##  [7] \"step_classdist\"     \"step_corr\"          \"step_count\"        \n## [10] \"step_cut\"           \"step_date\"          \"step_depth\"        \n## [13] \"step_discretize\"    \"step_downsample\"    \"step_dummy\"        \n## [16] \"step_factor2string\" \"step_filter\"        \"step_geodist\"      \n## [19] \"step_holiday\"       \"step_hyperbolic\"    \"step_ica\"          \n## [22] \"step_impute_bag\"    \"step_impute_knn\"    \"step_impute_linear\"\n## [25] \"step_impute_lower\"  \"step_impute_mean\"   \"step_impute_median\"\n## [28] \"step_impute_mode\"   \"step_impute_roll\"   \"step_indicate_na\"  \n## [31] \"step_integer\"       \"step_interact\"      \"step_intercept\"    \n## [34] \"step_inverse\"       \"step_invlogit\"      \"step_isomap\"       \n## [37] \"step_knnimpute\"     \"step_kpca\"          \"step_kpca_poly\"    \n## [40] \"step_kpca_rbf\"      \"step_lag\"           \"step_lincomb\"      \n## [43] \"step_log\"           \"step_logit\"         \"step_lowerimpute\"  \n## [46] \"step_meanimpute\"    \"step_medianimpute\"  \"step_modeimpute\"   \n## [49] \"step_mutate\"        \"step_mutate_at\"     \"step_naomit\"       \n## [52] \"step_nnmf\"          \"step_normalize\"     \"step_novel\"        \n## [55] \"step_ns\"            \"step_num2factor\"    \"step_nzv\"          \n## [58] \"step_ordinalscore\"  \"step_other\"         \"step_pca\"          \n## [61] \"step_pls\"           \"step_poly\"          \"step_profile\"      \n## [64] \"step_range\"         \"step_ratio\"         \"step_regex\"        \n## [67] \"step_relevel\"       \"step_relu\"          \"step_rename\"       \n## [70] \"step_rename_at\"     \"step_rm\"            \"step_rollimpute\"   \n## [73] \"step_sample\"        \"step_scale\"         \"step_select\"       \n## [76] \"step_shuffle\"       \"step_slice\"         \"step_spatialsign\"  \n## [79] \"step_sqrt\"          \"step_string2factor\" \"step_unknown\"      \n## [82] \"step_unorder\"       \"step_upsample\"      \"step_window\"       \n## [85] \"step_YeoJohnson\"    \"step_zv\""},{"path":"class.html","id":"select-a-model","chapter":"8 Classification","heading":"3. select a model","text":"specify model:pick modelset mode (regression vs classification, needed)set engineExamples engines classification algorithms cover class:","code":"\nshow_engines(\"nearest_neighbor\")## # A tibble: 2 × 2\n##   engine mode          \n##   <chr>  <chr>         \n## 1 kknn   classification\n## 2 kknn   regression\nshow_engines(\"decision_tree\")## # A tibble: 5 × 2\n##   engine mode          \n##   <chr>  <chr>         \n## 1 rpart  classification\n## 2 rpart  regression    \n## 3 C5.0   classification\n## 4 spark  classification\n## 5 spark  regression\nshow_engines(\"rand_forest\")## # A tibble: 6 × 2\n##   engine       mode          \n##   <chr>        <chr>         \n## 1 ranger       classification\n## 2 ranger       regression    \n## 3 randomForest classification\n## 4 randomForest regression    \n## 5 spark        classification\n## 6 spark        regression\nshow_engines(\"svm_poly\")## # A tibble: 2 × 2\n##   engine  mode          \n##   <chr>   <chr>         \n## 1 kernlab classification\n## 2 kernlab regression\nshow_engines(\"svm_rbf\")## # A tibble: 4 × 2\n##   engine    mode          \n##   <chr>     <chr>         \n## 1 kernlab   classification\n## 2 kernlab   regression    \n## 3 liquidSVM classification\n## 4 liquidSVM regression\nshow_engines(\"linear_reg\")## # A tibble: 5 × 2\n##   engine mode      \n##   <chr>  <chr>     \n## 1 lm     regression\n## 2 glmnet regression\n## 3 stan   regression\n## 4 spark  regression\n## 5 keras  regression"},{"path":"class.html","id":"create-a-workflow","chapter":"8 Classification","heading":"4. Create a workflow","text":"workflow combines model / engine recipe.","code":""},{"path":"class.html","id":"fit-the-model","chapter":"8 Classification","heading":"5. Fit the model","text":"Putting together, fit() give model specifications.","code":""},{"path":"class.html","id":"validate-the-model","chapter":"8 Classification","heading":"6. Validate the model","text":"model parametersSome model parameters tuned data (aren’t).\nlinear model coefficients optimized (tuned)\nk-nn value “k” tuned\nmodel parameters tuned data (aren’t).linear model coefficients optimized (tuned)k-nn value “k” tunedIf model tuned using data, data used assess model.model tuned using data, data used assess model.Cross Validation, iteratively put data pocket.Cross Validation, iteratively put data pocket.example, keep 1/5 data pocket, build model remaining 4/5 data.example, keep 1/5 data pocket, build model remaining 4/5 data.Cross validation tuning parameters. Note cross validation done training data.\nFigure 1.7: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 1.8: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 1.9: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.1: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.2: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.3: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.4: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.5: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.6: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 8.7: Image credit: Alison Hill\n","code":""},{"path":"class.html","id":"reflecting-on-model-building","chapter":"8 Classification","heading":"Reflecting on Model Building","text":"Tidy Modeling R, Kuhn Silge walk example entire model building process. Note stages visited often coming appropriate model.\nFigure 8.8: Image credit: https://www.tmwr.org/\n\nFigure 8.9: Image credit: https://www.tmwr.org/\n\nFigure 8.10: Image credit: https://www.tmwr.org/\n","code":""},{"path":"class.html","id":"r-model-penguins","chapter":"8 Classification","heading":"8.1.3 R model: penguins","text":"\nFigure 8.11: Image credit: Alison Hill\n","code":"\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>"},{"path":"class.html","id":"partition-the-data-1","chapter":"8 Classification","heading":"1. Partition the data","text":"","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)"},{"path":"class.html","id":"build-a-recipe-1","chapter":"8 Classification","heading":"2. build a recipe","text":"","code":"\npenguin_recipe <-\n  recipe(body_mass_g ~ species + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm + sex + year,\n         data = penguin_train) %>%\n  step_mutate(year = as.factor(year)) %>%\n  step_unknown(sex, new_level = \"unknown\") %>%\n  step_relevel(sex, ref_level = \"female\") %>%\n  update_role(island, new_role = \"id variable\")\nsummary(penguin_recipe)## # A tibble: 8 × 4\n##   variable          type    role        source  \n##   <chr>             <chr>   <chr>       <chr>   \n## 1 species           nominal predictor   original\n## 2 island            nominal id variable original\n## 3 bill_length_mm    numeric predictor   original\n## 4 bill_depth_mm     numeric predictor   original\n## 5 flipper_length_mm numeric predictor   original\n## 6 sex               nominal predictor   original\n## 7 year              numeric predictor   original\n## 8 body_mass_g       numeric outcome     original"},{"path":"class.html","id":"select-a-model-1","chapter":"8 Classification","heading":"3. select a model","text":"","code":"\npenguin_lm <- linear_reg() %>%\n  set_engine(\"lm\")\npenguin_lm## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"class.html","id":"create-a-workflow-1","chapter":"8 Classification","heading":"4. Create a workflow","text":"","code":"\npenguin_wflow <- workflow() %>%\n  add_model(penguin_lm) %>%\n  add_recipe(penguin_recipe)\npenguin_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 3 Recipe Steps\n## \n## • step_mutate()\n## • step_unknown()\n## • step_relevel()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"class.html","id":"fit-the-model-1","chapter":"8 Classification","heading":"5. Fit the model","text":"","code":"\npenguin_fit <- penguin_wflow %>%\n  fit(data = penguin_train)\npenguin_fit %>% tidy()## # A tibble: 10 × 5\n##    term              estimate std.error statistic  p.value\n##    <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)        -2417.     665.      -3.64  3.36e- 4\n##  2 speciesChinstrap    -208.      92.9     -2.24  2.58e- 2\n##  3 speciesGentoo        985.     152.       6.48  5.02e-10\n##  4 bill_length_mm        13.5      8.29     1.63  1.04e- 1\n##  5 bill_depth_mm         80.9     22.1      3.66  3.10e- 4\n##  6 flipper_length_mm     20.8      3.62     5.74  2.81e- 8\n##  7 sexmale              351.      52.6      6.67  1.72e-10\n##  8 sexunknown            47.6    103.       0.460 6.46e- 1\n##  9 year2008             -24.8     47.5     -0.521 6.03e- 1\n## 10 year2009             -61.9     46.0     -1.35  1.80e- 1"},{"path":"class.html","id":"cross-validation","chapter":"8 Classification","heading":"6. Cross validation","text":"(See Section 8.1.1 future R examples full description cross validation.)","code":""},{"path":"class.html","id":"knn","chapter":"8 Classification","heading":"8.2 \\(k\\)-Nearest Neighbors","text":"\\(k\\)-Nearest Neighbor algorithm exactly sounds like .user decides integer value \\(k\\)user decides integer value \\(k\\)user decides distance metric (\\(k\\)-NN algorithms default Euclidean distance)user decides distance metric (\\(k\\)-NN algorithms default Euclidean distance)point classified group majority \\(k\\) closest points training data.point classified group majority \\(k\\) closest points training data.","code":""},{"path":"class.html","id":"k-nn-algorithm","chapter":"8 Classification","heading":"8.2.1 \\(k\\)-NN algorithm","text":"Decide distance metric (e.g., Euclidean distance, 1 - correlation, etc.) find distances point test set point training set. distance measured feature space, , respect explanatory variables (response variable).n.b. machine learning algorithms use “distance” measure, “distance” required mathematical distance metric. Indeed, 1-correlation common distance measure, fails triangle inequality.Consider point test set. Find \\(k\\) closest points training set one test observation.Consider point test set. Find \\(k\\) closest points training set one test observation.Using majority vote, find dominate class \\(k\\) closest points. Predict class label test observation.Using majority vote, find dominate class \\(k\\) closest points. Predict class label test observation.Note: response variable continuous (instead categorical), find average response variable \\(k\\) training point predicted response one test observation.Shortcomings \\(k\\)-NN:one class can dominate large majorityEuclidean distance dominated scaleit can computationally unwieldy (unneeded!!) calculate distances (algorithms search smartly)output doesn’t provide information explanatory variables informative.doesn’t work well large datasets (cost prediction high, model doesn’t always find structure)doesn’t work well high dimensions (curse dimensionality – distance becomes meaningless high dimensions)need lot feature scalingsensitive noise outliersStrengths \\(k\\)-NN:can easily work number categories (outcome variable)can predict quantitative response variablethe bias 1-NN often low (variance high)distance metric can used (algorithm models data appropriately)method straightforward implement / understandthere training period (.e., discrimination function created)model nonparametric (distributional assumptions data)great model imputing missing data","code":""},{"path":"class.html","id":"r-k-nn-penguins","chapter":"8 Classification","heading":"8.2.2 R k-NN: penguins","text":"fit \\(k\\)-Nearest Neighbor algorithm penguins dataset. previously (come), ’ll use entire tidymodels workflow including partitioning data, build recipe, select model, create workflow, fit model, validate model","code":"\nlibrary(GGally) # for plotting\nlibrary(tidymodels)\ndata(penguins)"},{"path":"class.html","id":"penguin-data","chapter":"8 Classification","heading":"penguin data","text":"","code":"\nggpairs(penguins, mapping = ggplot2::aes(color = species), alpha=.4)"},{"path":"class.html","id":"k-nn-to-predict-penguin-species","chapter":"8 Classification","heading":"\\(k\\)-NN to predict penguin species","text":"","code":""},{"path":"class.html","id":"partition-the-data-2","chapter":"8 Classification","heading":"1. Partition the data","text":"","code":""},{"path":"class.html","id":"build-a-recipe-2","chapter":"8 Classification","heading":"2. Build a recipe","text":"","code":"\npenguin_knn_recipe <-\n  recipe(species ~ body_mass_g + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm,\n         data = penguin_train) %>%\n  update_role(island, new_role = \"id variable\") %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_knn_recipe)## # A tibble: 6 × 4\n##   variable          type    role        source  \n##   <chr>             <chr>   <chr>       <chr>   \n## 1 body_mass_g       numeric predictor   original\n## 2 island            nominal id variable original\n## 3 bill_length_mm    numeric predictor   original\n## 4 bill_depth_mm     numeric predictor   original\n## 5 flipper_length_mm numeric predictor   original\n## 6 species           nominal outcome     original"},{"path":"class.html","id":"select-a-model-2","chapter":"8 Classification","heading":"3. Select a model","text":"(note ’ve used default number neighbors (\\(k=7\\)).)","code":"\npenguin_knn <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\npenguin_knn## K-Nearest Neighbor Model Specification (classification)\n## \n## Computational engine: kknn"},{"path":"class.html","id":"create-a-workflow-2","chapter":"8 Classification","heading":"4. Create a workflow","text":"","code":"\npenguin_knn_wflow <- workflow() %>%\n  add_model(penguin_knn) %>%\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (classification)\n## \n## Computational engine: kknn"},{"path":"class.html","id":"fit-predict","chapter":"8 Classification","heading":"5. Fit (/ predict)","text":"next R code chunk break pieces – , run line one time.","code":"\npenguin_knn_fit <- penguin_knn_wflow %>%\n  fit(data = penguin_train)\npenguin_knn_fit %>% \n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  metrics(truth = species, estimate = .pred_class) %>%\n  filter(.metric == \"accuracy\")## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.988"},{"path":"class.html","id":"what-is-k","chapter":"8 Classification","heading":"What is \\(k\\)?","text":"turns default value \\(k\\) kknn engine 7. 7 best?Cross Validation!!!red observations used fit model, black observations used assess model.\nFigure 8.12: Image credit: Alison Hill\nsaw , cross validation randomly splits training data V distinct blocks roughly equal size.leave first block analysis data fit model.model used predict held-block assessment data.continue process V assessment blocks predictedThe final performance based hold-predictions averaging statistics V blocks.","code":""},{"path":"class.html","id":"b.-a-new-partition-of-the-training-data","chapter":"8 Classification","heading":"1b. A new partition of the training data","text":"","code":"\nset.seed(470)\npenguin_vfold <- vfold_cv(penguin_train,\n                          v = 3, strata = species)"},{"path":"class.html","id":"select-a-model-3","chapter":"8 Classification","heading":"3. Select a model","text":"Now knn model uses tune() indicate actually don’t know many neighbors use.","code":"\npenguin_knn_tune <- nearest_neighbor(neighbors = tune()) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")"},{"path":"class.html","id":"re-create-a-workflow","chapter":"8 Classification","heading":"4. Re-create a workflow","text":"time, use model set number neighbors.","code":"\npenguin_knn_wflow_tune <- workflow() %>%\n  add_model(penguin_knn_tune) %>%\n  add_recipe(penguin_knn_recipe)"},{"path":"class.html","id":"fit-the-model-2","chapter":"8 Classification","heading":"5. Fit the model","text":"model fit three folds created value \\(k\\) k_grid.","code":"\nk_grid <- data.frame(neighbors = seq(1, 15, by = 4))\nk_grid##   neighbors\n## 1         1\n## 2         5\n## 3         9\n## 4        13\npenguin_knn_wflow_tune %>%\n  tune_grid(resamples = penguin_vfold, \n           grid = k_grid) %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\")## # A tibble: 4 × 7\n##   neighbors .metric  .estimator  mean     n   std_err .config             \n##       <dbl> <chr>    <chr>      <dbl> <int>     <dbl> <chr>               \n## 1         1 accuracy multiclass 0.971     2 0.00595   Preprocessor1_Model1\n## 2         5 accuracy multiclass 0.977     2 0.000134  Preprocessor1_Model2\n## 3         9 accuracy multiclass 0.988     2 0.0000668 Preprocessor1_Model3\n## 4        13 accuracy multiclass 0.983     2 0.00568   Preprocessor1_Model4"},{"path":"class.html","id":"validate-the-model-1","chapter":"8 Classification","heading":"6. Validate the model","text":"Using \\(k\\) = 9, model re-trained training data tested test data (estimate overall model accuracy).","code":""},{"path":"class.html","id":"select-a-model-4","chapter":"8 Classification","heading":"3. select a model","text":"","code":"\npenguin_knn_final <- nearest_neighbor(neighbors = 9) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\npenguin_knn_final## K-Nearest Neighbor Model Specification (classification)\n## \n## Main Arguments:\n##   neighbors = 9\n## \n## Computational engine: kknn"},{"path":"class.html","id":"create-a-workflow-3","chapter":"8 Classification","heading":"4. create a workflow","text":"","code":"\npenguin_knn_wflow_final <- workflow() %>%\n  add_model(penguin_knn_final) %>%\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow_final## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (classification)\n## \n## Main Arguments:\n##   neighbors = 9\n## \n## Computational engine: kknn"},{"path":"class.html","id":"fit-the-model-3","chapter":"8 Classification","heading":"5. fit the model","text":"","code":"\npenguin_knn_fit_final <- penguin_knn_wflow_final %>%\n  fit(data = penguin_train)"},{"path":"class.html","id":"validate-the-model-2","chapter":"8 Classification","heading":"6. validate the model","text":"Huh. Seems like \\(k=9\\) didn’t well \\(k=7\\) (value tried beginning cross validating).Well, turns , ’s nature variability, randomness, model building.don’t know truth, won’t every find perfect model.","code":"\npenguin_knn_fit_final %>% \n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  metrics(truth = species, estimate = .pred_class) %>%\n  filter(.metric == \"accuracy\")## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.977"},{"path":"class.html","id":"cart","chapter":"8 Classification","heading":"8.3 Decision Trees","text":"Stephanie Yee Tony Chu created following (amazing!) demonstration tree intuition. Step--step, build recursive binary tree order model differences homes SF homes NYC.\nFigure 8.13: http://www.r2d3.us/visual-intro--machine-learning-part-1/ visual introduction machine learning.\nDecision trees used sorts predictive descriptive models. NYT created recursive binary decision tree show patterns identity political affiliation.\nFigure 4.1: https://www.nytimes.com/interactive/2019/08/08/opinion/sunday/party-polarization-quiz.html Quiz: Let Us Predict Whether ’re Democrat Republican NYT, Aug 8, 2019. Note race first dominant node, followed religion.\n","code":""},{"path":"class.html","id":"cart-algorithm","chapter":"8 Classification","heading":"8.3.1 CART algorithm","text":"Basic Classification Regression Trees (CART) Algorithm:Start observations one group.Find variable/split best separates response variable (successive binary partitions based different predictors / explanatory variables).\nEvaluation “homogeneity” within group\nDivide data two groups (“leaves”) split (“node”).\nWithin split, find best variable/split separates outcomes.\nEvaluation “homogeneity” within groupDivide data two groups (“leaves”) split (“node”).Within split, find best variable/split separates outcomes.Continue groups small sufficiently “pure.”Prune tree.Shortcomings CART:Straight CART generally predictive accuracy classification approaches. (improve model - see Random Forests, boosting, bagging)Difficult write / consider CART “model”Without proper pruning, model can easily lead overfittingWith lots predictors, (even greedy) partitioning can become computationally unwieldyOften, prediction performance poorStrengths CART:easy explain; trees easy display graphically (make easy interpret). (mirror typical human decision-making process.)Can handle categorical numerical predictors response variables (indeed, can handle mixed predictors time!).Can handle 2 groups categorical predictionsEasily ignore redundant variables.Perform better linear models non-linear settings. Classification trees non-linear models, immediately use interactions variables.Data transformations may less important (monotone transformations explanatory variables won’t change anything).","code":""},{"path":"class.html","id":"classification-trees","chapter":"8 Classification","heading":"Classification Trees","text":"classification tree used predict categorical response variable (rather quantitative one). end predicted value one commonly occurring class training observations region belongs. goal create regions homogeneous possible respect response variable - categories.measures impurityCalculate classification error rate fraction training observations region belong common class: \\[E_m = 1 - \\max_k(\\hat{p}_{mk})\\]\n\\(\\hat{p}_{mk}\\) represents proportion training observations \\(m\\)th region \\(k\\)th class. However, classification error rate particularly sensitive node purity, two additional measures typically used partition regions., Gini index defined \\[G_m= \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})\\]\nmeasure total variance across \\(K\\) classes. [Recall, variance Bernoulli random variable \\(\\pi\\) = P(success) \\(\\pi(1-\\pi)\\).] Note Gini index takes small value \\(\\hat{p}_{mk}\\) values close zero one. reason, Gini index referred measure node purity - small value indicates node contains predominantly observations single class.Last, cross-entropy defined \\[D_m = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}\\]\nSince \\(0 \\leq \\hat{p}_{mk} \\leq 1\\) follows \\(0 \\leq -\\hat{p}_{mk} \\log\\hat{p}_{mk}\\). One can show cross-entropy take value near zero \\(\\hat{p}_{mk}\\) values near zero near one. Therefore, like Gini index, cross-entropy take small value \\(m\\)th node pure.build tree, typically Gini index cross-entropy used evaluate particular split.prune tree, often classification error used (accuracy final pruned tree goal)Computationally, usually infeasible consider every possible partition observations. Instead looking partitions, perform top approach problem known recursive binary splitting (greedy look current split outcomes splits come).Recursive Binary Splitting Categories (given node)Select predictor \\(X_j\\) cutpoint \\(s\\) splitting predictor space regions \\(\\{X | X_j< s\\}\\) \\(\\{X | X_j \\geq s\\}\\) lead greatest reduction Gini index cross-entropy.\\(j\\) \\(s\\), define pair half-planes \n\\[R_1(j,s) = \\{X | X_j < s\\} \\mbox{ } R_2(j,s) = \\{X | X_j \\geq s\\}\\]\nseek value \\(j\\) \\(s\\) minimize equation:\n\\[\\begin{align}\n& \\sum_{:x_i \\R_1(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + \\sum_{:x_i \\R_2(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\mbox{equivalently: } & n_{R_1} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + n_{R_2} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\end{align}\\]Repeat process, looking best predictor best cutpoint within one previously identified regions (producing three regions, now).Keep repeating process stopping criterion reached - example, region contains 5 observations.","code":""},{"path":"class.html","id":"regression-trees","chapter":"8 Classification","heading":"Regression Trees","text":"goal algorithm regression tree split set possible value data \\(|T|\\) distinct non-overlapping regions, \\(R_1, R_2, \\ldots, R_{|T|}\\). every observation falls region \\(R_m\\), make prediction - mean response values training observations \\(R_m\\). find regions \\(R_1, \\ldots, R_{|T|}\\)?\\(\\Rightarrow\\) Minimize RSS, \\[RSS = \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2\\]\n\\(\\overline{y}_{R_m}\\) mean response training observations within \\(m\\)th region.(Note: chapter (James et al. 2021) refer MSE - mean squared error - addition RSS MSE simply RSS / n, see equation (2.5).), usually infeasible consider every possible partition observations. Instead looking partitions, perform top approach problem known recursive binary splitting (greedy look current split outcomes splits come).Recursive Binary Splitting Numerical Response (given node)Select predictor \\(X_j\\) cutpoint \\(s\\) splitting predictor space regions \\(\\{X | X_j< s\\}\\) \\(\\{X | X_j \\geq s\\}\\) lead greatest reduction RSS.\\(j\\) \\(s\\), define pair half-planes \n\\[R_1(j,s) = \\{X | X_j < s\\} \\mbox{ } R_2(j,s) = \\{X | X_j \\geq s\\}\\]\nsee value \\(j\\) \\(s\\) minimize equation:\n\\[\\sum_{:x_i \\R_1(j,s)} (y_i - \\overline{y}_{R_1})^2 + \\sum_{:x_i \\R_2(j,s)} (y_i - \\overline{y}_{R_2})^2\\]\n\\(\\overline{y}_{R_1}\\) mean response training observations \\(R_1(j,s)\\) \\(\\overline{y}_{R_2}\\) mean response training observations \\(R_2(j,s)\\).Repeat process, looking best predictor best cutpoint within one previously identified regions (producing three regions, now).Keep repeating process stopping criterion reached - example, region contains 5 observations.","code":""},{"path":"class.html","id":"avoiding-overfitting","chapter":"8 Classification","heading":"(Avoiding) Overfitting","text":"Ideally, tree overfit training data. One imagine easy grow tree training data end terminal nodes completely homogeneous (don’t represent test data).See following (amazing!) demonstration intuition model validation / overfitting: http://www.r2d3.us/visual-intro--machine-learning-part-2/One possible algorithm building tree split based reduction RSS (Gini index, etc.) exceeding (presumably high) threshold. However, strategy known short sighted, split later tree may contain large amount information. better strategy grow large tree \\(T_0\\) prune back order obtain subtree. Use cross validation build subtree overfit data.Algorithm: Building Regression TreeUse recursive binary splitting grow large tree training data, stopping terminal node fewer minimum number observations.Apply cost complexity pruning large tree order obtain sequence best subtrees, function \\(\\alpha\\).Use \\(V\\)-fold cross-validation choose \\(\\alpha\\). , divide training observations \\(V\\) folds. \\(v=1, 2, \\ldots, V\\):\nRepeat Steps 1 2 \\(V\\)th fold training data.\nEvaluate mean squared prediction error data left-\\(k\\)th fold, function \\(\\alpha\\).\nvalue \\(\\alpha\\), average prediction error (either misclassification RSS), pick \\(\\alpha\\) minimize average error.\nRepeat Steps 1 2 \\(V\\)th fold training data.Evaluate mean squared prediction error data left-\\(k\\)th fold, function \\(\\alpha\\).\nvalue \\(\\alpha\\), average prediction error (either misclassification RSS), pick \\(\\alpha\\) minimize average error.Return subtree Step 2 corresponds chosen value \\(\\alpha\\).","code":""},{"path":"class.html","id":"cost-complexity-pruning","chapter":"8 Classification","heading":"Cost Complexity Pruning","text":"Also known weakest link pruning, idea consider sequence trees indexed nonnegative tuning parameter \\(\\alpha\\) (instead considering every single subtree). Generally, idea cost larger (complex!) tree. define cost complexity criterion (\\(\\alpha > 0\\)):\n\\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\]\n\\(k(m)\\) class majority observations node \\(m\\) \\(|T|\\) number terminal nodes tree.\\(\\alpha\\) small: \\(\\alpha\\) set small, saying risk worrisome complexity larger trees favored reduce risk.\\(\\alpha\\) large: \\(\\alpha\\) set large, complexity tree worrisome smaller trees favored.way think cost complexity consider \\(\\alpha\\) increasing. \\(\\alpha\\) gets bigger, “best” tree smaller. test error monotonically related size training tree.note \\(\\alpha\\)text (Introduction Statistical Learning) almost everywhere else might look, cost complexity defined previous slides.However, might notice R cost_complexity value typically less 1. can tell, value function minimized R average squared errors missclassification rate.\\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\]","code":""},{"path":"class.html","id":"variations-on-a-theme","chapter":"8 Classification","heading":"Variations on a theme","text":"main ideas consistent throughout CART algorithms. However, exact details implementation can change function function, often times difficult decipher exactly equation used. tree function R, much decision making done deviance defined :\\[\\mbox{numerical: deviance} = \\sum_{m=1}^{|T|}  \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2\\]\\[\\mbox{categorical: deviance} = -2\\sum_{m=1}^{|T|} \\sum_{k=1}^K n_{mk} \\log \\hat{p}_{mk}\\]CART algorithm, minimize deviance (types variables). categorical deviance small observations majority group (high proportion). Also, \\(\\lim_{\\epsilon \\rightarrow 0} \\epsilon \\log(\\epsilon) = 0\\). Additionally, methods cross validation can also vary. particular, number variables large, tree algorithm can slow cross validation process - choice \\(\\alpha\\) - needs efficient.","code":""},{"path":"class.html","id":"cv-for-model-building-and-model-assessment","chapter":"8 Classification","heading":"CV for model building and model assessment","text":"Notice CV used model building model assessment. possible (practical, though quite computational!) use practices classification model. algorithm follows.Algorithm: CV \\(V_1\\)-fold CV building \\(V_2\\)-fold CV assessmentPartition data \\(V_1\\) groups.Remove first group, train data remaining \\(V_1-1\\) groups.Use \\(V_2\\)-fold cross-validation (\\(V_1-1\\) groups) choose \\(\\alpha\\). , divide training observations \\(V_2\\) folds find \\(\\alpha\\) minimizes error.Using subtree corresponds chosen value \\(\\alpha\\), predict first \\(V_1\\) hold samples.Repeat steps 2-4 using remaining \\(V_1 - 1\\) groups.","code":""},{"path":"class.html","id":"r-cart-example","chapter":"8 Classification","heading":"8.3.2 R CART Example","text":"Census Bureau divides country “tracts” approximately\nequal population. 1990 Census, California divided 20640 tracts. One data sets (houses http://lib.stat.cmu.edu/datasets/; http://lib.stat.cmu.edu/datasets/houses.zip) records following tract California: Median house price, median house age, total number rooms, total number bedrooms, total number occupants, total number houses, median income (thousands dollars), latitude longitude. appeared Pace Barry (1997), “Sparse Spatial Autoregressions,” Statistics Probability Letters.","code":""},{"path":"class.html","id":"classification-and-regression-trees","chapter":"8 Classification","heading":"Classification and Regression Trees","text":"Classification Trees used predict response class \\(Y\\) input \\(X_1, X_2, \\ldots, X_n\\). continuous response ’s called regression tree, categorical, ’s called classification tree. node tree, check value one input \\(X_i\\) depending (binary) answer continue left right subbranch. reach leaf find prediction (usually simple statistic dataset leaf represents, like common value available classes).Note maxdepth: might expect, maxdepth indicates longest length root tree terminal node. However, rpart (particular, using rpart rpart2 caret), default settings keep tree growing way singular nodes, even high maxdepth.","code":""},{"path":"class.html","id":"regression-trees-1","chapter":"8 Classification","heading":"Regression Trees","text":"technical reasons (e.g., see ), step_log() outcome variable step gives problems predictions end. Therefore, mutate outcome variable within dataset starting model building process.","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\n# recipe\nhouse_cart_recipe <-\n  recipe(logValue ~ Longitude + Latitude ,\n         data = house_train)\n# model\nhouse_cart <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_wflow <- workflow() %>%\n  add_model(house_cart) %>%\n  add_recipe(house_cart_recipe)\n\n# fit\nhouse_cart_fit <- house_cart_wflow %>%\n  fit(data = house_train)"},{"path":"class.html","id":"model-output","chapter":"8 Classification","heading":"Model Output","text":"following scatter plot can made CART built using two numerical predictor variables.","code":"\nhouse_cart_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 15480 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##   1) root 15480 5024.405000 12.08947  \n##     2) Latitude>=38.485 1541  283.738200 11.59436  \n##       4) Latitude>=39.355 506   48.267930 11.31530 *\n##       5) Latitude< 39.355 1035  176.803400 11.73079 *\n##     3) Latitude< 38.485 13939 4321.152000 12.14421  \n##       6) Longitude>=-121.645 10454 3320.946000 12.06198  \n##        12) Latitude>=34.635 2166  491.986400 11.52110  \n##          24) Longitude>=-120.265 1083  166.051200 11.28432 *\n##          25) Longitude< -120.265 1083  204.505800 11.75787 *\n##        13) Latitude< 34.635 8288 2029.685000 12.20333  \n##          26) Longitude>=-118.315 6240 1373.830000 12.09295  \n##            52) Longitude>=-117.575 2130  516.313400 11.87918  \n##             104) Latitude>=33.605 821  123.684300 11.64002 *\n##             105) Latitude< 33.605 1309  316.218800 12.02918  \n##               210) Longitude>=-116.33 97    8.931327 11.17127 *\n##               211) Longitude< -116.33 1212  230.181300 12.09784  \n##                 422) Longitude>=-117.165 796  101.805300 11.94935 *\n##                 423) Longitude< -117.165 416   77.245280 12.38196 *\n##            53) Longitude< -117.575 4110  709.740000 12.20373  \n##             106) Latitude>=33.735 3529  542.838300 12.14908  \n##               212) Latitude< 34.105 2931  379.526800 12.09154  \n##                 424) Longitude< -118.165 1114  147.375800 11.91911 *\n##                 425) Longitude>=-118.165 1817  178.722200 12.19726 *\n##               213) Latitude>=34.105 598  106.051400 12.43109 *\n##             107) Latitude< 33.735 581   92.340630 12.53568 *\n##          27) Longitude< -118.315 2048  348.149000 12.53967  \n##            54) Latitude>=34.165 949  106.791800 12.38022 *\n##            55) Latitude< 34.165 1099  196.395200 12.67735  \n##             110) Longitude>=-118.365 431   85.796770 12.38191 *\n##             111) Longitude< -118.365 668   48.703000 12.86798 *\n##       7) Longitude< -121.645 3485  717.479900 12.39087  \n##        14) Latitude>=37.925 796  133.300900 12.10055 *\n##        15) Latitude< 37.925 2689  497.226200 12.47681 *\n#remotes::install_github(\"grantmcdermott/parttree\")\nlibrary(parttree)\nhouse_train %>%\n  ggplot(aes(y = Longitude, x = Latitude)) + \n  geom_parttree(data = house_cart_fit, alpha = 0.2) +\n  geom_point(aes(color = MedianHouseValue)) "},{"path":"class.html","id":"predicting","chapter":"8 Classification","heading":"Predicting","text":"seen image , 12 region 12 predicted values. plot seems little odd first glance, make sense careful consideration outcome measurement predicted value.","code":"\nhouse_cart_fit %>%\n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1)"},{"path":"class.html","id":"finer-partition","chapter":"8 Classification","heading":"Finer partition","text":":node splits latitude greater 34.675 2182 houses. 513.9564 “deviance” sum squares value node. predicted value average points node: 11.5. terminal node (asterisk).","code":"       12) Latitude>=34.675 2182  513.95640 11.52385  "},{"path":"class.html","id":"more-variables","chapter":"8 Classification","heading":"More variables","text":"Including variables, latitude longitude. Note predictions much better!","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\n# recipe\nhouse_cart_full_recipe <-\n  recipe(logValue ~ . ,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_full_wflow <- workflow() %>%\n  add_model(house_cart) %>%\n  add_recipe(house_cart_full_recipe)\n\n# fit\nhouse_cart_full_fit <- house_cart_full_wflow %>%\n  fit(data = house_train)\nhouse_cart_full_fit %>%\n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.01)"},{"path":"class.html","id":"cross-validation-model-building","chapter":"8 Classification","heading":"Cross Validation (model building!)","text":"CV accuracyFinal model + prediction test dataTurns tree “better” complex – ? tree 14 nodes (depth 6) corresponds tree lowest deviance.Predicting final model test data","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\nset.seed(4321)\nhouse_vfold <- vfold_cv(house_train, v = 10)\n\ncart_grid <- expand.grid(tree_depth = seq(2, 20, by = 2))\n\n# recipe\nhouse_cart_tune_recipe <-\n  recipe(logValue ~ .,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_tune <- decision_tree(tree_depth = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_tune_wflow <- workflow() %>%\n  add_model(house_cart_tune) %>%\n  add_recipe(house_cart_tune_recipe)\n\n# tuning / fit\nhouse_tuned <- house_cart_tune_wflow %>%\n  tune_grid(resamples = house_vfold, \n           grid = cart_grid) \nhouse_tuned %>% collect_metrics() %>%\n  filter()## # A tibble: 20 × 7\n##    tree_depth .metric .estimator  mean     n std_err .config              \n##         <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1          2 rmse    standard   0.428    10 0.00224 Preprocessor1_Model01\n##  2          2 rsq     standard   0.436    10 0.00665 Preprocessor1_Model01\n##  3          4 rmse    standard   0.383    10 0.00242 Preprocessor1_Model02\n##  4          4 rsq     standard   0.547    10 0.00629 Preprocessor1_Model02\n##  5          6 rmse    standard   0.366    10 0.00239 Preprocessor1_Model03\n##  6          6 rsq     standard   0.588    10 0.00586 Preprocessor1_Model03\n##  7          8 rmse    standard   0.366    10 0.00239 Preprocessor1_Model04\n##  8          8 rsq     standard   0.588    10 0.00586 Preprocessor1_Model04\n##  9         10 rmse    standard   0.366    10 0.00239 Preprocessor1_Model05\n## 10         10 rsq     standard   0.588    10 0.00586 Preprocessor1_Model05\n## 11         12 rmse    standard   0.366    10 0.00239 Preprocessor1_Model06\n## 12         12 rsq     standard   0.588    10 0.00586 Preprocessor1_Model06\n## 13         14 rmse    standard   0.366    10 0.00239 Preprocessor1_Model07\n## 14         14 rsq     standard   0.588    10 0.00586 Preprocessor1_Model07\n## 15         16 rmse    standard   0.366    10 0.00239 Preprocessor1_Model08\n## 16         16 rsq     standard   0.588    10 0.00586 Preprocessor1_Model08\n## 17         18 rmse    standard   0.366    10 0.00239 Preprocessor1_Model09\n## 18         18 rsq     standard   0.588    10 0.00586 Preprocessor1_Model09\n## 19         20 rmse    standard   0.366    10 0.00239 Preprocessor1_Model10\n## 20         20 rsq     standard   0.588    10 0.00586 Preprocessor1_Model10\nhouse_tuned %>%\n  autoplot(metric = \"rmse\")\nhouse_tuned %>% \n  select_best(\"rmse\")## # A tibble: 1 × 2\n##   tree_depth .config              \n##        <dbl> <chr>                \n## 1          6 Preprocessor1_Model03\n# recipe\nhouse_cart_final_recipe <-\n  recipe(logValue ~ .,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_final <- decision_tree(tree_depth = 6) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_final_wflow <- workflow() %>%\n  add_model(house_cart_final) %>%\n  add_recipe(house_cart_final_recipe)\n\n# tuning / fit\nhouse_final <- house_cart_final_wflow %>%\n  fit(data = house_train)\nhouse_final## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 15480 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 15480 5024.40500 12.08947  \n##    2) MedianIncome< 3.54635 7696 1992.69800 11.77343  \n##      4) MedianIncome< 2.5165 3632  904.76740 11.57590  \n##        8) Latitude>=34.445 1897  412.81950 11.38488  \n##         16) Longitude>=-120.265 549   63.97662 11.08633 *\n##         17) Longitude< -120.265 1348  279.98120 11.50647 *\n##        9) Latitude< 34.445 1735  347.04430 11.78476  \n##         18) Longitude>=-117.775 645  111.86670 11.52607 *\n##         19) Longitude< -117.775 1090  166.47070 11.93784 *\n##      5) MedianIncome>=2.5165 4064  819.58450 11.94995  \n##       10) Latitude>=37.925 809   91.49688 11.68589 *\n##       11) Latitude< 37.925 3255  657.65510 12.01558  \n##         22) Longitude>=-122.235 2992  563.13610 11.97426  \n##           44) Latitude>=34.455 940  203.99070 11.77685  \n##             88) Longitude>=-120.155 338   31.54079 11.36422 *\n##             89) Longitude< -120.155 602   82.59029 12.00852 *\n##           45) Latitude< 34.455 2052  305.72870 12.06470  \n##             90) Longitude>=-118.285 1476  171.16160 11.95681 *\n##             91) Longitude< -118.285 576   73.36843 12.34115 *\n##         23) Longitude< -122.235 263   31.29310 12.48567 *\n##    3) MedianIncome>=3.54635 7784 1502.97400 12.40194  \n##      6) MedianIncome< 5.59185 5526  876.96730 12.25670  \n##       12) MedianHouseAge< 38.5 4497  651.27750 12.20567  \n##         24) MedianIncome< 4.53095 2616  388.38650 12.11491 *\n##         25) MedianIncome>=4.53095 1881  211.37640 12.33189 *\n##       13) MedianHouseAge>=38.5 1029  162.80030 12.47972 *\n##      7) MedianIncome>=5.59185 2258  224.13060 12.75740  \n##       14) MedianIncome< 7.393 1527  134.00030 12.64684 *\n##       15) MedianIncome>=7.393 731   32.47344 12.98835 *\nhouse_final %>% \n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1) + \n  xlab(\"log of the Median House Value\") +\n  ylab(\"predicted value of log Median House\")"},{"path":"class.html","id":"bagging","chapter":"8 Classification","heading":"8.4 Bagging","text":"tree based models given CART easy understand implement, suffer high variance. , split training data two parts random fit decision tree halves, results get quite different (might seen homework assignment!). ’d like model produces low variance - one ran different datasets, ’d get (close ) model every time.Bagging = Bootstrap Aggregating. idea sometimes fit multiple models aggregate models together, get smoother model fit give better balance bias fit variance fit. Bagging can applied classifier reduce variability.\nRecall variance sample mean variance data / n. ’ve seen idea averaging outcome gives reduced variability.\n","code":""},{"path":"class.html","id":"bagging-algorithm","chapter":"8 Classification","heading":"8.4.1 Bagging algorithm","text":"Algorithm: Bagging ForestResample (bootstrap) cases (observational units, variables).Build tree new set (bootstrapped) training observations.Average (regression) majority vote (classification).Note every bootstrap sample, approximately 2/3 observations chosen 1/3 chosen.\\[\\begin{align}\nP(\\mbox{observation $$ bootstrap sample}) &= \\bigg(1 - \\frac{1}{n} \\bigg)^n\\\\\n\\lim_{n \\rightarrow \\infty} \\bigg(1 - \\frac{1}{n} \\bigg)^n = \\frac{1}{e} \\approx \\frac{1}{3}\n\\end{align}\\]Shortcomings Bagging:Model even harder “write-” (CART)lots predictors, (even greedy) partitioning can become computationally unwieldy - now computational task even harder! (number trees grown bootstrap sample)Strengths Bagging:Can handle categorical numerical predictors response variables (indeed, can handle mixed predictors time!).Can handle 2 groups categorical predictionsEasily ignore redundant variables.Perform better linear models non-linear settings. Classification trees non-linear models, immediately use interactions variables.Data transformations may less important (monotone transformations explanatory variables won’t change anything).\n\nSimilar bias CART, reduced variance\n\n(can proved).\nSimilar bias CART, reduced variance\nNotes bagging:Bagging alone uses full set predictors determine every tree (observations bootstrapped).Note predict particular observation, start top, walk tree, get prediction. average (majority vote) predictions get one prediction observation hand.Bagging gives smoother decision boundaryBagging can done decision method (just trees).need prune CV trees. reason averaging keeps us overfitting particular observations (think averages contexts: law large numbers). Pruning wouldn’t bad thing terms fit, unnecessary good predictions (add lot complexity algorithm).","code":""},{"path":"class.html","id":"out-of-bag-oob-error-rate","chapter":"8 Classification","heading":"8.4.2 Out Of Bag (OOB) error rate","text":"Additionally, bagging, need cross-validation separate test set get unbiased estimate test set error. estimated internally, run, follows:tree constructed using different bootstrap sample original data. one-third cases left bootstrap sample used construction \\(b^{th}\\) tree.Put case left construction \\(b^{th}\\) tree \\(b^{th}\\) tree get classification. way, test set classification obtained case one-third trees.end run, take \\(j\\) class got votes every time case \\(\\) oob. proportion times \\(j\\) equal true class n averaged cases oob error estimate. proven unbiased many tests.work? Consider following predictions silly toy data set 9 observations. Recall \\(\\sim 1/3\\) observations left bootstrap sample. observations predictions made. table , X given prediction made value.Let OOB prediction \\(^{th}\\) observation \\(\\hat{y}_{(-)}\\)\\[\\begin{align}\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{=1}^n \\textrm{} (y_i \\ne \\hat{y}_{(-)}) \\ \\ \\ \\ \\ \\ \\ \\  \\mbox{classification}\\\\\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{=1}^n  (y_i - \\hat{y}_{(-)})^2  \\ \\ \\ \\ \\ \\ \\ \\ \\mbox{regression}\\\\\n\\end{align}\\]","code":""},{"path":"class.html","id":"rf","chapter":"8 Classification","heading":"8.5 Random Forests","text":"Random Forests extension bagging regression trees (note: bagging can done prediction method). , idea infusing extra variability averaging variability, RFs use subset predictor variables every node tree.“Random forests overfit. can run many trees want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","code":""},{"path":"class.html","id":"random-forest-algorithm","chapter":"8 Classification","heading":"8.5.1 Random Forest algorithm","text":"Algorithm: Random ForestBootstrap sample training set.Grow un-pruned tree bootstrap sample.split, select \\(m\\) variables determine best split using \\(m\\) predictors.\nTypically \\(m = \\sqrt{p}\\) \\(\\log_2 p\\), \\(p\\) number features. Random Forests overly sensitive value \\(m\\). [splits chosen trees: according either squared error gini index / cross entropy / classification error.]prune tree. Save tree !Repeat steps 1-2 many many trees.tree grown bootstrap sample, predict OOB samples. tree grown, \\(~1/3\\) training samples won’t bootstrap sample – called bootstrap (OOB) samples. OOB samples can used test data estimate error rate tree.Combine OOB predictions create “--bag” error rate (either majority vote average predictions / class probabilities).trees together represent model used new predictions (either majority vote average).\nFigure 8.14: Building multiple trees combining outputs (predictions). Note image makes choice average tree probabilities instead using majority vote. valid methods creating Random Forest prediction model. http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf\nShortcomings Random Forests:Model even harder “write-” (CART)lots predictors, (even greedy) partitioning can become computationally unwieldy - now computational task even harder! … bagging observations andStrengths Random Forests:refinement bagged trees; quite popular (Random Forests tries improve bagging “de-correlating” trees. tree expectation, average reduce variability.)subset predictors makes Random Forests much faster search predictorscreates diverse set trees can built. Note bootstrapping samples predictor variables, add another level randomness can average decrease variability.Random Forests quite accurategenerally, models overfit data CV needed. However, CV can used fit tuning parameters (\\(m\\), node size, max number nodes, etc.).Notes Random Forests:Bagging alone uses full set predictors determine every tree (observations bootstrapped). Random Forests use subset predictors.Note predict particular observation, start top, walk tree, get prediction. average (majority vote) predictions get one prediction observation hand.Bagging special case Random Forest \\(m=p\\).generally, models overfit data CV needed. However, CV can used fit tuning parameters (\\(m\\), node size, max number nodes, etc.).“Random forests overfit. can run many trees want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","code":""},{"path":"class.html","id":"how-to-choose-parameters","chapter":"8 Classification","heading":"How to choose parameters?","text":"\\(\\#\\) trees\nBuild trees error longer decreases\\(m\\)\nTry recommended defaults, half , twice - pick best (use CV avoid overfitting).","code":""},{"path":"class.html","id":"variable-importance","chapter":"8 Classification","heading":"Variable Importance","text":"learners bad many noisy variables response bound correlate . can measure contribution additional variable model much model accuracy decreased given variable excluded model.importance = decrease node impurity resulting splits variable, averaged trees(“impurity” defined RSS regression trees deviance classification trees).Variable importance measured two different metrics (R help importance):(permutation) accuracy: tree, prediction error --bag portion data recorded (error rate classification, MSE regression).Permute \\(j^{th}\\) variable recalculate prediction error. difference two averaged trees (\\(j^{th}\\) variable) give importance \\(j^{th}\\) variable.purity: decrease (increase, depending plot) node purity: root sum squares (RSS) [deviance/gini classification trees]. , amount total decrease RSS splitting variable, averaged trees.number variables large, forests can run variables, run using important variables first run.","code":""},{"path":"class.html","id":"r-rf-example","chapter":"8 Classification","heading":"8.5.2 R RF Example","text":"(“impurity” defined RSS regression trees deviance classification trees).method= 'ranger' zillion times faster method = 'randomForest' method = 'rf', work.mtry number trees?Get final model:Predict test data:","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins <- penguins %>%\n  drop_na()\n\n# partition\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\n# recipe\npenguin_rf_recipe <-\n  recipe(body_mass_g ~ . ,\n         data = penguin_train) %>%\n  step_unknown(sex, new_level = \"unknown\") %>%\n  step_mutate(year = as.factor(year)) \n\n#model\npenguin_rf <- rand_forest(mtry = tune(),\n                           trees = tune()) %>%\n  set_engine(\"ranger\", importance = \"permutation\") %>%\n  set_mode(\"regression\")\n\n# workflow\npenguin_rf_wflow <- workflow() %>%\n  add_model(penguin_rf) %>%\n  add_recipe(penguin_rf_recipe)\n\n# CV\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\npenguin_grid <- grid_regular(mtry(range = c(2,7)),\n                             trees(range = c(1,500)),\n                             levels = 5)\n\n# tune\npenguin_rf_tune <- \n  penguin_rf_wflow %>%\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\nselect_best(penguin_rf_tune, \"rmse\")## # A tibble: 1 × 3\n##    mtry trees .config              \n##   <int> <int> <chr>                \n## 1     2   375 Preprocessor1_Model16\npenguin_rf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  ggplot() + \n  geom_line(aes(x = trees, y = mean, color = as.factor(mtry)))\npenguin_rf_best <- finalize_model(\n  penguin_rf,\n  select_best(penguin_rf_tune, \"rmse\"))\n\npenguin_rf_best## Random Forest Model Specification (regression)\n## \n## Main Arguments:\n##   mtry = 2\n##   trees = 375\n## \n## Engine-Specific Arguments:\n##   importance = permutation\n## \n## Computational engine: ranger\npenguin_rf_final <-\n  workflow() %>%\n  add_model(penguin_rf_best) %>%\n  add_recipe(penguin_rf_recipe) %>%\n  fit(data = penguin_train)\n\npenguin_rf_final## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: rand_forest()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_unknown()\n## • step_mutate()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Ranger result\n## \n## Call:\n##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~2L,      x), num.trees = ~375L, importance = ~\"permutation\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n## \n## Type:                             Regression \n## Number of trees:                  375 \n## Sample size:                      249 \n## Number of independent variables:  7 \n## Mtry:                             2 \n## Target node size:                 5 \n## Variable importance mode:         permutation \n## Splitrule:                        variance \n## OOB prediction error (MSE):       84149.09 \n## R squared (OOB):                  0.8634591\npenguin_rf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  ggplot() +\n  geom_point(aes(x = body_mass_g, y = .pred)) + \n  geom_abline(intercept = 0, slope = 1)"},{"path":"class.html","id":"variable-importance-1","chapter":"8 Classification","heading":"Variable Importance","text":"order get variable importance, need specify importance within model forest.","code":"\nlibrary(vip)\n\npenguin_rf_final %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")"},{"path":"class.html","id":"model-choices","chapter":"8 Classification","heading":"8.6 Model Choices","text":"soooooo many choices ’ve made along way. following list make realize truth respect given model. Every choice () lead different model.","code":""},{"path":"class.html","id":"support-vector-machines","chapter":"8 Classification","heading":"8.7 Support Vector Machines","text":"Support Vector Machines one algorithm classification. ’ll see, excellent properties, one important aspect note use numeric predictor variables binary response variables (classify two groups).Vladimir Vapnik (b. 1936) created SVMs late 1990s. History: actually work PhD early 60s Soviet Union. Someone Bell Labs asked visit, ended immigrating US. one actually thought SVMs work, eventually (1995 - took 30 years idea implementation) bet dinner classifying handwriting via SVM (using simple kernel) versus neural networks rest history.basic idea SVMs figure way create really complicated decision boundaries. want put straight line widest possible street (draw street gutters 4 points, two positive two negative). decision rule dot product new sample vector \\({\\bf w}\\) perpendicular median “street.”Note: standard formulation SVM requires computer find dot products observations. order , explanatory variables must numeric. order dot products meaningful, data must scale.","code":""},{"path":"class.html","id":"linsvm","chapter":"8 Classification","heading":"8.7.0.1 Linear Separator","text":"Recall ideas kNN trees:today’s decision boundary going based hyperplane separates values “best” way. Certainly, data linearly separable, infinitely many hyperplanes partition data perfectly. SVM, idea find “street” separates positive negative samples give widest margin.\nFigure 8.15: correct project observations can often produce perfect one dimensional (.e., linear) classifier. http://www.rmki.kfki.hu/~banmi/elte/Bishop - Pattern Recognition Machine Learning.pdf\n","code":""},{"path":"class.html","id":"aside-what-is-a-dot-product","chapter":"8 Classification","heading":"Aside: what is a dot product?","text":"Let \\({\\bf x} = (x_1, x_2, \\ldots, x_p)^t\\) \\({\\bf y} = (y_1, y_2, \\ldots, y_p)^t\\) two vectors live \\(R^p\\). dot product defined :\n\\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} = {\\bf x}^t {\\bf y} = \\sum_{=1}^p x_i y_i\n\\end{align}\\]\nFigure 8.16: w known, projection new observation onto w lead linear partition space.\ncan street used get decision rule? known \\({\\bf w}\\) perpendicular street. don’t yet know \\({\\bf w}\\) \\(b\\).“width” street vector perpendicular street (median). don’t know width yet, know know can use perpendicular vector (\\({\\bf w}\\)) figure classify points. Project unknown point (\\({\\bf u}\\)) onto \\({\\bf w}\\) see side street unknown value lands. , projection large enough, classify point positive: \\[{\\bf w} \\cdot {\\bf u} \\geq c?\\][Keep mind \\({\\bf u} \\cdot {\\bf w} = ||{\\bf w}|| \\times\\)(length shadow). , projection length shadow \\({\\bf w}\\) unit vector. aren’t going constrain \\({\\bf w}\\) unit vector (though !). regardless, \\({\\bf u} \\cdot {\\bf w}\\) still gives ability classify proportional length shadow.]Decision rule:\n\\({\\bf w} \\cdot {\\bf u} + b \\geq 0\\) label new sample “positive”\\({\\bf w}\\) created way perpendicular median street. unknown (\\({\\bf u}\\)) vector projected onto \\({\\bf w}\\) see left right side street.don’t know values decision rule! need constraints. Assuming data linearly separable, initial step find \\({\\bf w}\\) \\(b\\), positive samples (\\(x_+\\)) negative samples (\\(x_-\\)) force:\n\\[\\begin{align}\n{\\bf w} \\cdot {\\bf x}_+ + b &\\geq 1 \\tag{8.1}\\\\\n{\\bf w} \\cdot {\\bf x}_- + b &\\leq -1 \\tag{8.2}\n\\end{align}\\]mathematical convenience (don’t 2 equations hanging around), introduce \\(y_i\\) \n\\[\\begin{align}\ny_i &= 1 \\mbox{ positive samples}\\\\\ny_i &= -1 \\mbox{ negative samples}\n\\end{align}\\]simplifies criteria finding \\({\\bf w}\\) \\(b\\) :\n\\[ y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1\\]\n(Multiplying -1 equation ((8.2) switches signs, equation ((8.1)) ((8.2) end types points.), working toward solving \\({\\bf w}\\) \\(b\\), add additional constraint points gutter (margin lines):\\(x_i\\) gutter (definition):\n\\[y_i({\\bf w} \\cdot {\\bf x}_i + b) - 1 = 0\\]Now consider two particular positive negative values live margin (gutter). difference almost width street (want find street wide possible), wrong angle (see street picture ). Remember, goal find street separating pluses minuses wide possible. unit vector, dot \\((x_+ - x_-)\\) get width street!\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||}\n\\end{align}\\]\ndoesn’t us much good yet.\nGoal: Try find wide street possible.\nremember, gutter points constrained: turns \\(x_+ \\cdot {\\bf w} = 1 - b\\) \\(x_- \\cdot {\\bf w} = -1 - b\\). Therefore:\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||} = \\frac{(1-b) - (-1-b)}{|| {\\bf w} ||} = \\frac{2}{||w||}\n\\end{align}\\]\nminimize \\((1/2)*||w||^2\\)\n(make mathematically easier). pieces making decision rules optimization problem. , minimize quantity subject constraints given problem.","code":""},{"path":"class.html","id":"lagrange-multipliers","chapter":"8 Classification","heading":"Lagrange multipliers","text":"Recall, Lagrange multipliers, first part optimization, second part constraint. point Lagrange multipliers put together constraint optimization one equation don’t worry constraints longer.\\(L\\) consists two parts. first thing minimize. second set constraints (, summation constraints). constraint multiplier \\(\\alpha_i\\), non-zero \\(\\alpha_i\\) ones connected values gutter.\\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1]\n\\end{align}\\]Find derivatives, set equal zero. Note can differentiate respect vector component wise, ’ll skip notation, one element time.\\[\\begin{align}\n\\frac{\\partial L}{\\partial {\\bf w}} &= {\\bf w} - \\sum \\alpha_i  y_i  {\\bf x}_i = 0 \\rightarrow {\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i \\\\\n\\frac{\\partial L}{\\partial b} &= -\\sum \\alpha_i y_i = 0\\\\\n\\end{align}\\]turns \\({\\bf w}\\) linear sum data vectors, either (turns \\(\\), \\(\\alpha_i=0\\)):\n\\[{\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i\\]Use value \\({\\bf w}\\) plug back \\(L\\) minimize\\[\\begin{align}\nL &= \\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum_i \\alpha_i [ y_i ((\\sum_j \\alpha_j y_j {\\bf x}_j) \\cdot{\\bf x}_i + b ) - 1]\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum \\alpha_i y_i b + \\sum \\alpha_i\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - 0 + \\sum \\alpha_i\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\n\\end{align}\\]Find minimum expression:\n\\[L = \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\\]\noptimization depends dot product pairs samples.\ndecision rule also depends dot product new observation original samples. [Note, points margin / gutter can used solve \\(b\\): \\(b =y_i - {\\bf w} \\cdot {\\bf x}_i\\), \\(y_i = 1/y_i\\).]Decision Rule, call positive :\n\\[\\sum \\alpha_i y_i {\\bf x}_i \\cdot {\\bf u} + b \\geq 0\\]Note convex space (can proved), can’t get stuck local maximum.","code":""},{"path":"class.html","id":"notlinsvm","chapter":"8 Classification","heading":"8.7.1 Not Linearly Separable","text":"","code":""},{"path":"class.html","id":"transformations","chapter":"8 Classification","heading":"Transformations","text":"\ndata can transformed new space data linearly separable.\ncan transform data different space (linearly separable), can transform data new space thing! , consider function \\(\\phi\\) new space consists vectors \\(\\phi({\\bf x})\\).Consider case circle plane. class boundary segment space considering points within circle belong one class, points outside circle another one. space linearly separable, mapping third dimension make separable. Two great videos: https://www.youtube.com/watch?v=3liCbRZPrZA https://www.youtube.com/watch?v=9NrALgHFwTo .Within transformed space, minimization procedure amount minimizing following:want minimum expression:\n\\[\\begin{align}\nL &= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j \\phi({\\bf x}_i) \\cdot  \\phi({\\bf x}_j)\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j K({\\bf x}_i, {\\bf x}_j)\n\\end{align}\\]Decision Rule, call positive :\n\\[\\begin{align}\n\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b &\\geq& 0\\\\\n\\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b &\\geq& 0\n\\end{align}\\]","code":""},{"path":"class.html","id":"kernel-examples","chapter":"8 Classification","heading":"Kernel Examples:","text":"Kernel 1Consider following transformation, \\(\\phi: R^2 \\rightarrow R^3\\):\n\\[\\begin{align}\n\\phi({\\bf x}) &= (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\\\\\nK({\\bf x}, {\\bf y}) &= \\phi({\\bf x}) \\cdot \\phi({\\bf y}) = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2\\\\\n&= (x_1y_1 + x_2y_2)^2\\\\\nK({\\bf x}, {\\bf y}) &= ({\\bf x} \\cdot {\\bf y})^2\n\\end{align}\\]\nsay, long know dot product original data, can recover dot product transformed space using quadratic kernel.Kernel 2\nWriting polynomial kernel (\\(d=2\\)), can find exact \\(\\phi\\) function. Consider following polynomial kernel \\(d=2\\).\n\\[K({\\bf x}, {\\bf y}) = ({\\bf x} \\cdot {\\bf y} + c)^2\\]\nwriting dot product considering square components separately, get\n\\[\\begin{align}\n({\\bf x} \\cdot {\\bf y} + c)^2 &= (c + \\sum_{=1}^p x_i y_i)^2\\\\\n&= c^2 + \\sum_{=1}^p x_i^2 y_i^2 + \\sum_{=1}^{p-1} \\sum_{j={+1}}^{p} 2x_i y_i x_j y_j + \\sum_{=1}^p 2 cx_i y_i\n\\end{align}\\]\npulling sum apart components \\({\\bf x}\\) \\({\\bf y}\\) vectors separately, find \n\\[\\begin{align}\n\\phi({\\bf x}) = (c, x_1^2, \\ldots, x_p^2, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_1x_p, \\sqrt{2}x_2x_3, \\ldots, \\sqrt{2}x_{p-1}x_p, \\sqrt{2c}x_1, \\ldots, \\sqrt{2c}x_p)\n\\end{align}\\]Kernel 3\nUsing radial kernel (see ) possible map observations infinite dimensional space yet still consider kernel associated dot product original data. Consider following example \\(x\\) one dimension mapped infinite dimensions.\\[\\begin{align}\n\\phi_{RBF}(x) &= e^{-\\gamma x} \\bigg(1, \\sqrt{\\frac{2\\gamma}{1!}} x, \\sqrt{\\frac{(2\\gamma)^2}{2!}} x^2, \\sqrt{\\frac{(2\\gamma)^3}{3!}} x^3, \\ldots \\bigg)^t\\\\\nK_{RBF} (x,y) &= \\exp( -\\gamma ||x-y||^2)\n\\end{align}\\]\ncross validation used find tuning value \\(\\gamma\\) well penalty parameter \\(C\\).Consider following example http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex8/ex8.html.","code":""},{"path":"class.html","id":"what-if-the-boundary-is-wiggly","chapter":"8 Classification","heading":"What if the boundary is wiggly?","text":"take home message wiggly boundary really best, value \\(\\gamma\\) high represent high model complexity.\nFigure 8.17: Extremely complicated decision boundary\n","code":""},{"path":"class.html","id":"what-if-the-boundary-isnt-wiggly","chapter":"8 Classification","heading":"What if the boundary isn’t wiggly?","text":"boundary low complexity, best value \\(\\gamma\\) probably much lower.\nFigure 8.18: Simple decision boundary\n\nFigure 8.19: Simple decision boundary – reasonable gamma\n\nFigure 8.20: Simple decision boundary – gamma big\n","code":""},{"path":"class.html","id":"kernels","chapter":"8 Classification","heading":"8.7.2 What is a Kernel?","text":"kernel: kernel function function obeys certain mathematical properties. won’t go properties right now, now think kernel function function dot product two vectors, (e.g., measure “similarity” two vectors). \\(K\\) function two vectors \\({\\bf x}\\) \\({\\bf y}\\), kernel function \\(K\\) dot product \\(\\phi()\\) applied vectors. know \\(\\phi()\\) exists \\(K\\) symmetric \\(K_{ij} = K({\\bf x}_i, {\\bf x}_j)\\), matrix \\({\\bf K} = [K_{ij}]\\) positive definite.helpful website kernels: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\\[\\begin{align}\nK({\\bf x},{\\bf y}) = \\phi({\\bf x}) \\cdot \\phi({\\bf y})\n\\end{align}\\]","code":""},{"path":"class.html","id":"examples-of-kernels","chapter":"8 Classification","heading":"Examples of kernels:","text":"linear\n\\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\]\nNote, tuning parameter penalty/cost parameter \\(C\\)).linear\n\\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\]\nNote, tuning parameter penalty/cost parameter \\(C\\)).polynomial\n\\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma > 0\\]\nNote, \\(\\gamma, r, d\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).polynomial\n\\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma > 0\\]\nNote, \\(\\gamma, r, d\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).RBF\nradial basis function also called Gaussian kernel similarity Gaussian distribution (aka normal distribution). RBF maps infinite dimensional space, can easily fit training data. Care must taken estimate \\(\\gamma\\).\n\\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} -  {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\]\nNote, \\(\\gamma\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).RBF\nradial basis function also called Gaussian kernel similarity Gaussian distribution (aka normal distribution). RBF maps infinite dimensional space, can easily fit training data. Care must taken estimate \\(\\gamma\\).\n\\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} -  {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\]\nNote, \\(\\gamma\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).sigmoid\nsigmoid kernel valid kernel method values \\(\\gamma\\) \\(r\\) [means certain parameter values, \\(\\phi()\\) function may exist].\n\\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\]\nNote, \\(\\gamma, r\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)). One benefit sigmoid kernel equivalence two-layer perceptron neural network.sigmoid\nsigmoid kernel valid kernel method values \\(\\gamma\\) \\(r\\) [means certain parameter values, \\(\\phi()\\) function may exist].\n\\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\]\nNote, \\(\\gamma, r\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)). One benefit sigmoid kernel equivalence two-layer perceptron neural network.","code":""},{"path":"class.html","id":"soft-margins","chapter":"8 Classification","heading":"Soft Margins","text":"data aren’t linearly separable? optimization problem can changed allow points side margin. optimization problem slightly complicated, basically idea:\n\\[y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i  \\ \\ \\ \\ \\ \\ 1 \\leq \\leq n, \\ \\  \\xi_i \\geq 0\\]\nFigure 8.21: Note now problem set points allowed cross boundary. Slack variables (xi_i) allow every point classified correctly slack. Note xi_i=0 point actually calculated correctly.\noptimization problem gets slightly complicated two ways, first, minimization piece includes penalty parameter, \\(C\\) (much misclassification allowed - value \\(C\\) set/tuned optimized), second, constraint now allows points misclassified.Minimize (\\({\\bf w}\\), \\(\\xi_i\\), \\(b\\)):\n\\[\\frac{1}{2} ||{\\bf w}||^2 + C \\sum_{=1}^n \\xi_i\\]\nSubject :\n\\[y_i ({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i \\ \\ \\ \\ \\xi_i \\geq 0\\]leads following Lagrangian equation:\n\\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 + C \\sum_{=1}^n \\xi_i - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1 + \\xi_i] - \\sum_{=1}^n \\beta_i \\xi_i \\ \\ \\ \\ \\alpha_i, \\beta_i \\geq 0\n\\end{align}\\]\n\\(C\\) now tuning parameter needs set user cross validation.\n","code":""},{"path":"class.html","id":"how-does-c-relate-to-margins","chapter":"8 Classification","heading":"How does \\(C\\) relate to margins?","text":"Notice minimization now many variables (\\(C\\) set/tuned - optimized). allowing misclassification \\(C=0\\), implies \\(\\xi_i\\) can large possible. means algorithm choose widest possible street. widest possible street one hits two extreme data points (“support vectors” now ones edge, ones near separating hyperplane). \\(C\\) small allows constraints (points crossing line) ignored.\\[C=0 \\rightarrow \\mbox{ can lead large training error}\\]\\(C\\) quite large, algorithm try hard classify exactly perfectly. , want \\(\\xi_i\\) close zero possible. projecting high dimensions, can always perfectly classify, large \\(C\\) tend overfit training data give small margin.\n\\[C>>> \\rightarrow \\mbox{ can lead classification rule generalize test data}\\]\nFigure 8.22: first figure, low C value gives large margin. right, high C value gives small margin. classifier better? Well, depends actual data (test, population, etc.) look like! second row large C classifier better; third row, small C classifier better. photo credit: http://stats.stackexchange.com/questions/31066/---influence--c--svms--linear-kernel\n","code":""},{"path":"class.html","id":"support-vector-machine-algorithm","chapter":"8 Classification","heading":"8.7.3 Support Vector Machine algorithm","text":"Algorithm: Support Vector MachineUsing cross validation, find values \\(C, \\gamma, d, r\\), etc. (kernel function!)Using Lagrange multipliers (read: computer), solve \\(\\alpha_i\\) \\(b\\).Classify unknown observation (\\({\\bf u}\\)) “positive” :\n\\[\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b  = \\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b \\geq 0\\]Shortcomings Support Vector Machines:Shortcomings Support Vector Machines:Can classify binary categories (response variable).Can classify binary categories (response variable).predictor variables must numeric.\ngreat differential range allow variables large range dominate predictions. Either linearly scale attribute range [ e.g., (-1, +1) (0,1)] divide standard deviation.\nCategorical variables can used formatted binary factor variables.\nWhatever done training data must also done test data!\npredictor variables must numeric.great differential range allow variables large range dominate predictions. Either linearly scale attribute range [ e.g., (-1, +1) (0,1)] divide standard deviation.Categorical variables can used formatted binary factor variables.Whatever done training data must also done test data!Another problem kernel function .\nprimitive data (e.g., 2d data points), good kernels easy come .\nharder data (e.g., MRI scans), finding sensible kernel function may much harder.\nAnother problem kernel function .primitive data (e.g., 2d data points), good kernels easy come .harder data (e.g., MRI scans), finding sensible kernel function may much harder.really large data, doesn’t perform well large amount required training timeWith really large data, doesn’t perform well large amount required training timeIt also doesn’t perform well data set lot noise .e., target classes overlappingIt also doesn’t perform well data set lot noise .e., target classes overlappingSVM doesn’t directly provide probability estimates, calculated using expensive five-fold cross-validation.SVM doesn’t directly provide probability estimates, calculated using expensive five-fold cross-validation.Strengths Support Vector Machines:Strengths Support Vector Machines:Can always fit linear separating hyper plane high enough dimensional space.Can always fit linear separating hyper plane high enough dimensional space.kernel trick makes possible know transformation functions, \\(\\phi\\).kernel trick makes possible know transformation functions, \\(\\phi\\).optimization convex function, numerical process finding solutions extremely efficient.optimization convex function, numerical process finding solutions extremely efficient.works really well clear margin separationIt works really well clear margin separationIt effective high dimensional spaces.effective high dimensional spaces.effective cases number dimensions greater number samples.effective cases number dimensions greater number samples.uses subset training points decision function (called support vectors), also memory efficient.uses subset training points decision function (called support vectors), also memory efficient.","code":""},{"path":"class.html","id":"classifying-more-than-one-group","chapter":"8 Classification","heading":"8.7.4 Classifying more than one group","text":"two classes, problem needs reduced binary classification problem. Consider groups associated Red, Green, Blue. order figure points get classified Red, two different methods can applied.One vs \ncategory can compared rest groups. create \\(K\\) different classifiers (\\(K=\\) number classes response variable can take ). test value classified according classifier, group assignment given group giving highest value \\({\\bf w}_K \\cdot {\\bf u} + b\\), projection represent classification farthest group center. end, \\(K\\) classifiers.One vs One\nAlternatively, group can compared group (e.g., Red vs. Green, Red vs. Blue, Green vs. Blue). Class membership determine group unknown point often classified. end, \\(K(K-1)/2\\) classifiers.","code":""},{"path":"class.html","id":"r-svm-example","chapter":"8 Classification","heading":"8.7.5 R SVM Example","text":"’ll go back penguin data. first pass, let’s use SVM distinguish male female penguins. removed missing data dataset make predictions easier.","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\npenguins <- penguins %>%\n  drop_na()\n\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)"},{"path":"class.html","id":"linear-svm-no-tuning","chapter":"8 Classification","heading":"Linear SVM (no tuning)","text":"","code":"\n# recipe\npenguin_svm_recipe <-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)## # A tibble: 5 × 4\n##   variable          type    role      source  \n##   <chr>             <chr>   <chr>     <chr>   \n## 1 bill_length_mm    numeric predictor original\n## 2 bill_depth_mm     numeric predictor original\n## 3 flipper_length_mm numeric predictor original\n## 4 body_mass_g       numeric predictor original\n## 5 sex               nominal outcome   original\n# model\npenguin_svm_lin <- svm_linear() %>%\n  set_engine(\"LiblineaR\") %>%\n  set_mode(\"classification\")\n\npenguin_svm_lin## Linear Support Vector Machine Specification (classification)\n## \n## Computational engine: LiblineaR\n# workflow\npenguin_svm_lin_wflow <- workflow() %>%\n  add_model(penguin_svm_lin) %>%\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_lin_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_linear()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Support Vector Machine Specification (classification)\n## \n## Computational engine: LiblineaR\n# fit\npenguin_svm_lin_fit <- \n  penguin_svm_lin_wflow %>%\n  fit(data = penguin_train)\n\npenguin_svm_lin_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_linear()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## $TypeDetail\n## [1] \"L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)\"\n## \n## $Type\n## [1] 1\n## \n## $W\n##      bill_length_mm bill_depth_mm flipper_length_mm body_mass_g       Bias\n## [1,]       0.248908      1.080195        -0.2256375    1.328448 0.06992734\n## \n## $Bias\n## [1] 1\n## \n## $ClassNames\n## [1] male   female\n## Levels: female male\n## \n## $NbClass\n## [1] 2\n## \n## attr(,\"class\")\n## [1] \"LiblineaR\""},{"path":"class.html","id":"rbf-svm-with-tuning","chapter":"8 Classification","heading":"RBF SVM (with tuning)","text":"","code":"\n# recipe\npenguin_svm_recipe <-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)## # A tibble: 5 × 4\n##   variable          type    role      source  \n##   <chr>             <chr>   <chr>     <chr>   \n## 1 bill_length_mm    numeric predictor original\n## 2 bill_depth_mm     numeric predictor original\n## 3 flipper_length_mm numeric predictor original\n## 4 body_mass_g       numeric predictor original\n## 5 sex               nominal outcome   original\n# model\npenguin_svm_rbf <- svm_rbf(cost = tune(),\n                           rbf_sigma = tune()) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\npenguin_svm_rbf## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = tune()\n##   rbf_sigma = tune()\n## \n## Computational engine: kernlab\n# workflow\npenguin_svm_rbf_wflow <- workflow() %>%\n  add_model(penguin_svm_rbf) %>%\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_rbf_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_rbf()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = tune()\n##   rbf_sigma = tune()\n## \n## Computational engine: kernlab\n# CV\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\n# the tuned parameters also have default values you can use\npenguin_grid <- grid_regular(cost(),\n                             rbf_sigma(),\n                             levels = 8)\n\npenguin_grid## # A tibble: 64 × 2\n##         cost     rbf_sigma\n##        <dbl>         <dbl>\n##  1  0.000977 0.0000000001 \n##  2  0.00431  0.0000000001 \n##  3  0.0190   0.0000000001 \n##  4  0.0841   0.0000000001 \n##  5  0.371    0.0000000001 \n##  6  1.64     0.0000000001 \n##  7  7.25     0.0000000001 \n##  8 32        0.0000000001 \n##  9  0.000977 0.00000000268\n## 10  0.00431  0.00000000268\n## # … with 54 more rows\n# tune\n# this takes a few minutes\npenguin_svm_rbf_tune <- \n  penguin_svm_rbf_wflow %>%\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\npenguin_svm_rbf_tune ## # Tuning results\n## # 4-fold cross-validation \n## # A tibble: 4 × 4\n##   splits           id    .metrics           .notes          \n##   <list>           <chr> <list>             <list>          \n## 1 <split [186/63]> Fold1 <tibble [128 × 6]> <tibble [0 × 1]>\n## 2 <split [187/62]> Fold2 <tibble [128 × 6]> <tibble [0 × 1]>\n## 3 <split [187/62]> Fold3 <tibble [128 × 6]> <tibble [0 × 1]>\n## 4 <split [187/62]> Fold4 <tibble [128 × 6]> <tibble [0 × 1]>"},{"path":"class.html","id":"what-is-best","chapter":"8 Classification","heading":"What is best?","text":"","code":"\npenguin_svm_rbf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  ggplot() + \n  geom_line(aes(color = as.factor(cost), y = mean, x = rbf_sigma)) +\n  labs(color = \"Cost\")\npenguin_svm_rbf_tune %>%\n  autoplot()"},{"path":"class.html","id":"rbf-svm-final-model","chapter":"8 Classification","heading":"RBF SVM final model","text":"","code":"\npenguin_svm_rbf_best <- finalize_model(\n  penguin_svm_rbf,\n  select_best(penguin_svm_rbf_tune, \"accuracy\"))\n\npenguin_svm_rbf_best## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = 0.371498572284237\n##   rbf_sigma = 1\n## \n## Computational engine: kernlab\npenguin_svm_rbf_final <-\n  workflow() %>%\n  add_model(penguin_svm_rbf_best) %>%\n  add_recipe(penguin_svm_recipe) %>%\n  fit(data = penguin_train)"},{"path":"class.html","id":"test-predictions","chapter":"8 Classification","heading":"Test predictions","text":"","code":"\nlibrary(yardstick)\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  select(sex, .pred_class) %>%\n  table()##         .pred_class\n## sex      female male\n##   female     39    5\n##   male        4   36\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  conf_mat(sex, .pred_class)##           Truth\n## Prediction female male\n##     female     39    4\n##     male        5   36\n# https://yardstick.tidymodels.org/articles/metric-types.html\nclass_metrics <- yardstick::metric_set(accuracy,sens,\n                                       spec, f_meas)\n\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  class_metrics(truth = sex, estimate = .pred_class)## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.893\n## 2 sens     binary         0.886\n## 3 spec     binary         0.9  \n## 4 f_meas   binary         0.897"},{"path":"unsup.html","id":"unsup","chapter":"9 Unsupervised Methods","heading":"9 Unsupervised Methods","text":"classification models ’ve discussed supervised learning techniques. word supervised refers fact know response variable training observations. Next , ’ll discuss clustering unsupervised technique – none observations given response variable. example, might want cluster hundred melanoma patients based genetic data. looking patterns groups together, don’t preconceived idea patients belong group.also semi-supervised techniques applied data observations labeled . discuss semi-supervised methods class.Clustering creates groups observations via unsupervised methods. cover hierarchical clustering, k-means, k-medoids. cluster two main reasons:Summary: describe data observations’ similarities .Discovery: find new ways groups observations similar.Classification – SUPERVISED! creates predictions (prediction models) unknown future observations via supervised methods. classification group membership (.e., response variable) known training data. covered k-NN, CART, bagging, Random Forests, SVMs.","code":""},{"path":"unsup.html","id":"latent-dirichlet-allocation","chapter":"9 Unsupervised Methods","heading":"9.1 Latent Dirichlet Allocation","text":"LDA views document mixture small (predefined) number topics describe set documents. word (typically common extremely rare words removed modeling) represents occurrence generated one document’s topics (document modeled mixture topics). LDA, model learns composition topic topic mixture document.Wikipedia:natural language processing, latent Dirichlet allocation (LDA) generative statistical model allows sets observations explained unobserved groups explain parts data similar. example, observations words collected documents, posits document mixture small number topics word’s creation attributable one document’s topics.algorithm finding words represent \\(K\\) topics (\\(K\\) chosen advance). [Explained detail http://blog.echen./2011/08/22/introduction--latent-dirichlet-allocation/.]document training data, randomly assign word one \\(K\\) topics.\\(K\\) topics now set words associated (albeit, set words meaningless). improve set words associated topic. word \\(w\\) document \\(d\\):\ntopic \\(t\\), compute two things:\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)\np(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).\n\nReassign \\(w\\) new topic, choose topic \\(t\\) probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according generative model, essentially probability topic \\(t\\) generated word \\(w\\), makes sense resample current word’s topic probability).\nwords, step, ’re assuming topic assignments except current word question correct, updating assignment current word using model documents generated.\ntopic \\(t\\), compute two things:\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)\np(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)p(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).Reassign \\(w\\) new topic, choose topic \\(t\\) probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according generative model, essentially probability topic \\(t\\) generated word \\(w\\), makes sense resample current word’s topic probability).words, step, ’re assuming topic assignments except current word question correct, updating assignment current word using model documents generated.repeating previous steps large number times, list words topic reach steady state. resulting assignments estimate topic mixtures document (counting proportion words assigned topic within document) words associated topic (counting proportion words assigned topic overall).https://ziqixiong.shinyapps.io/TopicModeling/","code":""},{"path":"unsup.html","id":"dissimilarities","chapter":"9 Unsupervised Methods","heading":"9.2 Dissimilarities","text":"Many, though , clustering algorithms based distances objects clustered. Mathematical properties distance function following. Consider two vectors \\({\\bf x}\\) \\({\\bf y}\\) (\\({\\bf x}, {\\bf y} \\\\mathbb{R}^p\\)), distance : \\(d({\\bf x}, {\\bf y})\\).\\(d({\\bf x}, {\\bf y}) \\geq 0\\)\\(d({\\bf x}, {\\bf y}) = d({\\bf y}, {\\bf x})\\)\\(d({\\bf x}, {\\bf y}) = 0\\) iff \\({\\bf x} = {\\bf y}\\)\\(d({\\bf x}, {\\bf y}) \\leq d({\\bf x}, {\\bf z}) + d({\\bf z}, {\\bf y})\\) vectors \\({\\bf z}\\).Triangle InequalityThe key proving triangle inequality distances relies Cauchy-Schwarz inequality.\n\\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} &= || {\\bf x} ||  ||{\\bf y}|| \\cos(\\theta) \\\\\n|{\\bf x} \\cdot {\\bf y}| &\\leq || {\\bf x} ||  ||{\\bf y}|| \n\\end{align}\\]","code":""},{"path":"unsup.html","id":"euclidean-distance","chapter":"9 Unsupervised Methods","heading":"Euclidean Distance","text":"\\[d_E({\\bf x}, {\\bf y}) = \\sqrt{\\sum_{=1}^p (x_i - y_i)^2}\\]Distance properties check .Cauchy-Schwarz:\n\\[\\begin{align}\n\\sum_{=1}^p(x_i - y_i)^2 = \\sum_{=1}^p ( (x_i - z_i) + (z_i - y_i))^2 &\\leq \\Bigg( \\sqrt{\\sum_{=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{=1}^p(z_i - y_i)^2} \\Bigg)^2\\\\\n\\sqrt{\\sum_{=1}^p(x_i - y_i)^2} &\\leq \\sqrt{\\sum_{=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{=1}^p(z_i - y_i)^2}\\\\\nd_E({\\bf x}, {\\bf y}) &\\leq d_E({\\bf x}, {\\bf z}) + d_E({\\bf z}, {\\bf y})\n\\end{align}\\]Shortcomings:\\(d_E\\) scale invariant.\\(d_E\\) measures magnitude differences, pattern differences.\\(d_E\\) sensitive outliers.Strengths:Directly measures commonly considered “distance.”","code":""},{"path":"unsup.html","id":"pearson-correlation-distance","chapter":"9 Unsupervised Methods","heading":"Pearson Correlation Distance","text":"\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n \\mbox{ } &= 1 - |r_P ({\\bf x}, {\\bf y})|\\\\\n \\mbox{ }   &= 1 - (r_P ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]Notice Euclidean distance Pearson correlation distance similar original observations scaled. Assume sample mean \\({\\bf x}\\) (, \\(\\frac{1}{p} \\sum x_i = \\overline{x} = 0\\)) zero sample standard deviation 1.\\[\\begin{align}\n r_P ({\\bf x}, {\\bf y}) &=  \\frac{\\sum x_i y_i - p \\ \\overline{x} \\ \\overline{y}}{(p-1)s_x s_y}\\\\\n &=  \\frac{1}{(p-1)} \\sum x_i y_i\\\\\n & \\ \\ & \\\\\n d_E({\\bf x}, {\\bf y}) &= \\sqrt{\\sum(x_i - y_i)^2}\\\\\n &=  \\sqrt{ \\sum x_i^2 + \\sum y_i^2 - 2 \\sum x_i y_i}\\\\\n d_E^2 &= 2[(p-1) - \\sum x_i y_i]\\\\\n &= 2(p-1)*[1 - r_P({\\bf x}, {\\bf y})]\n \\end{align}\\]","code":""},{"path":"unsup.html","id":"distance-properties-dont-hold-for-pearson-correlation","chapter":"9 Unsupervised Methods","heading":"Distance properties don’t hold for Pearson correlation","text":"\\({\\bf y}={\\bf x}\\)\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n&= 1 - r_P ({\\bf x}, {\\bf x})\\\\\n&= 1 - 1 = 0\n\\end{align}\\]\\({\\bf y}={\\bf x}\\)\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n&= 1 - r_P ({\\bf x}, {\\bf x})\\\\\n&= 1 - 1 = 0\n\\end{align}\\]\\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\)\n\\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\)\\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\)\n\\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\)\\(d_P({\\bf x}, {\\bf y}) + d_P({\\bf y}, {\\bf z}) < d_P({\\bf z}, {\\bf x})\\)\n\\(\\rightarrow\\leftarrow\\)Regular Pearson distanceAbsolute Pearson distanceUsing absolute distance doesn’t fix things.Shortcomings:\\(d_P\\) satisfy triangle inequality.\\(d_P\\) sensitive outliers.Strengths:Can measure distance variables different scales (although still sensitive extreme values).","code":"\nx1 <- c(1,2,3)\nx2 <- c(1, 4, 10)\nx3 <- c(9, 2, 2)\n\n# d(1,2)\n1 - cor(x1, x2)## [1] 0.01801949\n# d(1,3)\n1 - cor(x1, x3)## [1] 1.866025\n# d(2,3)\n1 - cor(x2, x3)## [1] 1.755929\n# d(1,3) > d(1,2) + d(2,3)\n1 - cor(x1, x2) + 1 - cor(x2, x3)## [1] 1.773948\n# d(1,2)\n1 - abs(cor(x1, x2))## [1] 0.01801949\n# d(1,3)\n1 - abs(cor(x1, x3))## [1] 0.1339746\n# d(2,3)\n1 - abs(cor(x2, x3))## [1] 0.2440711\n# d(2,3) > d(1,2) + d(1,3)\n1 - abs(cor(x1, x2)) + 1 - abs(cor(x1, x3))## [1] 0.1519941"},{"path":"unsup.html","id":"spearman-correlation-distance","chapter":"9 Unsupervised Methods","heading":"Spearman Correlation Distance","text":"Spearman correlation distance uses Spearman correlation instead Pearson correlation. Spearman correlation simply Pearson correlation applied ranks observations. ranking allows Spearman distance resistant outlying observations.\\[\\begin{align}\nd_S({\\bf x}, {\\bf y}) &= 1 - r_S ({\\bf x}, {\\bf y})\\\\\n \\mbox{ } &= 1 - |r_S ({\\bf x}, {\\bf y})|\\\\\n \\mbox{ }   &= 1 - (r_S ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]Shortcomings:\\(d_S\\) also satisfy triangle inequality.\\(d_S\\) loses information shape relationship.Strengths:resistant outlying values","code":""},{"path":"unsup.html","id":"cosine-distance","chapter":"9 Unsupervised Methods","heading":"Cosine Distance","text":"\\[\\begin{align}\nd_C({\\bf x}, {\\bf y}) &=  \\frac{{\\bf x} \\cdot {\\bf y}}{|| {\\bf x} ||  ||{\\bf y}||}\\\\\n&= \\frac{\\sum_{=1}^p x_i y_i}{\\sqrt{\\sum_{=1}^p x_i^2 \\sum_{=1}^p y_i^2}}\\\\\n&= 1 - r_P ({\\bf x}, {\\bf y})  \\ \\ \\ \\ \\mbox{} \\overline{\\bf x} = \\overline{\\bf y} = 0\n\\end{align}\\]Said differently,\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) = d_C({\\bf x} -  \\overline{\\bf x}, {\\bf y} -  \\overline{\\bf y})\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"haversine-distance","chapter":"9 Unsupervised Methods","heading":"Haversine Distance","text":"Haversine distance great-circle distance (.e., distance two points sphere) used measure distance two locations Earth. Let \\(R\\) radius Earth, (lat1,long1) (lat2, long2) two locations calculate distance.\\[d_{HV} = 2 R \\arcsin \\sqrt{\\sin^2 \\bigg( \\frac{lat2-lat1}{2} \\bigg) + \\cos(lat1) \\cos(lat2) \\sin^2 \\bigg(\\frac{long2 - long1}{2} \\bigg)} \\]Shortcomings:Earth perfect sphereDepending distance used, typically getting one point next done shortest distanceStrengths:Allows calculations, example, two cities.","code":""},{"path":"unsup.html","id":"hamming-distance","chapter":"9 Unsupervised Methods","heading":"Hamming Distance","text":"Hamming distance number coordinates across two vectors whose values differ. vectors binary, Hamming distance equivalent \\(L_1\\) norm difference. (Hamming distance satisfy properties distance metric.) methods, equivalently, calculate proportion coordinates differ.\\[\\begin{align}\nd_H({\\bf x}, {\\bf y}) = \\sum_{=1}^p (x_i \\ne y_i)\n\\end{align}\\]\nFigure 7.1: Hamming distance across two DNA strands 7.\nShortcomings:Can’t measure degree difference categorical variables.Strengths:distance metric.\nGives direct “distance” categorical variables.\nFigure 7.2: function dist R calculates distances given .\n","code":""},{"path":"unsup.html","id":"distance-on-strings","chapter":"9 Unsupervised Methods","heading":"Distance on strings","text":"Consider following infographic compares different methods computing distances strings.\nFigure 7.3: Comparison string distance metrics https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html.\n","code":""},{"path":"unsup.html","id":"hier","chapter":"9 Unsupervised Methods","heading":"9.3 Hierarchical Clustering","text":"Hierarchical Clustering set nested clusters organized tree. Note objects belong child cluster also belong parent cluster.Example: Consider following images / data (Laura Hoopes, personal communication; Molecular characterisation soft tissue tumours: gene expression study Nielsen et al., Lancet 2002). first represents microarray sample aging yeast. second set 41 samples soft-tissue tumors (columns) subset 5520 genes (rows) used characterize molecular signatures.Note: ordering variables (samples) affect clustering samples (variables). : can clustering variables / samples either sequentially parallel see trends relationships simultaneously. Clustering observations variables called biclustering.Algorithm: Agglomerative Hierarchical Clustering AlgorithmBegin \\(n\\) observations measure (Euclidean distance) \\({n \\choose 2} = n(n-1)/2\\) pairwise dissimilarities. Treat observation cluster.\\(= n, n - 1, \\ldots , 2\\):Examine pairwise inter-cluster dissimilarities among \\(\\) clusters identify pair clusters least dissimilar (, similar). Fuse two clusters. dissimilarity two clusters indicates height dendrogram fusion placed.Compute new pairwise inter-cluster dissimilarities among \\(- 1\\) remaining clusters.Agglomerative methods start object (e.g., gene) group. Groups merged objects together one group.Divisive methods start objects one group break groups sequentially objects individuals.Single Linkage algorithm defines distance groups closest pair individuals.Complete Linkage algorithm defines distance groups farthest pair individuals.Average Linkage algorithm defines distance groups average distances pairs individuals across groups.Toy Example Single Linkage Agglomerative Hierarchical ClusteringLink B!\n\\[\\begin{align}\nd_{(AB)C} &= \\min(d_{AC}, d_{BC}) = 0.5\\\\\nd_{(AB)D} &= \\min(d_{AD}, d_{BD}) = 0.9\\\\\nd_{(AB)E} &= \\min(d_{AE}, d_{}) = 0.8\\\\\n\\end{align}\\]Link D E!\n\\[\\begin{align}\nd_{(AB)C} &=  0.5\\\\\nd_{(AB)(DE)} &= \\min(d_{AD}, d_{BD}, d_{AE}, d_{}) = 0.8\\\\\nd_{(DE)C} &= \\min(d_{CD}, d_{CE}) = 0.4\\\\\n\\end{align}\\]Link C (DE)!\n\\[\\begin{align}\nd_{(AB)(CDE)} = d_{BC} = 0.5\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"r-hierarchical-example","chapter":"9 Unsupervised Methods","heading":"9.3.1 R hierarchical Example","text":", using penguins dataset, hierarchical clustering run. can look dendrogram, particular alignment categorical variables like species island.Notice using numerical variables. numerical variables scaled (subtract mean divide standard deviation).Full example adapted https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_h <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_hclust <- penguins_h %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\npenguin_hclust## \n## Call:\n## hclust(d = ., method = \"complete\")\n## \n## Cluster method   : complete \n## Distance         : euclidean \n## Number of objects: 342\npenguin_dend <- as.dendrogram(penguin_hclust)"},{"path":"unsup.html","id":"plotting","chapter":"9 Unsupervised Methods","heading":"Plotting","text":"basic dendrogram:Zooming cluster:Adding color branches nodes","code":"\nplot(penguin_hclust)\nplot(penguin_dend[[1]])\nlibrary(colorspace) # get nice colors\nspecies_col <- rev(rainbow_hcl(3))[as.numeric(penguins$species)]\nlibrary(dendextend)\n# order the branches as closely as possible to the original order\npenguin_dend <- rotate(penguin_dend, 1:342)\n# color branches based on the clusters\npenguin_dend <- color_branches(penguin_dend, k = 3)\n\n# Manually match the labels, as much as possible, to the real classification of the flowers:\nlabels_colors(penguin_dend) <-\n   rainbow_hcl(3)[sort_levels_values(\n      as.numeric(penguins$species)[order.dendrogram(penguin_dend)]\n   )]\n\n# We shall add the flower type to the labels:\nlabels(penguin_dend) <- paste(as.character(penguins$species)[order.dendrogram(penguin_dend)],\n                           \"(\",labels(penguin_dend),\")\", \n                           sep = \"\")\n# We hang the dendrogram a bit:\npenguin_dend <- hang.dendrogram(penguin_dend,hang_height=0.1)\n\n# reduce the size of the labels:\npenguin_dend <- assign_values_to_leaves_nodePar(penguin_dend, 0.5, \"lab.cex\")\npenguin_dend <- set(penguin_dend, \"labels_cex\", 0.5)\n# And plot:\npar(mar = c(3,3,3,7))\nplot(penguin_dend, \n     main = \"Clustered penguin data set\n     (the labels give the true penguin species)\", \n     horiz =  TRUE,  nodePar = list(cex = .007))\npenguin_species <- rev(levels(penguins$species))\nlegend(\"topleft\", legend = penguin_species, fill = rainbow_hcl(3))"},{"path":"unsup.html","id":"part","chapter":"9 Unsupervised Methods","heading":"9.4 Partitioning Clustering","text":"Partition Clustering division set data objects \\(K\\) non-overlapping subsets (clusters) observation falling exactly one cluster.contrast hierarchical clustering results given (!) number clusters, partitioning methods typically start given \\(k\\) value set distances. goal partition observations \\(k\\) groups objective function optimized. number possible partitions roughly \\(n^k / k!\\) (note: \\(100^{5} / 5! = 83\\) million). [exact number can computed using Sterling numbers.] instead looking partitions, step recursive algorithm.","code":""},{"path":"unsup.html","id":"k-means-clustering","chapter":"9 Unsupervised Methods","heading":"9.4.1 \\(k\\)-means Clustering","text":"fun applet!!https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\\(k\\)-means clustering unsupervised partitioning algorithm designed find partition observations following objective function minimized (find smallest within cluster sum squares):\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K 2 \\sum_{\\C_k} \\sum_{j=1}^p (x_{ij} - \\overline{x}_{kj})^2 \\Bigg\\}\\]described algorithm , reallocating observations can improve minimization criteria algorithm stopping changes observations lower objective function. algorithm leads local optimum, confirmation global minimum occurred. Often \\(k\\)- means algorithm run multiple times different random starts, partition leading lowest objective criteria chosen.Note following algorithm simply one \\(k\\)-means algorithm. algorithms include different way set starting values, different decision recalculate centers, ties, etc.\nFigure 8.5: Introduction Statistical Learning James, Witten, Hastie, Tibshirani.\nAlgorithm: \\(k\\)-Means ClusteringRandomly assign number, 1 \\(k\\), observations. serve initial cluster assignments observations.Iterate cluster assignments stop changing:\n\\(k\\) clusters, compute cluster centroid. \\(k^{th}\\) cluster centroid vector \\(p\\) feature means observations \\(k^{th}\\) cluster.\nAssign observation cluster whose centroid closest (closest defined using Euclidean distance).\n\\(k\\) clusters, compute cluster centroid. \\(k^{th}\\) cluster centroid vector \\(p\\) feature means observations \\(k^{th}\\) cluster.Assign observation cluster whose centroid closest (closest defined using Euclidean distance).Ties? something consistent: example, leave current cluster.\\(k\\)-means algorithm converge / (local) minimize objective function?point closer different center, moving lower objective function.Averages minimize squared differences, taking new average result lower objective function.point equidistant two clusters, point won’t move.algorithm must converge finite number steps finitely many points.","code":""},{"path":"unsup.html","id":"scaling-matters","chapter":"9 Unsupervised Methods","heading":"Scaling matters","text":"Note variables different scales, whichever variable larger magnitude dominate distances (therefore clustering). Unless explicitly want happen (odd), scale variables (subtract mean divide standard deviation) distance calculated Z-scores instead raw data.example , \\(k=2\\) k-means algorithm able see cigar-shaped structure (raw data) distances dominated x1 variable (differentiate clusters).strengthsNo hierarchical structure / points can move one cluster another.Can run range values \\(k\\).shortcomings\\(k\\) predefined run algorithm.\\(k\\)-means based Euclidean distance ().","code":"\nset.seed(47)\nnorm_clust <- data.frame(\n  x1 = rnorm(1000, 0, 15),\n  x2 = c(rnorm(500, 5, 1), rnorm(500, 0, 1)))\n\nnorm_clust %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on raw data\")\nnorm_clust %>%\n  mutate(across(everything(), scale)) %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on normalized / scaled data\")"},{"path":"unsup.html","id":"r-k-means-example","chapter":"9 Unsupervised Methods","heading":"9.4.2 R k-means Example","text":", using penguins dataset, \\(k\\)-means clustering run. try multiple different values \\(k\\) come partition penguins. can look clusterings scatterplots numerical variables. can also check see clustering aligns categorical variables like species island.Notice using numerical variables. numerical variables scaled (subtract mean divide standard deviation).Full example adapted https://www.tidymodels.org/learn/statistics/k-means/.function augment works observation level:function tidy() works per-cluster level:function glance() works per-model level:","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_km <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_kclust <- penguins_km %>%\n  kmeans(centers = 3)\n\npenguin_kclust## K-means clustering with 3 clusters of sizes 132, 123, 87\n## \n## Cluster means:\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n## 1     -1.0465260     0.4858415        -0.8899121  -0.7694891\n## 2      0.6562677    -1.0983711         1.1571696   1.0901639\n## 3      0.6600059     0.8157307        -0.2857869  -0.3737654\n## \n## Clustering vector:\n##   [1] 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 3 1\n##  [75] 3 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 3 1 1 1 3 1 3 1 1 1 1 1 1 1 3 1 3\n## [112] 1 3 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3\n## [297] 1 3 3 3 3 3 3 3 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3\n## [334] 3 3 3 3 3 3 3 3 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 122.1477 143.1502 112.9852\n##  (between_SS / total_SS =  72.3 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\npenguin_kclust %>% augment(penguins_km)## # A tibble: 342 × 5\n##    bill_length_mm[,1] bill_depth_mm[,… flipper_length_… body_mass_g[,1] .cluster\n##                 <dbl>            <dbl>            <dbl>           <dbl> <fct>   \n##  1             -0.883           0.784            -1.42          -0.563  1       \n##  2             -0.810           0.126            -1.06          -0.501  1       \n##  3             -0.663           0.430            -0.421         -1.19   1       \n##  4             -1.32            1.09             -0.563         -0.937  1       \n##  5             -0.847           1.75             -0.776         -0.688  1       \n##  6             -0.920           0.329            -1.42          -0.719  1       \n##  7             -0.865           1.24             -0.421          0.590  1       \n##  8             -1.80            0.480            -0.563         -0.906  1       \n##  9             -0.352           1.54             -0.776          0.0602 3       \n## 10             -1.12           -0.0259           -1.06          -1.12   1       \n## # … with 332 more rows\npenguin_kclust %>% tidy()## # A tibble: 3 × 7\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  size withinss\n##            <dbl>         <dbl>             <dbl>       <dbl> <int>    <dbl>\n## 1         -1.05          0.486            -0.890      -0.769   132     122.\n## 2          0.656        -1.10              1.16        1.09    123     143.\n## 3          0.660         0.816            -0.286      -0.374    87     113.\n## # … with 1 more variable: cluster <fct>\npenguin_kclust %>% glance()## # A tibble: 1 × 4\n##   totss tot.withinss betweenss  iter\n##   <dbl>        <dbl>     <dbl> <int>\n## 1  1364         378.      986.     2"},{"path":"unsup.html","id":"trying-various-values-of-k","chapter":"9 Unsupervised Methods","heading":"Trying various values of k","text":"values \\(k\\) augmented, tidyed, glanced information can calculated. Note might want find total within sum squares decreases function \\(k\\). (within sum squares always decrease function \\(k?\\) ? ?) seems though \\(k=3\\) \\(k=4\\) probably sufficient (larger values \\(k\\) reduce sums squares substantially).Adding back rest penguin information (species, sex, island, etc.) order determine unsupervised clusters align non-numeric information given dataframe.two variables flipper_length_mm bill_lengh_mm seems two clusters probably sufficient. However, probably make sense look cluster colorings across four variables higher dimensional space (e.g., 3-D projections).Based species, clustering (\\(k=2\\)) seems separate Gentoo, can’t really differentiate Adelie Chinstrap. let \\(k=4\\), get almost perfect partition three species.island, seems like clustering isn’t able (really, !) distinguish penguins three islands.","code":"\nkmax <- 9\npenguin_kclusts <- \n  tibble(k = 1:kmax) %>%\n  mutate(\n    penguin_kclust = map(k, ~kmeans(penguins_km, .x)),\n    tidied = map(penguin_kclust, tidy),\n    glanced = map(penguin_kclust, glance),\n    augmented = map(penguin_kclust, augment, penguins_km)\n  )\n\npenguin_kclusts## # A tibble: 9 × 5\n##       k penguin_kclust tidied           glanced          augmented         \n##   <int> <list>         <list>           <list>           <list>            \n## 1     1 <kmeans>       <tibble [1 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 2     2 <kmeans>       <tibble [2 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 3     3 <kmeans>       <tibble [3 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 4     4 <kmeans>       <tibble [4 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 5     5 <kmeans>       <tibble [5 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 6     6 <kmeans>       <tibble [6 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 7     7 <kmeans>       <tibble [7 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 8     8 <kmeans>       <tibble [8 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 9     9 <kmeans>       <tibble [9 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\nclusters <- \n  penguin_kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  penguin_kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  penguin_kclusts %>%\n  unnest(cols = c(glanced))\nclusterings %>%\n  ggplot(aes(x = k, y = tot.withinss)) + \n  geom_line() + \n  geom_point() + ylab(\"\") +\n  ggtitle(\"Total Within Sum of Squares\")\nassignments <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(species, island, sex, year) %>%\n  slice(rep(1:n(), times = kmax)) %>%\n  cbind(assignments)\nassignments %>%\n ggplot(aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\nassignments %>%\n  group_by(k) %>%\n  select(.cluster, species) %>%\n  table() %>%\n  as.data.frame() %>%\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = species, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") + \n  scale_fill_gradient(low = \"white\", high = \"red\") + \n  ggtitle(\"Species vs cluster prediction across different values of k\")\nassignments %>%\n  group_by(k) %>%\n  select(.cluster, island) %>%\n  table() %>%\n  as.data.frame() %>%\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = island, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  ggtitle(\"Island vs cluster prediction across different values of k\")"},{"path":"unsup.html","id":"partitioning-around-medoids","chapter":"9 Unsupervised Methods","heading":"9.4.3 Partitioning Around Medoids","text":"alternative \\(k\\)-means, Kaufman Rousseeuw developed Partitioning around Medoids (Finding Groups Data: introduction cluster analysis, 1990). particular strength PAM allows dissimilarity metric. , dissimilarity based correlations problem, algorithm gets complicated “center” longer defined Euclidean terms.two main steps Build (akin assigning points clusters) Swap (akin redefining cluster centers). objective function algorithm tries minimize average dissimilarity objects closest representative object. (PAM algorithm good (necessarily global optimum) solution minimizing objective function.)\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{\\C_k}D_i \\Bigg\\} = \\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{\\C_k}d(x_i, m_k) \\Bigg\\}\\]\n\\(D_i\\) represents distance observation \\(\\) closest medoid, \\(m_k\\).strengthsNo hierarchical structure / points can move one cluster another.Can run range values \\(k\\).can use distance measureshortcomings\\(k\\) predefined run algorithm.Algorithm: Partitioning Around Medoids (Elements Statistical Learning (2001), Hastie, Tibshirani, Friedman, pg 469.)Randomly assign number, 1 \\(K\\), observations. serve initial cluster assignments observations.Randomly assign number, 1 \\(K\\), observations. serve initial cluster assignments observations.Iterate cluster assignments stop changing:Iterate cluster assignments stop changing:(Repeat \\(k \\\\{1, 2, ...K\\}\\)) given cluster, \\(C_k\\), find observation cluster minimizing total distance points cluster:\n\\[^*_k = \\text{arg}\\,\\min\\limits_{\\C_k} \\sum_{' \\C_k} d(x_i, x_{'})\\]\n\\(m_k = x_{^*_k}, k=1, 2, \\ldots, K\\) current estimates cluster centers.Given current set cluster centers \\(\\{m_1, m_2, \\ldots, m_K\\}\\), minimize total error assigning observation closest (current) cluster center:\n\\[C_i = \\text{arg}\\,\\min\\limits_{1 \\leq k \\leq K} d(x_i, m_k)\\]","code":""},{"path":"unsup.html","id":"evaluation-metrics","chapter":"9 Unsupervised Methods","heading":"9.5 Evaluation Metrics","text":"Silhouette WidthConsider observation \\(\\\\) cluster \\(Clus1\\). Let\n\\[\\begin{align}\nd(, Clus2) &= \\mbox{average dissimilarity } \\mbox{ objects cluster } Clus2\\\\\n() &=  \\mbox{average dissimilarity } \\mbox{ objects } Clus1.\\\\\nb() &= \\min_{Clus2 \\ne Clus1} d(,Clus2) = \\mbox{distance next closest neighbor cluster}\\\\\n\\mbox{ silhouette width} &= s() = \\frac{b() - ()}{\\max \\{ (), b() \\}}\\\\\n& &\\\\\n\\mbox{average}_{\\Clus1} s() &= \\mbox{average silhouette width cluster $Clus1$}\n\\end{align}\\]\nNote \\(() < b()\\) \\(\\) well classified maximum \\(s() = 1\\). \\(() > b()\\) \\(\\) well classified maximum \\(s() = -1\\).Diameter cluster \\(Clus1\\) (within cluster measure)\n\\[\\begin{align}\n\\mbox{diameter} = \\max_{,j \\Clus1} d(,j)\n\\end{align}\\]Diameter cluster \\(Clus1\\) (within cluster measure)\n\\[\\begin{align}\n\\mbox{diameter} = \\max_{,j \\Clus1} d(,j)\n\\end{align}\\]Separation cluster \\(Clus1\\) (cluster measure)\n\\[\\begin{align}\n\\mbox{separation} = \\min_{\\Clus1, j \\notin Clus1} d(,j)\n\\end{align}\\]Separation cluster \\(Clus1\\) (cluster measure)\n\\[\\begin{align}\n\\mbox{separation} = \\min_{\\Clus1, j \\notin Clus1} d(,j)\n\\end{align}\\]\\(L^*\\): cluster diameter \\(<\\) separation; \\(L\\): cluster \\(\\max_{j \\Clus1} d(,j) < \\min_{k \\notin Clus1} d(,k)\\).\\(L^*\\): cluster diameter \\(<\\) separation; \\(L\\): cluster \\(\\max_{j \\Clus1} d(,j) < \\min_{k \\notin Clus1} d(,k)\\).","code":""},{"path":"unsup.html","id":"pam-example","chapter":"9 Unsupervised Methods","heading":"PAM example","text":"","code":""},{"path":"unsup.html","id":"building-the-clusters","chapter":"9 Unsupervised Methods","heading":"Building the clusters","text":"Start considering random allocation (AC) (BDE)Start considering random allocation (AC) (BDE)second step, calculate within cluster sums distances:second step, calculate within cluster sums distances:: 0.6C: 0.6B: 0.9 + 0.8 = 1.7D: 0.9 + 0.3 = 1.2E: 0.8 + 0.3 = 1.1For cluster 1, doesn’t matter choose C (let’s choose C). cluster 2, choose E (“central” measured closer distance B D).Reallocate points:Cluster1: C points closer C E. B closer C E. Cluster1 (,B,C).Cluster2: E points closer E C. D closer E C. Cluster2 (D,E)Redefine cluster centers:: 0.2 + 0.6 = 0.8B: 0.2 + 0.5 = 0.7C: 0.6 + 0.5 = 1.1D: 0.3E: 0.3Cluster1 now medoid B. Cluster2 (choose randomly) medoid D.Reallocate points:Cluster1: B (,B)Cluster2: D C, E (D, C, E)medoids now B (randomly choose) D. iteration process converged.","code":""},{"path":"unsup.html","id":"evaluating-the-clusters","chapter":"9 Unsupervised Methods","heading":"Evaluating the clusters","text":"(Note: matrix , 4 observations.)Consider data (AB)(CD) clusters, can calculate previous metrics:Silhouette Width\n\\[\\begin{align}\ns(=) = \\frac{b() - ()}{\\max \\{(), b()\\}} = \\frac{0.8 - 0.2}{0.8} = 0.75\\\\\ns(=B) = \\frac{b(B) - (B)}{\\max \\{(B), b(B)\\}} = \\frac{0.7 - 0.2}{0.7} = 0.71\\\\\ns(=C) = \\frac{b(C) - (C)}{\\max \\{(C), b(C)\\}} = \\frac{0.55 - 0.4}{0.55} = .27\\\\\ns(=D) = \\frac{b(D) - (D)}{\\max \\{(D), b(D)\\}} = \\frac{0.95 - 0.4}{0.95} = .57\\\\\n\\mbox{Ave SW} = 0.575\\\\\n\\end{align}\\]Diameter\n\\[\\begin{align}\n\\mbox{diameter}(AB) = 0.2\\\\\n\\mbox{diameter}(CD) = 0.4\\\\\n\\end{align}\\]Separation\n\\[\\begin{align}\n\\mbox{separation}(AB) = \\mbox{separation}(CD) = 0.5\\\\\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"rand-index-adjusted-rand-index","chapter":"9 Unsupervised Methods","heading":"Rand Index / Adjusted Rand Index","text":"based confusion matrix comparing either known truth (labels) comparing two different clusterings (e.g., comparing \\(k\\)-means hierarchical clustering). Let two different clusterings called partition1 partition2.\n* number pairs observations put together partition1 partition2\n* b number pairs observations together partition1 apart partition2\n* c number pairs observations together partition2 apart partition1\n* d number pairs observations apart partitions\\[\\mbox{Rand index} = \\frac{+d}{+b+c+d}\\]cool thing Rand index partitions don’t even number clusters. can absolutely two clusterings (one might known labels, example). Details Adjusted Rand index given http://faculty.washington.edu/kayee/pca/supp.pdf (basic idea center scale Rand index values meaningful).","code":""},{"path":"unsup.html","id":"em-algorithm","chapter":"9 Unsupervised Methods","heading":"9.6 EM algorithm","text":"EM algorithm incredibly useful tool solving complicated maximization procedures, particularly respect maximizing likelihoods (typically parameter estimation). describe procedure context estimating parameters two-component mixture model.Consider Old Faithful geyser Yellowstone National Park, Wyoming, USA following histogram data waiting times eruption:\\[\\begin{align}\nY_1 &\\sim N(\\mu_1, \\sigma_1^2)\\\\\nY_2 &\\sim N(\\mu_2, \\sigma_2^2)\\\\\nY &= (1-\\Delta) Y_1 + \\Delta Y_2\\\\\nP(\\Delta=1) &= \\pi\\\\\n\\end{align}\\]\nsimple two component case, can see representation indicates first generate \\(\\Delta \\\\{0,1\\}\\), , depending result, generate either \\(Y_1\\) \\(Y_2\\). likelihood associated setting :\\[\\begin{align}\ng_Y(y) = (1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)\n\\end{align}\\]\n\\(\\phi_\\theta\\) represents normal distribution vector \\(\\theta=(\\mu, \\sigma)\\) parameters. Typically, statistical theory, find \\(\\theta\\), take derivative log-likelihood find values maximize. , however, likelihood complicated solve \\(\\theta\\) closed form.\\[\\begin{align}\nl(\\theta; {\\bf y}) = \\sum_{=1}^N \\log [(1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)].\n\\end{align}\\]know point comes distribution, however, maximization straightforward can use points group one estimate parameters first distribution, points group two estimate parameters second distribution. process assigning points estimating parameters can thought two steps:Expectation: assignment (soft , points weighted) observation group.Maximization: update parameter estimates.Algorithm: EM Algorithm two-component Gaussian mixture. Elements Statistical Learning (2001), Hastie, Tibshirani, Friedman, pg 238.Take initial guesses parameters \\(\\hat{\\mu}_1, \\hat{\\sigma}_1^2, \\hat{\\mu}_2, \\hat{\\sigma}_2^2, \\hat{\\pi}\\).Expectation Step: compute responsibilities:\n\\[ \\hat{\\gamma}_i = \\frac{\\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}{(1-\\hat{\\pi}) \\phi_{\\hat{\\theta}_1} (y_i) + \\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}, =1, 2, \\ldots, N.\\]Maximization Step: compute weighted means variances:\n\\[\\begin{align}\n\\hat{\\mu}_1 = \\frac{\\sum_{=1}^N (1-\\hat{\\gamma_i})y_i}{\\sum_{=1}^N (1-\\hat{\\gamma_i})} && \\hat{\\sigma}_1^2 = \\frac{\\sum_{=1}^N (1-\\hat{\\gamma_i})(y_i - \\hat{\\mu}_1)^2}{\\sum_{=1}^N (1-\\hat{\\gamma_i})}\\\\\n\\hat{\\mu}_2 = \\frac{\\sum_{=1}^N \\hat{\\gamma_i}y_i}{\\sum_{=1}^N \\hat{\\gamma_i}} && \\hat{\\sigma}_2^2 = \\frac{\\sum_{=1}^N \\hat{\\gamma_i}(y_i - \\hat{\\mu}_2)^2}{\\sum_{=1}^N \\hat{\\gamma_i}}\n\\end{align}\\]\nmixing probability \\(\\hat{\\pi} = \\sum_{=1}^N \\hat{\\gamma}_i / N\\).Iterate Steps 2. 3. convergence.algorithm shows particular allocation points, can maximize given likelihood estimate parameter values (done Maximization Step). However, obvious algorithm first allocation step leads maximization (local global) likelihood. proof EM algorithm converging local maximum likelihood (necessarily converge global max) uses information marginal prior posterior likelihoods parameter values Jensen’s inequality show likelihood decrease iterative steps.Note previous \\(k\\)-means algorithm iterated two steps assigning points clusters estimating cluster centers (thought space scaled Euclidean distance appropriate dimensions). Two differences algorithms covered :\\(k\\)-means uses hard thresholding EM uses soft thresholding\\(k\\)-means uses fixed standard deviation 1, EM allows data/algorithm find standard deviationIndeed, although EM-algorithm slightly different previous \\(k\\)-means algorithm, two methods typically converge result considered different implementations \\(k\\)-means algorithm.See following applet visual representation EM-algorithm converges: http://www.socr.ucla.edu/applets.dir/mixtureem.html.","code":""},{"path":"misc.html","id":"misc","chapter":"10 Misc","heading":"10 Misc","text":"","code":""},{"path":"misc.html","id":"Nov26","chapter":"10 Misc","heading":"10.1 11/26/19 Agenda","text":"API / authenticatingparallel computingcloud computingreticulate (Python R!)SQL","code":""},{"path":"misc.html","id":"api","chapter":"10 Misc","heading":"10.2 API","text":"\nFigure 1.2: xkcd, https://xkcd.com/1481/\nAPI? (Application Programming Interface)Think API restaurant menu. menu provides list restaurant offer, order menu choosing dish want. order, restaurant figures bring food kitchen table way ’ve specified.API intermediary allows two applications talk one another. database server, instead code allows communication.","code":""},{"path":"misc.html","id":"examples-of-apis","chapter":"10 Misc","heading":"Examples of APIs","text":"use app phone, app connects internet sends information server somewhere. server retrieves data, interprets , , sends back . application takes data server presents readable way API.use app phone, app connects internet sends information server somewhere. server retrieves data, interprets , , sends back . application takes data server presents readable way API.Let’s say booking flight United. choose details, interact airline’s website. INSTEAD, interacting software like Expedia? Expedia talk United’s API get information available flights, costs, seats, etc.Let’s say booking flight United. choose details, interact airline’s website. INSTEAD, interacting software like Expedia? Expedia talk United’s API get information available flights, costs, seats, etc.’ve ever third party site clicked “Share Facebook” “Share Twitter” third party site communicating Facebook API Twitter API.’ve ever third party site clicked “Share Facebook” “Share Twitter” third party site communicating Facebook API Twitter API.sign go concert, StubHub asks whether want add concert Google calendar. StubHub needs talk Google via Google’s API.sign go concert, StubHub asks whether want add concert Google calendar. StubHub needs talk Google via Google’s API.want Twitter data? might get ? Well, email Twitter ask someone . Instead Twitter provides information data stored, allows query data automated way.want Twitter data? might get ? Well, email Twitter ask someone . Instead Twitter provides information data stored, allows query data automated way.\nFigure 1.3: Image taken https://rigor.com/blog/---api--brief-intro\n","code":""},{"path":"misc.html","id":"authenticating","chapter":"10 Misc","heading":"10.2.1 Authenticating","text":"Authenticating stating areAuthorization asking access resources (happens authentication)post credentials keys public GitHub repo!!almost cases, order communicate API, must tell API access information API providing.\nFigure 1.4: Image taken https://blog.restcase.com/restful-api-authentication-basics/\n","code":""},{"path":"misc.html","id":"parallel-computing","chapter":"10 Misc","heading":"10.3 Parallel Computing","text":"(Taken Teach Data Science blog: https://teachdatascience.com/parallel/)demonstrate parallel computing , ’ll perform tasks embarrassingly parallel means dependency communication parallel tasks. , parallel computing can powerful ways link computational tasks complicated ways. believe first pass teaching parallel computing, teach parallel structure bringing dependence across parallel tasks. Examples embarrassingly parallel algorithms include: Monte Carlo analysis, bootstrapping, growing trees Random Forests, group_by analyses, cross-validation. Additionally, data science methods increasingly use randomized algorithms can often written parallel.Indeed, isn’t always easy know use parallel construction. existing overhead processes (e.g., copying data across many threads, bring results together, etc.) algorithm run 10 parallel strands reduce original (non-parallel) run time 10-fold. Figuring parallel implementation appropriate beyond scope blog carefully considered embarking large projects.","code":""},{"path":"misc.html","id":"some-parallel-examples","chapter":"10 Misc","heading":"Some parallel examples","text":"running code parallel, valuable know many cores computer work . Note detectCores function provide information specific device using (logical = FALSE tells physical cores likely want). Note makeCluster separate threads information. [argument setup_strategy = \"sequential\" fix working R 4.0+.] stopCluster, code longer connecting cluster structure.","code":"\nlibrary(parallel)\nP <- detectCores(logical=FALSE)\nP## [1] 8\ncl <- makeCluster(P, setup_strategy = \"sequential\")\ncl[[1]]## node of a socket cluster on host 'localhost' with pid 63124\nstopCluster(cl)\ncl[[1]]## Error in summary.connection(connection): invalid connection"},{"path":"misc.html","id":"embarrassingly-embarrassing-example","chapter":"10 Misc","heading":"Embarrassingly embarrassing example","text":"example , generate Cauchy data find max sample. Note current device 8 cores, process happen 100/P = 12.5 times core. second argument clusterApply sequence numbers gets passed worker (first) argument func1. , ’ve specified value 50 (number reps) passed separately 100 different workers.many R functions implement parallel processing. example, code can processed using foreach.","code":"\nW <- 100\nP <- parallel::detectCores(logical=FALSE)\ncl <- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\nfunc1 <- function(reps){\n  max(rcauchy(reps))\n}\n\n\nclusterApply(cl, rep(50,W), fun = func1) %>% head(3)## [[1]]\n## [1] 4.973585\n## \n## [[2]]\n## [1] 12.45264\n## \n## [[3]]\n## [1] 27.81546\nstopCluster(cl)\nlibrary(doParallel)\ncl <- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\ndoParallel::registerDoParallel(cl)\nforeach(reps = rep(50, 100), .combine = 'c') %dopar% {\n  max(rcauchy(reps))\n       } %>% head(3)## [1]   40.43513   15.91192 7042.85346\nstopCluster(cl)"},{"path":"misc.html","id":"example-bootstrapping","chapter":"10 Misc","heading":"Example bootstrapping","text":"slightly less embarrassingly parallel example comes bootstrapping. used parallel implementation bootstrap mean iris data petal length (Virginica ).","code":"\ndata(iris)\n\niris_bs <- iris %>%\n  filter(Species == \"virginica\") %>%\n  select(Petal.Length)\ncl <- parallel::makeCluster(P, setup_strategy = \"sequential\")\n\ndoParallel::registerDoParallel(cl)\nbsmean_PL <- foreach(i = 1:100, .combine = 'c') %dopar% {\n  mean(sample(iris_bs$Petal.Length, replace = TRUE))\n}\nbootstrap <- tibble(bsmean_PL)\nstopCluster(cl)\n\nggplot(bootstrap, aes(x = bsmean_PL)) + geom_histogram(bins = 25) + ggtitle(\"Histogram of 100 Bootstrapped Means using foreach\")"},{"path":"misc.html","id":"spark-and-sparklyr","chapter":"10 Misc","heading":"10.3.1 Spark and sparklyr","text":"may familiar Apache Spark open-source product distributed cluster-computing. may want learn capabilities, including scheduling workflow, dispatching tasks, consolidating end results. incredibly powerful, historically steep learning curve getting R work smoothly Spark connection. Recently, RStudio come new package sparklyr integrates R Spark seamlessly. Note example , ’ve set local connection just purposes example. work, may want connect cluster cloud space many cores.RStudio sparklyr webpage provides plethora good examples demonstrating sophistication power technology. sparklyr particularly strong connections suite tidyverse functions. Indeed, power sparklyr distributing computing parallelizing . example, sparklyr computations delayed need results. Additionally, Spark heavy lifting end (results called) need worry size table, results, computational space. example repeats bootstrapping work done previously.Note, important look data structures variables names. example, copying local dataframe iris_samps remote data source called iris_samps_tbl, variable Petal.Length changed Petal_Length.particular application, adept reader probably noticed average variable using group_by quick easy task dplyr. Indeed, use sparklyr overkill presented way demonstrate using sparklyr. working big datasets require large computing infrastructure, RStudio help pages sparklyr fantastic. Additionally, many instances working Spark wild, might consider working someone else’s Spark analysis like fantastic example splitting large amounts raw DNA sequencing get data given genetic location.introduction parallel cloud computing help become adept less apprehensive using tools, also recognition sufficient background computer science needed able fully engage principles high performance computing.","code":"\nlibrary(sparklyr)\nspark_install()\n\nsc <- spark_connect(master = \"local\")\n\nn_sim = 100\niris_samps <- iris %>% dplyr::filter(Species == \"virginica\") %>%\n  sapply(rep.int, times=n_sim) %>% cbind(replicate = rep(1:n_sim, each = 50)) %>% \n  data.frame() %>%\n  dplyr::group_by(replicate) %>%\n  dplyr::sample_n(50, replace = TRUE)\n\niris_samps_tbl <- copy_to(sc, iris_samps)\n\niris_samps_tbl %>% \n  spark_apply(function(x) {mean(x$Petal_Length)}, \n    group_by = \"replicate\") %>%\n  ggplot(aes(x = result)) + geom_histogram(bins = 20) + ggtitle(\"Histogram of 100 Bootstrapped Means using sparklyr\")\nspark_disconnect(sc)\niris_samps %>% dplyr::group_by(replicate) %>%\n  dplyr::summarize(result = mean(Petal.Length)) %>%\n  ggplot(aes(x = result)) + geom_histogram(bins = 25) + ggtitle(\"Histogram of 100 Bootstrapped Means using dplyr\")"},{"path":"misc.html","id":"learn-more","chapter":"10 Misc","heading":"Learn more","text":"Hana Sevcikova Introduction parallel computing R useR 2017 Brussels, tutorial heresparklyr parallel cross-validationhttps://www.rstudio.com/resources/cheatsheets/\nhttps://www.rstudio.com/resources/cheatsheets/#sparklyr\nhttps://github.com/rstudio/cheatsheets/raw/master/parallel_computation.pdf\nhttps://www.rstudio.com/resources/cheatsheets/#sparklyrhttps://github.com/rstudio/cheatsheets/raw/master/parallel_computation.pdfGreat blog Two Flavors Parallel Simulation Mark LeBoeuf comparing different ways process code parallel.","code":""},{"path":"misc.html","id":"cloud-computing","chapter":"10 Misc","heading":"10.4 Cloud Computing","text":"great overview high performance computing (HPC) isn’t given : https://www.slideshare.net/raamana/high-performance-computing--checklist--tips-optimal-cluster-usage\nFigure 1.8: Image Pradeep Redddy Raamana High performance computing tutorial, checklist tips optimize cluster usage\n(rest, , taken Teach Data Science blog: https://teachdatascience.com/cloud2/, entry written Nick Horton)R package parallel designed send tasks multiple cores. Today’s computers (even small laptops!) typically multiple cores, server cloud computing infrastructure can easily handle dozens hundreds parallel tasks. structure R parallel implementation sends tasks workers don’t talk one another compiling results end. 2017 UseR! tutorial, Hana Sevcikova describes function workers run code/functions/iterations separately results subsequently combined.\nFigure 1.9: Image Sevcikova UseR! 2017 tutorial parallel computing\ncomputing infrastructure becomes sophisticated, important language describe connected components work. Parallel processing allows conversation differences distributed computing, cluster computing, grid computing, generally, framework high performance computing. benefit parallel computing introduction larger infrastructure task worker clear, important, easy describe.discussion motivated several recent papers blog posts describe complex, real-world data science computation can structured ways feasible past years without herculean efforts. worth noting fantastic example described multiple iterations needed parse huge amounts raw DNA sequencing data undertake analyses given set genetic locations. “Ambitious data science can painless” Monajemi et al. describe workflows take advantage new software stacks undertake massive cloud-based experiments. years older, Chamandy et al.’s Teaching statistics Google scale described three examples modern data challenges overcome creative statistical thinking (see companion report Estimating uncertainty massive data streams ). Finally, NSF-funded workshop report “Enabling computer information science engineering research education cloud” highlights opportunities university computing migrates cloud solutions .last, may enjoy reading recent Three strategies working Big Data R blog post.can prepare cloud computing undergraduate course?","code":""},{"path":"misc.html","id":"getting-started","chapter":"10 Misc","heading":"Getting started","text":"steps exploring cloud-based systems? main cloud providers active educational outreach programs.Google Compute Platform allows faculty apply receive $100 GCP credits $50 per student. Credits can used courses, student clubs, faculty-sponsored events. (replicate example later blog, ’ll want set account request credits.)Google Compute Platform allows faculty apply receive $100 GCP credits $50 per student. Credits can used courses, student clubs, faculty-sponsored events. (replicate example later blog, ’ll want set account request credits.)Azure Education provides access educators open source content classes $200 Azure credits, plus free services.Azure Education provides access educators open source content classes $200 Azure credits, plus free services.Amazon Web Services Educate provides $75 $200 AWS annual credits per educator (depending membership status) $30 $100 students.Amazon Web Services Educate provides $75 $200 AWS annual credits per educator (depending membership status) $30 $100 students.sign start explore! world cloud computing quickly changing. gaining experience investment time learning tools help instructors provide guidance students use modern computational tools.","code":""},{"path":"misc.html","id":"an-example-bigquery-in-googles-gcp","chapter":"10 Misc","heading":"An example: BigQuery in Google’s GCP","text":"Consider example using GCP (kudos Shukry Zablah assistance).BigQuery Google’s serverless, highly-scalable, cloud data warehouse. quickstart document available discusses use web user interface GCP console well access API interface. bigrquery package R makes easy work data stored Google BigQuery queries BigQuery tables.first step request GCP credits (see ) use online interface create project (called “Test Project Blog”).BigQuery includes number public datasets.\nanalysis public dataset revisions Wikipedia articles April 2010, hosted GCP BigQuery. size table 35.69GB. queries take seconds run.safety, always try make sure queries LIMIT set queries.previous bq_project_query() function run within RStudio, connection made Google (GCP) authentication window open local browser.heavy lifting perform done database end (note billed , though first 1TB accesses free). local machine receives data try display . Right now mostRevisions_tb just reference temporary table online. query accessed 7GB data.can get copy data local machine confident want.Let’s plot top 10 entries.’ve obviously just scratched surface . lots examples consider replicating classroom (e.g., returning tweets schedule). Hopefully intrigued enough request credits students start explore. sure begin? Check GCP Essentials Videos series.","code":"\nlibrary(dplyr)\nlibrary(bigrquery)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(purrr)\nlibrary(readr)\nprojectId <- \"bigquery-public-data\"  # replace with your own project\nbillingId <- \"test-project-for-blog\" # replace with your own billing ID\ndatasetName <- \"samples\"\ntableName <- \"wikipedia\"\nquery <- \"SELECT  title, COUNT(title) as n\n          FROM `bigquery-public-data.samples.wikipedia` \n          GROUP BY title\n          ORDER BY n DESC\n          LIMIT 500\"\nmostRevisions_tb <- \n  bigrquery::bq_project_query(x = billingId, \n    query = query) #creates temporary table\nmostRevisions <- bq_table_download(mostRevisions_tb) \nglimpse(mostRevisions)## Rows: 500\n## Columns: 2\n## $ title <chr> \"Wikipedia:Administrator intervention against vandalism\", \"Wikip…\n## $ n     <int> 643271, 419695, 326337, 257893, 226802, 204469, 191679, 186715, …\nclean <- mostRevisions %>% \n  filter(!grepl(\"Wikipedia|User|Template|Talk\", title)) %>%\n  mutate(title = fct_reorder(title, n)) %>% #to sort levels\n  glimpse()## Rows: 272\n## Columns: 2\n## $ title <fct> \"George W. Bush\", \"List of World Wrestling Entertainment employe…\n## $ n     <int> 43652, 30572, 27433, 23245, 21768, 20814, 20546, 20529, 20225, 2…\nggplot(clean %>% head(10), aes(x = title, y = n, fill = n)) + \n  geom_bar(stat = \"identity\") + \n  labs(x = \"Article Title\",\n       y = \"Number of Revisions\",\n       title = \"Most Revised Wikipedia Articles (Up to April 2010)\") +\n  scale_fill_gradient(low = \"darkblue\", high = \"darkred\", guide = FALSE) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 20, hjust = 1)) "},{"path":"misc.html","id":"reticulate","chapter":"10 Misc","heading":"10.5 reticulate","text":"(Taken Teach Data Science blog: https://teachdatascience.com/reticulate/)","code":""},{"path":"misc.html","id":"connect-to-python-within-rstudio","chapter":"10 Misc","heading":"Connect to Python within RStudio","text":"many statisticians, go-software language R. However, doubt Python important language data science. ??","code":"\nlibrary(tidyverse)\nlibrary(reticulate)\nuse_virtualenv(\"r-reticulate\")\nreticulate::import(\"statsmodels\")## Module(statsmodels)"},{"path":"misc.html","id":"i-can-run-python-inside-r","chapter":"10 Misc","heading":"I can run Python inside R??","text":"pandas data wrangling.R, chunk specified Python chunk (RStudio now running Python).view Python chunk actually run:","code":"```{python}\nimport pandas\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[flights[\"dest\"] == \"ORD\"]\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights = flights.dropna()\n```import pandas\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[flights[\"dest\"] == \"ORD\"]\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights = flights.dropna()"},{"path":"misc.html","id":"learn-about-the-dataset","chapter":"10 Misc","heading":"Learn about the dataset","text":"","code":"```{python}\nflights.shape\nflights.head(3)\nflights.describe()\n```flights.shape## (12590, 3)flights.head(3)##    carrier  dep_delay  arr_delay\n## 4       UA       -4.0       12.0\n## 5       AA       -2.0        8.0\n## 22      AA       -1.0       14.0flights.describe()##           dep_delay     arr_delay\n## count  12590.000000  12590.000000\n## mean      11.709770      2.917951\n## std       39.409704     44.885155\n## min      -20.000000    -62.000000\n## 25%       -6.000000    -22.000000\n## 50%       -2.000000    -10.000000\n## 75%        9.000000     10.000000\n## max      466.000000    448.000000"},{"path":"misc.html","id":"computations-using-pandas","chapter":"10 Misc","heading":"Computations using pandas","text":"","code":"```{python}\nflights = pandas.read_csv(\"flights.csv\")\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights.groupby(\"carrier\").mean()\n```flights = pandas.read_csv(\"flights.csv\")\nflights = flights[['carrier', 'dep_delay', 'arr_delay']]\nflights.groupby(\"carrier\").mean()##          dep_delay  arr_delay\n## carrier                      \n## AA        8.586016   0.364291\n## AS        5.804775  -9.930889\n## DL        9.264505   1.644341\n## UA       12.106073   3.558011\n## US        3.782418   2.129595"},{"path":"misc.html","id":"from-python-chunk-to-r-chunk","chapter":"10 Misc","heading":"From Python chunk to R chunk","text":"py$x accesses x variable created within Python Rr.x accesses x variable created within R Python","code":"\nlibrary(ggplot2)\nggplot(py$flights, \n       aes(x=carrier, \n           y=arr_delay)) + \n  geom_point() + \n  geom_jitter()"},{"path":"misc.html","id":"from-r-chunk-to-python-chunk","chapter":"10 Misc","heading":"From R chunk to Python chunk","text":"","code":"\ndata(diamonds)\nhead(diamonds)## # A tibble: 6 × 10\n##   carat cut       color clarity depth table price     x     y     z\n##   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n## 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n## 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n## 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n## 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n## 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n## 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48"},{"path":"misc.html","id":"python-chunks","chapter":"10 Misc","heading":"Python chunks","text":"Note ’re calling Python code R object.","code":"print(r.diamonds.describe())##               carat         depth  ...             y             z\n## count  53940.000000  53940.000000  ...  53940.000000  53940.000000\n## mean       0.797940     61.749405  ...      5.734526      3.538734\n## std        0.474011      1.432621  ...      1.142135      0.705699\n## min        0.200000     43.000000  ...      0.000000      0.000000\n## 25%        0.400000     61.000000  ...      4.720000      2.910000\n## 50%        0.700000     61.800000  ...      5.710000      3.530000\n## 75%        1.040000     62.500000  ...      6.540000      4.040000\n## max        5.010000     79.000000  ...     58.900000     31.800000\n## \n## [8 rows x 7 columns]import statsmodels.formula.api as smf\nmodel = smf.ols('price ~ carat', data = r.diamonds).fit()\nprint(model.summary())##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                  price   R-squared:                       0.849\n## Model:                            OLS   Adj. R-squared:                  0.849\n## Method:                 Least Squares   F-statistic:                 3.041e+05\n## Date:                Mon, 08 Nov 2021   Prob (F-statistic):               0.00\n## Time:                        05:39:52   Log-Likelihood:            -4.7273e+05\n## No. Observations:               53940   AIC:                         9.455e+05\n## Df Residuals:                   53938   BIC:                         9.455e+05\n## Df Model:                           1                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P>|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## Intercept  -2256.3606     13.055   -172.830      0.000   -2281.949   -2230.772\n## carat       7756.4256     14.067    551.408      0.000    7728.855    7783.996\n## ==============================================================================\n## Omnibus:                    14025.341   Durbin-Watson:                   0.986\n## Prob(Omnibus):                  0.000   Jarque-Bera (JB):           153030.525\n## Skew:                           0.939   Prob(JB):                         0.00\n## Kurtosis:                      11.035   Cond. No.                         3.65\n## ==============================================================================\n## \n## Warnings:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},{"path":"misc.html","id":"running-just-python","chapter":"10 Misc","heading":"Running just Python","text":"","code":""},{"path":"misc.html","id":"full-disclosure","chapter":"10 Misc","heading":"Full disclosure","text":"reticulate always trivial set . Indeed, ’ve trouble figuring Python version talking R different module versions live.","code":""},{"path":"misc.html","id":"learn-more-1","chapter":"10 Misc","heading":"Learn more","text":"RStudio R Interface Pythonhttps://rstudio.github.io/reticulate/RStudio blog Reticulated Pythonhttps://blog.rstudio.com/2018/10/09/rstudio-1-2-preview-reticulated-python","code":""},{"path":"misc.html","id":"sql-in-r","chapter":"10 Misc","heading":"10.6 SQL (in R)","text":"Note exists R interface work SQL commands within R Markdown file. consistency class notes, ’ve continued use R Markdown structure demonstrate course material.(Taken Teach Data Science blog: https://teachdatascience.com/sql/, entry written Nick Horton)SQL (pronounced sequel) stands Structured Query Language; language designed manage data relational database system.use public facing MySQL database containing wideband acoustic immittance (WAI) measures made normal ears adults. (project funded National Institutes Health, NIDCD, hosted server Smith College, PI Susan Voss, R15 DC014129-01.) database created enable auditory researchers share WAI measurements combine analyses multiple datasets.begin demonstrating SQL queries can sent database. necessary set connection using dbConnect() function.Next series SQL queries can sent database using DBI::dbGetQuery() function: query returns R dataframe.multiple tables within wai database.EXPLAIN command describes ten field names (variables) PI_Info table.SELECT statement can used select fields eight observations Measurements table.interesting complicated SELECT calls can used undertake grouping aggregation. calculate sample size study","code":"\nlibrary(mosaic)\nlibrary(RMySQL)  \ncon <- dbConnect(\n  RMySQL::MySQL(), host = \"scidb.smith.edu\", user = \"waiuser\", \n  password = \"smith_waiDB\", dbname = \"wai\")\nclass(dbGetQuery(con, \"SHOW TABLES\"))\ndbGetQuery(con, \"SHOW TABLES\")\ndbGetQuery(con, \"EXPLAIN PI_Info\")\neightobs <- dbGetQuery(con, \"SELECT * FROM Measurements LIMIT 8\")\neightobs\ndbGetQuery(con, \n  \"SELECT Identifier, count(*) AS NUM FROM Measurements GROUP BY Identifier ORDER BY NUM\")"},{"path":"misc.html","id":"accessing-a-database-using-dplyr-commands","chapter":"10 Misc","heading":"Accessing a database using dplyr commands","text":"Alternatively, connection can made server creating series dplyr tbl\nobjects. Connecting familiar dplyr syntax attractive , Hadley Wickham noted, SQL R similar syntax (sufficiently different confusing).setup process looks similar.explore PI_Info table using collect() function used force computation database (return results). One attractive aspect database systems feature lazy evaluation, computation optimized postponed long possible.Note number rows unknown (?? top output ) lazy query.Similarly, can explore Subjects table.Let’s explore Measurements table.quarter million observations.next step, download data given subject specific study, case paper Rosowski et al. (2012) entitled “Ear-canal reflectance, umbo velocity, tympanometry normal-hearing adults”.Arbitrarily choose collect data subject number three.Finally can display results measurements function frequency ear (left right) used.Always good idea terminate SQL connection.note number relational database systems exist, including MySQL (illustrated ), PostgreSQL, SQLite. information databases within R can found CRAN Databases R Task View.Setting managing database topic different day: focused SQL can used within R access data flexible powerful manner.","code":"\nMeasurements <- tbl(con, \"Measurements\")\nclass(Measurements)\nPI_Info <- tbl(con, \"PI_Info\")\nSubject <- tbl(con, \"Subjects\")\nPI_Info %>% collect() %>% data.frame()   \n# be careful with collect() when dealing with large tables!\n#Subject  %>% summarise(total = n())\nSubject %>% collect()  # be careful with collect() with large tables!\n#Measurements %>% summarise(total = n())\nMeasurements %>% collect()\nonesubj <- \n  Measurements %>% \n  filter(Identifier == \"Rosowski_2012\", Sub_Number == 3) %>%\n  collect %>%\n  mutate(SessionNum = as.factor(Session))\nhead(onesubj)\nonesubj <- mutate(onesubj, Ear = ifelse(Left_Ear == 1, \"Left\", \"Right\"))\nggplot(onesubj, aes(x = Freq, y = Absorbance)) + geom_point() +\n  aes(colour = Ear) + scale_x_log10() + labs(title=\"Absorbance by ear Rosowski subject 3\")\ndbDisconnect(con)"},{"path":"misc.html","id":"learn-more-2","chapter":"10 Misc","heading":"Learn more","text":"https://chance.amstat.org/2015/04/setting--stage/ (Setting stage data technologies)https://www.w3schools.com/sql/sql_intro.asp (Intro SQL)http://www.science.smith.edu/wai-database/home// (WAI SQL Database)https://cran.r-project.org/web/views/Databases.html (CRAN Task View Databases R)https://db.rstudio.com (RStudio Database resources)https://dbplyr.tidyverse.org/articles/dbplyr.html (dbplyr package)","code":""},{"path":"misc.html","id":"regexpr","chapter":"10 Misc","heading":"10.7 Regular Expressions","text":"regular expression … sequence characters define search pattern. Usually patterns used string searching algorithms “find” “find replace” operations strings, input validation. technique developed theoretical computer science formal language theory. [https://en.wikipedia.org/wiki/Regular_expression]","code":""},{"path":"misc.html","id":"main-tasks-in-character-matching","chapter":"10 Misc","heading":"Main tasks in character matching:","text":"basic string operationspattern matching (regular expressions)sentiment analysisThe ideas mostly taken Jenny Bryan’s STAT545 class: https://github.com/STAT545-UBC/STAT545-UBC-original-website/blob/master/block022_regular-expression.rmd","code":""},{"path":"misc.html","id":"r-packages-to-make-your-life-easier","chapter":"10 Misc","heading":"R packages to make your life easier","text":"stringr package core package tidyverse. installed via install.packages(\"tidyverse\") also loaded via library(tidyverse). course, can also install load individually.\nMany main functions start str_. Auto-complete friend.\nReplacements base functions re: string manipulation regular expressions (see ).\nMain advantages base functions: greater consistency inputs outputs. Outputs ready next analytical task.\nstringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf\nMany main functions start str_. Auto-complete friend.Replacements base functions re: string manipulation regular expressions (see ).Main advantages base functions: greater consistency inputs outputs. Outputs ready next analytical task.stringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdftidyr package Especially useful functions split one character vector many vice versa: separate(), unite(), extract().Base functions: nchar(), strsplit(), substr(), paste(), paste0().glue package fantastic string interpolation. stringr::str_interp() doesn’t get job done, check glue package.","code":""},{"path":"misc.html","id":"string-functions-related-to-regular-expression","chapter":"10 Misc","heading":"String functions related to regular expression","text":"Regular expression pattern describes specific set strings common structure. heavily used string matching / replacing programming languages, although specific syntax may differ bit. truly heart soul string operations. R, many string functions base R well stringr package use regular expressions, even Rstudio’s search replace allows regular expression:identify match pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()extract match pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()locate pattern within string, .e. give start position matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()replace pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()split string using pattern: strsplit(), stringr::str_split()Regular expressions typically specify characters (character classes) seek , possibly information repeats location within string. accomplished help metacharacters specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. use small examples introduce regular expression syntax metacharacters mean.","code":""},{"path":"misc.html","id":"escape-sequences","chapter":"10 Misc","heading":"Escape sequences","text":"special characters R directly coded string. example, let’s say specify pattern single quotes want find countries single quote '. “escape” single quote pattern, preceding \\, clear part string-specifying machinery.characters R require escaping, rule applies string functions R, including regular expressions. See complete list R escape sequences.\\': single quote. don’t need escape single quote inside double-quoted string, can also use \"'\" previous example.\\\": double quote. Similarly, double quotes can used inside single-quoted string, .e. '\"'.\\n: newline.\\r: carriage return.\\t: tab character.Note: cat() print() handle escape sequences differently, want print string sequences interpreted, use cat().","code":"\nprint(\"a\\nb\")## [1] \"a\\nb\"\ncat(\"a\\nb\")## a\n## b"},{"path":"misc.html","id":"quantifiers","chapter":"10 Misc","heading":"Quantifiers","text":"Quantifiers specify many repetitions pattern.*: matches least 0 times.+: matches least 1 times.?: matches 1 times.{n}: matches exactly n times.{n,}: matches least n times.{n,m}: matches n m times.","code":"\n(strings <- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\"))## [1] \"a\"      \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\ngrep(\"ac*b\", strings, value = TRUE)## [1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\ngrep(\"ac*b\", strings, value = FALSE)## [1] 2 3 4 5 6\ngrep(\"ac+b\", strings, value = TRUE)## [1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\ngrep(\"ac?b\", strings, value = TRUE)## [1] \"ab\"  \"acb\"\ngrep(\"ac{2}b\", strings, value = TRUE)## [1] \"accb\"\ngrep(\"ac{2}b\", strings, value = FALSE)## [1] 4\ngrep(\"ac{2,}b\", strings, value = TRUE)## [1] \"accb\"   \"acccb\"  \"accccb\"\ngrep(\"ac{2,3}b\", strings, value = TRUE)## [1] \"accb\"  \"acccb\""},{"path":"misc.html","id":"position-of-pattern-within-the-string","chapter":"10 Misc","heading":"Position of pattern within the string","text":"^: matches start string.$: matches end string.\\b: matches empty string either edge word. Don’t confuse ^ $ marks edge string.\\B: matches empty string provided edge word.","code":"\n(strings <- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\"))## [1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\ngrep(\"ab\", strings, value = TRUE)## [1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\ngrep(\"^ab\", strings, value = TRUE)## [1] \"abcd\"\ngrep(\"ab$\", strings, value = TRUE)## [1] \"cdab\"\ngrep(\"\\\\bab\", strings, value = TRUE)## [1] \"abcd\"  \"c abd\""},{"path":"misc.html","id":"operators","chapter":"10 Misc","heading":"Operators","text":".: matches single character, shown first example.[...]: character list, matches one characters inside square brackets. can also use - inside brackets specify range characters.[^...]: inverted character list, similar [...], matches characters except inside square brackets.\\: suppress special meaning metacharacters regular expression, .e. $ * + . ? [ ] ^ { } | ( ) \\, similar usage escape sequences. Since \\ needs escaped R, need escape metacharacters double backslash like \\\\$.|: “” operator, matches patterns either side |.(...): grouping regular expressions. allows retrieve bits matched various parts regular expression can alter use building new string. group can refer using \\\\N, N . (...) used. called backreference.","code":"\n(strings <- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\"))## [1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\ngrep(\"ab.\", strings, value = TRUE)## [1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\ngrep(\"ab[c-e]\", strings, value = TRUE)## [1] \"abc\" \"abd\" \"abe\"\ngrep(\"ab[^c]\", strings, value = TRUE)## [1] \"abd\"   \"abe\"   \"ab 12\"\ngrep(\"^ab\", strings, value = TRUE)## [1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\ngrep(\"\\\\^ab\", strings, value = TRUE)## [1] \"^ab\"\ngrep(\"abc|abd\", strings, value = TRUE)## [1] \"abc\" \"abd\"\ngsub(\"(ab) 12\", \"\\\\1 34\", strings)## [1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 34\""},{"path":"misc.html","id":"character-classes","chapter":"10 Misc","heading":"Character classes","text":"Character classes allow specifying entire classes characters, numbers, letters, etc. two flavors character classes, one uses [: :] around predefined name inside square brackets uses \\ special character. sometimes interchangeable.[:digit:] \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent [0-9].\\D: non-digits, equivalent [^0-9].[:lower:]: lower-case letters, equivalent [-z].[:upper:]: upper-case letters, equivalent [-Z].[:alpha:]: alphabetic characters, equivalent [[:lower:][:upper:]] [-z].[:alnum:]: alphanumeric characters, equivalent [[:alpha:][:digit:]] [-z0-9].\\w: word characters, equivalent [[:alnum:]_] [-z0-9_].\\W: word, equivalent [^-z0-9_].[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 B C D E F b c d e f, equivalent [0-9A-Fa-f].[:blank:]: blank characters, .e. space tab.[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\\s: space, .\\S: space.[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; < = > ? @ [  ] ^ _ ` { | } ~.[:graph:]: graphical (human readable) characters: equivalent [[:alnum:][:punct:]].[:print:]: printable characters, equivalent [[:alnum:][:punct:]\\\\s].[:cntrl:]: control characters, like \\n \\r, [\\x00-\\x1F\\x7F].Note:\n* [:...:] used inside square brackets, e.g. [[:digit:]].\n* \\ special character needs escape, e.g. \\\\d. confuse regular expressions R escape sequences \\t.","code":""},{"path":"misc.html","id":"stringr","chapter":"10 Misc","heading":"stringr","text":"many cases, want use incredibly useful tidy set functions available stringr package. (stringr core package tidyverse.) example, ’ve extracted first (last) word character string StreetName variable.stringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf","code":"\nlibrary(Stat2Data)\ndata(RailsTrails)\nRailsTrails <- RailsTrails %>% \n  select(HouseNum, Bedrooms, Price2014, StreetName) \nRailsTrails %>% head()##   HouseNum Bedrooms Price2014      StreetName\n## 1        1        3   210.729 Acrebrook Drive\n## 2        2        3   204.171       Autumn Dr\n## 3        3        3   338.662     Bridge Road\n## 4        4        3   276.250     Bridge Road\n## 5        5        4   169.173     Bridge Road\n## 6        6        3   211.487 Brierwood Drive\nRailsTrails %>%\n  mutate(first_piece = stringr::word(StreetName, start = 1)) %>% head()##   HouseNum Bedrooms Price2014      StreetName first_piece\n## 1        1        3   210.729 Acrebrook Drive   Acrebrook\n## 2        2        3   204.171       Autumn Dr      Autumn\n## 3        3        3   338.662     Bridge Road      Bridge\n## 4        4        3   276.250     Bridge Road      Bridge\n## 5        5        4   169.173     Bridge Road      Bridge\n## 6        6        3   211.487 Brierwood Drive   Brierwood\nRailsTrails %>%\n  mutate(last_piece = stringr::word(StreetName, start = -1)) %>% head()##   HouseNum Bedrooms Price2014      StreetName last_piece\n## 1        1        3   210.729 Acrebrook Drive      Drive\n## 2        2        3   204.171       Autumn Dr         Dr\n## 3        3        3   338.662     Bridge Road       Road\n## 4        4        3   276.250     Bridge Road       Road\n## 5        5        4   169.173     Bridge Road       Road\n## 6        6        3   211.487 Brierwood Drive      Drive"},{"path":"misc.html","id":"an-example-from-my-work","chapter":"10 Misc","heading":"An example from my work","text":"handful string characters represent genomic sequences measured RNA Sequencing dataset. task find intergenic regions (IGR) identify coding sequences (CDS) bookend intergenic regions. Note IGRs code proteins CDSs . Additionally, refers anti-sense identifies genomic sequence opposite orientation (e.g., CGGATCC vs CCTAGGC). [code written Madison Hobbs, Scripps ’19.]","code":""},{"path":"misc.html","id":"the-names-of-the-genomic-pieces","chapter":"10 Misc","heading":"The names of the genomic pieces","text":"First, important identify IGR, CDS, anti-sense.keep IGR AS_IGR strings, separate two bookends. Note, separation comes backslash.two bookend Genes, need separate feature rest. Note write feature1 second line code . bookends sequences CDS elements.CDS, now important find actual genenames IGR sequences. also keep element’s bnum represents unique gene identifier E. coli.bnum, genename, rna.name act place holders types elements need identify bookends IGRs.","code":"\nallCounts <- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\nallCounts <- allCounts %>% tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts##   feature\n## 1     CDS\n## 2     CDS\n## 3     IGR\n## 4  AS_IGR\n## 5     IGR\n##                                                                                                                                                                                        rest\n## 1                                                                                                                                                                                     b2743\n## 2                                                                                                                                                                                     b2764\n## 3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n## 4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n## 5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n##                                                                                                                                                                                                      GeneidBackup\n## 1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n## 2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n## 3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n## 4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n## 5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\nigr <- allCounts %>% filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr <- igr %>% tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)## [1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\nigr##   feature\n## 1     IGR\n## 2  AS_IGR\n## 3     IGR\n##                                                                                                                                                                                        rest\n## 1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n## 2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n## 3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n##                                                                                                            Geneid1\n## 1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n## 2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n## 3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n##                                                                                                    Geneid2\n## 1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n## 2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n## 3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\nigr$feature1 <- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 <- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 <- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)## [1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\nigr##   feature\n## 1     IGR\n## 2  AS_IGR\n## 3     IGR\n##                                                                                                                                                                                        rest\n## 1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n## 2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n## 3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n##                                                                                                            Geneid1\n## 1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n## 2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n## 3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n##                                                                                                    Geneid2\n## 1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n## 2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n## 3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n##   feature1 feature2\n## 1      CDS      CDS\n## 2      CDS      CDS\n## 3      CDS      CDS\nbnum = \"b[0-9]{4}\"\nbnum## [1] \"b[0-9]{4}\"\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\nigr$start.gene <- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene <- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum <- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum <- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr <- igr %>% tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") %>% \n  dplyr::select(-comma) %>% \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") %>% \n  dplyr::select(-comma)\nnames(igr)##  [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n##  [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"\nigr##   feature\n## 1     IGR\n## 2  AS_IGR\n## 3     IGR\n##                                                                                                                                                                                        rest\n## 1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n## 2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n## 3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n##                                                                                                            Geneid1\n## 1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n## 2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n## 3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n##                                                                                                    Geneid2\n## 1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n## 2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n## 3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n##   feature1 feature2 start.gene end.gene start.bnum end.bnum\n## 1      CDS      CDS        mlc     ynfL      b1594    b1595\n## 2      CDS      CDS       talB      mog      b0008    b0009\n## 3      CDS      CDS       yoaA     yoaB      b1808    b1809"},{"path":"misc.html","id":"helpful-tutorialsfiles","chapter":"10 Misc","heading":"Helpful tutorials/files","text":"stringr vignette: https://cran.r-project.org/web/packages/stringr/vignettes/stringr.htmlstringr packageJenny Bryan’s STAT 545 notes: https://stat545.com/character-vectors.htmlJenny Bryan’s STAT 545 lab: http://stat545.com/block022_regular-expression.htmlHadley Wickham’s book R Data ScienceregexpalRegExrRegular expression R official document.","code":""},{"path":"misc.html","id":"fun-examples","chapter":"10 Misc","heading":"Fun examples","text":"name Hilary: https://hilaryparker.com/2013/01/30/hilary---poisoned-baby-name--us-history/Trump’s tweets: http://varianceexplained.org/r/trump-tweets/Trump’s tweets, take two: http://varianceexplained.org/r/trump-followup/","code":""},{"path":"compstat.html","id":"compstat","chapter":"11 Computational Statistics","heading":"11 Computational Statistics","text":"","code":""},{"path":"compstat.html","id":"wearing-a-statistics-hat","chapter":"11 Computational Statistics","heading":"11.1 Wearing a Statistics Hat","text":"class, ’ve learned number ways dealing data order communicate information provided data. talked data wrangling, visualization, inference, classification techniques. Many ideas fall within paradigm “data science.” makes statistics? statisticians bring table?Primarily, statisticians good thinking variability. also skeptical. try skeptical. skeptical enough.Important Adage #1: perfect enemy good enough. (Voltaire?)Important Adage #2: models wrong, useful. (G.E.P. Box 1987)good thoughts: http://simplystatistics.org/2014/05/22/10-things-statistics-taught-us--big-data-analysis/statistics profession caught confusing moment: activities preoccupied centuries now limelight, activities claimed bright shiny new, carried (although actually invented ) upstarts strangers. [http://simplystatistics.org/2015/11/24/20-years--data-science--data-driven-discovery--music--genomics/]","code":""},{"path":"compstat.html","id":"samples","chapter":"11 Computational Statistics","heading":"Samples","text":"data sample population. good representation population? got left ? variables missing? Missing random? (Probably .)O’Hagan’s fire department, RAND built computer models replicated , , often fires broke city, predicted quickly fire companies respond . showing areas received faster slower responses, RAND determined companies closed least impact. 1972, RAND recommended closing 13 companies, oddly including busiest fire-prone South Bronx, opening seven new ones, including units suburban neighborhoods Staten Island North Bronx.RAND’s first mistake assuming response time – mediocre measure firefighting operations whole, aspect can easily quantified – factor necessary determining companies opened closed. calculate theoretical response times, RAND needed gather real ones. sample small, unrepresentative poorly compiled data indicated traffic played role quickly fire company responded.models also full mistakes omissions. One assumed fire companies always available respond fires firehouse – true enough Staten Island, rarity places like Bronx, every company neighborhood, sometimes entire borough, fighting fires time. Numerous corners cut, RAND reports routinely dismissing crucial legwork “laborious,” analysts writing data discrepancies “ignored many planning purposes.”http://nypost.com/2010/05/16/--bronx-burned/\nhttp://fivethirtyeight.com/datalab/--bronx-really-burned/","code":""},{"path":"compstat.html","id":"sample-size","chapter":"11 Computational Statistics","heading":"Sample Size","text":"Tiny samples show huge variability; huge samples always give significance (tweets first day class).Sampling distribution \\(\\overline{x}\\) … http://www.rossmanchance.com/applets/OneSample.html?showBoth=1Know real sample size!!! One group project using grocery store data sales measured daily last 3 years. 10 stores… sample size \\(10\\cdot 365\\cdot 3 = 10,950\\)? sample size 10 \\(365\\cdot3 = 1095\\) variables?Also: know whether result average individual whether “significance” statistical whether also practical.","code":""},{"path":"compstat.html","id":"correlation-v.-causation","chapter":"11 Computational Statistics","heading":"Correlation v. Causation","text":"article handwriting appeared October 11, 2006 issue Washington Post. article mentioned among students took essay portion SAT exam 2005-06, wrote cursive style scored significantly higher essay, average, students used printed block letters. Researchers wanted know whether simply writing cursive way increase scores.article also mentioned different study one essay given graders. graders shown cursive version essay graders shown version printed block letters. Researchers randomly decided version grader receive. average score assigned essay cursive style significantly higher average score assigned essay printed block letters.\n(Chance Rossman 2018a)Unless running randomized experiment, always try think many possible confounding variables can.","code":""},{"path":"compstat.html","id":"ensemble-learners","chapter":"11 Computational Statistics","heading":"Ensemble Learners","text":"’ve seen ideas ensembles bagging, random forests, first take home exam (average bootstrap confidence intervals). goal prediction accuracy, average many predictions together. different models use/provide different pieces information, average predictors balance information reduce variability prediction.Note wouldn’t want average set ensemble learners one bad. (E.g., relationship quadratic fit one model quadratic another linear… average worse.)course… world isn’t always prediction. Sometimes describing! Simpler models (e.g., regression) get heart impact specific variable response interest.","code":""},{"path":"compstat.html","id":"supervised-vs.-unsupervised","chapter":"11 Computational Statistics","heading":"Supervised vs. Unsupervised","text":"classification models ’ve discussed supervised learning techniques. word supervised refers fact know response variable training observations. Next , ’ll discuss clustering unsupervised technique – none observations given response variable. example, might want cluster hundred melanoma patients based genetic data. looking patterns groups together, don’t preconceived idea patients belong group.also semi-supervised techniques applied data observations labeled .","code":""},{"path":"compstat.html","id":"regression-to-the-mean","chapter":"11 Computational Statistics","heading":"Regression to the mean","text":"Regression mean phenomenon extreme effects even time. , measurement extreme first measurement, likely closer mean second measurement.Sports Sports Illustrated Jinx (good lucky)Drugs New pharmaceuticals likely less effective seem first.Testing Best scores get worse, worst scores get better (wary interventions)","code":""},{"path":"compstat.html","id":"measuring-accuracy","chapter":"11 Computational Statistics","heading":"Measuring Accuracy","text":"Note set data, observations closer model built model fits entire population. adjust model, run risk overfitting data. (Draw polynomial way overfits data.)Test / training dataCross validation – without cheatingChoosing variables (standardizing!) can make overfit… subsetting removing points analysis.Define metric success stick !Choose algorithm (algorithms!) work, front.Choose hypotheses looking data","code":""},{"path":"compstat.html","id":"exploratory-data-analysis","chapter":"11 Computational Statistics","heading":"Exploratory Data Analysis","text":"want understand dataset, play around . Graph . Look summary statistics. Look bivariate relationships. Plot colors markers.","code":""},{"path":"compstat.html","id":"computational-statistics","chapter":"11 Computational Statistics","heading":"11.2 Computational Statistics","text":"course mix computational statistics data science procedures. myriad topics covered. Indeed, many basic important statistical ideas computational counterparts allow us perform analyses calculus doesn’t provide neat clean solutions. ’ve seen haven’t seen include:Hypothesis Testing: Permutation testsConfidence Intervals: BootstrappingParameter Estimation: EM algorithmBayesian Analysis: Gibbs sampler, Metropolis-Hasting algorithmPolynomial regression: Smoothing methods (e.g., loess)Prediction: Supervised learning methods","code":""},{"path":"compstat.html","id":"always-consider-impact.","chapter":"11 Computational Statistics","heading":"11.3 Always consider impact.","text":"Keeping asking :\nstay accountable work?\nmight others impacted ’ve created?\ndata come , biases might inherent?\npopulation appropriate inferential claims ’m making?\nmight individual’s privacy anonymity impacted ’ve created?\npossible work misinterpreted misused?\nKeeping asking :stay accountable work?might others impacted ’ve created?data come , biases might inherent?population appropriate inferential claims ’m making?might individual’s privacy anonymity impacted ’ve created?possible work misinterpreted misused?Introduction Everyday Information Architecture Lisa Maria Martin","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
