[
["index.html", "Computational Statistics Class Information", " Computational Statistics Jo Hardin 2019-12-02 Class Information Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning (James et al. 2013) by James, Witten, Hastie, and Tibshiran; Modern Data Science (Baumer, Kaplan, and Horton 2017) with R by Baumer, Kaplan, and Horton; Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving (Nolan and Temple Lang 2015) by Nolan and Temple Lang; and Visual and Statistical Thinking: Displays of Evidence for Making Decisions (Tufte 1997) by Tufte. You are responsible for reading the relevant chapters in the text. The texts are very good &amp; readable, so you should use them. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chap Notes Section 9/3/19 Intro to Data / R ISL1 1 [intro], 1.1 [Sep3] 9/5/19 Intro to GitHub 1.4 [Sep5], 1.5 [repro] 9/10/19 Data Viz VST 2.1 [Sep10], 2.3 [thoughts] 9/12/19 MDS2 2.4 [Sep12], 2.5 [decon], 2.5.1 [gg] 9/17/19 Data Wrangling MDS4 3.1 [Sep17], 3.2 [structure] 9/19/19 3.1 [Sep17], 3.5 [highverb] 9/24/19 Simulating MDS8 4.1 [Sep24], 4.2 [sim models] 9/26/19 4.3 [Sep26], 4.4 [sim sens] 10/1/19 Permutations ISCAM 5.1 [Oct1], 5.2 [inf algs] 10/3/19 5.3 [Oct3], 5.4 [perm test] 10/8/19 Bootstrapping ISL 5 6.1 [Oct8], 6.3 [notation] 10/10/19 6.4 [Oct10],6.5 [BS CI] 10/15/19 Catch-up 10/17/19 In-class Exam 10/22/19 Fall Break 10/24/19 Ethics MDS6 7.1 [Oct24], 7 ethics 10/29/19 \\(k\\)-NN &amp; CV ISL sections 2.2.3 and 4.6.5 &amp; 5.1 8.1 [Oct29], 8.3 knn 8.2 [CV] 10/31/19 CART ISL 8.1 8.4 11/5/19 bagging ISL section 8.2.1 8.6 [Nov5], 8.7 bagging 11/7/19 Random Forests ISL 8.2.2 8.8 [Nov7], 8.9 [rf] 11/12/19 SVM ISL chapter 9 8.11 [Nov12], 8.12.0.1 [linear svm] 11/14/19 8.12.1 [Nov14], 8.12.2 [notlin svm] 11/19/19 8.12.3 [Nov19], 8.12.3.1 [kernels] 11/21/19 \\(k\\)-means ISL chapter 10.3.2 9.1 [Nov21], 9.4 [HierClus], 9.5 [PartClus] 11/26/19 Project 10.1 [Nov26] 12/3/19 Catch-up 12/5/19 In-class Exam 2 12/10/19 Reg Expr 10.7 [Dec10], 10.8 [RegExpr] References "],
["intro.html", "Chapter 1 Introduction 1.1 9/3/19 Agenda 1.2 Course Logistics 1.3 Course Content 1.4 9/5/19 Agenda 1.5 Reproducibility 1.6 Data Examples", " Chapter 1 Introduction 1.1 9/3/19 Agenda Questionnaire Syllabus &amp; Course Outline Stitch Fix Algorithm College Rankings Can Twitter predict election results? 1.2 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Figure 1.1: Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population What is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years. Who should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science. What are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Is there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability? When should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors. What is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class. What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Figure 1.2: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.3: Jessica Ward, PhD student at Newcastle University 1.3 Course Content 1.3.1 Topics Computational Statistics can be a very large umbrella for many ideas. Indeed, sometimes the topics can seem somewhat disjointed. Below, I’ve categorized the topics we will cover into four groups. The four different broad topics all play different roles and can be more or less important depending on the problem at hand. None of the topics should exist on their own, because only with the bigger focus on all topics will any sort of data analysis / interpretation be accurate and compelling. Letting the computer help: R, RStudio, GitHub, Reproducibility, Data Viz, Data Wrangling Statistics: Simulating, Randomization / Permutation Tests, Bootstrapping, Ethics Machine Learning: Classification, Clustering, Regular Expressions Mathematics: Support Vector Machines 1.3.2 Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. A parameter is a numerical measurement of the population. We never know the true value of the parameter. An estimator is a function of the unobserved data that tries to approximate the unknown parameter value. An estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.] One of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015) With thanks to Ben Baumer for perspective and sharing course materials. 1.3.3 The Workflow Figure 1.4: A schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015) Figure 1.5: Stitch Fix Algorithms Tour 1.3.4 Principles for the Data Science Process tl;dr (Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.) Duncan Temple Lang, University of California, Davis Duncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.] Duncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science. Temple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process: Asking a general question. Refining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood. Access to data. This is unrelated to the science but does require computational skill. Transforming to data structures. Exploratory data analyses to understand the data and determine whether the results will scale. This is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results. Temple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student: Mapping the general question to a statistical framework. Understanding the scope of inference, sampling, biases, and limitations. Exploratory data analyses, including missing values, data quality, cleaning, matching, and fusing. Understanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability. Conditional dependence and heterogeneity. Dimension reduction, variable selection, and sparsity. Spurious relationships and multiple testing. Parameter estimation versus “black box” prediction and classification. Diagnostics, residuals, and comparing models. Quantifying the uncertainty of a model. Sampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed. Statistical accuracy versus computational complexity and efficiency. Temple Lang then briefly discussed some of the practical aspects of computing, including the following: Accessing data. Manipulating raw data. Data structures and storage, including correlated data. Visualization at all stages (particularly in exploratory data analyses and conveying the results). Parallel computing, which can be challenging for a new student. Translating high-level descriptions to optimal programs. During the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).] A workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets. 1.4 9/5/19 Agenda Design Challenge Not So Standard Deviations Reproducibility &amp; GitHub 1.5 Reproducibility 1.5.1 Need for Reproducibility Figure 1.6: slide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016 Example 1 Science retracts gay marriage paper without agreement of lead author LaCour In May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior. Science Editor-in-Chief Marcia McNutt: Original survey data not made available for independent reproduction of results. Survey incentives misrepresented. Sponsorship statement false. Two Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked. Methods we’ll discuss can’t prevent this, but they can make it easier to discover issues. Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent Example 2 Seizure study retracted after authors realize data got “terribly mixed” From the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates: The article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness. Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/ Example 3 Bad spreadsheet merge kills depression paper, quick fix resurrects it The authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct. Original conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression … Revised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression … Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/ Example 4 PNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/ 1.5.2 The reproducible data analysis process Scriptability \\(\\rightarrow\\) R Literate programming \\(\\rightarrow\\) R Markdown Version control \\(\\rightarrow\\) Git / GitHub Scripting and literate programming Donald Knuth “Literate Programming” (1983) Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do. The ideas of literate programming have been around for many years! and tools for putting them to practice have also been around but they have never been as accessible as the current tools Reproducibility checklist Are the tables and figures reproducible from the code and data? Does the code actually do what you think it does? In addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?) Can the code be used for other data? Can you extend the code to do other things? Tools: R &amp; R Studio See this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&amp;feature=youtu.be You must use both R and RStudio software programs R does the programming R Studio brings everything together You may use Pomona’s server: https://rstudio.pomona.edu/ See course website for getting started: http://research.pomona.edu/johardin/math154f19/ Figure 1.7: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.8: Jessica Ward, PhD student at Newcastle University Tools: GitHub You must submit your assignments via GitHub Follow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/ Follow Jacob Fiksel’s advice on how to connect to our classroom: https://github.com/jfiksel/github-classroom-for-students Tools: a GitHub merge conflict (demo) On GitHub (on the web) edit the README document and Commit it with a message describing what you did. Then, in RStudio also edit the README document with a different change. Commit your changes Try to push \\(\\rightarrow\\) you’ll get an error! Try pulling Resolve the merge conflict and then commit and push As you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important. Figure 1.9: https://xkcd.com/1597/ Steps for weekly homework You will get a link to the new assignment (clicking on the link will create a new private repo) Use R Studio New Project, version control, Git Clone the repo using SSH If it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd Do the assignment commit and push after every problem All necessary files must be in the same folder (e.g., data) 1.6 Data Examples What can/can’t Data Science Do? Can model the data at hand! Can find patterns &amp; visualizations in large datasets. Can’t establish causation. Can’t represent data if it isn’t there. Stats / Data Science / Math are not apolitical/agnostic “Inner city crime is reaching record levels” (Donald Trump, 8/30/16) “The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16) “Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16) “We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16) “If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16) “Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16) Source: http://www.politifact.com/truth-o-meter/statements/ 1.6.1 College Rankings Systems Cheating Bucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html) Gaming the system Point by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/) No way to measure “quality of education” What is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!). 1.6.2 Trump and Twitter Analysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone. Aug 9, 2016 http://varianceexplained.org/r/trump-tweets/ My analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures. Aug 9, 2017 http://varianceexplained.org/r/trump-followup/ There is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post. I’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it. I’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement. 1.6.3 Can Twitter Predict Election Results? In 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013) What makes using Tweets to predict elections relevant to our class? (See Baumer (2015).) The data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet. The analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets). The dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed. The researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses. Activity Spend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions. working paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423 published in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449. editorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html editorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html editorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/ Statistics Hat Write a sentence summarizing the findings of the paper. Discuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently? should be proportion for the response variable. The bizarre scaling could dramatically change the results dots could then be scaled in proportion to the number of tweets linear fit may be questionable. How would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement? Interpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units. Discuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election? \\(R^2\\) is way higher after control variables are included, but duh! the full model will likely do a better job of predicting Why do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election? After controlling for how many Republicans are in the district, most of the effect disappears While the coefficient of the main term is still statistically significant, the size of the coefficient (155 +/- 43 votes) is of little practical significance Do you think the study holds water? Why or why not? What are the shortcomings of this study? Not really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model? Data Scientist Hat Imagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor. What steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform. What computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities &amp; knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in? How much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb. Advantages Cheap Can measure any political race (not just the wealthy ones). Disadvantages Is it really reflective of the voting populace? Who would it bias toward? Does simple mention of a candidate always reflect voting patterns? When wouldn’t it? Margin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\). Tweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling? Can’t measure any demographic information. What could be done differently? Gelman: look only at close races Gelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.” Gelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.” Gelman: Make the data publicly available so that others can try to reproduce the results Tweeting and R The twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions. References "],
["visualization.html", "Chapter 2 Visualization 2.1 9/10/19 Agenda 2.2 Examples 2.3 Thoughts on Plotting 2.4 9/12/19 Agenda 2.5 Deconstructing a graph", " Chapter 2 Visualization Data visualization is integral to understanding both data and models. Computational statistics and data science sometimes focus on models or resulting predictions from the models. But there is no doubt that the structure and format of the data are the key to whether or not a model is appropriate or good. A good data analyst will always spend a lot of time and effort on exploratory data analysis, much of which includes making as many visualizations of the data as possible. 2.1 9/10/19 Agenda Cholera: what went (didn’t go) well with the graphics? Challenger: what didn’t go (went) well with the graphics? Thoughts on plotting (with example(s)) Should have read: Tufte (1997) One more great reference is the following text: Fundamentals of Data Visualization by Wilke at http://serialmentor.com/dataviz/ Depending on the introductory (or other) statistics classes you’ve had, your instructor may have focused more or less on visualizations in class. They (I) may have even said something like making visualizations are incredibly important to the entire data analysis process. But even if you buy the perspective, why is it that we don’t see more good graphics in our analyses? Andrew Gelman (Gelman 2011) responds by stating, “Good statistical graphics are hard to do, much harder than running regressions and making tables.” Our goal will be to create graphics and visualizations that convey statistical information. Nolan (Nolan and Perrett 2016) describes three important ways that graphics can be used to convey statistical information. The “guiding principles” will be used as a way of evaluating others’ figures as well as a metric for creating our own visualizations to help with statistical analysis. Make the data stand out The important idea here is to find anything unusual in the data. Are there patterns? Outliers? What are the bounds of the variables? How should the axes be scaled? Are transformations warranted? Facilitate comparison The second item allows us to consider the research questions at hand. What are the important variables? How do we emphasize them? Which variables should be plotted together? Can they be super-imposed? Does color, plotting character, size of plot character help to bring out the important relationships? Be aware of over plotting and issues of color blindness! http://colorbrewer2.org/ Add information Plots should also add context to the comparison. Figure legends, axes scales, and reference markers (e.g., a line at \\(y=x\\)) go a long way toward helping the reader understand your message. Captions should be self-contained (and not assume the user has also read your text) and descriptive; they should summarize the content of the figure and the conclusion related to the message you want to convey. Randy Pruim asks the following question to decide whether or not a plot is good: Does my plot make the comparisons I am interested in… easily? and accurately? 2.2 Examples The first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased for $7 at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download. An aside Generally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi As mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (E. Tufte 1997). “An essential analytic task in making decisions based on evidence is to understand how things work.” Making decisions based on evidence requires the appropriate display of that evidence.&quot; Back to the examples… 2.2.1 Cholera via Tufte In September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after. Why was John Snow successful at solving the problem? Some thoughts to consider (as reported in E. Tufte (1997)): The bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means. Data in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area. Comparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease. Alternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case. Did removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline? Assessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told? 2.2.2 Challenger via Tufte John Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F). Unlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated}! The biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what?? The engineers who understood the problem created tables and engineering graphs which were Not visually appealing. Not decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”) There was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility. The information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure. Missing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.) Anecdotal evidence was misconstrued: SRM-15 at 57F at the most damage, but SRM-22 at 75F had the second most damage. In the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F. Tufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order. As with the cholera outbreak, a persuasive argument could have been made if the visualizations had been in context plot data versus temperature not time!, used appropriate comparisons: as compared with what?, consider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and the graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?. Tufte (E. Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms. Figure 1.2: The graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (E. Tufte 1997) 2.3 Thoughts on Plotting 2.3.1 Advice Basic plotting Avoid having other graph elements interfere with data Use visually prominent symbols Avoid over-plotting (One way to avoid over plotting: Jitter the values) Different values of data may obscure each other Include all or nearly all of the data Fill data region Eliminate superfluous material Chart junk &amp; stuff that adds no meaning, e.g. butterflies on top of barplots, background images Extra tick marks and grid lines Unnecessary text and arrows Decimal places beyond the measurement error or the level of difference Facilitate Comparisons Put juxtaposed plots on same scale Make it easy to distinguish elements of superposed plots (e.g. color) Emphasizes the important difference Comparison: volume, area, height (be careful, volume can seem bigger than you mean it to) Choosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.) Keep scales on x and y axes the same for both plots to facilitate the comparison Zoom in to focus on the region that contains the bulk of the data Keep the scale the same throughout the plot (i.e. don’t change it mid-axis) Origin need not be on the scale Choose a scale that improves resolution Avoid jiggling the baseline How to make a plot information rich Describe what you see in the caption Add context with reference markers (lines and points) including text Add legends and labels Use color and plotting symbols to add more information Plot the same thing more than once in different ways/scales Reduce clutter Captions should Be comprehensive Self-contained Describe what has been graphed Draw attention to important features Describe conclusions drawn from graph Good Plot Making Practice Put major conclusions in graphical form Provide reference information Proof read for clarity and consistency Graphing is an iterative process Multiplicity is OK, i.e. two plots of the same variable may provide different messages Make plots data rich Creating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data. 2.3.2 An example from Information is Beautiful (See HW2 for details on R code) Consider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph. Figure 1.3: http://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png I have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations. Figure 1.4: Calories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot? Data retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0 2.3.3 Assessing Graphics (and Other Analyses) Critical Task Needs Improvement Basic Surpassed Computation Perform computations Computations contain errors and extraneous code Computations are correct but contain extraneous / unnecessary computations Computations are correct and properly identified and labeled Analysis Choose and carry out analysis appropriate for data and content(s) Choice of analysis is overly simplistic, irrelevant, or missing key component Analysis appropriate, but incomplete, or not important features and assumptions not made explicit Analysis appropriate, complete, advanced, relevant, and informative Synthesis Identify key features of the analysis, and interpret results (including context) Conclusions are missing, incorrect, or not made based on results of analysis Conclusions reasonable, but is partially correct or partially complete Make relevant conclusions explicitly connect to analysis and to context Visual presentation Communicate findings graphically clearly, precisely, and concisely Inappropriate choice of plots; poorly labeled plots; plots missing Plots convey information correctly but lack context for interpretation Plots convey information correctly with adequate / appropriate reference information Written Communicate findings clearly, precisely, and concisely Explanation is illogical, incorrect, or incoherent Explanation is partially correct but incomplete or unconvincing Explanation is correct, complete, and convincing A rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016). 2.4 9/12/19 Agenda Grammar of graphics ggplot 2.5 Deconstructing a graph 2.5.1 The Grammar of Graphics (gg) Yau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence. One great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?) The most important questions you can ask with respect to creating figures are: What do we want R to do? (What is the goal?) What does R need to know? Yau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2017), chapter 2.) Visual Cues: the aspects of the figure where we should focus. Position (numerical) where in relation to other things? Length (numerical) how big (in one dimension)? Angle (numerical) how wide? parallel to something else? Direction (numerical) at what slope? In a time series, going up or down? Shape (categorical) belonging to what group? Area (numerical) how big (in two dimensions)? Beware of improper scaling! Volume (numerical) how big (in three dimensions)? Beware of improper scaling! Shade (either) to what extent? how severely? Color (either) to what extent? how severely? Beware of red/green color blindness. Coordinate System: rectangular, polar, geographic, etc. Scale: numeric (linear? logarithmic?), categorical (ordered?), time Context: in comparison to what (think back to ideas from Tufte) Order Matters Cues Together What are the visual cues on the plot? position? length? shape? area/volume? shade/color? coordinate System? scale? What are the visual cues on the plot? position? length? shape? area/volume? shade/color? coordinate System? scale? What are the visual cues on the plot? position? length? shape? area/volume? shade/color? coordinate System? scale? 2.5.1.1 The grammar of graphics in ggplot2 geom: the geometric “shape” used to display data bar, point, line, ribbon, text, etc. aesthetic: an attribute controlling how geom is displayed with respect to variables x position, y position, color, fill, shape, size, etc. scale: adjust information in the aesthetic to map onto the plot particular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values. guide: helps user convert visual data back into raw data (legends, axes) stat: a transformation applied to data before geom gets it example: histograms work on binned data 2.5.2 ggplot2 In ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2017). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births. Goals What I will try to do give a tour of ggplot2 explain how to think about plots the ggplot2 way prepare/encourage you to learn more later What I can’t do in one session show every bell and whistle make you an expert at using ggplot2 Getting help One of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/ ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Look at the end of the presentation. More help options there. require(mosaic) require(lubridate) # package for working with dates data(Births78) # restore fresh version of Births78 head(Births78, 3) ## date births wday year month day_of_year day_of_month day_of_week ## 1 1978-01-01 7701 Sun 1978 1 1 1 1 ## 2 1978-01-02 7527 Mon 1978 1 2 2 2 ## 3 1978-01-03 8825 Tue 1978 1 3 3 3 How can we make the plot? Two Questions: What do we want R to do? (What is the goal?) What does R need to know? data source: Births78 aesthetics: date -&gt; x births -&gt; y points (!) Goal: scatterplot = a plot with points ggplot() + geom_point() What does R need to know? data source: data = Births78 aesthetics: aes(x = date, y = births) How can we make the plot? What has changed? new aesthetic: mapping color to day of week Adding day of week to the data set The wday() function in the lubridate package computes the day of the week from a date. Births78 &lt;- Births78 %&gt;% mutate(wday = lubridate::wday(date, label=TRUE)) ggplot(data=Births78) + geom_point(aes(x=date, y=births, color=wday))+ ggtitle(&quot;US Births in 1978&quot;) How can we make the plot? Now we use lines instead of dots ggplot(data=Births78) + geom_line(aes(x=date, y=births, color=wday)) + ggtitle(&quot;US Births in 1978&quot;) How can we make the plot? Now we have two layers, one with points and one with lines ggplot(data=Births78, aes(x=date, y=births, color=wday)) + geom_point() + geom_line()+ ggtitle(&quot;US Births in 1978&quot;) The layers are placed one on top of the other: the points are below and the lines are above. data and aes specified in ggplot() affect all geoms Alternative Syntax Births78 %&gt;% ggplot(aes(x=date, y=births, color=wday)) + geom_point() + geom_line()+ ggtitle(&quot;US Births in 1978&quot;) What does adding the color argument do? Births78 %&gt;% ggplot(aes(x=date, y=births, color=&quot;navy&quot;)) + geom_point() + ggtitle(&quot;US Births in 1978&quot;) Because there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy. Setting vs. Mapping If we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable: Births78 %&gt;% ggplot(aes(x=date, y=births)) + # map x &amp; y geom_point(color = &quot;navy&quot;) + # set color ggtitle(&quot;US Births in 1978&quot;) Note that color = &quot;navy&quot; is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting. How can we make the plot? Births78 %&gt;% ggplot(aes(x=date, y=births)) + geom_line(aes(color=wday)) + # map color here geom_point(color=&quot;navy&quot;) + # set color here ggtitle(&quot;US Births in 1978&quot;) ggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults. good practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah() Setting vs. Mapping (again) Information gets passed to the plot via: map the variable information inside the aes (aesthetic) command set the non-variable information outside the aes (aesthetic) command Other geoms apropos(&quot;^geom_&quot;) [1] &quot;geom_abline&quot; &quot;geom_area&quot; &quot;geom_ash&quot; [4] &quot;geom_bar&quot; &quot;geom_barh&quot; &quot;geom_bin2d&quot; [7] &quot;geom_blank&quot; &quot;geom_boxplot&quot; &quot;geom_boxploth&quot; [10] &quot;geom_col&quot; &quot;geom_colh&quot; &quot;geom_contour&quot; [13] &quot;geom_count&quot; &quot;geom_crossbar&quot; &quot;geom_crossbarh&quot; [16] &quot;geom_curve&quot; &quot;geom_density&quot; &quot;geom_density_2d&quot; [19] &quot;geom_density2d&quot; &quot;geom_dotplot&quot; &quot;geom_errorbar&quot; [22] &quot;geom_errorbarh&quot; &quot;geom_errorbarh&quot; &quot;geom_freqpoly&quot; [25] &quot;geom_hex&quot; &quot;geom_histogram&quot; &quot;geom_histogramh&quot; [28] &quot;geom_hline&quot; &quot;geom_jitter&quot; &quot;geom_label&quot; [31] &quot;geom_line&quot; &quot;geom_linerange&quot; &quot;geom_linerangeh&quot; [34] &quot;geom_lm&quot; &quot;geom_map&quot; &quot;geom_path&quot; [37] &quot;geom_point&quot; &quot;geom_pointrange&quot; &quot;geom_pointrangeh&quot; [40] &quot;geom_polygon&quot; &quot;geom_qq&quot; &quot;geom_qq_line&quot; [43] &quot;geom_quantile&quot; &quot;geom_rangeframe&quot; &quot;geom_raster&quot; [46] &quot;geom_rect&quot; &quot;geom_ribbon&quot; &quot;geom_rug&quot; [49] &quot;geom_segment&quot; &quot;geom_sf&quot; &quot;geom_sf_label&quot; [52] &quot;geom_sf_text&quot; &quot;geom_smooth&quot; &quot;geom_spline&quot; [55] &quot;geom_spoke&quot; &quot;geom_step&quot; &quot;geom_text&quot; [58] &quot;geom_tile&quot; &quot;geom_tufteboxplot&quot; &quot;geom_violin&quot; [61] &quot;geom_violinh&quot; &quot;geom_vline&quot; help pages will tell you their aesthetics, default stats, etc. ?geom_area # for example Let’s try geom_area Births78 %&gt;% ggplot(aes(x=date, y=births, fill=wday)) + geom_area()+ ggtitle(&quot;US Births in 1978&quot;) Using area does not produce a good plot over plotting is hiding much of the data extending y-axis to 0 may or may not be desirable. Side note: what makes a plot good? Most (all?) graphics are intended to help us make comparisons How does something change over time? Do my treatments matter? How much? Do men and women respond the same way? Key plot metric: Does my plot make the comparisons I am interested in easily, and accurately? Time for some different data HELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial head(HELPrct) ## age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female ## 1 37 1 yes 49 3 177 225 0 NA 0 ## 2 37 1 yes 30 22 2 NA 0 NA 0 ## 3 26 1 yes 39 0 3 365 20 NA 0 ## 4 39 1 yes 15 2 189 343 0 1 1 ## 5 32 1 yes 39 12 2 57 0 1 0 ## 6 47 1 yes 6 1 31 365 0 NA 1 ## sex g1b homeless i1 i2 id indtot linkstatus link mcs pcs ## 1 male yes housed 13 26 1 39 1 yes 25.111990 58.41369 ## 2 male yes homeless 56 62 2 43 NA &lt;NA&gt; 26.670307 36.03694 ## 3 male no housed 0 0 3 41 0 no 6.762923 74.80633 ## 4 female no housed 5 5 4 28 0 no 43.967880 61.93168 ## 5 male no homeless 10 13 5 38 1 yes 21.675755 37.34558 ## 6 female no housed 4 4 6 29 0 no 55.508991 46.47521 ## pss_fr racegrp satreat sexrisk substance treat avg_drinks max_drinks ## 1 0 black no 4 cocaine yes 13 26 ## 2 1 white no 7 alcohol yes 56 62 ## 3 13 black no 2 heroin no 0 0 ## 4 11 white yes 4 heroin no 5 5 ## 5 10 black no 6 cocaine no 10 13 ## 6 5 black no 5 cocaine yes 4 4 Subjects admitted for treatment for addiction to one of three substances. Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance)) + geom_bar()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Hmm. What’s up with y? stat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values. Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Who are the people in the study? library(scales) HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar() + scale_y_continuous(labels = percent)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar(position=&quot;fill&quot;) + scale_y_continuous(&quot;actually, percent&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How old are people in the HELP study? HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice the messages stat_bin: Histograms are not mapping the raw data but binned data. stat_bin() performs the data transformation. binwidth: a default binwidth has been selected, but we should really choose our own. Setting the binwidth manually HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram(binwidth=2)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How old are people in the HELP study? – Other geoms HELPrct %&gt;% ggplot(aes(x=age)) + geom_freqpoly(binwidth=2)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_density()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Selecting stat and geom manually Every geom comes with a default stat for simple cases, the stat is stat_identity() which does nothing we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + geom_line(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Selecting stat and geom manually Every stat comes with a default geom, every geom with a default stat we can specify stats instead of geom, if we prefer we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + stat_density( geom=&quot;line&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) More combinations HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3) + geom_line(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_area(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3, aes(size=..count..)) + geom_line(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How much do they drink? (i1) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_histogram()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_density()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_area(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Covariates: Adding in more variables Using color and linetype: HELPrct %&gt;% ggplot(aes(x=i1, color=substance, linetype=sex)) + geom_line(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Using color and facets HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( . ~ sex )+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( sex ~ . )+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Boxplots Boxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”. The quantitative variable must be y, and there must be an additional x variable. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Horizontal boxplots Horizontal boxplots are obtained by flipping the coordinate system: HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) coord_flip() may be used with other plots as well to reverse the roles of x and y on the plot. Axes scaling with boxplots We can scale the continuous axis HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_trans(y=&quot;log&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Give me some space We’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot(position=position_dodge(width=1)) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Issues with bigger data require(NHANES) dim(NHANES) ## [1] 10000 76 NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) Although we can see a generally positive association (as we would expect), the over plotting may be hiding information. Using alpha (opacity) One way to deal with over plotting is to set the opacity low. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) geom_density2d Alternatively (or simultaneously) we might prefer a different geom altogether. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_density2d() + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_jitter(alpha=.6) + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Things I haven’t mentioned (much) coords (coord_flip() is good to know about) themes (for customizing appearance) position (position_dodge(), position_jitterdodge(), position_stack(), etc.) transforming axes require(ggthemes) ggplot(Births78, aes(x=date, y=births)) + geom_point() + theme_wsj() ggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge()) + geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) A little bit of everything ggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge(width=1)) + geom_point(aes(fill=sex), alpha=.5, position=position_jitterdodge(dodge.width=1)) + facet_wrap(~homeless)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Want to learn more? docs.ggplot2.org/ Winston Chang’s: R Graphics Cookbook What else can we do? shiny interactive graphics / modeling https://shiny.rstudio.com/ plotly Plotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function. https://plot.ly/ggplot2/getting-started/ Dynamic documents combination of RMarkdown, ggvis, and shiny References "],
["wrang.html", "Chapter 3 Data Wrangling 3.1 9/17/19 Agenda 3.2 Structure of Data 3.3 R examples, basic verbs 3.4 9/19/19 Agenda 3.5 Higher Level Data Verbs 3.6 R examples, higher level verbs 3.7 reprex", " Chapter 3 Data Wrangling As with data visualization, data wrangling is a fundamental part of being able to accurately, reproducibly, and efficiently work with data. The approach taken in the following chapter is based on the philosophy of tidy data and takes many of its precepts from database theory. If you have done much work in SQL, the functionality and approach of tidy data will feel very familiar. The more adept you are at data wrangling, the more effective you will be at data analysis. 3.1 9/17/19 Agenda Tidy Data (structure of data) Piping / chaining Basic data Verbs babynames / NHANES examples Information is what we want, but data are what we’ve got. (Kaplan 2015) Embrace all the ways to get help! cheat sheets: https://www.rstudio.com/resources/cheatsheets/ tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/ pivoting: https://tidyr.tidyverse.org/articles/pivot.html google what you need and include R tidy or tidyverse 3.2 Structure of Data For plotting, analyses, model building, etc., it’s important that the data be structured in a very particular way. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham (2014). Tidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and *every} column is a variable. No exceptions. Creating tidy data is not trivial. We work with objects (often data tables), functions, and arguments (often variables). The Active Duty data are not tidy! What are the cases? How are the data not tidy? What might the data look like in tidy form? Suppose that the case was “an individual in the armed forces.” What variables would you use to capture the information in the following table? https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794 Problem: totals and different sheets Better for R: longer format with columns - grade, gender, status, service, count (case is still the total pay grade) Case is individual (?): grade, gender, status, service (no count because each row does the counting) 3.2.1 Building Tidy Data Within R (really within any type of computing language, Python, SQL, Java, etc.), we need to understand how to build data using the patterns of the language. Some things to consider: object_name = function_name(arguments) is a way of using a function to create a new object. object_name = data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side. object_name = data_table %&gt;% function_name(arguments) %&gt;% function_name(arguments) is extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%. 3.2.2 Examples of Chaining The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) 3.2.2.1 Little Bunny Foo Foo From Hadley Wickham, how to think about tidy data. Little bunny Foo Foo Went hopping through the forest Scooping up the field mice And bopping them on the head The nursery rhyme could be created by a series of steps where the output from each step is saved as an object along the way. foo_foo &lt;- little_bunny() foo_foo_1 &lt;- hop(foo_foo, through = forest) foo_foo_2 &lt;- scoop(foo_foo_2, up = field_mice) foo_foo_3 &lt;- bop(foo_foo_2, on = head) Another approach is to concatenate the functions so that there is only one output. bop( scoop( hop(foo_foo, through = forest), up = field_mice), on = head) Or even worse, as one line: bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head))) Instead, the code can be written using the pipe in the order in which the function is evaluated: foo_foo %&gt;% hop(through = forest) %&gt;% scoop(up = field_mice) %&gt;% bop(on = head) babynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys. The babynames data table in the babynames package comes from the Social Security Administration’s listing of the names givens to babies in each year, and the number of babies of each sex given that name. (Only names with 5 or more babies are published by the SSA.) 3.2.3 Data Verbs (on single data frames) Super important resource: The RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf Data verbs take data tables as input and give data tables as output (that’s how we can use the chaining syntax!). We will use the R package dplyr to do much of our data wrangling. Below is a list of verbs which will be helpful in wrangling many different types of data. See the Data Wrangling cheat sheet from RStudio for additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf} sample_n() take a random row(s) head() grab the first few rows tail() grab the last few rows filter() removes unwanted *cases} arrange() reorders the cases select() removes unwanted *variables} (and rename() ) distinct() returns the unique values in a table mutate() transforms the variable (and transmute() like mutate, returns only new variables) group_by() group_by tells R that SUCCESSIVE functions keep in mind that there are groups of items. So group_by only makes sense with variables later on (like summarize). summarize() collapses a data frame to a single row. Not useful yet (will be useful with group_by()). Some functions that are used within summarize() include: \\begin{itemize} min(), max(), mean(), sum(), sd(), median(), and IQR() n(): number of observations in the current group n_distinct(x): count the number of unique values in x first_value(x), last_value(x) and nth_value(x, n): work similarly to x[1], x[length(x)], and x[n] 3.3 R examples, basic verbs 3.3.1 Datasets starwars is from dplyr , although originally from SWAPI, the Star Wars API, http://swapi.co/. NHANES From ?NHANES: NHANES is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC). babynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys. (Only names with 5 or more babies are published by the SSA.) 3.3.2 Examples of Chaining library(babynames) babynames %&gt;% nrow() ## [1] 1924665 babynames %&gt;% names() ## [1] &quot;year&quot; &quot;sex&quot; &quot;name&quot; &quot;n&quot; &quot;prop&quot; babynames %&gt;% glimpse() ## Observations: 1,924,665 ## Variables: 5 ## $ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,… ## $ sex &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;,… ## $ name &lt;chr&gt; &quot;Mary&quot;, &quot;Anna&quot;, &quot;Emma&quot;, &quot;Elizabeth&quot;, &quot;Minnie&quot;, &quot;Margaret&quot;, … ## $ n &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288,… ## $ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843,… babynames %&gt;% head() ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 babynames %&gt;% tail() ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2017 M Zyhier 5 0.00000255 ## 2 2017 M Zykai 5 0.00000255 ## 3 2017 M Zykeem 5 0.00000255 ## 4 2017 M Zylin 5 0.00000255 ## 5 2017 M Zylis 5 0.00000255 ## 6 2017 M Zyrie 5 0.00000255 babynames %&gt;% sample_n(size=5) ## # A tibble: 5 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1997 M Jaqwon 5 0.0000025 ## 2 1988 F Jamera 7 0.00000364 ## 3 2001 M Othon 6 0.0000029 ## 4 2001 M Dean 744 0.000360 ## 5 2006 F Ngoc 8 0.00000383 babynames %&gt;% mosaic::favstats(n ~ sex, data = .) ## sex min Q1 median Q3 max mean sd n missing ## 1 F 5 7 11 31 99686 151.4294 1180.557 1138293 0 ## 2 M 5 7 12 33 94756 223.4940 1932.338 786372 0 3.3.3 Data Verbs Taken from the dplyr tutorial: http://dplyr.tidyverse.org/ 3.3.3.1 Starwars library(dplyr) starwars %&gt;% dim() ## [1] 87 13 starwars %&gt;% names() ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;gender&quot; &quot;homeworld&quot; &quot;species&quot; ## [11] &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; starwars %&gt;% head() ## # A tibble: 6 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Leia… 150 49 brown light brown 19 female ## 6 Owen… 178 120 brown, gr… light blue 52 male ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% mosaic::favstats(mass~gender, data = .) ## gender min Q1 median Q3 max mean sd n ## 1 female 45 49.25 52.5 55.90 75 54.02000 8.37215 10 ## 2 hermaphrodite 1358 1358.00 1358.0 1358.00 1358 1358.00000 NA 1 ## 3 male 15 76.50 80.0 87.25 159 81.00455 28.22371 44 ## 4 none 140 140.00 140.0 140.00 140 140.00000 NA 1 ## missing ## 1 9 ## 2 0 ## 3 18 ## 4 1 starwars %&gt;% dplyr::filter(species == &quot;Droid&quot;) ## # A tibble: 5 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 2 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 3 R5-D4 97 32 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 4 IG-88 200 140 none metal red 15 none ## 5 BB8 NA NA none none black NA none ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% dplyr::filter(species != &quot;Droid&quot;) %&gt;% mosaic::favstats(mass~gender, data = .) ## gender min Q1 median Q3 max mean sd n ## 1 female 45 50.0 55 56.20 75 54.68889 8.591921 9 ## 2 hermaphrodite 1358 1358.0 1358 1358.00 1358 1358.00000 NA 1 ## 3 male 15 76.5 80 87.25 159 81.00455 28.223707 44 ## missing ## 1 7 ## 2 0 ## 3 16 starwars %&gt;% dplyr::select(name, ends_with(&quot;color&quot;)) ## # A tibble: 87 x 4 ## name hair_color skin_color eye_color ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker blond fair blue ## 2 C-3PO &lt;NA&gt; gold yellow ## 3 R2-D2 &lt;NA&gt; white, blue red ## 4 Darth Vader none white yellow ## 5 Leia Organa brown light brown ## 6 Owen Lars brown, grey light blue ## 7 Beru Whitesun lars brown light blue ## 8 R5-D4 &lt;NA&gt; white, red red ## 9 Biggs Darklighter black light brown ## 10 Obi-Wan Kenobi auburn, white fair blue-gray ## # … with 77 more rows starwars %&gt;% dplyr::mutate(name, bmi = mass / ((height / 100) ^ 2)) %&gt;% dplyr::select(name:mass, bmi) ## # A tibble: 87 x 4 ## name height mass bmi ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 26.0 ## 2 C-3PO 167 75 26.9 ## 3 R2-D2 96 32 34.7 ## 4 Darth Vader 202 136 33.3 ## 5 Leia Organa 150 49 21.8 ## 6 Owen Lars 178 120 37.9 ## 7 Beru Whitesun lars 165 75 27.5 ## 8 R5-D4 97 32 34.0 ## 9 Biggs Darklighter 183 84 25.1 ## 10 Obi-Wan Kenobi 182 77 23.2 ## # … with 77 more rows starwars %&gt;% dplyr::arrange(desc(mass)) ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabb… 175 1358 &lt;NA&gt; green-tan… orange 600 herma… ## 2 Grie… 216 159 none brown, wh… green, y… NA male ## 3 IG-88 200 140 none metal red 15 none ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Tarf… 234 136 brown brown blue NA male ## 6 Owen… 178 120 brown, gr… light blue 52 male ## 7 Bossk 190 113 none green red 53 male ## 8 Chew… 228 112 brown unknown blue 200 male ## 9 Jek … 180 110 brown fair blue NA male ## 10 Dext… 198 102 none brown yellow NA male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% dplyr::group_by(species) %&gt;% dplyr::summarize( num = n(), mass = mean(mass, na.rm = TRUE) ) %&gt;% dplyr::filter(num &gt; 1) ## # A tibble: 9 x 3 ## species num mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Droid 5 69.8 ## 2 Gungan 3 74 ## 3 Human 35 82.8 ## 4 Kaminoan 2 88 ## 5 Mirialan 2 53.1 ## 6 Twi&#39;lek 2 55 ## 7 Wookiee 2 124 ## 8 Zabrak 2 80 ## 9 &lt;NA&gt; 5 48 3.3.3.2 NHANES require(NHANES) names(NHANES) ## [1] &quot;ID&quot; &quot;SurveyYr&quot; &quot;Gender&quot; ## [4] &quot;Age&quot; &quot;AgeDecade&quot; &quot;AgeMonths&quot; ## [7] &quot;Race1&quot; &quot;Race3&quot; &quot;Education&quot; ## [10] &quot;MaritalStatus&quot; &quot;HHIncome&quot; &quot;HHIncomeMid&quot; ## [13] &quot;Poverty&quot; &quot;HomeRooms&quot; &quot;HomeOwn&quot; ## [16] &quot;Work&quot; &quot;Weight&quot; &quot;Length&quot; ## [19] &quot;HeadCirc&quot; &quot;Height&quot; &quot;BMI&quot; ## [22] &quot;BMICatUnder20yrs&quot; &quot;BMI_WHO&quot; &quot;Pulse&quot; ## [25] &quot;BPSysAve&quot; &quot;BPDiaAve&quot; &quot;BPSys1&quot; ## [28] &quot;BPDia1&quot; &quot;BPSys2&quot; &quot;BPDia2&quot; ## [31] &quot;BPSys3&quot; &quot;BPDia3&quot; &quot;Testosterone&quot; ## [34] &quot;DirectChol&quot; &quot;TotChol&quot; &quot;UrineVol1&quot; ## [37] &quot;UrineFlow1&quot; &quot;UrineVol2&quot; &quot;UrineFlow2&quot; ## [40] &quot;Diabetes&quot; &quot;DiabetesAge&quot; &quot;HealthGen&quot; ## [43] &quot;DaysPhysHlthBad&quot; &quot;DaysMentHlthBad&quot; &quot;LittleInterest&quot; ## [46] &quot;Depressed&quot; &quot;nPregnancies&quot; &quot;nBabies&quot; ## [49] &quot;Age1stBaby&quot; &quot;SleepHrsNight&quot; &quot;SleepTrouble&quot; ## [52] &quot;PhysActive&quot; &quot;PhysActiveDays&quot; &quot;TVHrsDay&quot; ## [55] &quot;CompHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;CompHrsDayChild&quot; ## [58] &quot;Alcohol12PlusYr&quot; &quot;AlcoholDay&quot; &quot;AlcoholYear&quot; ## [61] &quot;SmokeNow&quot; &quot;Smoke100&quot; &quot;Smoke100n&quot; ## [64] &quot;SmokeAge&quot; &quot;Marijuana&quot; &quot;AgeFirstMarij&quot; ## [67] &quot;RegularMarij&quot; &quot;AgeRegMarij&quot; &quot;HardDrugs&quot; ## [70] &quot;SexEver&quot; &quot;SexAge&quot; &quot;SexNumPartnLife&quot; ## [73] &quot;SexNumPartYear&quot; &quot;SameSex&quot; &quot;SexOrientation&quot; ## [76] &quot;PregnantNow&quot; # find the sleep variables NHANESsleep &lt;- NHANES %&gt;% select(Gender, Age, Weight, Race1, Race3, Education, SleepTrouble, SleepHrsNight, TVHrsDay, TVHrsDayChild, PhysActive) names(NHANESsleep) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Race1&quot; ## [5] &quot;Race3&quot; &quot;Education&quot; &quot;SleepTrouble&quot; &quot;SleepHrsNight&quot; ## [9] &quot;TVHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;PhysActive&quot; dim(NHANESsleep) ## [1] 10000 11 # subset for college students NHANESsleep &lt;- NHANESsleep %&gt;% filter(Age %in% c(18:22)) %&gt;% mutate(Weightlb = Weight*2.2) names(NHANESsleep) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Race1&quot; ## [5] &quot;Race3&quot; &quot;Education&quot; &quot;SleepTrouble&quot; &quot;SleepHrsNight&quot; ## [9] &quot;TVHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;PhysActive&quot; &quot;Weightlb&quot; dim(NHANESsleep) ## [1] 655 12 NHANESsleep %&gt;% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + geom_point(position=position_jitter(width=.25, height=0) ) + facet_grid(SleepTrouble ~ TVHrsDay) 3.3.4 summarize and group_by # number of people (cases) in NHANES NHANES %&gt;% summarize(n()) ## # A tibble: 1 x 1 ## `n()` ## &lt;int&gt; ## 1 10000 # total weight of all the people in NHANES (silly) NHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(sum(Weightlb, na.rm=TRUE)) ## # A tibble: 1 x 1 ## `sum(Weightlb, na.rm = TRUE)` ## &lt;dbl&gt; ## 1 1549419. # mean weight of all the people in NHANES NHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 1 x 1 ## `mean(Weightlb, na.rm = TRUE)` ## &lt;dbl&gt; ## 1 156. # repeat the above but for groups # males versus females NHANES %&gt;% group_by(Gender) %&gt;% summarize(n()) ## # A tibble: 2 x 2 ## Gender `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 female 5020 ## 2 male 4980 NHANES %&gt;% group_by(Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 2 x 2 ## Gender `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 146. ## 2 male 167. # smokers and non-smokers NHANES %&gt;% group_by(SmokeNow) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## SmokeNow `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 No 1745 ## 2 Yes 1466 ## 3 &lt;NA&gt; 6789 NHANES %&gt;% group_by(SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 3 x 2 ## SmokeNow `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No 186. ## 2 Yes 177. ## 3 &lt;NA&gt; 144. # people with and without diabetes NHANES %&gt;% group_by(Diabetes) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## Diabetes `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 No 9098 ## 2 Yes 760 ## 3 &lt;NA&gt; 142 NHANES %&gt;% group_by(Diabetes) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 3 x 2 ## Diabetes `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No 155. ## 2 Yes 202. ## 3 &lt;NA&gt; 21.6 # break down the smokers versus non-smokers further, by sex NHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% summarize(n()) ## # A tibble: 6 x 3 ## # Groups: SmokeNow [3] ## SmokeNow Gender `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No female 764 ## 2 No male 981 ## 3 Yes female 638 ## 4 Yes male 828 ## 5 &lt;NA&gt; female 3618 ## 6 &lt;NA&gt; male 3171 NHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 6 x 3 ## # Groups: SmokeNow [3] ## SmokeNow Gender `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 No female 167. ## 2 No male 201. ## 3 Yes female 167. ## 4 Yes male 185. ## 5 &lt;NA&gt; female 138. ## 6 &lt;NA&gt; male 151. # break down the people with diabetes further, by smoking NHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% summarize(n()) ## # A tibble: 8 x 3 ## # Groups: Diabetes [3] ## Diabetes SmokeNow `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No No 1476 ## 2 No Yes 1360 ## 3 No &lt;NA&gt; 6262 ## 4 Yes No 267 ## 5 Yes Yes 106 ## 6 Yes &lt;NA&gt; 387 ## 7 &lt;NA&gt; No 2 ## 8 &lt;NA&gt; &lt;NA&gt; 140 NHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 8 x 3 ## # Groups: Diabetes [3] ## Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 No No 183. ## 2 No Yes 175. ## 3 No &lt;NA&gt; 143. ## 4 Yes No 204. ## 5 Yes Yes 204. ## 6 Yes &lt;NA&gt; 199. ## 7 &lt;NA&gt; No 193. ## 8 &lt;NA&gt; &lt;NA&gt; 19.1 3.3.5 babynames babynames %&gt;% group_by(sex) %&gt;% summarize(total=sum(n)) ## # A tibble: 2 x 2 ## sex total ## &lt;chr&gt; &lt;int&gt; ## 1 F 172371079 ## 2 M 175749438 babynames %&gt;% group_by(year, sex) %&gt;% summarize(name_count = n_distinct(name)) %&gt;% head() ## # A tibble: 6 x 3 ## # Groups: year [3] ## year sex name_count ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 F 942 ## 2 1880 M 1058 ## 3 1881 F 938 ## 4 1881 M 997 ## 5 1882 F 1028 ## 6 1882 M 1099 babynames %&gt;% group_by(year, sex) %&gt;% summarize(name_count = n_distinct(name)) %&gt;% tail() ## # A tibble: 6 x 3 ## # Groups: year [3] ## year sex name_count ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 2015 F 19074 ## 2 2015 M 14024 ## 3 2016 F 18817 ## 4 2016 M 14162 ## 5 2017 F 18309 ## 6 2017 M 14160 babysamp &lt;- babynames %&gt;% sample_n(size=50) babysamp %&gt;% select(year) %&gt;% distinct() %&gt;% table() ## . ## 1898 1903 1911 1913 1917 1919 1920 1921 1922 1928 1929 1935 1946 1948 1950 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1951 1954 1956 1958 1962 1970 1972 1973 1975 1977 1978 1980 1983 1985 1989 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1991 1998 1999 2001 2002 2003 2006 2007 2008 2009 2011 2014 2015 2017 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 babysamp %&gt;% distinct() %&gt;% select(year) %&gt;% table() ## . ## 1898 1903 1911 1913 1917 1919 1920 1921 1922 1928 1929 1935 1946 1948 1950 ## 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 ## 1951 1954 1956 1958 1962 1970 1972 1973 1975 1977 1978 1980 1983 1985 1989 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 ## 1991 1998 1999 2001 2002 2003 2006 2007 2008 2009 2011 2014 2015 2017 ## 2 3 1 1 1 1 1 1 1 1 1 1 2 1 Frances &lt;- babynames %&gt;% filter(name== &quot;Frances&quot;) %&gt;% group_by(year, sex) %&gt;% summarize(yrTot = sum(n)) Frances %&gt;% ggplot(aes(x=year, y=yrTot)) + geom_point(aes(color=sex)) + geom_vline(xintercept=2006) + scale_y_log10() + ylab(&quot;Yearly total on log10 scale&quot;) 3.4 9/19/19 Agenda Higher level data verbs: pivot_longer, pivot_wider, join lubridate 3.5 Higher Level Data Verbs There are more complicated verbs which may be important for more sophisticated analyses. See the RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}. pivot_longer makes many columns into 2 columns: pivot_longer(data, cols, names_to = , value_to = ) pivot_wider makes one column into multiple columns: pivot_wider(data, names_from = , values_from = ) left_join returns all rows from the left table, and any rows with matching keys from the right table. inner_join returns only the rows in which the left table have matching keys in the right table (i.e., matching rows in both sets). full_join returns all rows from both tables, join records from the left which have matching keys in the right table. Good practice: always specify the by argument when joining data frames. If you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti 3.6 R examples, higher level verbs tidyr 1.0.0 has just been released! The new release means that you need to update tidyr. You will know if you have the latest version if the following command works in the console (window below): ?tidyr::pivot_longer If you are familiar with spread and gather, you should acquaint yourself with pivot_longer() and pivot_wider(). The idea is to go from very wide dataframes to very long dataframes and vice versa. 3.6.1 pivot_longer pivot the military pay grade to become longer? https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794 library(googlesheets4) sheets_deauth() navy_gs = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408&quot;, col_types = &quot;ccnnnnnnnnnnnnnnn&quot;) dplyr::glimpse(navy_gs) ## Observations: 38 ## Variables: 17 ## $ ...1 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `Active Duty Family` &lt;chr&gt; NA, &quot;Marital Status Report&quot;, NA, &quot;Data Refl… ## $ ...3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 31229, 5309… ## $ ...4 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388,… ## $ ...5 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36946, 6148… ## $ ...6 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, … ## $ ...7 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1… ## $ ...8 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, … ## $ ...9 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3… ## $ ...10 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4… ## $ ...11 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, … ## $ ...12 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483… ## $ ...13 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, … ## $ ...14 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165… ## $ ...15 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36991, 6747… ## $ ...16 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924… ## $ ...17 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 43690, 7839… names(navy_gs) = c(&quot;X&quot;,&quot;pay.grade&quot;, &quot;male.sing.wo&quot;, &quot;female.sing.wo&quot;, &quot;tot.sing.wo&quot;, &quot;male.sing.w&quot;, &quot;female.sing.w&quot;, &quot;tot.sing.w&quot;, &quot;male.joint.NA&quot;, &quot;female.joint.NA&quot;, &quot;tot.joint.NA&quot;, &quot;male.civ.NA&quot;, &quot;female.civ.NA&quot;, &quot;tot.civ.NA&quot;, &quot;male.tot.NA&quot;, &quot;female.tot.NA&quot;, &quot;tot.tot.NA&quot;) navy = navy_gs[-c(1:8), -1] dplyr::glimpse(navy) ## Observations: 30 ## Variables: 16 ## $ pay.grade &lt;chr&gt; &quot;E-1&quot;, &quot;E-2&quot;, &quot;E-3&quot;, &quot;E-4&quot;, &quot;E-5&quot;, &quot;E-6&quot;, &quot;E-7&quot;,… ## $ male.sing.wo &lt;dbl&gt; 31229, 53094, 131091, 112710, 57989, 19125, 5446… ## $ female.sing.wo &lt;dbl&gt; 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438… ## $ tot.sing.wo &lt;dbl&gt; 36946, 61482, 152110, 129091, 69010, 23779, 7359… ## $ male.sing.w &lt;dbl&gt; 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786,… ## $ female.sing.w &lt;dbl&gt; 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144… ## $ tot.sing.w &lt;dbl&gt; 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299… ## $ male.joint.NA &lt;dbl&gt; 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 4… ## $ female.joint.NA &lt;dbl&gt; 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 15… ## $ tot.joint.NA &lt;dbl&gt; 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074… ## $ male.civ.NA &lt;dbl&gt; 5060, 12483, 54795, 105556, 130944, 110322, 7000… ## $ female.civ.NA &lt;dbl&gt; 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 29… ## $ tot.civ.NA &lt;dbl&gt; 5779, 14165, 61436, 115517, 139536, 116149, 7320… ## $ male.tot.NA &lt;dbl&gt; 36991, 67472, 193729, 236418, 212329, 148290, 87… ## $ female.tot.NA &lt;dbl&gt; 6699, 10924, 34482, 40782, 37306, 22404, 10995, … ## $ tot.tot.NA &lt;dbl&gt; 43690, 78396, 228211, 277200, 249635, 170694, 98… # get rid of total columns &amp; rows: navyWR = navy %&gt;% dplyr::select(-contains(&quot;tot&quot;)) %&gt;% dplyr::filter(substr(pay.grade, 1, 5) != &quot;TOTAL&quot; &amp; substr(pay.grade, 1, 5) != &quot;GRAND&quot; ) %&gt;% tidyr::pivot_longer(-pay.grade, values_to = &quot;numPeople&quot;, names_to = &quot;status&quot;) %&gt;% tidyr::separate(status, into = c(&quot;sex&quot;, &quot;marital&quot;, &quot;kids&quot;)) navyWR %&gt;% head() ## # A tibble: 6 x 5 ## pay.grade sex marital kids numPeople ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E-1 male sing wo 31229 ## 2 E-1 female sing wo 5717 ## 3 E-1 male sing w 563 ## 4 E-1 female sing w 122 ## 5 E-1 male joint NA 139 ## 6 E-1 female joint NA 141 Does a graph tell us if we did it right? what if we had done it wrong…? navyWR %&gt;% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + geom_point() + facet_grid(kids~marital) 3.6.2 pivot_wider library(babynames) babynames %&gt;% dplyr::select(-prop) %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% head() ## # A tibble: 6 x 4 ## year name F M ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1880 Mary 7065 27 ## 2 1880 Anna 2604 12 ## 3 1880 Emma 2003 10 ## 4 1880 Elizabeth 1939 9 ## 5 1880 Minnie 1746 9 ## 6 1880 Margaret 1578 NA babynames %&gt;% dplyr::select(-prop) %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% dplyr::filter(!is.na(F) &amp; !is.na(M)) %&gt;% arrange(desc(year), desc(M)) ## # A tibble: 168,381 x 4 ## year name F M ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2017 Liam 36 18728 ## 2 2017 Noah 170 18326 ## 3 2017 William 18 14904 ## 4 2017 James 77 14232 ## 5 2017 Logan 1103 13974 ## 6 2017 Benjamin 8 13733 ## 7 2017 Mason 58 13502 ## 8 2017 Elijah 26 13268 ## 9 2017 Oliver 15 13141 ## 10 2017 Jacob 16 13106 ## # … with 168,371 more rows babynames %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% dplyr::filter(!is.na(F) &amp; !is.na(M)) %&gt;% arrange(desc(prop)) ## # A tibble: 12 x 5 ## year name prop F M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1986 Marquette 0.0000130 24 25 ## 2 1996 Dariel 0.0000115 22 23 ## 3 2014 Laramie 0.0000108 21 22 ## 4 1939 Earnie 0.00000882 10 10 ## 5 1939 Vertis 0.00000882 10 10 ## 6 1921 Vernis 0.00000703 9 8 ## 7 1939 Alvia 0.00000529 6 6 ## 8 1939 Eudell 0.00000529 6 6 ## 9 1939 Ladell 0.00000529 6 6 ## 10 1939 Lory 0.00000529 6 6 ## 11 1939 Maitland 0.00000529 6 6 ## 12 1939 Delaney 0.00000441 5 5 3.6.3 join (use join to merge two datasets) 3.6.3.1 First get the data (GapMinder) Both of the following datasets come from GapMinder. The first represents country, year, and female literacy rate. The second represents country, year, and GDP (in fixed 2000 US$). sheets_deauth() litF = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0&quot;) litF = litF %&gt;% dplyr::select(country=starts_with(&quot;Adult&quot;), starts_with(&quot;1&quot;), starts_with(&quot;2&quot;)) %&gt;% tidyr::pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;litRateF&quot;) %&gt;% dplyr::filter(!is.na(litRateF)) sheets_deauth() GDP = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0&quot;) GDP = GDP %&gt;% dplyr::select(country = starts_with(&quot;Income&quot;), starts_with(&quot;1&quot;), starts_with(&quot;2&quot;)) %&gt;% tidyr::pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;gdp&quot;) %&gt;% dplyr::filter(!is.na(gdp)) head(litF) ## # A tibble: 6 x 3 ## country year litRateF ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 ## 2 Afghanistan 2011 13 ## 3 Albania 2001 98.3 ## 4 Albania 2008 94.7 ## 5 Albania 2011 95.7 ## 6 Algeria 1987 35.8 head(GDP) ## # A tibble: 6 x 3 ## country year gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Albania 1980 1061. ## 2 Albania 1981 1100. ## 3 Albania 1982 1111. ## 4 Albania 1983 1101. ## 5 Albania 1984 1065. ## 6 Albania 1985 1060. # left litGDPleft = dplyr::left_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPleft) ## [1] 571 4 sum(is.na(litGDPleft$gdp)) ## [1] 66 head(litGDPleft) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 NA ## 2 Afghanistan 2011 13 NA ## 3 Albania 2001 98.3 1282. ## 4 Albania 2008 94.7 1804. ## 5 Albania 2011 95.7 1966. ## 6 Algeria 1987 35.8 1902. # right litGDPright = dplyr::right_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPright) ## [1] 7988 4 sum(is.na(litGDPright$gdp)) ## [1] 0 head(litGDPright) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1980 NA 1061. ## 2 Albania 1981 NA 1100. ## 3 Albania 1982 NA 1111. ## 4 Albania 1983 NA 1101. ## 5 Albania 1984 NA 1065. ## 6 Albania 1985 NA 1060. # inner litGDPinner = dplyr::inner_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPinner) ## [1] 505 4 sum(is.na(litGDPinner$gdp)) ## [1] 0 head(litGDPinner) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 2001 98.3 1282. ## 2 Albania 2008 94.7 1804. ## 3 Albania 2011 95.7 1966. ## 4 Algeria 1987 35.8 1902. ## 5 Algeria 2002 60.1 1872. ## 6 Algeria 2006 63.9 2125. # full litGDPfull = dplyr::full_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPfull) ## [1] 8054 4 sum(is.na(litGDPfull$gdp)) ## [1] 66 head(litGDPfull) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 NA ## 2 Afghanistan 2011 13 NA ## 3 Albania 2001 98.3 1282. ## 4 Albania 2008 94.7 1804. ## 5 Albania 2011 95.7 1966. ## 6 Algeria 1987 35.8 1902. 3.6.4 lubridate lubridate is a another R package meant for data wrangling (Grolemund and Wickham 2011). In particular, lubridate makes it very easy to work with days, times, and dates. The base idea is to start with dates in a ymd (year month day) format and transform the information into whatever you want. The linked table is from the original paper and provides many of the basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}. Example from https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html 3.6.4.1 If anyone drove a time machine, they would crash The length of months and years change so often that doing arithmetic with them can be unintuitive. Consider a simple operation, January 31st + one month. Should the answer be: February 31st (which doesn’t exist) March 4th (31 days after January 31), or February 28th (assuming its not a leap year) A basic property of arithmetic is that a + b - b = a. Only solution 1 obeys the mathematical property, but it is an invalid date. Wickham wants to make lubridate as consistent as possible by invoking the following rule: if adding or subtracting a month or a year creates an invalid date, lubridate will return an NA. If you thought solution 2 or 3 was more useful, no problem. You can still get those results with clever arithmetic, or by using the special %m+% and %m-% operators. %m+% and %m-% automatically roll dates back to the last day of the month, should that be necessary. 3.6.4.2 R examples, lubridate Some basics in lubridate require(lubridate) rightnow &lt;- now() day(rightnow) ## [1] 17 week(rightnow) ## [1] 38 month(rightnow, label=FALSE) ## [1] 9 month(rightnow, label=TRUE) ## [1] Sep ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec year(rightnow) ## [1] 2019 minute(rightnow) ## [1] 54 hour(rightnow) ## [1] 4 yday(rightnow) ## [1] 260 mday(rightnow) ## [1] 17 wday(rightnow, label=FALSE) ## [1] 3 wday(rightnow, label=TRUE) ## [1] Tue ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat But how do I create a date object? jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) ## [1] &quot;2013-01-31&quot; NA &quot;2013-03-31&quot; NA &quot;2013-05-31&quot; ## [6] NA &quot;2013-07-31&quot; &quot;2013-08-31&quot; NA &quot;2013-10-31&quot; ## [11] NA &quot;2013-12-31&quot; floor_date(jan31, &quot;month&quot;) + months(0:11) + days(31) ## [1] &quot;2013-02-01&quot; &quot;2013-03-04&quot; &quot;2013-04-01&quot; &quot;2013-05-02&quot; &quot;2013-06-01&quot; ## [6] &quot;2013-07-02&quot; &quot;2013-08-01&quot; &quot;2013-09-01&quot; &quot;2013-10-02&quot; &quot;2013-11-01&quot; ## [11] &quot;2013-12-02&quot; &quot;2014-01-01&quot; jan31 + months(0:11) + days(31) ## [1] &quot;2013-03-03&quot; NA &quot;2013-05-01&quot; NA &quot;2013-07-01&quot; ## [6] NA &quot;2013-08-31&quot; &quot;2013-10-01&quot; NA &quot;2013-12-01&quot; ## [11] NA &quot;2014-01-31&quot; jan31 %m+% months(0:11) ## [1] &quot;2013-01-31&quot; &quot;2013-02-28&quot; &quot;2013-03-31&quot; &quot;2013-04-30&quot; &quot;2013-05-31&quot; ## [6] &quot;2013-06-30&quot; &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2013-09-30&quot; &quot;2013-10-31&quot; ## [11] &quot;2013-11-30&quot; &quot;2013-12-31&quot; NYC flights library(nycflights13) names(flights) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; flightsWK &lt;- flights %&gt;% mutate(ymdday = ymd(paste(year, month,day, sep=&quot;-&quot;))) %&gt;% mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday)) head(flightsWK) ## # A tibble: 6 x 22 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # … with 15 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt;, ymdday &lt;date&gt;, weekdy &lt;ord&gt;, whichweek &lt;dbl&gt; flightsWK &lt;- flights %&gt;% mutate(ymdday = ymd(paste(year,&quot;-&quot;, month,&quot;-&quot;,day))) %&gt;% mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday)) flightsWK %&gt;% select(year, month, day, ymdday, weekdy, whichweek, dep_time, arr_time, air_time) %&gt;% head() ## # A tibble: 6 x 9 ## year month day ymdday weekdy whichweek dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 2013-01-01 Tue 1 517 830 227 ## 2 2013 1 1 2013-01-01 Tue 1 533 850 227 ## 3 2013 1 1 2013-01-01 Tue 1 542 923 160 ## 4 2013 1 1 2013-01-01 Tue 1 544 1004 183 ## 5 2013 1 1 2013-01-01 Tue 1 554 812 116 ## 6 2013 1 1 2013-01-01 Tue 1 554 740 150 3.7 reprex Help me help you In order to create a reproducible example … Step 1. Copy code onto the clipboard Step 2. Type reprex() into the Console Step 3. Look at the Viewer to the right. Copy the Viewer output into GitHub, Piazza, an email, stackexchange, etc. Some places to learn more about reprex include A blog about it: https://teachdatascience.com/reprex/ The reprex vignette: https://reprex.tidyverse.org/index.html reprex dos and donts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html Jenny Bryan webinar on reprex: “Help me help you. Creating reproducible examples” https://resources.rstudio.com/webinars/help-me-help-you-creating-reproducible-examples-jenny-bryan 3.7.0.1 reprex demo reprex( jan31 + months(0:11) + days(31) ) multiple lines of code: reprex({ jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) + days(31) }) reprex({ library(lubridate) jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) + days(31) }) References "],
["sims.html", "Chapter 4 Simulating 4.1 9/24/19 Agenda 4.2 Simulating Complicated Models 4.3 9/26/19 Agenda 4.4 Simulating to Assess Sensitivity", " Chapter 4 Simulating 4.1 9/24/19 Agenda Why simulate? What makes a good simulation? pigs / blackjack examples Below, computer simulations will be used for two main objectives: To understand complicated models To assess sensitivity of procedures We can use simulation studies to understand complex estimators, stochastic processes, etc. Often times, such analytic solutions exist in theory, but are extremely complicated to solve. In particular, as slight variations to the model are added, the simulation is often trivial to change whereas the analytic solution often becomes intractable. Similarly, repeated applications of a procedure (e.g., linear regression) to a scenario (e.g., a dataset, a set of parameters, etc.) can provide important insight into how the procedure varies / behaves. 4.2 Simulating Complicated Models Simulation is done to model a scenario which allows us to understand random behavior without actually replicating the entire study multiple times or trying to model the process analytically. For example, what if you have a keen interest in understanding the probability of getting a single during room draw? Or getting a single on north campus? You wouldn’t actually run room draw thousands of times to find your probability of getting a single room. Similarly, the situation (e.g., room draw) may have too much information (e.g., all the different permutations of integers assigned to groups of 3 or 4 people) to model (easily) in a closed form solution. With a few moderate assumptions (proportion of students in groups of 1, 2, 3, 4; probability of choosing dorm X over dorm Y; random allocation of integers to students; etc.) it is straightforward to simulate the scenario thousands of time and measure the proportion of times your rank (47) will give you the room you want (single in Sontag). Consider the following simulation where the top 10 GOP candidates get to participate in the debate, and the remaining 6 are kept out (example taken for debate on August 6, 2015). The write-up (and example) is a few years old, but the process is identical to the process used for deciding who is eligible for the 2020 Democtratic debates for president. http://www.nytimes.com/interactive/2015/07/21/upshot/election-2015-the-first-gop-debate-and-the-role-of-chance.html?_r=0 A candidate needed to get at least two percent support in four different polls published from a list of approved pollsters between June 28 and August 28, 2019, which cannot be based on open-ended questions and may cover either the national level or one of the first four primary/caucus states (Iowa, New Hampshire, Nevada, and South Carolina). Only one poll from each approved pollster counted towards meeting the criterion in each region. Wikipedia Figure 1.2: For the 2016 election the Republican primary debates allowed only the top 10 candidates, ranked by national polls NYT Example Consider the following problem from probability. Two points are selected randomly on a line of length \\(L\\) so as to be on opposite sides of the midpoint of the line. [In other words, the two points \\(X\\) and \\(Y\\) are independent random variables such that \\(X\\) is uniformly distributed over \\((0,L/2)\\) and \\(Y\\) is uniformly distributed over \\((L/2, 1)\\).] Find the probability that the 3 line segments from \\(0\\) to \\(X\\), from \\(X\\) to \\(Y\\), and from \\(Y\\) to \\(L\\) could be made to form the three sides of a triangle. (Note that three line segments can be made to form a triangle if the length of each of them is less than the sum of the lengths of the others.) The joint density is: \\[ f(x,y) = \\begin{cases} \\frac{4}{L^2} &amp; 0 \\le x \\le L/2, \\, L/2 \\le y \\le L \\\\ 0 &amp; else \\end{cases} \\] The three pieces have lengths: \\(X\\), \\(Y- X\\) and \\(L - Y\\). Three conditions need to be satisfied in order that the three pieces form a triangle: \\[\\begin{align} X + (Y- X) &amp;&gt; (L - Y) \\Rightarrow Y &gt; L - Y \\Rightarrow 2 Y &gt; L \\Rightarrow Y &gt; L/2 \\\\ X + (L-Y ) &amp;&gt; Y - X \\Rightarrow 2X + L &gt; 2Y \\Rightarrow X + \\frac{L}{2} &gt; Y \\\\ Y + (L - Y) &amp;&gt; X \\Rightarrow L &gt; X \\end{align}\\] The first and third conditions are always satisfied, so we just need to find the probability that \\(Y\\) is below the line \\(X + \\frac{L}{2}\\). The density is the same as in the previous problem, so, as before, we just need to find the area of the region below the line that is within the square \\([0, L/2] \\times [L/2, L]\\), and then multiply it by \\(\\frac{4}{L^2}\\). Area = \\(\\displaystyle{ \\frac{1}{2}\\frac{L}{2}\\frac{L}{2} = \\frac{L^2}{8} }\\). Thus, the probability is \\[\\begin{align} \\int_{area} f(x,y)dxdy = \\frac{4}{L^2} \\frac{L^2}{8} = \\frac{1}{2}. \\end{align}\\] What happens for different values of \\(f(x,y)\\)? For example, if \\(x\\) and \\(y\\) have Beta(3,47) distributions on [0,.5] and [.5,1]? Simulating the probability in R is quite straightforward. What is the confidence bounds on the point estimates for the probabilities?? [n.b., we could simulate repeatedly to get a sense for the variability of our estimate!] sticks &lt;- function() { pointx &lt;- runif(1,0,.5) # runif is &quot;random uniform&quot;, not &quot;run if&quot; pointy &lt;- runif(1,.5,1) l1 &lt;- pointx l2 &lt;- pointy-pointx l3 &lt;- 1 - pointy max(l1,l2,l3) &gt; 1-max(l1,l2,l3)} sum(replicate(100000, sticks())) / 100000 ## [1] 0.5 sticks_beta &lt;- function() { pointx &lt;- rbeta(1,3, 47) / 2 # rbeta is random beta pointy &lt;- (rbeta(1, 3, 47) + 1)/2 l1 &lt;- pointx l2 &lt;- pointy-pointx l3 &lt;- 1 - pointy max(l1,l2,l3) &gt; 1-max(l1,l2,l3)} sum(replicate(100000, sticks_beta())) / 100000 ## [1] 0.5 Example Or consider the problem where the goal is to estimate \\(E(X)\\) where \\(X=\\max \\{ k: \\sum_{i=1}^k U_i &lt; 1 \\}\\) and \\(U_i\\) are uniform(0,1). The simulation problem is quite straightforward. Look carefully at the pieces. How are they broken down into steps? Notice that the steps go from inside out. Set k (the number of random numbers) equal to zero. And the running sum to zero. Generate a uniform random variable. Add the random variable to the running sum. Repeat steps 1 and 2 until the sum is larger than 1. Figure out how many random variables were needed to get the sum larger than 1. Repeat the entire process many times so as to account for variability in the simulation. Use the law of large numbers to conclude that the average of the simulation approximates the expected value. allk &lt;- c() for(i in 1:1000){ k &lt;- 0; sumU &lt;- 0 while(sumU &lt; 1) { sumU &lt;- sumU + runif(1) k &lt;- k+1 } allk &lt;- c(allk, k-1) } mean(allk) ## [1] 1.71 4.2.1 Goals of Simulating Complicated Models The goal of simulating a complicated model is not only to create a program which will provide the desired results. We also hope to be able to code such that: The problem is broken down into small pieces The problem has checks in it to see what works (run the lines inside the if statements!) Simple code is best 4.2.2 Examples of Pigs and Blackjack 4.2.2.1 Pass the Pigs Familiarize yourself with how to play Pass the Pigs at http://www.hasbro.com/common/instruct/passthepigs.pdf and https://en.wikipedia.org/wiki/Pass_the_Pigs. For more information on how to play Pass the Pigs, google online resources and see the following manuscript, http://pubsonline.informs.org/doi/pdf/10.1287/ited.1120.0088 Analytics, Pedagogy and the Pass the Pigs Game, (2012), Gorman, INFORMS Transactions on Education 1. More sophisticated modeling: http://www.amstat.org/publications/jse/v14n3/datasets.kern.html Some strategies for playing: http://passpigs.tripod.com/strat.html (The link has other stuff, too.) 4.2.2.2 Blackjack Example and code come from **Data Science in R: a case studies approach to computational reasoning and problem solving*, by Nolan and Temple Lang. Chapter 9 Simulating Blackjack, by Hadley Wickham All R code is online at http://rdatasciencecases.org/ More about the game of blackjack, there are many online resources that you can use to learn about the came. Two resources that Nolan and Temple Lang recommend are http://wizardofodds.com/games/blackjack/ and http://hitorstand.net/. Basic Blackjack Card game, goal: sum cards as close to 21 without going over A few nuances to card value (e.g., Ace can be 1 or 11) Start with 2 cards, build up one card at a time Lots of different strategies (also based on dealer’s cards) What do we need to simulate poker? set-up of cards, dealing, hands “score” (both sum of cards and payout) strategies result of strategies (summary of outcomes) Source Example and code come from Data Science in R: a case studies approach to computational reasoning and problem solving by Nolan and Temple Lang. Chapter 9 Simulating Blackjack by Hadley Wickham All R code is online at http://rdatasciencecases.org/ Link is also on the HW4 assignment Setting up the Game in R deck = rep(c(1:10, 10, 10, 10), 4) shuffle_decks = function(ndecks){sample(rep(deck, ndecks))} head(shuffle_decks(4), 10) ## [1] 2 6 6 1 6 10 10 10 2 10 Outcome of cards in hand handValue = function(cards) { value = sum(cards) # Check for an Ace and change value if it doesn&#39;t bust if (any(cards == 1) &amp;&amp; value &lt;= 11) value = value + 10 value # Check bust (set to 0); check Blackjack (set to 21.5) if(value &gt; 21) 0 else if (value == 21 &amp;&amp; length(cards) == 2) 21.5 # Blackjack else value } handValue(c(10,4)) ## [1] 14 $ of cards in hand winnings = function(dealer, players) { if (dealer &gt; 21) { # Dealer=Blackjack, ties players with Blackjack -1 * (players &lt;= 21) } else if (dealer == 0) { # Dealer busts - all non-busted players win 1.5 * (players &gt; 21) + 1 * (players &lt;= 21 &amp; players &gt; 0) + -1 * (players == 0) } else { # Dealer 21 or below, all players &gt; dealer win 1.5 * (players &gt; 21) + 1 * (players &lt;= 21 &amp; players &gt; dealer) + -1 * (players &lt;= 21 &amp; players &lt; dealer) } } winnings(17,c(20, 21.5, 14, 0, 21)) ## [1] 1.0 1.5 -1.0 -1.0 1.0 Better $ of cards in hand winnings = function(dealer, players){ (players &gt; dealer &amp; players &gt; 21) * 1.5 + # Blackjack (players &gt; dealer &amp; players &lt;= 21) * 1 + # win (players &lt; dealer | players == 0) * -1 # lose } winnings(17,c(20, 21.5, 14, 0, 21)) ## [1] 1.0 1.5 -1.0 -1.0 1.0 winnings(21.5,c(20, 21.5, 14, 0, 21)) ## [1] -1 0 -1 -1 -1 How well does handValue work? test_cards = list( c(10, 1), c(10, 5, 6), c(10, 1, 1), c(7, 6, 1, 5), c(3, 6, 1, 1), c(2, 3, 4, 10), c(5, 1, 9, 1, 1), c(5, 10, 7), c(10, 9, 1, 1, 1)) test_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0) sapply(test_cards, handValue) # apply the function handValue to test_cards ## [1] 21.5 21.0 12.0 19.0 21.0 19.0 17.0 0.0 0.0 identical(test_cards_val, sapply(test_cards, handValue)) ## [1] TRUE Testing winnings (create known) test_vals = c(0, 16, 19, 20, 21, 21.5) testWinnings = matrix(c( -1, 1, 1, 1, 1, 1.5, -1, 0, 1, 1, 1, 1.5, -1, -1, 0, 1, 1, 1.5, -1, -1, -1, 0, 1, 1.5, -1, -1, -1, -1, 0, 1.5, -1, -1, -1, -1, -1, 0), nrow = length(test_vals), byrow = TRUE) dimnames(testWinnings) = list(dealer = test_vals, player = test_vals) testWinnings ## player ## dealer 0 16 19 20 21 21.5 ## 0 -1 1 1 1 1 1.5 ## 16 -1 0 1 1 1 1.5 ## 19 -1 -1 0 1 1 1.5 ## 20 -1 -1 -1 0 1 1.5 ## 21 -1 -1 -1 -1 0 1.5 ## 21.5 -1 -1 -1 -1 -1 0.0 Does winnings work? check = testWinnings # make the matrix the right size check[] = NA # make all entries NA for(i in seq_along(test_vals)) { for(j in seq_along(test_vals)) { check[i, j] = winnings(test_vals[i], test_vals[j]) } } identical(check, testWinnings) ## [1] TRUE Function for getting more cards shoe = function(m = 1) sample(deck, m, replace = TRUE) new_hand = function(shoe, cards = shoe(2), bet = 1) { list(bet = bet, shoe = shoe, cards = cards) } myCards = new_hand(shoe, bet = 7) myCards ## $bet ## [1] 7 ## ## $shoe ## function(m = 1) sample(deck, m, replace = TRUE) ## ## $cards ## [1] 4 9 First action: hit receive another card hit = function(hand) { hand$cards = c(hand$cards, hand$shoe(1)) hand } hit(myCards)$cards ## [1] 4 9 10 Second action: stand stay with current cards stand = function(hand) hand stand(myCards)$cards ## [1] 4 9 Third action: double down double the bet after receiving exactly one more card dd = function(hand) { hand$bet = hand$bet * 2 hand = hit(hand) stand(hand) } dd(myCards)$cards ## [1] 4 9 3 Fourth action: split a pair create two different hands from initial hand with two cards of the same value splitPair = function(hand) { list( new_hand(hand$shoe, cards = c(hand$cards[1], hand$shoe(1)), bet = hand$bet), new_hand(hand$shoe, cards = c(hand$cards[2], hand$shoe(1)), bet = hand$bet)) } splitHand = splitPair(myCards) Results of splitting (can we always split?) splitHand[[1]]$cards ## [1] 4 1 splitHand[[2]]$cards ## [1] 9 5 Let’s play! Not yet automated… set.seed(470); dealer = new_hand(shoe); player = new_hand(shoe); dealer$cards[1] ## [1] 2 player$cards; player = hit(player); player$cards ## [1] 5 10 ## [1] 5 10 9 dealer$cards; dealer = hit(dealer); dealer$cards ## [1] 2 3 ## [1] 2 3 3 Who won? dealer$cards; player$cards ## [1] 2 3 3 ## [1] 5 10 9 handValue(dealer$cards); handValue(player$cards) ## [1] 8 ## [1] 0 winnings(handValue(dealer$cards), handValue(player$cards)) ## [1] -1 Simply strategy recall the handValue function – what if player busts? strategy_simple = function(mine, dealerFaceUp) { if (handValue(dealerFaceUp) &gt; 6 &amp;&amp; handValue(mine) &lt; 17) &quot;H&quot; else &quot;S&quot; } Better simple strategy strategy_simple = function(mine, dealerFaceUp) { if (handValue(mine) == 0) return(&quot;S&quot;) if (handValue(dealerFaceUp) &gt; 6 &amp;&amp; handValue(mine) &lt; 17) &quot;H&quot; else &quot;S&quot; } Dealer The dealer gets cards regardless of what the player does dealer_cards = function(shoe) { cards = shoe(2) while(handValue(cards) &lt; 17 &amp;&amp; handValue(cards) &gt; 0) { cards = c(cards, shoe(1)) } cards } dealer_cards(shoe) ## [1] 5 9 1 8 dealer_cards(shoe) ## [1] 4 4 7 10 Playing a hand play_hand = function(shoe, strategy, hand = new_hand(shoe), dealer = dealer_cards(shoe)) { face_up_card = dealer[1] action = strategy(hand$cards, face_up_card) while(action != &quot;S&quot; &amp;&amp; handValue(hand$cards) != 0) { if (action == &quot;H&quot;) { hand = hit(hand) action = strategy(hand$cards, face_up_card) } else { stop(&quot;Unknown action: should be one of S, H&quot;) } } winnings(handValue(dealer), handValue(hand$cards)) * hand$bet } Play a few hands play_hand(shoe, strategy_simple) ## [1] 0 play_hand(shoe, strategy_simple) ## [1] -1 play_hand(shoe, strategy_simple, new_hand(shoe, bet=7)) ## [1] 0 play_hand(shoe, strategy_simple, new_hand(shoe, bet=7)) ## [1] 7 Repeated games To repeat the game, we simply repeat the play_hand function and keep track of the dollars gained or lost. reps=10 money=20 for(i in 1:reps){ money &lt;- money + play_hand(shoe, strategy_simple) print(money)} ## [1] 19 ## [1] 20 ## [1] 19 ## [1] 18 ## [1] 19.5 ## [1] 18.5 ## [1] 20 ## [1] 19 ## [1] 18 ## [1] 19.5 4.3 9/26/19 Agenda Understanding bias in modeling Sensitivity of statistical inferential procedures to technical conditions (Not responsible for: Generating random numbers) 4.4 Simulating to Assess Sensitivity As a second use of simulations, we can assess the sensitivity of parameters, model assumptions, sample size, etc. Ideally, the results will be summarized graphically, instead of as a table. A graphical representation can often provide insight into how parameters are related, whereas a table can be very hard to read. 4.4.1 Bias in Models The example below is taken directly (and mostly verbatim) from a blog by Aaron Roth Algorithmic Unfairness Without Any Bias Baked In. Bias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is “unfair”. Here is the (toy) scenario – the specifics aren’t important. High school students are applying to college, and each student has some innate “talent” \\(I\\), which we will imagine is normally distributed, with mean 100 and standard deviation 15: \\(I \\sim N(100,15)\\). The college would like to admit students who are sufficiently talented — say one standard deviation above the mean (so, it would like to admit students with \\(I \\geq 115\\)). The problem is that talent isn’t directly observable. Instead, the college can observe grades \\(g\\) and SAT scores \\(s\\), which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student’s talent level, and also with standard deviation 15: \\(g \\sim N(I, 15)\\), \\(s \\sim N(I, 15)\\). In this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose predicted talent is at least 115. This is indeed “driven by math” – since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college. The data Ok. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population – the Blues only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above. But there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds. n = 100000 n.red = n*0.99 n.blue = n*0.01 reds &lt;- rnorm(n.red, mean = 100, sd = 15) blues &lt;- rnorm(n.blue, mean = 100, sd = 15) red.sat &lt;- reds + rnorm(n.red, mean = 0, sd = 15) blue.sat &lt;- blues + pmax(rnorm(n.blue, mean = 0, sd = 15), rnorm(n.blue, mean = 0, sd = 15)) red.grade &lt;- reds + rnorm(n.red, mean = 0, sd = 15) blue.grade &lt;- blues + rnorm(n.blue, mean = 0, sd = 15) college.data &lt;- data.frame(talent = c(reds, blues), SAT = c(red.sat, blue.sat), grades = c(red.grade, blue.grade), color = c(rep(&quot;red&quot;, n.red), rep(&quot;blue&quot;, n.blue))) ggplot(college.data, aes(x = grades, y = SAT, color = color)) + geom_point(size = 0.5) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) + geom_abline(intercept = 0, slope = 1) Two separate models So what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don’t see much difference: red.lm = college.data %&gt;% dplyr::filter(color == &quot;red&quot;) %&gt;% lm(talent ~ SAT + grades, data = .) blue.lm = college.data %&gt;% dplyr::filter(color == &quot;blue&quot;) %&gt;% lm(talent ~ SAT + grades, data = .) global.lm = college.data %&gt;% lm(talent ~ SAT + grades, data = .) red.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 33.2 0.153 216. 0 ## 2 SAT 0.333 0.00151 220. 0 ## 3 grades 0.335 0.00151 222. 0 blue.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.6 1.64 15.6 1.85e- 49 ## 2 SAT 0.429 0.0169 25.3 1.12e-109 ## 3 grades 0.280 0.0157 17.9 3.50e- 62 global.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 33.2 0.153 217. 0 ## 2 SAT 0.333 0.00151 221. 0 ## 3 grades 0.335 0.00150 223. 0 new.reds &lt;- rnorm(n.red, mean = 100, sd = 15) new.blues &lt;- rnorm(n.blue, mean = 100, sd = 15) new.red.sat &lt;- new.reds + rnorm(n.red, mean = 0, sd = 15) new.blue.sat &lt;- new.blues + pmax(rnorm(n.blue, mean = 0, sd = 15), rnorm(n.blue, mean = 0, sd = 15)) new.red.grade &lt;- new.reds + rnorm(n.red, mean = 0, sd = 15) new.blue.grade &lt;- new.blues + rnorm(n.blue, mean = 0, sd = 15) new.college.data &lt;- data.frame(talent = c(new.reds, new.blues), SAT = c(new.red.sat, new.blue.sat), grades = c(new.red.grade, new.blue.grade), color = c(rep(&quot;red&quot;, n.red), rep(&quot;blue&quot;, n.blue))) new.red.pred &lt;- new.college.data %&gt;% filter(color == &quot;red&quot;) %&gt;% predict.lm(red.lm, newdata = .) new.blue.pred &lt;- new.college.data %&gt;% filter(color == &quot;blue&quot;) %&gt;% predict.lm(blue.lm, newdata = .) new.college.data &lt;- new.college.data %&gt;% cbind(predicted = c(new.red.pred, new.blue.pred)) ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + geom_point(size = 0.5) + geom_hline(yintercept = 115) + geom_vline(xintercept = 115) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) new.college.data &lt;- new.college.data %&gt;% mutate(fp = ifelse(talent &lt; 115 &amp; predicted &gt; 115, 1, 0), tp = ifelse(talent &gt; 115 &amp; predicted &gt; 115, 1, 0), fn = ifelse(talent &gt; 115 &amp; predicted &lt; 115, 1, 0), tn = ifelse(talent &lt; 115 &amp; predicted &lt; 115, 1, 0)) error.rates &lt;- new.college.data %&gt;% group_by(color) %&gt;% summarize(tpr = sum(tp) / (sum(tp) + sum(fn)), fpr = sum(fp) / (sum(fp) + sum(tn)), fnr = sum(fn) / (sum(fn) + sum(tp)), fdr = sum(fp) / (sum(fp) + sum(tp)), error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) )) error.rates ## # A tibble: 2 x 6 ## color tpr fpr fnr fdr error ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 0.548 0.0367 0.452 0.267 0.101 ## 2 red 0.506 0.0379 0.494 0.284 0.111 The Red classifier makes errors approximately 11.053% of the time. The Blue classifier does about the same — it makes errors about 10.1% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate! And since we are interested in fairness, lets think about the false negative rate of our classifiers. “False Negatives” in this setting are the people who are qualified to attend the college (\\(I &gt; 115\\)), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier’s mistakes. And the False Negative Rate is the probability that a randomly selected qualified person is mistakenly rejected from college — i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier’s mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness — sometimes referred to as “equal opportunity.” So how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 45.161% on the blues, and the Red model has a false negative rate of 49.413% on the reds — so the difference between these two is a satisfyingly small 4.252%. One global model But you might reasonably object: because we have learned separate models for the Blues and the Reds, we are explicitly making admissions decisions as a function of a student’s color! This might sound like a form of discrimination, baked in by the algorithm designer — and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending. new.pred &lt;- new.college.data %&gt;% predict.lm(global.lm, newdata = .) new.college.data &lt;- new.college.data %&gt;% cbind(global.predicted = new.pred) ggplot(new.college.data, aes(x = talent, y = global.predicted, color = color)) + geom_point(size = 0.5) + geom_hline(yintercept = 115) + geom_vline(xintercept = 115) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) new.college.data &lt;- new.college.data %&gt;% mutate(fp = ifelse(talent &lt; 115 &amp; global.predicted &gt; 115, 1, 0), tp = ifelse(talent &gt; 115 &amp; global.predicted &gt; 115, 1, 0), fn = ifelse(talent &gt; 115 &amp; global.predicted &lt; 115, 1, 0), tn = ifelse(talent &lt; 115 &amp; global.predicted &lt; 115, 1, 0)) error.rates &lt;- new.college.data %&gt;% group_by(color) %&gt;% summarize(tpr = sum(tp) / (sum(tp) + sum(fn)), fpr = sum(fp) / (sum(fp) + sum(tn)), fnr = sum(fn) / (sum(fn) + sum(tp)), fdr = sum(fp) / (sum(fp) + sum(tp)), error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) )) error.rates ## # A tibble: 2 x 6 ## color tpr fpr fnr fdr error ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 0.632 0.0627 0.368 0.351 0.11 ## 2 red 0.504 0.0377 0.496 0.283 0.111 So what happens if we don’t allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.791%. Additionally, the Blues now have a higher false positive rate (people who don’t have talent about 115 are let in accidentally) and the Reds now have a higher false negative rate (people who do have talent are mistakenly kept out). This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population. What happened???? What happened? There wasn’t any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population – which contributed much more to the average. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them – all in their favor. This is the kind of thing that happens all the time: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, it is exacerbated if we artificially force the algorithm to be group blind. Well intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time. 4.4.2 Technical Conditions Definitions p-value is the probability of obtaining the observed data or more extreme given the null hypothesis is true. confidence interval is a range of values collected in such a way that repeated samples of data (using the same mechanism) would capture the parameter of interest in \\((1-\\alpha)100\\)% of the intervals. Examples Equal variance in the t-test Recall that one of the technical conditions for the t-test is that the two samples come from populations where the variance is equal (at least when var.equal=TRUE is specified). What happens if the null hypothesis is true (i.e., the means are equal!) but the technical conditions are violated (i.e., the variances are unequal)? pvals &lt;- c() reps &lt;- 10000 for(i in 1:reps){ x1 &lt;- rnorm(10, mean=47, sd=1) x2 &lt;- rnorm(10, mean=47, sd=1) pvals &lt;- c(pvals, t.test(x1,x2, var.equal=TRUE)$p.value) } sum(pvals &lt; .05)/reps ## [1] 0.0502 Unequal variance in the t-test pvals &lt;- c() reps &lt;- 10000 for(i in 1:reps){ x1 &lt;- rnorm(10, mean=47, sd=1) x2 &lt;- rnorm(10, mean=47, sd=100) pvals &lt;- c(pvals, t.test(x1,x2, var.equal=TRUE)$p.value) } sum(pvals &lt; .05)/reps ## [1] 0.0637 Equal variance in the linear model The ISCAM applet by Beth Chance and Allan Rossman (Chance and Rossman 2018b) demonstrates ideas of confidence intervals and what the analyst should expect with inferential assessment. Consider the following linear model with the points normally distributed with equal variance around the line. [Spoiler: when the technical conditions are met, the theory works out well. It turns out that the confidence interval will capture the true parameter in 95% of samples!] \\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1)\\] beta2.in &lt;- c() beta0 &lt;- -1 beta1 &lt;- 0.5 beta2 &lt;- 1.5 n &lt;- 100 reps &lt;- 10000 set.seed(4747) for(i in 1:reps){ x1 &lt;- rep(c(0,1), each=n/2) x2 &lt;- runif(n, min=-1, max=1) y &lt;- beta0 + beta1*x1 + beta2*x2 + rnorm(n, mean=0, sd = 1) CI &lt;- lm(y~x1+x2) %&gt;% tidy(conf.int=TRUE) %&gt;% data.frame() beta2.in &lt;- c(beta2.in, between(beta2, CI[3,6], CI[3,7])) } # coverage rate of the CI is given by: sum(beta2.in)/reps ## [1] 0.952 Unequal variance in the linear model Consider the following linear model with the points normally distributed with unequal variance around the line. [Spoiler: when the technical conditions are met, the theory does not work out as well. It turns out that the confidence interval will not capture the true parameter in 95% of samples!] \\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1+ X_1 + 10 \\cdot |X_2|)\\] beta2.in &lt;- c() beta0 &lt;- -1 beta1 &lt;- 0.5 beta2 &lt;- 1.5 n &lt;- 100 reps &lt;- 10000 set.seed(4747) for(i in 1:reps){ x1 &lt;- rep(c(0,1), each=n/2) x2 &lt;- runif(n, min=-1, max=1) y &lt;- beta0 + beta1*x1 + beta2*x2 + rnorm(n, mean=0, sd = 1 + x1 + 10*abs(x2)) CI &lt;- lm(y~x1+x2) %&gt;% broom::tidy(conf.int=TRUE) %&gt;% data.frame() beta2.in &lt;- c(beta2.in, between(beta2, CI[3,6], CI[3,7])) } # coverage rate of the CI is given by: sum(beta2.in)/reps ## [1] 0.872 4.4.3 Generating random numbers You are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation. 4.4.3.1 How do we generate uniform[0,1] numbers? LCG - linear congruence generators. Set \\(a,b,m\\) to be large integers. The sequence of numbers \\(X_i / m\\) will pass all tests for uniformly distributed variables. \\[ X_{n+1} = (aX_n + b) \\mod m \\] where \\(m\\) and \\(b\\) are relatively prime, \\(a - 1\\) is divisible by all prime factors of \\(m\\), \\(a - 1\\) is divisible by 4 if \\(m\\) is divisible by 4. a &lt;- 31541435235 b &lt;- 23462146143 m &lt;- 423514351351 xval &lt;- 47 reps &lt;- 10000 unif.val &lt;- c() for(i in 1:reps){ xval &lt;- (a*xval + b) %% m unif.val &lt;- c(unif.val, xval/m) } data.frame(uniformRVs = unif.val) %&gt;% ggplot(aes(x = uniformRVs)) + geom_histogram(bins = 25) 4.4.4 Generating other RVs: The Inverse Transform Method You are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation. Continuous RVs Use the inverse of the cumulative distribution function to generate data that come from a particular continuous distribution. For example, generate 100 random normal deviates. Start by assuming that \\(F\\) is a continuous and increasing function. Also assume that \\(F^{-1}\\) exists. \\[F(x) = P(X \\leq x)\\] Note that \\(F\\) is just the area function describing the density (histogram) of the data. Algorithm: Generate Continuous RV Generate a uniform random variable \\(U\\) Set \\(X = F^{-1}(U)\\) Proof: that the algorithm above generates variables that come from the probability distribution represented by \\(F\\). \\[\\begin{align} P(X \\leq x) &amp;= P(F^{-1}(U) \\leq x)\\\\ &amp;= P(U \\leq F(x))\\\\ &amp;= F(x)\\\\ \\end{align}\\] Example: \\[ f(x) = \\begin{cases} 2x e^{-x^2} &amp; 0 &lt; x \\\\ 0 &amp; x &lt; 0 \\end{cases}\\] Note: This is known as a Weibull(\\(\\lambda=1\\), \\(k=2\\)) distribution. Figure 4.1: Weibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons What is \\(F(x)\\)? \\[ F(x) = \\int_0^x 2w e^{-w^2} dw = 1 - e^{-x^2}\\] What is \\(F^{-1}(u)\\)? \\[\\begin{align} u &amp;= F(x)\\\\ &amp;= 1 - e^{-x^2}\\\\ 1-u &amp;= e^{-x^2}\\\\ -\\ln(1-u) &amp;= x^2\\\\ \\sqrt{-\\ln(1-u)} &amp;= x\\\\ F^{-1}(u) &amp;= \\sqrt{-\\ln(1-u)} \\end{align}\\] Suppose you could simulate uniform random variables, \\(U_1, U_2, \\dots\\). How could you use these to simulate RV’s with the Weibull density, \\(f(x)\\), given above? \\[ \\mbox{Let: } X_i = \\sqrt{-\\ln(1-U_i)}\\] unifdata = runif(10000,0,1) weib1data = sqrt(-log(1-unifdata)) weib2data = rweibull(10000,2,1) weibdata &lt;- data.frame(weibull = c(weib1data, weib2data), sim.method = c(rep(&quot;InvTrans&quot;, 10000), rep(&quot;rweibull&quot;, 10000))) ggplot(weibdata, aes(x = weibull)) + geom_histogram(bins = 25) + facet_grid(~sim.method) Discrete RVs A similar algorithm is used to generate data that come from a particular discrete distribution. For example, generate 100 random normal deviates. We start by assuming the probability mass function of \\(X\\) is \\[ P(X = x_i) = p_i, i=1, \\ldots, m\\] Algorithm: Generate Discrete RV Generate a uniform random variable \\(U\\) Transform \\(U\\) into \\(X\\) as follows, \\[X = x_j \\mbox{ if } \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\] Proof: that the algorithm above generates variables that come from the probability mass function \\(\\{p_1, p_2, \\ldots, p_m\\}\\). \\[\\begin{align} P(X = x_j) &amp;= \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\\\ &amp;= \\sum_{i=1}^j p_i - \\sum_{i=1}^{j-1} p_i\\\\ &amp;= p_j\\\\ \\end{align}\\] What if you don’t know \\(F\\)? Or can’t calculate \\(F^{-1}\\)? In the case that the CDF cannot be calculated explicitly (the normal for example), one could still use this methodology by estimating F at a collection of points \\(x_i, u_i = F(x_i)\\). Now we temporarily mimic the discrete inverse transform, as we generate a \\(U\\) and see which subinterval it falls in, i.e. \\(u_i \\leq U \\leq u_{i+1}\\). Assuming the \\(x_i\\) are close enough, we expect the CDF to be approximately linear on this subinterval, so then we take a linear interpolation of the CDF on the subinterval to get \\(X\\) via \\[\\begin{align} X = \\frac{u_{i+1} - U}{u_{i+1} - u_i} x_i + \\frac{U - u_i}{u_{i+1} - u_i} x_j \\end{align}\\] However, the linear interpolation requires a complete approximation of \\(F(x)\\), regardless of the sample size desired, and doesn’t generalize to higher dimensions, and of course only gives you something with the approximate distribution back, even if you have your hands on real uniform random variables. References "],
["permschp.html", "Chapter 5 Permutation Tests 5.1 10/1/19 Agenda 5.2 Inference Algorithms 5.3 10/3/19 Agenda 5.4 Permutation tests in practice 5.5 R examples", " Chapter 5 Permutation Tests 5.1 10/1/19 Agenda Review: logic of hypothesis testing Logic of permutation tests Examples - 2 samples and beyond Motivation: Great video of how/why computational statistical methods can be extremely useful. And it’s about beer and mosquitoes! John Rauser from Pintrest gives the keynote address at Strata + Hadoop World Conference October 16, 2014. David Smith, Revolution Analytics blog, October 17, 2014. http://blog.revolutionanalytics.com/2014/10/statistics-doesnt-have-to-be-that-hard.html A more complicated scenario with the same tools being applied. Here the point is to understand gerrymandering. https://www.youtube.com/watch?v=gRCZR_BbjTo&amp;t=125s The big lesson here, IMO, is that so many statistical problems can seem complex, but you can actually get a lot of insight by recognizing that your data is just one possible instance of a random process. If you have a hypothesis for what that process is, you can simulate it, and get an intuitive sense of how surprising your data is. R has excellent tools for simulating data, and a couple of hours spent writing code to simulate data can often give insights that will be valuable for the formal data analysis to come. (David Smith) Rauser says that the in order to follow a statistical argument that uses simulation, you need three things: Ability to follow a simple logical argument. Random number generation. Iteration 5.2 Inference Algorithms 5.2.1 Hypothesis Test Algorithm Before working out the nitty gritty details, recall the structure of hypothesis testing. Consider the applet on Simulating ANOVA Tables (Chance &amp; Rossman) http://www.rossmanchance.com/applets/AnovaSim.html Choose a statistic that measures the effect you are looking for. For example, the ANOVA F statistic is: \\[\\begin{align} F &amp;= \\frac{\\text{between-group variability}}{\\text{within-group variability}}\\\\ &amp;= \\frac{\\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{i\\cdot})^2/(N-K)} \\end{align}\\] Construct the sampling distribution that this statistic would have if the effect were not present in the population. [The sampling distributions for t statistics and F statistics are based on the Central Limit Theorem and derived in Math 152.] Locate the observed statistic in this distribution. A value in the main body of the distribution could easily occur just by chance. A value in the tail would rarely occur by chance and so is evidence that something other than chance is operating. [This piece is going to happen in permutation tests as well as in analytic tests – the point is to see if the observed data is consistent with the null distribution.] 5.2.1.0.1 p-value {-} is the probability of the observed data or more extreme if the null hypothesis is true. [Also true for both types of tests!] To estimate the p-value for a test of significance, estimate the sampling distribution of the test statistic when the null hypothesis is true by resampling in a manner that is consistent with the null hypothesis. 5.2.2 Permutation Tests Algorithm To evaluate the p-value for a permutation test, estimate the sampling distribution of the test statistic when the null hypothesis is true by resampling in a manner that is consistent with the null hypothesis (the number of resamples is finite but can be large!). Choose a test statistic Shuffle the data (enforce the null hypothesis to be true) Create a null sampling distribution of the test statistic (under \\(H_0\\)) Find the observed test statistic on the null sampling distribution and compute the p-value (observed data or more extreme). The p-value can be one or two-sided. Technical Conditions Permutation tests fall into a broad class of tests called “non-parametric” tests. The label indicates that there are no distributional assumptions made on the data (i.e., no assumption that the data come from a normal or binomial distribution). However, a test which is ``non-parametric&quot; does not meant that there are no assumptions on the data, simply that there are no distributional or parametric assumptions on the data. The parameters are at the heart of almost all parametric tests. For permutation tests, we are not basing the test on population parameters, so we don’t need to make any assumptions about them (i.e., that they are the mean of a particular distribution). Permutation The different treatments have the same effect. [Note: exchangeability, same population, etc.] If the null hypothesis is true, the labels assigning groups are interchangeable. Note that it is our choice of statistic which makes the test more sensitive to some kinds of difference (e.g., difference in mean) than other kinds (e.g., difference in variance). Parametric The different populations have the same mean. IMPORTANT KEY IDEA the point of technical conditions for parametric or permutation tests is to create a sampling distribution that accurately reflects the null sampling distribution for the statistic of interest (the statistic which captures the relevant research question information). 5.3 10/3/19 Agenda R code, examples Assumptions, exchangeability, random structure Different statistics within the permutation test Permutation vs. Randomization tests (Binomial) 5.4 Permutation tests in practice How is the test interpreted given the different types of sampling which are possibly used to collect the data? Random Sample The concept of a p-value usually comes from the idea of taking a sample from a population and comparing it to a sampling distribution (from many many random samples). Random Experiment In the context of a randomized experiment, the p-value represents the observed data compared to “happening by chance.” The interpretation is easy: if there is only a very small chance that the observed statistic would take such an extreme value, as a result only of the randomization of cases: we reject the null treatment effect hypothesis. CAUSAL! Observational Study In the context of observational studies the results are less strong, but it is reasonable to conclude that the effect observed in the sample reflects an effect present in the population. In a sample, consider the difference (or ratio) and ask “Is this difference so large it would rarely occur by chance in a particular sample constructed under the null setting?” If the data come from a random sample, then the sample (or results from the sample) are probably consistent with the population [i.e., we can infer the results back to the larger population]. 5.4.0.1 Other Test Statistics The example in class used a modification of the ANOVA F-statistic to compare the observed data with the permuted data test statistics. Depending on the data and question, the permuted test statistic can take on any of a variety of forms. Data Hypothesis Question Statistic 2 categorical diff in prop \\(\\hat{p}_1 - \\hat{p}_2\\) or \\(\\chi^2\\) variables ratio of prop \\(\\hat{p}_1 / \\hat{p}_2\\) 1 numeric diff in means \\(\\overline{X}_1 - \\overline{X}_2\\) 1 binary ratio of means \\(\\overline{X}_1 / \\overline{X}_2\\) diff in medians \\(\\mbox{median}_1 - \\mbox{median}_2\\) ratio of medians \\(\\mbox{median}_1 / \\mbox{median}_2\\) diff in SD \\(s_1 - s_2\\) diff in var \\(s^2_1 - s^2_2\\) ratio of SD or VAR \\(s_1 / s_2\\) 1 numeric diff in means \\(\\sum n_i (\\overline{X}_i - \\overline{X})^2\\) or k groups F stat paired or (permute within row) \\(\\overline{X}_1 - \\overline{X}_2\\) repeated measures regression correlation least sq slope time series no serial core lag 1 autocross Depending on the data, hypotheses, and original data collection structure (e.g., random sampling vs random allocation), the choice of statistic for the permutation test will vary. 5.4.1 Permutation vs. Randomization Tests We will call randomization tests those that enumerate all possible data permutations. permutation tests, on the other hand, will permute the data \\(B\\) ($&lt; &lt; $ all) times. Some authors call a permutation test applied to a randomized experiment a randomization test. Main difference: randomization tests consider every possible permutation of the labels, permutation tests take a random sample of permutations of the labels. Both can only be applied to a comparison situation (e.g., no one sample t-tests). Both permute labels under \\(H_0\\), for example, \\(H_0: F(x) = G(x)\\). Both can be used in situations where sampling distributions are unknown (e.g., differences in medians). Both can be used in situations where sampling distributions are based on population distributions (e.g., ratio of variances). Randomization tests were the first nonparametric tests conceived (R.A. Fisher, 1935). Randomization p-value Let \\(t^*\\) be the observed test statistic. For a two sample test with \\(N\\) total observations and \\(n\\) observations in group 1, there are \\({N \\choose n}\\) randomizations, all of which are equally likely under \\(H_0\\). The p-value then becomes: \\[\\begin{align} p_R &amp;= P(T \\leq t^* | H_0) = \\frac{\\sum_{i=1}^{{N \\choose n}} I(t_i \\leq t*)}{{N \\choose n}} \\end{align}\\] If we choose a significance level of \\(\\alpha = k/{N \\choose n}\\), then the type I error rate is: \\[\\begin{align} P(\\text{type I error}) &amp;= P(p_R \\leq \\alpha | H_0)\\\\ &amp;= P\\bigg(\\sum_{i=1}^{{N \\choose n}} I(t_i \\leq t*) \\leq k | H_0 \\bigg)\\\\ &amp;= \\frac{k}{{N \\choose n}}= \\alpha\\\\ \\text{alternatively } k&amp;= \\alpha {N \\choose n} \\end{align}\\] The point of which is to say that the randomization test controls the probability of a Type I error under the very minimal conditions that the subjects are randomized to treatments (minimal assumption, but hard to do in practice!!) Permutation p-value Now consider a permutation test that randomly permutes the data \\(B\\) times (instead of all \\({N \\choose n}\\) times). A permutation test approximates a randomization test. In fact, the permutation test can be analyzed using the following binomial random variable: \\[\\begin{align} X_P &amp;= \\# \\ \\mbox{permutations out of B that give a more extreme value than the observed test statistic}\\\\ X_P &amp;\\sim Bin(p_R, B)\\\\ SE(X_P) &amp;= \\sqrt{\\frac{p_R (1-p_R)}{B}} \\approx \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}} \\end{align}\\] Consider a situation where interest is in a small effect, say p-value\\(\\approx 0.01\\). The SE should be less than 0.001. \\[\\begin{align} 0.001 &amp;= \\sqrt{ (0.01)\\cdot(0.99) / B}\\\\ B &amp;= (0.01) \\cdot (0.99) / (0.001)^2\\\\ &amp;= 9900 \\end{align}\\] Another way to look at the same problem is to use the estimated p-value = \\(\\hat{p}_P = \\frac{X_P}{B}\\) to come up with a confidence interval for \\(p_R\\). CI for \\(p_R \\approx \\hat{p}_P \\pm 1.96 \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\\) 5.4.2 CI from Permutation Tests Use shifts or rescaling to create a CI for a parameter value using permutation tests. That is, consider a situation with data from \\(X\\) and \\(Y\\) Use one of the following transformation (depending on the study): \\[\\begin{align} W &amp;= Y + a\\\\ \\mbox{or } U &amp;= Y / b \\end{align}\\] and run the permutation test of interest on \\(X\\) vs. \\(W\\) or \\(X\\) vs. \\(U\\). For a series of \\(a\\) or \\(b\\) values we can find which we don’t reject at a particular level of significance (\\(\\alpha\\)) to create a \\((1-\\alpha)100\\%\\) confidence interval. Usually, however, we use bootstrapping for confidence intervals and permutation tests for hypothesis testing. 5.4.3 Randomization Example 5.4.3.1 Fisher’s Exact Test – computationally efficient randomization test N observations are classified into a 2x2 table. Each observation is classified into exactly one cell. Row and column totals are fixed. Given fixed row and column totals, we can easily calculate the interior distribution using the hypergeometric. Note that once a single cell is filled, all other cells are determined. Col 1 Col 2 Total Row 1 X r-X r Row 2 c-X N-r-c+X N-r Total c N-c N \\[\\begin{align} P(X=x) &amp;= \\frac{{r \\choose x}{{N-r} \\choose{c-x}}}{{N \\choose c}}\\\\ &amp; \\mbox{out of those in col 1, how many are in row 1?}\\\\ P(X \\leq x) &amp;= \\sum_{i=0}^x \\frac{{r \\choose i}{{N-r} \\choose {c-i}}}{{N \\choose c}}\\\\ &amp;= \\mbox{p-value} \\end{align}\\] Not common for both row and column totals to be fixed. (More likely for just column totals to be fixed, e.g., men and women.) Instead, consider all subsets of the sample space with \\(N\\) observations. For any particular combination of row and column totals (\\(rc\\)): \\[\\begin{align} P(\\mbox{rejecting } H_0 | rc, H_0) &amp;\\leq&amp; \\alpha\\\\ P(\\mbox{rejecting } H_0 \\ \\ \\forall \\mbox{ subsets } | H_0) &amp;\\leq&amp; \\sum_{rc \\ combos} P(\\mbox{rejecting } H_0 | rc, H_0) P(rc | H_0)\\\\ &amp;\\leq&amp; \\alpha \\end{align}\\] (Note: assume $P(rc | H_0) = 1 / # rc $ combos.) The test will be valid at any \\(\\alpha\\) level, but it won’t be as powerful as one in which fixed columns/rows is actually meaningful. 5.4.3.2 \\(r \\times c\\) tables permute data in a new way new test stat \\[\\begin{align} T = \\sum_{i,j} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}} \\end{align}\\] 2-sided p-value. what do we expect? \\[\\begin{align} E_{i,j} = \\frac{R_i C_j}{N} \\end{align}\\] 5.4.3.2.1 Example: Observer In a study published in the Journal of Personality and Social Psychology (Butler and Baumeister, 1998), researchers investigated a conjecture that having an observer with a vested interest would decrease subjects’ performance on a skill-based task. Subjects were given time to practice playing a video game that required them to navigate an obstacle course as quickly as possible. They were then told to play the game on final time with an observer present. Subjects were randomly assigned to one of two groups: Group A was told that the participant and the observer would each win $3 if the participant beat a certain threshold. Group B was told that only that the participant would win the prize if the threshold was beaten. The goal of this data analysis is to determine whether or not there is an effect from the observer on the performance. That is, like the \\(\\chi^2\\) test, our hypotheses are: \\(H_0:\\) there is no association between the two variables \\(H_a:\\) there is an association between the two variables The data from the 24 subjects is given below: A: shares prize B: no sharing Total Beat threshold 3 8 11 Did not beat threshold 9 4 13 Total 12 12 24 Card simulation (to demonstrate how the permutation test works) Permutation Test (see Chance and Rossman applet for automated permutation test, http://www.rossmanchance.com/applets/ChisqShuffle.htm?FET=1) \\[\\begin{align} SE(\\mbox{p-value}) = \\sqrt{\\frac{\\hat{p}_r (1-\\hat{p}_r)}{100}} = 0.02 \\end{align}\\] Randomization Test \\[\\begin{align} P(X \\leq 3) = \\sum_{i=0}^3 \\frac{{11 \\choose i}{12 \\choose {12-i}}}{{24 \\choose 12}} = 0.0436 \\end{align}\\] 5.5 R examples 5.5.1 Cloud Seeding (Two sample test – computationally very difficult to do a randomization test) Cloud seeding data: seeding or not seeding was randomly allocated to 52 days when seeding was appropriate. The pilot did not know whether or not the plane was seeding. Rain is measured in acre-feet. After running tests to compare means and variances we obtain the following p-values: comparison of means comparison of variances Permutation t-test Permutation F-test Raw Data 0.031 0.054 0.068 0.000067 Logged Data 0.010 0.014 0.535 0.897 5.5.1.1 R code Before doing anything, let’s look at the data. Here, we visualize with both boxplots and histograms. Also, we visualize on the raw scale as well as the log scale. Certainly, the log10 scale indicates that a transformation makes the data more symmetric. clouds &lt;- read_delim(&quot;https://dasl.datadescription.com/download/data/3117/cloud-seeding.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) names(clouds) &lt;- c(&quot;unseeded&quot;, &quot;seeded&quot;) clouds &lt;- tidyr::pivot_longer(clouds, cols = 1:2, names_to = &quot;seeding&quot;, values_to = &quot;rainfall&quot;) %&gt;% mutate(seeding = as.factor(seeding)) clouds %&gt;% ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() clouds %&gt;% ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) clouds %&gt;% ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() + scale_y_log10() clouds %&gt;% ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) + scale_x_log10() unlogged data: clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) ## # A tibble: 2 x 3 ## seeding meanrain meanlnrain ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 seeded 442. 5.13 ## 2 unseeded 165. 3.99 clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %&gt;% summarize(diff(meanrain), diff(meanlnrain)) ## # A tibble: 1 x 2 ## `diff(meanrain)` `diff(meanlnrain)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -277. -1.14 raindiffs &lt;- clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %&gt;% summarize(diffrain = diff(meanrain), difflnrain = diff(meanlnrain)) raindiffs ## # A tibble: 1 x 2 ## diffrain difflnrain ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -277. -1.14 Below, we’ve formally gone through a permutation. here, the resampling is not coded in a particularly tidy way, but there is a tidy way to code loops! Generally, loops are not the fasted way to code in R, so if you need to quickly run code that seems like it should go in a loop, it is very likely that purrr is the direction you want to go, https://purrr.tidyverse.org/. 5.5.1.1.1 Difference in means after permuting reps &lt;- 1000 permdiffs &lt;- c() for(i in 1:reps){ onediff &lt;- clouds %&gt;% mutate(permseed = sample(seeding)) %&gt;% group_by(permseed) %&gt;% summarize(meanrain = mean(rainfall)) %&gt;% summarize(diff(meanrain)) %&gt;% pull() permdiffs &lt;- c(permdiffs, onediff) } permdiffs %&gt;% data.frame() %&gt;% ggplot(aes(x = permdiffs)) + geom_histogram(bins=30) + geom_vline(xintercept = raindiffs$diffrain, color = &quot;red&quot;) 5.5.1.1.2 Ratio of variances after permuting rainvarratio &lt;- clouds %&gt;% group_by(seeding) %&gt;% summarize(varrain = var(rainfall)) %&gt;% summarize(rainratio = varrain[1] / varrain[2]) reps &lt;- 1000 permvars &lt;- c() for(i in 1:reps){ oneratio &lt;- clouds %&gt;% mutate(permseed = sample(seeding)) %&gt;% group_by(permseed) %&gt;% summarize(varrain = var(rainfall)) %&gt;% summarize(varrain[1] / varrain[2]) %&gt;% pull() permvars &lt;- c(permvars, oneratio) } permvars %&gt;% data.frame() %&gt;% ggplot(aes(x = permvars)) + geom_histogram(bins=30) + geom_vline(xintercept = rainvarratio$rainratio , color = &quot;red&quot;) 5.5.1.1.3 Testing differences in means or ratios of variances As evidenced in the histograms above, the permutation test (one-sided) for the difference in means will count the number of permuted differences that are less than or equal to the observed difference in means, just over 1%. the permutation test (one-sided) for the ratio of variances will count the number of permuted ratios that are greater than or equal to the observed ratio of variances, about 7%. (sum(raindiffs$diffrain &gt;= permdiffs) + 1) /1000 ## [1] 0.027 (sum(rainvarratio$rainratio &lt;= permvars)+1)/1000 ## [1] 0.081 5.5.2 MacNell Teaching Evaluations (Stratified two-sample t-test) Boring et al. (2016) reanalyze data from MacNell et al. (2014). Students were randomized to 4 online sections of a course. In two sections, the instructors swapped identities. Was the instructor who identified as female rated lower on average? (https://www.math.upenn.edu/~pemantle/active-papers/Evals/stark2016.pdf) Figure 5.1: Kraj (2017) Figure 5.2: Kraj (2017) Figure 5.3: Mengel, Sauermann, and Zölitz (2019) Figure 5.4: MacNell, Driscoll, and Hunt (2015) Figure 5.5: MacNell, Driscoll, and Hunt (2015) 5.5.2.0.1 R code macnell &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/statlab/permuter/master/data-raw/macnell.csv&quot;) macnell %&gt;% mutate(TAID = ifelse(taidgender==1, &quot;male&quot;, &quot;female&quot;)) %&gt;% mutate(TAGend = ifelse(tagender==1, &quot;male&quot;, &quot;female&quot;)) %&gt;% mutate(gendjitter = ifelse(tagender==1, -0.2, 0.2)) %&gt;% ggplot(aes(x=TAGend, y=overall, color=TAID, fill=TAID)) + geom_boxplot() + scale_fill_manual(values=rep(&#39;white&#39;, 2)) + geom_point(position=position_jitterdodge(jitter.width=0.2), alpha=0.5) + stat_summary(fun.y=&quot;mean&quot;, geom=&quot;point&quot;, size=3, position=position_dodge(width=0.75)) 5.5.2.1 Analysis goal Want to know if the score for the perceived gender is different. \\[H_0: \\mu_{ID.Female} = \\mu_{ID.Male}\\] 5.5.2.2 MacNell Data without permutation macnell %&gt;% select(overall, tagender, taidgender) %&gt;% head(15) ## # A tibble: 15 x 3 ## overall tagender taidgender ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0 1 ## 2 4 0 1 ## 3 5 0 1 ## 4 5 0 1 ## 5 5 0 1 ## 6 4 0 1 ## 7 4 0 1 ## 8 5 0 1 ## 9 4 0 1 ## 10 3 0 1 ## 11 5 0 1 ## 12 4 0 1 ## 13 5 1 1 ## 14 5 1 1 ## 15 4 1 1 5.5.2.3 Permuting MacNell data Conceptually, there are two levels of randomization: \\(N_m\\) students are randomly assigned to the male instructor and the remaining \\(N_f\\) get the female instructor. Of the \\(N_j\\) assigned to instructor \\(j\\), \\(N_{jm}\\) are told that the instructor is male, for \\(j=1,2\\). Stratified two-sample test: For each instructor, permute perceived gender assignments. Use difference in mean ratings for female-identified vs male-identified instructors. macnell %>% group_by(tagender) %>% mutate(permTAID = sample(taidgender, replace=FALSE)) %>% select(overall, tagender, taidgender, permTAID) %>% head(15) ## # A tibble: 15 x 4 ## # Groups: tagender [2] ## overall tagender taidgender permTAID ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0 1 1 ## 2 4 0 1 1 ## 3 5 0 1 0 ## 4 5 0 1 0 ## 5 5 0 1 1 ## 6 4 0 1 1 ## 7 4 0 1 0 ## 8 5 0 1 0 ## 9 4 0 1 1 ## 10 3 0 1 1 ## 11 5 0 1 0 ## 12 4 0 1 0 ## 13 5 1 1 1 ## 14 5 1 1 1 ## 15 4 1 1 1 macnell %>% group_by(tagender) %>% mutate(permTAID = sample(taidgender, replace=FALSE)) %>% ungroup(tagender) %>% group_by(permTAID) %>% summarize(pmeans = mean(overall, na.rm=TRUE)) %>% summarize(diff(pmeans)) ## # A tibble: 1 x 1 ## `diff(pmeans)` ## &lt;dbl&gt; ## 1 -0.380 replicate(5, macnell %>% group_by(tagender) %>% mutate(permTAID = sample(taidgender, replace=FALSE)) %>% ungroup(tagender) %>% group_by(permTAID) %>% summarize(pmeans = mean(overall, na.rm=TRUE)) %>% summarize(diff(pmeans))) ## $`diff(pmeans)` ## [1] -0.671 ## ## $`diff(pmeans)` ## [1] -0.1 ## ## $`diff(pmeans)` ## [1] 0.18 ## ## $`diff(pmeans)` ## [1] -0.00652 ## ## $`diff(pmeans)` ## [1] 0.0952 set.seed(47) reps = 10000 ov.stats &lt;- replicate(reps, macnell %&gt;% group_by(tagender) %&gt;% mutate(permTAID = sample(taidgender, replace=FALSE)) %&gt;% ungroup(tagender) %&gt;% group_by(permTAID) %&gt;% summarize(pmeans = mean(overall, na.rm=TRUE)) %&gt;% summarize(diff(pmeans)) ) ov.stats &lt;- unlist(ov.stats) 5.5.2.4 permutation sampling distribution: hist(ov.stats) abline(v=0.47, col=&quot;red&quot;) # brute force permuation 2*sum(ov.stats &gt; 0.47) / reps ## [1] 0.113 5.5.2.5 MacNell Data with different permutation tests The permuter package contains functions for a variety of different permutation tests, including one with stratification. distr1 &lt;- stratified_two_sample(response = macnell$overall, group = macnell$taidgender, stratum = macnell$tagender, stat = &quot;mean&quot;, reps=reps) distr2 &lt;- stratified_two_sample(response = macnell$overall, group = macnell$taidgender, stratum = macnell$tagender, stat = &quot;t&quot;, reps=reps) macnell %&gt;% group_by(taidgender) %&gt;% summarize(means = mean(overall, na.rm = TRUE)) %&gt;% summarize(diff(means)) ## # A tibble: 1 x 1 ## `diff(means)` ## &lt;dbl&gt; ## 1 0.474 macnell %&gt;% group_by(taidgender) %&gt;% summarize(means = mean(overall, na.rm=TRUE), vars = var(overall, na.rm=TRUE), ns = n() ) %&gt;% summarize((means[1] - means[2])/ sqrt(vars[1]/ns[1] + vars[2]/ns[2])) ## # A tibble: 1 x 1 ## `(means[1] - means[2])/sqrt(vars[1]/ns[1] + vars[2]/ns[2])` ## &lt;dbl&gt; ## 1 -1.59 # brute force permuation 2*sum(ov.stats &gt; 0.47) / reps ## [1] 0.113 # mean(x) - mean(y) t2p(0.47, distr1, alternative=&quot;two-sided&quot;) ## Two-sided ## 0.13 # t permutation t2p(-1.56, distr2, alternative=&quot;two-sided&quot;) ## Two-sided ## 0.0978 5.5.2.6 Actual MacNell results 5.5.3 Income and Health (F-like test) Consider the NHANES dataset. Income (HHIncomeMid - Numerical version of HHIncome derived from the middle income in each category) Health (HealthGen - Self-reported rating of participant’s health in general Reported for participants aged 12 years or older. One of Excellent, Vgood, Good, Fair, or Poor.) 5.5.3.1 Summary of the variables of interest NHANES %&gt;% select(HealthGen) %&gt;% table() ## . ## Excellent Vgood Good Fair Poor ## 878 2508 2956 1010 187 NHANES %&gt;% select(HHIncomeMid) %&gt;% summary() ## HHIncomeMid ## Min. : 2500 ## 1st Qu.: 30000 ## Median : 50000 ## Mean : 57206 ## 3rd Qu.: 87500 ## Max. :100000 ## NA&#39;s :811 5.5.3.2 Mean Income broken down by Health NH.means &lt;- NHANES %&gt;% filter(!is.na(HealthGen) &amp; !is.na(HHIncomeMid)) %&gt;% group_by(HealthGen) %&gt;% summarize(IncMean = mean(HHIncomeMid, na.rm=TRUE), count=n()) NH.means ## # A tibble: 5 x 3 ## HealthGen IncMean count ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Excellent 69354. 817 ## 2 Vgood 65011. 2342 ## 3 Good 55662. 2744 ## 4 Fair 44194. 899 ## 5 Poor 37027. 164 Are the differences in means simply due to random chance?? NHANES %&gt;% filter(!is.na(HealthGen)&amp; !is.na(HHIncomeMid)) %&gt;% ggplot(aes(x=HealthGen, y=HHIncomeMid)) + geom_boxplot() + geom_jitter(width=0.1, alpha=.2) The differences in health, can be calculated directly, but we still don’t know if the differences are due to randome chance or some other larger structure. diff.mat = data.frame(matrix(ncol=5, nrow=5)) names(diff.mat) = NH.means$HealthGen rownames(diff.mat) = NH.means$HealthGen for(i in 1:5){ for(j in 1:5){ diff.mat[i,j] = NH.means$IncMean[i] - NH.means$IncMean[j] }} diff.mat ## Excellent Vgood Good Fair Poor ## Excellent 0 4344 13692 25161 32327 ## Vgood -4344 0 9348 20817 27983 ## Good -13692 -9348 0 11469 18635 ## Fair -25161 -20817 -11469 0 7166 ## Poor -32327 -27983 -18635 -7166 0 5.5.3.3 Overall difference We can measure the overall differences as the amount of variability between each of the means and the overall mean: \\[F = \\frac{\\text{between-group variability}}{\\text{within-group variability}}\\] \\[F = \\frac{\\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{i\\cdot})^2/(N-K)}\\] \\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2\\] 5.5.3.4 Creating a test statistic NHANES %&gt;% select(HHIncomeMid, HealthGen) %&gt;% filter(!is.na(HealthGen)&amp; !is.na(HHIncomeMid)) %&gt;% head() ## # A tibble: 6 x 2 ## HHIncomeMid HealthGen ## &lt;int&gt; &lt;fct&gt; ## 1 30000 Good ## 2 30000 Good ## 3 30000 Good ## 4 40000 Good ## 5 87500 Vgood ## 6 87500 Vgood GM = mean(NHANES$HHIncomeMid, na.rm=TRUE); GM ## [1] 57206 NH.means ## # A tibble: 5 x 3 ## HealthGen IncMean count ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Excellent 69354. 817 ## 2 Vgood 65011. 2342 ## 3 Good 55662. 2744 ## 4 Fair 44194. 899 ## 5 Poor 37027. 164 NH.means$IncMean - GM ## [1] 12148 7805 -1544 -13013 -20179 (NH.means$IncMean - GM)^2 ## [1] 1.48e+08 6.09e+07 2.38e+06 1.69e+08 4.07e+08 NH.means$count ## [1] 817 2342 2744 899 164 NH.means$count * (NH.means$IncMean - GM)^2 ## [1] 1.21e+11 1.43e+11 6.54e+09 1.52e+11 6.68e+10 \\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2\\] sum(NH.means$count * (NH.means$IncMean - GM)^2) ## [1] 4.89e+11 5.5.3.5 Permuting the data NHANES %&gt;% filter(!is.na(HealthGen)&amp; !is.na(HHIncomeMid)) %&gt;% mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %&gt;% select(HealthGen, HHIncomeMid, IncomePerm) %&gt;% head() ## # A tibble: 6 x 3 ## HealthGen HHIncomeMid IncomePerm ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Good 30000 100000 ## 2 Good 30000 87500 ## 3 Good 30000 17500 ## 4 Good 40000 60000 ## 5 Vgood 87500 50000 ## 6 Vgood 87500 100000 5.5.3.6 Permuting the data &amp; a new test statistic NHANES %&gt;% filter(!is.na(HealthGen)&amp; !is.na(HHIncomeMid)) %&gt;% mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %&gt;% group_by(HealthGen) %&gt;% summarize(IncMeanP = mean(IncomePerm), count=n()) %&gt;% summarize(teststat = sum(count*(IncMeanP - GM)^2)) ## # A tibble: 1 x 1 ## teststat ## &lt;dbl&gt; ## 1 12553059728. 5.5.3.7 Lots of times… reps = 1000 SSB &lt;- unlist(replicate(reps, NHANES %&gt;% filter(!is.na(HealthGen)&amp; !is.na(HHIncomeMid)) %&gt;% mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %&gt;% group_by(HealthGen) %&gt;% summarize(IncMeanP = mean(IncomePerm), count=n()) %&gt;% summarize(teststat = sum(count*(IncMeanP - GM)^2)) )) head(SSB) ## teststat teststat teststat teststat teststat teststat ## 1.45e+10 1.60e+10 1.38e+10 1.32e+10 1.37e+10 1.44e+10 5.5.3.8 Compared to the real data obsSSB &lt;- NHANES %&gt;% filter(!is.na(HealthGen) &amp; !is.na(HHIncomeMid)) %&gt;% group_by(HealthGen) %&gt;% summarize(IncMean = mean(HHIncomeMid), count=n()) %&gt;% summarize(obs.teststat = sum(count*(IncMean - GM)^2)) obsSSB ## # A tibble: 1 x 1 ## obs.teststat ## &lt;dbl&gt; ## 1 488767088754. sum(SSB&gt;obsSSB) / reps ## [1] 0 hist(SSB, xlim=c(0, 6e+11)); abline(v=obsSSB) References "],
["boot.html", "Chapter 6 Bootstrapping 6.1 10/8/19 Agenda 6.2 Introduction 6.3 Basics &amp; Notation 6.4 10/10/19 Agenda 6.5 Bootstrap Confidence Intervals 6.6 R example: Heroin", " Chapter 6 Bootstrapping 6.1 10/8/19 Agenda Review: logic of confidence intervals Logic of bootstrapping (resample from the sample with replacement) BS SE of a statistic 6.2 Introduction As we did with permutation tests, we are going to use random samples to describe the population (assuming we have a simple random sample). Main idea: we will be able to estimate the variability of our estimator (difference in medians, ordinary least square with non-normal errors, etc.). It’s not so strange to get \\(\\hat{\\theta}\\) and SE(\\(\\hat{\\theta}\\)) from the data (consider \\(\\hat{p}\\) &amp; \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) and \\(\\overline{X}\\) &amp; \\(s/\\sqrt{n}\\)). We’ll only consider confidence intervals for now. Bootstrapping doesn’t help get around small samples. The following applets may be helpful: The logic of confidence intervals http://www.rossmanchance.com/applets/ConfSim.html Bootstrapping from actual datasets http://lock5stat.com/statkey/index.html 6.3 Basics &amp; Notation Let \\(\\theta\\) be the parameter of interest, and let \\(\\hat{\\theta}\\) be the estimate of \\(\\theta\\). If we could, we’d take lots of samples of size \\(n\\) from the population to create a sampling distribution for \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples from \\(F\\): \\[\\begin{align} \\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{i=1}^B \\hat{\\theta}_i \\end{align}\\] is our best guess for \\(\\theta\\). If \\(\\hat{\\theta}\\) is very different from \\(\\theta\\), we would call it biased. \\[\\begin{align} SE(\\hat{\\theta}) &amp;= \\bigg[ \\frac{1}{B-1} \\sum_{i=1}^B(\\hat{\\theta}_i - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\ q_1 &amp;= [0.25 n] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\ q_3 &amp;= [0.75 n] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\ \\end{align}\\] If we could, we would completely characterize the sampling distribution (as a function of \\(\\theta\\)) which would allow us to make inference on \\(\\theta\\) when we only had \\(\\hat{\\theta}\\). Figure 1.2: From Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig 6.3.1 The Plug-in Principle Recall \\[\\begin{align} F(x) &amp;= P(X \\leq x)\\\\ \\hat{F}(x) &amp;= S(x) = \\frac{\\# \\{X_i \\leq x\\} }{n} \\end{align}\\] \\(\\hat{F}(x)\\) is a sufficient statistic for \\(F(x)\\). That is, all the information about \\(F\\) that is in the data is contained in \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) is the MLE of \\(F(x)\\) (they are both probabilities, so it’s a binomial argument). Note that, in general, we are interested in a parameter, \\(\\theta\\). \\[\\begin{align} \\theta = t(F) \\ \\ \\ \\ [\\mbox{e.g., } \\mu = \\int x f(x) dx ] \\end{align}\\] The plug-in estimate of \\(\\theta\\) is: \\[\\begin{align} \\hat{\\theta} = t(\\hat{F}) \\ \\ \\ \\ [\\mbox{e.g., } \\overline{X} = \\frac{1}{n} \\sum X_i ] \\end{align}\\] That is: to estimate a parameter, use the statistic that is the corresponding quantity for the sample. \\[\\begin{align} \\mbox{Ideal Real World} &amp; \\mbox{Boostrap World}\\\\ F \\rightarrow x &amp;\\Rightarrow \\hat{F} \\rightarrow x^*\\\\ \\downarrow &amp; \\downarrow\\\\ \\hat{\\theta} &amp; \\hat{\\theta}^* \\end{align}\\] The idea of boostrapping (and in fact, the bootstrap samples themselves), depends on the double arrow. We must have a random sample: that is, \\(\\hat{F}\\) must do a good job of estimating \\(F\\) in order for bootstrap concepts to be meaningful. Note that you’ve seen the plug-in-principle before: \\[\\begin{align} \\sqrt{\\frac{p(1-p)}{n}} &amp;\\approx&amp; \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\ \\end{align}\\] 6.3.2 The Bootstrap Idea We can resample from the sample to represent samples from the actual population! The boostrap distribution of a statistic, based on many resamples, represents the sampling distribution of the statistic based on many samples. Is this okay?? What are we assuming? As \\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\) As \\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (with large \\(n\\)). Or really, what we typically see if \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) or \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\) 6.3.3 Bootstrap Procedure Resample data with replacement. Calculate the statistic of interest for each resample. Repeat 1. and 2. \\(B\\) times. Use the bootstrap distribution for inference. In R: install.packages(&#39;bootstrap&#39;) library(bootstrap) install.packages(&#39;boot&#39;) library(boot) ?bootstrap ?boott ?bootpred 6.3.4 Bootstrap Notation Take lots (\\(B\\)) of sample of size n from the sample, \\(\\hat{F}(x)\\) (instead of from the population, \\(F(x)\\) ) to create a bootstrap distribution for \\(\\hat{\\theta}^*\\) (instead of the sampling distribution for \\(\\hat{\\theta}\\)). Let \\(\\hat{\\theta}^*(b)\\) be the calculated statistic of interest for the \\(b^{th}\\) bootstrap sample. Our best guess for \\(\\theta\\) is: \\[\\begin{align} \\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*(b) \\end{align}\\] (if \\(\\hat{\\theta}^*\\) is very different from \\(\\hat{\\theta}\\), we call it biased.) And the estimated value for the standard error of our estimate is \\[\\begin{align} \\hat{SE}^* = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*(b) - \\hat{\\theta}^*)^2 \\bigg]^{1/2} \\end{align}\\] Just like repeatedly taking samples from the population, taking resamples from the sample allows us to characterize the bootstrap distribution which approximates the sampling distribution. The bootstrap distribution approximates the shape, spread, &amp; bias of the actual sampling distribution. Figure 1.3: From Hesterberg et al., Chapter 16 of Introduction to the Practice of Statistics by Moore, McCabe, and Craig. The left image represents the mean with n=50. The center image represents the mean with n=9. The right image represents the median with n=15. 6.4 10/10/19 Agenda Normal CI using BS SE Bootstrap-t (studentized) CIs Percentile CIs properties / advantages / disadvantages The StatKey applets which demonstrate bootstrapping are here: http://www.lock5stat.com/StatKey/ 6.5 Bootstrap Confidence Intervals 6.5.1 Normal (standard) CI with BS SE: type=&quot;norm&quot; Keep in mind that what we are trying to do is approximate the sampling distribution of \\(\\hat{\\theta}\\). In fact, what we are really able to do here is to estimate the sampling distribution of \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). We hope that: \\[\\begin{align} \\hat{F}\\Big(\\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)} \\Big) \\rightarrow F\\Big(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\Big) \\end{align}\\] Recall the derivation of conventional confidence intervals (based on the assumption that the sampling distribution of the test statistic is normal or close): \\[\\begin{align} P\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&amp;= 1 - \\alpha\\\\ P\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &amp;= 1 - \\alpha\\\\ \\end{align}\\] That is, it’s the endpoints that are random, and we have a 0.95 probability that we’ll get a random sample which will produce endpoints which will capture the true parameter. A 95% CI for \\(\\theta\\) would then be: \\[\\hat{\\theta} \\pm z_{(\\alpha/2)} \\hat{SE}^*\\] 6.5.2 Bootstrap-t Confidence Intervals: type=&quot;stud&quot; (The idea here is that we are calculating the “t-multiplier” that is used in the CI. It was William Gosset who went my the pseudonym of “Student” who originally figured out the distribution of the t-multiplier, so the following intervals are called either “studentized” or “t” bootstrap confidence intervals.) Recall the derivation of conventional confidence intervals: \\[\\begin{align} P\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&amp;= 1 - \\alpha\\\\ P\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &amp;= 1 - \\alpha\\\\ \\end{align}\\] That is, it’s the endpoints that are random, and we have a 0.95 probability that we’ll get a random sample which will produce endpoints which will capture the true parameter. We could simply use the BS SE within the CI formula (and did for the interval above). The problem is that such an interval will only be accurate if the distribution for \\(\\hat{\\theta}\\) is reasonably normal. If there is any bias or skew, the CI will not have desired coverage levels (Efron and Tibshirani (1993), pg 161 and chapter 22). Now consider using the bootstrap to estimate the distribution for \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). \\[\\begin{align} T^*(b) &amp;= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)} \\end{align}\\] where \\(\\hat{\\theta}^*(b)\\) is the value of \\(\\hat{\\theta}\\) for the \\(b^{th}\\) bootstrap sample, and \\(\\hat{SE}^*(b)\\) is the estimated standard error of \\(\\hat{\\theta}^*(b)\\) for the \\(b^{th}\\) bootstrap sample. The \\(\\alpha^{th}\\) percentile of \\(T^*(b)\\) is estimated by the value of \\(\\hat{t}_\\alpha\\) such that \\[\\begin{align} \\frac{\\# \\{T^*(b) \\leq \\hat{t}_{\\alpha/2} \\} }{B} = \\alpha/2 \\end{align}\\] For example, if \\(B=1000\\), the estimate of the 5% point is the \\(50^{th}\\) smallest value of the \\(T^*(b)\\)s, and the estimate of the 95% point is the \\(950^{th}\\) smallest value of the \\(T^*(b)\\)s. Finally, the boostrap-t confidence interval is: \\[\\begin{equation} (\\hat{\\theta} - \\hat{t}_{1-\\alpha/2}\\hat{SE}^*, \\hat{\\theta} - \\hat{t}_{\\alpha/2}\\hat{SE}^*) \\tag{6.1} \\end{equation}\\] To find a bootstrap-t interval, we have to bootstrap twice. The algorithm is as follows: Generate \\(B_1\\) bootstrap samples, and for each sample \\(\\underline{X}^{*b}\\) compute the bootstrap estimate \\(\\hat{\\theta}^*(b)\\). Take \\(B_2\\) bootstrap samples from \\(\\underline{X}^{*b}\\), and estimate the standard error, \\(\\hat{SE}^*(b)\\). Find \\(B_1\\) values for \\(T^*(b)\\). Calculate \\(\\hat{t}_\\alpha/2\\) and \\(\\hat{t}_{1-\\alpha/2}\\). Calculate the CI as in equation ((6.1)). If \\(B\\cdot \\alpha\\) is not an integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) and \\(B+1-k\\). Bootstrap-t intervals are somewhat erratic and can be influenced by a few outliers. Percentile methods can be more reliable. [The balance of which is best when is an open question depending a lot on the data distribution and statistic of interest.] \\(B=100\\) or 200 is probably not enough for a bootstrap-t CI (500 or 1000 is better). However, \\(B=25\\) may be enough to estimate the SE in the inner-BS procedure. (\\(B=1000\\) is needed for computing percentiles.) In choosing the appropriate multiplier: When it is the correct multiplier to use, the normal multiplier (\\(z\\)) is good for all \\(n\\) and all samples. When it is the correct multiplier to use, the t multiplier is good for all samples but a specified \\(n\\). When it is the correct multiplier to use, the bootstrap-t multiplier is good for *this} sample only. The resulting intervals will typically not be symmetric (that is \\(\\hat{t}_\\alpha \\ne - \\hat{t}_{1-\\alpha}\\)). This is part of the improvement over \\(z\\) or \\(t\\) intervals. Bootstrap-t intervals are good for location statistics (mean, quantiles, trimmed means) but cannot be trusted for other statistics like the correlation (which do not necessarily vary based on ideas of shift). 6.5.3 Percentile Confidence Intervals: type=&quot;perc&quot; The interval between the \\(\\alpha/2\\) and \\(1-\\alpha/2\\) quantiles of the bootstrap distribution of a statistic is a \\((1-\\alpha)100\\%\\) bootstrap percentile confidence interval for the corresponding parameter: \\[\\begin{align} [\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}] \\end{align}\\] You do not need to know why the percentile interval works… but isn’t it so cool to see how it works??? Why does it work? It isn’t immediately obvious that the interval above will capture the true parameter, \\(\\theta\\), at a rate or 95%. Consider a skewed sampling distribution. If your \\(\\hat{\\theta}\\) comes from the long tail, is it obvious that the short tail side of your CI will get up to the true parameter value at the correct rate? (Hall (*The Bootstrap and Edgeworth Expansion}, Springer, 1992, and earlier papers) refers to these as Efron’s “backwards” intervals.) Or, if your sampling distribution is biased, the percentiles of the bootstrap interval won’t capture the parameter with the correct rate. To see how / why percentiles intervals work, we first start by considering normal sampling distributions for a function of our statistic. Let \\(\\phi = g(\\theta), \\hat{\\phi} = g(\\hat{\\theta}), \\hat{\\phi}^* = g(\\hat{\\theta}^*)\\), where g is a monotonic function (assume wlog that g is increasing). The point is to choose (if possible) \\(g(\\cdot)\\) such that \\[\\begin{equation} \\hat{\\phi}^* - \\hat{\\phi} \\sim \\hat{\\phi} - \\phi \\sim N(0, \\sigma^2) \\tag{6.2}. \\end{equation}\\] Again, consider the logic for the conventional confidence interval. Because \\(\\hat{\\phi} - \\phi \\sim N(0, \\sigma^2)\\), the interval for \\(\\theta\\) is derived by: \\[\\begin{align} P(z_{0.05} \\leq \\frac{\\hat{\\phi} - \\phi}{\\sigma} ) = 0.95 \\nonumber \\\\ P(-\\infty \\leq \\phi \\leq \\hat{\\phi} - z_{0.05} \\sigma) = 0.95 \\nonumber \\\\ P(-\\infty \\leq \\phi \\leq \\hat{\\phi} + z_{0.95} \\sigma) = 0.95 \\nonumber \\\\ P(-\\infty \\leq \\theta \\leq g^{-1}(\\hat{\\phi} + z_{0.95} \\sigma)) = 0.95 \\nonumber \\\\ \\Rightarrow \\mbox{CI for } \\theta: \\ \\ \\ (-\\infty, g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})) \\tag{6.3} \\end{align}\\] where \\(z_{1-\\alpha}\\) is the \\(100(1-\\alpha)\\) percent point of the standard normal distribution. Ideally, if we knew \\(g\\) and \\(\\sigma\\), we’d be able to do the transformation and find \\(g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})\\) (which would give the endpoint of the confidence interval). Going back to ((6.2)) indicates that \\(\\hat{\\phi} + \\sigma z_{1-\\alpha} = F^{-1}_{\\hat{\\phi}^*}(1-\\alpha)\\) (because \\(\\hat{\\phi} ^* \\sim N(\\hat{\\phi}, \\sigma^2)\\)). Further, since \\(g\\) is monotonically increasing, \\(F^{-1}_{\\hat{\\phi}^*}(1-\\alpha) = g(F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\\) Substituting in ((6.3)), gives the percentile interval for \\(\\theta\\), \\[\\begin{align} (-\\infty, F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)). \\end{align}\\] (A similar argument gives the same derivation of the two sided confidence interval. Proof from Carpenter and Bithell (2000)) In order for a percentile interval to be appropriate, we simply need to know that a normalizing transformation exists. We do not need to actually find the transformation! The transformation respecting property A CI is transformation respecting if, for any monotone transformation, the CI for the transformed parameter is simply the transformed CI for the unstransformed parameter. Let \\(\\phi = m(\\theta)\\). \\[\\begin{align} [\\phi_{lo}, \\phi_{up}] = [m(\\theta_{lo}), m(\\theta_{up})] \\end{align}\\] Note that the idea has to do with the process of creating the CI. That is, if we create the confidence interval using \\(\\phi\\), we’ll get the same thing as if we created the CI using \\(\\theta\\) and then transformed it. It is straightforward to see that the percentile CI is transformation respecting. That is, for any monotone transformation of the statistic and parameter, the CI will be transformed appropriately. Let \\[\\begin{align} \\hat{\\phi} &amp;= 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg)\\\\ r &amp;=\\frac{e^{2\\phi}+1}{e^{2\\phi}-1}\\\\ \\end{align}\\] We know that \\(\\hat{\\phi}\\) does have an approximated normal distribution. So, the percentile CI for \\(\\phi\\) will approximate the normal theory CI which we know to be correct (for a given \\(\\alpha\\)). But once we have a CI for \\(\\phi\\) we can find the CI for \\(\\rho\\) by taking the inverse monotonic transformation; or rather… we can just use the r percentile CI to start with! The range preserving property Another advantage of the percentile interval is that it is range preserving. That is, the CI always produces endpoints that fall within the allowable range of the parameter. Bias The percentile interval is not, however, perfect. If the statistic is a biased estimator of the parameter, there will not exist a transformation such that the distribution is centered around the correct function of the parameter. Formally, if \\[\\begin{align} \\hat{\\theta} \\sim N(\\theta + bias, \\hat{SE}^2) \\end{align}\\] no transformation \\(\\phi = m(\\theta)\\) can fix things up. Keep in mind that standard intervals can fail in a variety of ways, and the percentile method has simply fixed the situation when the distribution is non-normal. 6.5.4 What makes a CI good? Symmetry (??): the interval is symmetric, pivotal around some value. Not necessarily a good thing. Maybe a bad thing to force? Resistant: BS-t is particularly not resistant to outliers or crazy sampling distributions of the statistic (can make it more robust with a variance stabilizing transformation) Range preserving: the CI always contains only values that fall within an allowable range (\\(p, \\rho\\),…) Transformation respecting: for any monotone transformation, \\(\\phi = m(\\theta)\\), the interval for \\(\\theta\\) is simply mapped by \\(m(\\theta)\\). If \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) is a \\((1-\\alpha)100\\)% interval for \\(\\theta\\), then \\[\\begin{align} [\\hat{\\phi}_{(lo)},\\hat{\\phi}_{(hi)}] = [m(\\hat{\\theta}_{(lo)}),m(\\hat{\\theta}_{(hi)})] \\end{align}\\] are exactly the same interval. Level of confidence: A central (not symmetric) confidence interval, \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) should have probability \\(\\alpha/2\\) of not covering \\(\\theta\\) from above or below: \\[\\begin{align} P(\\theta &lt; \\hat{\\theta}_{(lo)})&amp;=\\alpha/2\\\\ P(\\theta &gt; \\hat{\\theta}_{(hi)})&amp;=\\alpha/2\\\\ \\end{align}\\] Note: all of our intervals are approximate. We judge them based on how accurately they cover \\(\\theta\\). A CI is first order accurate if: \\[\\begin{align} P(\\theta &lt; \\hat{\\theta}_{(lo)})&amp;=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\ P(\\theta &gt; \\hat{\\theta}_{(hi)})&amp;=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\ \\end{align}\\] A CI is second order accurate if: \\[\\begin{align} P(\\theta &lt; \\hat{\\theta}_{(lo)})&amp;=\\alpha/2 + \\frac{const_{lo}}{n}\\\\ P(\\theta &gt; \\hat{\\theta}_{(hi)})&amp;=\\alpha/2+ \\frac{const_{hi}}{n}\\\\ \\end{align}\\] BS-t is \\(2^{nd}\\) order accurate for a large general class of functions. However, in practice, the coverage rate doesn’t kick in for small/med sample sizes unless appropriate transformations make the distribution more bell-shaped. (Tibshirani 1988) CI Symmetric Range Resp Trans Resp Accuracy Normal Samp Dist? Other BS SE Yes No No \\(1^{st}\\) order Yes param assump \\(F(\\hat{\\theta})\\) BS-t No No No \\(2^{nd}\\) order Yes/No computer intensive perc No Yes Yes \\(1^{st}\\) order No small \\(n \\rightarrow\\) low accuracy BCa No Yes Yes \\(2^{nd}\\) order No limited param assump All of the above criteria speak to the coverage rates of the parameters. But note that they must be taken in context. Much also depends on: the choice of statistic itself; the original data distribution; any outlying observations; etc. 6.5.4.1 Advantages and Disadvantages Normal Approximation Advantages similar to the familiar parametric approach; useful with a normally distributed \\(\\hat{\\theta}\\); requires the least computation (\\(B=50-200\\)) Disadvantages fails to use the entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); only works if \\(\\hat{\\theta}\\) is reasonably normal to start with Bootstrap-t Confidence Interval Advantages highly accurate CI in many cases; handles skewed \\(F(\\hat{\\theta})\\) better than the percentile method Disadvantages not invariant to transformations; computationally expensive with the double bootstrap; coverage probabilities are best if the distribution of \\(\\hat{\\theta}\\) is nice (e.g., normal) Percentile Advantages uses the entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) to be asymmetrical; invariant to transformations; range respecting; simple to execute Disadvantages small samples may result in low accuracy (because of the dependence on the tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) to be unbiased BCa Advantages all of those of the percentile method; allows for bias in \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can be calculated easily from \\(\\hat{F}^*(\\hat{\\theta}^*)\\) Disadvantages requires a limited parametric assumption; more computational than other intervals 6.5.4.2 Bootstrap CI and Hypothesis Testing If a null value for a parameter is not contained in a CI, we reject the null hypothesis; similarly, we do not reject a null value if it does lie inside the CI. Using BS CIs, we can apply the same logic, and test any hypothesis of interest (note: we can always create one-sided intervals as well!). But simply using CIs leaves out the p-value information. How do we get a p-value from a CI? Consider an alternative definition for the p-value: p-value: The smallest level of significance at which you would reject \\(H_0\\). So, what we want is for the null value (\\(\\theta_0\\)) to be one of the endpoints of the confidence interval with some level of confidence \\(1-2\\alpha_0\\). \\(\\alpha_0\\) will then be the one-sided p-value, \\(2\\alpha_0\\) will be the two-sided p-value. For percentile intervals, \\[\\begin{align} p-value = \\alpha_0 = \\frac{\\# \\hat{\\theta}^*(b) &lt; \\theta_0}{B} \\end{align}\\] (without loss of generality, assuming we set \\(\\hat{\\theta}^*_{lo} = \\theta_0\\)). BCa CI: type=&quot;bca&quot; Another cool bootstrap CI method that we won’t have time to cover. You are not responsible for the remainder of the bootstrap material in these notes. In the percentile method, we’ve assumed that there exists a transformation of \\(\\theta\\), \\(\\phi(\\theta)\\), such that \\[\\begin{align} \\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(0,1) \\end{align}\\] The transformation assumes that neither \\(\\theta\\) nor \\(\\phi\\) are biased, and it assumes that the variance is constant for all values of the parameter. That is, in the percentage intervals, we assume the normalizing transformation creates a sampling distribution that is unbiased and variance stabilizing. Consider a monotone transformation that *normalizes} the sampling distribution (we no longer assume unbiased or constant variance). We now consider the case where \\(\\theta\\) is a biased estimator. That is: \\[\\begin{align} \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{c} \\sim N(-z_0,1) \\end{align}\\] We’ve corrected for the bias, but if there is non-constant variance, we need a further adjustment to stabilize the variance: \\[\\begin{align} \\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(-z_0 \\sigma_\\phi,\\sigma_\\phi), \\ \\ \\ \\ \\ \\ \\sigma_\\phi = 1 + a \\phi \\end{align}\\] That is, there must exist a monotone transformation \\(\\phi\\) such that \\(\\phi(\\hat{\\theta}) \\sim N\\) where \\[\\begin{align} E(\\phi(\\hat{\\theta})) = \\phi(\\theta) - z_0 [1 + a \\phi(\\theta)] &amp; SE(\\phi(\\hat{\\theta})) = 1 + a \\phi(\\theta) \\end{align}\\] (Note: in the expected value and SE we’ve assumed that \\(c=1\\). If \\(c\\ne1\\), then we can always choose a different transformation, \\(\\phi&#39;\\) so that \\(c=1\\).) Then \\[\\begin{align} P(z_{\\alpha/2} \\leq \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{1 + a \\phi(\\theta)} + z_0 \\leq z_{1-\\alpha/2}) = 1 - \\alpha \\end{align}\\] A $(1-)$100% CI for \\(\\phi(\\theta)\\) is \\[\\begin{align} \\bigg[ \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{1 + a (z_{1-\\alpha/2} - z_0)}, \\frac{\\phi(\\hat{\\theta}) - (z_{\\alpha/2} - z_0)}{1 + a (z_{\\alpha/2} - z_0)} \\bigg] \\end{align}\\] Let’s consider an interesting probability question: \\[\\begin{align} P\\bigg( \\phi(\\hat{\\theta}^*) &amp;\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))} \\bigg) = ?\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &amp;\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta}) - \\phi(\\hat{\\theta})a(z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &amp;\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta})a(z_{1-\\alpha/2} - z_0)}{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &amp;\\leq \\frac{ -(1+a \\phi(\\hat{\\theta})) (z_{1-\\alpha/2} - z_0) }{(1 + a (z_{1-\\alpha/2} - z_0))(1+a \\phi(\\hat{\\theta}))} \\bigg)\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &amp;\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) }{(1 + a (z_{1-\\alpha/2} - z_0))} \\bigg)\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} &amp;\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} \\bigg)\\\\ = P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + a \\phi(\\hat{\\theta})} + z_0 &amp;\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\ = P\\bigg( Z &amp;\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg) = \\gamma_1\\\\ \\mbox{where } \\gamma_1 &amp;= \\Phi \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\ &amp;= \\verb;pnorm; \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg) \\end{align}\\] What we’ve shown is that the \\(\\gamma_1\\) quantile of the \\(\\phi(\\hat{\\theta}^*)\\) sampling distribution will be a good estimate for the lower bound of the confidence interval for \\(\\phi(\\theta)\\). Using the same argument on the upper bound, we find a $(1-)$100% confidence interval for \\(\\phi(\\theta)\\) to be: \\[\\begin{align} &amp;[\\phi(\\hat{\\theta}^*)_{\\gamma_1}, \\phi(\\hat{\\theta}^*)_{\\gamma_2}]\\\\ &amp; \\\\ \\mbox{where } \\gamma_1 &amp;= \\Phi\\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - a (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\ \\gamma_2 &amp;= \\Phi \\bigg(\\frac{ (z_{1-\\alpha/2} + z_0) }{(1 - a (z_{1-\\alpha/2} + z_0))} + z_0 \\bigg)\\\\ \\end{align}\\] Using the transformation respecting property of percentile intervals, we know that a $(1-)$100% confidence interval for \\(\\theta\\) is: \\[\\begin{align} &amp;[\\hat{\\theta}^*_{\\gamma_1}, \\hat{\\theta}^*_{\\gamma_2}] \\end{align}\\] How do we estimate \\(a\\) and \\(z_0\\)? bias: \\(z_0\\) is a measure of the bias. Recall: \\[\\begin{align} bias &amp;= E(\\hat{\\theta}) - \\theta\\\\ \\hat{bias} &amp;= \\hat{\\theta}^* - \\hat{\\theta}\\\\ \\end{align}\\] But remember that \\(z_0\\) represents the bias for \\(\\phi(\\hat{\\theta})\\), not for \\(\\hat{\\theta}\\) (and we don’t know \\(\\phi\\)!). So, we use \\(\\theta\\) to see what proportion of \\(\\theta\\) values are too low, and we can map it back to the \\(\\phi\\) space using the normal distribution: \\[\\begin{align} \\hat{z}_0 &amp;= \\Phi^{-1} \\bigg( \\frac{ \\# \\hat{\\theta}^*(b) &lt; \\hat{\\theta}}{B} \\bigg) \\end{align}\\] That is, if \\(\\hat{\\theta}^*\\) underestimates \\(\\hat{\\theta}\\), then \\(\\hat{\\theta}\\) likely underestimates \\(\\theta\\); \\(z_0 &gt; 0\\). We think of \\(z_0\\) and the normal quantile associated with the proportion of BS replicates less than \\(\\hat{\\theta}\\). skew: \\(a\\) is a measure of skew. \\[\\begin{align} bias&amp;= E(\\hat{\\theta} - \\theta)\\\\ var &amp;= E(\\hat{\\theta} - \\theta)^2 = \\sigma^2\\\\ skew &amp;= E(\\hat{\\theta} - \\theta)^3 / \\sigma^3\\\\ \\end{align}\\] We can think of the skew as the rate of chance of the standard error on a normalized scale. If there is no skew, we will estimate \\(a=0\\). Our estimate of \\(a\\) comes from a procedure known as the jackknife. \\[\\begin{align} \\hat{a} = \\frac{\\sum_{i=1}^n (\\hat{\\theta} - \\hat{\\theta}_{(i)})^3}{6 [ \\sum_{i=1}^n (\\hat{\\theta} - \\hat{\\theta}_{(i)})^2 ] ^{3/2}} \\end{align}\\] 6.6 R example: Heroin Hesketh and Everitt (2000) report on a study by Caplehorn and Bell (1991) that investigated the times (in days) that heroin addicts remained in a clinic for methadone maintenance treatment. The data in heroin.txt include the amount of time that the subjects stayed in the facility until treatment was terminated (column 4). For about 37% of the subjects, the study ended while they were still the in clinic (status=0). Thus, their survival time has been truncated. For this reason we might not want to estimate the mean survival time, but rather some other measure of typical survival time. Below we explore using the median as well as the 25% trimmed mean. We treat the group of 238 patients as representative of the population. (From Chance and Rossman 2018a, Investigation 4.5.3) Why bootstrap? Motivation: to estimate the variability of a statistic (not dependent on \\(H_0\\) being true). Reading in the data heroin &lt;- readr::read_table2(&quot;http://www.rossmanchance.com/iscam2/data/heroin.txt&quot;) names(heroin) ## [1] &quot;id&quot; &quot;clinic&quot; &quot;status&quot; &quot;times&quot; &quot;prison&quot; &quot;dose&quot; head(heroin) ## # A tibble: 6 x 6 ## id clinic status times prison dose ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 428 0 50 ## 2 2 1 1 275 1 55 ## 3 3 1 1 262 0 55 ## 4 4 1 1 183 0 30 ## 5 5 1 1 259 1 65 ## 6 6 1 1 714 0 55 Observed Test Statistic(s) obs.stat&lt;-heroin %&gt;% summarize(medtime = median(times)) %&gt;% pull() obs.stat2&lt;-heroin %&gt;% summarize(tmeantime = mean(times, trim=0.25)) %&gt;% pull() obs.stat ## [1] 368 obs.stat2 ## [1] 378 Bootstrapped data! set.seed(4747) heroin.rs&lt;-heroin %&gt;% sample_frac(size=1, replace=TRUE) heroin.rs %&gt;% summarize(medtime = median(times)) %&gt;% pull() ## [1] 368 heroin.rs %&gt;% summarize(tmeantime = mean(times, trim=0.25)) %&gt;% pull() ## [1] 372 Need to bootstrap a lot of times… Below is the code showing how to bootstrap using for loops (nested to create the t multipliers needed for the BS-t intervals). However, the package and funciton boot will do the boostrapping for you. test.stat&lt;-c() test.stat2&lt;-c() sd.test.stat&lt;-c() sd.test.stat2&lt;-c() reps1 &lt;- 1000 reps2 &lt;- 100 set.seed(4747) for(i in 1:reps1){ heroin.rs&lt;-heroin %&gt;% sample_frac(size=1, replace=TRUE) test.stat&lt;-c(test.stat,heroin.rs %&gt;% summarize(medtime = median(times)) %&gt;% pull()) test.stat2&lt;-c(test.stat2,heroin.rs %&gt;% summarize(tmeantime = mean(times, trim=0.25)) %&gt;% pull()) test.stat.rs&lt;-c() test.stat2.rs&lt;-c() for(j in 1:reps2){ heroin.rsrs&lt;-heroin.rs %&gt;% sample_frac(size=1, replace=TRUE) test.stat.rs&lt;-c(test.stat.rs,heroin.rsrs %&gt;% summarize(medtime = median(times)) %&gt;% pull()) test.stat2.rs&lt;-c(test.stat2.rs,heroin.rsrs %&gt;% summarize(tmeantime = mean(times, trim=0.25)) %&gt;% pull()) } sd.test.stat&lt;-c(sd.test.stat,sd(test.stat.rs)) sd.test.stat2&lt;-c(sd.test.stat2,sd(test.stat2.rs)) } What do the distributions look like? The distributions of both the median and the trimmed mean are symmetric and bell-shaped. However, the trimmed mean has a more normal distribution (as evidenced by the points of the qq plot falling on the line y=x). What do the distributions look like? The distributions of both the median and the trimmed mean are symmetric and bell-shaped. However, the trimmed mean has a more normal distribution (as evidenced by the points of the qq plot falling on the line y=x). bs.stats &lt;- data.frame(test.stat, test.stat2) ggplot(bs.stats, aes(x=test.stat)) + geom_histogram(bins=20) + ggtitle(&quot;dist of median&quot;) + xlab(paste(&quot;mean=&quot;,round(mean(test.stat),2), &quot;;SE=&quot;, round(sd(test.stat),2))) ggplot(bs.stats, aes(x=test.stat2)) + geom_histogram(bins=20) + ggtitle(&quot;dist of trimmed mean&quot;) + xlab(paste(&quot;mean=&quot;,round(mean(test.stat),2), &quot;;SE=&quot;, round(sd(test.stat),2))) OR using the built in functions sampletmean &lt;- function(x,d,trimperc){ return(mean(x[d], trim=trimperc)) } set.seed(4747) bs.tmean.resamps &lt;- boot::boot(heroin$times,sampletmean, reps1, trimperc=.25) What does the boot output look like? #bs.tmean.resamps &lt;- boot(heroin$times,sampletmean, reps1, trimperc=.25) str(bs.tmean.resamps) ## List of 11 ## $ t0 : num 378 ## $ t : num [1:1000, 1] 364 377 372 392 349 ... ## $ R : num 1000 ## $ data : num [1:238] 428 275 262 183 259 714 438 796 892 393 ... ## $ seed : int [1:626] 10403 624 -1645349161 -2081516244 1489809469 823736794 -755145325 950390200 -1779428263 1453445190 ... ## $ statistic:function (x, d, trimperc) ## ..- attr(*, &quot;srcref&quot;)= &#39;srcref&#39; int [1:8] 1 16 3 1 16 1 1 3 ## .. ..- attr(*, &quot;srcfile&quot;)=Classes &#39;srcfilecopy&#39;, &#39;srcfile&#39; &lt;environment: 0x7fa91addbff0&gt; ## $ sim : chr &quot;ordinary&quot; ## $ call : language boot::boot(data = heroin$times, statistic = sampletmean, R = reps1, trimperc = 0.25) ## $ stype : chr &quot;i&quot; ## $ strata : num [1:238] 1 1 1 1 1 1 1 1 1 1 ... ## $ weights : num [1:238] 0.0042 0.0042 0.0042 0.0042 0.0042 ... ## - attr(*, &quot;class&quot;)= chr &quot;boot&quot; ## - attr(*, &quot;boot_type&quot;)= chr &quot;boot&quot; samplemed &lt;- function(x,d){ return(median(x[d])) } set.seed(4747) bs.med.resamps &lt;- boot::boot(heroin$times,samplemed, reps1) SE of median Whew! They are very close (one using for loops, one using the boot function). sd(test.stat) # SE of median ## [1] 30.8 bs.med.resamps ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot::boot(data = heroin$times, statistic = samplemed, R = reps1) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 368 7.64 32.2 SE of trimmed mean Whew! They are very close (one using for loops, one using the boot function). sd(test.stat2) # SE of trimmed mean ## [1] 21.9 bs.tmean.resamps ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot::boot(data = heroin$times, statistic = sampletmean, R = reps1, ## trimperc = 0.25) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 378 0.635 22.3 95% normal CI with BS SE Without built in functions obs.stat + qnorm(c(.025,.975))*sd(test.stat) ## [1] 307 428 obs.stat2 + qnorm(c(.025,.975))*sd(test.stat2) ## [1] 335 421 With built in functions se.bs &lt;- sd(bs.med.resamps$t) se.bs2 &lt;- sd(bs.tmean.resamps$t) obs.stat + qnorm(c(0.025,.975))*se.bs ## [1] 304 431 obs.stat2 + qnorm(c(0.025,.975))*se.bs2 ## [1] 335 422 95% Percentile CI Without built in functions quantile(test.stat, c(.025, .975)) ## 2.5% 97.5% ## 322 450 quantile(test.stat2, c(.025, .975)) ## 2.5% 97.5% ## 337 423 With built in functions quantile(bs.med.resamps$t, c(.025, .975)) ## 2.5% 97.5% ## 320 452 quantile(bs.tmean.resamps$t, c(.025, .975)) ## 2.5% 97.5% ## 334 423 With built in functions more directly boot::boot.ci(bs.med.resamps, type=&quot;perc&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.med.resamps, type = &quot;perc&quot;) ## ## Intervals : ## Level Percentile ## 95% (320, 452 ) ## Calculations and Intervals on Original Scale boot::boot.ci(bs.tmean.resamps, type=&quot;perc&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.tmean.resamps, type = &quot;perc&quot;) ## ## Intervals : ## Level Percentile ## 95% (334, 423 ) ## Calculations and Intervals on Original Scale 95% Bootstrap-t CI Without built in functions Note that the t-value is needed (which requires a different SE for each bootstrap sample). t.hat&lt;-(test.stat - obs.stat)/sd.test.stat t.hat2&lt;-(test.stat2 - obs.stat2)/sd.test.stat2 t.hat.95 = quantile(t.hat, c(.025,.975)) t.hat2.95 = quantile(t.hat2, c(.025,.975)) obs.stat + t.hat.95*sd(test.stat) ## 2.5% 97.5% ## 322 446 obs.stat2 + t.hat2.95*sd(test.stat2) ## 2.5% 97.5% ## 337 423 With built in functions Trimmed mean: sampletmean2 &lt;- function(x, d, R2, trimperc) { boot.samp = x[d] # boostrapped sample m.bs = mean(boot.samp, trim=trimperc) # bootstrapped mean v.bs = var(boot::boot(boot.samp, sampletmean, R2, trim=trimperc)$t) return(c(m.bs, v.bs)) # boot expects the statistic to be the 1st and the var to be the 2nd } set.seed(4747) bs.tmean.reresamps &lt;- boot::boot(heroin$times, sampletmean2, R=reps1, R2=reps2, trimperc=.25) Median: samplemed2 &lt;- function(x, d, R2) { boot.samp = x[d] # boostrapped sample m.bs = median(boot.samp) # bootstrapped mean v.bs = var(boot::boot(boot.samp, samplemed, R2)$t) return(c(m.bs, v.bs)) # boot expects the statistic to be the 1st and the var to be the 2nd } set.seed(4747) bs.med.reresamps &lt;- boot::boot(heroin$times, samplemed2, R=reps1, R2=reps2) The confidence intervals (BS-t intervals, called “studentized”): boot::boot.ci(bs.med.reresamps, type=&quot;stud&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.med.reresamps, type = &quot;stud&quot;) ## ## Intervals : ## Level Studentized ## 95% (292, 421 ) ## Calculations and Intervals on Original Scale boot::boot.ci(bs.tmean.reresamps, type=&quot;stud&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.tmean.reresamps, type = &quot;stud&quot;) ## ## Intervals : ## Level Studentized ## 95% (335, 423 ) ## Calculations and Intervals on Original Scale 95% BCa interval (not responsible for BCa) {-} With built in functions boot::boot.ci(bs.med.reresamps, type=&quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.med.reresamps, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% (316, 450 ) ## Calculations and Intervals on Original Scale boot::boot.ci(bs.tmean.reresamps, type=&quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 1000 bootstrap replicates ## ## CALL : ## boot::boot.ci(boot.out = bs.tmean.reresamps, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% (333, 423 ) ## Calculations and Intervals on Original Scale Without built in functions test.stat.jk&lt;-c() test.stat2.jk&lt;-c() set.seed(4747) for(i in 1:length(heroin$times)){ test.stat.jk&lt;-c(test.stat.jk,median(heroin$times[-i])) test.stat2.jk&lt;-c(test.stat2.jk,mean(heroin$times[-i],trim=.25)) } zo.hat&lt;-qnorm(sum(test.stat&lt;obs.stat)/reps1,0,1) a.hat&lt;- sum((mean(test.stat.jk) - test.stat.jk)^3)/ (6*(sum((mean(test.stat.jk)-test.stat.jk)^2)^1.5)) zo.hat2&lt;- qnorm(sum(test.stat2&lt; obs.stat2)/reps1,0,1) a.hat2&lt;- sum((mean(test.stat2.jk) - test.stat2.jk)^3)/ (6*(sum((mean(test.stat2.jk)-test.stat2.jk)^2)^1.5)) alpha1.bca&lt;-pnorm(zo.hat + (zo.hat + qnorm(.975))/(1 - a.hat*(zo.hat + qnorm(.975)))) alpha2.bca&lt;-pnorm(zo.hat + (zo.hat + qnorm(.025))/(1 - a.hat*(zo.hat + qnorm(.025)))) alpha1.bca2&lt;-pnorm(zo.hat2 + (zo.hat2 + qnorm(.975))/(1 - a.hat2*(zo.hat2 + qnorm(.975)))) alpha2.bca2&lt;-pnorm(zo.hat2 + (zo.hat2 + qnorm(.025))/(1 - a.hat2*(zo.hat2 + qnorm(.025)))) c(sort(test.stat)[ceiling(reps1*alpha2.bca)],sort(test.stat)[ceiling(reps1*alpha1.bca)]) ## [1] 317 444 c(sort(test.stat2)[ceiling(reps1*alpha2.bca2)],sort(test.stat2)[ceiling(reps1*alpha1.bca2)]) ## [1] 337 424 Comparison of intervals The first three columns correspond to the CIs for the true median of the survival times. The second three columns correspond to the CIs for the true trimmed mean of the survival times. median trimmed mean CI lower observed upper lower observed upper — —– —– —– —– —— ——– Percentile 321.00 367.50 452.00 339.38 378.30 423.46 CI w BS SE 306.33 367.50 428.67 335.21 378.30 421.39 BS-t 294.98 367.50 418.00 334.28 378.30 418.09 BCa 317.00 367.50 444.00 338.29 378.30 422.43 References "],
["ethics.html", "Chapter 7 Ethics 7.1 10/24/19 Agenda 7.2 Doing Data Science 7.3 Graphics 7.4 p-hacking 7.5 Human Subjects Research 7.6 Authorship 7.7 Algorithms 7.8 Guiding Ethical Principles", " Chapter 7 Ethics 7.1 10/24/19 Agenda graphics p-hacking human subjects research authorship algorithms guided ethical principles 7.2 Doing Data Science Questions to ask yourself in every single data analysis you perform (taken from Data Science for Social Good at the University of Chicago https://dssg.uchicago.edu/): What biases may exist in the data you’ve been given? How can you find out? How will your choices with tuning parameters affect different populations represented in the data? How do you know you aren’t getting the right answer to the wrong question? How would you justify what you’d built to someone whose welfare is made worse off by the implementation of your algorithm? See the section on bias in modeling (4.4.1) for times when there are no inherent biases but the structure of the data create unequal model results. 7.3 Graphics There are so many ways to lie with graphics. You may already be familiar with the book How to Lie with Statistics. Many of the ideas are replicated here: http://www.rci.rutgers.edu/~roos/Courses/grstat502/wainer.pdf [just the plots are provided here: http://www.stat.berkeley.edu/~nolan/stat133/Fall05/lectures/DirtyDozen.pdf] For a recent and relevant example, consider the following image. What do you think is wrong? (Hint: examine the y-axis carefully) Figure 1.2: Reproduction of a data graphic reporting the number of gun deaths in Florida over time. The original image was published by Reuters. (Baumer, Kaplan, and Horton 2017) Or another plot that has gotten a lot of press is the following. What is wrong with this plot? (Hint: again, think about the y-axis) Figure 1.3: A tweet by National Review on December 14, 2015 showing the change in global temperature over time. (Baumer, Kaplan, and Horton 2017) 7.4 p-hacking Great applet from 538 on how to get significance by just trying enough things: https://projects.fivethirtyeight.com/p-hacking/ J. Ioannidis, Why most published research findings are false. PLoS Medicine, 2. e124 2005. Ioannidis focuses on multiple testing with specific understanding of the effect of testing in three different contexts: When looking for as many possible significant findings as possible (publish or perish) When bias exists in our work When the researchers study the same effect 7.4.1 Multiple Studies \\(\\alpha\\) If a study is null, the probability of seeing null is \\((1-\\alpha)\\) If 3 of us test the same thing, the probability that we will all see null is \\((1-\\alpha)^3\\) and the probability that at least one of use will see significance goes from \\(\\alpha\\) to \\(1 - (1-\\alpha)^3\\) As \\(n \\uparrow\\) someone will definitely see significance (bad!) \\(\\beta\\) If a study is significant, the probability of seeing null is \\(\\beta\\) If 3 of us test the same thing, the probability that we’ll all see null is \\(\\beta^3\\) and the probability that at least one of us will see significance goes from \\((1-\\beta)\\) to \\((1-\\beta^3)\\) As \\(n \\uparrow\\), someone will definitely see significance (good!) 7.4.2 p-values In 1929 RA Fisher said the following (and thus 0.05 was born): …An observation is judged significant, if it would rarely have been produced, in the absence of a real cause of the kind we are seeking. It is a common practice to judge a result significant, if it is of such a magnitude that it would have been produced by chance not more frequently than once in twenty trials. This is an arbitrary, but convenient, level of significance for the practical investigator, but it does not mean that he allows himself to be deceived once in every twenty experiments. The test of significance only tells him what to ignore, namely all experiments in which significant results are not obtained. He should only claim that a phenomenon is experimentally demonstrable when he knows how to design an experiment so that it will rarely fail to give a significant result. Consequently, isolated significant results which he does not know how to reproduce are left in suspense pending further investigation. Note the Fisher is telling us that studies with p-values above 0.05 are not worth pursuing. He is not saying the studies with p-values less than 0.05 establish any sort of truth. In 2014 George Cobb (Amherst College) posed the following questions: Q: Why do so many colleges and grad schools teach p = .05? A: Because that’s still what the scientific community and journal editors use. Q: Why do so many people still use p = 0.05? A: Because that’s what they were taught in college or grad school. In 2015, Basic and Applied Social Psychology banned all NHSTP (null hypothesis significance testing procedures) from scientific articles. What are the implications for authors? http://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991 Question 3. Are any inferential statistical procedures required? Answer to Question 3. No, because the state of the art remains uncertain. However, BASP will require strong descriptive statistics, including effect sizes. We also encourage the presentation of frequency or distributional data when this is feasible. Finally, we encourage the use of larger sample sizes than is typical in much psychology research, because as the sample size increases, descriptive statistics become increasingly stable and sampling error is less of a problem. However, we will stop short of requiring particular sample sizes, because it is possible to imagine circumstances where more typical sample sizes might be justifiable. Many people think CIs are far superior to p-values. Not only can you assess significance, but you can also assess effect size. Here is a video that makes clear the comparison between a p-value and a confidence interval: https://www.youtube.com/watch?v=5OL1RqHrZQ8 In 2016, the American Statistical Association came out with a statement on p-values http://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1154108. The 6 main tenants of the statement are: P-values can indicate how incompatible the data are with a specified statistical model. P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone. Scientific conclusions and business or policy decisions should not be based only on whether a p- value passes a specific threshold. Proper inference requires full reporting and transparency. A p-value, or statistical significance, does not measure the size of an effect or the importance of a result. By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis. “Statisticians issue warning over misuse of P values” (Nature, March 7, 2016) http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503 Many people think CIs are far superior to p-values. Not only can you assess significance, but you can also assess effect size. Here is a video that makes clear the comparison between a p-value and a confidence interval, it also addresses how p-values can be misinterpreted. The Dance of the P-values (think about power when you watch it): https://www.youtube.com/watch?v=5OL1RqHrZQ8 Last, you can discover your own level of significance through this activity: https://www.openintro.org/stat/why05.php?stat_book=os 7.5 Human Subjects Research There are many ways to learn about federal regulations on studying human subjects. Any study that goes through an academic institution must be approved by the institution’s Institutional Review Board (IRB); each of the Claremont Colleges has an IRB. Some of you may be familiar with HIPPA (Health Insurance Portability and Accountability Act of 1996) which is United States legislation that provides data privacy and security provisions for safeguarding medical information. HIPPA is also the legislation that keeps your academic records private. Training in IRB policies can be found by Citi Program here: https://about.citiprogram.org/en/series/human-subjects-research-hsr/ Consider the following article: Cami Gearhart, “IRB Review of the Use of Social Media in Research”, Monitor, 2012. https://www.quorumreview.com/wp-content/uploads/2012/12/IRB-Review-of-the-Use-of-Social-Media-in-Research_Gearhart_Quorum-Review_Monitor_2012_12_01.pdf The use of interactive social media to collect information during study recruitment raises additional issues under the HIPAA Privacy Rule. The Privacy Rule prohibits the collection of an individual?s personal health information (or PHI) by a covered entity without prior written authorization from that individual.18 As PHI includes an individual?s contact information, including name, age, e-mail address, and mailing address, the Privacy Rule prohibits the collection of contact information via a website without prior authorization. This rule creates a conundrum when using social media, as it may be impractical to obtain a written authorization prior to collecting contact information during the recruitment process. To get around this restriction, a researcher generally must obtain a partial waiver of the HIPAA authorization requirement. (A waiver in this situation is considered “partial” because it is needed only for the recruitment phase of the clinical study.) A researcher can apply to either an IRB or a privacy board for such a waiver; the researcher will be asked to explain why it is impractical to obtain written authorizations, the plan for collecting information, and the planned safeguards for the data. OKCupid Consider a study done on a dataset of nearly 70,000 users of the online dating site OkCupid (including usernames, age, gender, location, relationship interests, personality traits, and many other profile variables). The authors did not violate any technical policies such as breaking passwords. However, their work indicates a violation of privacy ethics as indicated by HIPPA and use of IRBs. [Kirkegaard and Bjerrekaer, “The OKCupid dataset: A very large public dataset of dating site users,” Open Differential Pyschology, 2016.] https://openpsych.net/paper/46 Figure 1.4: Not only is it worth discussing the ethics of how the data were collected, but it also seems like maybe the study did some p-hacking. 7.6 Authorship From the International Committee of Medical Journal Editors, http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html. Many other organizations have suggested guidelines for authorship, but such guidelines generally follow the same criteria. Authors Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND Drafting the work or revising it critically for important intellectual content; AND Final approval of the version to be published; AND Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. Non-authors Contributors who meet fewer than all 4 of the above criteria for authorship should not be listed as authors, but they should be acknowledged. Examples of activities that alone (without other contributions) do not qualify a contributor for authorship are acquisition of funding; general supervision of a research group or general administrative support; and writing assistance, technical editing, language editing, and proofreading. Figure 1.5: The paper was retracted because the authors could not agree on the order of authorship. 7.7 Algorithms We could spend days talking about bias in algorithms. The take away from the examples below is that the data that is used to train the model can have huge effects on the creation of the model. A fantastic book on the issue is Weapons of Math Destruction by Cathy O’Neil. A podcast about the book is at: https://99percentinvisible.org/episode/the-age-of-the-algorithm/. Also keep in mind the various laws which are designed to protect privacy and civil liberties. Just because you didn’t try to build an algorithm that is biased against a protected group does not mean that you are off the hook. There are two ways that laws are enforced (both equally important): disparate treatment \\(\\rightarrow\\) means that the differential treatment is intentional disparate impact \\(\\rightarrow\\) means that the differential treatment is unintentional or implicit (some examples include advancing mortgage credit, employment selection, predictive policing) Alexandria Ocasio-Cortez, Jan 22, 2019 MLK event with Ta-Nehisi Coates http://aaronsadventures.blogspot.com/2019/01/discussion-of-unfairness-in-machine.html S. Barocas and A. Selbst, “Big Data’s Disparate Impact”, California Law Review, 671, 2016. Anti-discrimination Laws Civil Rights Acts of 1964 and 1991 Americans with Disabilities Act Genetic Information Nondiscrimination Act Equal Credit Opportunity Act Fair Housing Act Sentencing “Machine Bias” in Pro Publica by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, May 23, 2016 https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing/ Figure 1.6: Dylan Fugett had three subsequent arrests for drug possession. Bernard Parker had no subsequent offenses. DYLAN FUGETT BERNARD PARKER Prior Offense Prior Offense 1 attempted burglary 1 resisting arrest without violence LOW RISK3 HIGH RISK10 Subsequent Offenses Subsequent Offenses 3 drug possessions None Figure 7.1: False positive and false negative rates broken down by race. Algorithmic Justice League https://www.ajlunited.org/ The Algorithmic Justice League is a collective that aims to: Highlight algorithmic bias through media, art, and science Provide space for people to voice concerns and experiences with coded bias Develop practices for accountability during design, development, and deployment of coded systems Joy Buolamwini – AI, Ain’t I A Woman? https://www.youtube.com/embed/QxuyfWoVV98 Sentiment Analysis https://blog.dominodatalab.com/video-how-machine-learning-amplifies-societal-privilege/ In this talk, Mike Williams, Research Engineer at Fast Forward Labs, looks at how supervised machine learning has the potential to amplify power and privilege in society. Using sentiment analysis, he demonstrates how text analytics often favors the voices of men. Mike discusses how bias can inadvertently be introduced into any model, and how to recognize and mitigate these harms. There isn’t an option which is objective and fair. That option doesn’t exist… The whole premise is based on bias in your training set. If there are no biases, there are no patterns in your training set. Then the system is not going to work… Supervised machine learning, when it goes really well, when you do a really good job, reproduces the biases in the training data. Williams’ full (biased!) sentiment analysis on GitHub: https://github.com/williamsmj/sentiment/blob/master/sentiment.ipynb R packages wru Who are You? Bayesian Prediction of Racial Category Using Surname and Geolocation https://cran.r-project.org/web/packages/wru/index.html tm A framework for text mining applications within R. https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf RSentiment Analyses sentiment of a sentence in English and assigns score to it. It can classify sentences to the following categories of sentiments:- Positive, Negative, very Positive, very negative, Neutral. For a vector of sentences, it counts the number of sentences in each category of sentiment.In calculating the score, negation and various degrees of adjectives are taken into consideration. It deals only with English sentences. https://cran.r-project.org/web/packages/RSentiment/index.html SentimentAnalysis Performs a sentiment analysis of textual contents in R. This implementation utilizes various existing dictionaries, such as Harvard IV, or finance-specific dictionaries. Furthermore, it can also create customized dictionaries. The latter uses LASSO regularization as a statistical approach to select relevant terms based on an exogenous response variable. https://cran.r-project.org/web/packages/SentimentAnalysis/index.html 7.8 Guiding Ethical Principles From Baumer, Kaplan, and Horton (2017): 1. Do your work well by your own standards and by the standards of your profession (avoid using your skills in a way that is effectively lying - leading others to believe one thing when in fact something different is true). Recognize the parties to whom you have a special professional obligation (you should worry about conflict of interest!). Report results and methods honestly and respect your responsibility to identify and report flaws and shortcomings in your work (don’t be over-confident, do use reproducible methods). ASA Ethical Guidelines for Statistical Practice http://www.amstat.org/ASA/Your-Career/Ethical-Guidelines-for-Statistical-Practice.aspx Professional Integrity and Accountability The ethical statistician uses methodology and data that are relevant and appropriate, without favoritism or prejudice, and in a manner intended to produce valid, interpretable, and reproducible results. The ethical statistician does not knowingly accept work for which they are not sufficiently qualified, is honest with the client about any limitation of expertise, and consults other statisticians when necessary or in doubt. Integrity of Data and Methods The ethical statistician is candid about any known or suspected limitations, defects, or biases in the data that may impact the integrity or reliability of the statistical analysis. Objective and valid interpretation of the results requires that the underlying analysis recognizes and acknowledges the degree of reliability and integrity of the data. Responsibilities to Science/Public/Funder/Client The ethical statistician supports valid inferences, transparency, and good science in general, keeping the interests of the public, funder, client, or customer in mind (as well as professional colleagues, patients, the public, and the scientific community). Responsibilities to Research Subjects The ethical statistician protects and respects the rights and interests of human and animal subjects at all stages of their involvement in a project. This includes respondents to the census or to surveys, those whose data are contained in administrative records, and subjects of physically or psychologically invasive research. Responsibilities to Research Team Colleagues Science and statistical practice are often conducted in teams made up of professionals with different professional standards. The statistician must know how to work ethically in this environment. Responsibilities to Other Statisticians or Statistics Practitioners The practice of statistics requires consideration of the entire range of possible explanations for observed phenomena, and distinct observers drawing on their own unique sets of experiences can arrive at different and potentially diverging judgments about the plausibility of different explanations. Even in adversarial settings, discourse tends to be most successful when statisticians treat one another with mutual respect and focus on scientific principles, methodology and the substance of data interpretations. Responsibilities Regarding Allegations of Misconduct The ethical statistician understands the difference between questionable scientific practices and practices that constitute misconduct, avoids both, but knows how each should be handled. Responsibilities of Employers, Including Organizations, Individuals, Attorneys, or Other Clients Employing Statistical Practitioners Those employing any person to analyze data are implicitly relying on the profession’s reputation for objectivity. However, this creates an obligation on the part of the employer to understand and respect statisticians’ obligation of objectivity. References "],
["class.html", "Chapter 8 Classification 8.1 10/29/19 Agenda 8.2 Cross Validation 8.3 \\(k\\)-Nearest Neighbors 8.4 10/31/19 Agenda 8.5 CART 8.6 11/5/19 Agenda 8.7 Bagging 8.8 11/7/19 Agenda 8.9 Random Forests 8.10 Model Choices 8.11 11/12/19 Agenda 8.12 Support Vector Machines", " Chapter 8 Classification 8.1 10/29/19 Agenda classification \\(k\\)-Nearest Neighbors bias-variance trade-off cross validation Important Note: For the majority of the classification and clustering methods, we will use the caret package in R. For more information see: http://topepo.github.io/caret/index.html Also, check out the caret cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/caret.pdf Baumer (2015) provides a concise explanation of how both statistics and data science work to enhance ideas of machine learning, one aspect of which is classification: In order to understand machine learning, one must recognize the differences between the mindset of the data miner and the statistician, notably characterized by Breiman (2001), who distinguished two types of models f for y, the response variable, and x, a vector of explanatory variables. One might consider a data model f such that y \\(\\sim\\) f(x), assess whether f could reasonably have been the process that generated y from x, and then make inferences about f. The goal here is to learn about the real process that generated y from x, and the conceit is Alternatively, one might construct an algorithmic model f, such that \\(y \\sim f(x)\\), and use f to predict unobserved values of y. If it can be determined that f does in fact do a good job of predicting values of y, one might not care to learn much about f. In the former case, since we want to learn about f, a simpler model may be preferred. Conversely, in the latter case, since we want to predict new values of y, we may be indifferent to model complexity (other than concerns about overfitting and scalability). Classification is a supervised learning technique to extract general patterns from the data in order to build a predictor for a new test or validation data set. That is, the model should classify new points into groups (or with a numerical response values) based on a model built from a set of data which provides known group membership for each value. For most of the methods below, we will consider classifying into categories (in fact, usually only two categories), but sometimes (e.g., support vector machines and linear regression) the goal is to predict a numeric variable. Some examples of classification techniques include: linear regression, logistic regression, neural networks, classification trees, Random Forests, k-nearest neighbors, support vector machines, näive Bayes, and linear discriminant analysis. We will cover the methods in bold. Simple is Better (From Fielding (2007), p. 87) We want to avoid over-fitting the model (certainly, it is a bad idea to model the noise!) Future prediction performance goes down with too many predictors. Simple models provide better insight into causality and specific associations. Fewer predictors implies fewer variables to collect in later studies. That said, the model should still represent the complexity of the data! We describe the trade-off above as the “bias-variance” trade-off. In order to fully understand that trade-off, let’s first cover the classification method known as \\(k\\)-Nearest Neighbors. 8.2 Cross Validation 8.2.1 Bias-variance trade-off Excellent resource for explaining the bias-variance trade-off: http://scott.fortmann-roe.com/docs/BiasVariance.html Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training set. Generally, the closer the model fits the data, the more variable it will be (it’ll be different for each data set!). A model with many many explanatory variables will often fit the data too closely. Bias refers to the error that is introduced by approximating the “truth” by a model which is too simple. For example, we often use linear models to describe complex relationships, but it is unlikely that any real life situation actually has a true linear model. However, if the true relationship is close to linear, then the linear model will have a low bias. Generally, the simpler the model, the lower the variance. The more complicated the model, the lower the bias. In this class, cross validation will be used to assess model fit. [If time permits, Receiver Operating Characteristic (ROC) curves will also be covered.] \\[\\begin{align} \\mbox{prediction error } = \\mbox{ irreducible error } + \\mbox{ bias } + \\mbox{ variance} \\end{align}\\] irreducible error The irreducible error is the natural variability that comes with observations. No matter how good the model is, we will never be able to predict perfectly. bias The bias of the model represents the difference between the true model and a model which is too simple. That is, the more complicated the model (e.g., smaller \\(k\\) in \\(k\\)NN), the closer the points are to the prediction. As the model gets more complicated (e.g., as \\(k\\) decreases), the bias goes down. variance The variance represents the variability of the model from sample to sample. That is, a simple model (big \\(k\\) in \\(k\\)NN) would not change a lot from sample to sample. The variance decreases as the model becomes more simple (e.g., as \\(k\\) increases). Note the bias-variance trade-off. We want our prediction error to be small, so we choose a model that is medium with respect to both bias and variance. We cannot control the irreducible error. Figure 1.2: Test and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! (Hastie, Tibshirani, and Friedman 2001) The following visualization does an excellent job of communicating the trade-off between bias and variance as a function of a specific tuning parameter, here: minimum node size of a classification tree. http://www.r2d3.us/visual-intro-to-machine-learning-part-2/ 8.2.2 Implementing Cross Validation Figure 1.3: (Flach 2012) Cross validation is typically used in two ways. To assess a model’s accuracy (model assessment). To build a model (model selection). Different ways to CV Suppose that we build a classifier on a given data set. We’d like to know how well the model classifies observations, but if we test on the samples at hand, the error rate will be much lower than the model’s inherent accuracy rate. Instead, we’d like to predict new observations that were not used to create the model. There are various ways of creating test or validation sets of data: one training set, one test set [two drawbacks: estimate of error is highly variable because it depends on which points go into the training set; and because the training data set is smaller than the full data set, the error rate is biased in such a way that it overestimates the actual error rate of the modeling technique.] leave one out cross validation (LOOCV) remove one observation build the model using the remaining n-1 points predict class membership for the observation which was removed repeat by removing each observation one at a time \\(k\\)-fold cross validation (\\(k\\)-fold CV) like LOOCV except that the algorithm is run \\(k\\) times on each group (of approximately equal size) from a partition of the data set.] LOOCV is a special case of \\(k\\)-fold CV with \\(k=n\\) advantage of \\(k\\)-fold is computational \\(k\\)-fold often has a better bias-variance trade-off [bias is lower with LOOCV. however, because LOOCV predicts \\(n\\) observations from \\(n\\) models which are basically the same, the variability will be higher (i.e., based on the \\(n\\) data values). with \\(k\\)-fold, prediction is on \\(n\\) values from \\(k\\) models which are much less correlated. the effect is to average out the predicted values in such a way that there will be less variability from data set to data set.] CV for Model assessment 10-fold assume \\(k\\) is given for \\(k\\)-NN remove 10% of the data build the model using the remaining 90% predict class membership / continuous response for the 10% of the observations which were removed repeat by removing each decile one at a time a good measure of the model’s ability to predict is the error rate associated with the predictions on the data which have been independently predicted CV for Model selection 10-fold set \\(k\\) in \\(k\\)-NN build the model using the \\(k\\) value set above: remove 10% of the data build the model using the remaining 90% predict class membership / continuous response for the 10% of the observations which were removed repeat by removing each decile one at a time measure the CV prediction error for the \\(k\\) value at hand repeat steps 1-3 and choose the \\(k\\) for which the prediction error is lowest CV for Model assessment and selection 10-fold To do both, one approach is to use test/training data and CV in order to both model assessment and selection. Note that CV could be used in both steps, but the algorithm is slightly more complicated. split the data into training and test observations set \\(k\\) in \\(k\\)-NN build the model using the \\(k\\) value set above on only the training data: remove 10% of the training data build the model using the remaining 90% of the training data predict class membership / continuous response for the 10% of the training observations which were removed repeat by removing each decile one at a time from the training data measure the CV prediction error for the \\(k\\) value at hand on the training data repeat steps 2-4 and choose the \\(k\\) for which the prediction error is lowest for the training data using the \\(k\\) value given in step 5, assess the prediction error on the test data Figure 1.4: Nested cross-validation: two cross-validation loops are run one inside the other. (Varoquaux et al. 2017) 8.3 \\(k\\)-Nearest Neighbors The \\(k\\)-Nearest Neighbor algorithm does exactly what it sounds like it does. The user decides on the integer value for \\(k\\), and a point is classified to be in the group for which the majority of the \\(k\\) closest points in the training data. 8.3.1 \\(k\\)-NN algorithm Decide on a distance metric (e.g., Euclidean distance, 1 - correlation, etc.) and find the distances from each point in the test set to each point in the training set. The distance is measured in the feature space, that is, with respect to the explanatory variables (not the response variable). n.b. In most machine learning algorithms that use “distance” as a measure, the “distance” is not required to be a mathematical distance metric. Indeed, 1-correlation is a very common distance measure, and it fails the triangle inequality. Consider a point in the test set. Find the \\(k\\) closest points in the training set to the one test observation. Using majority vote, find the dominate class of the \\(k\\) closest points. Predict that class label to the test observation. Note: if the response variable is continuous (instead of categorical), find the average response variable of the \\(k\\) training point to be the predicted response for the one test observation. Shortcomings of \\(k\\)-NN: one class can dominate if it has a large majority Euclidean distance is dominated by scale it can be computationally unwieldy (and unneeded!!) to calculate all distances (there are algorithms to search smartly) the output doesn’t provide any information about which explanatory variables are informative. Strengths of \\(k\\)-NN: it can easily work for any number of categories it can predict a quantitative response variable the bias of 1-NN is often low (but the variance is high) any distance metric can be used (so the algorithm models the data appropriately) the method is simple to implement / understand model is nonparametric (no distributional assumptions on the data) great model for imputing missing data 8.3.2 R knn Example R code for using the caret package to cluster the iris data. The caret package vignette for knn is here: http://topepo.github.io/caret/miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function library(GGally) # for plotting library(caret) # for partitioning &amp; classification data(iris) iris Data ggpairs(iris, color=&quot;Species&quot;, alpha=.4) kNN Without thinking about test / training data, a naive model is: fitControl &lt;-caret::trainControl(method=&quot;none&quot;, classProbs = TRUE) tr.iris &lt;- caret::train(Species ~ ., data=iris, method=&quot;knn&quot;, trControl = fitControl, tuneGrid= data.frame(k=3)) caret::confusionMatrix(data=predict(tr.iris, newdata = iris), reference = iris$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 3 ## virginica 0 3 47 ## ## Overall Statistics ## ## Accuracy : 0.96 ## 95% CI : (0.915, 0.9852) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.94 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9400 0.9400 ## Specificity 1.0000 0.9700 0.9700 ## Pos Pred Value 1.0000 0.9400 0.9400 ## Neg Pred Value 1.0000 0.9700 0.9700 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3133 0.3133 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9550 0.9550 Why naive? Not good to train and test on the same data set! Assumed the knowledge of \\(k\\) groups. Was Euclidean distance the right thing to use? [The knn package in R only uses Euclidean distance.] Using test/training data sets. One of the common pieces to use in the caret package is creating test and training datasets for cross validation. set.seed(4747) inTrain &lt;- caret::createDataPartition(y = iris$Species, p=0.7, list=FALSE) iris.train &lt;- iris[inTrain,] iris.test &lt;- iris[-c(inTrain),] fitControl &lt;- caret::trainControl(method=&quot;none&quot;) tr.iris &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;knn&quot;, trControl = fitControl, tuneGrid= data.frame(k=5)) caret::confusionMatrix(data=predict(tr.iris, newdata = iris.test), reference = iris.test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 14 1 ## virginica 0 1 14 ## ## Overall Statistics ## ## Accuracy : 0.9556 ## 95% CI : (0.8485, 0.9946) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9333 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9333 0.9333 ## Specificity 1.0000 0.9667 0.9667 ## Pos Pred Value 1.0000 0.9333 0.9333 ## Neg Pred Value 1.0000 0.9667 0.9667 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3111 0.3111 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9500 0.9500 \\(k\\) neighbors? CV on TRAINING to find \\(k\\) set.seed(47) fitControl &lt;- caret::trainControl(method=&quot;cv&quot;, number=10) tr.iris &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;knn&quot;, trControl = fitControl, tuneGrid= data.frame(k=c(1,3,5,7,9,11))) tr.iris ## k-Nearest Neighbors ## ## 105 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 95, 93, 95, 95, 94, 95, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.9516667 0.9275521 ## 3 0.9316667 0.8974822 ## 5 0.9205556 0.8805824 ## 7 0.9500000 0.9249006 ## 9 0.9616667 0.9427036 ## 11 0.9716667 0.9580882 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 11. Then measure accuracy by testing on test data! caret::confusionMatrix(data=predict(tr.iris, newdata = iris.test), reference = iris.test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 14 1 ## virginica 0 1 14 ## ## Overall Statistics ## ## Accuracy : 0.9556 ## 95% CI : (0.8485, 0.9946) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9333 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9333 0.9333 ## Specificity 1.0000 0.9667 0.9667 ## Pos Pred Value 1.0000 0.9333 0.9333 ## Neg Pred Value 1.0000 0.9667 0.9667 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3111 0.3111 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 1.0000 0.9500 0.9500 8.4 10/31/19 Agenda trees (CART) building trees (binary recursive splitting) homogeneity measures pruning trees 8.5 CART See the following (amazing!) demonstration for tree intuition: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/ Figure 1.8: http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg Best information was whether or not the county was more than 20 percent black. Then each successive node is split again on the best possible informative variable. Note that the leaves on the tree are reasonably homogenous. NYT, April 16, 2008. 8.5.1 CART algorithm Basic Classification and Regression Trees (CART) Algorithm: Start with all observations in one group. Find the variable/split that best separates the response variable (successive binary partitions based on the different predictors / explanatory variables). Evaluation “homogeneity” within each group Divide the data into two groups (“leaves”) on that split (“node”). Within each split, find the best variable/split that separates the outcomes. Continue until the groups are too small or sufficiently “pure”. Prune tree. Shortcomings of CART: Straight CART do not generally have the same predictive accuracy as other classification approaches. (we will improve the model - see Random Forests, boosting, bagging) Difficult to write down / consider the CART “model” Without proper pruning, the model can easily lead to overfitting With lots of predictors, (even greedy) partitioning can become computationally unwieldy Often, prediction performance is poor Strengths of CART: They are easy to explain; trees are easy to display graphically (which make them easy to interpret). (They mirror the typical human decision-making process.) Can handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!). Can handle more than 2 groups for categorical predictions Easily ignore redundant variables. Perform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables. Data transformations may be less important (monotone transformations on the explanatory variables won’t change anything). 8.5.1.1 Classification Trees A classification tree is used to predict a categorical response variable (rather than a quantitative one). The end predicted value will be the one of the most commonly occurring class of training observations in the region to which it belongs. The goal is to create regions which are as homogeneous as possible with respect to the response variable - categories. measures of impurity Calculate the classification error rate as the fraction of the training observations in that region that do not belong to the most common class: \\[E_m = 1 - \\max_k(\\hat{p}_{mk})\\] where \\(\\hat{p}_{mk}\\) represents the proportion of training observations in the \\(m\\)th region that are from the \\(k\\)th class. However, the classification error rate is not particularly sensitive to node purity, and so two additional measures are typically used to partition the regions. Further, the Gini index is defined by \\[G_m= \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})\\] a measure of total variance across the \\(K\\) classes. [Recall, the variance of a Bernoulli random variable with \\(\\pi\\) = P(success) is \\(\\pi(1-\\pi)\\).] Note that the Gini index takes on a small value if all of the \\(\\hat{p}_{mk}\\) values are close to zero or one. For this reason, the Gini index is referred to as a measure of node purity - a small value indicates that a node contains predominantly observations from a single class. Last, the cross-entropy is defined as \\[D_m = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}\\] Since \\(0 \\leq \\hat{p}_{mk} \\leq 1\\) it follows that \\(0 \\leq -\\hat{p}_{mk} \\log\\hat{p}_{mk}\\). One can show that the cross-entropy will take on a value near zero if the \\(\\hat{p}_{mk}\\) values are all near zero or all near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the \\(m\\)th node is pure. To build the tree, typically the Gini index or the cross-entropy are used to evaluate a particular split. To prune the tree, often classification error is used (if accuracy of the final pruned tree is the goal) Computationally, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a top down approach to the problem which is known as recursive binary splitting (greedy because we look only at the current split and not at the outcomes of the splits to come). Recursive Binary Splitting on Categories (for a given node) Select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j&lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) lead to the greatest reduction in Gini index or cross-entropy. For any \\(j\\) and \\(s\\), define the pair of half-planes to be \\[R_1(j,s) = \\{X | X_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{X | X_j \\geq s\\}\\] and we seek the value of \\(j\\) and \\(s\\) that minimize the equation: \\[\\begin{align} &amp; \\sum_{i:x_i \\in R_1(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + \\sum_{i:x_i \\in R_2(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\ \\mbox{equivalently: } &amp; n_{R_1} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + n_{R_2} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\ \\end{align}\\] Repeat the process, looking for the best predictor and best cutpoint within one of the previously identified regions (producing three regions, now). Keep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations. 8.5.1.2 Regression Trees The goal of the algorithm in a regression tree is to split the set of possible value for the data into \\(|T|\\) distinct and non-overlapping regions, \\(R_1, R_2, \\ldots, R_{|T|}\\). For every observation that falls into the region \\(R_m\\), we make the same prediction - the mean of the response values for the training observations in \\(R_m\\). So how do we find the regions \\(R_1, \\ldots, R_{|T|}\\)? \\(\\Rightarrow\\) Minimize RSS, \\[RSS = \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2\\] where \\(\\overline{y}_{R_m}\\) is the mean response for the training observations within the \\(m\\)th region. (Note: in the chapter (James et al. 2013) they refer to MSE - mean squared error - in addition to RSS where MSE is simply RSS / n, see equation (2.5).) Again, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a top down approach to the problem which is known as recursive binary splitting (greedy because we look only at the current split and not at the outcomes of the splits to come). Recursive Binary Splitting on Numerical Response (for a given node) Select the predictor \\(X_j\\) and the cutpoint \\(s\\) such that splitting the predictor space into the regions \\(\\{X | X_j&lt; s\\}\\) and \\(\\{X | X_j \\geq s\\}\\) lead to the greatest reduction in RSS. For any \\(j\\) and \\(s\\), define the pair of half-planes to be \\[R_1(j,s) = \\{X | X_j &lt; s\\} \\mbox{ and } R_2(j,s) = \\{X | X_j \\geq s\\}\\] and we see the value of \\(j\\) and \\(s\\) that minimize the equation: \\[\\sum_{i:x_i \\in R_1(j,s)} (y_i - \\overline{y}_{R_1})^2 + \\sum_{i:x_i \\in R_2(j,s)} (y_i - \\overline{y}_{R_2})^2\\] where \\(\\overline{y}_{R_1}\\) is the mean response for the training observations in \\(R_1(j,s)\\) and \\(\\overline{y}_{R_2}\\) is the mean response for training observations in \\(R_2(j,s)\\). Repeat the process, looking for the best predictor and best cutpoint within one of the previously identified regions (producing three regions, now). Keep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations. 8.5.1.3 (Avoiding) Overfitting Ideally, the tree would not overfit the training data. One could imagine how easy it would be to grow the tree over the training data so as to end up with terminal nodes which are completely homogeneous (but then don’t represent the test data). See the following (amazing!) demonstration for intuition on model validation / overfitting: http://www.r2d3.us/visual-intro-to-machine-learning-part-2/ One possible algorithm for building a tree is to split based on the reduction in RSS (or Gini index, etc.) exceeding some (presumably high) threshold. However, the strategy is known to be short sighted, as a split later down the tree may contain a large amount of information. A better strategy is to grow a very large tree \\(T_0\\) and then prune it back in order to obtain a subtree. We use cross validation to build the subtree so as to not overfit the data. Algorithm: Building a Regression Tree Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of \\(\\alpha\\). Use \\(K\\)-fold cross-validation to choose \\(\\alpha\\). That is, divide the training observations into \\(K\\) folds. For each \\(k=1, 2, \\ldots, K\\): Repeat Steps 1 and 2 on all but the \\(k\\)th fold of the training data. Evaluate the mean squared prediction error on the data in the left-out \\(k\\)th fold, as a function of \\(\\alpha\\). For each value of \\(\\alpha\\), average the prediction error (either misclassification or RSS), and pick \\(\\alpha\\) to minimize the average error. Return the subtree from Step 2 that corresponds to the chosen value of \\(\\alpha\\). 8.5.1.4 Cost Complexity Pruning Also known as weakest link pruning, the idea is to consider a sequence of trees indexed by a nonnegative tuning parameter \\(\\alpha\\) (instead of considering every single subtree). Generally, the idea is that there is a cost to having a larger (more complex!) tree. We define the cost complexity criterion (\\(\\alpha &gt; 0\\)): \\[\\begin{align} \\mbox{numerical: } C_\\alpha(T) &amp;= \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha|T|\\\\ \\mbox{categorical: } C_\\alpha(T) &amp;= \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} I(y_i \\ne k(m)) + \\alpha|T| \\end{align}\\] where \\(k(m)\\) is the class with the majority of observations in node \\(m\\) and \\(|T|\\) is the number of terminal nodes in the tree. \\(\\alpha\\) small: If \\(\\alpha\\) is set to be small, we are saying that the risk is more worrisome than the complexity and larger trees are favored because they reduce the risk. \\(\\alpha\\) large: If \\(\\alpha\\) is set to be large, then the complexity of the tree is more worrisome and smaller trees are favored. The way to think about cost complexity is to consider \\(\\alpha\\) increasing. As \\(\\alpha\\) gets bigger, the “best” tree will be smaller. But the test error will not be monotonically related to the size of the training tree. Variations on a theme The main ideas above are consistent throughout all CART algorithms. However, the exact details of implementation can change from function to function, and often times it is very difficult to decipher exactly which equation is being used. In the tree function in R, much of the decision making is done on deviance which is defined as: \\[\\begin{align} \\mbox{numerical: } \\mbox{deviance} &amp;= \\sum_{m=1}^{|T|} \\sum_{i \\in R_m} (y_i - \\overline{y}_{R_m})^2\\\\ \\mbox{categorical: } \\mbox{deviance} &amp;= -2\\sum_{m=1}^{|T|} \\sum_{k=1}^K n_{mk} \\log \\hat{p}_{mk}\\\\ \\end{align}\\] For the CART algorithm, minimize the deviance (for both types of variables). The categorical deviance will be small if most of the observations are in the majority group (with high proportion). Also, \\(\\lim_{\\epsilon \\rightarrow 0} \\epsilon \\log(\\epsilon) = 0\\). Additionally, methods of cross validation can also vary. In particular, if the number of variables is large, the tree algorithm can be slow and so the cross validation process - choice of \\(\\alpha\\) - needs to be efficient. CV for model building and model assessment Notice that CV is used for both model building and model assessment. It is possible (and practical, though quite computational!) to use both practices on the same classification model. The algorithm could be as follows. Algorithm: CV for both \\(k_1\\)-fold CV building and \\(k_2\\)-fold CV assessment Partition the data in \\(k_1\\) groups. Remove the first group, and train the data on the remaining \\(k_1-1\\) groups. Use \\(k_2\\)-fold cross-validation (on the \\(k_1-1\\) groups) to choose \\(\\alpha\\). That is, divide the training observations into \\(k_2\\) folds and find \\(\\alpha\\) that minimizes the error. Using the subtree that corresponds to the chosen value of \\(\\alpha\\), predict the first of the \\(k_1\\) hold out samples. Repeat steps 2-4 using the remaining \\(k_1 - 1\\) groups. 8.5.2 R CART Example There are multiple tree building options in R both in the caret package and party, rpart, and tree packages. The Census Bureau divides the country up into “tracts” of approximately equal population. For the 1990 Census, California was divided into 20640 tracts. One data sets (houses on http://lib.stat.cmu.edu/datasets/; http://lib.stat.cmu.edu/datasets/houses.zip) records the following for each tract in California: Median house price, median house age, total number of rooms, total number of bedrooms, total number of occupants, total number of houses, median income (in thousands of dollars), latitude and longitude. It appeared in Pace and Barry (1997), “Sparse Spatial Autoregressions”, Statistics and Probability Letters. Classification and Regression Trees Classification Trees are used to predict a response or class \\(Y\\) from input \\(X_1, X_2, \\ldots, X_n\\). If it is a continuous response it’s called a regression tree, if it is categorical, it’s called a classification tree. At each node of the tree, we check the value of one the input \\(X_i\\) and depending of the (binary) answer we continue to the left or to the right subbranch. When we reach a leaf we will find the prediction (usually it is a simple statistic of the dataset the leaf represents, like the most common value from the available classes). Note on maxdepth: as you might expect, maxdepth indicates the longest length from the root of the tree to a terminal node. However, for rpart (in particular, using rpart or rpart2 in caret), there are other default settings that keep the tree from growing all the way to singular nodes, even with a high maxdepth. Regression Trees real.estate &lt;- read.table(&quot;http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt&quot;, header=TRUE) set.seed(4747) fitControl &lt;- caret::trainControl(method=&quot;none&quot;) tr.house &lt;- caret::train(log(MedianHouseValue) ~ Longitude + Latitude, data=real.estate, method=&quot;rpart2&quot;, trControl = fitControl, tuneGrid= data.frame(maxdepth=5)) rpart.plot::rpart.plot(tr.house$finalModel) Scatterplot Compare the predictions with the dataset (darker is more expensive) which seem to capture the global price trend. Note that this plot uses the tree model (instead of the rpart2 model) because the optimization is different. tree.model &lt;- tree::tree(log(MedianHouseValue) ~ Longitude + Latitude, data=real.estate) price.deciles &lt;- quantile(real.estate$MedianHouseValue, 0:10/10) cut.prices &lt;- cut(real.estate$MedianHouseValue, price.deciles, include.lowest=TRUE) plot(real.estate$Longitude, real.estate$Latitude, col=grey(10:2/11)[cut.prices], pch=20, xlab=&quot;Longitude&quot;,ylab=&quot;Latitude&quot;) tree::partition.tree(tree.model, ordvars=c(&quot;Longitude&quot;,&quot;Latitude&quot;), add=TRUE) Finer partition 12) Latitude&gt;=34.7 2844 645.0 11.5 the node that splits at latitude greater than 34.7 has 2844 houses. 645 is the “deviance” which is the sum of squares value for that node. the predicted value is the average of the points in that node: 11.5. it is not a terminal node (no asterisk). set.seed(4747) fitControl &lt;- caret::trainControl(method=&quot;none&quot;) tr.house &lt;- caret::train(log(MedianHouseValue) ~ Longitude + Latitude, data=real.estate, method=&quot;rpart2&quot;, trControl = fitControl, tuneGrid= data.frame(maxdepth=5)) tr.house$finalModel ## n= 20640 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 20640 6685.26300 12.08488 ## 2) Latitude&gt;=38.485 2061 383.26410 11.59422 ## 4) Latitude&gt;=39.355 674 65.51082 11.31630 * ## 5) Latitude&lt; 39.355 1387 240.39580 11.72928 * ## 3) Latitude&lt; 38.485 18579 5750.77400 12.13931 ## 6) Longitude&gt;=-121.655 13941 4395.52000 12.05527 ## 12) Latitude&gt;=34.675 2844 645.27310 11.51018 ## 24) Longitude&gt;=-120.275 1460 212.47730 11.28145 * ## 25) Longitude&lt; -120.275 1384 275.83120 11.75148 * ## 13) Latitude&lt; 34.675 11097 2688.68000 12.19497 ## 26) Longitude&gt;=-118.315 8384 1823.33000 12.08687 ## 52) Longitude&gt;=-117.545 2839 691.79800 11.87672 * ## 53) Longitude&lt; -117.545 5545 941.96340 12.19446 * ## 27) Longitude&lt; -118.315 2713 464.62720 12.52902 * ## 7) Longitude&lt; -121.655 4638 960.79250 12.39194 ## 14) Latitude&gt;=37.925 1063 177.59430 12.09533 * ## 15) Latitude&lt; 37.925 3575 661.87260 12.48013 * More variables Including all the variables, not only the latitude and longitude: set.seed(4747) fitControl &lt;- caret::trainControl(method=&quot;none&quot;) tr.full.house &lt;- caret::train(log(MedianHouseValue) ~ ., data=real.estate, method=&quot;rpart2&quot;, trControl = fitControl, tuneGrid= data.frame(maxdepth=5)) tr.full.house$finalModel ## n= 20640 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 20640 6685.26300 12.08488 ## 2) MedianIncome&lt; 3.5471 10381 2662.31300 11.77174 ## 4) MedianIncome&lt; 2.51025 4842 1193.71700 11.57572 ## 8) Latitude&gt;=34.465 2520 557.77450 11.38771 ## 16) Longitude&gt;=-120.275 728 77.14396 11.08365 * ## 17) Longitude&lt; -120.275 1792 385.97890 11.51124 ## 34) Latitude&gt;=37.905 1103 150.31490 11.35795 * ## 35) Latitude&lt; 37.905 689 168.25420 11.75664 * ## 9) Latitude&lt; 34.465 2322 450.19880 11.77976 ## 18) Longitude&gt;=-117.775 878 144.15330 11.52580 * ## 19) Longitude&lt; -117.775 1444 214.98520 11.93418 * ## 5) MedianIncome&gt;=2.51025 5539 1119.89800 11.94310 ## 10) Latitude&gt;=37.925 1104 123.65980 11.68124 * ## 11) Latitude&lt; 37.925 4435 901.69050 12.00829 ## 22) Longitude&gt;=-122.235 4084 770.65270 11.96811 ## 44) Latitude&gt;=34.455 1270 284.66500 11.76617 * ## 45) Latitude&lt; 34.455 2814 410.82510 12.05924 * ## 23) Longitude&lt; -122.235 351 47.73002 12.47579 * ## 3) MedianIncome&gt;=3.5471 10259 1974.99300 12.40175 ## 6) MedianIncome&lt; 5.5892 7265 1156.09500 12.25720 ## 12) MedianHouseAge&lt; 38.5 5907 858.59850 12.20694 * ## 13) MedianHouseAge&gt;=38.5 1358 217.69860 12.47578 * ## 7) MedianIncome&gt;=5.5892 2994 298.73550 12.75251 ## 14) MedianIncome&lt; 7.393 2008 176.41530 12.64297 * ## 15) MedianIncome&gt;=7.393 986 49.16749 12.97557 * rpart.plot::rpart.plot(tr.full.house$finalModel) Cross Validation (model building!) Turns out that the tree does “better” by being more complex – why is that? The tree with 14 nodes corresponds to the tree with the highest accuracy / lowest deviance. # here, let&#39;s use all the variables and all the samples set.seed(4747) fitControl &lt;- caret::trainControl(method=&quot;cv&quot;) tree.cv.house &lt;- caret::train(log(MedianHouseValue) ~ ., data=real.estate, method=&quot;rpart2&quot;, trControl=fitControl, tuneGrid=data.frame(maxdepth=1:20), parms=list(split=&quot;gini&quot;)) tree.cv.house ## CART ## ## 20640 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 18576, 18576, 18576, 18575, 18576, 18576, ... ## Resampling results across tuning parameters: ## ## maxdepth RMSE Rsquared MAE ## 1 0.4748682 0.3041572 0.3848606 ## 2 0.4478756 0.3809354 0.3563586 ## 3 0.4282733 0.4340116 0.3393296 ## 4 0.4178448 0.4611563 0.3296215 ## 5 0.4054431 0.4924175 0.3184901 ## 6 0.3962472 0.5155365 0.3103266 ## 7 0.3948428 0.5189584 0.3092563 ## 8 0.3935306 0.5221099 0.3080369 ## 9 0.3891254 0.5326804 0.3044392 ## 10 0.3836652 0.5456808 0.3000226 ## 11 0.3786873 0.5574177 0.2956848 ## 12 0.3739131 0.5685161 0.2907504 ## 13 0.3712711 0.5746216 0.2868830 ## 14 0.3703641 0.5767271 0.2858720 ## 15 0.3703641 0.5767271 0.2858720 ## 16 0.3703641 0.5767271 0.2858720 ## 17 0.3703641 0.5767271 0.2858720 ## 18 0.3703641 0.5767271 0.2858720 ## 19 0.3703641 0.5767271 0.2858720 ## 20 0.3703641 0.5767271 0.2858720 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was maxdepth = 14. rpart.plot::rpart.plot(tree.cv.house$finalModel) plot(tree.cv.house) Training / test data for model building AND model accuracy # first create two datasets: one training, one test inTrain &lt;- caret::createDataPartition(y = real.estate$MedianHouseValue, p=.8, list=FALSE) house.train &lt;- real.estate[inTrain,] house.test &lt;- real.estate[-c(inTrain),] # then use CV on the training data to find the best maxdepth set.seed(4747) fitControl &lt;- caret::trainControl(method=&quot;cv&quot;) tree.cvtrain.house &lt;- caret::train(log(MedianHouseValue) ~ ., data=house.train, method=&quot;rpart2&quot;, trControl=fitControl, tuneGrid=data.frame(maxdepth=1:20), parms=list(split=&quot;gini&quot;)) tree.cvtrain.house ## CART ## ## 16513 samples ## 8 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 14862, 14862, 14862, 14862, 14862, 14861, ... ## Resampling results across tuning parameters: ## ## maxdepth RMSE Rsquared MAE ## 1 0.4756049 0.2994314 0.3851775 ## 2 0.4497958 0.3734197 0.3581148 ## 3 0.4289837 0.4302083 0.3396992 ## 4 0.4192079 0.4558965 0.3304520 ## 5 0.4026435 0.4979229 0.3153937 ## 6 0.3960649 0.5141665 0.3102682 ## 7 0.3960649 0.5141665 0.3102682 ## 8 0.3924388 0.5229961 0.3072433 ## 9 0.3875832 0.5347306 0.3027262 ## 10 0.3830783 0.5454004 0.2981715 ## 11 0.3783297 0.5566766 0.2941443 ## 12 0.3724797 0.5702362 0.2883089 ## 13 0.3694837 0.5770987 0.2850918 ## 14 0.3694837 0.5770987 0.2850918 ## 15 0.3694837 0.5770987 0.2850918 ## 16 0.3694837 0.5770987 0.2850918 ## 17 0.3694837 0.5770987 0.2850918 ## 18 0.3694837 0.5770987 0.2850918 ## 19 0.3694837 0.5770987 0.2850918 ## 20 0.3694837 0.5770987 0.2850918 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was maxdepth = 13. tree.train.house &lt;- caret::train(log(MedianHouseValue) ~ ., data=house.train, method=&quot;rpart2&quot;, trControl=caret::trainControl(method=&quot;none&quot;), tuneGrid=data.frame(maxdepth=14), parms=list(split=&quot;gini&quot;)) for classification results use confusionMatrix instead of postResample test.pred &lt;- predict(tree.train.house, house.test) caret::postResample(pred = test.pred, obs=log(house.test$MedianHouseValue)) ## RMSE Rsquared MAE ## 0.3696649 0.5840583 0.2846395 Other tree R packages rpart is faster than tree party gives great plotting options maptree also gives trees from hierarchical clustering randomForest up next! Reference: slides built from http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf 8.6 11/5/19 Agenda pruning variable selection bagging (no boosting) OOB error rate 8.7 Bagging The tree based models given by CART are easy to understand and implement, but they suffer from high variance. That is, if we split the training data into two parts at random and fit a decision tree to both halves, the results that we get could be quite different (you might have seen this in your homework assignment!). We’d like a model that produces low variance - one for which if we ran it on different datasets, we’d get (close to) the same model every time. Bagging = Bootstrap Aggregating. The idea is that sometimes when you fit multiple models and aggregate those models together, you get a smoother model fit which will give you a better balance between bias in your fit and variance in your fit. Bagging can be applied to any classifier to reduce variability. Recall that the variance of the sample mean is variance / n. So we’ve seen the idea that averaging an outcome gives reduced variability. 8.7.1 Bagging algorithm Algorithm: Bagging Forest Resample cases (observational units, not variables) and recalculate predictions. Choose \\(N&#39; \\leq N\\) for the number of observations in the new training set through random sampling with replacement. Almost always we use \\(N&#39; = N\\) for a full bootstrap. Build a tree on each new set of \\(N&#39;\\) training observations. Average (regression) or majority vote (classification). Note that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen. \\[\\begin{align} P(\\mbox{observation $n$ is not in the bootstrap sample}) &amp;= \\bigg(1 - \\frac{1}{n} \\bigg)^n\\\\ \\lim_{n \\rightarrow \\infty} \\bigg(1 - \\frac{1}{n} \\bigg)^n = \\frac{1}{e} \\approx \\frac{1}{3} \\end{align}\\] Shortcomings of Bagging: Model is even harder to “write-down” (than CART) With lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! (because of the number of trees grown for each bootstrap sample) Strengths of Bagging: Can handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!). Can handle more than 2 groups for categorical predictions Easily ignore redundant variables. Perform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables. Data transformations may be less important (monotone transformations on the explanatory variables won’t change anything). Similar bias to CART, but reduced variance (can be proved). Notes on bagging: Bagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped). Note that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand. Bagging gives a smoother decision boundary Bagging can be done on any decision method (not just trees). No need to prune or CV trees. The reason is that averaging keeps us from overfitting a particular few observations (think of averages in other contexts: law of large numbers). Pruning wouldn’t be a bad thing to do in terms of fit, but it is unnecessary for good predictions (and would add a lot to the complexity of the algorithm). 8.7.2 Out Of Bag (OOB) error rate Additionally, with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows: Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree. Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests. How does it work? Consider the following predictions for a silly toy data set of 9 observations. Recall that \\(\\sim 1/3\\) of the observations will be left out at each bootstrap sample. Those are the observations for which predictions will be made. In the table below, an X is given if there is a prediction made for that value. obs tree1 tree2 tree3 tree4 \\(\\cdots\\) tree100 average 1 X X \\(\\sum(pred)/38\\) 2 X \\(\\sum(pred)/30\\) 3 X X \\(\\sum(pred)/33\\) 4 X \\(\\sum(pred)/32\\) 5 X \\(\\sum(pred)/39\\) 6 X X \\(\\sum(pred)/29\\) 7 X \\(\\sum(pred)/29\\) 8 X X X \\(\\sum(pred)/31\\) 9 X \\(\\sum(pred)/36\\) Let the OOB prediction for the \\(i^{th}\\) observation to be \\(\\hat{y}_{(-i)}\\) \\[\\begin{align} \\mbox{OOB}_{\\mbox{error}} &amp;= \\frac{1}{n} \\sum_{i=1}^n \\textrm{I} (y_i \\ne \\hat{y}_{(-i)}) \\ \\ \\ \\ \\ \\ \\ \\ \\mbox{classification}\\\\ \\mbox{OOB}_{\\mbox{error}} &amp;= \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_{(-i)})^2 \\ \\ \\ \\ \\ \\ \\ \\ \\mbox{regression}\\\\ \\end{align}\\] 8.8 11/7/19 Agenda OOB again Random Forests variable importance R code / examples 8.9 Random Forests Random Forests are an extension to bagging for regression trees (note: bagging can be done on any prediction method). Again, with the idea of infusing extra variability and then averaging over that variability, RFs use a subset of predictor variables at every node in the tree. “Random forests does not overfit. You can run as many trees as you want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm 8.9.1 Random Forest algorithm Algorithm: Random Forest Bootstrap sample from the training set. Grow an un-pruned tree on this bootstrap sample. At each split, select \\(m\\) variables and determine the best split using only these predictors. Typically \\(m = \\sqrt{p}\\) or \\(\\log_2 p\\), where \\(p\\) is the number of features. Random Forests are not overly sensitive to the value of \\(m\\). [splits are chosen as with trees: according to either squared error or gini index / cross entropy / classification error.] Do not prune the tree. Save the tree as is! Repeat steps 1-2 for many many trees. For each tree grown on a bootstrap sample, predict the OOB samples. For each tree grown, \\(~1/3\\) of the training samples won’t be in the bootstrap sample – those are called out of bootstrap (OOB) samples. OOB samples can be used as test data to estimate the error rate of the tree. Combine the OOB predictions to create the “out-of-bag” error rate (either majority vote or average of predictions / class probabilities). All trees together represent the model that is used for new predictions (either majority vote or average). Figure 8.1: Building multiple trees and then combining the outputs (predictions). Note that this image makes the choice to average the tree probabilities instead of using majority vote. Both are valid methods for creating a Random Forest prediction model. http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf Shortcomings of Random Forests: Model is even harder to “write-down” (than CART) With lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! … bagging the observations and Strengths of Random Forests: refinement of bagged trees; quite popular (Random Forests tries to improve on bagging by “de-correlating” the trees. Each tree has the same expectation, but the average will again reduce the variability.) subset of predictors makes Random Forests much faster to search through than all predictors creates a diverse set of trees that can be built. Note that by bootstrapping the samples and the predictor variables, we add another level of randomness over which we can average to again decrease the variability. Random Forests are quite accurate generally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (\\(m\\), node size, max number of nodes, etc.). Notes on Random Forests: Bagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped). Random Forests use a subset of predictors. Note that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand. Bagging is a special case of Random Forest where \\(m=k\\). generally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (\\(m\\), node size, max number of nodes, etc.). “Random forests does not overfit. You can run as many trees as you want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm How to choose parameters? \\(\\#\\) trees Build trees until the error no longer decreases \\(m\\) Try the recommended defaults, half of them, and twice of them - pick the best (use CV to avoid overfitting). \\(N&#39;\\) samples \\(N&#39;\\) should be the same size as the training data. Variable Importance All learners are bad when there are too many noisy variables because the response is bound to correlate with some of them. We can measure the contribution of each additional variable in the model by how much the model accuracy decreased when the given variable was excluded from the model. importance = decrease in node impurity resulting from splits over that variable, averaged over all trees (“impurity” is defined as RSS for regression trees and deviance for classification trees). Variable importance is measured by two different metrics (from R help on importance): (permutation) accuracy: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Within the oob values, permute the \\(j^{th}\\) variable and recalculate the prediction error. The difference between the two are then averaged over all trees (with the \\(j^{th}\\) variable) to give the importance for the \\(j^{th}\\) variable. purity: The decrease (or increase, depending on the plot) in node purity: root sum of squares (RSS) [deviance/gini for classification trees]. That is, the amount of total decrease in RSS from splitting on that variable, averaged over all trees. If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run. 8.9.2 R RF Example (“impurity” is defined as RSS for regression trees and deviance for classification trees). method= 'ranger' is about a zillion times faster than method = 'randomForest' or method = 'rf', but they all do the work. data(iris) library(tidyverse) library(caret) library(ranger) library(e1071) inTrain &lt;- createDataPartition(y = iris$Species, p=0.7, list=FALSE) iris.train &lt;- iris[inTrain,] iris.test &lt;- iris[-inTrain,] modFit &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;ranger&quot;) modFit ## Random Forest ## ## 105 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 105, 105, 105, 105, 105, 105, ... ## Resampling results across tuning parameters: ## ## mtry splitrule Accuracy Kappa ## 2 gini 0.9519882 0.9268222 ## 2 extratrees 0.9501667 0.9239468 ## 3 gini 0.9509638 0.9252363 ## 3 extratrees 0.9511923 0.9254990 ## 4 gini 0.9489015 0.9221052 ## 4 extratrees 0.9501289 0.9238519 ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 1 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 2, splitrule = gini ## and min.node.size = 1. caret::confusionMatrix(data=predict(modFit, newdata = iris.test), reference = iris.test$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 14 2 ## virginica 0 1 13 ## ## Overall Statistics ## ## Accuracy : 0.9333 ## 95% CI : (0.8173, 0.986) ## No Information Rate : 0.3333 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.9 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.0000 0.9333 0.8667 ## Specificity 1.0000 0.9333 0.9667 ## Pos Pred Value 1.0000 0.8750 0.9286 ## Neg Pred Value 1.0000 0.9655 0.9355 ## Prevalence 0.3333 0.3333 0.3333 ## Detection Rate 0.3333 0.3111 0.2889 ## Detection Prevalence 0.3333 0.3556 0.3111 ## Balanced Accuracy 1.0000 0.9333 0.9167 8.9.2.1 Parameters… mtry and num.trees Working with both parameters is a little bit odd in this case because the iris data only has 4 variables (which makes it a poor candidate for Random Forests). Hopefully the code below will help for other problems where Random Forests are more appropriate. modFit.m &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;ranger&quot;, trControl = trainControl(method=&quot;oob&quot;), num.trees = 500, tuneGrid= data.frame(mtry=1:4, splitrule = &quot;gini&quot;, min.node.size = 5)) modFit.m ## Random Forest ## ## 105 samples ## 4 predictor ## 3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; ## ## No pre-processing ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 1 0.9428571 0.9142857 ## 2 0.9523810 0.9285714 ## 3 0.9523810 0.9285714 ## 4 0.9523810 0.9285714 ## ## Tuning parameter &#39;splitrule&#39; was held constant at a value of gini ## ## Tuning parameter &#39;min.node.size&#39; was held constant at a value of 5 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were mtry = 2, splitrule = gini ## and min.node.size = 5. plot(modFit.m) set.seed(47) acc.ntree = c() for(i in seq(10, 260, by = 50)){ modFit.ntree &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;ranger&quot;, trControl = trainControl(method=&quot;oob&quot;), num.trees = i, tuneGrid= data.frame(mtry=3, splitrule = &quot;gini&quot;, min.node.size = 5)) acc.ntree &lt;- c(acc.ntree, modFit.ntree$finalModel$prediction.error) } data.frame( ntree = seq(10, 260, by = 50), acc.ntree) %&gt;% ggplot( aes(x=ntree, y = acc.ntree)) + geom_line() + xlab(&quot;number of trees in the RF&quot;) + ylab(&quot;OOB Accuracy&quot;) + ylim(c(0.04, 0.06)) Variable Importance In order to get the variable importance, you need to specify importance within the building of the forest. See if you can figure out every single line of the ggplot code. modFit.VI &lt;- caret::train(Species ~ ., data=iris.train, importance = &quot;permutation&quot;, method=&quot;ranger&quot;) ranger::importance(modFit.VI$finalModel) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 0.026859630 0.006130369 0.297439677 0.323197349 data.frame(importance = modFit.VI$finalModel$variable.importance, variables = names(modFit.VI$finalModel$variable.importance)) %&gt;% ggplot(aes(x = reorder(variables, importance), y = importance)) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Variable&quot;) + coord_flip() plot both the given labels as well as the predicted labels iris.test &lt;- iris.test %&gt;% mutate(testSpecies = predict(modFit, iris.test)) ggplot(iris.test, aes(x=Petal.Width, y=Petal.Length, shape = Species, col = testSpecies)) + geom_point(size=3) 8.10 Model Choices There are soooooo many choices we’ve made along the way. The following list should make you realize that there is no truth with respect to any given model. Every choice will (could) lead to a different model. \\(\\mbox{ }\\) \\(\\mbox{ }\\) * explanatory variable choice * k (kNN) * number of explanatory variables * distance measure * functions/transformation of explanatory * k (CV) * transformation of response * CV set.seed * response:continuous vs. categorical * alpha prune * how missing data is dealt with * maxdepth prune * train/test split (set.seed) * prune or not * train/test proportion * gini / entropy (split) * type of classification model * # trees / # BS samples * use of cost complexity / parameter * grid search etc. for tuning * majority / average prob (tree error rate) * value(s) of mtry * accuracy vs sensitivity vs specificity * OOB vs CV for tuning 8.11 11/12/19 Agenda linearly separable dot products support vector formulation 8.12 Support Vector Machines Support Vector Machines are one more algorithm for classification. As you’ll see, they have some excellent properties, but one important aspect to note is that they use only numeric predictor variables and only binary response variables (classify two groups). Vladimir Vapnik (b. 1936) created SVMs in the late 1990s. History: he actually did the work as his PhD in the early 60s in the Soviet Union. Someone from Bell Labs asked him to visit, and he ended up immigrating to the US. No one actually thought that SVMs would work, but he eventually (1995 - took 30 years between the idea and the implementation) bet a dinner on classifying handwriting via SVM (using a very simple kernel) versus neural networks and the rest is history. The basic idea of SVMs is to figure out a way to create really complicated decision boundaries. We want to put in a straight line with the widest possible street (draw street with gutters and 4 points, two positive and two negative). The decision rule has to do with a dot product of the new sample with a vector \\({\\bf w}\\) which is perpendicular to the median of the “street.” Note: the standard formulation of SVM requires the computer to find dot products between each of the observations. In order to do so, the explanatory variables must be numeric. In order for the dot products to be meaningful, the data must be on the same scale. 8.12.0.1 Linear Separator Recall ideas of kNN and trees: But today’s decision boundary is going to be based on a hyperplane which separates the values in the “best” way. Certainly, if the data are linearly separable, then there are infinitely many hyperplanes which will partition the data perfectly. For SVM, the idea is to find the “street” which separates the positive and negative samples to give the widest margin. Figure 8.2: The correct project of the observations can often produce a perfect one dimensional (i.e., linear) classifier. http://www.rmki.kfki.hu/~banmi/elte/Bishop - Pattern Recognition and Machine Learning.pdf Aside: what is a dot product? Let \\({\\bf x} = (x_1, x_2, \\ldots, x_p)^t\\) and \\({\\bf y} = (y_1, y_2, \\ldots, y_p)^t\\) be two vectors which live in \\(R^p\\). Then their dot product is defined as: \\[\\begin{align} {\\bf x} \\cdot {\\bf y} = {\\bf x}^t {\\bf y} = \\sum_{i=1}^p x_i y_i \\end{align}\\] Figure 8.3: If w is known, then the projection of any new observation onto w will lead to a linear partition of the space. How can the street be used to get a decision rule? All that is known is that \\({\\bf w}\\) is perpendicular to the street. We don’t yet know \\({\\bf w}\\) or \\(b\\). The “width” of the street will be a vector which is perpendicular to the street (median). We don’t know the width yet, but we know know that we can use that perpendicular vector (\\({\\bf w}\\)) to figure out how to classify the points. Project an unknown point (\\({\\bf u}\\)) onto \\({\\bf w}\\) to see which side of the street the unknown value lands. That is, if the projection is large enough, we would classify the point as positive: \\[{\\bf w} \\cdot {\\bf u} \\geq c?\\] [Keep in mind that \\({\\bf u} \\cdot {\\bf w} = ||{\\bf w}|| \\times\\)(the length of the shadow). That is, the projection will only be the length of the shadow if \\({\\bf w}\\) is a unit vector. And we aren’t going to constrain \\({\\bf w}\\) to be unit vector (though we could!). But regardless, \\({\\bf u} \\cdot {\\bf w}\\) still gives the ability to classify because it is proportional to the length of the shadow.] Decision rule: if \\({\\bf w} \\cdot {\\bf u} + b \\geq 0\\) then label the new sample “positive” where \\({\\bf w}\\) is created in such a way that it is perpendicular to the median of the street. Then the unknown (\\({\\bf u}\\)) vector is projected onto \\({\\bf w}\\) to see if it is on the left or the right side of the street. But we don’t know the values in the decision rule! We need more constraints. Assuming that the data are linearly separable, as an initial step to find \\({\\bf w}\\) and \\(b\\), for all positive samples (\\(x_+\\)) and all negative samples (\\(x_-\\)) force: \\[\\begin{align} {\\bf w} \\cdot {\\bf x}_+ + b &amp;\\geq 1 \\tag{8.1}\\\\ {\\bf w} \\cdot {\\bf x}_- + b &amp;\\leq -1 \\tag{8.2} \\end{align}\\] For mathematical convenience (so that we don’t have 2 equations hanging around), introduce \\(y_i\\) such that \\[\\begin{align} y_i &amp;= 1 \\mbox{ for positive samples}\\\\ y_i &amp;= -1 \\mbox{ for negative samples} \\end{align}\\] Which simplifies the criteria for finding \\({\\bf w}\\) and \\(b\\) to be: \\[ y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1\\] (Multiplying through by -1 on equation ((8.2) switches the signs, and both equation ((8.1)) and ((8.2) end up as the same for both types of points.) Again, working toward solving for \\({\\bf w}\\) and \\(b\\), add the additional constraint that for the points in the gutter (on the margin lines): For \\(x_i\\) in the gutter (by definition): \\[y_i({\\bf w} \\cdot {\\bf x}_i + b) - 1 = 0\\] Now consider two particular positive and negative values that live on the margin (gutter). The difference is almost the width of the street (we want to find the street that is as wide as possible), but it is at the wrong angle (see street picture again). Remember, our goal here is to find the street separating the pluses and the minuses that is as wide as possible. If we had a unit vector, we could dot it with \\((x_+ - x_-)\\) to get the width of the street! \\[\\begin{align} width = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||} \\end{align}\\] which doesn’t do us much good yet. Goal: Try to find as wide a street as possible. But remember, the gutter points are constrained: it turns out that \\(x_+ \\cdot {\\bf w} = 1 - b\\) and \\(x_- \\cdot {\\bf w} = -1 - b\\). Therefore: \\[\\begin{align} width = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||} = \\frac{(1-b) - (-1-b)}{|| {\\bf w} ||} = \\frac{2}{||w||} \\end{align}\\] In order to maximize \\(\\frac{2}{||w||}\\), minimize \\(||w||\\), or minimize \\((1/2)*||w||^2\\) (to make it mathematically easier). We have all the pieces of making the decision rules as an optimization problem. That is, minimize some quantity subject to the constraints given by the problem. Lagrange multipliers Recall, with Lagrange multipliers, the first part is the optimization, the second part is the constraint. The point of Lagrange multipliers is to put together the constraint and the optimization into one equation where you don’t worry about the constraints any longer. \\(L\\) consists of two parts. The first is the thing to minimize. The second is the set of constraints (here, the summation over all the constraints). Each constraint has a multiplier \\(\\alpha_i\\), the non-zero \\(\\alpha_i\\) will be the ones connected to the values on the gutter. \\[\\begin{align} L = \\frac{1}{2}||{\\bf w}||^2 - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1] \\end{align}\\] Find derivatives, set them equal to zero. Note that we can differentiate with respect to the vector component wise, but we’ll skip that notation, but you could do it one element at a time. \\[\\begin{align} \\frac{\\partial L}{\\partial {\\bf w}} &amp;= {\\bf w} - \\sum \\alpha_i y_i {\\bf x}_i = 0 \\rightarrow {\\bf w} = \\sum \\alpha_i y_i {\\bf x}_i \\\\ \\frac{\\partial L}{\\partial b} &amp;= -\\sum \\alpha_i y_i = 0\\\\ \\end{align}\\] It turns out that \\({\\bf w}\\) is a linear sum of data vectors, either all of them or some of them (it turns out that for some \\(i\\), \\(\\alpha_i=0\\)): \\[{\\bf w} = \\sum \\alpha_i y_i {\\bf x}_i\\] Use the value of \\({\\bf w}\\) to plug back into \\(L\\) to maximize. \\[\\begin{align} L &amp;= \\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum_i \\alpha_i [ y_i ((\\sum_j \\alpha_j y_j {\\bf x}_j) {\\bf x}_i + b ) - 1]\\\\ &amp;= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum \\alpha_i y_i b + \\sum \\alpha_i\\\\ &amp;= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - 0 + \\sum \\alpha_i\\\\ &amp;= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot {\\bf x}_j \\end{align}\\] Find the minimum of this expression: \\[L = \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot {\\bf x}_j\\] The computer / numerical analyst is going to solve \\(L\\) for the \\(\\alpha_i\\), so why did we go to all the work? We need to understand the dependencies of sample vectors. That is, the optimization depends only on the dot product of pairs of samples . And the decision rule also depends only on the dot product of the new observation with the original samples. [Note, the points on the margin / gutter can be used to solve for \\(b\\): \\(b =y_i - {\\bf w} \\cdot {\\bf x}_i\\), because \\(y_i = 1/y_i\\).] Decision Rule, call positive if: \\[\\sum \\alpha_i y_i {\\bf x}_i \\cdot {\\bf u} + b \\geq 0\\] Note that we have a convex space (can be proved), and so we can’t get stuck in a local maximum. 8.12.1 11/14/19 Agenda not linearly separable (SVM) kernels (SVM) support vector formulation 8.12.2 Not Linearly Separable 8.12.2.1 Transformations Simultaneously, the data can be transformed into a new space where the data are linearly separable. If we can transform the data into a different space (where they are linearly separable), then we can transform the data into the new space and then do the same thing! That is, consider the function \\(\\phi\\) such that our new space consists of vectors \\(\\phi({\\bf x})\\). Consider the case with a circle on the plane. The class boundary should segment the space by considering the points within that circle to belong to one class, and the points outside that circle to another one. The space is not linearly separable, but mapping it into a third dimension will make it separable. Two great videos: https://www.youtube.com/watch?v=3liCbRZPrZA and https://www.youtube.com/watch?v=9NrALgHFwTo . Within the transformed space, the minimization procedure will amount to minimizing the following: We want the minimum of this expression: \\[\\begin{align} L &amp;= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j \\phi({\\bf x}_i) \\cdot \\phi({\\bf x}_j)\\\\ &amp;= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j K({\\bf x}_i, {\\bf x}_j) \\end{align}\\] Decision Rule, call positive if: \\[\\begin{align} \\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b &amp;\\geq&amp; 0\\\\ \\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b &amp;\\geq&amp; 0 \\end{align}\\] 8.12.3 11/19/19 Agenda kernels not separable: soft margins / cost one vs. one / one vs. all Kernel Examples: Kernel 1 Consider the following transformation, \\(\\phi: R^2 \\rightarrow R^3\\): \\[\\begin{align} \\phi({\\bf x}) &amp;= (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\\\\ K({\\bf x}, {\\bf y}) &amp;= \\phi({\\bf x}) \\cdot \\phi({\\bf y}) = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2\\\\ &amp;= (x_1y_1 + x_2y_2)^2\\\\ K({\\bf x}, {\\bf y}) &amp;= ({\\bf x} \\cdot {\\bf y})^2 \\end{align}\\] Which is to say, as long as we know the dot product of the original data, then we can recover the dot product in the transformed space using the quadratic kernel. Kernel 2 Writing the polynomial kernel out (for \\(d=2\\)), we can find the exact \\(\\phi\\) function. Consider the following polynomial kernel for \\(d=2\\). \\[K({\\bf x}, {\\bf y}) = ({\\bf x} \\cdot {\\bf y} + c)^2\\] By writing down the dot product and then considering the square of each of the components separately, we get \\[\\begin{align} ({\\bf x} \\cdot {\\bf y} + c)^2 &amp;= (c + \\sum_{i=1}^p x_i y_i)^2\\\\ &amp;= c^2 + \\sum_{i=1}^p x_i^2 y_i^2 + \\sum_{i=1}^{p-1} \\sum_{j={i+1}}^{p} 2x_i y_i x_j y_j + \\sum_{i=1}^p 2 cx_i y_i \\end{align}\\] By pulling the sum apart into all the components of the \\({\\bf x}\\) and \\({\\bf y}\\) vectors separately, we find that \\[\\begin{align} \\phi({\\bf x}) = (c, x_1^2, \\ldots, x_p^2, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_1x_p, \\sqrt{2}x_2x_3, \\ldots, \\sqrt{2}x_{p-1}x_p, \\sqrt{2c}x_1, \\ldots, \\sqrt{2c}x_p) \\end{align}\\] Kernel 3 Using the radial kernel (see below) it is possible to map the observations into an infinite dimensional space yet still only consider the kernel associated with the dot product of the original data. Consider the following example for \\(x\\) in one dimension mapped to infinite dimensions. \\[\\begin{align} \\phi_{RBF}(x) &amp;= e^{-\\gamma x} \\bigg(1, \\sqrt{\\frac{2\\gamma}{1!}} x, \\sqrt{\\frac{(2\\gamma)^2}{2!}} x^2, \\sqrt{\\frac{(2\\gamma)^3}{3!}} x^3, \\ldots \\bigg)^t\\\\ K_{RBF} (x,y) &amp;= \\exp( -\\gamma ||x-y||^2) \\end{align}\\] where cross validation is used to find the tuning value \\(\\gamma\\) as well as the penalty parameter \\(C\\). Consider the following example from http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex8/ex8.html. What if the boundary is wiggly? The take home message here is that a wiggly boundary is really best, and the value of \\(\\gamma\\) should be high to represent the high model complexity. Figure 8.4: Extremely complicated decision boundary What if the boundary isn’t wiggly? But if the boundary has low complexity, then the best value of \\(\\gamma\\) is probably much lower. Figure 8.5: Simple decision boundary Figure 8.6: Simple decision boundary – reasonable gamma Figure 8.7: Simple decision boundary – gamma too big 8.12.3.1 What is a Kernel? What is a kernel: A kernel function is a function that obeys certain mathematical properties. I won’t go into these properties right now, but for now think of a kernel as a function as a function of the dot product between two vectors, (e.g. a measure of “similarity” between the two vectors). If \\(K\\) is a function of two vectors \\({\\bf x}\\) and \\({\\bf y}\\), then it is a kernel function if \\(K\\) is the dot product of \\(\\phi()\\) applied to those vectors. We know that \\(\\phi()\\) exists if \\(K\\) is symmetric and if when \\(K_{ij} = K({\\bf x}_i, {\\bf x}_j)\\), the matrix \\({\\bf K} = [K_{ij}]\\) is positive definite. A helpful website about kernels: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html \\[\\begin{align} K({\\bf x},{\\bf y}) = \\phi({\\bf x}) \\cdot \\phi({\\bf y}) \\end{align}\\] Examples of kernels: linear \\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\] Note, the only tuning parameter is the penalty/cost parameter \\(C\\)). polynomial \\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma &gt; 0\\] Note, here \\(\\gamma, r, d\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)). RBF The radial basis function is also called the Gaussian kernel because of its similarity to the Gaussian distribution (aka the normal distribution). Because the RBF maps to infinite dimensional space, it can easily over fit the training data. Care must be taken to estimate \\(\\gamma\\). \\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} - {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\] Note, here \\(\\gamma\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)). sigmoid The sigmoid kernel is not a valid kernel method for all values of \\(\\gamma\\) and \\(r\\) [which means that for certain parameter values, the \\(\\phi()\\) function may not exist]. \\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\] Note, here \\(\\gamma, r\\) must be tuned using cross validation (along with the penalty/cost parameter \\(C\\)). One benefit of the sigmoid kernel is that it has equivalence to a two-layer perceptron neural network. 8.12.3.2 Soft Margins But what if the data aren’t linearly separable? The optimization problem can be changed to allow for points to be on the other side of the margin. The optimization problem is slightly more complicated, but basically the same idea: \\[y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i \\ \\ \\ \\ \\ \\ 1 \\leq i \\leq n, \\ \\ \\xi_i \\geq 0\\] Figure 8.8: Note that now the problem is set up such that points are allowed to cross the boundary. Slack variables (the xi_i) allow for every point to be classified correctly up to the slack. Note that xi_i=0 for any point that is actually calculated correctly. The optimization problem gets slightly more complicated in two ways, first, the minimization piece includes a penalty parameter, \\(C\\) (how much misclassification is allowed - the value of \\(C\\) is set/tuned not optimized), and second, the constraint now allows for points to be misclassified. Minimize (for \\({\\bf w}\\), \\(\\xi_i\\), \\(b\\)): \\[\\frac{1}{2} ||{\\bf w}||^2 + C \\sum_{i=1}^n \\xi_i\\] Subject to: \\[y_i ({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i \\ \\ \\ \\ \\xi_i \\geq 0\\] Which leads to the following Lagrangian equation: \\[\\begin{align} L = \\frac{1}{2}||{\\bf w}||^2 + C \\sum_{i=1}^n \\xi_i - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1 + \\xi_i] - \\sum_{i=1}^n \\beta_i \\xi_i \\ \\ \\ \\ \\alpha_i, \\beta_i \\geq 0 \\end{align}\\] That is, the objective function now allows for a trade-off between a large margin and a small error penalty. Again, Lagrange multipliers can be shown to give classification rule that is based only on the dot product of the observations. The key here is that although quadratic programming can be used to solve for most of the parameters, \\(C\\) is now a tuning parameter that needs to be set by the user or by cross validation. How does \\(C\\) relate to margins? Notice that the minimization is now over many more variables (with \\(C\\) set/tuned - not optimized). If we are allowing for misclassification and \\(C=0\\), that implies that \\(\\xi_i\\) can be as large as possible. Which means the algorithm will choose the widest possible street. The widest possible street will be the one that hits at the two most extreme data points (the “support vectors” will now be the ones on the edge, not the ones near the separating hyperplane). \\(C\\) small allows the constraints (on points crossing the line) to be ignored. \\[C=0 \\rightarrow \\mbox{ can lead to large training error}\\] If \\(C\\) is quite large, then the algorithm will try very hard to classify exactly perfectly. That is, it will want \\(\\xi_i\\) to be as close to zero as possible. When projecting into high dimensions, we can always perfectly classify, so a large \\(C\\) will tend to overfit the training data and give a very small margin. \\[C&gt;&gt;&gt; \\rightarrow \\mbox{ can lead to classification rule which does not generalize to test data}\\] Figure 8.9: In the first figure, the low C value gives a large margin. On the right, the high C value gives a small margin. Which classifier is better? Well, it depends on what the actual data (test, population, etc.) look like! In the second row the large C classifier is better; in the third row, the small C classifier is better. photo credit: http://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel 8.12.4 Support Vector Machine algorithm Algorithm: Support Vector Machine Using cross validation, find values of \\(C, \\gamma, d, r\\), etc. (and the kernel function!) Using Lagrange multipliers (read: the computer), solve for \\(\\alpha_i\\) and \\(b\\). Classify an unknown observation (\\({\\bf u}\\)) as “positive” if: \\[\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b = \\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b \\geq 0\\] Shortcomings of Support Vector Machines: Can only classify binary categories (response variable). All predictor variables must be numeric. A great differential in range will allow variables with large range to dominate the predictions. Either linearly scale each attribute to some range [ e.g., (-1, +1) or (0,1)] or divide by the standard deviation. Categorical variables can be used if formatted as binary factor variables. Whatever is done to the training data must also be done to the test data! Another problem is the kernel function itself. With primitive data (e.g., 2d data points), good kernels are easy to come by. With harder data (e.g., MRI scans), finding a sensible kernel function may be much harder. With really large data, it doesn’t perform well because of the large amount of required training time It also doesn’t perform very well when the data set has a lot of noise i.e., target classes are overlapping SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. Strengths of Support Vector Machines: Can always fit a linear separating hyper plane in a high enough dimensional space. The kernel trick makes it possible to not know the transformation functions, \\(\\phi\\). Because the optimization is on a convex function, the numerical process for finding solutions are extremely efficient. It works really well with clear margin of separation It is effective in high dimensional spaces. It is effective in cases where number of dimensions is greater than the number of samples. It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. 8.12.5 Classifying more than one group When there are more than two classes, the problem needs to be reduced into a binary classification problem. Consider the groups associated with Red, Green, and Blue. In order to figure out which points get classified as Red, two different methods can be applied. One vs All Each category can be compared to the rest of the groups. This will create \\(K\\) different classifiers (here \\(K=\\) the number of classes the response variable can take on). Each test value would then be classified according to each classifier, and the group assignment would be given by the group giving the highest value of \\({\\bf w}_K \\cdot {\\bf u} + b\\), as the projection would represent the classification farthest into the group center. In the end, there will be \\(K\\) classifiers. One vs One Alternatively, each group can be compared with each other group (e.g., Red vs. Green, Red vs. Blue, Green vs. Blue). Class membership will be determine by the group to which the unknown point is most often classified. In the end, there will be \\(K(K-1)/2\\) classifiers. 8.12.6 R SVM Example We’ll go back to the iris data. As a first pass, let’s use SVM to distinguish between Setosa and Versicolor/Virginica. library(kernlab) library(e1071) library(caret) data(iris) iris2 &lt;- iris %&gt;% dplyr::mutate(Species2 = as.factor(ifelse(Species == &quot;setosa&quot;, &quot;S&quot;, &quot;V&quot;))) %&gt;% dplyr::select(-Species) 8.12.6.1 Building an SVM on training data with radial kernel set.seed(4747) iris.svm.rad &lt;- train(Species2 ~ ., data=iris2, method=&quot;svmRadial&quot;, trControl = trainControl(method=&quot;cv&quot;), tuneGrid= expand.grid(C=c(0.01,0.1,1,10,100), sigma=c(.5,1,2,3,4)), preProcess = c(&quot;center&quot;, &quot;scale&quot;)) iris.svm.rad ## Support Vector Machines with Radial Basis Function Kernel ## ## 150 samples ## 4 predictor ## 2 classes: &#39;S&#39;, &#39;V&#39; ## ## Pre-processing: centered (4), scaled (4) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 135, 135, 135, 135, 135, 135, ... ## Resampling results across tuning parameters: ## ## C sigma Accuracy Kappa ## 1e-02 0.5 0.6666667 0.0000000 ## 1e-02 1.0 0.6666667 0.0000000 ## 1e-02 2.0 0.6666667 0.0000000 ## 1e-02 3.0 0.6666667 0.0000000 ## 1e-02 4.0 0.6666667 0.0000000 ## 1e-01 0.5 0.9866667 0.9684211 ## 1e-01 1.0 0.9800000 0.9508772 ## 1e-01 2.0 0.9466667 0.8684211 ## 1e-01 3.0 0.8800000 0.6892931 ## 1e-01 4.0 0.7933333 0.4411765 ## 1e+00 0.5 0.9933333 0.9842105 ## 1e+00 1.0 0.9933333 0.9842105 ## 1e+00 2.0 0.9866667 0.9684211 ## 1e+00 3.0 0.9866667 0.9684211 ## 1e+00 4.0 0.9866667 0.9684211 ## 1e+01 0.5 0.9933333 0.9842105 ## 1e+01 1.0 0.9933333 0.9842105 ## 1e+01 2.0 0.9866667 0.9684211 ## 1e+01 3.0 0.9866667 0.9684211 ## 1e+01 4.0 0.9866667 0.9684211 ## 1e+02 0.5 0.9933333 0.9842105 ## 1e+02 1.0 0.9933333 0.9842105 ## 1e+02 2.0 0.9866667 0.9684211 ## 1e+02 3.0 0.9866667 0.9684211 ## 1e+02 4.0 0.9866667 0.9684211 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 1 and C = 1. 8.12.6.2 Training Error iris.train.pred &lt;- predict(iris.svm.rad, iris2) caret::confusionMatrix(iris.train.pred, iris2$Species2)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 1.000000e+00 1.000000e+00 9.757074e-01 1.000000e+00 6.666667e-01 ## AccuracyPValue McnemarPValue ## 3.857546e-27 NaN caret::confusionMatrix(iris.train.pred, iris2$Species2) ## Confusion Matrix and Statistics ## ## Reference ## Prediction S V ## S 50 0 ## V 0 100 ## ## Accuracy : 1 ## 95% CI : (0.9757, 1) ## No Information Rate : 0.6667 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0000 ## Specificity : 1.0000 ## Pos Pred Value : 1.0000 ## Neg Pred Value : 1.0000 ## Prevalence : 0.3333 ## Detection Rate : 0.3333 ## Detection Prevalence : 0.3333 ## Balanced Accuracy : 1.0000 ## ## &#39;Positive&#39; Class : S ## 8.12.6.3 Visualizing the Boundary caret doesn’t allow for visualization, so I’ve applied the svm function directly here. I had to try various combinations of variables before finding a boundary that was visually interesting. Note that just being in the yellow or red part doesn’t necessarily indicate missclassification. Remember that the boundary is in 4 dimensions and is not linear. iris.svm &lt;- e1071::svm(Species2 ~ ., data = iris2, kernel = &quot;radial&quot;) plot(iris.svm, data = iris2, Sepal.Width ~ Petal.Width, slice=list(Sepal.Length = 3, Petal.Length = 3)) 8.12.6.4 3 groups For multiclass-classification with k classes, k &gt; 2, ksvm uses the ‘one-against-one’-approach, in which k(k-1)/2 binary classifiers are trained; the appropriate class is found by a voting scheme set.seed(474747) train.obs &lt;- sample(1:150, 100, replace = FALSE) iris.train &lt;- iris[train.obs, ] iris.test &lt;- iris[-train.obs, ] iris.svm3 &lt;- caret::train(Species ~ ., data=iris.train, method=&quot;svmRadial&quot;, trControl = trainControl(method=&quot;none&quot;), tuneGrid= expand.grid(C=1, sigma=2), preProcess = c(&quot;center&quot;, &quot;scale&quot;)) predict(iris.svm3, iris.test) ## [1] setosa setosa setosa setosa setosa setosa ## [7] setosa setosa setosa setosa setosa setosa ## [13] virginica setosa setosa setosa versicolor versicolor ## [19] versicolor versicolor versicolor versicolor versicolor virginica ## [25] versicolor versicolor versicolor versicolor versicolor versicolor ## [31] versicolor versicolor versicolor virginica virginica virginica ## [37] virginica virginica virginica versicolor virginica virginica ## [43] virginica versicolor virginica virginica virginica virginica ## [49] virginica virginica ## Levels: setosa versicolor virginica table(predict(iris.svm3, iris.test), iris.test$Species) ## ## setosa versicolor virginica ## setosa 15 0 0 ## versicolor 0 16 2 ## virginica 1 1 15 References "],
["unsup.html", "Chapter 9 Unsupervised Methods 9.1 11/21/19 Agenda 9.2 Latent Dirichlet Allocation 9.3 Dissimilarities 9.4 Hierarchical Clustering 9.5 Partitioning Clustering 9.6 Evaluation Metrics 9.7 EM algorithm", " Chapter 9 Unsupervised Methods 9.1 11/21/19 Agenda unsupervised methods distance / dissimilarity hierarchical clustering partitioning clustering The classification models we’ve discussed are all supervised learning techniques. The word supervised refers to the fact that we know the response variable of all of the training observations. Next up, we’ll discuss clustering which is an unsupervised technique – none of the observations have a given response variable. For example, we might want to cluster a few hundred melanoma patients based on their genetic data. We are looking for patterns in who groups together, but we don’t have a preconceived idea of which patients belong to which group. There are also semi-supervised techniques applied to data which have some observations that are labeled and some that are not. We will not discuss semi-supervised methods in this class. Clustering creates groups of observations via unsupervised methods. We will cover hierarchical clustering, k-means, and k-medoids. We cluster for two main reasons: Summary: to describe the data and the observations’ similarities to each other. Discovery: to find new ways in which groups of observations are similar. Classification – SUPERVISED! creates predictions (and prediction models) for unknown future observations via supervised methods. With classification the group membership (i.e., response variable) is known for all the training data. We covered k-NN, CART, bagging, Random Forests, and SVMs. 9.2 Latent Dirichlet Allocation LDA views each document as a mixture of a small (predefined) number of topics that describe a set of documents. Each word (typically very common and extremely rare words are removed before modeling) represents a an occurrence generated by one of the document’s topics (where the document is modeled as a mixture of topics). Through LDA, the model learns both the composition of each topic and the topic mixture of each document. From Wikipedia: In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word’s creation is attributable to one of the document’s topics. Here is an algorithm for finding words that represent \\(K\\) topics (where \\(K\\) is chosen in advance). [Explained in more detail at http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/.] For each document in the training data, randomly assign each word to one of the \\(K\\) topics. Each of the \\(K\\) topics now has a set of words associated with it (albeit, that set of words is meaningless). To improve on the set of words associated with each topic. For each word \\(w\\) in document \\(d\\): For each topic \\(t\\), compute two things: p(topic \\(t\\) \\(|\\) document \\(d\\)) = the proportion of words in document \\(d\\) that are currently assigned to topic \\(t\\) p(word \\(w\\) \\(|\\) topic \\(t\\)) = the proportion of assignments to topic \\(t\\) over all documents that come from this word \\(w\\). Reassign \\(w\\) to a new topic, where we choose topic \\(t\\) with probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according to our generative model, this is essentially the probability that topic \\(t\\) generated word \\(w\\), so it makes sense that we resample the current word’s topic with this probability). In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated. After repeating the previous steps a large number of times, the list of words in each topic will reach a steady state. The resulting assignments estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall). https://ziqixiong.shinyapps.io/TopicModeling/ 9.3 Dissimilarities Many, though not all, clustering algorithms are based on distances between the objects being clustered. Mathematical properties of a distance function are the following. Consider two vectors \\({\\bf x}\\) and \\({\\bf y}\\) (\\({\\bf x}, {\\bf y} \\in \\mathbb{R}^p\\)), and the distance between them: \\(d({\\bf x}, {\\bf y})\\). \\(d({\\bf x}, {\\bf y}) \\geq 0\\) \\(d({\\bf x}, {\\bf y}) = d({\\bf y}, {\\bf x})\\) \\(d({\\bf x}, {\\bf y}) = 0\\) iff \\({\\bf x} = {\\bf y}\\) \\(d({\\bf x}, {\\bf y}) \\leq d({\\bf x}, {\\bf z}) + d({\\bf z}, {\\bf y})\\) for all other vectors \\({\\bf z}\\). Triangle Inequality The key to proving the triangle inequality for most of the distances relies on the Cauchy-Schwarz inequality. \\[\\begin{align} {\\bf x} \\cdot {\\bf y} &amp;= || {\\bf x} || ||{\\bf y}|| \\cos(\\theta) \\\\ |{\\bf x} \\cdot {\\bf y}| &amp;\\leq || {\\bf x} || ||{\\bf y}|| \\end{align}\\] Euclidean Distance \\[d_E({\\bf x}, {\\bf y}) = \\sqrt{\\sum_{i=1}^p (x_i - y_i)^2}\\] Distance properties all check out. Cauchy-Schwarz: \\[\\begin{align} \\sum_{i=1}^p(x_i - y_i)^2 = \\sum_{i=1}^p ( (x_i - z_i) + (z_i - y_i))^2 &amp;\\leq \\Bigg( \\sqrt{\\sum_{i=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{i=1}^p(z_i - y_i)^2} \\Bigg)^2\\\\ \\sqrt{\\sum_{i=1}^p(x_i - y_i)^2} &amp;\\leq \\sqrt{\\sum_{i=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{i=1}^p(z_i - y_i)^2}\\\\ d_E({\\bf x}, {\\bf y}) &amp;\\leq d_E({\\bf x}, {\\bf z}) + d_E({\\bf z}, {\\bf y}) \\end{align}\\] Shortcomings: \\(d_E\\) is not scale invariant. \\(d_E\\) measures magnitude differences, not pattern differences. \\(d_E\\) is sensitive to outliers. Strengths: Directly measures what is commonly considered to be “distance.” Pearson Correlation Distance \\[\\begin{align} d_P({\\bf x}, {\\bf y}) &amp;= 1 - r_P ({\\bf x}, {\\bf y})\\\\ \\mbox{ or } &amp;= 1 - |r_P ({\\bf x}, {\\bf y})|\\\\ \\mbox{ or } &amp;= 1 - (r_P ({\\bf x}, {\\bf y}))^2\\\\ \\end{align}\\] Notice that Euclidean distance and Pearson correlation distance are similar if the original observations are scaled. Assume that the sample mean for \\({\\bf x}\\) (that is, \\(\\frac{1}{p} \\sum x_i = \\overline{x} = 0\\)) is zero and the sample standard deviation is 1. \\[\\begin{align} r_P ({\\bf x}, {\\bf y}) &amp;= \\frac{\\sum x_i y_i - p \\ \\overline{x} \\ \\overline{y}}{(p-1)s_x s_y}\\\\ &amp;= \\frac{1}{(p-1)} \\sum x_i y_i\\\\ &amp; \\ \\ &amp; \\\\ d_E({\\bf x}, {\\bf y}) &amp;= \\sqrt{\\sum(x_i - y_i)^2}\\\\ &amp;= \\sqrt{ \\sum x_i^2 + \\sum y_i^2 - 2 \\sum x_i y_i}\\\\ d_E^2 &amp;= 2[(p-1) - \\sum x_i y_i]\\\\ &amp;= 2(p-1)*[1 - r_P({\\bf x}, {\\bf y})] \\end{align}\\] Distance properties don’t hold for Pearson correlation. \\({\\bf y}=a{\\bf x}\\) \\[\\begin{align} d_P({\\bf x}, {\\bf y}) &amp;= 1 - r_P ({\\bf x}, {\\bf y})\\\\ &amp;= 1 - r_P ({\\bf x}, a{\\bf x})\\\\ &amp;= 1 - 1 = 0 \\end{align}\\] \\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\) \\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\) \\(d_P({\\bf x}, {\\bf y}) + d_P({\\bf y}, {\\bf z}) &lt; d_P({\\bf z}, {\\bf x})\\) \\(\\rightarrow\\leftarrow\\) Shortcomings: \\(d_P\\) does not satisfy the triangle inequality. \\(d_P\\) is sensitive to outliers. Strengths: Can measure the distance between variables on different scales (although will still be sensitive to extreme values). Spearman Correlation Distance Spearman correlation distance uses the Spearman correlation instead of the Pearson correlation. The Spearman correlation is simply the Pearson correlation applied to the ranks of the observations. The ranking allows the Spearman distance to be resistant to outlying observations. \\[\\begin{align} d_S({\\bf x}, {\\bf y}) &amp;= 1 - r_S ({\\bf x}, {\\bf y})\\\\ \\mbox{ or } &amp;= 1 - |r_S ({\\bf x}, {\\bf y})|\\\\ \\mbox{ or } &amp;= 1 - (r_S ({\\bf x}, {\\bf y}))^2\\\\ \\end{align}\\] Shortcomings: \\(d_S\\) also does not satisfy the triangle inequality. \\(d_S\\) loses information about the shape of the relationship. Strengths: Is resistant to outlying values Cosine Distance \\[\\begin{align} d_C({\\bf x}, {\\bf y}) &amp;= \\frac{{\\bf x} \\cdot {\\bf y}}{|| {\\bf x} || ||{\\bf y}||}\\\\ &amp;= \\frac{\\sum_{i=1}^p x_i y_i}{\\sqrt{\\sum_{i=1}^p x_i^2 \\sum_{i=1}^p y_i^2}}\\\\ &amp;= 1 - r_P ({\\bf x}, {\\bf y}) \\ \\ \\ \\ \\mbox{if } \\overline{\\bf x} = \\overline{\\bf y} = 0 \\end{align}\\] Said differently, \\[\\begin{align} d_P({\\bf x}, {\\bf y}) = d_C({\\bf x} - \\overline{\\bf x}, {\\bf y} - \\overline{\\bf y}) \\end{align}\\] Haversine Distance Haversine distance is the great-circle distance (i.e., the distance between two points on a sphere) which is used to measure distance between two locations on the Earth. Let \\(R\\) be the radius of the Earth, and (lat1,long1) and (lat2, long2) be the two locations between which to calculate a distance. \\[d_{HV} = 2 R \\arcsin \\sqrt{\\sin^2 \\bigg( \\frac{lat2-lat1}{2} \\bigg) + \\cos(lat1) \\cos(lat2) \\sin^2 \\bigg(\\frac{long2 - long1}{2} \\bigg)} \\] Shortcomings: Earth is not a perfect sphere Depending on how the distance is used, typically getting from one point to the next is not done by the shortest distance Strengths: Allows calculations, for example, between two cities. Hamming Distance Hamming distance is the number of coordinates across two vectors whose values differ. If the vectors are binary, the Hamming distance is equivalent to the \\(L_1\\) norm of the difference. (Hamming distance does satisfy the properties of a distance metric.) Some methods, equivalently, calculate the proportion of coordinates that differ. \\[\\begin{align} d_H({\\bf x}, {\\bf y}) = \\sum_{i=1}^p I(x_i \\ne y_i) \\end{align}\\] Figure 1.4: The Hamming distance across the two DNA strands is 7. Shortcomings: Can’t measure degree of difference between categorical variables. Strengths: It is a distance metric. Gives a more direct “distance” between categorical variables. Figure 1.5: The function dist in R calculates the distances given above. 9.4 Hierarchical Clustering Hierarchical Clustering is a set of nested clusters that are organized as a tree. Note that objects that belong to a child cluster also belong to the parent cluster. Example: Consider the following images / data (from Laura Hoopes, personal communication; Molecular characterisation of soft tissue tumours: a gene expression study by Nielsen et al., The Lancet 2002). The first represents a microarray sample from aging yeast. The second is a set of 41 samples of soft-tissue tumors (columns) and a subset of 5520 genes (rows) used to characterize their molecular signatures. Note: the ordering of the variables (or samples) does not affect the clustering of the samples (or variables). That is: we can clustering the variables / samples either sequentially or in parallel to see trends in both relationships simultaneously. Clustering both the observations and the variables is called biclustering. Algorithm: Agglomerative Hierarchical Clustering Algorithm Begin with \\(n\\) observations and a measure (such as Euclidean distance) of all the \\({n \\choose 2} = n(n-1)/2\\) pairwise dissimilarities. Treat each observation as its own cluster. For \\(i = n, n - 1, \\ldots , 2\\): Examine all pairwise inter-cluster dissimilarities among the \\(i\\) clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed. Compute the new pairwise inter-cluster dissimilarities among the \\(i - 1\\) remaining clusters. Agglomerative methods start with each object (e.g., gene) in its own group. Groups are merged until all objects are together in one group. Divisive methods start with all objects in one group and break up the groups sequentially until all objects are individuals. Single Linkage algorithm defines the distance between groups as that of the closest pair of individuals. Complete Linkage algorithm defines the distance between groups as that of the farthest pair of individuals. Average Linkage algorithm defines the distance between groups as the average of the distances between all pairs of individuals across the groups. Toy Example of Single Linkage Agglomerative Hierarchical Clustering A B C D E A 0 B 0.2 0 C 0.6 0.5 0 D 1 0.9 0.4 0 E 0.9 0.8 5 0.3 0 Link A and B! \\[\\begin{align} d_{(AB)C} &amp;= \\min(d_{AC}, d_{BC}) = 0.5\\\\ d_{(AB)D} &amp;= \\min(d_{AD}, d_{BD}) = 0.9\\\\ d_{(AB)E} &amp;= \\min(d_{AE}, d_{BE}) = 0.8\\\\ \\end{align}\\] AB C D E AB 0 C 0.5 0 D 0.9 0.4 0 E 0.8 0.5 0.3 0 Link D and E! \\[\\begin{align} d_{(AB)C} &amp;= 0.5\\\\ d_{(AB)(DE)} &amp;= \\min(d_{AD}, d_{BD}, d_{AE}, d_{BE}) = 0.8\\\\ d_{(DE)C} &amp;= \\min(d_{CD}, d_{CE}) = 0.4\\\\ \\end{align}\\] AB C DE AB 0 C 0.5 0 DE 0.8 0.4 0 Link C with (DE)! \\[\\begin{align} d_{(AB)(CDE)} = d_{BC} = 0.5 \\end{align}\\] 9.5 Partitioning Clustering Partition Clustering is a division of the set of data objects into \\(K\\) non-overlapping subsets (clusters) with each observation falling into exactly one cluster. In contrast to hierarchical clustering where results were given for any (and all!) number of clusters, partitioning methods typically start with a given \\(k\\) value and a set of distances. The goal is to partition the observations into \\(k\\) groups such that an objective function is optimized. The number of possible partitions is roughly to \\(n^k / k!\\) (note: \\(100^{5} / 5! = 83\\) million). [The exact number can be computed using Sterling numbers.] So instead of looking through all of the partitions, we step through a recursive algorithm. 9.5.1 \\(k\\)-means Clustering A fun applet!! https://www.naftaliharris.com/blog/visualizing-k-means-clustering/ \\(k\\)-means clustering is an unsupervised partitioning algorithm designed to find a partition of the observations such that the following objective function is minimized (find the smallest within cluster sum of squares): \\[\\argmin_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K 2 \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\overline{x}_{kj})^2 \\Bigg\\}\\] As described in the algorithm below, reallocating observations can only improve the minimization criteria with the algorithm stopping when no changes of the observations will lower the objective function. The algorithm leads to a local optimum, with no confirmation that the global minimum has occurred. Often the \\(k\\)- means algorithm is run multiple times with different random starts, and the partition leading to the lowest objective criteria is chosen. Note that the following algorithm is simply one \\(k\\)-means algorithm. Other algorithms could include a different way to set the starting values, a different decision on when to recalculate the centers, what to do with ties, etc. Figure 9.1: From An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani. Algorithm: \\(k\\)-Means Clustering Randomly assign a number, from 1 to \\(k\\), to each of the observations. These serve as initial cluster assignments for the observations. Iterate until the cluster assignments stop changing: For each of the \\(k\\) clusters, compute the cluster centroid. The \\(k^{th}\\) cluster centroid is the vector of the \\(p\\) feature means for the observations in the \\(k^{th}\\) cluster. Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance). Ties? Do something consistent: for example, leave in the current cluster. Why does the \\(k\\)-means algorithm converge / (local) minimize the objective function? If a point is closer to a different center, moving it will lower the objective function. Averages minimize squared differences, so taking the new average will result in a lower objective function. If a point is equidistant from two clusters, the point won’t move. The algorithm must converge in finite number of steps because there are finitely many points. strengths No hierarchical structure / points can move from one cluster to another. Can run for a range of values of \\(k\\). shortcomings \\(k\\) has to be predefined to run the algorithm. \\(k\\)-means is based on Euclidean distance (only). 9.5.2 Partitioning Around Medoids As an alternative to \\(k\\)-means, Kaufman and Rousseeuw developed Partitioning around Medoids (Finding Groups in Data: an introduction to cluster analysis, 1990). The particular strength of PAM is that it allows for any dissimilarity metric. That is, a dissimilarity based on correlations is no problem, but the algorithm gets more complicated because the “center” is no longer defined in Euclidean terms. The two main steps are to Build (akin to assigning points to clusters) and to Swap (akin to redefining cluster centers). The objective function the algorithm tries to minimize is the average dissimilarity of objects to the closest representative object. (The PAM algorithm is a good (not necessarily global optimum) solution to minimizing the objective function.) \\[\\argmin_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k}D_i \\Bigg\\} = \\argmin_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{i \\in C_k}d(x_i, m_k) \\Bigg\\}\\] Where \\(D_i\\) represents this distance from observation \\(i\\) to the closest medoid, \\(m_k\\). strengths No hierarchical structure / points can move from one cluster to another. Can run for a range of values of \\(k\\). It can use any distance measure shortcomings \\(k\\) has to be predefined to run the algorithm. Algorithm: Partitioning Around Medoids (From The Elements of Statistical Learning (2001), by Hastie, Tibshirani, and Friedman, pg 469.) Randomly assign a number, from 1 to \\(K\\), to each of the observations. These serve as initial cluster assignments for the observations. Iterate until the cluster assignments stop changing: (Repeat for \\(k \\in \\{1, 2, ...K\\}\\)) For a given cluster, \\(C_k\\), find the observation in the cluster minimizing total distance to other points in that cluster: \\[i^*_k = \\argmin_{i \\in C_k} \\sum_{i&#39; \\in C_k} d(x_i, x_{i&#39;})\\] Then \\(m_k = x_{i^*_k}, k=1, 2, \\ldots, K\\) are the current estimates of the cluster centers. Given a current set of cluster centers \\(\\{m_1, m_2, \\ldots, m_K\\}\\), minimize the total error by assigning each observation to the closest (current) cluster center: \\[C_i = \\argmin_{1 \\leq k \\leq K} d(x_i, m_k)\\] 9.6 Evaluation Metrics Silhouette Width Consider observation \\(i \\in\\) cluster \\(Clus1\\). Let \\[\\begin{align} d(i, Clus2) &amp;= \\mbox{average dissimilarity of } i \\mbox{ to all objects in cluster } Clus2\\\\ a(i) &amp;= \\mbox{average dissimilarity of } i \\mbox{ to all objects in } Clus1.\\\\ b(i) &amp;= \\min_{Clus2 \\ne Clus1} d(i,Clus2) = \\mbox{distance to the next closest neighbor cluster}\\\\ \\mbox{ silhouette width} &amp;= s(i) = \\frac{b(i) - a(i)}{\\max \\{ a(i), b(i) \\}}\\\\ &amp; &amp;\\\\ \\mbox{average}_{i \\in Clus1} s(i) &amp;= \\mbox{average silhouette width for cluster $Clus1$} \\end{align}\\] Note that if \\(a(i) &lt; b(i)\\) then \\(i\\) is well classified with a maximum \\(s(i) = 1\\). If \\(a(i) &gt; b(i)\\) then \\(i\\) is not well classified with a maximum \\(s(i) = -1\\). Diameter of cluster \\(Clus1\\) (within cluster measure) \\[\\begin{align} \\mbox{diameter} = \\max_{i,j \\in Clus1} d(i,j) \\end{align}\\] Separation of cluster \\(Clus1\\) (between cluster measure) \\[\\begin{align} \\mbox{separation} = \\min_{i \\in Clus1, j \\notin Clus1} d(i,j) \\end{align}\\] \\(L^*\\): a cluster with diameter \\(&lt;\\) separation; \\(L\\): a cluster with \\(\\max_{j \\in Clus1} d(i,j) &lt; \\min_{k \\notin Clus1} d(i,k)\\). PAM example Building the clusters A B C D E A 0 B 0.2 0 C 0.6 0.5 0 D 1 0.9 0.4 0 E 0.9 0.8 5 0.3 0 Start by considering the random allocation (AC) and (BDE) As a second step, calculate the within cluster sums of distances: A: 0.6 C: 0.6 B: 0.9 + 0.8 = 1.7 D: 0.9 + 0.3 = 1.2 E: 0.8 + 0.3 = 1.1 For cluster 1, it doesn’t matter if we choose A or C (let’s choose C). For cluster 2, we should choose E (it is the most “central” as measured by its closer distance to both B and D). Reallocate points: Cluster1: C and all the points that are closer to C than E. A and B are both closer to C than to E. Cluster1 will be (A,B,C). Cluster2: E and all the points that are closer to E than C. Only D is closer to E than C. Cluster2 will be (D,E) Redefine cluster centers: A: 0.2 + 0.6 = 0.8 B: 0.2 + 0.5 = 0.7 C: 0.6 + 0.5 = 1.1 D: 0.3 E: 0.3 Cluster1 now has a medoid of B. Cluster2 (we choose randomly) has a medoid of D. Reallocate points: Cluster1: B and A (A,B) Cluster2: D and C, E (D, C, E) The medoids are now A or B (randomly choose) and D. The iteration process has converged. Evaluating the clusters A B C D A 0 B 0.2 0 C 0.6 0.5 0 D 1 0.9 0.4 0 (Note: this is the same matrix as before, but with only 4 observations.) Consider the data with (AB)(CD) as the clusters, we can calculate the previous metrics: Silhouette Width \\[\\begin{align} s(i=A) = \\frac{b(A) - a(A)}{\\max \\{a(A), b(A)\\}} = \\frac{0.8 - 0.2}{0.8} = 0.75\\\\ s(i=B) = \\frac{b(B) - a(B)}{\\max \\{a(B), b(B)\\}} = \\frac{0.7 - 0.2}{0.7} = 0.71\\\\ s(i=C) = \\frac{b(C) - a(C)}{\\max \\{a(C), b(C)\\}} = \\frac{0.55 - 0.4}{0.55} = .27\\\\ s(i=D) = \\frac{b(D) - a(D)}{\\max \\{a(D), b(D)\\}} = \\frac{0.95 - 0.4}{0.95} = .57\\\\ \\mbox{Ave SW} = 0.575\\\\ \\end{align}\\] Diameter \\[\\begin{align} \\mbox{diameter}(AB) = 0.2\\\\ \\mbox{diameter}(CD) = 0.4\\\\ \\end{align}\\] Separation \\[\\begin{align} \\mbox{separation}(AB) = \\mbox{separation}(CD) = 0.5\\\\ \\end{align}\\] Rand Index / Adjusted Rand Index is based on a confusion matrix comparing either a known truth (labels) or comparing two different clusterings (e.g., comparing \\(k\\)-means and hierarchical clustering). Let the two different clusterings be called partition1 and partition2. * a is the number of pairs of observations put together in both partition1 and partition2 * b is the number of pairs of observations together in partition1 and apart in partition2 * c is the number of pairs of observations together in partition2 and apart in partition1 * d is the number of pairs of observations apart in both partitions \\[\\mbox{Rand index} = \\frac{a+d}{a+b+c+d}\\] The cool thing about the Rand index is that the partitions don’t have to even have the same number of clusters. They can be absolutely any two clusterings (one might be known labels, for example). Details on the Adjusted Rand index are given at http://faculty.washington.edu/kayee/pca/supp.pdf (basic idea is to center and scale the Rand index so that the values are more meaningful). 9.7 EM algorithm The EM algorithm is an incredibly useful tool for solving complicated maximization procedures, particularly with respect to maximizing likelihoods (typically for parameter estimation). We will describe the procedure here in the context of estimating the parameters of a two-component mixture model. Consider the Old Faithful geyser Yellowstone National Park, Wyoming, USA with the following histogram of data on waiting times between each eruption: \\[\\begin{align} Y_1 &amp;\\sim N(\\mu_1, \\sigma_1^2)\\\\ Y_2 &amp;\\sim N(\\mu_2, \\sigma_2^2)\\\\ Y &amp;= (1-\\Delta) Y_1 + \\Delta Y_2\\\\ P(\\Delta=1) &amp;= \\pi\\\\ \\end{align}\\] In the simple two component case, we can see that the representation above indicates that first we generate a \\(\\Delta \\in \\{0,1\\}\\), and then, depending on the result, we generate either \\(Y_1\\) or \\(Y_2\\). The likelihood associated with the above setting is: \\[\\begin{align} g_Y(y) = (1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y) \\end{align}\\] where \\(\\phi_\\theta\\) represents the normal distribution with a vector \\(\\theta=(\\mu, \\sigma)\\) of parameters. Typically, in statistical theory, to find \\(\\theta\\), we would take the derivative of the log-likelihood to find the values which maximize. Here, however, the likelihood is too complicated to solve for \\(\\theta\\) in closed form. \\[\\begin{align} l(\\theta; {\\bf y}) = \\sum_{i=1}^N \\log [(1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)]. \\end{align}\\] If we know which point comes from which distribution, however, the maximization is straightforward in that we can use the points in group one to estimate the parameters from the first distribution, and the points in group two to estimate the parameters in the second distribution. The process of assigning points and estimating parameters can be thought of as two steps: Expectation: an assignment (soft here, because the points are weighted) of each observation to a group. Maximization: update the parameter estimates. Algorithm: EM Algorithm for two-component Gaussian mixture. From The Elements of Statistical Learning (2001), by Hastie, Tibshirani, and Friedman, pg 238. Take initial guesses for the parameters \\(\\hat{\\mu}_1, \\hat{\\sigma}_1^2, \\hat{\\mu}_2, \\hat{\\sigma}_2^2, \\hat{\\pi}\\). Expectation Step: compute the responsibilities: \\[ \\hat{\\gamma}_i = \\frac{\\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}{(1-\\hat{\\pi}) \\phi_{\\hat{\\theta}_1} (y_i) + \\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}, i=1, 2, \\ldots, N.\\] Maximization Step: compute the weighted means and variances: \\[\\begin{align} \\hat{\\mu}_1 = \\frac{\\sum_{i=1}^N (1-\\hat{\\gamma_i})y_i}{\\sum_{i=1}^N (1-\\hat{\\gamma_i})} &amp;&amp; \\hat{\\sigma}_1^2 = \\frac{\\sum_{i=1}^N (1-\\hat{\\gamma_i})(y_i - \\hat{\\mu}_1)^2}{\\sum_{i=1}^N (1-\\hat{\\gamma_i})}\\\\ \\hat{\\mu}_2 = \\frac{\\sum_{i=1}^N \\hat{\\gamma_i}y_i}{\\sum_{i=1}^N \\hat{\\gamma_i}} &amp;&amp; \\hat{\\sigma}_2^2 = \\frac{\\sum_{i=1}^N \\hat{\\gamma_i}(y_i - \\hat{\\mu}_2)^2}{\\sum_{i=1}^N \\hat{\\gamma_i}} \\end{align}\\] and the mixing probability \\(\\hat{\\pi} = \\sum_{i=1}^N \\hat{\\gamma}_i / N\\). Iterate Steps 2. and 3. until convergence. The algorithm shows that for a particular allocation of the points, we can maximize the given likelihood to estimate the parameter values (done in the Maximization Step). However, it is not obvious from the algorithm that the first allocation step leads to a maximization (local or global) of the likelihood. The proof of the EM algorithm converging to a local maximum likelihood (it does not necessarily converge to a global max) uses information on the marginal prior and posterior likelihoods of the parameter values and Jensen’s inequality to show that the likelihood does not decrease through the iterative steps. Note that in the previous \\(k\\)-means algorithm we iterated between two steps of assigning points to clusters and estimating the cluster centers (we thought of the space as scaled so that the Euclidean distance was appropriate in all dimensions). Two differences in the algorithms we covered are: \\(k\\)-means uses hard thresholding and EM uses soft thresholding \\(k\\)-means uses a fixed standard deviation of 1, EM allows the data/algorithm to find the standard deviation Indeed, although the EM-algorithm above is slightly different than the previous \\(k\\)-means algorithm, the two methods typically converge to the same result and are both considered to be different implementations of a \\(k\\)-means algorithm. See the following applet for a visual representation of how the EM-algorithm converges: http://www.socr.ucla.edu/applets.dir/mixtureem.html. "],
["misc.html", "Chapter 10 Misc 10.1 11/26/19 Agenda 10.2 API 10.3 Parallel Computing 10.4 Cloud Computing 10.5 reticulate 10.6 SQL (in R) 10.7 12/10/19 Agenda 10.8 Regular Expressions", " Chapter 10 Misc 10.1 11/26/19 Agenda API / authenticating parallel computing cloud computing reticulate (Python in R!) SQL 10.2 API Figure 1.2: xkcd, https://xkcd.com/1481/ What is an API? (Application Programming Interface) Think of an API as a restaurant menu. The menu provides a list of what the restaurant has to offer, and you order off the menu by choosing the dish that you want. After you order, the restaurant figures out how to bring the food from the kitchen to your table in the way that you’ve specified. An API is an intermediary that allows two applications to talk to one another. It is not the database or the server, instead it is the code that allows communication. Examples of APIs When you use an app on your phone, the app connects to the internet and sends information to a server somewhere. The server retrieves the data, interprets is, does what it does, and sends it back to you. The application which takes the data from the server and presents it to you in a readable way is an API. Let’s say you are booking a flight on United. You choose all the details, you interact with the airline’s website. BUT INSTEAD, what if you are interacting with a software like Expedia? Then Expedia has to talk to United’s API to get all the information about available flights, costs, seats, etc. If you’ve ever been to a third party site and clicked on “Share on Facebook” or “Share on Twitter” your third party site is communicating with the Facebook API or the Twitter API. You sign up to go to a concert, and StubHub asks whether you want to add the concert to your Google calendar. StubHub needs to talk to Google via Google’s API. What if you want some Twitter data? How might you get it? Well, you could email Twitter and ask someone for it. Instead Twitter provides information about how their data is stored, and allows you to query their data in an automated way. Figure 1.3: Image taken from https://rigor.com/blog/what-is-an-api-a-brief-intro 10.2.1 Authenticating Authenticating is stating who you are Authorization is asking for access to a resources (and happens after authentication) DO NOT post your credentials and keys to a public GitHub repo!! In almost all cases, in order to communicate with an API, you must tell the API who you are and that you should have access to the information the API is providing. Figure 1.4: Image taken from https://blog.restcase.com/restful-api-authentication-basics/ 10.3 Parallel Computing (Taken from the Teach Data Science blog: https://teachdatascience.com/parallel/) To demonstrate what parallel computing is, we’ll perform tasks that are embarrassingly parallel which means there is no dependency or communication between the parallel tasks. Again, parallel computing can be powerful in ways that link computational tasks in complicated ways. But we believe that as a first pass at teaching parallel computing, we should teach the parallel structure before bringing in dependence across the parallel tasks. Examples of embarrassingly parallel algorithms include: Monte Carlo analysis, bootstrapping, growing trees for Random Forests, group_by analyses, and cross-validation. Additionally, data science methods increasingly use randomized algorithms which can often be written in parallel. Indeed, it isn’t always easy to know when to use a parallel construction. Because of existing overhead processes (e.g., copying data across many threads, bring results together, etc.) an algorithm run on 10 parallel strands will not reduce an original (non-parallel) run time by 10-fold. Figuring out when a parallel implementation is appropriate is beyond the scope of this blog but should be carefully considered before embarking on large projects. Some parallel examples Before running code in parallel, it is valuable to know how many cores your computer has to work with. Note that the detectCores function will provide information about the specific device you are using (logical = FALSE tells you only the physical cores which is likely what you want). Note that after makeCluster the separate threads have information. After stopCluster, the code is no longer connecting to the cluster structure. library(parallel) P &lt;- detectCores(logical=FALSE) P ## [1] 8 cl &lt;- makeCluster(P) cl[[1]] ## node of a socket cluster on host &#39;localhost&#39; with pid 6567 stopCluster(cl) cl[[1]] ## Error in summary.connection(connection): invalid connection Embarrassingly embarrassing example In the example below, we generate some Cauchy data and find the max of each sample. Note that for the current device there are 8 cores, so the process will happen 100/P = 12.5 times on each core. The second argument of clusterApply is a sequence of numbers that gets passed to each worker as the (first) argument of func1. Below, I’ve specified that the value 50 (number of reps) should be passed separately to 100 different workers. W &lt;- 100 P &lt;- detectCores(logical=FALSE) cl &lt;- makeCluster(P) func1 &lt;- function(reps){ max(rcauchy(reps)) } clusterApply(cl, rep(50,W), fun = func1) %&gt;% head(3) ## [[1]] ## [1] 12.83799 ## ## [[2]] ## [1] 7.578554 ## ## [[3]] ## [1] 8.749791 stopCluster(cl) There are many R functions which implement parallel processing. For example, the same code from above can be processed using foreach. library(doParallel) cl &lt;- parallel::makeCluster(P) doParallel::registerDoParallel(cl) foreach(reps = rep(50, 100), .combine = &#39;c&#39;) %dopar% { max(rcauchy(reps)) } %&gt;% head(3) ## [1] 230.797839 53.562465 8.958124 stopCluster(cl) Example bootstrapping A slightly less embarrassingly parallel example comes with bootstrapping. Below we have used parallel implementation to bootstrap the mean of the iris data petal length (Virginica only). data(iris) iris_bs &lt;- iris %&gt;% filter(Species == &quot;virginica&quot;) %&gt;% select(Petal.Length) cl &lt;- parallel::makeCluster(P) doParallel::registerDoParallel(cl) bsmean_PL &lt;- foreach(i = 1:100, .combine = &#39;c&#39;) %dopar% { mean(sample(iris_bs$Petal.Length, replace = TRUE)) } bootstrap &lt;- tibble(bsmean_PL) stopCluster(cl) ggplot(bootstrap, aes(x = bsmean_PL)) + geom_histogram(bins = 25) + ggtitle(&quot;Histogram of 100 Bootstrapped Means using foreach&quot;) 10.3.1 Spark and sparklyr Some of you may be familiar with Apache Spark which is an open-source product for distributed cluster-computing. You may want to learn more about its capabilities, including scheduling workflow, dispatching tasks, and consolidating end results. While incredibly powerful, there has historically been a steep learning curve to getting R to work smoothly with a Spark connection. Recently, RStudio has come out with a new package sparklyr which integrates R and Spark seamlessly. Note that in the example below, we’ve set up a local connection just for the purposes of the example. For your work, you may want to connect to a cluster or cloud space with many cores. The RStudio sparklyr webpage provides a plethora of good examples demonstrating the sophistication and power of the technology. sparklyr has particularly strong connections to the suite of tidyverse functions. Indeed, the power of sparklyr is more about distributing the computing than about parallelizing it. For example, with sparklyr the computations are delayed until you need the results. Additionally, Spark is doing the heavy lifting and only at the very end (when your results are called) do you need to worry about the size of the table, results, or computational space. The example below repeats the bootstrapping work that was done previously. Note, it is important to look at your data structures and variables names. For example, when copying the local dataframe iris_samps to the remote data source called iris_samps_tbl, the variable Petal.Length was changed to Petal_Length. library(sparklyr) spark_install() sc &lt;- spark_connect(master = &quot;local&quot;) n_sim = 100 iris_samps &lt;- iris %&gt;% dplyr::filter(Species == &quot;virginica&quot;) %&gt;% sapply(rep.int, times=n_sim) %&gt;% cbind(replicate = rep(1:n_sim, each = 50)) %&gt;% data.frame() %&gt;% dplyr::group_by(replicate) %&gt;% dplyr::sample_n(50, replace = TRUE) iris_samps_tbl &lt;- copy_to(sc, iris_samps) iris_samps_tbl %&gt;% spark_apply(function(x) {mean(x$Petal_Length)}, group_by = &quot;replicate&quot;) %&gt;% ggplot(aes(x = result)) + geom_histogram(bins = 20) + ggtitle(&quot;Histogram of 100 Bootstrapped Means using sparklyr&quot;) spark_disconnect(sc) For our particular application, the adept reader has probably noticed that the average of a variable using group_by is a very quick and easy task for dplyr. Indeed, the use of sparklyr above is overkill and is presented only as a way to demonstrate using sparklyr. If you are working with big datasets that require large computing infrastructure, the RStudio help pages on sparklyr are fantastic. Additionally, there are many instances of working with Spark in the wild, and you might consider working through someone else’s Spark analysis like this fantastic example on splitting up large amounts of raw DNA sequencing to get data for a given genetic location. iris_samps %&gt;% dplyr::group_by(replicate) %&gt;% dplyr::summarize(result = mean(Petal.Length)) %&gt;% ggplot(aes(x = result)) + geom_histogram(bins = 25) + ggtitle(&quot;Histogram of 100 Bootstrapped Means using dplyr&quot;) While an introduction to parallel and cloud computing will help you become more adept and less apprehensive about using the tools, there is also a recognition that sufficient background in computer science is needed to be able to fully engage with principles of high performance computing. Learn more Hana Sevcikova Introduction to parallel computing with R useR 2017 in Brussels, tutorial here sparklyr to do parallel cross-validation https://www.rstudio.com/resources/cheatsheets/ https://www.rstudio.com/resources/cheatsheets/#sparklyr https://github.com/rstudio/cheatsheets/raw/master/parallel_computation.pdf Great blog Two Flavors of Parallel Simulation by Mark LeBoeuf comparing different ways to process code in parallel. 10.4 Cloud Computing A great overview on high performance computing (HPC) what it is and what it isn’t is given here: https://www.slideshare.net/raamana/high-performance-computing-with-checklist-and-tips-optimal-cluster-usage Figure 1.8: Image from Pradeep Redddy Raamana High performance computing tutorial, with checklist and tips to optimize cluster usage (The rest, below, is taken from the Teach Data Science blog: https://teachdatascience.com/cloud2/, this entry written by Nick Horton) The R package parallel is designed to send tasks to each of multiple cores. Today’s computers (even small laptops!) typically have multiple cores, and any server or cloud computing infrastructure can easily handle dozens or hundreds of parallel tasks. The structure of the R parallel implementation sends tasks to workers that don’t talk to one another until compiling their results at the end. In her 2017 UseR! tutorial, Hana Sevcikova describes the function of workers which run code/functions/iterations separately before results are subsequently combined. Figure 1.9: Image from Sevcikova UseR! 2017 tutorial on parallel computing As computing infrastructure becomes more sophisticated, it is important to have the language to describe how connected components work. Parallel processing allows for a conversation on the differences between distributed computing, cluster computing, and grid computing, and generally, the framework of high performance computing. The benefit of parallel computing as an introduction to the larger infrastructure is that the task of each worker is clear, important, and easy to describe. This discussion is motivated by several recent papers and blog posts that describe how complex, real-world data science computation can be structured in ways that would not have been feasible in past years without herculean efforts. It is worth noting the fantastic example that described multiple iterations needed to parse huge amounts of raw DNA sequencing data to undertake analyses for a given set of genetic locations. In “Ambitious data science can be painless” Monajemi et al. describe workflows that take advantage of new software stacks to undertake massive cloud-based experiments. While a few years older, Chamandy et al.’s Teaching statistics at Google scale described three examples where modern data challenges were overcome with creative statistical thinking (see companion report on Estimating uncertainty for massive data streams ). Finally, the NSF-funded workshop report on “Enabling computer and information science and engineering research and education in the cloud” highlights opportunities as university computing migrates to cloud solutions more and more. And last, you may enjoy reading the recent Three strategies for working with Big Data in R blog post. How can we prepare for cloud computing in an undergraduate course? Getting started What are the steps to exploring cloud-based systems? Each of the main cloud providers have active educational outreach programs. Google Compute Platform allows faculty to apply to receive $100 in GCP credits and $50 per student. Credits can be used in courses, student clubs, and other faculty-sponsored events. (To replicate our example later in this blog, you’ll want to set up an account and request credits.) Azure for Education provides access for educators to open source content for classes and $200 in Azure credits, plus free services. Amazon Web Services Educate provides between $75 and $200 in AWS annual credits per educator (depending on membership status) and between $30 and $100 for students. You should sign up and start to explore! The world of cloud computing is quickly changing. By gaining experience through investment in time in learning these tools will help instructors provide guidance to their students in use of these modern computational tools. An example: BigQuery in Google’s GCP Consider an example using GCP (kudos to Shukry Zablah for his assistance). BigQuery is Google’s serverless, highly-scalable, cloud data warehouse. A quickstart document is available which discusses use of the web user interface and the GCP console as well as access through an API interface. The bigrquery package in R makes it easy to work with data stored in Google BigQuery through queries to BigQuery tables. The first step is to request GCP credits (see above) and use the online interface to create a project (below called “Test Project for Blog”). library(dplyr) library(bigrquery) library(ggplot2) library(forcats) library(purrr) library(readr) projectId &lt;- &quot;bigquery-public-data&quot; # replace with your own project billingId &lt;- &quot;test-project-for-blog&quot; # replace with your own billing ID datasetName &lt;- &quot;samples&quot; tableName &lt;- &quot;wikipedia&quot; BigQuery includes a number of public datasets. Below is an analysis of the public dataset of the revisions of Wikipedia articles up to April 2010, hosted in GCP BigQuery. The size of the table is 35.69GB. The queries take only seconds to run. query &lt;- &quot;SELECT title, COUNT(title) as n FROM `bigquery-public-data.samples.wikipedia` GROUP BY title ORDER BY n DESC LIMIT 500&quot; For safety, always try to make sure that your queries have the LIMIT set on your queries. mostRevisions_tb &lt;- bigrquery::bq_project_query(x = billingId, query = query) #creates temporary table When the previous bq_project_query() function is run within RStudio, a connection is made to Google (GCP) and an authentication window will open up in a local browser. All the heavy lifting we perform is done on the database end (note that we are billed for it, though the first 1TB of accesses are free). The local machine only receives the data once we try to display it. Right now mostRevisions_tb is just a reference to a temporary table online. The query accessed 7GB of data. We can get a copy of the data on our local machine once we are confident that it is what we want. mostRevisions &lt;- bq_table_download(mostRevisions_tb) glimpse(mostRevisions) ## Observations: 500 ## Variables: 2 ## $ title &lt;chr&gt; &quot;Wikipedia:Administrator intervention against vandalism&quot;, … ## $ n &lt;int&gt; 643271, 419695, 326337, 257893, 226802, 204469, 191679, 18… clean &lt;- mostRevisions %&gt;% filter(!grepl(&quot;Wikipedia|User|Template|Talk&quot;, title)) %&gt;% mutate(title = fct_reorder(title, n)) %&gt;% #to sort levels glimpse() ## Observations: 272 ## Variables: 2 ## $ title &lt;fct&gt; George W. Bush, List of World Wrestling Entertainment empl… ## $ n &lt;int&gt; 43652, 30572, 27433, 23245, 21768, 20814, 20546, 20529, 20… Let’s plot the top 10 entries. ggplot(clean %&gt;% head(10), aes(x = title, y = n, fill = n)) + geom_bar(stat = &quot;identity&quot;) + labs(x = &quot;Article Title&quot;, y = &quot;Number of Revisions&quot;, title = &quot;Most Revised Wikipedia Articles (Up to April 2010)&quot;) + scale_fill_gradient(low = &quot;darkblue&quot;, high = &quot;darkred&quot;, guide = FALSE) + theme_minimal() + theme(axis.text.x = element_text(angle = 20, hjust = 1)) We’ve obviously just scratched the surface here. There are lots of other examples out there to consider replicating in your classroom (e.g., returning tweets on a schedule). Hopefully you are intrigued enough to request some credits for you and your students and start to explore. Not sure where to begin? Check out the GCP Essentials Videos series. 10.5 reticulate (Taken from the Teach Data Science blog: https://teachdatascience.com/reticulate/) Connect to Python within RStudio For many statisticians, the go-to software language is R. However, there is no doubt that Python is a very important language in data science. Why not do both?? library(tidyverse) library(reticulate) use_virtualenv(&quot;r-reticulate&quot;) reticulate::import(&quot;statsmodels&quot;) ## Module(statsmodels) I can run Python inside R?? pandas for data wrangling. In R, the chunk is specified to be a Python chunk (RStudio is now running Python). ```{python} import pandas flights = pandas.read_csv(&quot;flights.csv&quot;) flights = flights[flights[&quot;dest&quot;] == &quot;ORD&quot;] flights = flights[[&#39;carrier&#39;, &#39;dep_delay&#39;, &#39;arr_delay&#39;]] flights = flights.dropna() ``` A view of the Python chunk which is actually run: import pandas flights = pandas.read_csv(&quot;flights.csv&quot;) flights = flights[flights[&quot;dest&quot;] == &quot;ORD&quot;] flights = flights[[&#39;carrier&#39;, &#39;dep_delay&#39;, &#39;arr_delay&#39;]] flights = flights.dropna() Learn about the dataset ```{python} flights.shape flights.head(3) flights.describe() ``` flights.shape ## (12590, 3) flights.head(3) ## carrier dep_delay arr_delay ## 4 UA -4.0 12.0 ## 5 AA -2.0 8.0 ## 22 AA -1.0 14.0 flights.describe() ## dep_delay arr_delay ## count 12590.000000 12590.000000 ## mean 11.709770 2.917951 ## std 39.409704 44.885155 ## min -20.000000 -62.000000 ## 25% -6.000000 -22.000000 ## 50% -2.000000 -10.000000 ## 75% 9.000000 10.000000 ## max 466.000000 448.000000 Computations using pandas ```{python} flights = pandas.read_csv(&quot;flights.csv&quot;) flights = flights[[&#39;carrier&#39;, &#39;dep_delay&#39;, &#39;arr_delay&#39;]] flights.groupby(&quot;carrier&quot;).mean() ``` flights = pandas.read_csv(&quot;flights.csv&quot;) flights = flights[[&#39;carrier&#39;, &#39;dep_delay&#39;, &#39;arr_delay&#39;]] flights.groupby(&quot;carrier&quot;).mean() ## dep_delay arr_delay ## carrier ## AA 8.586016 0.364291 ## AS 5.804775 -9.930889 ## DL 9.264505 1.644341 ## UA 12.106073 3.558011 ## US 3.782418 2.129595 From Python chunk to R chunk py$x accesses an x variable created within Python from R r.x accesses an x variable created within R from Python library(ggplot2) ggplot(py$flights, aes(x=carrier, y=arr_delay)) + geom_point() + geom_jitter() From R chunk to Python chunk data(diamonds) head(diamonds) ## # A tibble: 6 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.290 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 Python chunks Note that we’re calling Python code on an R object. print(r.diamonds.describe()) ## carat depth ... y z ## count 53940.000000 53940.000000 ... 53940.000000 53940.000000 ## mean 0.797940 61.749405 ... 5.734526 3.538734 ## std 0.474011 1.432621 ... 1.142135 0.705699 ## min 0.200000 43.000000 ... 0.000000 0.000000 ## 25% 0.400000 61.000000 ... 4.720000 2.910000 ## 50% 0.700000 61.800000 ... 5.710000 3.530000 ## 75% 1.040000 62.500000 ... 6.540000 4.040000 ## max 5.010000 79.000000 ... 58.900000 31.800000 ## ## [8 rows x 7 columns] import statsmodels.formula.api as smf model = smf.ols(&#39;price ~ carat&#39;, data = r.diamonds).fit() print(model.summary()) ## OLS Regression Results ## ============================================================================== ## Dep. Variable: price R-squared: 0.849 ## Model: OLS Adj. R-squared: 0.849 ## Method: Least Squares F-statistic: 3.041e+05 ## Date: Mon, 02 Dec 2019 Prob (F-statistic): 0.00 ## Time: 05:34:26 Log-Likelihood: -4.7273e+05 ## No. Observations: 53940 AIC: 9.455e+05 ## Df Residuals: 53938 BIC: 9.455e+05 ## Df Model: 1 ## Covariance Type: nonrobust ## ============================================================================== ## coef std err t P&gt;|t| [0.025 0.975] ## ------------------------------------------------------------------------------ ## Intercept -2256.3606 13.055 -172.830 0.000 -2281.949 -2230.772 ## carat 7756.4256 14.067 551.408 0.000 7728.855 7783.996 ## ============================================================================== ## Omnibus: 14025.341 Durbin-Watson: 0.986 ## Prob(Omnibus): 0.000 Jarque-Bera (JB): 153030.525 ## Skew: 0.939 Prob(JB): 0.00 ## Kurtosis: 11.035 Cond. No. 3.65 ## ============================================================================== ## ## Warnings: ## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Running just Python Full disclosure reticulate is not always trivial to set up. Indeed, I’ve had trouble figuring out which Python version is talking to R and where different module versions live. Learn more RStudio R Interface to Python https://rstudio.github.io/reticulate/ RStudio blog on Reticulated Python https://blog.rstudio.com/2018/10/09/rstudio-1-2-preview-reticulated-python 10.6 SQL (in R) Note that there exists an R interface to work with SQL commands from within an R Markdown file. For consistency with the class notes, we’ve continued to use the R Markdown structure to demonstrate the course material. (Taken from the Teach Data Science blog: https://teachdatascience.com/sql/, this entry written by Nick Horton) SQL (pronounced sequel) stands for Structured Query Language; it is a language designed to manage data in a relational database system. We will use a public facing MySQL database containing wideband acoustic immittance (WAI) measures made on normal ears of adults. (The project is funded by the National Institutes of Health, NIDCD, and hosted on a server at Smith College, PI Susan Voss, R15 DC014129-01.) The database was created to enable auditory researchers to share WAI measurements and combine analyses over multiple datasets. We begin by demonstrating how SQL queries can be sent to a database. It is necessary to set up a connection using the dbConnect() function. library(mosaic) library(RMySQL) con &lt;- dbConnect( MySQL(), host = &quot;scidb.smith.edu&quot;, user = &quot;waiuser&quot;, password = &quot;smith_waiDB&quot;, dbname = &quot;wai&quot;) Next a series of SQL queries can be sent to the database using the DBI::dbGetQuery() function: each query returns an R dataframe. class(dbGetQuery(con, &quot;SHOW TABLES&quot;)) ## [1] &quot;data.frame&quot; There are multiple tables within the wai database. dbGetQuery(con, &quot;SHOW TABLES&quot;) ## Tables_in_wai ## 1 Measurements ## 2 PI_Info ## 3 Subject The EXPLAIN command describes the ten field names (variables) in the PI_Info table. dbGetQuery(con, &quot;EXPLAIN PI_Info&quot;) ## Field Type Null Key Default Extra ## 1 Identifier varchar(20) YES &lt;NA&gt; ## 2 PI_Year int(11) YES &lt;NA&gt; ## 3 PI varchar(500) YES &lt;NA&gt; ## 4 Affiliation varchar(500) YES &lt;NA&gt; ## 5 Email varchar(30) YES &lt;NA&gt; ## 6 Title varchar(140) YES &lt;NA&gt; ## 7 Pub varchar(10000) YES &lt;NA&gt; ## 8 Date char(20) YES &lt;NA&gt; ## 9 URL varchar(140) YES &lt;NA&gt; ## 10 PI_Notes text NO &lt;NA&gt; The SELECT statement can be used to select all fields for eight observations in the Measurements table. eightobs &lt;- dbGetQuery(con, &quot;SELECT * FROM Measurements LIMIT 8&quot;) eightobs ## Identifier Sub_Number Session Left_Ear MEP Instrument Freq Absorbance ## 1 Abur_2014 1 1 0 -5 1 210.938 0.0451375 ## 2 Abur_2014 1 1 0 -5 1 234.375 0.0441247 ## 3 Abur_2014 1 1 0 -5 1 257.812 0.0495935 ## 4 Abur_2014 1 1 0 -5 1 281.250 0.0516088 ## 5 Abur_2014 1 1 0 -5 1 304.688 0.0590836 ## 6 Abur_2014 1 1 0 -5 1 328.125 0.0628038 ## 7 Abur_2014 1 1 0 -5 1 351.562 0.0682962 ## 8 Abur_2014 1 1 0 -5 1 375.000 0.0738373 ## Zmag Zang Canal_Area ## 1 110638000 -0.228113 NA ## 2 100482000 -0.230561 NA ## 3 90561100 -0.230213 NA ## 4 83515500 -0.230959 NA ## 5 77476800 -0.229652 NA ## 6 71229100 -0.230026 NA ## 7 66615500 -0.229576 NA ## 8 61996200 -0.229327 NA More interesting and complicated SELECT calls can be used to undertake grouping and aggregation. Here we calculate the sample size for each study dbGetQuery(con, &quot;SELECT Identifier, count(*) AS NUM FROM Measurements GROUP BY Identifier ORDER BY NUM&quot;) ## Identifier NUM ## 1 Sun_2016 2604 ## 2 Shaver_2013 2880 ## 3 Feeney_2017 3162 ## 4 Voss_1994 5120 ## 5 Liu_2008 5520 ## 6 Werner_2010 7935 ## 7 Rosowski_2012 14384 ## 8 Voss_2010 14880 ## 9 Abur_2014 21328 ## 10 Groon_2015 35469 ## 11 Shahnaz_2006 58776 ## 12 Lewis_2015 114716 Accessing a database using dplyr commands Alternatively, a connection can be made to the server by creating a series of dplyr tbl objects. Connecting with familiar dplyr syntax is attractive because, as Hadley Wickham has noted, SQL and R have similar syntax (but sufficiently different to be confusing). The setup process looks similar. db &lt;- src_mysql(dbname = &quot;wai&quot;, host = &quot;scidb.smith.edu&quot;, user = &quot;waiuser&quot;, password=&quot;smith_waiDB&quot;) Measurements &lt;- tbl(db, &quot;Measurements&quot;) class(Measurements) ## [1] &quot;tbl_MySQLConnection&quot; &quot;tbl_dbi&quot; &quot;tbl_sql&quot; ## [4] &quot;tbl_lazy&quot; &quot;tbl&quot; PI_Info &lt;- tbl(db, &quot;PI_Info&quot;) Subject &lt;- tbl(db, &quot;Subject&quot;) We explore the PI_Info table using the collect() function used to force computation on the database (and return the results). One attractive aspect of database systems is that they feature lazy evaluation, where computation is optimized and postponed as long as possible. PI_Info %&gt;% summarise(total = n()) ## # Source: lazy query [?? x 1] ## # Database: mysql 5.5.58-0ubuntu0.14.04.1-log ## # [waiuser@scidb.smith.edu:/wai] ## total ## &lt;dbl&gt; ## 1 12 PI_Info %&gt;% select(-Email) %&gt;% collect() %&gt;% data.frame() ## Identifier PI_Year ## 1 Werner_2010 2010 ## 2 Voss_1994 1994 ## 3 Lewis_2015 2015 ## 4 Voss_2010 2010 ## 5 Sun_2016 2016 ## 6 Shaver_2013 2013 ## 7 Rosowski_2012 2012 ## 8 Liu_2008 2008 ## 9 Abur_2014 2014 ## 10 Feeney_2017 2017 ## 11 Groon_2015 2015 ## 12 Shahnaz_2006 2006 ## PI ## 1 Douglas Keefe ## 2 Susan E. Voss ## 3 James D. Lewis; Stephen Neely ## 4 Susan E. Voss ## 5 Xiao-Ming Sun ## 6 Mark D. Shaver, Xiao-Ming Sun ## 7 John J. Rosowski ## 8 Yi-Wen Liu; Chris A. Sanford; John C. Ellison; Denis F. Fitzpatrick; Michael P. Gorga; Douglas H. Keefe ## 9 Defne Abur; Nicholas J. Horton; Susan E. Voss ## 10 M. Patrick Feeney; Douglas H. Keefe ## 11 Stephen Neely ## 12 Navid Shahnaz; Karin Bork ## Affiliation ## 1 Boys Town National Research Laboratory ## 2 Smith College, formerly ATT Bell Labs ## 3 Boys Town National Research Laboratory ## 4 Smith College ## 5 Wichita State University ## 6 Wichita State University ## 7 Eaton-Peabody Laboratory, Massachusetts Eye and Ear Infirmary, Boston ## 8 Boys Town National Research Hospital ## 9 Smith College ## 10 National Center for Rehabilitative Auditory Research (NCRAR) and Boys Town National Research Hospital ## 11 Boys Town National Research Hospital ## 12 University of British Columbia ## Title ## 1 Ear-Canal Wideband Acoustic Transfer Functions of Adults and Two- to Nine-Month-Old Infants ## 2 Measurement of acoustic impedance and reflectance in the human ear canal ## 3 Non-invasive estimation of middle-ear input impedance and efficiency ## 4 Posture systematically alters ear-canal reflectance and DPOAE properties ## 5 Wideband acoustic immittance: Normative study and test-retest reliability of tympanometric measurements in adults ## 6 Wideband energy reflectance measurements: Effects of negative middle ear pressure and application of a pressure compensation procedure ## 7 Ear-Canal Reflectance, Umbo Velocity, and Tympanometry in Normal-Hearing Adult ## 8 Wideband absorbance tympanometry using pressure sweeps: System development and results on adults with normal hearing ## 9 Intrasubject Variability in Power Reflectance ## 10 Normative wideband reflectance, equivalent admittance at the tympanic membrane, and acoustic stapedius reflex threshold in adults ## 11 Air-Leak Effects on Ear-Canal Acoustic Absorbance ## 12 Wideband Reflectance Norms for Caucasian and Chinese Young Adults ## Pub Date ## 1 Ear and Hearing 9/1/2017 ## 2 Journal of the Acoustical Society of America 02/08/2017 ## 3 Journal of the Acoustical Society of America 10/10/2018 ## 4 Hearing Research 06/05/2018 ## 5 Journal of Speech, Language, and Hearing Research 10/31/2017 ## 6 The Journal of the Acoustical Society of America 10/06/2018 ## 7 Ear and Hearing 11/06/2015 ## 8 The Journal of the Acoustical Society of America 6/26/2018 ## 9 J Am Acad Audiol 08/24/2016 ## 10 Ear and Hearing 06/07/2018 ## 11 Ear and Hearing 06/18/2019 ## 12 Ear and Hearing 08/24/2016 ## URL ## 1 https://www.ncbi.nlm.nih.gov/pubmed/20517155 ## 2 https://asa.scitation.org/doi/abs/10.1121/1.408329 ## 3 https://asa.scitation.org/doi/abs/10.1121/1.4927408 ## 4 https://www.ncbi.nlm.nih.gov/pubmed/20227475 ## 5 https://www.ncbi.nlm.nih.gov/pubmed/27517667 ## 6 https://www.ncbi.nlm.nih.gov/pubmed/23862811 ## 7 http://www.ncbi.nlm.nih.gov/pubmed/21857517 ## 8 https://www.ncbi.nlm.nih.gov/pubmed/19206798 ## 9 https://www.ncbi.nlm.nih.gov/pubmed/25257718 ## 10 https://www.ncbi.nlm.nih.gov/pubmed/28045835 ## 11 https://journals.lww.com/ear-hearing/fulltext/2015/01000/Air_Leak_Effects_on_Ear_Canal_Acoustic_Absorbance.16.aspx ## 12 http://journals.lww.com/ear-hearing/Abstract/2006/12000/Wideband_Reflectance_Norms_for_Caucasian_and.15.aspx ## PI_Notes ## 1 Used an ER-1 earphone and ER-7C microphone. Data provided by Doug Keefe and formatted by Susan Voss with help. Lynne Werner is retired. In Subject Table, Sub_Notes=1 means part of 183 subjects included in the means in paper and Sub_Notes=0 means not part of mean in paper. ## 2 Measurements taken with a system using sysid and the Etymotic ER-2 pressure transducer and ER-7c probe microphone. ## 3 Used acoustically calculated areas for absorbance calculations, included in Measurement Table ## 4 No notes ## 5 No notes ## 6 A research version of Titan (Interacoustics) was used. In this study, a total of five reflectance measurements at ambient pressure were taken per ear (detailed in the article). But, results from only two sessions were reported in this article. Included in this database is the second session (baseline), as the normative data. ## 7 No Notes ## 8 No Notes ## 9 Database includes measurements at Position 1 and Channel B only ## 10 Sub_Notes=Boys Town National Lab are data taken by Keefe; Sub_Notes=NCRAR are data taken by Feeney ## 11 First author is Groon, PI for grant is Steve Neely, data collected on system described in Rasetshwane and Neely (2011) ## 12 Impedance angles not available and set to Null # be careful with collect() when dealing with large tables! Note how the number of rows is unknown (?? at the top of the output above) for the lazy query. Similarly, we can explore the Subjects table. Subject %&gt;% summarise(total = n()) ## # Source: lazy query [?? x 1] ## # Database: mysql 5.5.58-0ubuntu0.14.04.1-log ## # [waiuser@scidb.smith.edu:/wai] ## total ## &lt;dbl&gt; ## 1 640 Subject %&gt;% collect() # be careful with collect() with large tables! ## # A tibble: 640 x 10 ## Identifier Sub_Number Session_Total Age Female Race Ethnicity ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Voss_2010 1 5 20 1 0 0 ## 2 Voss_2010 2 5 39 1 0 0 ## 3 Voss_2010 3 5 18 1 0 0 ## 4 Voss_2010 4 5 19 1 0 0 ## 5 Voss_2010 5 5 21 1 0 0 ## 6 Voss_2010 6 5 21 1 0 0 ## 7 Voss_2010 7 5 21 1 0 0 ## 8 Voss_2010 8 5 42 1 0 0 ## 9 Voss_2010 9 5 38 0 0 0 ## 10 Voss_2010 10 5 20 1 0 0 ## # … with 630 more rows, and 3 more variables: Left_Ear_Status &lt;int&gt;, ## # Right_Ear_Status &lt;int&gt;, Sub_Notes &lt;chr&gt; Let’s explore the Measurements table. Measurements %&gt;% summarise(total = n()) ## # Source: lazy query [?? x 1] ## # Database: mysql 5.5.58-0ubuntu0.14.04.1-log ## # [waiuser@scidb.smith.edu:/wai] ## total ## &lt;dbl&gt; ## 1 286774 There are more than a quarter million observations. In the next step, we will download the data from a given subject for a specific study, in this case a paper by Rosowski et al. (2012) entitled “Ear-canal reflectance, umbo velocity, and tympanometry in normal-hearing adults”. Arbitrarily we choose to collect data from subject number three. onesubj &lt;- Measurements %&gt;% filter(Identifier == &quot;Rosowski_2012&quot;, Sub_Number == 3) %&gt;% collect %&gt;% mutate(SessionNum = as.factor(Session)) head(onesubj) ## # A tibble: 6 x 12 ## Identifier Sub_Number Session Left_Ear MEP Instrument Freq Absorbance ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rosowski_… 3 1 1 NA 1 211. 0.0852 ## 2 Rosowski_… 3 1 1 NA 1 234. 0.0903 ## 3 Rosowski_… 3 1 1 NA 1 258. 0.112 ## 4 Rosowski_… 3 1 1 NA 1 281. 0.103 ## 5 Rosowski_… 3 1 1 NA 1 305. 0.129 ## 6 Rosowski_… 3 1 1 NA 1 328. 0.136 ## # … with 4 more variables: Zmag &lt;dbl&gt;, Zang &lt;dbl&gt;, Canal_Area &lt;dbl&gt;, ## # SessionNum &lt;fct&gt; Finally we can display the results of the measurements as a function of frequency and which ear (left or right) that was used. onesubj &lt;- mutate(onesubj, Ear = ifelse(Left_Ear == 1, &quot;Left&quot;, &quot;Right&quot;)) ggplot(onesubj, aes(x = Freq, y = Absorbance)) + geom_point() + aes(colour = Ear) + scale_x_log10() + labs(title=&quot;Absorbance by ear Rosowski subject 3&quot;) We note that a number of relational database systems exist, including MySQL (illustrated here), PostgreSQL, and SQLite. More information about databases within R can be found in the CRAN Databases with R Task View. Setting up and managing a database is a topic for a different day: here we focused on how SQL can be used within R to access data in a flexible and powerful manner. Learn more https://chance.amstat.org/2015/04/setting-the-stage/ (Setting the stage for data technologies) https://www.w3schools.com/sql/sql_intro.asp (Intro to SQL) http://www.science.smith.edu/wai-database/home/about/ (WAI SQL Database) https://cran.r-project.org/web/views/Databases.html (CRAN Task View on Databases with R) https://db.rstudio.com (RStudio Database resources) https://dbplyr.tidyverse.org/articles/dbplyr.html (dbplyr package) 10.7 12/10/19 Agenda Regular Expressions 10.8 Regular Expressions "],
["references.html", "References", " References "]
]
