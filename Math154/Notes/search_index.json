[
["index.html", "Computational Statistics Class Information", " Computational Statistics Jo Hardin 2019-09-05 Class Information Class notes for Math 154 at Pomona College: Methods in Biostatistics. The notes are based extensively on An Introduction to Statistical Learning (James et al. 2013) by James, Witten, Hastie, and Tibshiran; Modern Data Science (Baumer, Kaplan, and Horton 2017) with R by Baumer, Kaplan, and Horton; Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving (Nolan and Temple Lang 2015) by Nolan and Temple Lang; and Visual and Statistical Thinking: Displays of Evidence for Making Decisions (Tufte 1997) by Tufte. You are responsible for reading the relevant chapters in the text. The texts are very good &amp; readable, so you should use them. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chp Notes Section 9/3/19 Intro to Data / R ISL1 1 [intro], 1.1 [Sep3] 9/5/19 Intro to GitHub 1.5 [repro], 1.4 [Sep5] 9/10/19 Data Viz VST @ref(), @ref() [] 9/12/19 MDS2 @ref(), @ref() [] References "],
["intro.html", "Chapter 1 Introduction 1.1 9/3/19 Agenda 1.2 Course Logistics 1.3 Course Content 1.4 9/5/19 Agenda 1.5 Reproducibility 1.6 Data Examples", " Chapter 1 Introduction 1.1 9/3/19 Agenda Questionnaire Syllabus &amp; Course Outline Stitch Fix Algorithm College Rankings Can Twitter predict election results? 1.2 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Figure 1.1: Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population What is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years. Who should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science. What are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Is there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability? When should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors. What is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class. What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Figure 1.2: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.3: Jessica Ward, PhD student at Newcastle University 1.3 Course Content 1.3.1 Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. A parameter is a numerical measurement of the population. We never know the true value of the parameter. An estimator is a function of the unobserved data that tries to approximate the unknown parameter value. An estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.] One of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015) With thanks to Ben Baumer for perspective and sharing course materials. 1.3.2 The Workflow Figure 1.4: A schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015) Figure 1.5: Stitch Fix Algorithms Tour 1.3.3 Principles for the Data Science Process tl;dr (Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.) Duncan Temple Lang, University of California, Davis Duncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.] Duncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science. Temple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process: Asking a general question. Refining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood. Access to data. This is unrelated to the science but does require computational skill. Transforming to data structures. Exploratory data analyses to understand the data and determine whether the results will scale. This is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results. Temple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student: Mapping the general question to a statistical framework. Understanding the scope of inference, sampling, biases, and limitations. Exploratory data analyses, including missing values, data quality, cleaning, matching, and fusing. Understanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability. Conditional dependence and heterogeneity. Dimension reduction, variable selection, and sparsity. Spurious relationships and multiple testing. Parameter estimation versus “black box” prediction and classification. Diagnostics, residuals, and comparing models. Quantifying the uncertainty of a model. Sampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed. Statistical accuracy versus computational complexity and efficiency. Temple Lang then briefly discussed some of the practical aspects of computing, including the following: Accessing data. Manipulating raw data. Data structures and storage, including correlated data. Visualization at all stages (particularly in exploratory data analyses and conveying the results). Parallel computing, which can be challenging for a new student. Translating high-level descriptions to optimal programs. During the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).] A workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets. 1.4 9/5/19 Agenda Design Challenge Not So Standard Deviations Reproducibility &amp; GitHub 1.5 Reproducibility 1.5.1 Need for Reproducibility Figure 1.6: slide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016 Example 1 Science retracts gay marriage paper without agreement of lead author LaCour In May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior. Science Editor-in-Chief Marcia McNutt: Original survey data not made available for independent reproduction of results. Survey incentives misrepresented. Sponsorship statement false. Two Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked. Methods we’ll discuss can’t prevent this, but they can make it easier to discover issues. Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent Example 2 Seizure study retracted after authors realize data got “terribly mixed” * From the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates: The article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness. Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/ Example 3 Bad spreadsheet merge kills depression paper, quick fix resurrects it The authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct. Original conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression … Revised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression … Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/ Example 4 PNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/ 1.5.2 The reproducible data analysis process Scriptability \\(\\rightarrow\\) R Literate programming \\(\\rightarrow\\) R Markdown Version control \\(\\rightarrow\\) Git / GitHub Scripting and literate programming Donald Knuth “Literate Programming” (1983) &gt; Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do. The ideas of literate programming have been around for many years! and tools for putting them to practice have also been around but they have never been as accessible as the current tools Reproducibility checklist Are the tables and figures reproducible from the code and data? Does the code actually do what you think it does? In addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?) Can the code be used for other data? Can you extend the code to do other things? Tools: R &amp; R Studio See this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&amp;feature=youtu.be You must use both R and RStudio software programs R does the programming R Studio brings everything together You may use Pomona’s server: https://rstudio.pomona.edu/ See course website for getting started: http://research.pomona.edu/johardin/math154f19/ Figure 1.7: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.8: Jessica Ward, PhD student at Newcastle University Tools: GitHub You must submit your assignments via GitHub Follow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/ Follow Jacob Fiksel’s advice on how to connect to our classroom: https://github.com/jfiksel/github-classroom-for-students Tools: a GitHub merge conflict (demo) On GitHub (on the web) edit the README document and Commit it with a message describing what you did. Then, in RStudio also edit the README document with a different change. Commit your changes Try to push \\(rightarrow\\) you’ll get an error! Try pulling Resolve the merge conflict and then commit and push As you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important. Steps for weekly homework You will get a link to the new assignment (clicking on the link will create a new private repo) Use R Studio New Project, version control, Git Clone the repo using SSH If it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd Do the assignment commit and push after every problem All necessary files must be in the same folder (e.g., data) 1.6 Data Examples What can/can’t Data Science Do? Can model the data at hand! Can find patterns &amp; visualizations in large datasets. Can’t establish causation. Can’t represent data if it isn’t there. Stats / Data Science / Math are not apolitical/agnostic “Inner city crime is reaching record levels” (Donald Trump, 8/30/16) “The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16) “Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16) “We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16) “If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16) “Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16) Source: http://www.politifact.com/truth-o-meter/statements/ 1.6.1 College Rankings Systems Cheating Bucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html) Gaming the system Point by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/) No way to measure “quality of education” What is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!). 1.6.2 Trump and Twitter Analysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone. Aug 9, 2016 http://varianceexplained.org/r/trump-tweets/ My analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures. Aug 9, 2017 http://varianceexplained.org/r/trump-followup/ There is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post. I’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it. I’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement. 1.6.3 Can Twitter Predict Election Results? In 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013) What makes using Tweets to predict elections relevant to our class? (See Baumer (2015).) The data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet. The analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets). The dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed. The researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses. Activity Spend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions. * working paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423 * published in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449. * editorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html * editorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html * editorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/ Statistics Hat Write a sentence summarizing the findings of the paper. Discuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently? should be proportion for the response variable. The bizarre scaling could dramatically change the results dots could then be scaled in proportion to the number of tweets linear fit may be questionable. How would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement? Interpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units. Discuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election? \\(R^2\\) is way higher after control variables are included, but duh! the full model will likely do a better job of predicting Why do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election? After controlling for how many Republicans are in the district, most of the effect disappears While the coefficient of the main term is still statistically significant, the size of the coefficient (155 +/- 43 votes) is of little practical significance Do you think the study holds water? Why or why not? What are the shortcomings of this study? Not really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model? Data Scientist Hat Imagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor. What steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform. What computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities &amp; knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in? How much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb. Advantages Cheap Can measure any political race (not just the wealthy ones). Disadvantages Is it really reflective of the voting populace? Who would it bias toward? Does simple mention of a candidate always reflect voting patterns? When wouldn’t it? Margin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\). Tweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling? Can’t measure any demographic information. What could be done differently? Gelman: look only at close races Gelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.” Gelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.” Gelman: Make the data publicly available so that others can try to reproduce the results Tweeting and R The twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions. References "],
["visualization.html", "Chapter 2 Visualization 2.1 9/10/19 Agenda 2.2 Examples 2.3 Deconstructing / Creating Plots 2.4 9/12/19 Agenda 2.5 Pieces of a Data Graphic", " Chapter 2 Visualization 2.1 9/10/19 Agenda Cholera: what went (didn’t go) well with the graphics? Challenger: what didn’t go (went) well with the graphics? Deconstructing a plot (with example(s)) Should have read: Tufte (1997) One more great reference is the following text: Fundamentals of Data Visualization by Wilke at http://serialmentor.com/dataviz/ Depending on the introductory (or other) statistics classes you’ve had, your instructor may have focused more or less on visualizations in class. They (I) may have even said something like making visualizations are incredibly important to the entire data analysis process. But even if you buy this perspective, why is it that we don’t see more good graphics in our analyses? Andrew Gelman (Gelman 2011) responds to this point by stating, “Good statistical graphics are hard to do, much harder than running regressions and making tables.” Our goal will be to create graphics and visualizations that convey statistical information. Nolan (Nolan and Perrett 2016) describes three important ways that graphics can be used to convey statistical information. These ``guiding principles&quot; will be used as a way of evaluating others’ figures as well as a metric for creating our own visualizations to help with statistical analysis. Make the data stand out The important idea here is to find anything unusual in the data. Are there patterns? Outliers? What are the bounds of the variables? How should the axes be scaled? Are transformations warranted? Facilitate comparison The second item allows us to consider the research questions at hand. What are the important variables? How do we emphasize them? Which variables should be plotted together? Can they be super-imposed? Does color, plotting character, size of plot character help to bring out the important relationships? Be aware of overplotting and issues of color blindness! http://colorbrewer2.org/ Add information Plots should also add context to the comparison. Figure legends, axes scales, and reference markers (e.g., a line at \\(y=x\\)) go a long way toward helping the reader understand your message. Captions should be self-contained (and not assume the user has also read your text) and descriptive; they should summarize the content of the figure and the conclusion related to the message you want to convey. Randy Pruim asks the following question to decide whether or not a plot is good: Does my plot make the comparisons I am interested in… * easily? and * accurately? 2.2 Examples The first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased for $7 at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download. An aside Generally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi As mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (E. Tufte 1997). 1. “An essential analytic task in making decisions based on evidence is to understand how things work.” 2. Making decisions based on evidence requires the appropriate display of that evidence.&quot; Back to the examples… 2.2.1 Cholera via Tufte In September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after. Why was John Snow successful at solving the problem? Some thoughts to consider (as reported in E. Tufte (1997)): The bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means. Data in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area. Comparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease. Alternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case. Did removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline? Assessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told? 2.2.2 Challenger via Tufte John Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F). Unlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated}! The biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what?? The engineers who understood the problem created tables and engineering graphs which were Not visually appealing. Not decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”) There was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility. The information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure. Missing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.) Anecdotal evidence was misconstrued: SRM-15 at 57F at the most damage, but SRM-22 at 75F had the second most damage. In the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F. Tufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order. As with the cholera outbreak, a persuasive argument could have been made if the visualizations had (1) been in context plot data versus temperature not time!, (2) used appropriate comparisons: as compared with what?, (3) consider alternative scenarios * when else did O-rings fail? What is the science behind O-ring failure?, and (4) the graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?*. Tufte (E. Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. At this point, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms. Figure 1.2: The graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (E. Tufte 1997) 2.3 Deconstructing / Creating Plots 2.3.1 Advice Basic plotting Avoid having other graph elements interfere with data Use visually prominent symbols Avoid over-plotting (One way to avoid over plotting: Jitter the values) Different values of data may obscure each other Include all or nearly all of the data Fill data region Eliminate superfluous material Chart junk &amp; stuff that adds no meaning, e.g. butterflies on top of barplots, background images Extra tick marks and grid lines Unnecessary text and arrows Decimal places beyond the measurement error or the level of difference Facilitate Comparisons Put juxtaposed plots on same scale Make it easy to distinguish elements of superposed plots (e.g. color) Emphasizes the important difference Comparison: volume, area, height (be careful, volume can seem bigger than you mean it to) Choosing the Scale (n.b., some of these principles may go counter to one another, use your judgment.) Keep scales on x and y axes the same for both plots to facilitate the comparison Zoom in to focus on the region that contains the bulk of the data Keep the scale the same throughout the plot (i.e. don’t change it mid-axis) Origin need not be on the scale Choose a scale that improves resolution Avoid jiggling the baseline How to make a plot information rich Describe what you see in the caption Add context with reference markers (lines and points) including text Add legends and labels Use color and plotting symbols to add more information Plot the same thing more than once in different ways/scales Reduce clutter Captions should Be comprehensive Self-contained Describe what has been graphed Draw attention to important features Describe conclusions drawn from graph Good Plot Making Practice Put major conclusions in graphical form Provide reference information Proof read for clarity and consistency Graphing is an iterative process Multiplicity is OK, i.e. two plots of the same variable may provide different messages Make plots data rich Creating a statistical graphic is an iterative process of discovery and fine tuning. We try to model this process in the course by dedicating class time to an interactive iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data. 2.3.2 An example from Information is Beautiful (See HW2 for details on R code) Consider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over this hurdle, it is not what is expected when looking at a graph. Figure 1.3: http://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png I have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that a different difference between my plot and the plot above is that I have many more observations than she did. Figure 1.4: Calories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot? Data retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0 2.3.3 Assessing Graphics \\begin{table}[H] \\begin{center} \\begin{tabular}{p{0.2}|p{0.2}p{0.2}p{0.2}} &amp; Critical Task &amp; Needs Improvement &amp; Basic &amp; Surpassed Computation} perform computations &amp; Computations contain errors and extraneous code &amp; Computations are correct but contain extraneous / unnecessary computations &amp; Computations are correct and properly identified and labeled Analysis} Choose and carry out analysis appropriate for data and content(s) &amp; Choice of analysis is overly simplistic, irrelevant, or missing key component &amp; Analysis appropriate, but incomplete, or not important features and assumptions not made explicit &amp; Analysis appropriate, complete, advanced, relevant, and informative Synthesis} Identify key features of the analysis, and interpret results (including context) &amp; Conclusions are missing, incorrect, or not made based on results of analysis &amp; Conclusions reasonable, but is partially correct or partially complete &amp; Make relevant conclusions explicitly connect to analysis and to context Visual presentation} Communicate findings graphically clearly, precisely, and concisely &amp; Inappropriate choice of plots; poorly labeled plots; plots missing &amp; Plots convey information correctly but lack context for interpretation &amp; Plots convey information correctly with adequate / appropriate reference information **Written} Communicate findings clearly, precisely, and concisely &amp; Explanation is illogical, incorrect, or incoherent &amp; Explanation is partially correct but incomplete or unconvincing &amp; Explanation is correct, complete, and convincing \\end{tabular} \\end{center} \\end{table} 2.4 9/12/19 Agenda Grammar of graphics Null plots ggplot % look at Di’s plenary talk from rstudio::conf 2018 % (1) making many different plots from the same data, they show very different things! % (2) ideas of null plots 2.5 Pieces of a Data Graphic Yau (???) ({}, 2013) and Wickham (???) (**Tidy Data}, {}, 2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence. One great way of thinking of this new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that this transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?) The most important questions you can ask with respect to creating figures are: 1. What do we want R to do? (What is the goal?) 2. What does R need to know? Yau gives us nine visual cues, and Wickham translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2017), chapter 2.) Visual Cues: the aspects of the figure where we should focus. Position (numerical) where in relation to other things? Length (numerical) how big (in one dimension)? Angle (numerical) how wide? parallel to something else? Direction (numerical) at what slope? In a time series, going up or down? Shape (categorical) belonging to what group? Area (numerical) how big (in two dimensions)? Beware of improper scaling! Volume (numerical) how big (in three dimensions)? Beware of improper scaling! Shade (either) to what extent? how severely? Color (either) to what extent? how severely? Beware of red/green color blindness. Coordinate System: rectangular, polar, geographic, etc. Scale: numeric (linear? logarithmic?), categorical (ordered?), time Context: in comparison to what (think back to ideas from Tufte) 2.5.1 ggplot2 In ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2017). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births. %Things to talk about: %layers %facets %tidy data %scale: the computer maps the data to the aesthetic (computer doing the work) %guide: the aesthetic gets mapped back to the data (the human interpreting the graph) %frame: the variables that define the space (e.g., x &amp; y coordinates and faceting) Goals What I will try to do give a tour of ggplot2 explain how to think about plots the ggplot2 way prepare/encourage you to learn more later What I can’t do in one session show every bell and whistle make you an expert at using ggplot2 Getting help One of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/ Look at the end of this presentation. More help options there. Pieces of the graph Yau gives us nine visual cues, and Wickham translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2017), chapter 2.) Visual Cues: the aspects of the figure where we should focus. Position (numerical) where in relation to other things? Length (numerical) how big (in one dimension)? Angle (numerical) how wide? parallel to something else? Direction (numerical) at what slope? In a time series, going up or down? Shape (categorical) belonging to what group? Area (numerical) how big (in two dimensions)? Beware of improper scaling! Volume (numerical) how big (in three dimensions)? Beware of improper scaling! Shade (either) to what extent? how severely? Color (either) to what extent? how severely? Beware of red/green color blindness. Coordinate System: rectangular, polar, geographic, etc. Scale: numeric (linear? logarithmic?), categorical (ordered?), time Context: in comparison to what (think back to ideas from Tufte) require(mosaic) require(lubridate) # package for working with dates data(Births78) # restore fresh version of Births78 head(Births78, 3) ## date births wday year month day_of_year day_of_month day_of_week ## 1 1978-01-01 7701 Sun 1978 1 1 1 1 ## 2 1978-01-02 7527 Mon 1978 1 2 2 2 ## 3 1978-01-03 8825 Tue 1978 1 3 3 3 The grammar of graphics geom: the geometric “shape” used to display data bar, point, line, ribbon, text, etc. aesthetic: an attribute controlling how geom is displayed wih respect to variables x position, y position, color, fill, shape, size, etc. scale: adjust information in the aesthetic to map onto the plot particular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values. guide: helps user convert visual data back into raw data (legends, axes) stat: a transformation applied to data before geom gets it example: histograms work on binned data How can we make this plot? Two Questions: What do we want R to do? (What is the goal?) What does R need to know? data source: Births78 aesthetics: date -&gt; x births -&gt; y default color (same for all points) Goal: scatterplot = a plot with points ggplot() + geom_point() What does R need to know? data source: data = Births78 aesthetics: aes(x = date, y = births) What are the visual cues on this plot? position length shape area/volume shade/color Coordinate System? Scale? What are the visual cues on this plot? position length shape area/volume shade/color Coordinate System? Scale? What are the visual cues on this plot? position length shape area/volume shade/color Coordinate System? Scale? How can we make this plot? What has changed? new aesthetic: mapping color to day of week Adding day of week to the data set The wday() function in the lubridate package computes the day of the week from a date. Births78 &lt;- Births78 %&gt;% mutate(wday = wday(date, label=TRUE)) ggplot(data=Births78) + geom_point(aes(x=date, y=births, color=wday)) How can we make this plot? This time we use lines instead of dots ggplot(data=Births78) + geom_line(aes(x=date, y=births, color=wday)) How can we make this plot? This time we have two layers, one with points and one with lines ggplot(data=Births78, aes(x=date, y=births, color=wday)) + geom_point() + geom_line() The layers are placed one on top of the other: the points are below and the lines are above. data and aes specified in ggplot() affect all geoms Alternative Syntax Births78 %&gt;% ggplot(aes(x=date, y=births, color=wday)) + geom_point() + geom_line() What does this do? Births78 %&gt;% ggplot(aes(x=date, y=births, color=&quot;navy&quot;)) + geom_point() This is mapping the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy. Setting vs. Mapping If we want to set the color to be navy for all of the dots, we do it this way: Births78 %&gt;% ggplot(aes(x=date, y=births)) + # map these geom_point(color = &quot;navy&quot;) # set this Note that color = &quot;navy&quot; is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting. How can we make this plot? Births78 %&gt;% ggplot(aes(x=date, y=births)) + geom_line(aes(color=wday)) + # map color here geom_point(color=&quot;navy&quot;) # set color here ggplot() establishes the default data and aesthetics for the geoms, but each geom may change these defaults. good practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah() Setting vs. Mapping (again) If I want information to be passed to all data points (not variable): map the information inside the aes (aesthetic) command set the information outside the aes (aesthetic) command Other geoms apropos(&quot;^geom_&quot;) [1] &quot;geom_abline&quot; &quot;geom_area&quot; &quot;geom_ash&quot; [4] &quot;geom_bar&quot; &quot;geom_barh&quot; &quot;geom_bin2d&quot; [7] &quot;geom_blank&quot; &quot;geom_boxplot&quot; &quot;geom_boxploth&quot; [10] &quot;geom_col&quot; &quot;geom_colh&quot; &quot;geom_contour&quot; [13] &quot;geom_count&quot; &quot;geom_crossbar&quot; &quot;geom_crossbarh&quot; [16] &quot;geom_curve&quot; &quot;geom_density&quot; &quot;geom_density_2d&quot; [19] &quot;geom_density2d&quot; &quot;geom_dotplot&quot; &quot;geom_errorbar&quot; [22] &quot;geom_errorbarh&quot; &quot;geom_errorbarh&quot; &quot;geom_freqpoly&quot; [25] &quot;geom_hex&quot; &quot;geom_histogram&quot; &quot;geom_histogramh&quot; [28] &quot;geom_hline&quot; &quot;geom_jitter&quot; &quot;geom_label&quot; [31] &quot;geom_line&quot; &quot;geom_linerange&quot; &quot;geom_linerangeh&quot; [34] &quot;geom_lm&quot; &quot;geom_map&quot; &quot;geom_path&quot; [37] &quot;geom_point&quot; &quot;geom_pointrange&quot; &quot;geom_pointrangeh&quot; [40] &quot;geom_polygon&quot; &quot;geom_qq&quot; &quot;geom_qq_line&quot; [43] &quot;geom_quantile&quot; &quot;geom_raster&quot; &quot;geom_rect&quot; [46] &quot;geom_ribbon&quot; &quot;geom_rug&quot; &quot;geom_segment&quot; [49] &quot;geom_sf&quot; &quot;geom_sf_label&quot; &quot;geom_sf_text&quot; [52] &quot;geom_smooth&quot; &quot;geom_spline&quot; &quot;geom_spoke&quot; [55] &quot;geom_step&quot; &quot;geom_text&quot; &quot;geom_tile&quot; [58] &quot;geom_violin&quot; &quot;geom_violinh&quot; &quot;geom_vline&quot; help pages will tell you their aesthetics, default stats, etc. ?geom_area # for example Let’s try geom_area Births78 %&gt;% ggplot(aes(x=date, y=births, fill=wday)) + geom_area() This is not a good plot overplotting is hiding much of the data extending y-axis to 0 may or may not be desirable. Side note: what makes a plot good? Most (all?) graphics are intended to help us make comparisons How does something change over time? Do my treatments matter? How much? Do men and women respond the same way? Key plot metric: Does my plot make the comparisions I am interested in easily, and accurately? Time for some different data HELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial head(HELPrct) ## age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female ## 1 37 1 yes 49 3 177 225 0 NA 0 ## 2 37 1 yes 30 22 2 NA 0 NA 0 ## 3 26 1 yes 39 0 3 365 20 NA 0 ## 4 39 1 yes 15 2 189 343 0 1 1 ## 5 32 1 yes 39 12 2 57 0 1 0 ## 6 47 1 yes 6 1 31 365 0 NA 1 ## sex g1b homeless i1 i2 id indtot linkstatus link mcs pcs ## 1 male yes housed 13 26 1 39 1 yes 25.111990 58.41369 ## 2 male yes homeless 56 62 2 43 NA &lt;NA&gt; 26.670307 36.03694 ## 3 male no housed 0 0 3 41 0 no 6.762923 74.80633 ## 4 female no housed 5 5 4 28 0 no 43.967880 61.93168 ## 5 male no homeless 10 13 5 38 1 yes 21.675755 37.34558 ## 6 female no housed 4 4 6 29 0 no 55.508991 46.47521 ## pss_fr racegrp satreat sexrisk substance treat avg_drinks max_drinks ## 1 0 black no 4 cocaine yes 13 26 ## 2 1 white no 7 alcohol yes 56 62 ## 3 13 black no 2 heroin no 0 0 ## 4 11 white yes 4 heroin no 5 5 ## 5 10 black no 6 cocaine no 10 13 ## 6 5 black no 5 cocaine yes 4 4 Subjects admitted for treatment for addiction to one of three substances. Why are these people in the study? HELPrct %&gt;% ggplot(aes(x=substance)) + geom_bar() Hmm. What’s up with y? stat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values. Why are these people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar() Why are these people in the study? library(scales) HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar() + scale_y_continuous(labels = percent) Why are these people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar(position=&quot;fill&quot;) + scale_y_continuous(&quot;actually, percent&quot;) How old are people in the HELP study? HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice the messages stat_bin: Histograms are not mapping the raw data but binned data. stat_bin() performs the data transformation. binwidth: a default binwidth has been selected, but we should really choose our own. Setting the binwidth manually HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram(binwidth=2) How old are people in the HELP study? – Other geoms HELPrct %&gt;% ggplot(aes(x=age)) + geom_freqpoly(binwidth=2) HELPrct %&gt;% ggplot(aes(x=age)) + geom_density() Selecting stat and geom manually Every geom comes with a default stat for simple cases, the stat is stat_identity() which does nothing we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + geom_line(stat=&quot;density&quot;) Selecting stat and geom manually Every stat comes with a default geom, every geom with a default stat we can specify stats instead of geom, if we prefer we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + stat_density( geom=&quot;line&quot;) More combinations HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3) + geom_line(stat=&quot;bin&quot;, binwidth=3) HELPrct %&gt;% ggplot(aes(x=age)) + geom_area(stat=&quot;bin&quot;, binwidth=3) HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3, aes(size=..count..)) + geom_line(stat=&quot;bin&quot;, binwidth=3) How much do they drink? (i1) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_histogram() HELPrct %&gt;% ggplot(aes(x=i1)) + geom_density() HELPrct %&gt;% ggplot(aes(x=i1)) + geom_area(stat=&quot;density&quot;) Covariates: Adding in more variables Using color and linetype: HELPrct %&gt;% ggplot(aes(x=i1, color=substance, linetype=sex)) + geom_line(stat=&quot;density&quot;) Using color and facets HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( . ~ sex ) HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( sex ~ . ) Boxplots Boxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”. The quantitative variable must be y, and there must be an additional x variable. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() Horizontal boxplots Horizontal boxplots are obtained by flipping the coordinate system: HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_flip() coord_flip() may be used with other plots as well to reverse the roles of x and y on the plot. Axes scaling with boxplots We can scale the continuous axis HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_trans(y=&quot;log&quot;) Give me some space We’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot(position=position_dodge(width=1)) Issues with bigger data require(NHANES) dim(NHANES) ## [1] 10000 76 NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + facet_grid( Gender ~ PregnantNow ) Although we can see a generally positive association (as we would expect), the overplotting may be hiding information. Using alpha (opacity) One way to deal with overplotting is to set the opacity low. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) geom_density2d Alternatively (or simultaneously) we might prefere a different geom altogether. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_density2d() + facet_grid( Gender ~ PregnantNow ) Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_jitter(alpha=.6) + coord_flip() Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) + coord_flip() Things I haven’t mentioned (much) scales (fine tuning mapping from data to plot) guides (so reader can map from plot to data) coords (coord_flip() is good to know about) themes (for customizing appearance) require(ggthemes) qplot( x=date, y=births, data=Births78) + theme_wsj() Things I haven’t mentioned (much) scales (fine tuning mapping from data to plot) guides (so reader can map from plot to data) coords (coord_flip() is good to know about) themes (for customizing appearance) position (position_dodge() can be used for side by side bars) ggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge()) + geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) Things I haven’t mentioned (much) scales (fine tuning mapping from data to plot) guides (so reader can map from plot to data) themes (for customizing appearance) position (position_dodge(), position_jitterdodge(), position_stack(), etc.) transforming axes A little bit of everything ggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge(width=1)) + geom_point(aes(fill=sex), alpha=.5, position=position_jitterdodge(dodge.width=1)) + facet_wrap(~homeless) Want to learn more? docs.ggplot2.org/ Winston Chang’s: R Graphics Cookbook What else can we do? shiny interactive graphics / modeling ggvis dynamic graphics (brushing, sliders, tooltips, etc.) uses Vega (D3) to animate plots in a browser similar structure to ggplot2 but different syntax and names Dynamic documents combination of RMarkdown, ggvis, and shiny References "],
["references.html", "References", " References "]
]
