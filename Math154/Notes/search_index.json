[
["index.html", "Computational Statistics Class Information", " Computational Statistics Jo Hardin 2019-09-02 Class Information Class notes for Math 154 at Pomona College: Methods in Biostatistics. The notes are based extensively on An Introduction to Statistical Learning (James et al. 2013) by James, Witten, Hastie, and Tibshiran; Modern Data Science (Baumer, Kaplan, and Horton 2017) with R by Baumer, Kaplan, and Horton; Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving (Nolan and Temple Lang 2015) by Nolan and Temple Lang; and Visual and Statistical Thinking: Displays of Evidence for Making Decisions (Tufte 1997) by Tufte. You are responsible for reading the relevant chapters in the text. The texts are very good &amp; readable, so you should use them. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chp Notes Section 1/23/19 t-tests / SLR / Intro to R 2 1, 2.1, 2.2 [t-test] 1/28/19 SLR 2 ?? [SLR] 1/30/19 ??, ?? [SLR CI] 2/4/19 Contingency Analysis 6 ??, ?? [.Fisher’s Exact Test] 2/6/19 ?? [Types of studies] 2/11/19 Contingency Analysis 6 ?? [RR and OR] References "],
["intro.html", "Chapter 1 Introduction 1.1 Course Logistics 1.2 Course Content 1.3 Reproducibility 1.4 Data Examples", " Chapter 1 Introduction 1.1 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population What is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years. Who should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science. What are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Is there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability? When should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors. What is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class. What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Jessica Ward, PhD student at Newcastle University 1.2 Course Content 1.2.1 Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. A parameter is a numerical measurement of the population. We never know the true value of the parameter. An estimator is a function of the unobserved data that tries to approximate the unknown parameter value. An estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.] One of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. With thanks to Ben Baumer for perspective and sharing course materials. (Baumer 2015) 1.2.2 The Workflow A schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015) Stitch Fix Algorithms Tour 1.2.3 Principles for the Data Science Process Duncan Temple Lang, University of California, Davis Duncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.] Duncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science. Temple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process: Asking a general question. Refining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood. Access to data. This is unrelated to the science but does require computational skill. Transforming to data structures. Exploratory data analyses to understand the data and determine whether the results will scale. This is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results. Temple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student: Mapping the general question to a statistical framework. Understanding the scope of inference, sampling, biases, and limitations. Exploratory data analyses, including missing values, data quality, cleaning, matching, and fusing. Understanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability. Conditional dependence and heterogeneity. Dimension reduction, variable selection, and sparsity. Spurious relationships and multiple testing. Parameter estimation versus “black box” prediction and classification. Diagnostics, residuals, and comparing models. Quantifying the uncertainty of a model. Sampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed. Statistical accuracy versus computational complexity and efficiency. Temple Lang then briefly discussed some of the practical aspects of computing, including the following: Accessing data. Manipulating raw data. Data structures and storage, including correlated data. Visualization at all stages (particularly in exploratory data analyses and conveying the results). Parallel computing, which can be challenging for a new student. Translating high-level descriptions to optimal programs. During the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).] A workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets. 1.3 Reproducibility 1.3.1 Need for Reproducibility slide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016 1.3.1.1 Example 1 Science retracts gay marriage paper without agreement of lead author LaCour In May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior. Science Editor-in-Chief Marcia McNutt: Original survey data not made available for independent reproduction of results. Survey incentives misrepresented. Sponsorship statement false. Two Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked. Methods we’ll discuss can’t prevent this, but they can make it easier to discover issues. Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent 1.3.2 Example 2 Seizure study retracted after authors realize data got “terribly mixed” From the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates: The article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness. Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/ 1.3.3 Example 3 Bad spreadsheet merge kills depression paper, quick fix resurrects it The authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct. Original conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression … Revised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression … Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/ 1.3.4 Example 4 PNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/ 1.3.5 Reproducible data analysis process Scriptability \\(\\rightarrow\\) R Literate programming \\(\\rightarrow\\) R Markdown Version control \\(\\rightarrow\\) Git / GitHub 1.3.5.1 {-} Scripting and literate programming Donald Knuth “Literate Programming” (1983) &gt; Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do. The ideas of literate programming have been around for many years! and tools for putting them to practice have also been around but they have never been as accessible as the current tools 1.3.5.2 {-} Reproducibility checklist Are the tables and figures reproducible from the code and data? Does the code actually do what you think it does? In addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?) Can the code be used for other data? Can you extend the code to do other things? 1.3.5.3 {-} Tools: R &amp; R Studio See this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&amp;feature=youtu.be You must use both R and RStudio software programs R does the programming R Studio brings everything together You may use Pomona’s server: https://rstudio.pomona.edu/ See course website for getting started: http://research.pomona.edu/johardin/math154f19/ 1.3.5.4 {-} Tools: GitHub You must submit your assignments via GitHub Follow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/ Follow Jacob Fiksel’s advice on how to connect to our classroom: https://github.com/jfiksel/github-classroom-for-students 1.3.5.5 {-} Tools: a GitHub merge conflict (demo) On GitHub (on the web) edit the README document and Commit it with a message describing what you did. Then, in RStudio also edit the README document with a different change. Commit your changes Try to push \\(rightarrow\\) you’ll get an error! Try pulling Resolve the merge conflict and then commit and push As you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important. 1.3.5.6 {-} Steps for weekly homework You will get a link to the new assignment (clicking on the link will create a new private repo) Use R Studio New Project, version control, Git Clone the repo using SSH If it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd Do the assignment commit and push after every problem All necessary files must be in the same folder (e.g., data) 1.4 Data Examples 1.4.0.1 {-} What can/can’t Data Science Do? Can model the data at hand! Can find patterns &amp; visualizations in large datasets. Can’t establish causation. Can’t represent data if it isn’t there. 1.4.0.2 {-} Stats / Data Science / Math are not apolitical “Inner city crime is reaching record levels” (Donald Trump, 8/30/16) “The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16) “Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16) “We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16) “If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16) “Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16) Source: http://www.politifact.com/truth-o-meter/statements/ 1.4.1 College Rankings Systems Cheating Bucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html) Gaming the system Point by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/) No way to measure “quality of education” What is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!). 1.4.2 Trump and Twitter Analysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone * http://varianceexplained.org/r/trump-tweets/ My analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What?s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures. http://varianceexplained.org/r/trump-followup/ *There is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017.} This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump?s tweets from his campaign?s by training on the kinds of features I used in my original post. *I’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump?s tweets. I since learned there?s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it. *I’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement. 1.4.3 Can Twitter Predict Election Results? In 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: ``Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.&quot; (Rojas 2013) What makes using Tweets to predict elections relevant to our class? (See Baumer (2015).) The data come from neither an experiment nor a random sample - there must be careful thought applied to the question of *to whom} the analysis can be generalized. The data were also scraped from the internet. The analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets). The dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed. The researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses. 1.4.3.1 {-} Activity Spend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions. * working paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423 * published in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449. * editorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html * editorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html * editorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/ 1.4.3.2 {-} Statistics Hat Write a sentence summarizing the findings of the paper. Discuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently? should be proportion for the response variable. The bizarre scaling could dramatically change the results dots could then be scaled in proportion to the number of tweets linear fit may be questionable. How would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement? Interpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units. Discuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election? \\(R^2\\) is way higher after control variables are included, but duh! the full model will likely do a better job of predicting Why do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election? After controlling for how many Republicans are in the district, most of the effect disappears While the coefficient of the main term is still statistically significant, the size of the coefficient (155 +/- 43 votes) is of little practical significance Do you think the study holds water? Why or why not? What are the shortcomings of this study? Not really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model? 1.4.3.3 {-} Data Scientist Hat Imagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor. What steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform. What computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities &amp; knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in? How much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb. 1.4.3.4 {-} Advantages Cheap Can measure any political race (not just the wealthy ones). 1.4.3.5 {-} Disadvantages Is it really reflective of the voting populace? Who would it bias toward? Does simple mention of a candidate always reflect voting patterns? When wouldn’t it? Margin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\). Tweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling? Can’t measure any demographic information. 1.4.3.6 {-} What could be done differently? Gelman: look only at close races Gelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.” Gelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.” Gelman: Make the data publicly available so that others can try to reproduce the results 1.4.3.7 {-} Tweeting and R The twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions. References "],
["t-tests-vs-slr.html", "Chapter 2 t-tests vs. SLR 2.1 t-test (book: 2.1) ANOVA 2.2 Simple Linear Regression (book: 2.3) 2.3 Confidence Intervals (section 2.11) 2.4 Random Sample vs. Random allocation", " Chapter 2 t-tests vs. SLR We are going to build on a very basic model of the following form: data = deterministic model + random error planned variability your experimental conditions, hopefully represented by an interesting deterministic model random error natural variability due to individuals. systematic error error that is not contained within the model. It can happen because of poor sampling or poor experimental conditions. Surgery Timing The study, “Operation Timing and 30-Day Mortality After Elective General Surgery”, tested the hypotheses that the risk of 30-day mortality associated with elective general surgery: 1) increases from morning to evening throughout the routine workday; 2) increases from Monday to Friday through the workweek; and 3) is more frequent in July and August than during other months of the year. As a presumed negative control, the investigators also evaluated mortality as a function of the phase of the moon. Secondarily, they evaluated these hypotheses as they pertain to a composite in-hospital morbidity endpoint. The related data set contains 32,001 elective general surgical patients. Age, gender, race, BMI, several comorbidities, several surgical risk indices, the surgical timing predictors (hour, day of week, month,moon phase) and the outcomes (30-day mortality and in-hosptial complication) are provided. The dataset is cleaned and complete (no missing data except for BMI). There are no outliers or data problems. The data are from (Sessler et al. 2011) Note that in the example, mortality rates are compared for patients electing to have surgery in July vs August. We’d like to compare the average age of the participants from the July group to the August group. Even if the mortality difference is significant, we can’t conclude causation because it was an observational study. However, the more similar the groups are based on clinical variables, the more likely any differences in mortality are due to timing. How different are the groups based on clinical variables? surgeryurl &lt;- url(&quot;https://www.causeweb.org/tshs/datasets/surgery_timing.Rdata&quot;) load(surgeryurl) surgery &lt;- stata_data surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% dplyr::group_by(month) %&gt;% dplyr::summarize(agemean = mean(age, na.rm=TRUE), agesd = sd(age, na.rm=TRUE), agen = sum(!is.na(age))) ## # A tibble: 2 x 4 ## month agemean agesd agen ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aug 58.1 15.2 3176 ## 2 Jul 57.6 15.5 2325 2.1 t-test (book: 2.1) A t-test is a test of means. For the surgery timing data, the groups would ideally have similar age distributions. Why? What are the advantages and disadvantages of running a retrospective cohort study? The two-sample t-test starts with the assumption that the population means of the two groups are equal, \\(H_0: \\mu_1 = \\mu_2\\). The sample means \\(\\overline{y}_1\\) and \\(\\overline{y}_2\\) will always be different. How different must the \\(\\overline{y}\\) values be in order to reject the null hypothesis? Definition 2.1 (Model 1) \\[\\begin{align} y_{1j} &amp;= \\mu_{1} + \\epsilon_{1j} \\ \\ \\ \\ j=1, 2, \\ldots, n_1\\\\ y_{2j} &amp;= \\mu_{2} + \\epsilon_{2j} \\ \\ \\ \\ j=1, 2, \\ldots, n_2\\\\ \\epsilon_{ij} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\mu_i \\end{align}\\] That is, we are assuming that for each group the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean. Note that we have assumed that the variances of the two groups are equal. We have also assumed that there is independence between and within the groups. Note: we will assume the population variances are equal if neither sample variance is more than twice as big as the other. Example1 Are the mean ages of the July vs August patients statistically different? (why two sided?) \\[\\begin{align} H_0: \\mu_1 = \\mu_2\\\\ H_1: \\mu_1 \\ne \\mu_2 \\end{align}\\] \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - 0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\\\ s_p &amp;= \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 -2}}\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;\\\\ t &amp;= \\frac{(58.05 - 57.57) - 0}{15.34 \\sqrt{\\frac{1}{3176} + \\frac{1}{2325}}}\\\\ &amp;= 1.15\\\\ s_p &amp;= \\sqrt{ \\frac{(3176-1)15.22^2 + (2325-1) 15.5^2}{3176 + 2325 -2}}\\\\ &amp;= 15.34\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;= 5499\\\\ \\mbox{p-value} &amp;= 2 \\cdot (1-pt(1.15,5499)) = 0.25\\\\ \\end{align}\\] The same analysis can be done in R (with and without tydying the output): surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) ## ## Welch Two Sample t-test ## ## data: age by month ## t = 1.1585, df = 4954.5, p-value = 0.2467 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3366687 1.3092918 ## sample estimates: ## mean in group Aug mean in group Jul ## 58.05220 57.56589 surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) %&gt;% tidy() ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.486 58.1 57.6 1.16 0.247 4954. -0.337 ## # … with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, ## # alternative &lt;chr&gt; Look at SD and SEM What is the statistic? What is the sampling distribution of the statistic? Why do we use the t-distribution? Why is the big p-value important? (It’s a good thing!) How do we interpret the p-value? What can we conclude? applet from (Chance and Rossman 2018): [http://www.rossmanchance.com/applets/SampleMeans/SampleMeans.html] What are the model assumptions? (independence between &amp; within groups, random sample, pop values don’t change, additive error, \\(\\epsilon_{i,j} \\ \\sim \\ iid \\ N(0, \\sigma^2)\\), … basically all the assumptions are given in the original linear model) Considerations when running a t-test: one-sample vs two-sample t-test one-sided vs. two-sided hypotheses t-test with unequal variance (less powerful, more conservative) \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - (\\mu_1 - \\mu_2)}{ \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\\\ df &amp;= \\min(n_1-1, n_2-1)\\\\ \\end{align}\\] two dependent (paired) samples – one sample t-test! Example 2 Assume we have two very small samples: (\\(y_{11}=3, y_{12} = 9, y_{21} = 5, y_{22}=1, y_{23}=9\\)). Find \\(\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\epsilon}_{11}, \\hat{\\epsilon}_{12}, \\hat{\\epsilon}_{21}, \\hat{\\epsilon}_{22}, \\hat{\\epsilon}_{23}, n_1, n_2\\). 2.1.1 What is an Alternative Hypothesis? Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: [https://www.youtube.com/watch?v=b-U_I1DCGEY] In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on. What is the point? Why watch the video? How does it relate the to the material from class? What is the relationship to sampling distributions? [Thanks to Ben Baumer at Smith College for the pointer to the specific video.] ANOVA Skip ANOVA in your text (2.4 and part of 2.9) 2.2 Simple Linear Regression (book: 2.3) Simple Linear Regression is a model (hopefully discussed in introductory statistics) used for describing a {linear} relationship between two variables. It typically has the form of: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i = 1, 2, \\ldots, n\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2)\\\\ E(Y|x) &amp;= \\beta_0 + \\beta_1 x \\end{align}\\] For this model, the deterministic component (\\(\\beta_0 + \\beta_1 x\\)) is a linear function of the two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and the explanatory variable \\(x\\). The random error terms, \\(\\epsilon_i\\), are assumed to be independent and to follow a normal distribution with mean 0 and variance \\(\\sigma^2\\). How can we use this model to describe the two sample means case we discussed on the esophageal data? Consider \\(x\\) to be a dummy variable that takes on the value 0 if the observation is a control and 1 if the observation is a case. Assume we have \\(n_1\\) controls and \\(n_2\\) cases. It turns out that, coded in this way, the regression model and the two-sample t-test model are mathematically equivalent! (For the color game, the natural way to code is 1 for the color distracter and 0 for the standard game. Why?) \\[\\begin{align} \\mu_1 &amp;= \\beta_0 + \\beta_1 (0) = \\beta_0 \\\\ \\mu_2 &amp;= \\beta_0 + \\beta_1 (1) = \\beta_0 + \\beta_1\\\\ \\mu_2 - \\mu_1 &amp;= \\beta_1 \\end{align}\\] Why are they the same? \\[\\begin{align} b_1= \\hat{\\beta}_1 &amp;= \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - (\\sum x_i )^2}\\\\ &amp;= \\frac{n \\sum_2 y_i - n_2 \\sum y_i}{(n n_2-n_2^2)}\\\\ &amp;= \\frac{ n \\sum_2 y_i - n_2 (\\sum_1 y_i + \\sum_2 y_i)}{n_2(n-n_2)}\\\\ &amp;= \\frac{(n_1 + n_2) \\sum_2 y_i - n_2 \\sum_1 y_i - n_2 \\sum_2 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 \\sum_2 y_i - n_2 \\sum_1 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 n_2 \\overline{y}_2 - n_2 n_1 \\overline{y}_1}{n_1 n_2}\\\\ &amp;= \\overline{y}_2 - \\overline{y}_1\\\\ b_0 = \\hat{\\beta}_0 &amp;= \\frac{\\sum y_i - b_1 \\sum x_i}{n}\\\\ &amp;= \\frac{\\sum_1 y_i + \\sum_2 y_i - b_1 n_2}{n}\\\\ &amp;= \\frac{n_1 \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1}{n} = \\overline{y}_1 \\end{align}\\] Definition 2.2 (Model 2) \\[\\begin{align} y_{i} &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i=1, 2, \\ldots, n\\\\ \\epsilon_{i} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\beta_0 + \\beta_1 x_i\\\\ \\hat{y}_i &amp;= b_0 + b_1 x_i \\end{align}\\] That is, we are assuming that for each observation the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean at their value for the explanatory variable, \\(x_i\\). Note that we have assumed that the variance is constant across any level of the explanatory variable. We have also assumed that there is independence across individuals. [Note: there are no assumptions about the distribution of the explanatory variable, \\(X\\)]. What are the similarities in the t-test vs. SLR models? predicting average assuming independent, constant errors errors follow a normal distribution with zero mean and variance \\(\\sigma^2\\) What are the differences in the two models? one subscript versus two (or similarly, two models for the t-test) two samples for the t-test (two variables for the regression… or is that a similarity??) both variables are quantitative in SLR 2.3 Confidence Intervals (section 2.11) [http://www.rossmanchance.com/applets/NewConfsim/Confsim.html] In general, the format of a confidence interval is (INTERPRETATION!!!!): estimate +/- critical value x standard error of the estimate Age data: \\[\\begin{align} 90\\% \\mbox{ CI for } \\mu_1: &amp; \\overline{y}_1 \\pm t^*_{3176-1} \\times \\hat{\\sigma}_{\\overline{y}_1}\\\\ &amp; 58.05 \\pm 1.645 \\times 15.22/\\sqrt{3176}\\\\ &amp; (57.61, 58.49)\\\\ 95\\% \\mbox{ CI for }\\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{5499} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 0.48 \\pm 1.96 \\times 0.42\\\\ &amp; (-0.34, 1.30) \\end{align}\\] Note the CI on pgs 54/55, there is a typo. The correct interval for \\(\\mu_1 - \\mu_2\\) for the games data should be: \\[\\begin{align} 95\\% \\mbox{ CI for } \\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} \\hat{\\sigma}_{\\overline{y}_1 - \\overline{y}_2}\\\\ &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 38.1 - 35.55 \\pm 2.02 \\times \\sqrt{\\frac{(19)3.65^2 + (19)3.39^2}{20+20-2}} \\sqrt{\\frac{1}{20} + \\frac{1}{20}}\\\\ &amp; (0.29 s, 4.81 s) \\end{align}\\] 2.4 Random Sample vs. Random allocation Recall what you’ve learned about how good random samples lead to inference about a population. On the other hand, in order to make a causal conclusion, you need a randomized experiment with random allocation of the treatments (impossible to happen in many settings). Random sampling and random allocation are DIFFERENT ideas that should be clear in your mind. Figure taken from (Chance and Rossman 2018) Note: no ANOVA (section 2.4) or normal probability plots (section 2.8) References "],
["references.html", "References", " References "]
]
