[
["index.html", "Computational Statistics Class Information", " Computational Statistics Jo Hardin 2019-09-30 Class Information Class notes for Math 154 at Pomona College: Methods in Biostatistics. The notes are based extensively on An Introduction to Statistical Learning (James et al. 2013) by James, Witten, Hastie, and Tibshiran; Modern Data Science (Baumer, Kaplan, and Horton 2017) with R by Baumer, Kaplan, and Horton; Data Science in R: A Case Studies Approach to Computational Reasoning and Problem Solving (Nolan and Temple Lang 2015) by Nolan and Temple Lang; and Visual and Statistical Thinking: Displays of Evidence for Making Decisions (Tufte 1997) by Tufte. You are responsible for reading the relevant chapters in the text. The texts are very good &amp; readable, so you should use them. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chap Notes Section 9/3/19 Intro to Data / R ISL1 1 [intro], 1.1 [Sep3] 9/5/19 Intro to GitHub 1.4 [Sep5], 1.5 [repro] 9/10/19 Data Viz VST 2.1 [Sep10], 2.3 [thoughts] 9/12/19 MDS2 2.4 [Sep12], 2.5 [decon], 2.5.1 [gg] 9/17/19 Data Wrangling MDS4 3.1 [Sep17], 3.2 [structure] 9/19/19 3.1 [Sep17], 3.5 [highverb] 9/24/19 Simulating 4.1 [Sep24], 4.2 [sim models] 9/26/19 4.3 [Sep26], 4.4 [sim sens] 10/1/19 Permutations 5.1 [Oct1], 5.2 [inf algs] 10/3/19 5.3 [Oct3], 5.4 [perm test] References "],
["intro.html", "Chapter 1 Introduction 1.1 9/3/19 Agenda 1.2 Course Logistics 1.3 Course Content 1.4 9/5/19 Agenda 1.5 Reproducibility 1.6 Data Examples", " Chapter 1 Introduction 1.1 9/3/19 Agenda Questionnaire Syllabus &amp; Course Outline Stitch Fix Algorithm College Rankings Can Twitter predict election results? 1.2 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Figure 1.1: Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population What is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years. Who should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science. What are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Is there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability? When should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors. What is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class. What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Figure 1.2: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.3: Jessica Ward, PhD student at Newcastle University 1.3 Course Content 1.3.1 Topics Computational Statistics can be a very large umbrella for many ideas. Indeed, sometimes the topics can seem somewhat disjointed. Below, I’ve categorized the topics we will cover into four groups. The four different broad topics all play different roles and can be more or less important depending on the problem at hand. None of the topics should exist on their own, because only with the bigger focus on all topics will any sort of data analysis / interpretation be accurate and compelling. Letting the computer help: R, RStudio, GitHub, Reproducibility, Data Viz, Data Wrangling Statistics: Simulating, Randomization / Permutation Tests, Bootstrapping, Ethics Machine Learning: Classification, Clustering, Regular Expressions Mathematics: Support Vector Machines 1.3.2 Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. A parameter is a numerical measurement of the population. We never know the true value of the parameter. An estimator is a function of the unobserved data that tries to approximate the unknown parameter value. An estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.] One of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015) With thanks to Ben Baumer for perspective and sharing course materials. 1.3.3 The Workflow Figure 1.4: A schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015) Figure 1.5: Stitch Fix Algorithms Tour 1.3.4 Principles for the Data Science Process tl;dr (Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.) Duncan Temple Lang, University of California, Davis Duncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.] Duncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science. Temple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process: Asking a general question. Refining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood. Access to data. This is unrelated to the science but does require computational skill. Transforming to data structures. Exploratory data analyses to understand the data and determine whether the results will scale. This is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results. Temple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student: Mapping the general question to a statistical framework. Understanding the scope of inference, sampling, biases, and limitations. Exploratory data analyses, including missing values, data quality, cleaning, matching, and fusing. Understanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability. Conditional dependence and heterogeneity. Dimension reduction, variable selection, and sparsity. Spurious relationships and multiple testing. Parameter estimation versus “black box” prediction and classification. Diagnostics, residuals, and comparing models. Quantifying the uncertainty of a model. Sampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed. Statistical accuracy versus computational complexity and efficiency. Temple Lang then briefly discussed some of the practical aspects of computing, including the following: Accessing data. Manipulating raw data. Data structures and storage, including correlated data. Visualization at all stages (particularly in exploratory data analyses and conveying the results). Parallel computing, which can be challenging for a new student. Translating high-level descriptions to optimal programs. During the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).] A workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets. 1.4 9/5/19 Agenda Design Challenge Not So Standard Deviations Reproducibility &amp; GitHub 1.5 Reproducibility 1.5.1 Need for Reproducibility Figure 1.6: slide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016 Example 1 Science retracts gay marriage paper without agreement of lead author LaCour In May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior. Science Editor-in-Chief Marcia McNutt: Original survey data not made available for independent reproduction of results. Survey incentives misrepresented. Sponsorship statement false. Two Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked. Methods we’ll discuss can’t prevent this, but they can make it easier to discover issues. Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent Example 2 Seizure study retracted after authors realize data got “terribly mixed” From the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates: The article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness. Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/ Example 3 Bad spreadsheet merge kills depression paper, quick fix resurrects it The authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct. Original conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression … Revised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression … Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/ Example 4 PNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/ 1.5.2 The reproducible data analysis process Scriptability \\(\\rightarrow\\) R Literate programming \\(\\rightarrow\\) R Markdown Version control \\(\\rightarrow\\) Git / GitHub Scripting and literate programming Donald Knuth “Literate Programming” (1983) Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do. The ideas of literate programming have been around for many years! and tools for putting them to practice have also been around but they have never been as accessible as the current tools Reproducibility checklist Are the tables and figures reproducible from the code and data? Does the code actually do what you think it does? In addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?) Can the code be used for other data? Can you extend the code to do other things? Tools: R &amp; R Studio See this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&amp;feature=youtu.be You must use both R and RStudio software programs R does the programming R Studio brings everything together You may use Pomona’s server: https://rstudio.pomona.edu/ See course website for getting started: http://research.pomona.edu/johardin/math154f19/ Figure 1.7: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.8: Jessica Ward, PhD student at Newcastle University Tools: GitHub You must submit your assignments via GitHub Follow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/ Follow Jacob Fiksel’s advice on how to connect to our classroom: https://github.com/jfiksel/github-classroom-for-students Tools: a GitHub merge conflict (demo) On GitHub (on the web) edit the README document and Commit it with a message describing what you did. Then, in RStudio also edit the README document with a different change. Commit your changes Try to push \\(\\rightarrow\\) you’ll get an error! Try pulling Resolve the merge conflict and then commit and push As you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important. Figure 1.9: https://xkcd.com/1597/ Steps for weekly homework You will get a link to the new assignment (clicking on the link will create a new private repo) Use R Studio New Project, version control, Git Clone the repo using SSH If it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd Do the assignment commit and push after every problem All necessary files must be in the same folder (e.g., data) 1.6 Data Examples What can/can’t Data Science Do? Can model the data at hand! Can find patterns &amp; visualizations in large datasets. Can’t establish causation. Can’t represent data if it isn’t there. Stats / Data Science / Math are not apolitical/agnostic “Inner city crime is reaching record levels” (Donald Trump, 8/30/16) “The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16) “Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16) “We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16) “If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16) “Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16) Source: http://www.politifact.com/truth-o-meter/statements/ 1.6.1 College Rankings Systems Cheating Bucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html) Gaming the system Point by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/) No way to measure “quality of education” What is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!). 1.6.2 Trump and Twitter Analysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone. Aug 9, 2016 http://varianceexplained.org/r/trump-tweets/ My analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures. Aug 9, 2017 http://varianceexplained.org/r/trump-followup/ There is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post. I’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it. I’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement. 1.6.3 Can Twitter Predict Election Results? In 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013) What makes using Tweets to predict elections relevant to our class? (See Baumer (2015).) The data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet. The analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets). The dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed. The researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses. Activity Spend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions. working paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423 published in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449. editorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html editorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html editorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/ Statistics Hat Write a sentence summarizing the findings of the paper. Discuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently? should be proportion for the response variable. The bizarre scaling could dramatically change the results dots could then be scaled in proportion to the number of tweets linear fit may be questionable. How would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement? Interpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units. Discuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election? \\(R^2\\) is way higher after control variables are included, but duh! the full model will likely do a better job of predicting Why do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election? After controlling for how many Republicans are in the district, most of the effect disappears While the coefficient of the main term is still statistically significant, the size of the coefficient (155 +/- 43 votes) is of little practical significance Do you think the study holds water? Why or why not? What are the shortcomings of this study? Not really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model? Data Scientist Hat Imagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor. What steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform. What computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities &amp; knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in? How much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb. Advantages Cheap Can measure any political race (not just the wealthy ones). Disadvantages Is it really reflective of the voting populace? Who would it bias toward? Does simple mention of a candidate always reflect voting patterns? When wouldn’t it? Margin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\). Tweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling? Can’t measure any demographic information. What could be done differently? Gelman: look only at close races Gelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.” Gelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.” Gelman: Make the data publicly available so that others can try to reproduce the results Tweeting and R The twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions. References "],
["visualization.html", "Chapter 2 Visualization 2.1 9/10/19 Agenda 2.2 Examples 2.3 Thoughts on Plotting 2.4 9/12/19 Agenda 2.5 Deconstructing a graph", " Chapter 2 Visualization Data visualization is integral to understanding both data and models. Computational statistics and data science sometimes focus on models or resulting predictions from the models. But there is no doubt that the structure and format of the data are the key to whether or not a model is appropriate or good. A good data analyst will always spend a lot of time and effort on exploratory data analysis, much of which includes making as many visualizations of the data as possible. 2.1 9/10/19 Agenda Cholera: what went (didn’t go) well with the graphics? Challenger: what didn’t go (went) well with the graphics? Thoughts on plotting (with example(s)) Should have read: Tufte (1997) One more great reference is the following text: Fundamentals of Data Visualization by Wilke at http://serialmentor.com/dataviz/ Depending on the introductory (or other) statistics classes you’ve had, your instructor may have focused more or less on visualizations in class. They (I) may have even said something like making visualizations are incredibly important to the entire data analysis process. But even if you buy the perspective, why is it that we don’t see more good graphics in our analyses? Andrew Gelman (Gelman 2011) responds by stating, “Good statistical graphics are hard to do, much harder than running regressions and making tables.” Our goal will be to create graphics and visualizations that convey statistical information. Nolan (Nolan and Perrett 2016) describes three important ways that graphics can be used to convey statistical information. The “guiding principles” will be used as a way of evaluating others’ figures as well as a metric for creating our own visualizations to help with statistical analysis. Make the data stand out The important idea here is to find anything unusual in the data. Are there patterns? Outliers? What are the bounds of the variables? How should the axes be scaled? Are transformations warranted? Facilitate comparison The second item allows us to consider the research questions at hand. What are the important variables? How do we emphasize them? Which variables should be plotted together? Can they be super-imposed? Does color, plotting character, size of plot character help to bring out the important relationships? Be aware of over plotting and issues of color blindness! http://colorbrewer2.org/ Add information Plots should also add context to the comparison. Figure legends, axes scales, and reference markers (e.g., a line at \\(y=x\\)) go a long way toward helping the reader understand your message. Captions should be self-contained (and not assume the user has also read your text) and descriptive; they should summarize the content of the figure and the conclusion related to the message you want to convey. Randy Pruim asks the following question to decide whether or not a plot is good: Does my plot make the comparisons I am interested in… easily? and accurately? 2.2 Examples The first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased for $7 at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download. An aside Generally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi As mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (E. Tufte 1997). “An essential analytic task in making decisions based on evidence is to understand how things work.” Making decisions based on evidence requires the appropriate display of that evidence.&quot; Back to the examples… 2.2.1 Cholera via Tufte In September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after. Why was John Snow successful at solving the problem? Some thoughts to consider (as reported in E. Tufte (1997)): The bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means. Data in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area. Comparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease. Alternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case. Did removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline? Assessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told? 2.2.2 Challenger via Tufte John Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F). Unlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated}! The biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what?? The engineers who understood the problem created tables and engineering graphs which were Not visually appealing. Not decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”) There was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility. The information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure. Missing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.) Anecdotal evidence was misconstrued: SRM-15 at 57F at the most damage, but SRM-22 at 75F had the second most damage. In the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F. Tufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order. As with the cholera outbreak, a persuasive argument could have been made if the visualizations had been in context plot data versus temperature not time!, used appropriate comparisons: as compared with what?, consider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and the graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?. Tufte (E. Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms. Figure 1.2: The graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (E. Tufte 1997) 2.3 Thoughts on Plotting 2.3.1 Advice Basic plotting Avoid having other graph elements interfere with data Use visually prominent symbols Avoid over-plotting (One way to avoid over plotting: Jitter the values) Different values of data may obscure each other Include all or nearly all of the data Fill data region Eliminate superfluous material Chart junk &amp; stuff that adds no meaning, e.g. butterflies on top of barplots, background images Extra tick marks and grid lines Unnecessary text and arrows Decimal places beyond the measurement error or the level of difference Facilitate Comparisons Put juxtaposed plots on same scale Make it easy to distinguish elements of superposed plots (e.g. color) Emphasizes the important difference Comparison: volume, area, height (be careful, volume can seem bigger than you mean it to) Choosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.) Keep scales on x and y axes the same for both plots to facilitate the comparison Zoom in to focus on the region that contains the bulk of the data Keep the scale the same throughout the plot (i.e. don’t change it mid-axis) Origin need not be on the scale Choose a scale that improves resolution Avoid jiggling the baseline How to make a plot information rich Describe what you see in the caption Add context with reference markers (lines and points) including text Add legends and labels Use color and plotting symbols to add more information Plot the same thing more than once in different ways/scales Reduce clutter Captions should Be comprehensive Self-contained Describe what has been graphed Draw attention to important features Describe conclusions drawn from graph Good Plot Making Practice Put major conclusions in graphical form Provide reference information Proof read for clarity and consistency Graphing is an iterative process Multiplicity is OK, i.e. two plots of the same variable may provide different messages Make plots data rich Creating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data. 2.3.2 An example from Information is Beautiful (See HW2 for details on R code) Consider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph. Figure 1.3: http://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png I have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations. Figure 1.4: Calories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot? Data retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0 2.3.3 Assessing Graphics (and Other Analyses) Critical Task Needs Improvement Basic Surpassed Computation Perform computations Computations contain errors and extraneous code Computations are correct but contain extraneous / unnecessary computations Computations are correct and properly identified and labeled Analysis Choose and carry out analysis appropriate for data and content(s) Choice of analysis is overly simplistic, irrelevant, or missing key component Analysis appropriate, but incomplete, or not important features and assumptions not made explicit Analysis appropriate, complete, advanced, relevant, and informative Synthesis Identify key features of the analysis, and interpret results (including context) Conclusions are missing, incorrect, or not made based on results of analysis Conclusions reasonable, but is partially correct or partially complete Make relevant conclusions explicitly connect to analysis and to context Visual presentation Communicate findings graphically clearly, precisely, and concisely Inappropriate choice of plots; poorly labeled plots; plots missing Plots convey information correctly but lack context for interpretation Plots convey information correctly with adequate / appropriate reference information Written Communicate findings clearly, precisely, and concisely Explanation is illogical, incorrect, or incoherent Explanation is partially correct but incomplete or unconvincing Explanation is correct, complete, and convincing A rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016). 2.4 9/12/19 Agenda Grammar of graphics ggplot 2.5 Deconstructing a graph 2.5.1 The Grammar of Graphics (gg) Yau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence. One great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?) The most important questions you can ask with respect to creating figures are: What do we want R to do? (What is the goal?) What does R need to know? Yau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2017), chapter 2.) Visual Cues: the aspects of the figure where we should focus. Position (numerical) where in relation to other things? Length (numerical) how big (in one dimension)? Angle (numerical) how wide? parallel to something else? Direction (numerical) at what slope? In a time series, going up or down? Shape (categorical) belonging to what group? Area (numerical) how big (in two dimensions)? Beware of improper scaling! Volume (numerical) how big (in three dimensions)? Beware of improper scaling! Shade (either) to what extent? how severely? Color (either) to what extent? how severely? Beware of red/green color blindness. Coordinate System: rectangular, polar, geographic, etc. Scale: numeric (linear? logarithmic?), categorical (ordered?), time Context: in comparison to what (think back to ideas from Tufte) What are the visual cues on the plot? position length shape area/volume shade/color Coordinate System? Scale? What are the visual cues on the plot? position length shape area/volume shade/color Coordinate System? Scale? What are the visual cues on the plot? position length shape area/volume shade/color Coordinate System? Scale? 2.5.1.1 The grammar of graphics in ggplot2 geom: the geometric “shape” used to display data bar, point, line, ribbon, text, etc. aesthetic: an attribute controlling how geom is displayed with respect to variables x position, y position, color, fill, shape, size, etc. scale: adjust information in the aesthetic to map onto the plot particular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values. guide: helps user convert visual data back into raw data (legends, axes) stat: a transformation applied to data before geom gets it example: histograms work on binned data 2.5.2 ggplot2 In ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2017). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births. Goals What I will try to do give a tour of ggplot2 explain how to think about plots the ggplot2 way prepare/encourage you to learn more later What I can’t do in one session show every bell and whistle make you an expert at using ggplot2 Getting help One of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/ ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Look at the end of the presentation. More help options there. require(mosaic) require(lubridate) # package for working with dates data(Births78) # restore fresh version of Births78 head(Births78, 3) ## date births wday year month day_of_year day_of_month day_of_week ## 1 1978-01-01 7701 Sun 1978 1 1 1 1 ## 2 1978-01-02 7527 Mon 1978 1 2 2 2 ## 3 1978-01-03 8825 Tue 1978 1 3 3 3 How can we make the plot? Two Questions: What do we want R to do? (What is the goal?) What does R need to know? data source: Births78 aesthetics: date -&gt; x births -&gt; y points (!) Goal: scatterplot = a plot with points ggplot() + geom_point() What does R need to know? data source: data = Births78 aesthetics: aes(x = date, y = births) How can we make the plot? What has changed? new aesthetic: mapping color to day of week Adding day of week to the data set The wday() function in the lubridate package computes the day of the week from a date. Births78 &lt;- Births78 %&gt;% mutate(wday = lubridate::wday(date, label=TRUE)) ggplot(data=Births78) + geom_point(aes(x=date, y=births, color=wday))+ ggtitle(&quot;US Births in 1978&quot;) How can we make the plot? Now we use lines instead of dots ggplot(data=Births78) + geom_line(aes(x=date, y=births, color=wday)) + ggtitle(&quot;US Births in 1978&quot;) How can we make the plot? Now we have two layers, one with points and one with lines ggplot(data=Births78, aes(x=date, y=births, color=wday)) + geom_point() + geom_line()+ ggtitle(&quot;US Births in 1978&quot;) The layers are placed one on top of the other: the points are below and the lines are above. data and aes specified in ggplot() affect all geoms Alternative Syntax Births78 %&gt;% ggplot(aes(x=date, y=births, color=wday)) + geom_point() + geom_line()+ ggtitle(&quot;US Births in 1978&quot;) What does adding the color argument do? Births78 %&gt;% ggplot(aes(x=date, y=births, color=&quot;navy&quot;)) + geom_point() + ggtitle(&quot;US Births in 1978&quot;) Because there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy. Setting vs. Mapping If we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable: Births78 %&gt;% ggplot(aes(x=date, y=births)) + # map x &amp; y geom_point(color = &quot;navy&quot;) + # set color ggtitle(&quot;US Births in 1978&quot;) Note that color = &quot;navy&quot; is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting. How can we make the plot? Births78 %&gt;% ggplot(aes(x=date, y=births)) + geom_line(aes(color=wday)) + # map color here geom_point(color=&quot;navy&quot;) + # set color here ggtitle(&quot;US Births in 1978&quot;) ggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults. good practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah() Setting vs. Mapping (again) Information gets passed to the plot via: map the variable information inside the aes (aesthetic) command set the non-variable information outside the aes (aesthetic) command Other geoms apropos(&quot;^geom_&quot;) [1] &quot;geom_abline&quot; &quot;geom_area&quot; &quot;geom_ash&quot; [4] &quot;geom_bar&quot; &quot;geom_barh&quot; &quot;geom_bin2d&quot; [7] &quot;geom_blank&quot; &quot;geom_boxplot&quot; &quot;geom_boxploth&quot; [10] &quot;geom_col&quot; &quot;geom_colh&quot; &quot;geom_contour&quot; [13] &quot;geom_count&quot; &quot;geom_crossbar&quot; &quot;geom_crossbarh&quot; [16] &quot;geom_curve&quot; &quot;geom_density&quot; &quot;geom_density_2d&quot; [19] &quot;geom_density2d&quot; &quot;geom_dotplot&quot; &quot;geom_errorbar&quot; [22] &quot;geom_errorbarh&quot; &quot;geom_errorbarh&quot; &quot;geom_freqpoly&quot; [25] &quot;geom_hex&quot; &quot;geom_histogram&quot; &quot;geom_histogramh&quot; [28] &quot;geom_hline&quot; &quot;geom_jitter&quot; &quot;geom_label&quot; [31] &quot;geom_line&quot; &quot;geom_linerange&quot; &quot;geom_linerangeh&quot; [34] &quot;geom_lm&quot; &quot;geom_map&quot; &quot;geom_path&quot; [37] &quot;geom_point&quot; &quot;geom_pointrange&quot; &quot;geom_pointrangeh&quot; [40] &quot;geom_polygon&quot; &quot;geom_qq&quot; &quot;geom_qq_line&quot; [43] &quot;geom_quantile&quot; &quot;geom_raster&quot; &quot;geom_rect&quot; [46] &quot;geom_ribbon&quot; &quot;geom_rug&quot; &quot;geom_segment&quot; [49] &quot;geom_sf&quot; &quot;geom_sf_label&quot; &quot;geom_sf_text&quot; [52] &quot;geom_smooth&quot; &quot;geom_spline&quot; &quot;geom_spoke&quot; [55] &quot;geom_step&quot; &quot;geom_text&quot; &quot;geom_tile&quot; [58] &quot;geom_violin&quot; &quot;geom_violinh&quot; &quot;geom_vline&quot; help pages will tell you their aesthetics, default stats, etc. ?geom_area # for example Let’s try geom_area Births78 %&gt;% ggplot(aes(x=date, y=births, fill=wday)) + geom_area()+ ggtitle(&quot;US Births in 1978&quot;) Using area does not produce a good plot over plotting is hiding much of the data extending y-axis to 0 may or may not be desirable. Side note: what makes a plot good? Most (all?) graphics are intended to help us make comparisons How does something change over time? Do my treatments matter? How much? Do men and women respond the same way? Key plot metric: Does my plot make the comparisons I am interested in easily, and accurately? Time for some different data HELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial head(HELPrct) ## age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female ## 1 37 1 yes 49 3 177 225 0 NA 0 ## 2 37 1 yes 30 22 2 NA 0 NA 0 ## 3 26 1 yes 39 0 3 365 20 NA 0 ## 4 39 1 yes 15 2 189 343 0 1 1 ## 5 32 1 yes 39 12 2 57 0 1 0 ## 6 47 1 yes 6 1 31 365 0 NA 1 ## sex g1b homeless i1 i2 id indtot linkstatus link mcs pcs ## 1 male yes housed 13 26 1 39 1 yes 25.111990 58.41369 ## 2 male yes homeless 56 62 2 43 NA &lt;NA&gt; 26.670307 36.03694 ## 3 male no housed 0 0 3 41 0 no 6.762923 74.80633 ## 4 female no housed 5 5 4 28 0 no 43.967880 61.93168 ## 5 male no homeless 10 13 5 38 1 yes 21.675755 37.34558 ## 6 female no housed 4 4 6 29 0 no 55.508991 46.47521 ## pss_fr racegrp satreat sexrisk substance treat avg_drinks max_drinks ## 1 0 black no 4 cocaine yes 13 26 ## 2 1 white no 7 alcohol yes 56 62 ## 3 13 black no 2 heroin no 0 0 ## 4 11 white yes 4 heroin no 5 5 ## 5 10 black no 6 cocaine no 10 13 ## 6 5 black no 5 cocaine yes 4 4 Subjects admitted for treatment for addiction to one of three substances. Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance)) + geom_bar()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Hmm. What’s up with y? stat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values. Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Who are the people in the study? library(scales) HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar() + scale_y_continuous(labels = percent)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Who are the people in the study? HELPrct %&gt;% ggplot(aes(x=substance, fill=sex)) + geom_bar(position=&quot;fill&quot;) + scale_y_continuous(&quot;actually, percent&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How old are people in the HELP study? HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notice the messages stat_bin: Histograms are not mapping the raw data but binned data. stat_bin() performs the data transformation. binwidth: a default binwidth has been selected, but we should really choose our own. Setting the binwidth manually HELPrct %&gt;% ggplot(aes(x=age)) + geom_histogram(binwidth=2)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How old are people in the HELP study? – Other geoms HELPrct %&gt;% ggplot(aes(x=age)) + geom_freqpoly(binwidth=2)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_density()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Selecting stat and geom manually Every geom comes with a default stat for simple cases, the stat is stat_identity() which does nothing we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + geom_line(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Selecting stat and geom manually Every stat comes with a default geom, every geom with a default stat we can specify stats instead of geom, if we prefer we can mix and match geoms and stats however we like HELPrct %&gt;% ggplot(aes(x=age)) + stat_density( geom=&quot;line&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) More combinations HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3) + geom_line(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_area(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=age)) + geom_point(stat=&quot;bin&quot;, binwidth=3, aes(size=..count..)) + geom_line(stat=&quot;bin&quot;, binwidth=3) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) How much do they drink? (i1) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_histogram()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_density()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1)) + geom_area(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Covariates: Adding in more variables Using color and linetype: HELPrct %&gt;% ggplot(aes(x=i1, color=substance, linetype=sex)) + geom_line(stat=&quot;density&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Using color and facets HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( . ~ sex )+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) HELPrct %&gt;% ggplot(aes(x=i1, color=substance)) + geom_line(stat=&quot;density&quot;) + facet_grid( sex ~ . )+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Boxplots Boxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”. The quantitative variable must be y, and there must be an additional x variable. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Horizontal boxplots Horizontal boxplots are obtained by flipping the coordinate system: HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) coord_flip() may be used with other plots as well to reverse the roles of x and y on the plot. Axes scaling with boxplots We can scale the continuous axis HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot() + coord_trans(y=&quot;log&quot;)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Give me some space We’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually. HELPrct %&gt;% ggplot(aes(x=substance, y=age, color=sex)) + geom_boxplot(position=position_dodge(width=1)) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Issues with bigger data require(NHANES) dim(NHANES) ## [1] 10000 76 NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) Although we can see a generally positive association (as we would expect), the over plotting may be hiding information. Using alpha (opacity) One way to deal with over plotting is to set the opacity low. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) geom_density2d Alternatively (or simultaneously) we might prefer a different geom altogether. NHANES %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_density2d() + facet_grid( Gender ~ PregnantNow ) + ggtitle(&quot;National Health and Nutrition Examination Survey&quot;) Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_jitter(alpha=.6) + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Multiple layers ggplot( data=HELPrct, aes(x=sex, y=age)) + geom_boxplot(outlier.size=0) + geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) + coord_flip()+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Things I haven’t mentioned (much) coords (coord_flip() is good to know about) themes (for customizing appearance) position (position_dodge(), position_jitterdodge(), position_stack(), etc.) transforming axes require(ggthemes) ggplot(Births78, aes(x=date, y=births)) + geom_point() + theme_wsj() ggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge()) + geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) + ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) A little bit of everything ggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) + geom_boxplot(coef = 10, position=position_dodge(width=1)) + geom_point(aes(fill=sex), alpha=.5, position=position_jitterdodge(dodge.width=1)) + facet_wrap(~homeless)+ ggtitle(&quot;HELP clinical trial at detoxification unit&quot;) Want to learn more? docs.ggplot2.org/ Winston Chang’s: R Graphics Cookbook What else can we do? shiny interactive graphics / modeling https://shiny.rstudio.com/ plotly Plotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function. https://plot.ly/ggplot2/getting-started/ Dynamic documents combination of RMarkdown, ggvis, and shiny References "],
["wrang.html", "Chapter 3 Data Wrangling 3.1 9/17/19 Agenda 3.2 Structure of Data 3.3 R examples, basic verbs 3.4 9/19/19 Agenda 3.5 Higher Level Data Verbs 3.6 R examples, higher level verbs 3.7 reprex", " Chapter 3 Data Wrangling As with data visualization, data wrangling is a fundamental part of being able to accurately, reproducibly, and efficiently work with data. The approach taken in the following chapter is based on the philosophy of tidy data and takes many of its precepts from database theory. If you have done much work in SQL, the functionality and approach of tidy data will feel very familiar. The more adept you are at data wrangling, the more effective you will be at data analysis. 3.1 9/17/19 Agenda Tidy Data (structure of data) Piping / chaining Basic data Verbs babynames / NHANES examples Information is what we want, but data are what we’ve got. (Kaplan 2015) Embrace all the ways to get help! cheat sheets: https://www.rstudio.com/resources/cheatsheets/ tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/ pivoting: https://tidyr.tidyverse.org/articles/pivot.html google what you need and include R tidy or tidyverse 3.2 Structure of Data For plotting, analyses, model building, etc., it’s important that the data be structured in a very particular way. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham (2014). Tidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and *every} column is a variable. No exceptions. Creating tidy data is not trivial. We work with objects (often data tables), functions, and arguments (often variables). The Active Duty data are not tidy! What are the cases? How are the data not tidy? What might the data look like in tidy form? Suppose that the case was “an individual in the armed forces.” What variables would you use to capture the information in the following table? https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794 Problem: totals and different sheets Better for R: longer format with columns - grade, gender, status, service, count (case is still the total pay grade) Case is individual (?): grade, gender, status, service (no count because each row does the counting) 3.2.1 Building Tidy Data Within R (really within any type of computing language, Python, SQL, Java, etc.), we need to understand how to build data using the patterns of the language. Some things to consider: object_name = function_name(arguments) is a way of using a function to create a new object. object_name = data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side. object_name = data_table %&gt;% function_name(arguments) %&gt;% function_name(arguments) is extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%. 3.2.2 Examples of Chaining The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) 3.2.2.1 Little Bunny Foo Foo From Hadley Wickham, how to think about tidy data. Little bunny Foo Foo Went hopping through the forest Scooping up the field mice And bopping them on the head The nursery rhyme could be created by a series of steps where the output from each step is saved as an object along the way. foo_foo &lt;- little_bunny() foo_foo_1 &lt;- hop(foo_foo, through = forest) foo_foo_2 &lt;- scoop(foo_foo_2, up = field_mice) foo_foo_3 &lt;- bop(foo_foo_2, on = head) Another approach is to concatenate the functions so that there is only one output. bop( scoop( hop(foo_foo, through = forest), up = field_mice), on = head) Or even worse, as one line: bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head))) Instead, the code can be written using the pipe in the order in which the function is evaluated: foo_foo %&gt;% hop(through = forest) %&gt;% scoop(up = field_mice) %&gt;% bop(on = head) babynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys. The babynames data table in the babynames package comes from the Social Security Administration’s listing of the names givens to babies in each year, and the number of babies of each sex given that name. (Only names with 5 or more babies are published by the SSA.) 3.2.3 Data Verbs (on single data frames) Super important resource: The RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf Data verbs take data tables as input and give data tables as output (that’s how we can use the chaining syntax!). We will use the R package dplyr to do much of our data wrangling. Below is a list of verbs which will be helpful in wrangling many different types of data. See the Data Wrangling cheat sheet from RStudio for additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf} sample_n() take a random row(s) head() grab the first few rows tail() grab the last few rows filter() removes unwanted *cases} arrange() reorders the cases select() removes unwanted *variables} (and rename() ) distinct() returns the unique values in a table mutate() transforms the variable (and transmute() like mutate, returns only new variables) group_by() group_by tells R that SUCCESSIVE functions keep in mind that there are groups of items. So group_by only makes sense with variables later on (like summarize). summarize() collapses a data frame to a single row. Not useful yet (will be useful with group_by()). Some functions that are used within summarize() include: \\begin{itemize} min(), max(), mean(), sum(), sd(), median(), and IQR() n(): number of observations in the current group n_distinct(x): count the number of unique values in x first_value(x), last_value(x) and nth_value(x, n): work similarly to x[1], x[length(x)], and x[n] 3.3 R examples, basic verbs 3.3.1 Datasets starwars is from dplyr , although originally from SWAPI, the Star Wars API, http://swapi.co/. NHANES From ?NHANES: NHANES is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC). babynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys. (Only names with 5 or more babies are published by the SSA.) 3.3.2 Examples of Chaining library(babynames) babynames %&gt;% nrow() ## [1] 1924665 babynames %&gt;% names() ## [1] &quot;year&quot; &quot;sex&quot; &quot;name&quot; &quot;n&quot; &quot;prop&quot; babynames %&gt;% glimpse() ## Observations: 1,924,665 ## Variables: 5 ## $ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,… ## $ sex &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;,… ## $ name &lt;chr&gt; &quot;Mary&quot;, &quot;Anna&quot;, &quot;Emma&quot;, &quot;Elizabeth&quot;, &quot;Minnie&quot;, &quot;Margaret&quot;, … ## $ n &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288,… ## $ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843,… babynames %&gt;% head() ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 babynames %&gt;% tail() ## # A tibble: 6 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2017 M Zyhier 5 0.00000255 ## 2 2017 M Zykai 5 0.00000255 ## 3 2017 M Zykeem 5 0.00000255 ## 4 2017 M Zylin 5 0.00000255 ## 5 2017 M Zylis 5 0.00000255 ## 6 2017 M Zyrie 5 0.00000255 babynames %&gt;% sample_n(size=5) ## # A tibble: 5 x 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1997 M Jaqwon 5 0.0000025 ## 2 1988 F Jamera 7 0.00000364 ## 3 2001 M Othon 6 0.0000029 ## 4 2001 M Dean 744 0.000360 ## 5 2006 F Ngoc 8 0.00000383 babynames %&gt;% mosaic::favstats(n ~ sex, data = .) ## sex min Q1 median Q3 max mean sd n missing ## 1 F 5 7 11 31 99686 151.4294 1180.557 1138293 0 ## 2 M 5 7 12 33 94756 223.4940 1932.338 786372 0 3.3.3 Data Verbs Taken from the dplyr tutorial: http://dplyr.tidyverse.org/ 3.3.3.1 Starwars library(dplyr) starwars %&gt;% dim() ## [1] 87 13 starwars %&gt;% names() ## [1] &quot;name&quot; &quot;height&quot; &quot;mass&quot; &quot;hair_color&quot; &quot;skin_color&quot; ## [6] &quot;eye_color&quot; &quot;birth_year&quot; &quot;gender&quot; &quot;homeworld&quot; &quot;species&quot; ## [11] &quot;films&quot; &quot;vehicles&quot; &quot;starships&quot; starwars %&gt;% head() ## # A tibble: 6 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Leia… 150 49 brown light brown 19 female ## 6 Owen… 178 120 brown, gr… light blue 52 male ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% mosaic::favstats(mass~gender, data = .) ## gender min Q1 median Q3 max mean sd n ## 1 female 45 49.25 52.5 55.90 75 54.02000 8.37215 10 ## 2 hermaphrodite 1358 1358.00 1358.0 1358.00 1358 1358.00000 NA 1 ## 3 male 15 76.50 80.0 87.25 159 81.00455 28.22371 44 ## 4 none 140 140.00 140.0 140.00 140 140.00000 NA 1 ## missing ## 1 9 ## 2 0 ## 3 18 ## 4 1 starwars %&gt;% dplyr::filter(species == &quot;Droid&quot;) ## # A tibble: 5 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 &lt;NA&gt; ## 2 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 &lt;NA&gt; ## 3 R5-D4 97 32 &lt;NA&gt; white, red red NA &lt;NA&gt; ## 4 IG-88 200 140 none metal red 15 none ## 5 BB8 NA NA none none black NA none ## # … with 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;, ## # vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% dplyr::filter(species != &quot;Droid&quot;) %&gt;% mosaic::favstats(mass~gender, data = .) ## gender min Q1 median Q3 max mean sd n ## 1 female 45 50.0 55 56.20 75 54.68889 8.591921 9 ## 2 hermaphrodite 1358 1358.0 1358 1358.00 1358 1358.00000 NA 1 ## 3 male 15 76.5 80 87.25 159 81.00455 28.223707 44 ## missing ## 1 7 ## 2 0 ## 3 16 starwars %&gt;% dplyr::select(name, ends_with(&quot;color&quot;)) ## # A tibble: 87 x 4 ## name hair_color skin_color eye_color ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke Skywalker blond fair blue ## 2 C-3PO &lt;NA&gt; gold yellow ## 3 R2-D2 &lt;NA&gt; white, blue red ## 4 Darth Vader none white yellow ## 5 Leia Organa brown light brown ## 6 Owen Lars brown, grey light blue ## 7 Beru Whitesun lars brown light blue ## 8 R5-D4 &lt;NA&gt; white, red red ## 9 Biggs Darklighter black light brown ## 10 Obi-Wan Kenobi auburn, white fair blue-gray ## # … with 77 more rows starwars %&gt;% dplyr::mutate(name, bmi = mass / ((height / 100) ^ 2)) %&gt;% dplyr::select(name:mass, bmi) ## # A tibble: 87 x 4 ## name height mass bmi ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Luke Skywalker 172 77 26.0 ## 2 C-3PO 167 75 26.9 ## 3 R2-D2 96 32 34.7 ## 4 Darth Vader 202 136 33.3 ## 5 Leia Organa 150 49 21.8 ## 6 Owen Lars 178 120 37.9 ## 7 Beru Whitesun lars 165 75 27.5 ## 8 R5-D4 97 32 34.0 ## 9 Biggs Darklighter 183 84 25.1 ## 10 Obi-Wan Kenobi 182 77 23.2 ## # … with 77 more rows starwars %&gt;% dplyr::arrange(desc(mass)) ## # A tibble: 87 x 13 ## name height mass hair_color skin_color eye_color birth_year gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabb… 175 1358 &lt;NA&gt; green-tan… orange 600 herma… ## 2 Grie… 216 159 none brown, wh… green, y… NA male ## 3 IG-88 200 140 none metal red 15 none ## 4 Dart… 202 136 none white yellow 41.9 male ## 5 Tarf… 234 136 brown brown blue NA male ## 6 Owen… 178 120 brown, gr… light blue 52 male ## 7 Bossk 190 113 none green red 53 male ## 8 Chew… 228 112 brown unknown blue 200 male ## 9 Jek … 180 110 brown fair blue NA male ## 10 Dext… 198 102 none brown yellow NA male ## # … with 77 more rows, and 5 more variables: homeworld &lt;chr&gt;, ## # species &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; starwars %&gt;% dplyr::group_by(species) %&gt;% dplyr::summarize( num = n(), mass = mean(mass, na.rm = TRUE) ) %&gt;% dplyr::filter(num &gt; 1) ## # A tibble: 9 x 3 ## species num mass ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Droid 5 69.8 ## 2 Gungan 3 74 ## 3 Human 35 82.8 ## 4 Kaminoan 2 88 ## 5 Mirialan 2 53.1 ## 6 Twi&#39;lek 2 55 ## 7 Wookiee 2 124 ## 8 Zabrak 2 80 ## 9 &lt;NA&gt; 5 48 3.3.3.2 NHANES require(NHANES) names(NHANES) ## [1] &quot;ID&quot; &quot;SurveyYr&quot; &quot;Gender&quot; ## [4] &quot;Age&quot; &quot;AgeDecade&quot; &quot;AgeMonths&quot; ## [7] &quot;Race1&quot; &quot;Race3&quot; &quot;Education&quot; ## [10] &quot;MaritalStatus&quot; &quot;HHIncome&quot; &quot;HHIncomeMid&quot; ## [13] &quot;Poverty&quot; &quot;HomeRooms&quot; &quot;HomeOwn&quot; ## [16] &quot;Work&quot; &quot;Weight&quot; &quot;Length&quot; ## [19] &quot;HeadCirc&quot; &quot;Height&quot; &quot;BMI&quot; ## [22] &quot;BMICatUnder20yrs&quot; &quot;BMI_WHO&quot; &quot;Pulse&quot; ## [25] &quot;BPSysAve&quot; &quot;BPDiaAve&quot; &quot;BPSys1&quot; ## [28] &quot;BPDia1&quot; &quot;BPSys2&quot; &quot;BPDia2&quot; ## [31] &quot;BPSys3&quot; &quot;BPDia3&quot; &quot;Testosterone&quot; ## [34] &quot;DirectChol&quot; &quot;TotChol&quot; &quot;UrineVol1&quot; ## [37] &quot;UrineFlow1&quot; &quot;UrineVol2&quot; &quot;UrineFlow2&quot; ## [40] &quot;Diabetes&quot; &quot;DiabetesAge&quot; &quot;HealthGen&quot; ## [43] &quot;DaysPhysHlthBad&quot; &quot;DaysMentHlthBad&quot; &quot;LittleInterest&quot; ## [46] &quot;Depressed&quot; &quot;nPregnancies&quot; &quot;nBabies&quot; ## [49] &quot;Age1stBaby&quot; &quot;SleepHrsNight&quot; &quot;SleepTrouble&quot; ## [52] &quot;PhysActive&quot; &quot;PhysActiveDays&quot; &quot;TVHrsDay&quot; ## [55] &quot;CompHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;CompHrsDayChild&quot; ## [58] &quot;Alcohol12PlusYr&quot; &quot;AlcoholDay&quot; &quot;AlcoholYear&quot; ## [61] &quot;SmokeNow&quot; &quot;Smoke100&quot; &quot;Smoke100n&quot; ## [64] &quot;SmokeAge&quot; &quot;Marijuana&quot; &quot;AgeFirstMarij&quot; ## [67] &quot;RegularMarij&quot; &quot;AgeRegMarij&quot; &quot;HardDrugs&quot; ## [70] &quot;SexEver&quot; &quot;SexAge&quot; &quot;SexNumPartnLife&quot; ## [73] &quot;SexNumPartYear&quot; &quot;SameSex&quot; &quot;SexOrientation&quot; ## [76] &quot;PregnantNow&quot; # find the sleep variables NHANESsleep &lt;- NHANES %&gt;% select(Gender, Age, Weight, Race1, Race3, Education, SleepTrouble, SleepHrsNight, TVHrsDay, TVHrsDayChild, PhysActive) names(NHANESsleep) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Race1&quot; ## [5] &quot;Race3&quot; &quot;Education&quot; &quot;SleepTrouble&quot; &quot;SleepHrsNight&quot; ## [9] &quot;TVHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;PhysActive&quot; dim(NHANESsleep) ## [1] 10000 11 # subset for college students NHANESsleep &lt;- NHANESsleep %&gt;% filter(Age %in% c(18:22)) %&gt;% mutate(Weightlb = Weight*2.2) names(NHANESsleep) ## [1] &quot;Gender&quot; &quot;Age&quot; &quot;Weight&quot; &quot;Race1&quot; ## [5] &quot;Race3&quot; &quot;Education&quot; &quot;SleepTrouble&quot; &quot;SleepHrsNight&quot; ## [9] &quot;TVHrsDay&quot; &quot;TVHrsDayChild&quot; &quot;PhysActive&quot; &quot;Weightlb&quot; dim(NHANESsleep) ## [1] 655 12 NHANESsleep %&gt;% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + geom_point(position=position_jitter(width=.25, height=0) ) + facet_grid(SleepTrouble ~ TVHrsDay) 3.3.4 summarize and group_by # number of people (cases) in NHANES NHANES %&gt;% summarize(n()) ## # A tibble: 1 x 1 ## `n()` ## &lt;int&gt; ## 1 10000 # total weight of all the people in NHANES (silly) NHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(sum(Weightlb, na.rm=TRUE)) ## # A tibble: 1 x 1 ## `sum(Weightlb, na.rm = TRUE)` ## &lt;dbl&gt; ## 1 1549419. # mean weight of all the people in NHANES NHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 1 x 1 ## `mean(Weightlb, na.rm = TRUE)` ## &lt;dbl&gt; ## 1 156. # repeat the above but for groups # males versus females NHANES %&gt;% group_by(Gender) %&gt;% summarize(n()) ## # A tibble: 2 x 2 ## Gender `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 female 5020 ## 2 male 4980 NHANES %&gt;% group_by(Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 2 x 2 ## Gender `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 146. ## 2 male 167. # smokers and non-smokers NHANES %&gt;% group_by(SmokeNow) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## SmokeNow `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 No 1745 ## 2 Yes 1466 ## 3 &lt;NA&gt; 6789 NHANES %&gt;% group_by(SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 3 x 2 ## SmokeNow `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No 186. ## 2 Yes 177. ## 3 &lt;NA&gt; 144. # people with and without diabetes NHANES %&gt;% group_by(Diabetes) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## Diabetes `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 No 9098 ## 2 Yes 760 ## 3 &lt;NA&gt; 142 NHANES %&gt;% group_by(Diabetes) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 3 x 2 ## Diabetes `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 No 155. ## 2 Yes 202. ## 3 &lt;NA&gt; 21.6 # break down the smokers versus non-smokers further, by sex NHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% summarize(n()) ## # A tibble: 6 x 3 ## # Groups: SmokeNow [3] ## SmokeNow Gender `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No female 764 ## 2 No male 981 ## 3 Yes female 638 ## 4 Yes male 828 ## 5 &lt;NA&gt; female 3618 ## 6 &lt;NA&gt; male 3171 NHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 6 x 3 ## # Groups: SmokeNow [3] ## SmokeNow Gender `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 No female 167. ## 2 No male 201. ## 3 Yes female 167. ## 4 Yes male 185. ## 5 &lt;NA&gt; female 138. ## 6 &lt;NA&gt; male 151. # break down the people with diabetes further, by smoking NHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% summarize(n()) ## # A tibble: 8 x 3 ## # Groups: Diabetes [3] ## Diabetes SmokeNow `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No No 1476 ## 2 No Yes 1360 ## 3 No &lt;NA&gt; 6262 ## 4 Yes No 267 ## 5 Yes Yes 106 ## 6 Yes &lt;NA&gt; 387 ## 7 &lt;NA&gt; No 2 ## 8 &lt;NA&gt; &lt;NA&gt; 140 NHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE)) ## # A tibble: 8 x 3 ## # Groups: Diabetes [3] ## Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)` ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 No No 183. ## 2 No Yes 175. ## 3 No &lt;NA&gt; 143. ## 4 Yes No 204. ## 5 Yes Yes 204. ## 6 Yes &lt;NA&gt; 199. ## 7 &lt;NA&gt; No 193. ## 8 &lt;NA&gt; &lt;NA&gt; 19.1 3.3.5 babynames babynames %&gt;% group_by(sex) %&gt;% summarize(total=sum(n)) ## # A tibble: 2 x 2 ## sex total ## &lt;chr&gt; &lt;int&gt; ## 1 F 172371079 ## 2 M 175749438 babynames %&gt;% group_by(year, sex) %&gt;% summarize(name_count = n_distinct(name)) %&gt;% head() ## # A tibble: 6 x 3 ## # Groups: year [3] ## year sex name_count ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1880 F 942 ## 2 1880 M 1058 ## 3 1881 F 938 ## 4 1881 M 997 ## 5 1882 F 1028 ## 6 1882 M 1099 babynames %&gt;% group_by(year, sex) %&gt;% summarize(name_count = n_distinct(name)) %&gt;% tail() ## # A tibble: 6 x 3 ## # Groups: year [3] ## year sex name_count ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 2015 F 19074 ## 2 2015 M 14024 ## 3 2016 F 18817 ## 4 2016 M 14162 ## 5 2017 F 18309 ## 6 2017 M 14160 babysamp &lt;- babynames %&gt;% sample_n(size=50) babysamp %&gt;% select(year) %&gt;% distinct() %&gt;% table() ## . ## 1898 1903 1911 1913 1917 1919 1920 1921 1922 1928 1929 1935 1946 1948 1950 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1951 1954 1956 1958 1962 1970 1972 1973 1975 1977 1978 1980 1983 1985 1989 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 1991 1998 1999 2001 2002 2003 2006 2007 2008 2009 2011 2014 2015 2017 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 babysamp %&gt;% distinct() %&gt;% select(year) %&gt;% table() ## . ## 1898 1903 1911 1913 1917 1919 1920 1921 1922 1928 1929 1935 1946 1948 1950 ## 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 ## 1951 1954 1956 1958 1962 1970 1972 1973 1975 1977 1978 1980 1983 1985 1989 ## 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 ## 1991 1998 1999 2001 2002 2003 2006 2007 2008 2009 2011 2014 2015 2017 ## 2 3 1 1 1 1 1 1 1 1 1 1 2 1 Frances &lt;- babynames %&gt;% filter(name== &quot;Frances&quot;) %&gt;% group_by(year, sex) %&gt;% summarize(yrTot = sum(n)) Frances %&gt;% ggplot(aes(x=year, y=yrTot)) + geom_point(aes(color=sex)) + geom_vline(xintercept=2006) + scale_y_log10() + ylab(&quot;Yearly total on log10 scale&quot;) 3.4 9/19/19 Agenda Higher level data verbs: pivot_longer, pivot_wider, join lubridate 3.5 Higher Level Data Verbs There are more complicated verbs which may be important for more sophisticated analyses. See the RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}. pivot_longer makes many columns into 2 columns: pivot_longer(data, cols, names_to = , value_to = ) pivot_wider makes one column into multiple columns: pivot_wider(data, names_from = , values_from = ) left_join returns all rows from the left table, and any rows with matching keys from the right table. inner_join returns only the rows in which the left table have matching keys in the right table (i.e., matching rows in both sets). full_join returns all rows from both tables, join records from the left which have matching keys in the right table. Good practice: always specify the by argument when joining data frames. If you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti 3.6 R examples, higher level verbs tidyr 1.0.0 has just been released! The new release means that you need to update tidyr. You will know if you have the latest version if the following command works in the console (window below): ?tidyr::pivot_longer If you are familiar with spread and gather, you should acquaint yourself with pivot_longer() and pivot_wider(). The idea is to go from very wide dataframes to very long dataframes and vice versa. 3.6.1 pivot_longer pivot the military pay grade to become longer? https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794 library(googlesheets4) sheets_deauth() navy_gs = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408&quot;, col_types = &quot;ccnnnnnnnnnnnnnnn&quot;) dplyr::glimpse(navy_gs) ## Observations: 38 ## Variables: 17 ## $ ...1 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,… ## $ `Active Duty Family` &lt;chr&gt; NA, &quot;Marital Status Report&quot;, NA, &quot;Data Refl… ## $ ...3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 31229, 5309… ## $ ...4 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388,… ## $ ...5 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36946, 6148… ## $ ...6 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, … ## $ ...7 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1… ## $ ...8 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, … ## $ ...9 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3… ## $ ...10 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4… ## $ ...11 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, … ## $ ...12 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483… ## $ ...13 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, … ## $ ...14 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165… ## $ ...15 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36991, 6747… ## $ ...16 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924… ## $ ...17 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 43690, 7839… names(navy_gs) = c(&quot;X&quot;,&quot;pay.grade&quot;, &quot;male.sing.wo&quot;, &quot;female.sing.wo&quot;, &quot;tot.sing.wo&quot;, &quot;male.sing.w&quot;, &quot;female.sing.w&quot;, &quot;tot.sing.w&quot;, &quot;male.joint.NA&quot;, &quot;female.joint.NA&quot;, &quot;tot.joint.NA&quot;, &quot;male.civ.NA&quot;, &quot;female.civ.NA&quot;, &quot;tot.civ.NA&quot;, &quot;male.tot.NA&quot;, &quot;female.tot.NA&quot;, &quot;tot.tot.NA&quot;) navy = navy_gs[-c(1:8), -1] dplyr::glimpse(navy) ## Observations: 30 ## Variables: 16 ## $ pay.grade &lt;chr&gt; &quot;E-1&quot;, &quot;E-2&quot;, &quot;E-3&quot;, &quot;E-4&quot;, &quot;E-5&quot;, &quot;E-6&quot;, &quot;E-7&quot;,… ## $ male.sing.wo &lt;dbl&gt; 31229, 53094, 131091, 112710, 57989, 19125, 5446… ## $ female.sing.wo &lt;dbl&gt; 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438… ## $ tot.sing.wo &lt;dbl&gt; 36946, 61482, 152110, 129091, 69010, 23779, 7359… ## $ male.sing.w &lt;dbl&gt; 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786,… ## $ female.sing.w &lt;dbl&gt; 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144… ## $ tot.sing.w &lt;dbl&gt; 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299… ## $ male.joint.NA &lt;dbl&gt; 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 4… ## $ female.joint.NA &lt;dbl&gt; 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 15… ## $ tot.joint.NA &lt;dbl&gt; 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074… ## $ male.civ.NA &lt;dbl&gt; 5060, 12483, 54795, 105556, 130944, 110322, 7000… ## $ female.civ.NA &lt;dbl&gt; 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 29… ## $ tot.civ.NA &lt;dbl&gt; 5779, 14165, 61436, 115517, 139536, 116149, 7320… ## $ male.tot.NA &lt;dbl&gt; 36991, 67472, 193729, 236418, 212329, 148290, 87… ## $ female.tot.NA &lt;dbl&gt; 6699, 10924, 34482, 40782, 37306, 22404, 10995, … ## $ tot.tot.NA &lt;dbl&gt; 43690, 78396, 228211, 277200, 249635, 170694, 98… # get rid of total columns &amp; rows: navyWR = navy %&gt;% dplyr::select(-contains(&quot;tot&quot;)) %&gt;% dplyr::filter(substr(pay.grade, 1, 5) != &quot;TOTAL&quot; &amp; substr(pay.grade, 1, 5) != &quot;GRAND&quot; ) %&gt;% tidyr::pivot_longer(-pay.grade, values_to = &quot;numPeople&quot;, names_to = &quot;status&quot;) %&gt;% tidyr::separate(status, into = c(&quot;sex&quot;, &quot;marital&quot;, &quot;kids&quot;)) navyWR %&gt;% head() ## # A tibble: 6 x 5 ## pay.grade sex marital kids numPeople ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 E-1 male sing wo 31229 ## 2 E-1 female sing wo 5717 ## 3 E-1 male sing w 563 ## 4 E-1 female sing w 122 ## 5 E-1 male joint NA 139 ## 6 E-1 female joint NA 141 Does a graph tell us if we did it right? what if we had done it wrong…? navyWR %&gt;% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + geom_point() + facet_grid(kids~marital) 3.6.2 pivot_wider library(babynames) babynames %&gt;% dplyr::select(-prop) %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% head() ## # A tibble: 6 x 4 ## year name F M ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 1880 Mary 7065 27 ## 2 1880 Anna 2604 12 ## 3 1880 Emma 2003 10 ## 4 1880 Elizabeth 1939 9 ## 5 1880 Minnie 1746 9 ## 6 1880 Margaret 1578 NA babynames %&gt;% dplyr::select(-prop) %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% dplyr::filter(!is.na(F) &amp; !is.na(M)) %&gt;% arrange(desc(year), desc(M)) ## # A tibble: 168,381 x 4 ## year name F M ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 2017 Liam 36 18728 ## 2 2017 Noah 170 18326 ## 3 2017 William 18 14904 ## 4 2017 James 77 14232 ## 5 2017 Logan 1103 13974 ## 6 2017 Benjamin 8 13733 ## 7 2017 Mason 58 13502 ## 8 2017 Elijah 26 13268 ## 9 2017 Oliver 15 13141 ## 10 2017 Jacob 16 13106 ## # … with 168,371 more rows babynames %&gt;% tidyr::pivot_wider(names_from = sex, values_from = n) %&gt;% dplyr::filter(!is.na(F) &amp; !is.na(M)) %&gt;% arrange(desc(prop)) ## # A tibble: 12 x 5 ## year name prop F M ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1986 Marquette 0.0000130 24 25 ## 2 1996 Dariel 0.0000115 22 23 ## 3 2014 Laramie 0.0000108 21 22 ## 4 1939 Earnie 0.00000882 10 10 ## 5 1939 Vertis 0.00000882 10 10 ## 6 1921 Vernis 0.00000703 9 8 ## 7 1939 Alvia 0.00000529 6 6 ## 8 1939 Eudell 0.00000529 6 6 ## 9 1939 Ladell 0.00000529 6 6 ## 10 1939 Lory 0.00000529 6 6 ## 11 1939 Maitland 0.00000529 6 6 ## 12 1939 Delaney 0.00000441 5 5 3.6.3 join (use join to merge two datasets) 3.6.3.1 First get the data (GapMinder) Both of the following datasets come from GapMinder. The first represents country, year, and female literacy rate. The second represents country, year, and GDP (in fixed 2000 US$). sheets_deauth() litF = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0&quot;) litF = litF %&gt;% dplyr::select(country=starts_with(&quot;Adult&quot;), starts_with(&quot;1&quot;), starts_with(&quot;2&quot;)) %&gt;% tidyr::pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;litRateF&quot;) %&gt;% dplyr::filter(!is.na(litRateF)) sheets_deauth() GDP = read_sheet(&quot;https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0&quot;) GDP = GDP %&gt;% dplyr::select(country = starts_with(&quot;Income&quot;), starts_with(&quot;1&quot;), starts_with(&quot;2&quot;)) %&gt;% tidyr::pivot_longer(-country, names_to = &quot;year&quot;, values_to = &quot;gdp&quot;) %&gt;% dplyr::filter(!is.na(gdp)) head(litF) ## # A tibble: 6 x 3 ## country year litRateF ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 ## 2 Afghanistan 2011 13 ## 3 Albania 2001 98.3 ## 4 Albania 2008 94.7 ## 5 Albania 2011 95.7 ## 6 Algeria 1987 35.8 head(GDP) ## # A tibble: 6 x 3 ## country year gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Albania 1980 1061. ## 2 Albania 1981 1100. ## 3 Albania 1982 1111. ## 4 Albania 1983 1101. ## 5 Albania 1984 1065. ## 6 Albania 1985 1060. # left litGDPleft = dplyr::left_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPleft) ## [1] 571 4 sum(is.na(litGDPleft$gdp)) ## [1] 66 head(litGDPleft) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 NA ## 2 Afghanistan 2011 13 NA ## 3 Albania 2001 98.3 1282. ## 4 Albania 2008 94.7 1804. ## 5 Albania 2011 95.7 1966. ## 6 Algeria 1987 35.8 1902. # right litGDPright = dplyr::right_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPright) ## [1] 7988 4 sum(is.na(litGDPright$gdp)) ## [1] 0 head(litGDPright) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1980 NA 1061. ## 2 Albania 1981 NA 1100. ## 3 Albania 1982 NA 1111. ## 4 Albania 1983 NA 1101. ## 5 Albania 1984 NA 1065. ## 6 Albania 1985 NA 1060. # inner litGDPinner = dplyr::inner_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPinner) ## [1] 505 4 sum(is.na(litGDPinner$gdp)) ## [1] 0 head(litGDPinner) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 2001 98.3 1282. ## 2 Albania 2008 94.7 1804. ## 3 Albania 2011 95.7 1966. ## 4 Algeria 1987 35.8 1902. ## 5 Algeria 2002 60.1 1872. ## 6 Algeria 2006 63.9 2125. # full litGDPfull = dplyr::full_join(litF, GDP, by=c(&quot;country&quot;, &quot;year&quot;)) dim(litGDPfull) ## [1] 8054 4 sum(is.na(litGDPfull$gdp)) ## [1] 66 head(litGDPfull) ## # A tibble: 6 x 4 ## country year litRateF gdp ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1979 4.99 NA ## 2 Afghanistan 2011 13 NA ## 3 Albania 2001 98.3 1282. ## 4 Albania 2008 94.7 1804. ## 5 Albania 2011 95.7 1966. ## 6 Algeria 1987 35.8 1902. 3.6.4 lubridate lubridate is a another R package meant for data wrangling (Grolemund and Wickham 2011). In particular, lubridate makes it very easy to work with days, times, and dates. The base idea is to start with dates in a ymd (year month day) format and transform the information into whatever you want. The linked table is from the original paper and provides many of the basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}. Example from https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html 3.6.4.1 If anyone drove a time machine, they would crash The length of months and years change so often that doing arithmetic with them can be unintuitive. Consider a simple operation, January 31st + one month. Should the answer be: February 31st (which doesn’t exist) March 4th (31 days after January 31), or February 28th (assuming its not a leap year) A basic property of arithmetic is that a + b - b = a. Only solution 1 obeys the mathematical property, but it is an invalid date. Wickham wants to make lubridate as consistent as possible by invoking the following rule: if adding or subtracting a month or a year creates an invalid date, lubridate will return an NA. If you thought solution 2 or 3 was more useful, no problem. You can still get those results with clever arithmetic, or by using the special %m+% and %m-% operators. %m+% and %m-% automatically roll dates back to the last day of the month, should that be necessary. 3.6.4.2 R examples, lubridate Some basics in lubridate require(lubridate) rightnow &lt;- now() day(rightnow) ## [1] 17 week(rightnow) ## [1] 38 month(rightnow, label=FALSE) ## [1] 9 month(rightnow, label=TRUE) ## [1] Sep ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec year(rightnow) ## [1] 2019 minute(rightnow) ## [1] 54 hour(rightnow) ## [1] 4 yday(rightnow) ## [1] 260 mday(rightnow) ## [1] 17 wday(rightnow, label=FALSE) ## [1] 3 wday(rightnow, label=TRUE) ## [1] Tue ## Levels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat But how do I create a date object? jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) ## [1] &quot;2013-01-31&quot; NA &quot;2013-03-31&quot; NA &quot;2013-05-31&quot; ## [6] NA &quot;2013-07-31&quot; &quot;2013-08-31&quot; NA &quot;2013-10-31&quot; ## [11] NA &quot;2013-12-31&quot; floor_date(jan31, &quot;month&quot;) + months(0:11) + days(31) ## [1] &quot;2013-02-01&quot; &quot;2013-03-04&quot; &quot;2013-04-01&quot; &quot;2013-05-02&quot; &quot;2013-06-01&quot; ## [6] &quot;2013-07-02&quot; &quot;2013-08-01&quot; &quot;2013-09-01&quot; &quot;2013-10-02&quot; &quot;2013-11-01&quot; ## [11] &quot;2013-12-02&quot; &quot;2014-01-01&quot; jan31 + months(0:11) + days(31) ## [1] &quot;2013-03-03&quot; NA &quot;2013-05-01&quot; NA &quot;2013-07-01&quot; ## [6] NA &quot;2013-08-31&quot; &quot;2013-10-01&quot; NA &quot;2013-12-01&quot; ## [11] NA &quot;2014-01-31&quot; jan31 %m+% months(0:11) ## [1] &quot;2013-01-31&quot; &quot;2013-02-28&quot; &quot;2013-03-31&quot; &quot;2013-04-30&quot; &quot;2013-05-31&quot; ## [6] &quot;2013-06-30&quot; &quot;2013-07-31&quot; &quot;2013-08-31&quot; &quot;2013-09-30&quot; &quot;2013-10-31&quot; ## [11] &quot;2013-11-30&quot; &quot;2013-12-31&quot; NYC flights library(nycflights13) names(flights) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;dep_time&quot; ## [5] &quot;sched_dep_time&quot; &quot;dep_delay&quot; &quot;arr_time&quot; &quot;sched_arr_time&quot; ## [9] &quot;arr_delay&quot; &quot;carrier&quot; &quot;flight&quot; &quot;tailnum&quot; ## [13] &quot;origin&quot; &quot;dest&quot; &quot;air_time&quot; &quot;distance&quot; ## [17] &quot;hour&quot; &quot;minute&quot; &quot;time_hour&quot; flightsWK &lt;- flights %&gt;% mutate(ymdday = ymd(paste(year, month,day, sep=&quot;-&quot;))) %&gt;% mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday)) head(flightsWK) ## # A tibble: 6 x 22 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # … with 15 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt;, ymdday &lt;date&gt;, weekdy &lt;ord&gt;, whichweek &lt;dbl&gt; flightsWK &lt;- flights %&gt;% mutate(ymdday = ymd(paste(year,&quot;-&quot;, month,&quot;-&quot;,day))) %&gt;% mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday)) flightsWK %&gt;% select(year, month, day, ymdday, weekdy, whichweek, dep_time, arr_time, air_time) %&gt;% head() ## # A tibble: 6 x 9 ## year month day ymdday weekdy whichweek dep_time arr_time air_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt; &lt;ord&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 2013-01-01 Tue 1 517 830 227 ## 2 2013 1 1 2013-01-01 Tue 1 533 850 227 ## 3 2013 1 1 2013-01-01 Tue 1 542 923 160 ## 4 2013 1 1 2013-01-01 Tue 1 544 1004 183 ## 5 2013 1 1 2013-01-01 Tue 1 554 812 116 ## 6 2013 1 1 2013-01-01 Tue 1 554 740 150 3.7 reprex Help me help you In order to create a reproducible example … Step 1. Copy code onto the clipboard Step 2. Type reprex() into the Console Step 3. Look at the Viewer to the right. Copy the Viewer output into GitHub, Piazza, an email, stackexchange, etc. Some places to learn more about reprex include A blog about it: https://teachdatascience.com/reprex/ The reprex vignette: https://reprex.tidyverse.org/index.html reprex dos and donts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html Jenny Bryan webinar on reprex: “Help me help you. Creating reproducible examples” https://resources.rstudio.com/webinars/help-me-help-you-creating-reproducible-examples-jenny-bryan 3.7.0.1 reprex demo reprex( jan31 + months(0:11) + days(31) ) multiple lines of code: reprex({ jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) + days(31) }) reprex({ library(lubridate) jan31 &lt;- ymd(&quot;2013-01-31&quot;) jan31 + months(0:11) + days(31) }) References "],
["sims.html", "Chapter 4 Simulating 4.1 9/24/19 Agenda 4.2 Simulating Complicated Models 4.3 9/26/19 Agenda 4.4 Simulating to Assess Sensitivity", " Chapter 4 Simulating 4.1 9/24/19 Agenda Why simulate? What makes a good simulation? pigs / blackjack examples Below, computer simulations will be used for two main objectives: To understand complicated models To assess sensitivity of procedures We can use simulation studies to understand complex estimators, stochastic processes, etc. Often times, such analytic solutions exist in theory, but are extremely complicated to solve. In particular, as slight variations to the model are added, the simulation is often trivial to change whereas the analytic solution often becomes intractable. Similarly, repeated applications of a procedure (e.g., linear regression) to a scenario (e.g., a dataset, a set of parameters, etc.) can provide important insight into how the procedure varies / behaves. 4.2 Simulating Complicated Models Simulation is done to model a scenario which allows us to understand random behavior without actually replicating the entire study multiple times or trying to model the process analytically. For example, what if you have a keen interest in understanding the probability of getting a single during room draw? Or getting a single on north campus? You wouldn’t actually run room draw thousands of times to find your probability of getting a single room. Similarly, the situation (e.g., room draw) may have too much information (e.g., all the different permutations of integers assigned to groups of 3 or 4 people) to model (easily) in a closed form solution. With a few moderate assumptions (proportion of students in groups of 1, 2, 3, 4; probability of choosing dorm X over dorm Y; random allocation of integers to students; etc.) it is straightforward to simulate the scenario thousands of time and measure the proportion of times your rank (47) will give you the room you want (single in Sontag). Consider the following simulation where the top 10 GOP candidates get to participate in the debate, and the remaining 6 are kept out (example taken for debate on August 6, 2015). The write-up (and example) is a few years old, but the process is identical to the process used for deciding who is eligible for the 2020 Democtratic debates for president. http://www.nytimes.com/interactive/2015/07/21/upshot/election-2015-the-first-gop-debate-and-the-role-of-chance.html?_r=0 A candidate needed to get at least two percent support in four different polls published from a list of approved pollsters between June 28 and August 28, 2019, which cannot be based on open-ended questions and may cover either the national level or one of the first four primary/caucus states (Iowa, New Hampshire, Nevada, and South Carolina). Only one poll from each approved pollster counted towards meeting the criterion in each region. Wikipedia Figure 1.2: For the 2016 election the Republican primary debates allowed only the top 10 candidates, ranked by national polls NYT Example Consider the following problem from probability. Two points are selected randomly on a line of length \\(L\\) so as to be on opposite sides of the midpoint of the line. [In other words, the two points \\(X\\) and \\(Y\\) are independent random variables such that \\(X\\) is uniformly distributed over \\((0,L/2)\\) and \\(Y\\) is uniformly distributed over \\((L/2, 1)\\).] Find the probability that the 3 line segments from \\(0\\) to \\(X\\), from \\(X\\) to \\(Y\\), and from \\(Y\\) to \\(L\\) could be made to form the three sides of a triangle. (Note that three line segments can be made to form a triangle if the length of each of them is less than the sum of the lengths of the others.) The joint density is: \\[ f(x,y) = \\begin{cases} \\frac{4}{L^2} &amp; 0 \\le x \\le L/2, \\, L/2 \\le y \\le L \\\\ 0 &amp; else \\end{cases} \\] The three pieces have lengths: \\(X\\), \\(Y- X\\) and \\(L - Y\\). Three conditions need to be satisfied in order that the three pieces form a triangle: \\[\\begin{align} X + (Y- X) &amp;&gt; (L - Y) \\Rightarrow Y &gt; L - Y \\Rightarrow 2 Y &gt; L \\Rightarrow Y &gt; L/2 \\\\ X + (L-Y ) &amp;&gt; Y - X \\Rightarrow 2X + L &gt; 2Y \\Rightarrow X + \\frac{L}{2} &gt; Y \\\\ Y + (L - Y) &amp;&gt; X \\Rightarrow L &gt; X \\end{align}\\] The first and third conditions are always satisfied, so we just need to find the probability that \\(Y\\) is below the line \\(X + \\frac{L}{2}\\). The density is the same as in the previous problem, so, as before, we just need to find the area of the region below the line that is within the square \\([0, L/2] \\times [L/2, L]\\), and then multiply it by \\(\\frac{4}{L^2}\\). Area = \\(\\displaystyle{ \\frac{1}{2}\\frac{L}{2}\\frac{L}{2} = \\frac{L^2}{8} }\\). Thus, the probability is \\[\\begin{align} \\int_{area} f(x,y)dxdy = \\frac{4}{L^2} \\frac{L^2}{8} = \\frac{1}{2}. \\end{align}\\] What happens for different values of \\(f(x,y)\\)? For example, if \\(x\\) and \\(y\\) have Beta(3,47) distributions on [0,.5] and [.5,1]? Simulating the probability in R is quite straightforward. What is the confidence bounds on the point estimates for the probabilities?? [n.b., we could simulate repeatedly to get a sense for the variability of our estimate!] sticks &lt;- function() { pointx &lt;- runif(1,0,.5) # runif is &quot;random uniform&quot;, not &quot;run if&quot; pointy &lt;- runif(1,.5,1) l1 &lt;- pointx l2 &lt;- pointy-pointx l3 &lt;- 1 - pointy max(l1,l2,l3) &gt; 1-max(l1,l2,l3)} sum(replicate(100000, sticks())) / 100000 ## [1] 0.5 sticks_beta &lt;- function() { pointx &lt;- rbeta(1,3, 47) / 2 # rbeta is random beta pointy &lt;- (rbeta(1, 3, 47) + 1)/2 l1 &lt;- pointx l2 &lt;- pointy-pointx l3 &lt;- 1 - pointy max(l1,l2,l3) &gt; 1-max(l1,l2,l3)} sum(replicate(100000, sticks_beta())) / 100000 ## [1] 0.5 Example Or consider the problem where the goal is to estimate \\(E(X)\\) where \\(X=\\max \\{ k: \\sum_{i=1}^k U_i &lt; 1 \\}\\) and \\(U_i\\) are uniform(0,1). The simulation problem is quite straightforward. Look carefully at the pieces. How are they broken down into steps? Notice that the steps go from inside out. Set k (the number of random numbers) equal to zero. And the running sum to zero. Generate a uniform random variable. Add the random variable to the running sum. Repeat steps 1 and 2 until the sum is larger than 1. Figure out how many random variables were needed to get the sum larger than 1. Repeat the entire process many times so as to account for variability in the simulation. Use the law of large numbers to conclude that the average of the simulation approximates the expected value. allk &lt;- c() for(i in 1:1000){ k &lt;- 0; sumU &lt;- 0 while(sumU &lt; 1) { sumU &lt;- sumU + runif(1) k &lt;- k+1 } allk &lt;- c(allk, k-1) } mean(allk) ## [1] 1.71 4.2.1 Goals of Simulating Complicated Models The goal of simulating a complicated model is not only to create a program which will provide the desired results. We also hope to be able to code such that: The problem is broken down into small pieces The problem has checks in it to see what works (run the lines inside the if statements!) Simple code is best 4.2.2 Examples of Pigs and Blackjack 4.2.2.1 Pass the Pigs Familiarize yourself with how to play Pass the Pigs at http://www.hasbro.com/common/instruct/passthepigs.pdf and https://en.wikipedia.org/wiki/Pass_the_Pigs. For more information on how to play Pass the Pigs, google online resources and see the following manuscript, http://pubsonline.informs.org/doi/pdf/10.1287/ited.1120.0088 Analytics, Pedagogy and the Pass the Pigs Game, (2012), Gorman, INFORMS Transactions on Education 1. More sophisticated modeling: http://www.amstat.org/publications/jse/v14n3/datasets.kern.html Some strategies for playing: http://passpigs.tripod.com/strat.html (The link has other stuff, too.) 4.2.2.2 Blackjack Example and code come from **Data Science in R: a case studies approach to computational reasoning and problem solving*, by Nolan and Temple Lang. Chapter 9 Simulating Blackjack, by Hadley Wickham All R code is online at http://rdatasciencecases.org/ More about the game of blackjack, there are many online resources that you can use to learn about the came. Two resources that Nolan and Temple Lang recommend are http://wizardofodds.com/games/blackjack/ and http://hitorstand.net/. Basic Blackjack Card game, goal: sum cards as close to 21 without going over A few nuances to card value (e.g., Ace can be 1 or 11) Start with 2 cards, build up one card at a time Lots of different strategies (also based on dealer’s cards) What do we need to simulate poker? set-up of cards, dealing, hands “score” (both sum of cards and payout) strategies result of strategies (summary of outcomes) Source Example and code come from Data Science in R: a case studies approach to computational reasoning and problem solving by Nolan and Temple Lang. Chapter 9 Simulating Blackjack by Hadley Wickham All R code is online at http://rdatasciencecases.org/ Link is also on the HW4 assignment Setting up the Game in R deck = rep(c(1:10, 10, 10, 10), 4) shuffle_decks = function(ndecks){sample(rep(deck, ndecks))} head(shuffle_decks(4), 10) ## [1] 2 6 6 1 6 10 10 10 2 10 Outcome of cards in hand handValue = function(cards) { value = sum(cards) # Check for an Ace and change value if it doesn&#39;t bust if (any(cards == 1) &amp;&amp; value &lt;= 11) value = value + 10 value # Check bust (set to 0); check Blackjack (set to 21.5) if(value &gt; 21) 0 else if (value == 21 &amp;&amp; length(cards) == 2) 21.5 # Blackjack else value } handValue(c(10,4)) ## [1] 14 $ of cards in hand winnings = function(dealer, players) { if (dealer &gt; 21) { # Dealer=Blackjack, ties players with Blackjack -1 * (players &lt;= 21) } else if (dealer == 0) { # Dealer busts - all non-busted players win 1.5 * (players &gt; 21) + 1 * (players &lt;= 21 &amp; players &gt; 0) + -1 * (players == 0) } else { # Dealer 21 or below, all players &gt; dealer win 1.5 * (players &gt; 21) + 1 * (players &lt;= 21 &amp; players &gt; dealer) + -1 * (players &lt;= 21 &amp; players &lt; dealer) } } winnings(17,c(20, 21.5, 14, 0, 21)) ## [1] 1.0 1.5 -1.0 -1.0 1.0 Better $ of cards in hand winnings = function(dealer, players){ (players &gt; dealer &amp; players &gt; 21) * 1.5 + # Blackjack (players &gt; dealer &amp; players &lt;= 21) * 1 + # win (players &lt; dealer | players == 0) * -1 # lose } winnings(17,c(20, 21.5, 14, 0, 21)) ## [1] 1.0 1.5 -1.0 -1.0 1.0 winnings(21.5,c(20, 21.5, 14, 0, 21)) ## [1] -1 0 -1 -1 -1 How well does handValue work? test_cards = list( c(10, 1), c(10, 5, 6), c(10, 1, 1), c(7, 6, 1, 5), c(3, 6, 1, 1), c(2, 3, 4, 10), c(5, 1, 9, 1, 1), c(5, 10, 7), c(10, 9, 1, 1, 1)) test_cards_val = c(21.5, 21, 12, 19, 21, 19, 17, 0, 0) sapply(test_cards, handValue) # apply the function handValue to test_cards ## [1] 21.5 21.0 12.0 19.0 21.0 19.0 17.0 0.0 0.0 identical(test_cards_val, sapply(test_cards, handValue)) ## [1] TRUE Testing winnings (create known) test_vals = c(0, 16, 19, 20, 21, 21.5) testWinnings = matrix(c( -1, 1, 1, 1, 1, 1.5, -1, 0, 1, 1, 1, 1.5, -1, -1, 0, 1, 1, 1.5, -1, -1, -1, 0, 1, 1.5, -1, -1, -1, -1, 0, 1.5, -1, -1, -1, -1, -1, 0), nrow = length(test_vals), byrow = TRUE) dimnames(testWinnings) = list(dealer = test_vals, player = test_vals) testWinnings ## player ## dealer 0 16 19 20 21 21.5 ## 0 -1 1 1 1 1 1.5 ## 16 -1 0 1 1 1 1.5 ## 19 -1 -1 0 1 1 1.5 ## 20 -1 -1 -1 0 1 1.5 ## 21 -1 -1 -1 -1 0 1.5 ## 21.5 -1 -1 -1 -1 -1 0.0 Does winnings work? check = testWinnings # make the matrix the right size check[] = NA # make all entries NA for(i in seq_along(test_vals)) { for(j in seq_along(test_vals)) { check[i, j] = winnings(test_vals[i], test_vals[j]) } } identical(check, testWinnings) ## [1] TRUE Function for getting more cards shoe = function(m = 1) sample(deck, m, replace = TRUE) new_hand = function(shoe, cards = shoe(2), bet = 1) { list(bet = bet, shoe = shoe, cards = cards) } myCards = new_hand(shoe, bet = 7) myCards ## $bet ## [1] 7 ## ## $shoe ## function(m = 1) sample(deck, m, replace = TRUE) ## ## $cards ## [1] 4 9 First action: hit receive another card hit = function(hand) { hand$cards = c(hand$cards, hand$shoe(1)) hand } hit(myCards)$cards ## [1] 4 9 10 Second action: stand stay with current cards stand = function(hand) hand stand(myCards)$cards ## [1] 4 9 Third action: double down double the bet after receiving exactly one more card dd = function(hand) { hand$bet = hand$bet * 2 hand = hit(hand) stand(hand) } dd(myCards)$cards ## [1] 4 9 3 Fourth action: split a pair create two different hands from initial hand with two cards of the same value splitPair = function(hand) { list( new_hand(hand$shoe, cards = c(hand$cards[1], hand$shoe(1)), bet = hand$bet), new_hand(hand$shoe, cards = c(hand$cards[2], hand$shoe(1)), bet = hand$bet)) } splitHand = splitPair(myCards) Results of splitting (can we always split?) splitHand[[1]]$cards ## [1] 4 1 splitHand[[2]]$cards ## [1] 9 5 Let’s play! Not yet automated… set.seed(470); dealer = new_hand(shoe); player = new_hand(shoe); dealer$cards[1] ## [1] 2 player$cards; player = hit(player); player$cards ## [1] 5 10 ## [1] 5 10 9 dealer$cards; dealer = hit(dealer); dealer$cards ## [1] 2 3 ## [1] 2 3 3 Who won? dealer$cards; player$cards ## [1] 2 3 3 ## [1] 5 10 9 handValue(dealer$cards); handValue(player$cards) ## [1] 8 ## [1] 0 winnings(handValue(dealer$cards), handValue(player$cards)) ## [1] -1 Simply strategy recall the handValue function – what if player busts? strategy_simple = function(mine, dealerFaceUp) { if (handValue(dealerFaceUp) &gt; 6 &amp;&amp; handValue(mine) &lt; 17) &quot;H&quot; else &quot;S&quot; } Better simple strategy strategy_simple = function(mine, dealerFaceUp) { if (handValue(mine) == 0) return(&quot;S&quot;) if (handValue(dealerFaceUp) &gt; 6 &amp;&amp; handValue(mine) &lt; 17) &quot;H&quot; else &quot;S&quot; } Dealer The dealer gets cards regardless of what the player does dealer_cards = function(shoe) { cards = shoe(2) while(handValue(cards) &lt; 17 &amp;&amp; handValue(cards) &gt; 0) { cards = c(cards, shoe(1)) } cards } dealer_cards(shoe) ## [1] 5 9 1 8 dealer_cards(shoe) ## [1] 4 4 7 10 Playing a hand play_hand = function(shoe, strategy, hand = new_hand(shoe), dealer = dealer_cards(shoe)) { face_up_card = dealer[1] action = strategy(hand$cards, face_up_card) while(action != &quot;S&quot; &amp;&amp; handValue(hand$cards) != 0) { if (action == &quot;H&quot;) { hand = hit(hand) action = strategy(hand$cards, face_up_card) } else { stop(&quot;Unknown action: should be one of S, H&quot;) } } winnings(handValue(dealer), handValue(hand$cards)) * hand$bet } Play a few hands play_hand(shoe, strategy_simple) ## [1] 0 play_hand(shoe, strategy_simple) ## [1] -1 play_hand(shoe, strategy_simple, new_hand(shoe, bet=7)) ## [1] 0 play_hand(shoe, strategy_simple, new_hand(shoe, bet=7)) ## [1] 7 Repeated games To repeat the game, we simply repeat the play_hand function and keep track of the dollars gained or lost. reps=10 money=20 for(i in 1:reps){ money &lt;- money + play_hand(shoe, strategy_simple) print(money)} ## [1] 19 ## [1] 20 ## [1] 19 ## [1] 18 ## [1] 19.5 ## [1] 18.5 ## [1] 20 ## [1] 19 ## [1] 18 ## [1] 19.5 4.3 9/26/19 Agenda Understanding bias in modeling Sensitivity of statistical inferential procedures to technical conditions (Not responsible for: Generating random numbers) 4.4 Simulating to Assess Sensitivity As a second use of simulations, we can assess the sensitivity of parameters, model assumptions, sample size, etc. Ideally, the results will be summarized graphically, instead of as a table. A graphical representation can often provide insight into how parameters are related, whereas a table can be very hard to read. 4.4.1 Bias in Models The example below is taken directly (and mostly verbatim) from a blog by Aaron Roth Algorithmic Unfairness Without Any Bias Baked In. Bias in the data is certainly a problem, especially when labels are gathered by human beings. But its far from being the only problem. In this post, I want to walk through a very simple example in which the algorithm designer is being entirely reasonable, there are no human beings injecting bias into the labels, and yet the resulting outcome is “unfair”. Here is the (toy) scenario – the specifics aren’t important. High school students are applying to college, and each student has some innate “talent” \\(I\\), which we will imagine is normally distributed, with mean 100 and standard deviation 15: \\(I \\sim N(100,15)\\). The college would like to admit students who are sufficiently talented — say one standard deviation above the mean (so, it would like to admit students with \\(I \\geq 115\\)). The problem is that talent isn’t directly observable. Instead, the college can observe grades \\(g\\) and SAT scores \\(s\\), which are a noisy estimate of talent. For simplicity, lets imagine that both grades and SAT scores are independently and normally distributed, centered at a student’s talent level, and also with standard deviation 15: \\(g \\sim N(I, 15)\\), \\(s \\sim N(I, 15)\\). In this scenario, the college has a simple, optimal decision rule: It should run a linear regression to try and predict student talent from grades and SAT scores, and then it should admit the students whose predicted talent is at least 115. This is indeed “driven by math” – since we assumed everything was normally distributed here, this turns out to correspond to the Bayesian optimal decision rule for the college. The data Ok. Now lets suppose there are two populations of students, which we will call Reds and Blues. Reds are the majority population, and Blues are a small minority population – the Blues only make up about 1% of the student body. But the Reds and the Blues are no different when it comes to talent: they both have the same talent distribution, as described above. And there is no bias baked into the grading or the exams: both the Reds and the Blues also have exactly the same grade and exam score distributions, as described above. But there is one difference: the Blues have a bit more money than the Reds, so they each take the SAT twice, and report only the highest of the two scores to the college. This results in a small but noticeable bump in their average SAT scores, compared to the Reds. n = 100000 n.red = n*0.99 n.blue = n*0.01 reds &lt;- rnorm(n.red, mean = 100, sd = 15) blues &lt;- rnorm(n.blue, mean = 100, sd = 15) red.sat &lt;- reds + rnorm(n.red, mean = 0, sd = 15) blue.sat &lt;- blues + pmax(rnorm(n.blue, mean = 0, sd = 15), rnorm(n.blue, mean = 0, sd = 15)) red.grade &lt;- reds + rnorm(n.red, mean = 0, sd = 15) blue.grade &lt;- blues + rnorm(n.blue, mean = 0, sd = 15) college.data &lt;- data.frame(talent = c(reds, blues), SAT = c(red.sat, blue.sat), grades = c(red.grade, blue.grade), color = c(rep(&quot;red&quot;, n.red), rep(&quot;blue&quot;, n.blue))) ggplot(college.data, aes(x = grades, y = SAT, color = color)) + geom_point(size = 0.5) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) + geom_abline(intercept = 0, slope = 1) Two separate models So what is the effect of this when we use our reasonable inference procedure? First, lets consider what happens when we learn two different regression models: one for the Blues, and a different one for the Reds. We don’t see much difference: red.lm = college.data %&gt;% dplyr::filter(color == &quot;red&quot;) %&gt;% lm(talent ~ SAT + grades, data = .) blue.lm = college.data %&gt;% dplyr::filter(color == &quot;blue&quot;) %&gt;% lm(talent ~ SAT + grades, data = .) global.lm = college.data %&gt;% lm(talent ~ SAT + grades, data = .) red.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 33.2 0.153 216. 0 ## 2 SAT 0.333 0.00151 220. 0 ## 3 grades 0.335 0.00151 222. 0 blue.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.6 1.64 15.6 1.85e- 49 ## 2 SAT 0.429 0.0169 25.3 1.12e-109 ## 3 grades 0.280 0.0157 17.9 3.50e- 62 global.lm %&gt;% broom::tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 33.2 0.153 217. 0 ## 2 SAT 0.333 0.00151 221. 0 ## 3 grades 0.335 0.00150 223. 0 new.reds &lt;- rnorm(n.red, mean = 100, sd = 15) new.blues &lt;- rnorm(n.blue, mean = 100, sd = 15) new.red.sat &lt;- new.reds + rnorm(n.red, mean = 0, sd = 15) new.blue.sat &lt;- new.blues + pmax(rnorm(n.blue, mean = 0, sd = 15), rnorm(n.blue, mean = 0, sd = 15)) new.red.grade &lt;- new.reds + rnorm(n.red, mean = 0, sd = 15) new.blue.grade &lt;- new.blues + rnorm(n.blue, mean = 0, sd = 15) new.college.data &lt;- data.frame(talent = c(new.reds, new.blues), SAT = c(new.red.sat, new.blue.sat), grades = c(new.red.grade, new.blue.grade), color = c(rep(&quot;red&quot;, n.red), rep(&quot;blue&quot;, n.blue))) new.red.pred &lt;- new.college.data %&gt;% filter(color == &quot;red&quot;) %&gt;% predict.lm(red.lm, newdata = .) new.blue.pred &lt;- new.college.data %&gt;% filter(color == &quot;blue&quot;) %&gt;% predict.lm(blue.lm, newdata = .) new.college.data &lt;- new.college.data %&gt;% cbind(predicted = c(new.red.pred, new.blue.pred)) ggplot(new.college.data, aes(x = talent, y = predicted, color = color)) + geom_point(size = 0.5) + geom_hline(yintercept = 115) + geom_vline(xintercept = 115) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) new.college.data &lt;- new.college.data %&gt;% mutate(fp = ifelse(talent &lt; 115 &amp; predicted &gt; 115, 1, 0), tp = ifelse(talent &gt; 115 &amp; predicted &gt; 115, 1, 0), fn = ifelse(talent &gt; 115 &amp; predicted &lt; 115, 1, 0), tn = ifelse(talent &lt; 115 &amp; predicted &lt; 115, 1, 0)) error.rates &lt;- new.college.data %&gt;% group_by(color) %&gt;% summarize(tpr = sum(tp) / (sum(tp) + sum(fn)), fpr = sum(fp) / (sum(fp) + sum(tn)), fnr = sum(fn) / (sum(fn) + sum(tp)), fdr = sum(fp) / (sum(fp) + sum(tp)), error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) )) error.rates ## # A tibble: 2 x 6 ## color tpr fpr fnr fdr error ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 0.548 0.0367 0.452 0.267 0.101 ## 2 red 0.506 0.0379 0.494 0.284 0.111 The Red classifier makes errors approximately 11.053% of the time. The Blue classifier does about the same — it makes errors about 10.1% of the time. This makes sense: the Blues artificially inflated their SAT score distribution without increasing their talent, and the classifier picked up on this and corrected for it. In fact, it is even a little more accurate! And since we are interested in fairness, lets think about the false negative rate of our classifiers. “False Negatives” in this setting are the people who are qualified to attend the college (\\(I &gt; 115\\)), but whom the college mistakenly rejects. These are really the people who have come to harm as a result of the classifier’s mistakes. And the False Negative Rate is the probability that a randomly selected qualified person is mistakenly rejected from college — i.e. the probability that a randomly selected student is harmed by the classifier. We should want that the false negative rates are approximately equal across the two populations: this would mean that the burden of harm caused by the classifier’s mistakes is not disproportionately borne by one population over the other. This is one reason why the difference between false negative rates across different populations has become a standard fairness metric in algorithmic fairness — sometimes referred to as “equal opportunity.” So how do we fare on this metric? Not so badly! The Blue model has a false negative rate of 45.161% on the blues, and the Red model has a false negative rate of 49.413% on the reds — so the difference between these two is a satisfyingly small 4.252%. One global model But you might reasonably object: because we have learned separate models for the Blues and the Reds, we are explicitly making admissions decisions as a function of a student’s color! This might sound like a form of discrimination, baked in by the algorithm designer — and if the two populations represent e.g. racial groups, then its explicitly illegal in a number of settings, including lending. new.pred &lt;- new.college.data %&gt;% predict.lm(global.lm, newdata = .) new.college.data &lt;- new.college.data %&gt;% cbind(global.predicted = new.pred) ggplot(new.college.data, aes(x = talent, y = global.predicted, color = color)) + geom_point(size = 0.5) + geom_hline(yintercept = 115) + geom_vline(xintercept = 115) + scale_color_identity(name = &quot;Color Group&quot;, guide = &quot;legend&quot;) new.college.data &lt;- new.college.data %&gt;% mutate(fp = ifelse(talent &lt; 115 &amp; global.predicted &gt; 115, 1, 0), tp = ifelse(talent &gt; 115 &amp; global.predicted &gt; 115, 1, 0), fn = ifelse(talent &gt; 115 &amp; global.predicted &lt; 115, 1, 0), tn = ifelse(talent &lt; 115 &amp; global.predicted &lt; 115, 1, 0)) error.rates &lt;- new.college.data %&gt;% group_by(color) %&gt;% summarize(tpr = sum(tp) / (sum(tp) + sum(fn)), fpr = sum(fp) / (sum(fp) + sum(tn)), fnr = sum(fn) / (sum(fn) + sum(tp)), fdr = sum(fp) / (sum(fp) + sum(tp)), error = (sum(fp) + sum(fn)) / (sum(fp) + sum(tp) + sum(fn) + sum(tn) )) error.rates ## # A tibble: 2 x 6 ## color tpr fpr fnr fdr error ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 blue 0.632 0.0627 0.368 0.351 0.11 ## 2 red 0.504 0.0377 0.496 0.283 0.111 So what happens if we don’t allow our classifier to see group membership, and just train one classifier on the whole student body? The gap in false negative rates between the two populations balloons to 12.791%. Additionally, the Blues now have a higher false positive rate (people who don’t have talent about 115 are let in accidentally) and the Reds now have a higher false negative rate (people who do have talent are mistakenly kept out). This means if you are a qualified member of the Red population, you are substantially more likely to be mistakenly rejected by our classifier than if you are a qualified member of the Blue population. What happened???? What happened? There wasn’t any malice anywhere in this data pipeline. Its just that the Red population was much larger than the Blue population, so when we trained a classifier to minimize its average error over the entire student body, it naturally fit the Red population – which contributed much more to the average. But this means that the classifier was no longer compensating for the artificially inflated SAT scores of the Blues, and so was making a disproportionate number of errors on them – all in their favor. This is the kind of thing that happens all the time: whenever there are two populations that have different feature distributions, learning a single classifier (that is prohibited from discriminating based on population) will fit the bigger of the two populations, simply because they contribute more to average error. Depending on the nature of the distribution difference, this can be either to the benefit or the detriment of the minority population. And not only does this not involve any explicit human bias, either on the part of the algorithm designer or the data gathering process, it is exacerbated if we artificially force the algorithm to be group blind. Well intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes into account can actually make things less fair and less accurate at the same time. 4.4.2 Technical Conditions Definitions p-value is the probability of obtaining the observed data or more extreme given the null hypothesis is true. confidence interval is a range of values collected in such a way that repeated samples of data (using the same mechanism) would capture the parameter of interest in \\((1-\\alpha)100\\)% of the intervals. Examples Equal variance in the t-test Recall that one of the technical conditions for the t-test is that the two samples come from populations where the variance is equal (at least when var.equal=TRUE is specified). What happens if the null hypothesis is true (i.e., the means are equal!) but the technical conditions are violated (i.e., the variances are unequal)? pvals &lt;- c() reps &lt;- 10000 for(i in 1:reps){ x1 &lt;- rnorm(10, mean=47, sd=1) x2 &lt;- rnorm(10, mean=47, sd=1) pvals &lt;- c(pvals, t.test(x1,x2, var.equal=TRUE)$p.value) } sum(pvals &lt; .05)/reps ## [1] 0.0502 Unequal variance in the t-test pvals &lt;- c() reps &lt;- 10000 for(i in 1:reps){ x1 &lt;- rnorm(10, mean=47, sd=1) x2 &lt;- rnorm(10, mean=47, sd=100) pvals &lt;- c(pvals, t.test(x1,x2, var.equal=TRUE)$p.value) } sum(pvals &lt; .05)/reps ## [1] 0.0637 Equal variance in the linear model The ISCAM applet by Beth Chance and Allan Rossman (Chance and Rossman 2018) demonstrates ideas of confidence intervals and what the analyst should expect with inferential assessment. Consider the following linear model with the points normally distributed with equal variance around the line. [Spoiler: when the technical conditions are met, the theory works out well. It turns out that the confidence interval will capture the true parameter in 95% of samples!] \\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1)\\] beta2.in &lt;- c() beta0 &lt;- -1 beta1 &lt;- 0.5 beta2 &lt;- 1.5 n &lt;- 100 reps &lt;- 10000 set.seed(4747) for(i in 1:reps){ x1 &lt;- rep(c(0,1), each=n/2) x2 &lt;- runif(n, min=-1, max=1) y &lt;- beta0 + beta1*x1 + beta2*x2 + rnorm(n, mean=0, sd = 1) CI &lt;- lm(y~x1+x2) %&gt;% tidy(conf.int=TRUE) %&gt;% data.frame() beta2.in &lt;- c(beta2.in, between(beta2, CI[3,6], CI[3,7])) } # coverage rate of the CI is given by: sum(beta2.in)/reps ## [1] 0.952 Unequal variance in the linear model Consider the following linear model with the points normally distributed with unequal variance around the line. [Spoiler: when the technical conditions are met, the theory does not work out as well. It turns out that the confidence interval will not capture the true parameter in 95% of samples!] \\[ Y = -1 + 0.5 X_1 + 1.5 X_2 + \\epsilon, \\ \\ \\ \\epsilon \\sim N(0,1+ X_1 + 10 \\cdot |X_2|)\\] beta2.in &lt;- c() beta0 &lt;- -1 beta1 &lt;- 0.5 beta2 &lt;- 1.5 n &lt;- 100 reps &lt;- 10000 set.seed(4747) for(i in 1:reps){ x1 &lt;- rep(c(0,1), each=n/2) x2 &lt;- runif(n, min=-1, max=1) y &lt;- beta0 + beta1*x1 + beta2*x2 + rnorm(n, mean=0, sd = 1 + x1 + 10*abs(x2)) CI &lt;- lm(y~x1+x2) %&gt;% broom::tidy(conf.int=TRUE) %&gt;% data.frame() beta2.in &lt;- c(beta2.in, between(beta2, CI[3,6], CI[3,7])) } # coverage rate of the CI is given by: sum(beta2.in)/reps ## [1] 0.872 4.4.3 Generating random numbers (You are not responsible for the material on generating random numbers, but it’s pretty cool stuff that relies heavily on simulation.) 4.4.3.1 How do we generate uniform[0,1] numbers? LCG - linear congruence generators. Set \\(a,b,m\\) to be large integers. The sequence of numbers \\(X_i / m\\) will pass all tests for uniformly distributed variables. \\[ X_{n+1} = (aX_n + b) \\mod m \\] where \\(m\\) and \\(b\\) are relatively prime, \\(a - 1\\) is divisible by all prime factors of \\(m\\), \\(a - 1\\) is divisible by 4 if \\(m\\) is divisible by 4. a &lt;- 31541435235 b &lt;- 23462146143 m &lt;- 423514351351 xval &lt;- 47 reps &lt;- 10000 unif.val &lt;- c() for(i in 1:reps){ xval &lt;- (a*xval + b) %% m unif.val &lt;- c(unif.val, xval/m) } data.frame(uniformRVs = unif.val) %&gt;% ggplot(aes(x = uniformRVs)) + geom_histogram(bins = 25) 4.4.4 Generating other RVs: The Inverse Transform Method Continuous RVs Use the inverse of the cumulative distribution function to generate data that come from a particular continuous distribution. For example, generate 100 random normal deviates. Start by assuming that \\(F\\) is a continuous and increasing function. Also assume that \\(F^{-1}\\) exists. \\[F(x) = P(X \\leq x)\\] Note that \\(F\\) is just the area function describing the density (histogram) of the data. Algorithm: Generate Continuous RV Generate a uniform random variable \\(U\\) Set \\(X = F^{-1}(U)\\) Proof: that the algorithm above generates variables that come from the probability distribution represented by \\(F\\). \\[\\begin{align} P(X \\leq x) &amp;= P(F^{-1}(U) \\leq x)\\\\ &amp;= P(U \\leq F(x))\\\\ &amp;= F(x)\\\\ \\end{align}\\] Example: \\[ f(x) = \\begin{cases} 2x e^{-x^2} &amp; 0 &lt; x \\\\ 0 &amp; x &lt; 0 \\end{cases}\\] Note: This is known as a Weibull(\\(\\lambda=1\\), \\(k=2\\)) distribution. Figure 4.1: Weibull PDF by Calimo - Own work, after Philip Leitch.. Licensed under CC BY-SA 3.0 via Commons What is \\(F(x)\\)? \\[ F(x) = \\int_0^x 2w e^{-w^2} dw = 1 - e^{-x^2}\\] What is \\(F^{-1}(u)\\)? \\[\\begin{align} u &amp;= F(x)\\\\ &amp;= 1 - e^{-x^2}\\\\ 1-u &amp;= e^{-x^2}\\\\ -\\ln(1-u) &amp;= x^2\\\\ \\sqrt{-\\ln(1-u)} &amp;= x\\\\ F^{-1}(u) &amp;= \\sqrt{-\\ln(1-u)} \\end{align}\\] Suppose you could simulate uniform random variables, \\(U_1, U_2, \\dots\\). How could you use these to simulate RV’s with the Weibull density, \\(f(x)\\), given above? \\[ \\mbox{Let: } X_i = \\sqrt{-\\ln(1-U_i)}\\] unifdata = runif(10000,0,1) weib1data = sqrt(-log(1-unifdata)) weib2data = rweibull(10000,2,1) weibdata &lt;- data.frame(weibull = c(weib1data, weib2data), sim.method = c(rep(&quot;InvTrans&quot;, 10000), rep(&quot;rweibull&quot;, 10000))) ggplot(weibdata, aes(x = weibull)) + geom_histogram(bins = 25) + facet_grid(~sim.method) Discrete RVs A similar algorithm is used to generate data that come from a particular discrete distribution. For example, generate 100 random normal deviates. We start by assuming the probability mass function of \\(X\\) is \\[ P(X = x_i) = p_i, i=1, \\ldots, m\\] Algorithm: Generate Discrete RV Generate a uniform random variable \\(U\\) Transform \\(U\\) into \\(X\\) as follows, \\[X = x_j \\mbox{ if } \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\] Proof: that the algorithm above generates variables that come from the probability mass function \\(\\{p_1, p_2, \\ldots, p_m\\}\\). \\[\\begin{align} P(X = x_j) &amp;= \\sum_{i=1}^{j-1} p_i \\leq U \\leq \\sum_{i=1}^j p_i\\\\ &amp;= \\sum_{i=1}^j p_i - \\sum_{i=1}^{j-1} p_i\\\\ &amp;= p_j\\\\ \\end{align}\\] What if you don’t know \\(F\\)? Or can’t calculate \\(F^{-1}\\)? In the case that the CDF cannot be calculated explicitly (the normal for example), one could still use this methodology by estimating F at a collection of points \\(x_i, u_i = F(x_i)\\). Now we temporarily mimic the discrete inverse transform, as we generate a \\(U\\) and see which subinterval it falls in, i.e. \\(u_i \\leq U \\leq u_{i+1}\\). Assuming the \\(x_i\\) are close enough, we expect the CDF to be approximately linear on this subinterval, so then we take a linear interpolation of the CDF on the subinterval to get \\(X\\) via \\[\\begin{align} X = \\frac{u_{i+1} - U}{u_{i+1} - u_i} x_i + \\frac{U - u_i}{u_{i+1} - u_i} x_j \\end{align}\\] However, the linear interpolation requires a complete approximation of \\(F(x)\\), regardless of the sample size desired, and doesn’t generalize to higher dimensions, and of course only gives you something with the approximate distribution back, even if you have your hands on real uniform random variables. References "],
["permschp.html", "Chapter 5 Randomization &amp; Permutation Tests 5.1 10/1/19 Agenda 5.2 Inference Algorithms 5.3 10/3/19 Agenda 5.4 Permutation tests", " Chapter 5 Randomization &amp; Permutation Tests 5.1 10/1/19 Agenda Review: logic of hypothesis testing Logic of permutation tests Examples - 2 samples and beyond Motivation: Great video of how/why computational statistical methods can be extremely useful. And it’s about beer and mosquitos! John Rauser from Pintrest gives the keynote address at Strata + Hadoop World Conference October 16, 2014. David Smith, Revolution Analytics blog, October 17, 2014. http://blog.revolutionanalytics.com/2014/10/statistics-doesnt-have-to-be-that-hard.html A more complicated scenario with the same tools being applied. Here the point is to understand gerrymandering. https://www.youtube.com/watch?v=gRCZR_BbjTo&amp;t=125s The big lesson here, IMO, is that so many statistical problems can seem complex, but you can actually get a lot of insight by recognizing that your data is just one possible instance of a random process. If you have a hypothesis for what that process is, you can simulate it, and get an intuitive sense of how surprising your data is. R has excellent tools for simulating data, and a couple of hours spent writing code to simulate data can often give insights that will be valuable for the formal data analysis to come. (David Smith) Rauser says that the in order to follow a statistical argument that uses simulation, you need three things: Ability to follow a simple logical argument. Random number generation. Iteration 5.2 Inference Algorithms 5.2.1 Hypothesis Test Algorithm Before working out the nitty gritty details, recall the structure of hypothesis testing. Consider the applet on Simulating ANOVA Tables (Chance &amp; Rossman) http://www.rossmanchance.com/applets/AnovaSim.html Choose a statistic that measures the effect you are looking for. For example, the ANOVA F statistic is: \\[\\begin{align} F &amp;=&amp; \\frac{\\text{between-group variability}}{\\text{within-group variability}}\\\\ &amp;=&amp; \\frac{\\sum_i n_i(\\overline{X}_{i\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{i\\cdot})^2/(N-K)} \\end{align}\\] Construct the sampling distribution that this statistic would have if the effect were not present in the population. [The sampling distributions for t statistics and F statistics are based on the Central Limit Theorem and derived in Math 152.] Locate the observed statistic in this distribution. A value in the main body of the distribution could easily occur just by chance. A value in the tail would rarely occur by chance and so is evidence that something other than chance is operating. [This piece is going to happen in permutation tests as well as in analytic tests – the point is to see if the observed data is consistent with the null distribution.] 5.2.1.0.1 p-value {-} is the probability of the observed data or more extreme if the null hypothesis is true. [Also true for both types of tests!] To estimate the p-value for a test of significance, estimate the sampling distribution of the test statistic when the null hypothesis is true by resampling in a manner that is consistent with the null hypothesis. 5.2.2 Permutation Tests Algorithm To evaluate the p-value for a permutation test, estimate the sampling distribution of the test statistic when the null hypothesis is true by resampling in a manner that is consistent with the null hypothesis (the number of resamples is finite but can be large!). Procedure 1. Choose a test statistic 2. Shuffle the data (enforce the null hypothesis to be true) 3. Create a null sampling distribution of the test statistic (under \\(H_0\\)) 4. Find the observed test statistic on the null sampling distribution and compute the p-value (observed data or more extreme). The p-value can be one or two-sided. Assumptions Permutation tests fall into a broad class of tests called “non-parametric” tests. The label indicates that there are no distributional assumptions made on the data (i.e., no assumption that the data come from a normal or binomial distribution). However, a test which is ``non-parametric&quot; does not meant that there are no assumptions on the data, simply that there are no distributional or parametric assumptions on the data. For permutation tests, we are not basing the test on population parameters, so we don’t need to make any assumptions about them (i.e., that they are the mean of a particular distribution). Permutation The different treatments have the same effect. [Note: exchangeability, same population, etc.] If the null hypothesis is true, the labels assigning groups are interchangeable. Note that it is our choice of *statistic} which makes the test more sensitive to some kinds of difference (e.g., difference in mean) than other kinds (e.g., difference in variance). Parametric The different populations have the same mean. 5.3 10/3/19 Agenda R code, examples Assumptions, exchangeability, random structure Different statistics within the permutation test Permutation vs. Randomization tests (Binomial) Other test statistics 5.4 Permutation tests How is the test interpreted given the different types of sampling which are possibly used to collect the data? * Random Sample The concept of a p-value usually comes from the idea of taking a sample from a population and comparing it to a sampling distribution (from many many random samples). * Random Experiment In the context of a randomized experiment, the p-value represents the observed data compared to “happening by chance.” * The interpretation is easy: if there is only a very small chance that the observed statistic would take such an extreme value, as a result only of the randomization of cases: we reject the null treatment effect hypothesis. CAUSAL! * Observational Study In the context of observational studies the results are less strong, but it is reasonable to conclude that the effect observed in the sample reflects an effect present in the population. * In a sample, consider the difference (or ratio) and ask “Is this difference so large it would rarely occur by chance in a particular sample constructed under the null setting?” * If the data come from a random sample, then the sample (or results from the sample) are probably consistent with the population [i.e., we can infer the results back to the larger population]. 5.4.0.1 Other Test Statistics The example in class used a modification of the ANOVA F-statistic to compare the observed data with the permuted data test statistics. Depending on the data and question, the permuted test statistic can take on any of a variety of forms. Data Hypothesis Question Statistic 2 categorical diff in prop \\(\\hat{p}_1 - \\hat{p}_2\\) or \\(\\chi^2\\) variables ratio of prop \\(\\hat{p}_1 / \\hat{p}_2\\) 1 numeric diff in means \\(\\overline{X}_1 - \\overline{X}_2\\) 1 binary ratio of means \\(\\overline{X}_1 / \\overline{X}_2\\) diff in medians \\(\\mbox{median}_1 - \\mbox{median}_2\\) ratio of medians \\(\\mbox{median}_1 / \\mbox{median}_2\\) diff in SD \\(s_1 - s_2\\) diff in var \\(s^2_1 - s^2_2\\) ratio of SD or VAR $ s_1 / s_2$ 1 numeric diff in means \\(\\sum n_i (\\overline{X}_i - \\overline{X})^2\\) or k groups F stat paired or (permute within row) \\(\\overline{X}_1 - \\overline{X}_2\\) repeated measures regression correlation least sq slope time series no serial core lag 1 autocross Depending on the data, hypotheses, and original data collection structure (e.g., random sampling vs random allocation), the choice of statistic for the permutation test will vary. 5.4.1 Permutation vs. Randomization Tests We will call randomization tests those that enumerate all possible data permutations. permutation tests, on the other hand, will permute the data \\(B\\) ($&lt; &lt; $ all) times. Some authors call a permutation test applied to a randomized experiment a randomization test. Main difference: randomization tests consider every possible permutation of the labels, permutation tests take a random sample of permutations of the labels. Both can only be applied to a comparison situation (e.g., no one sample t-tests). Both permute labels under \\(H_0\\), for example, \\(H_0: F(x) = G(x)\\). Both can be used in situations where sampling distributions are unknown (e.g., differences in medians). Both can be used in situations where sampling distributions are based on population distributions (e.g., ratio of variances). Randomization tests were the first nonparametric tests conceived (R.A. Fisher, 1935). Randomization p-value Let \\(t^*\\) be the observed test statistic. For a two sample test with \\(N\\) total observations and \\(n\\) observations in group 1, there are \\({N \\choose n}\\) randomizations, all of which are equally likely under \\(H_0\\). The p-value then becomes: \\[\\begin{align} p_R &amp;=&amp; P(T \\leq t^* | H_0) = \\frac{\\sum_{i=1}^{ {N \\choose n}} I(t_i \\leq t*)}{ {N \\choose n}} \\end{align}\\] If we choose a significance level of \\(\\alpha = k/{N \\choose n}\\), then the type I error rate is: \\[\\begin{align} P(\\text{type I error}) &amp;=&amp; P(p_R \\leq \\alpha | H_0)\\\\ &amp;=&amp; P\\bigg(\\sum_{i=1}^{ {N \\choose n}} I(t_i \\leq t*) \\leq k | H_0 \\bigg)\\\\ &amp;=&amp; \\frac{k}{{N \\choose n} }= \\alpha\\\\ \\text{alternatively } k&amp;=&amp; \\alpha {N \\choose n} \\end{align}\\] The point of which is to say that the randomization test controls the probability of a Type I error under the very minimal conditions that the subjects are randomized to treatments (minimal assumption, but hard to do in practice!!) Permutation p-value Now consider a permutation test that randomly permutes the data \\(B\\) times (instead of all \\({N \\choose n}\\) times). A permutation test approximates a randomization test. In fact, the permutation test can be analyzed using the following binomial random variable: \\[\\begin{align} X_P &amp;=&amp; \\# \\ \\mbox{permutations out of B that give a more extreme value than the observed test statistic}\\\\ X_P &amp;\\sim&amp; Bin(p_R, B)\\\\ SE(X_P) &amp;=&amp; \\sqrt{\\frac{p_R (1-p_R)}{B}} \\approx \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}} \\end{align}\\] Consider a situation where interest is in a small effect, say p-value\\(\\approx 0.01\\). The SE should be less than 0.001. \\[\\begin{align} 0.001 &amp;=&amp; \\sqrt{ (0.01)\\cdot(0.99) / B}\\\\ B &amp;=&amp; (0.01) \\cdot (0.99) / (0.001)^2\\\\ &amp;=&amp; 9900 \\end{align}\\] Another way to look at the same problem is to use the estimated p-value = \\(\\hat{p}_P = \\frac{X_P}{B}\\) to come up with a confidence interval for \\(p_R\\). CI for \\(p_R \\approx \\hat{p}_P \\pm 1.96 \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\\) 5.4.2 CI from Permutation Tests Use shifts or rescaling to create a CI for a parameter value using permutation tests. That is, consider a situation with data from \\(X\\) and \\(Y\\) Use one of the following transformation (depending on the study): \\[\\begin{align} W &amp;=&amp; Y + a\\\\ \\mbox{or } Y &amp;=&amp; Y / b \\end{align}\\] and run the permutation test of interest on \\(X\\) vs. \\(W\\) or \\(X\\) vs. \\(U\\). For a series of \\(a\\) or \\(b\\) values we can find which we don’t reject at a particular level of significance (\\(\\alpha\\)) to create a \\((1-\\alpha)100\\%\\) confidence interval. Usually, however, we use bootstrapping for confidence intervals and permutation tests for hypothesis testing. 5.4.3 Permutation/Randomization Examples 5.4.3.1 Fisher’s Exact Test – computationally efficient randomization test N observations are classified into a 2x2 table. Each observation is classified into exactly one cell. Row and column totals are fixed. Given fixed row and column totals, we can easily calculate the interior distribution using the hypergeometric. Note that once a single cell is filled, all other cells are determined. Col 1 Col 2 Total Row 1 X r-X r Row 2 c-X N-r-c+X N-r Total c N-c N \\[\\begin{align} P(X=x) &amp;=&amp; \\frac{ {r \\choose x}{{N-r} \\choose {c-x}}}{ {N \\choose c}}\\\\ &amp;&amp; \\mbox{out of those in col 1, how many are in row 1?}\\\\ P(X \\leq x) &amp;=&amp; \\sum_{i=0}^x \\frac{ {r \\choose i}{{N-r} \\choose {c-i}}}{ {N \\choose c}}\\\\ &amp;=&amp; \\mbox{p-value} \\end{align}\\] Not common for both row and column totals to be fixed. (More likely for just column totals to be fixed, e.g., men and women.) Instead, consider all subsets of the sample space with \\(N\\) observations. For any particular combination of row and column totals (\\(rc\\)): \\[\\begin{align} P(\\mbox{rejecting } H_0 | rc, H_0) &amp;\\leq&amp; \\alpha\\\\ P(\\mbox{rejecting } H_0 \\ \\ \\forall \\mbox{ subsets } | H_0) &amp;\\leq&amp; \\sum_{rc \\ combos} P(\\mbox{rejecting } H_0 | rc, H_0) P(rc | H_0)\\\\ &amp;\\leq&amp; \\alpha \\end{align}\\] (Note: assume $P(rc | H_0) = 1 / # rc $ combos.) The test will be valid at any \\(\\alpha\\) level, but it won’t be as powerful as one in which fixed columns/rows is actually meaningful. 5.4.3.2 \\(r \\times c\\) tables permute data in a new way new test stat \\[\\begin{align} T = \\sum_{i,j} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}} \\end{align}\\] 2-sided p-value. what do we expect? \\[\\begin{align} E_{i,j} = \\frac{R_i C_j}{N} \\end{align}\\] 5.4.3.2.1 Example: Observer In a study published in the Journal of Personality and Social Psychology (Butler and Baumeister, 1998), researchers investigated a conjecture that having an observer with a vested interest would decrease subjects’ performance on a skill-based task. Subjects were given time to practice playing a video game that required them to navigate an obstacle course as quickly as possible. They were then told to play the game on final time with an observer present. Subjects were randomly assigned to one of two groups: Group A was told that the participant and the observer would each win $3 if the participant beat a certain threshold. Group B was told that only that the participant would win the prize if the threshold was beaten. The goal of this data analysis is to determine whether or not there is an effect from the observer on the performance. That is, like the \\(\\chi^2\\) test, our hypotheses are: \\(H_0:\\) there is no association between the two variables \\(H_a:\\) there is an association between the two variables The data from the 24 subjects is given below: A: shares prize B: no sharing Total Beat threshold 3 8 11 Did not beat threshold 9 4 13 Total 12 12 24 Card simulation (to demonstrate how the permutation test works) Permutation Test (see Chance and Rossman applet for automated permutation test, http://www.rossmanchance.com/applets/ChisqShuffle.htm?FET=1) \\[\\begin{align} SE(\\mbox{p-value}) = \\sqrt{\\frac{\\hat{p}_r (1-\\hat{p}_r)}{100}} = 0.02 \\end{align}\\] Randomization Test \\[\\begin{align} P(X \\leq 3) = \\sum_{i=0}^3 \\frac{ { 11 \\choose i}{12 \\choose {12-i}}}{ { 24 \\choose 12 }} = 0.0436 \\end{align}\\] 5.4.3.3 Two sample test – computationally very difficult to do a randomization test 5.4.3.3.1 Example: Cloud Seeding Cloud seeding data: seeding or not seeding was randomly allocated to 52 days when seeding was appropriate. The pilot did not know whether or not the plane was seeding. Rain is measured in acre-feet. After running tests to compare means and variances we obtain the following p-values: comparison of means comparison of variances Permutation t-test Permutation F-test Raw Data 0.031 0.054 0.068 0.000067 Logged Data 0.010 0.014 0.535 0.897 5.4.3.3.1.1 R code Before doing anything, let’s look at the data. Here, we visualize with both boxplots and histograms. Also, we visualize on the raw scale as well as the log scale. Certainly, the log10 scale indicates that a transformation makes the data more symmetric. clouds &lt;- read_delim(&quot;https://dasl.datadescription.com/download/data/3117/cloud-seeding.txt&quot;, &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) names(clouds) &lt;- c(&quot;unseeded&quot;, &quot;seeded&quot;) clouds &lt;- tidyr::pivot_longer(clouds, cols = 1:2, names_to = &quot;seeding&quot;, values_to = &quot;rainfall&quot;) %&gt;% mutate(seeding = as.factor(seeding)) clouds %&gt;% ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() clouds %&gt;% ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) clouds %&gt;% ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() + scale_y_log10() clouds %&gt;% ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) + scale_x_log10() #unlogged data: clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) ## # A tibble: 2 x 3 ## seeding meanrain meanlnrain ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 seeded 442. 5.13 ## 2 unseeded 165. 3.99 clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %&gt;% summarize(diff(meanrain), diff(meanlnrain)) ## # A tibble: 1 x 2 ## `diff(meanrain)` `diff(meanlnrain)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -277. -1.14 raindiffs &lt;- clouds %&gt;% mutate(lnrain = log(rainfall)) %&gt;% group_by(seeding) %&gt;% summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %&gt;% summarize(diffrain = diff(meanrain), difflnrain = diff(meanlnrain)) raindiffs ## # A tibble: 1 x 2 ## diffrain difflnrain ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -277. -1.14 Below, we’ve formally gone through a permutation. hhere, the resampling is not coded in a particularly tidy way, but there is a tidy way to code loops! Generally, loops are not the fasted way to code in R, so if you need to quickly run code that seems like it should go in a loop, it is very likely that purrr is the direction you want to go, https://purrr.tidyverse.org/. 5.4.3.3.2 Difference in means after permuting reps &lt;- 1000 permdiffs &lt;- c() for(i in 1:reps){ onediff &lt;- clouds %&gt;% mutate(permseed = sample(seeding)) %&gt;% group_by(permseed) %&gt;% summarize(meanrain = mean(rainfall)) %&gt;% summarize(diff(meanrain)) %&gt;% pull() permdiffs &lt;- c(permdiffs, onediff) } permdiffs %&gt;% data.frame() %&gt;% ggplot(aes(x = permdiffs)) + geom_histogram(bins=30) + geom_vline(xintercept = raindiffs$diffrain, color = &quot;red&quot;) 5.4.3.3.3 Ratio of variances after permuting rainvarratio &lt;- clouds %&gt;% group_by(seeding) %&gt;% summarize(varrain = var(rainfall)) %&gt;% summarize(rainratio = varrain[1] / varrain[2]) reps &lt;- 1000 permvars &lt;- c() for(i in 1:reps){ oneratio &lt;- clouds %&gt;% mutate(permseed = sample(seeding)) %&gt;% group_by(permseed) %&gt;% summarize(varrain = var(rainfall)) %&gt;% summarize(varrain[1] / varrain[2]) %&gt;% pull() permvars &lt;- c(permvars, oneratio) } permvars %&gt;% data.frame() %&gt;% ggplot(aes(x = permvars)) + geom_histogram(bins=30) + geom_vline(xintercept = rainvarratio$rainratio , color = &quot;red&quot;) 5.4.3.3.4 Testing differences in means or ratios of variances As evidenced in the histograms above, the permutation test (one-sided) for the difference in means will count the number of permuted differences that are less than or equal to the observed difference in means, just over 1%. the permutation test (one-sided) for the ratio of variances will count the number of permuted ratios that are greater than or equal to the observered ratio of varances, about 7%. (sum(raindiffs$diffrain &gt;= permdiffs) + 1) /1000 ## [1] 0.027 (sum(rainvarratio$rainratio &lt;= permvars)+1)/1000 ## [1] 0.081 "],
["references.html", "References", " References "]
]
