<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 9 Unsupervised Methods | Computational Statistics</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="9.1 11/21/19 Agenda unsupervised methods distance / dissimilarity hierarchical clustering partitioning clustering The classification models we’ve discussed are all supervised learning techniques....">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="Chapter 9 Unsupervised Methods | Computational Statistics">
<meta property="og:type" content="book">
<meta property="og:description" content="9.1 11/21/19 Agenda unsupervised methods distance / dissimilarity hierarchical clustering partitioning clustering The classification models we’ve discussed are all supervised learning techniques....">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 9 Unsupervised Methods | Computational Statistics">
<meta name="twitter:description" content="9.1 11/21/19 Agenda unsupervised methods distance / dissimilarity hierarchical clustering partitioning clustering The classification models we’ve discussed are all supervised learning techniques....">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11.1/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Computational Statistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="visualization.html"><span class="header-section-number">2</span> Visualization</a></li>
<li><a class="" href="wrang.html"><span class="header-section-number">3</span> Data Wrangling</a></li>
<li><a class="" href="sims.html"><span class="header-section-number">4</span> Simulating</a></li>
<li><a class="" href="permschp.html"><span class="header-section-number">5</span> Permutation Tests</a></li>
<li><a class="" href="boot.html"><span class="header-section-number">6</span> Bootstrapping</a></li>
<li><a class="" href="ethics.html"><span class="header-section-number">7</span> Ethics</a></li>
<li><a class="" href="class.html"><span class="header-section-number">8</span> Classification</a></li>
<li><a class="active" href="unsup.html"><span class="header-section-number">9</span> Unsupervised Methods</a></li>
<li><a class="" href="misc.html"><span class="header-section-number">10</span> Misc</a></li>
<li><a class="" href="compstat.html"><span class="header-section-number">11</span> Computational Statistics</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="unsup" class="section level1" number="9">
<h1>
<span class="header-section-number">9</span> Unsupervised Methods<a class="anchor" aria-label="anchor" href="#unsup"><i class="fas fa-link"></i></a>
</h1>
<div id="Nov21" class="section level2" number="9.1">
<h2>
<span class="header-section-number">9.1</span> 11/21/19 Agenda<a class="anchor" aria-label="anchor" href="#Nov21"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>unsupervised methods</li>
<li>distance / dissimilarity</li>
<li>hierarchical clustering</li>
<li>partitioning clustering</li>
</ol>
<p>The classification models we’ve discussed are all supervised learning techniques. The word <em>supervised</em> refers to the fact that we know the response variable of all of the training observations. Next up, we’ll discuss clustering which is an <em>unsupervised</em> technique – none of the observations have a given response variable. For example, we might want to cluster a few hundred melanoma patients based on their genetic data. We are looking for patterns in who groups together, but we don’t have a preconceived idea of which patients belong to which group.</p>
<p>There are also semi-supervised techniques applied to data which have some observations that are labeled and some that are not. We will not discuss semi-supervised methods in this class.</p>
<p><strong>Clustering</strong> creates groups of observations via unsupervised methods. We will cover hierarchical clustering, k-means, and k-medoids. We cluster for two main reasons:</p>
<ul>
<li>Summary: to describe the data and the observations’ similarities to each other.</li>
<li>Discovery: to find new ways in which groups of observations are similar.</li>
</ul>
<p><strong>Classification – SUPERVISED!</strong> creates predictions (and prediction models) for unknown future observations via supervised methods. With classification the group membership (i.e., response variable) is <em>known</em> for all the training data. We covered k-NN, CART, bagging, Random Forests, and SVMs.</p>
</div>
<div id="latent-dirichlet-allocation" class="section level2" number="9.2">
<h2>
<span class="header-section-number">9.2</span> Latent Dirichlet Allocation<a class="anchor" aria-label="anchor" href="#latent-dirichlet-allocation"><i class="fas fa-link"></i></a>
</h2>
<p>LDA views each document as a mixture of a small (predefined) number of topics that describe a set of documents. Each word (typically very common and extremely rare words are removed before modeling) represents a an occurrence generated by one of the document’s topics (where the document is modeled as a mixture of topics). Through LDA, the model learns both the composition of each topic and the topic mixture of each document.</p>
<p>From Wikipedia:</p>
<blockquote>
<p>In natural language processing, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word’s creation is attributable to one of the document’s topics.</p>
</blockquote>
<p>Here is an algorithm for finding words that represent <span class="math inline">\(K\)</span> topics (where <span class="math inline">\(K\)</span> is chosen in advance). [Explained in more detail at <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/" class="uri">http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/</a>.]</p>
<ol style="list-style-type: decimal">
<li>For each document in the training data, randomly assign each word to one of the <span class="math inline">\(K\)</span> topics.</li>
<li>Each of the <span class="math inline">\(K\)</span> topics now has a set of words associated with it (albeit, that set of words is meaningless). To improve on the set of words associated with each topic. For each word <span class="math inline">\(w\)</span> in document <span class="math inline">\(d\)</span>:
<ol style="list-style-type: lower-alpha">
<li>For each topic <span class="math inline">\(t\)</span>, compute two things:
<ul>
<li>p(topic <span class="math inline">\(t\)</span> <span class="math inline">\(|\)</span> document <span class="math inline">\(d\)</span>) = the proportion of words in document <span class="math inline">\(d\)</span> that are currently assigned to topic <span class="math inline">\(t\)</span>
</li>
<li>p(word <span class="math inline">\(w\)</span> <span class="math inline">\(|\)</span> topic <span class="math inline">\(t\)</span>) = the proportion of assignments to topic <span class="math inline">\(t\)</span> over all documents that come from this word <span class="math inline">\(w\)</span>.</li>
</ul>
</li>
<li>Reassign <span class="math inline">\(w\)</span> to a new topic, where we choose topic <span class="math inline">\(t\)</span> with probability = p(topic <span class="math inline">\(t\)</span> <span class="math inline">\(|\)</span> document <span class="math inline">\(d\)</span>) * p(word <span class="math inline">\(w\)</span> <span class="math inline">\(|\)</span> topic <span class="math inline">\(t\)</span>) (according to our generative model, this is essentially the probability that topic <span class="math inline">\(t\)</span> generated word <span class="math inline">\(w\)</span>, so it makes sense that we resample the current word’s topic with this probability).</li>
<li>In other words, in this step, we’re assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated.</li>
</ol>
</li>
<li>After repeating the previous steps a large number of times, the list of words in each topic will reach a steady state. The resulting assignments estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall).</li>
</ol>
<p><a href="https://ziqixiong.shinyapps.io/TopicModeling/" class="uri">https://ziqixiong.shinyapps.io/TopicModeling/</a></p>
</div>
<div id="dissimilarities" class="section level2" number="9.3">
<h2>
<span class="header-section-number">9.3</span> Dissimilarities<a class="anchor" aria-label="anchor" href="#dissimilarities"><i class="fas fa-link"></i></a>
</h2>
<p>Many, though not all, clustering algorithms are based on distances between the objects being clustered. Mathematical properties of a distance function are the following. Consider two vectors <span class="math inline">\({\bf x}\)</span> and <span class="math inline">\({\bf y}\)</span> (<span class="math inline">\({\bf x}, {\bf y} \in \mathbb{R}^p\)</span>), and the distance between them: <span class="math inline">\(d({\bf x}, {\bf y})\)</span>.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(d({\bf x}, {\bf y}) \geq 0\)</span></li>
<li><span class="math inline">\(d({\bf x}, {\bf y}) = d({\bf y}, {\bf x})\)</span></li>
<li>
<span class="math inline">\(d({\bf x}, {\bf y}) = 0\)</span> iff <span class="math inline">\({\bf x} = {\bf y}\)</span>
</li>
<li>
<span class="math inline">\(d({\bf x}, {\bf y}) \leq d({\bf x}, {\bf z}) + d({\bf z}, {\bf y})\)</span> for all other vectors <span class="math inline">\({\bf z}\)</span>.</li>
</ol>
<p><strong>Triangle Inequality</strong></p>
<p>The key to proving the triangle inequality for most of the distances relies on the Cauchy-Schwarz inequality.
<span class="math display">\[\begin{align}
{\bf x} \cdot {\bf y} &amp;= || {\bf x} ||  ||{\bf y}|| \cos(\theta) \\
|{\bf x} \cdot {\bf y}| &amp;\leq || {\bf x} ||  ||{\bf y}|| 
\end{align}\]</span></p>
<div id="euclidean-distance" class="section level4 unnumbered">
<h4>Euclidean Distance<a class="anchor" aria-label="anchor" href="#euclidean-distance"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[d_E({\bf x}, {\bf y}) = \sqrt{\sum_{i=1}^p (x_i - y_i)^2}\]</span></p>
<p>Distance properties all check out.</p>
<ol start="4" style="list-style-type: decimal">
<li>Cauchy-Schwarz:
<span class="math display">\[\begin{align}
\sum_{i=1}^p(x_i - y_i)^2 = \sum_{i=1}^p ( (x_i - z_i) + (z_i - y_i))^2 &amp;\leq \Bigg( \sqrt{\sum_{i=1}^p(x_i - z_i)^2} + \sqrt{\sum_{i=1}^p(z_i - y_i)^2} \Bigg)^2\\
\sqrt{\sum_{i=1}^p(x_i - y_i)^2} &amp;\leq \sqrt{\sum_{i=1}^p(x_i - z_i)^2} + \sqrt{\sum_{i=1}^p(z_i - y_i)^2}\\
d_E({\bf x}, {\bf y}) &amp;\leq d_E({\bf x}, {\bf z}) + d_E({\bf z}, {\bf y})
\end{align}\]</span>
</li>
</ol>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>
<span class="math inline">\(d_E\)</span> is not scale invariant.</li>
<li>
<span class="math inline">\(d_E\)</span> measures magnitude differences, not pattern differences.</li>
<li>
<span class="math inline">\(d_E\)</span> is sensitive to outliers.</li>
</ul>
<p><strong>Strengths:</strong></p>
<ul>
<li>Directly measures what is commonly considered to be “distance.”</li>
</ul>
</div>
<div id="pearson-correlation-distance" class="section level4 unnumbered">
<h4>Pearson Correlation Distance<a class="anchor" aria-label="anchor" href="#pearson-correlation-distance"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{align}
d_P({\bf x}, {\bf y}) &amp;= 1 - r_P ({\bf x}, {\bf y})\\
 \mbox{ or } &amp;= 1 - |r_P ({\bf x}, {\bf y})|\\
 \mbox{ or }   &amp;= 1 - (r_P ({\bf x}, {\bf y}))^2\\
  \end{align}\]</span></p>
<p>Notice that Euclidean distance and Pearson correlation distance are similar if the original observations are scaled. Assume that the sample mean for <span class="math inline">\({\bf x}\)</span> (that is, <span class="math inline">\(\frac{1}{p} \sum x_i = \overline{x} = 0\)</span>) is zero and the sample standard deviation is 1.</p>
<p><span class="math display">\[\begin{align}
 r_P ({\bf x}, {\bf y}) &amp;=  \frac{\sum x_i y_i - p \ \overline{x} \ \overline{y}}{(p-1)s_x s_y}\\
 &amp;=  \frac{1}{(p-1)} \sum x_i y_i\\
 &amp; \ \ &amp; \\
 d_E({\bf x}, {\bf y}) &amp;= \sqrt{\sum(x_i - y_i)^2}\\
 &amp;=  \sqrt{ \sum x_i^2 + \sum y_i^2 - 2 \sum x_i y_i}\\
 d_E^2 &amp;= 2[(p-1) - \sum x_i y_i]\\
 &amp;= 2(p-1)*[1 - r_P({\bf x}, {\bf y})]
 \end{align}\]</span></p>
<p>Distance properties don’t hold for Pearson correlation.</p>
<ol start="3" style="list-style-type: decimal">
<li><p><span class="math inline">\({\bf y}=a{\bf x}\)</span>
<span class="math display">\[\begin{align}
d_P({\bf x}, {\bf y}) &amp;= 1 - r_P ({\bf x}, {\bf y})\\
&amp;= 1 - r_P ({\bf x}, a{\bf x})\\
&amp;= 1 - 1 = 0
\end{align}\]</span></p></li>
<li><p><span class="math inline">\({\bf x}=(1,1,0)\)</span>, <span class="math inline">\({\bf y} = (2,1,0)\)</span>, <span class="math inline">\({\bf z} = (1,-1,0)\)</span>
<span class="math inline">\(r_P({\bf x}, {\bf y}) = 0.87\)</span>, <span class="math inline">\(r_P({\bf x}, {\bf z}) = 0\)</span>, <span class="math inline">\(r_P({\bf y}, {\bf z}) = 0.5\)</span></p></li>
</ol>
<p><span class="math inline">\(d_P({\bf x}, {\bf y}) + d_P({\bf y}, {\bf z}) &lt; d_P({\bf z}, {\bf x})\)</span>
<span class="math inline">\(\rightarrow\leftarrow\)</span></p>
<div class="inline-figure"><img src="figs/corEucdist.jpeg" width="80%" style="display: block; margin: auto;"></div>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>
<span class="math inline">\(d_P\)</span> does not satisfy the triangle inequality.</li>
<li>
<span class="math inline">\(d_P\)</span> is sensitive to outliers.</li>
</ul>
<p><strong>Strengths:</strong></p>
<ul>
<li>Can measure the distance between variables on different scales (although will still be sensitive to extreme values).</li>
</ul>
</div>
<div id="spearman-correlation-distance" class="section level4 unnumbered">
<h4>Spearman Correlation Distance<a class="anchor" aria-label="anchor" href="#spearman-correlation-distance"><i class="fas fa-link"></i></a>
</h4>
<p>Spearman correlation distance uses the Spearman correlation instead of the Pearson correlation. The Spearman correlation is simply the Pearson correlation applied to the ranks of the observations. The ranking allows the Spearman distance to be resistant to outlying observations.</p>
<p><span class="math display">\[\begin{align}
d_S({\bf x}, {\bf y}) &amp;= 1 - r_S ({\bf x}, {\bf y})\\
 \mbox{ or } &amp;= 1 - |r_S ({\bf x}, {\bf y})|\\
 \mbox{ or }   &amp;= 1 - (r_S ({\bf x}, {\bf y}))^2\\
  \end{align}\]</span></p>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>
<span class="math inline">\(d_S\)</span> also does not satisfy the triangle inequality.</li>
<li>
<span class="math inline">\(d_S\)</span> loses information about the shape of the relationship.</li>
</ul>
<p><strong>Strengths:</strong></p>
<ul>
<li>Is resistant to outlying values</li>
</ul>
</div>
<div id="cosine-distance" class="section level4 unnumbered">
<h4>Cosine Distance<a class="anchor" aria-label="anchor" href="#cosine-distance"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{align}
d_C({\bf x}, {\bf y}) &amp;=  \frac{{\bf x} \cdot {\bf y}}{|| {\bf x} ||  ||{\bf y}||}\\
&amp;= \frac{\sum_{i=1}^p x_i y_i}{\sqrt{\sum_{i=1}^p x_i^2 \sum_{i=1}^p y_i^2}}\\
&amp;= 1 - r_P ({\bf x}, {\bf y})  \ \ \ \ \mbox{if } \overline{\bf x} = \overline{\bf y} = 0
\end{align}\]</span></p>
<p>Said differently,</p>
<p><span class="math display">\[\begin{align}
d_P({\bf x}, {\bf y}) = d_C({\bf x} -  \overline{\bf x}, {\bf y} -  \overline{\bf y})
\end{align}\]</span></p>
</div>
<div id="haversine-distance" class="section level4 unnumbered">
<h4>Haversine Distance<a class="anchor" aria-label="anchor" href="#haversine-distance"><i class="fas fa-link"></i></a>
</h4>
<p>Haversine distance is the great-circle distance (i.e., the distance between two points on a sphere) which is used to measure distance between two locations on the Earth. Let <span class="math inline">\(R\)</span> be the radius of the Earth, and (lat1,long1) and (lat2, long2) be the two locations between which to calculate a distance.</p>
<p><span class="math display">\[d_{HV} = 2 R \arcsin \sqrt{\sin^2 \bigg( \frac{lat2-lat1}{2} \bigg) + \cos(lat1) \cos(lat2) \sin^2 \bigg(\frac{long2 - long1}{2} \bigg)} \]</span></p>
<div class="inline-figure"><img src="figs/havdist.png" width="40%" style="display: block; margin: auto;"></div>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>Earth is not a perfect sphere</li>
<li>Depending on how the distance is used, typically getting from one point to the next is not done by the shortest distance</li>
</ul>
<p><strong>Strengths:</strong></p>
<ul>
<li>Allows calculations, for example, between two cities.</li>
</ul>
</div>
<div id="hamming-distance" class="section level4 unnumbered">
<h4>Hamming Distance<a class="anchor" aria-label="anchor" href="#hamming-distance"><i class="fas fa-link"></i></a>
</h4>
<p>Hamming distance is the number of coordinates across two vectors whose values differ. If the vectors are binary, the Hamming distance is equivalent to the <span class="math inline">\(L_1\)</span> norm of the difference. (Hamming distance does satisfy the properties of a distance metric.) Some methods, equivalently, calculate the proportion of coordinates that differ.</p>
<p><span class="math display">\[\begin{align}
d_H({\bf x}, {\bf y}) = \sum_{i=1}^p I(x_i \ne y_i)
\end{align}\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="figs/hamdistGCTA.png" alt="The Hamming distance across the two DNA strands is 7." width="60%"><p class="caption">
Figure 1.4: The Hamming distance across the two DNA strands is 7.
</p>
</div>
<p><strong>Shortcomings:</strong></p>
<ul>
<li>Can’t measure degree of difference between categorical variables.</li>
</ul>
<p><strong>Strengths:</strong></p>
<ul>
<li>It is a distance metric.
Gives a more direct “distance” between categorical variables.</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="figs/distR.png" alt="The function `dist` in `R` calculates the distances given above." width="100%"><p class="caption">
Figure 1.5: The function <code>dist</code> in <code>R</code> calculates the distances given above.
</p>
</div>
</div>
</div>
<div id="hier" class="section level2" number="9.4">
<h2>
<span class="header-section-number">9.4</span> Hierarchical Clustering<a class="anchor" aria-label="anchor" href="#hier"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Hierarchical Clustering</strong> is a set of nested clusters that are organized as a tree. Note that objects that belong to a child cluster also belong to the parent cluster.</p>
<p><strong>Example:</strong> Consider the following images / data (from Laura Hoopes, personal communication; <em>Molecular characterisation of soft tissue tumours: a gene expression study</em> by Nielsen et al., The Lancet 2002). The first represents a microarray sample from aging yeast. The second is a set of 41 samples of soft-tissue tumors (columns) and a subset of 5520 genes (rows) used to characterize their molecular signatures.</p>
<p><img src="figs/dendro.jpg" width="30%" style="display: block; margin: auto;"><img src="figs/LH_microarray_small.jpg" width="30%" style="display: block; margin: auto;"></p>
<p>Note: the ordering of the variables (or samples) does not affect the clustering of the samples (or variables). That is: we can clustering the variables / samples either sequentially or in parallel to see trends in both relationships simultaneously. Clustering both the observations and the variables is called <em>biclustering</em>.</p>
<hr>
<p><strong>Algorithm</strong>: Agglomerative Hierarchical Clustering Algorithm</p>
<hr>
<ol style="list-style-type: decimal">
<li>Begin with <span class="math inline">\(n\)</span> observations and a measure (such as Euclidean distance) of all the <span class="math inline">\({n \choose 2} = n(n-1)/2\)</span> pairwise dissimilarities. Treat each observation as its own cluster.</li>
<li>For <span class="math inline">\(i = n, n - 1, \ldots , 2\)</span>:</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Examine all pairwise inter-cluster dissimilarities among the <span class="math inline">\(i\)</span> clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.</li>
<li>Compute the new pairwise inter-cluster dissimilarities among the <span class="math inline">\(i - 1\)</span> remaining clusters.</li>
</ol>
<hr>
<p><strong>Agglomerative</strong> methods start with each object (e.g., gene) in its own group. Groups are merged until all objects are together in one group.</p>
<p><strong>Divisive</strong> methods start with all objects in one group and break up the groups sequentially until all objects are individuals.</p>
<p><strong>Single Linkage</strong> algorithm defines the distance between groups as that of the closest pair of individuals.</p>
<p><strong>Complete Linkage</strong> algorithm defines the distance between groups as that of the farthest pair of individuals.</p>
<p><strong>Average Linkage</strong> algorithm defines the distance between groups as the average of the distances between all pairs of individuals across the groups.</p>
<p>Toy Example of <strong>Single Linkage Agglomerative Hierarchical Clustering</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>B</td>
<td>0.2</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.6</td>
<td>0.5</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>D</td>
<td>1</td>
<td>0.9</td>
<td>0.4</td>
<td>0</td>
<td></td>
</tr>
<tr class="odd">
<td>E</td>
<td>0.9</td>
<td>0.8</td>
<td>5</td>
<td>0.3</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>Link A and B!
<span class="math display">\[\begin{align}
d_{(AB)C} &amp;= \min(d_{AC}, d_{BC}) = 0.5\\
d_{(AB)D} &amp;= \min(d_{AD}, d_{BD}) = 0.9\\
d_{(AB)E} &amp;= \min(d_{AE}, d_{BE}) = 0.8\\
\end{align}\]</span></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>AB</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>AB</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>C</td>
<td>0.5</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>D</td>
<td>0.9</td>
<td>0.4</td>
<td>0</td>
<td></td>
</tr>
<tr class="even">
<td>E</td>
<td>0.8</td>
<td>0.5</td>
<td>0.3</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>Link D and E!
<span class="math display">\[\begin{align}
d_{(AB)C} &amp;=  0.5\\
d_{(AB)(DE)} &amp;= \min(d_{AD}, d_{BD}, d_{AE}, d_{BE}) = 0.8\\
d_{(DE)C} &amp;= \min(d_{CD}, d_{CE}) = 0.4\\
\end{align}\]</span></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>AB</th>
<th>C</th>
<th>DE</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>AB</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>C</td>
<td>0.5</td>
<td>0</td>
<td></td>
</tr>
<tr class="odd">
<td>DE</td>
<td>0.8</td>
<td>0.4</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>Link C with (DE)!
<span class="math display">\[\begin{align}
d_{(AB)(CDE)} = d_{BC} = 0.5
\end{align}\]</span></p>
<div class="inline-figure"><img src="figs/singlexamp.JPG" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="part" class="section level2" number="9.5">
<h2>
<span class="header-section-number">9.5</span> Partitioning Clustering<a class="anchor" aria-label="anchor" href="#part"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Partition Clustering</strong> is a division of the set of data objects into <span class="math inline">\(K\)</span> non-overlapping subsets (clusters) with each observation falling into exactly one cluster.</p>
<p>In contrast to hierarchical clustering where results were given for any (and all!) number of clusters, partitioning methods typically start with a given <span class="math inline">\(k\)</span> value and a set of distances. The goal is to partition the observations into <span class="math inline">\(k\)</span> groups such that an objective function is optimized. The number of possible partitions is roughly to <span class="math inline">\(n^k / k!\)</span> (note: <span class="math inline">\(100^{5} / 5! = 83\)</span> million). [The exact number can be computed using Sterling numbers.] So instead of looking through all of the partitions, we step through a recursive algorithm.</p>
<div id="k-means-clustering" class="section level3" number="9.5.1">
<h3>
<span class="header-section-number">9.5.1</span> <span class="math inline">\(k\)</span>-means Clustering<a class="anchor" aria-label="anchor" href="#k-means-clustering"><i class="fas fa-link"></i></a>
</h3>
<p>A fun applet!!</p>
<p><a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/" class="uri">https://www.naftaliharris.com/blog/visualizing-k-means-clustering/</a></p>
<!--
http://www.math.le.ac.uk/people/ag153/homepage/KmeansKmedoids/Kmeans_Kmedoids.html#L2

See also the guide to the applet which give lots of great advice.

% good datasets and also some interesting reflections on using k-means in R
% https://www.datacamp.com/community/tutorials/k-means-clustering-r?utm_campaign=News&utm_medium=Community&utm_source=DataCamp.com
-->
<p><span class="math inline">\(k\)</span>-means clustering is an unsupervised partitioning algorithm designed to find a partition of the observations such that the following objective function is minimized (find the smallest within cluster sum of squares):</p>
<p><span class="math display">\[\argmin_{C_1, \ldots, C_k} \Bigg\{ \sum_{k=1}^K 2 \sum_{i \in C_k} \sum_{j=1}^p (x_{ij} - \overline{x}_{kj})^2 \Bigg\}\]</span></p>
<p>As described in the algorithm below, reallocating observations can only improve the minimization criteria with the algorithm stopping when no changes of the observations will lower the objective function. The algorithm leads to a <em>local optimum</em>, with no confirmation that the global minimum has occurred. Often the <span class="math inline">\(k\)</span>- means algorithm is run multiple times with different random starts, and the partition leading to the lowest objective criteria is chosen.</p>
<p>Note that the following algorithm is simply one <span class="math inline">\(k\)</span>-means algorithm. Other algorithms could include a different way to set the starting values, a different decision on when to recalculate the centers, what to do with ties, etc.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-8"></span>
<img src="figs/kmeansISLRswap.png" alt="From *An Introduction to Statistical Learning* by James, Witten, Hastie, and Tibshirani." width="45%"><img src="figs/kmeansISLRrand.png" alt="From *An Introduction to Statistical Learning* by James, Witten, Hastie, and Tibshirani." width="45%"><p class="caption">
Figure 7.2: From <em>An Introduction to Statistical Learning</em> by James, Witten, Hastie, and Tibshirani.
</p>
</div>
<hr>
<p><strong>Algorithm</strong>: <span class="math inline">\(k\)</span>-Means Clustering</p>
<hr>
<ol style="list-style-type: decimal">
<li>Randomly assign a number, from 1 to <span class="math inline">\(k\)</span>, to each of the observations. These serve as initial cluster assignments for the observations.</li>
<li>Iterate until the cluster assignments stop changing:
<ol style="list-style-type: lower-alpha">
<li>For each of the <span class="math inline">\(k\)</span> clusters, compute the cluster centroid. The <span class="math inline">\(k^{th}\)</span> cluster centroid is the vector of the <span class="math inline">\(p\)</span> feature means for the observations in the <span class="math inline">\(k^{th}\)</span> cluster.</li>
<li>Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).</li>
</ol>
</li>
<li>Ties? Do something consistent: for example, leave in the current cluster.</li>
</ol>
<hr>
<p>Why does the <span class="math inline">\(k\)</span>-means algorithm converge / (local) minimize the objective function?</p>
<ol style="list-style-type: decimal">
<li>If a point is closer to a different center, moving it will lower the objective function.</li>
<li>Averages minimize squared differences, so taking the new average will result in a lower objective function.</li>
<li>If a point is equidistant from two clusters, the point won’t move.</li>
<li>The algorithm must converge in finite number of steps because there are finitely many points.</li>
</ol>
<p><strong>strengths</strong></p>
<ul>
<li>No hierarchical structure / points can move from one cluster to another.</li>
<li>Can run for a range of values of <span class="math inline">\(k\)</span>.</li>
</ul>
<p><strong>shortcomings</strong></p>
<ul>
<li>
<span class="math inline">\(k\)</span> has to be predefined to run the algorithm.</li>
<li>
<span class="math inline">\(k\)</span>-means is based on Euclidean distance (<em>only</em>).</li>
</ul>
</div>
<div id="partitioning-around-medoids" class="section level3" number="9.5.2">
<h3>
<span class="header-section-number">9.5.2</span> Partitioning Around Medoids<a class="anchor" aria-label="anchor" href="#partitioning-around-medoids"><i class="fas fa-link"></i></a>
</h3>
<p>As an alternative to <span class="math inline">\(k\)</span>-means, Kaufman and Rousseeuw developed Partitioning around Medoids (<em>Finding Groups in Data: an introduction to cluster analysis</em>, 1990). The particular strength of PAM is that it allows for <em>any</em> dissimilarity metric. That is, a dissimilarity based on correlations is no problem, but the algorithm gets more complicated because the “center” is no longer defined in Euclidean terms.</p>
<p>The two main steps are to Build (akin to assigning points to clusters) and to Swap (akin to redefining cluster centers). The objective function the algorithm tries to minimize is the average dissimilarity of objects to the closest representative object. (The PAM algorithm is a good (not necessarily global optimum) solution to minimizing the objective function.)<br><span class="math display">\[\argmin_{C_1, \ldots, C_k} \Bigg\{ \sum_{k=1}^K \sum_{i \in C_k}D_i \Bigg\} = \argmin_{C_1, \ldots, C_k} \Bigg\{ \sum_{k=1}^K \sum_{i \in C_k}d(x_i, m_k) \Bigg\}\]</span>
Where <span class="math inline">\(D_i\)</span> represents this distance from observation <span class="math inline">\(i\)</span> to the closest medoid, <span class="math inline">\(m_k\)</span>.</p>
<p><strong>strengths</strong></p>
<ul>
<li>No hierarchical structure / points can move from one cluster to another.</li>
<li>Can run for a range of values of <span class="math inline">\(k\)</span>.</li>
<li>It can use any distance measure</li>
</ul>
<p><strong>shortcomings</strong></p>
<ul>
<li>
<span class="math inline">\(k\)</span> has to be predefined to run the algorithm.</li>
</ul>
<hr>
<p><strong>Algorithm</strong>: Partitioning Around Medoids (From <em>The Elements of Statistical Learning</em> (2001), by Hastie, Tibshirani, and Friedman, pg 469.)</p>
<hr>
<ol style="list-style-type: decimal">
<li><p>Randomly assign a number, from 1 to <span class="math inline">\(K\)</span>, to each of the observations. These serve as initial cluster assignments for the observations.</p></li>
<li><p>Iterate until the cluster assignments stop changing:</p></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>(Repeat for <span class="math inline">\(k \in \{1, 2, ...K\}\)</span>) For a given cluster, <span class="math inline">\(C_k\)</span>, find the observation in the cluster minimizing total distance to other points in that cluster:
<span class="math display">\[i^*_k = \argmin_{i \in C_k} \sum_{i' \in C_k} d(x_i, x_{i'})\]</span>
Then <span class="math inline">\(m_k = x_{i^*_k}, k=1, 2, \ldots, K\)</span> are the current estimates of the cluster centers.</li>
<li>Given a current set of cluster centers <span class="math inline">\(\{m_1, m_2, \ldots, m_K\}\)</span>, minimize the total error by assigning each observation to the closest (current) cluster center:
<span class="math display">\[C_i = \argmin_{1 \leq k \leq K} d(x_i, m_k)\]</span>
</li>
</ol>
<hr>
</div>
</div>
<div id="evaluation-metrics" class="section level2" number="9.6">
<h2>
<span class="header-section-number">9.6</span> Evaluation Metrics<a class="anchor" aria-label="anchor" href="#evaluation-metrics"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Silhouette Width</li>
</ul>
<p>Consider observation <span class="math inline">\(i \in\)</span> cluster <span class="math inline">\(Clus1\)</span>. Let
<span class="math display">\[\begin{align}
d(i, Clus2) &amp;= \mbox{average dissimilarity of } i \mbox{ to all objects in cluster } Clus2\\
a(i) &amp;=  \mbox{average dissimilarity of } i \mbox{ to all objects in } Clus1.\\
b(i) &amp;= \min_{Clus2 \ne Clus1} d(i,Clus2) = \mbox{distance to the next closest neighbor cluster}\\
\mbox{ silhouette width} &amp;= s(i) = \frac{b(i) - a(i)}{\max \{ a(i), b(i) \}}\\
&amp; &amp;\\
\mbox{average}_{i \in Clus1} s(i) &amp;= \mbox{average silhouette width for cluster $Clus1$}
\end{align}\]</span>
Note that if <span class="math inline">\(a(i) &lt; b(i)\)</span> then <span class="math inline">\(i\)</span> is well classified with a maximum <span class="math inline">\(s(i) = 1\)</span>. If <span class="math inline">\(a(i) &gt; b(i)\)</span> then <span class="math inline">\(i\)</span> is <em>not</em> well classified with a maximum <span class="math inline">\(s(i) = -1\)</span>.</p>
<ul>
<li><p>Diameter of cluster <span class="math inline">\(Clus1\)</span> (within cluster measure)
<span class="math display">\[\begin{align}
\mbox{diameter} = \max_{i,j \in Clus1} d(i,j)
\end{align}\]</span></p></li>
<li><p>Separation of cluster <span class="math inline">\(Clus1\)</span> (between cluster measure)
<span class="math display">\[\begin{align}
\mbox{separation} = \min_{i \in Clus1, j \notin Clus1} d(i,j)
\end{align}\]</span></p></li>
<li><p><span class="math inline">\(L^*\)</span>: a cluster with diameter <span class="math inline">\(&lt;\)</span> separation; <span class="math inline">\(L\)</span>: a cluster with <span class="math inline">\(\max_{j \in Clus1} d(i,j) &lt; \min_{k \notin Clus1} d(i,k)\)</span>.</p></li>
</ul>
<div id="pam-example" class="section level4 unnumbered">
<h4>PAM example<a class="anchor" aria-label="anchor" href="#pam-example"><i class="fas fa-link"></i></a>
</h4>
<div id="building-the-clusters" class="section level5 unnumbered">
<h5>Building the clusters<a class="anchor" aria-label="anchor" href="#building-the-clusters"><i class="fas fa-link"></i></a>
</h5>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
<th>E</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>B</td>
<td>0.2</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.6</td>
<td>0.5</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>D</td>
<td>1</td>
<td>0.9</td>
<td>0.4</td>
<td>0</td>
<td></td>
</tr>
<tr class="odd">
<td>E</td>
<td>0.9</td>
<td>0.8</td>
<td>5</td>
<td>0.3</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<ul>
<li><p>Start by considering the random allocation (AC) and (BDE)</p></li>
<li><p>As a second step, calculate the within cluster sums of distances:</p></li>
</ul>
<p>A: 0.6</p>
<p>C: 0.6</p>
<p>B: 0.9 + 0.8 = 1.7</p>
<p>D: 0.9 + 0.3 = 1.2</p>
<p>E: 0.8 + 0.3 = 1.1</p>
<p>For cluster 1, it doesn’t matter if we choose A or C (let’s choose C). For cluster 2, we should choose E (it is the most “central” as measured by its closer distance to both B and D).</p>
<ul>
<li>Reallocate points:</li>
</ul>
<p>Cluster1: C <strong>and</strong> all the points that are closer to C than E. A and B are both closer to C than to E. Cluster1 will be (A,B,C).</p>
<p>Cluster2: E <strong>and</strong> all the points that are closer to E than C. Only D is closer to E than C. Cluster2 will be (D,E)</p>
<ul>
<li>Redefine cluster centers:</li>
</ul>
<p>A: 0.2 + 0.6 = 0.8</p>
<p>B: 0.2 + 0.5 = 0.7</p>
<p>C: 0.6 + 0.5 = 1.1</p>
<p>D: 0.3</p>
<p>E: 0.3</p>
<p>Cluster1 now has a medoid of B. Cluster2 (we choose randomly) has a medoid of D.</p>
<ul>
<li>Reallocate points:</li>
</ul>
<p>Cluster1: B <strong>and</strong> A (A,B)</p>
<p>Cluster2: D <strong>and</strong> C, E (D, C, E)</p>
<ul>
<li>The medoids are now A or B (randomly choose) and D. The iteration process has converged.</li>
</ul>
</div>
</div>
<div id="evaluating-the-clusters" class="section level4 unnumbered">
<h4>Evaluating the clusters<a class="anchor" aria-label="anchor" href="#evaluating-the-clusters"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>A</th>
<th>B</th>
<th>C</th>
<th>D</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>A</td>
<td>0</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>B</td>
<td>0.2</td>
<td>0</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>C</td>
<td>0.6</td>
<td>0.5</td>
<td>0</td>
<td></td>
</tr>
<tr class="even">
<td>D</td>
<td>1</td>
<td>0.9</td>
<td>0.4</td>
<td>0</td>
</tr>
</tbody>
</table></div>
<p>(Note: this is the same matrix as before, but with only 4 observations.)</p>
<p>Consider the data with (AB)(CD) as the clusters, we can calculate the previous metrics:</p>
<ul>
<li>
<strong>Silhouette Width</strong>
<span class="math display">\[\begin{align}
s(i=A) = \frac{b(A) - a(A)}{\max \{a(A), b(A)\}} = \frac{0.8 - 0.2}{0.8} = 0.75\\
s(i=B) = \frac{b(B) - a(B)}{\max \{a(B), b(B)\}} = \frac{0.7 - 0.2}{0.7} = 0.71\\
s(i=C) = \frac{b(C) - a(C)}{\max \{a(C), b(C)\}} = \frac{0.55 - 0.4}{0.55} = .27\\
s(i=D) = \frac{b(D) - a(D)}{\max \{a(D), b(D)\}} = \frac{0.95 - 0.4}{0.95} = .57\\
\mbox{Ave SW} = 0.575\\
\end{align}\]</span>
</li>
<li>
<strong>Diameter</strong>
<span class="math display">\[\begin{align}
\mbox{diameter}(AB) = 0.2\\
\mbox{diameter}(CD) = 0.4\\
\end{align}\]</span>
</li>
<li>
<strong>Separation</strong>
<span class="math display">\[\begin{align}
\mbox{separation}(AB) = \mbox{separation}(CD) = 0.5\\
\end{align}\]</span>
</li>
</ul>
</div>
<div id="rand-index-adjusted-rand-index" class="section level4 unnumbered">
<h4>Rand Index / Adjusted Rand Index<a class="anchor" aria-label="anchor" href="#rand-index-adjusted-rand-index"><i class="fas fa-link"></i></a>
</h4>
<p>is based on a confusion matrix comparing either a known truth (labels) or comparing two different clusterings (e.g., comparing <span class="math inline">\(k\)</span>-means and hierarchical clustering). Let the two different clusterings be called partition1 and partition2.
* <strong>a</strong> is the number of pairs of observations put together in both partition1 and partition2
* <strong>b</strong> is the number of pairs of observations together in partition1 and apart in partition2
* <strong>c</strong> is the number of pairs of observations together in partition2 and apart in partition1
* <strong>d</strong> is the number of pairs of observations apart in both partitions</p>
<p><span class="math display">\[\mbox{Rand index} = \frac{a+d}{a+b+c+d}\]</span></p>
<p>The cool thing about the Rand index is that the partitions don’t have to even have the same number of clusters. They can be absolutely any two clusterings (one might be known labels, for example). Details on the Adjusted Rand index are given at <a href="http://faculty.washington.edu/kayee/pca/supp.pdf" class="uri">http://faculty.washington.edu/kayee/pca/supp.pdf</a> (basic idea is to center and scale the Rand index so that the values are more meaningful).</p>
<!--
% http://statweb.stanford.edu/~tibs/stat315a/LECTURES/em.pdf
-->
</div>
</div>
<div id="em-algorithm" class="section level2" number="9.7">
<h2>
<span class="header-section-number">9.7</span> EM algorithm<a class="anchor" aria-label="anchor" href="#em-algorithm"><i class="fas fa-link"></i></a>
</h2>
<p>The EM algorithm is an incredibly useful tool for solving complicated maximization procedures, particularly with respect to maximizing likelihoods (typically for parameter estimation). We will describe the procedure here in the context of estimating the parameters of a two-component mixture model.</p>
<p>Consider the Old Faithful geyser Yellowstone National Park, Wyoming, USA with the following histogram of data on waiting times between each eruption:</p>
<div class="inline-figure"><img src="09-clustering_files/figure-html/unnamed-chunk-9-1.png" width="480" style="display: block; margin: auto;"></div>
<p><span class="math display">\[\begin{align}
Y_1 &amp;\sim N(\mu_1, \sigma_1^2)\\
Y_2 &amp;\sim N(\mu_2, \sigma_2^2)\\
Y &amp;= (1-\Delta) Y_1 + \Delta Y_2\\
P(\Delta=1) &amp;= \pi\\
\end{align}\]</span>
In the simple two component case, we can see that the representation above indicates that first we generate a <span class="math inline">\(\Delta \in \{0,1\}\)</span>, and then, depending on the result, we generate either <span class="math inline">\(Y_1\)</span> or <span class="math inline">\(Y_2\)</span>. The likelihood associated with the above setting is:</p>
<p><span class="math display">\[\begin{align}
g_Y(y) = (1-\pi) \phi_{\theta_1}(y) + \pi \phi_{\theta_2}(y)
\end{align}\]</span>
where <span class="math inline">\(\phi_\theta\)</span> represents the normal distribution with a vector <span class="math inline">\(\theta=(\mu, \sigma)\)</span> of parameters. Typically, in statistical theory, to find <span class="math inline">\(\theta\)</span>, we would take the derivative of the log-likelihood to find the values which maximize. Here, however, the likelihood is too complicated to solve for <span class="math inline">\(\theta\)</span> in closed form.</p>
<p><span class="math display">\[\begin{align}
l(\theta; {\bf y}) = \sum_{i=1}^N \log [(1-\pi) \phi_{\theta_1}(y) + \pi \phi_{\theta_2}(y)].
\end{align}\]</span></p>
<p>If we know which point comes from which distribution, however, the maximization is straightforward in that we can use the points in group one to estimate the parameters from the first distribution, and the points in group two to estimate the parameters in the second distribution. The process of assigning points and estimating parameters can be thought of as two steps:</p>
<ol style="list-style-type: decimal">
<li>
<em>Expectation:</em> an assignment (soft here, because the points are weighted) of each observation to a group.</li>
<li>
<em>Maximization:</em> update the parameter estimates.</li>
</ol>
<hr>
<p><strong>Algorithm</strong>: EM Algorithm for two-component Gaussian mixture. From <em>The Elements of Statistical Learning</em> (2001), by Hastie, Tibshirani, and Friedman, pg 238.</p>
<hr>
<ol style="list-style-type: decimal">
<li>Take initial guesses for the parameters <span class="math inline">\(\hat{\mu}_1, \hat{\sigma}_1^2, \hat{\mu}_2, \hat{\sigma}_2^2, \hat{\pi}\)</span>.</li>
<li>
<em>Expectation Step:</em> compute the responsibilities:
<span class="math display">\[ \hat{\gamma}_i = \frac{\hat{\pi} \phi_{\hat{\theta}_2} (y_i)}{(1-\hat{\pi}) \phi_{\hat{\theta}_1} (y_i) + \hat{\pi} \phi_{\hat{\theta}_2} (y_i)}, i=1, 2, \ldots, N.\]</span>
</li>
<li>
<em>Maximization Step:</em> compute the weighted means and variances:
<span class="math display">\[\begin{align}
\hat{\mu}_1 = \frac{\sum_{i=1}^N (1-\hat{\gamma_i})y_i}{\sum_{i=1}^N (1-\hat{\gamma_i})} &amp;&amp; \hat{\sigma}_1^2 = \frac{\sum_{i=1}^N (1-\hat{\gamma_i})(y_i - \hat{\mu}_1)^2}{\sum_{i=1}^N (1-\hat{\gamma_i})}\\
\hat{\mu}_2 = \frac{\sum_{i=1}^N \hat{\gamma_i}y_i}{\sum_{i=1}^N \hat{\gamma_i}} &amp;&amp; \hat{\sigma}_2^2 = \frac{\sum_{i=1}^N \hat{\gamma_i}(y_i - \hat{\mu}_2)^2}{\sum_{i=1}^N \hat{\gamma_i}}
\end{align}\]</span>
and the mixing probability <span class="math inline">\(\hat{\pi} = \sum_{i=1}^N \hat{\gamma}_i / N\)</span>.</li>
<li>Iterate Steps 2. and 3. until convergence.</li>
</ol>
<hr>
<p>The algorithm shows that for a particular allocation of the points, we can maximize the given likelihood to estimate the parameter values (done in the Maximization Step). However, it is not obvious from the algorithm that the first allocation step leads to a maximization (local or global) of the likelihood. The proof of the EM algorithm converging to a local maximum likelihood (it does not necessarily converge to a global max) uses information on the marginal prior and posterior likelihoods of the parameter values and Jensen’s inequality to show that the likelihood does not decrease through the iterative steps.</p>
<p>Note that in the previous <span class="math inline">\(k\)</span>-means algorithm we iterated between two steps of assigning points to clusters and estimating the cluster centers (we thought of the space as scaled so that the Euclidean distance was appropriate in all dimensions). Two differences in the algorithms we covered are:</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(k\)</span>-means uses hard thresholding and EM uses soft thresholding</li>
<li>
<span class="math inline">\(k\)</span>-means uses a fixed standard deviation of 1, EM allows the data/algorithm to find the standard deviation</li>
</ol>
<p>Indeed, although the EM-algorithm above is slightly different than the previous <span class="math inline">\(k\)</span>-means algorithm, the two methods typically converge to the same result and are both considered to be different implementations of a <span class="math inline">\(k\)</span>-means algorithm.</p>
<p>See the following applet for a visual representation of how the EM-algorithm converges: <a href="http://www.socr.ucla.edu/applets.dir/mixtureem.html" class="uri">http://www.socr.ucla.edu/applets.dir/mixtureem.html</a>.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="class.html"><span class="header-section-number">8</span> Classification</a></div>
<div class="next"><a href="misc.html"><span class="header-section-number">10</span> Misc</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#unsup"><span class="header-section-number">9</span> Unsupervised Methods</a></li>
<li><a class="nav-link" href="#Nov21"><span class="header-section-number">9.1</span> 11/21/19 Agenda</a></li>
<li><a class="nav-link" href="#latent-dirichlet-allocation"><span class="header-section-number">9.2</span> Latent Dirichlet Allocation</a></li>
<li><a class="nav-link" href="#dissimilarities"><span class="header-section-number">9.3</span> Dissimilarities</a></li>
<li><a class="nav-link" href="#hier"><span class="header-section-number">9.4</span> Hierarchical Clustering</a></li>
<li>
<a class="nav-link" href="#part"><span class="header-section-number">9.5</span> Partitioning Clustering</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#k-means-clustering"><span class="header-section-number">9.5.1</span> \(k\)-means Clustering</a></li>
<li><a class="nav-link" href="#partitioning-around-medoids"><span class="header-section-number">9.5.2</span> Partitioning Around Medoids</a></li>
</ul>
</li>
<li><a class="nav-link" href="#evaluation-metrics"><span class="header-section-number">9.6</span> Evaluation Metrics</a></li>
<li><a class="nav-link" href="#em-algorithm"><span class="header-section-number">9.7</span> EM algorithm</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/09-clustering.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/09-clustering.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Computational Statistics</strong>" was written by Jo Hardin. It was last built on 2021-09-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
