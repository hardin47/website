<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Classification | Computational Statistics</title>
  <meta name="description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Classification | Computational Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  <meta name="github-repo" content="hardin47/website/Math154/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Classification | Computational Statistics" />
  
  <meta name="twitter:description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  

<meta name="author" content="Jo Hardin" />


<meta name="date" content="2019-10-26" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ethics.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Class Information</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#Sep3"><i class="fa fa-check"></i><b>1.1</b> 9/3/19 Agenda</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#course-logistics"><i class="fa fa-check"></i><b>1.2</b> Course Logistics</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#course-content"><i class="fa fa-check"></i><b>1.3</b> Course Content</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#topics"><i class="fa fa-check"></i><b>1.3.1</b> Topics</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#vocabulary"><i class="fa fa-check"></i><b>1.3.2</b> Vocabulary</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#the-workflow"><i class="fa fa-check"></i><b>1.3.3</b> The Workflow</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#principles-for-the-data-science-process-tldr"><i class="fa fa-check"></i><b>1.3.4</b> Principles for the Data Science Process tl;dr</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#Sep5"><i class="fa fa-check"></i><b>1.4</b> 9/5/19 Agenda</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#repro"><i class="fa fa-check"></i><b>1.5</b> Reproducibility</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#need-for-reproducibility"><i class="fa fa-check"></i><b>1.5.1</b> Need for Reproducibility</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-1"><i class="fa fa-check"></i>Example 1</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-2"><i class="fa fa-check"></i>Example 2</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-3"><i class="fa fa-check"></i>Example 3</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-4"><i class="fa fa-check"></i>Example 4</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#the-reproducible-data-analysis-process"><i class="fa fa-check"></i><b>1.5.2</b> The reproducible data analysis process</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#data-examples"><i class="fa fa-check"></i><b>1.6</b> Data Examples</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#college-rankings-systems"><i class="fa fa-check"></i><b>1.6.1</b> College Rankings Systems</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#trump-and-twitter"><i class="fa fa-check"></i><b>1.6.2</b> Trump and Twitter</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#can-twitter-predict-election-results"><i class="fa fa-check"></i><b>1.6.3</b> Can Twitter Predict Election Results?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>2</b> Visualization</a><ul>
<li class="chapter" data-level="2.1" data-path="visualization.html"><a href="visualization.html#Sep10"><i class="fa fa-check"></i><b>2.1</b> 9/10/19 Agenda</a></li>
<li class="chapter" data-level="2.2" data-path="visualization.html"><a href="visualization.html#examples"><i class="fa fa-check"></i><b>2.2</b> Examples</a><ul>
<li class="chapter" data-level="2.2.1" data-path="visualization.html"><a href="visualization.html#cholera-via-tufte"><i class="fa fa-check"></i><b>2.2.1</b> Cholera via Tufte</a></li>
<li class="chapter" data-level="2.2.2" data-path="visualization.html"><a href="visualization.html#challenger-via-tufte"><i class="fa fa-check"></i><b>2.2.2</b> Challenger via Tufte</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="visualization.html"><a href="visualization.html#thoughts"><i class="fa fa-check"></i><b>2.3</b> Thoughts on Plotting</a><ul>
<li class="chapter" data-level="2.3.1" data-path="visualization.html"><a href="visualization.html#advice"><i class="fa fa-check"></i><b>2.3.1</b> Advice</a></li>
<li class="chapter" data-level="2.3.2" data-path="visualization.html"><a href="visualization.html#an-example-from-information-is-beautiful"><i class="fa fa-check"></i><b>2.3.2</b> An example from Information is Beautiful</a></li>
<li class="chapter" data-level="2.3.3" data-path="visualization.html"><a href="visualization.html#assessing-graphics-and-other-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Assessing Graphics (and Other Analyses)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="visualization.html"><a href="visualization.html#Sep12"><i class="fa fa-check"></i><b>2.4</b> 9/12/19 Agenda</a></li>
<li class="chapter" data-level="2.5" data-path="visualization.html"><a href="visualization.html#deconstruct"><i class="fa fa-check"></i><b>2.5</b> Deconstructing a graph</a><ul>
<li class="chapter" data-level="2.5.1" data-path="visualization.html"><a href="visualization.html#gg"><i class="fa fa-check"></i><b>2.5.1</b> The Grammar of Graphics (<code>gg</code>)</a></li>
<li class="chapter" data-level="2.5.2" data-path="visualization.html"><a href="visualization.html#ggplot2"><i class="fa fa-check"></i><b>2.5.2</b> <code>ggplot2</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrang.html"><a href="wrang.html"><i class="fa fa-check"></i><b>3</b> Data Wrangling</a><ul>
<li class="chapter" data-level="3.1" data-path="wrang.html"><a href="wrang.html#Sep17"><i class="fa fa-check"></i><b>3.1</b> 9/17/19 Agenda</a></li>
<li class="chapter" data-level="3.2" data-path="wrang.html"><a href="wrang.html#datastruc"><i class="fa fa-check"></i><b>3.2</b> Structure of Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="wrang.html"><a href="wrang.html#building-tidy-data"><i class="fa fa-check"></i><b>3.2.1</b> Building Tidy Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="wrang.html"><a href="wrang.html#examples-of-chaining"><i class="fa fa-check"></i><b>3.2.2</b> Examples of Chaining</a></li>
<li class="chapter" data-level="3.2.3" data-path="wrang.html"><a href="wrang.html#data-verbs-on-single-data-frames"><i class="fa fa-check"></i><b>3.2.3</b> Data Verbs (on single data frames)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="wrang.html"><a href="wrang.html#r-examples-basic-verbs"><i class="fa fa-check"></i><b>3.3</b> R examples, basic verbs</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrang.html"><a href="wrang.html#datasets"><i class="fa fa-check"></i><b>3.3.1</b> Datasets</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrang.html"><a href="wrang.html#examples-of-chaining-1"><i class="fa fa-check"></i><b>3.3.2</b> Examples of Chaining</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrang.html"><a href="wrang.html#data-verbs"><i class="fa fa-check"></i><b>3.3.3</b> Data Verbs</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrang.html"><a href="wrang.html#summarize-and-group_by"><i class="fa fa-check"></i><b>3.3.4</b> <code>summarize</code> and <code>group_by</code></a></li>
<li class="chapter" data-level="3.3.5" data-path="wrang.html"><a href="wrang.html#babynames"><i class="fa fa-check"></i><b>3.3.5</b> babynames</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrang.html"><a href="wrang.html#Sep19"><i class="fa fa-check"></i><b>3.4</b> 9/19/19 Agenda</a></li>
<li class="chapter" data-level="3.5" data-path="wrang.html"><a href="wrang.html#highverb"><i class="fa fa-check"></i><b>3.5</b> Higher Level Data Verbs</a></li>
<li class="chapter" data-level="3.6" data-path="wrang.html"><a href="wrang.html#r-examples-higher-level-verbs"><i class="fa fa-check"></i><b>3.6</b> R examples, higher level verbs</a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrang.html"><a href="wrang.html#pivot_longer"><i class="fa fa-check"></i><b>3.6.1</b> <code>pivot_longer</code></a></li>
<li class="chapter" data-level="3.6.2" data-path="wrang.html"><a href="wrang.html#pivot_wider"><i class="fa fa-check"></i><b>3.6.2</b> <code>pivot_wider</code></a></li>
<li class="chapter" data-level="3.6.3" data-path="wrang.html"><a href="wrang.html#join-use-join-to-merge-two-datasets"><i class="fa fa-check"></i><b>3.6.3</b> <code>join</code> (use <code>join</code> to <strong>merge</strong> two datasets)</a></li>
<li class="chapter" data-level="3.6.4" data-path="wrang.html"><a href="wrang.html#lubridate"><i class="fa fa-check"></i><b>3.6.4</b> <code>lubridate</code></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrang.html"><a href="wrang.html#reprex"><i class="fa fa-check"></i><b>3.7</b> <code>reprex</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sims.html"><a href="sims.html"><i class="fa fa-check"></i><b>4</b> Simulating</a><ul>
<li class="chapter" data-level="4.1" data-path="sims.html"><a href="sims.html#Sep24"><i class="fa fa-check"></i><b>4.1</b> 9/24/19 Agenda</a></li>
<li class="chapter" data-level="4.2" data-path="sims.html"><a href="sims.html#simmodels"><i class="fa fa-check"></i><b>4.2</b> Simulating Complicated Models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sims.html"><a href="sims.html#goals-of-simulating-complicated-models"><i class="fa fa-check"></i><b>4.2.1</b> Goals of Simulating Complicated Models</a></li>
<li class="chapter" data-level="4.2.2" data-path="sims.html"><a href="sims.html#examples-of-pigs-and-blackjack"><i class="fa fa-check"></i><b>4.2.2</b> Examples of Pigs and Blackjack</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sims.html"><a href="sims.html#Sep26"><i class="fa fa-check"></i><b>4.3</b> 9/26/19 Agenda</a></li>
<li class="chapter" data-level="4.4" data-path="sims.html"><a href="sims.html#simsens"><i class="fa fa-check"></i><b>4.4</b> Simulating to Assess Sensitivity</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sims.html"><a href="sims.html#biasmodels"><i class="fa fa-check"></i><b>4.4.1</b> Bias in Models</a></li>
<li class="chapter" data-level="4.4.2" data-path="sims.html"><a href="sims.html#technical-conditions"><i class="fa fa-check"></i><b>4.4.2</b> Technical Conditions</a></li>
<li class="chapter" data-level="4.4.3" data-path="sims.html"><a href="sims.html#generating-random-numbers"><i class="fa fa-check"></i><b>4.4.3</b> Generating random numbers</a></li>
<li class="chapter" data-level="4.4.4" data-path="sims.html"><a href="sims.html#generating-other-rvs-the-inverse-transform-method"><i class="fa fa-check"></i><b>4.4.4</b> Generating other RVs: <strong>The Inverse Transform Method</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permschp.html"><a href="permschp.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permschp.html"><a href="permschp.html#Oct1"><i class="fa fa-check"></i><b>5.1</b> 10/1/19 Agenda</a></li>
<li class="chapter" data-level="5.2" data-path="permschp.html"><a href="permschp.html#algs"><i class="fa fa-check"></i><b>5.2</b> Inference Algorithms</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permschp.html"><a href="permschp.html#hypothesis-test-algorithm"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Test Algorithm</a></li>
<li class="chapter" data-level="5.2.2" data-path="permschp.html"><a href="permschp.html#permutation-tests-algorithm"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Tests Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permschp.html"><a href="permschp.html#Oct3"><i class="fa fa-check"></i><b>5.3</b> 10/3/19 Agenda</a></li>
<li class="chapter" data-level="5.4" data-path="permschp.html"><a href="permschp.html#perms"><i class="fa fa-check"></i><b>5.4</b> Permutation tests in practice</a><ul>
<li class="chapter" data-level="5.4.1" data-path="permschp.html"><a href="permschp.html#permutation-vs.randomization-tests"><i class="fa fa-check"></i><b>5.4.1</b> Permutation vs. Randomization Tests</a></li>
<li class="chapter" data-level="5.4.2" data-path="permschp.html"><a href="permschp.html#ci-from-permutation-tests"><i class="fa fa-check"></i><b>5.4.2</b> CI from Permutation Tests</a></li>
<li class="chapter" data-level="5.4.3" data-path="permschp.html"><a href="permschp.html#randomization-example"><i class="fa fa-check"></i><b>5.4.3</b> Randomization Example</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="permschp.html"><a href="permschp.html#r-examples"><i class="fa fa-check"></i><b>5.5</b> R examples</a><ul>
<li class="chapter" data-level="5.5.1" data-path="permschp.html"><a href="permschp.html#cloud-seeding-two-sample-test-computationally-very-difficult-to-do-a-randomization-test"><i class="fa fa-check"></i><b>5.5.1</b> Cloud Seeding (Two sample test – computationally very difficult to do a randomization test)</a></li>
<li class="chapter" data-level="5.5.2" data-path="permschp.html"><a href="permschp.html#macnell-teaching-evaluations-stratified-two-sample-t-test"><i class="fa fa-check"></i><b>5.5.2</b> MacNell Teaching Evaluations (Stratified two-sample t-test)</a></li>
<li class="chapter" data-level="5.5.3" data-path="permschp.html"><a href="permschp.html#income-and-health-f-like-test"><i class="fa fa-check"></i><b>5.5.3</b> Income and Health (F-like test)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="boot.html"><a href="boot.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping</a><ul>
<li class="chapter" data-level="6.1" data-path="boot.html"><a href="boot.html#Oct8"><i class="fa fa-check"></i><b>6.1</b> 10/8/19 Agenda</a></li>
<li class="chapter" data-level="6.2" data-path="boot.html"><a href="boot.html#introduction"><i class="fa fa-check"></i><b>6.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3" data-path="boot.html"><a href="boot.html#BSnotation"><i class="fa fa-check"></i><b>6.3</b> Basics &amp; Notation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="boot.html"><a href="boot.html#the-plug-in-principle"><i class="fa fa-check"></i><b>6.3.1</b> The Plug-in Principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="boot.html"><a href="boot.html#the-bootstrap-idea"><i class="fa fa-check"></i><b>6.3.2</b> The Bootstrap Idea</a></li>
<li class="chapter" data-level="6.3.3" data-path="boot.html"><a href="boot.html#bootstrap-procedure"><i class="fa fa-check"></i><b>6.3.3</b> Bootstrap Procedure</a></li>
<li class="chapter" data-level="6.3.4" data-path="boot.html"><a href="boot.html#bootstrap-notation"><i class="fa fa-check"></i><b>6.3.4</b> Bootstrap Notation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="boot.html"><a href="boot.html#Oct10"><i class="fa fa-check"></i><b>6.4</b> 10/10/19 Agenda</a></li>
<li class="chapter" data-level="6.5" data-path="boot.html"><a href="boot.html#BSCI"><i class="fa fa-check"></i><b>6.5</b> Bootstrap Confidence Intervals</a><ul>
<li class="chapter" data-level="6.5.1" data-path="boot.html"><a href="boot.html#normal-standard-ci-with-bs-se-typenorm"><i class="fa fa-check"></i><b>6.5.1</b> Normal (standard) CI with BS SE: <code>type=&quot;norm&quot;</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="boot.html"><a href="boot.html#bootstrap-t-confidence-intervals-typestud"><i class="fa fa-check"></i><b>6.5.2</b> Bootstrap-t Confidence Intervals: <code>type=&quot;stud&quot;</code></a></li>
<li class="chapter" data-level="6.5.3" data-path="boot.html"><a href="boot.html#percentile-confidence-intervals-typeperc"><i class="fa fa-check"></i><b>6.5.3</b> Percentile Confidence Intervals: <code>type=&quot;perc&quot;</code></a></li>
<li class="chapter" data-level="6.5.4" data-path="boot.html"><a href="boot.html#what-makes-a-ci-good"><i class="fa fa-check"></i><b>6.5.4</b> What makes a CI good?</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="boot.html"><a href="boot.html#r-example-heroin"><i class="fa fa-check"></i><b>6.6</b> R example: Heroin</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ethics.html"><a href="ethics.html"><i class="fa fa-check"></i><b>7</b> Ethics</a><ul>
<li class="chapter" data-level="7.1" data-path="ethics.html"><a href="ethics.html#Oct24"><i class="fa fa-check"></i><b>7.1</b> 10/24/19 Agenda</a></li>
<li class="chapter" data-level="7.2" data-path="ethics.html"><a href="ethics.html#doing-data-science"><i class="fa fa-check"></i><b>7.2</b> Doing Data Science</a></li>
<li class="chapter" data-level="7.3" data-path="ethics.html"><a href="ethics.html#graphics"><i class="fa fa-check"></i><b>7.3</b> Graphics</a></li>
<li class="chapter" data-level="7.4" data-path="ethics.html"><a href="ethics.html#p-hacking"><i class="fa fa-check"></i><b>7.4</b> p-hacking</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ethics.html"><a href="ethics.html#multiple-studies"><i class="fa fa-check"></i><b>7.4.1</b> Multiple Studies</a></li>
<li class="chapter" data-level="7.4.2" data-path="ethics.html"><a href="ethics.html#p-values"><i class="fa fa-check"></i><b>7.4.2</b> p-values</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ethics.html"><a href="ethics.html#human-subjects-research"><i class="fa fa-check"></i><b>7.5</b> Human Subjects Research</a></li>
<li class="chapter" data-level="7.6" data-path="ethics.html"><a href="ethics.html#authorship"><i class="fa fa-check"></i><b>7.6</b> Authorship</a></li>
<li class="chapter" data-level="7.7" data-path="ethics.html"><a href="ethics.html#algorithms"><i class="fa fa-check"></i><b>7.7</b> Algorithms</a><ul>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#sentencing"><i class="fa fa-check"></i>Sentencing</a></li>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#algorithmic-justice-league"><i class="fa fa-check"></i>Algorithmic Justice League</a></li>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#sentiment-analysis"><i class="fa fa-check"></i>Sentiment Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ethics.html"><a href="ethics.html#guiding-ethical-principles"><i class="fa fa-check"></i><b>7.8</b> Guiding Ethical Principles</a><ul>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#asa-ethical-guidelines-for-statistical-practice"><i class="fa fa-check"></i>ASA Ethical Guidelines for Statistical Practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="class.html"><a href="class.html"><i class="fa fa-check"></i><b>8</b> Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="class.html"><a href="class.html#Oct29"><i class="fa fa-check"></i><b>8.1</b> 10/29/19 Agenda</a></li>
<li class="chapter" data-level="8.2" data-path="class.html"><a href="class.html#knn"><i class="fa fa-check"></i><b>8.2</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a><ul>
<li class="chapter" data-level="8.2.1" data-path="class.html"><a href="class.html#k-nn-algorithm"><i class="fa fa-check"></i><b>8.2.1</b> <span class="math inline">\(k\)</span>-NN algorithm</a></li>
<li class="chapter" data-level="8.2.2" data-path="class.html"><a href="class.html#r-knn-example"><i class="fa fa-check"></i><b>8.2.2</b> R knn Example</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="class.html"><a href="class.html#cv"><i class="fa fa-check"></i><b>8.3</b> Cross Validation</a><ul>
<li class="chapter" data-level="8.3.1" data-path="class.html"><a href="class.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>8.3.1</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="8.3.2" data-path="class.html"><a href="class.html#implementing-cross-validation"><i class="fa fa-check"></i><b>8.3.2</b> Implementing Cross Validation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://st47s.com/Math154" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="class" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Classification</h1>
<div id="Oct29" class="section level2">
<h2><span class="header-section-number">8.1</span> 10/29/19 Agenda</h2>
<ol style="list-style-type: decimal">
<li>classification</li>
<li><span class="math inline">\(k\)</span>-Nearest Neighbors</li>
<li>bias-variance trade-off</li>
<li>cross validation</li>
</ol>
<p><strong>Important Note</strong>: For the majority of the classification and clustering methods, we will use the <code>caret</code> package in R. For more information see: <a href="http://topepo.github.io/caret/index.html" class="uri">http://topepo.github.io/caret/index.html</a></p>
<p><span class="citation">Baumer (<a href="#ref-Baumer15">2015</a>)</span> provides a concise explanation of how both statistics and data science work to enhance ideas of machine learning, one aspect of which is classification:</p>
<blockquote>
<p>In order to understand machine learning, one must recognize the differences between the mindset of the data miner and the statistician, notably characterized by <span class="citation">Breiman (<a href="#ref-brei01">2001</a>)</span>, who distinguished two types of models f for y, the response variable, and x, a vector of explanatory variables. One might consider a <em>data model</em> f such that y <span class="math inline">\(\sim\)</span> f(x), assess whether f could reasonably have been the process that generated y from x, and then make inferences about f. The goal here is to learn about the real process that generated y from x, and the conceit is</p>
</blockquote>
<blockquote>
<p>Alternatively, one might construct an <em>algorithmic model</em> f, such that <span class="math inline">\(y \sim f(x)\)</span>, and use f to predict unobserved values of y. If it can be determined that f does in fact do a good job of predicting values of y, one might not care to learn much about f. In the former case, since we want to learn about f, a simpler model may be preferred. Conversely, in the latter case, since we want to predict new values of y, we may be indifferent to model complexity (other than concerns about overfitting and scalability).</p>
</blockquote>
<p>Classification is a supervised learning technique to extract general patterns from the data in order to build a predictor for a new test or validation data set. That is, the model should <em>classify</em> new points into groups (or with a numerical response values) based on a model built from a set of data which provides known group membership for each value. For most of the methods below, we will consider classifying into categories (in fact, usually only two categories), but sometimes (e.g., support vector machines and linear regression) the goal is to predict a numeric variable.</p>
<p>Some examples of classification techniques include: linear regression, logistic regression, neural networks, <strong>classification trees</strong>, <strong>random forests</strong>, <strong>k-nearest neighbors</strong>, <strong>support vector machines</strong>, n{&quot;a}ive Bayes, and linear discriminant analysis. We will cover the methods in <strong>bold</strong>.</p>
<p><strong>Simple is Better</strong> (From <span class="citation">Fielding (<a href="#ref-field07">2007</a>)</span>, p. 87)</p>
<ol style="list-style-type: decimal">
<li>We want to avoid over-fitting the model (certainly, it is a bad idea to model the noise!)</li>
<li>Future prediction performance goes down with too many predictors.</li>
<li>Simple models provide better insight into causality and specific associations.</li>
<li>Fewer predictors implies fewer variables to collect in later studies.</li>
</ol>
<p>That said, the model should still represent the complexity of the data! We describe the tradeoff above as the ``bias-variance&quot; trade-off. In order to fully understand that trade-off, let’s first cover the classification method known as <span class="math inline">\(k\)</span>-Nearest Neighbors.</p>
</div>
<div id="knn" class="section level2">
<h2><span class="header-section-number">8.2</span> <span class="math inline">\(k\)</span>-Nearest Neighbors</h2>
<p>The <span class="math inline">\(k\)</span>-Nearest Neighbor algorithm does exactly what it sounds like it does. The user decides on the integer value for <span class="math inline">\(k\)</span>, and a point is classified to be in the group for which the majority of the <span class="math inline">\(k\)</span> closest points in the training data.</p>
<div id="k-nn-algorithm" class="section level3">
<h3><span class="header-section-number">8.2.1</span> <span class="math inline">\(k\)</span>-NN algorithm</h3>
<ol style="list-style-type: decimal">
<li>Decide on a distance metric (e.g., Euclidean distance, 1 - correlation, etc.) and find the distances from each point in the test set to each point in the training set. The distance is measured in the feature space, that is, with respect to the explanatory variables (not the response variable).</li>
</ol>
<p>n.b. In most machine learning algorithms that use “distance” as a measure, the “distance” is not required to be a mathematical distance metric. Indeed, 1-correlation is a very common distance measure, and it fails the triangle inequality.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Consider a point in the training set. Find the <span class="math inline">\(k\)</span> closest points in the test set to the one test observation.</p></li>
<li><p>Using majority vote, find the dominate class of the <span class="math inline">\(k\)</span> closest points. Predict that class label to the test observation.</p></li>
</ol>
<p>Note: if the response variable is continuous (instead of categorical), find the average response variable of the <span class="math inline">\(k\)</span> training point to be the predicted response for the one test observation.</p>
<p><strong>Shortcomings of <span class="math inline">\(k\)</span>-NN</strong>:
* one class can dominate if it has a large majority
* Euclidean distance is dominated by scale
* it can be computationally unwieldy (and unneeded!!) to calculate all distances (there are algorithms to search smartly)
* the output doesn’t provide any information about which explanatory variables are informative.</p>
<p><strong>Strengths of <span class="math inline">\(k\)</span>-NN</strong>:
* it can easily work for any number of categories
* it can predict a quantitative response variable
* the bias of 1-NN is often low (but the variance is high)
* any distance metric can be used (so the algorithm models the data appropriately)
* the method is simple to implement / understand
* model is nonparametric (no distributional assumptions on the data)
* great model for imputing missing data</p>
<p><img src="figs/knnmodel.jpg" width="100%" style="display: block; margin: auto;" /><img src="figs/knnK.jpg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="r-knn-example" class="section level3">
<h3><span class="header-section-number">8.2.2</span> R knn Example</h3>
<p>R code for using the <code>knn</code> package to cluster the <code>iris</code> data. The <code>caret</code> package vignette for <code>knn</code> is here: <a href="http://topepo.github.io/caret/miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function" class="uri">http://topepo.github.io/caret/miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function</a></p>
</div>
</div>
<div id="cv" class="section level2">
<h2><span class="header-section-number">8.3</span> Cross Validation</h2>
<div id="bias-variance-trade-off" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Bias-variance trade-off</h3>
<ul>
<li><p><strong>Variance</strong> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training set. Generally, the closer the model fits the data, the more variable it will be (it’ll be different for each data set!). A model with many many explanatory variables will often fit the data too closely.</p></li>
<li><p><strong>Bias</strong> refers to the error that is introduced by approximating the “truth” by a model which is too simple. For example, we often use linear models to describe complex relationships, but it is unlikely that any real life situation actually has a <em>true</em> linear model. However, if the true relationship is close to linear, then the linear model will have a low bias.</p></li>
</ul>
<p>Generally, the simpler the model, the lower the variance. The more complicated the model, the lower the bias. In this class, cross validation will be used to assess model fit. [If time permits, Receiver Operating Characteristic (ROC) curves will also be covered.]</p>
<p><span class="math display">\[\begin{align}
\mbox{prediction error } = \mbox{ irreducible error } + \mbox{ bias } + \mbox{ variance}
\end{align}\]</span></p>
<ul>
<li><strong>irreducible error</strong> The irreducible error is the natural variability that comes with observations. No matter how good the model is, we will never be able to predict perfectly.</li>
<li><strong>bias</strong> The bias of the model represents the difference between the true model and a model which is too simple. That is, with more complicated models (e.g., smaller <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN) the closer the points are to the prediction. As the model gets more complicated (e.g., as <span class="math inline">\(k\)</span> decreases), the bias goes down.</li>
<li><strong>variance</strong> The variance represents the variability of the model from sample to sample. That is, a simple model (big <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN) would not change a lot from sample to sample. The variance decreases as the model becomes more simple (e.g., as <span class="math inline">\(k\)</span> increases).</li>
</ul>
<p>Note the bias-variance trade-off. We want our prediction error to be small, so we choose a model that is medium with respect to both bias and variance. We cannot control the irreducible error.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="figs/varbias.pdf" alt="Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  [@ESL]" width="100%" />
<p class="caption">
Figure 1.3: Test and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-ESL">2001</a>)</span>
</p>
</div>
<p>The following visualization does an excellent job of communicating the trade-off between bias and variance as a function of a specific tuning parameter, here: minimum node size of a classification tree. <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-2/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-2/</a></p>
</div>
<div id="implementing-cross-validation" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Implementing Cross Validation</h3>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="figs/overfitting.jpg" alt="[@flach12]" width="100%" />
<p class="caption">
Figure 1.4: <span class="citation">(Flach <a href="#ref-flach12">2012</a>)</span>
</p>
</div>
<p>Cross validation is typically used in two ways.</p>
<ol style="list-style-type: decimal">
<li>To assess a model’s accuracy (<em>model assessment</em>).<br />
</li>
<li>To build a model (<em>model selection</em>).</li>
</ol>
<div id="different-ways-to-cv" class="section level4 unnumbered">
<h4>Different ways to CV</h4>
<p>Suppose that we build a classifier on a given data set. We’d like to know how well the model classifies observations, but if we test on the samples at hand, the error rate will be much lower than the model’s inherent accuracy rate. Instead, we’d like to predict <em>new</em> observations that were not used to create the model. There are various ways of creating <em>test</em> or <em>validation</em> sets of data:</p>
<ul>
<li>one training set, one test set [two drawbacks: estimate of error is highly variable because it depends on which points go into the training set; and because the training data set is smaller than the full data set, the error rate is biased in such a way that it overestimates the actual error rate of the modeling technique.]</li>
<li>leave one out cross validation (LOOCV)</li>
</ul>
<ol style="list-style-type: decimal">
<li>remove one observation</li>
<li>build the model using the remaining n-1 points</li>
<li>predict class membership for the observation which was removed</li>
<li>repeat by removing each observation one at a time</li>
</ol>
<ul>
<li><span class="math inline">\(k\)</span>-fold cross validation (<span class="math inline">\(k\)</span>-fold CV)
<ul>
<li>like LOOCV except that the algorithm is run <span class="math inline">\(k\)</span> times on each group (of approximately equal size) from a partition of the data set.]</li>
<li>LOOCV is a special case of <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k=n\)</span></li>
<li>advantage of <span class="math inline">\(k\)</span>-fold is computational</li>
<li><span class="math inline">\(k\)</span>-fold often has a better bias-variance trade-off [bias is lower with LOOCV. however, because LOOCV predicts <span class="math inline">\(n\)</span> observations from <span class="math inline">\(n\)</span> models which are basically the same, the variability will be higher (i.e., based on the <span class="math inline">\(n\)</span> data values). with <span class="math inline">\(k\)</span>-fold, prediction is on <span class="math inline">\(n\)</span> values from <span class="math inline">\(k\)</span> models which are much less correlated. the effect is to average out the predicted values in such a way that there will be less variability from data set to data set.]</li>
</ul></li>
</ul>
</div>
<div id="cv-for-model-assessment-10-fold" class="section level4 unnumbered">
<h4>CV for Model assessment 10-fold</h4>
<ol style="list-style-type: decimal">
<li>assume <span class="math inline">\(k\)</span> is given for <span class="math inline">\(k\)</span>NN</li>
<li>remove 10% of the data</li>
<li>build the model using the remaining 90%</li>
<li>predict class membership / continuous response for the 10% of the observations which were removed</li>
<li>repeat by removing each decile one at a time</li>
<li>a good measure of the model’s ability to predict is the error rate associated with the predictions on the data which have been independently predicted</li>
</ol>
</div>
<div id="cv-for-model-selection-10-fold" class="section level4 unnumbered">
<h4>CV for Model selection 10-fold</h4>
<ol style="list-style-type: decimal">
<li>set <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN</li>
<li>build the model using the <span class="math inline">\(k\)</span> value set above:
<ol style="list-style-type: lower-alpha">
<li>remove 10% of the data</li>
<li>build the model using the remaining 90%</li>
<li>predict class membership / continuous response for the 10% of the observations which were removed</li>
<li>repeat by removing each decile one at a time</li>
</ol></li>
<li>measure the CV prediction error for the <span class="math inline">\(k\)</span> value at hand</li>
<li>repeat steps 1-3 and choose the <span class="math inline">\(k\)</span> for which the prediction error is lowest</li>
</ol>
</div>
<div id="cv-for-model-assessment-and-selection-10-fold" class="section level4 unnumbered">
<h4>CV for Model assessment and selection 10-fold</h4>
<p>For now we will talk about test/training data <em>and</em> CV in order to both model assessment and selection. Note that CV could be used in both steps, but the algorithm is slightly more complicated.</p>
<ol style="list-style-type: decimal">
<li>split the data into training and test observations</li>
<li>set <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN</li>
<li>build the model using the <span class="math inline">\(k\)</span> value set above on <em>only the training data</em>:
<ol style="list-style-type: lower-alpha">
<li>remove 10% of the training data</li>
<li>build the model using the remaining 90% of the training data</li>
<li>predict class membership / continuous response for the 10% of the training observations which were removed</li>
<li>repeat by removing each decile one at a time from the training data</li>
</ol></li>
<li>measure the CV prediction error for the <span class="math inline">\(k\)</span> value at hand on the training data</li>
<li>repeat steps 2-4 and choose the <span class="math inline">\(k\)</span> for which the prediction error is lowest for the training data</li>
<li>using the <span class="math inline">\(k\)</span> value given in step 5, assess the prediction error on the test data</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-5"></span>
<img src="figs/CV.jpg" alt="Nested cross-validation: two cross-validation loops are run one inside the other.  [@CVpaper]" width="100%" />
<p class="caption">
Figure 1.5: Nested cross-validation: two cross-validation loops are run one inside the other. <span class="citation">(Varoquaux et al. <a href="#ref-CVpaper">2017</a>)</span>
</p>
</div>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Baumer15">
<p>Baumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” <em>The American Statistician</em>.</p>
</div>
<div id="ref-brei01">
<p>Breiman, L. 2001. “Statistical Modeling: The Two Cultures.” <em>Statistical Science</em> 16 (3): 199–215.</p>
</div>
<div id="ref-field07">
<p>Fielding, Alan. 2007. <em>Cluster and Classification Techniques for the Biosciences</em>. Cambridge.</p>
</div>
<div id="ref-flach12">
<p>Flach, P. 2012. <em>Machine Learning</em>. Cambridge University Press.</p>
</div>
<div id="ref-ESL">
<p>Hastie, T., R. Tibshirani, and J. Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
<div id="ref-CVpaper">
<p>Varoquaux, G., P. Reddy Raamana, D. Engemann, A. Hoyos-Idrobo, Y. Schwartz, and B. Thirion. 2017. “Assessing and Tuning Brain Decoders: Cross-Validation, Caveats, and Guidelines.” <em>NeuroImage</em> 145: 166–79.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ethics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Math-154-Notes.pdf", "Math-154-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
