<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Classification | Computational Statistics</title>
  <meta name="description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Classification | Computational Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  <meta name="github-repo" content="hardin47/website/Math154/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Classification | Computational Statistics" />
  
  <meta name="twitter:description" content="Class notes for Math 154 at Pomona College: Computational Statistics. The notes are based extensively on An Introduction to Statistical Learning by James, Witten, Hastie, and Tibshirani as well as Modern Data Science with R by Baumer, Kaplan, and Horton." />
  

<meta name="author" content="Jo Hardin" />


<meta name="date" content="2019-11-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ethics.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Computational Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Class Information</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#Sep3"><i class="fa fa-check"></i><b>1.1</b> 9/3/19 Agenda</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#course-logistics"><i class="fa fa-check"></i><b>1.2</b> Course Logistics</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#course-content"><i class="fa fa-check"></i><b>1.3</b> Course Content</a><ul>
<li class="chapter" data-level="1.3.1" data-path="intro.html"><a href="intro.html#topics"><i class="fa fa-check"></i><b>1.3.1</b> Topics</a></li>
<li class="chapter" data-level="1.3.2" data-path="intro.html"><a href="intro.html#vocabulary"><i class="fa fa-check"></i><b>1.3.2</b> Vocabulary</a></li>
<li class="chapter" data-level="1.3.3" data-path="intro.html"><a href="intro.html#the-workflow"><i class="fa fa-check"></i><b>1.3.3</b> The Workflow</a></li>
<li class="chapter" data-level="1.3.4" data-path="intro.html"><a href="intro.html#principles-for-the-data-science-process-tldr"><i class="fa fa-check"></i><b>1.3.4</b> Principles for the Data Science Process tl;dr</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#Sep5"><i class="fa fa-check"></i><b>1.4</b> 9/5/19 Agenda</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#repro"><i class="fa fa-check"></i><b>1.5</b> Reproducibility</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro.html"><a href="intro.html#need-for-reproducibility"><i class="fa fa-check"></i><b>1.5.1</b> Need for Reproducibility</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-1"><i class="fa fa-check"></i>Example 1</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-2"><i class="fa fa-check"></i>Example 2</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-3"><i class="fa fa-check"></i>Example 3</a></li>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#example-4"><i class="fa fa-check"></i>Example 4</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro.html"><a href="intro.html#the-reproducible-data-analysis-process"><i class="fa fa-check"></i><b>1.5.2</b> The reproducible data analysis process</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#data-examples"><i class="fa fa-check"></i><b>1.6</b> Data Examples</a><ul>
<li class="chapter" data-level="1.6.1" data-path="intro.html"><a href="intro.html#college-rankings-systems"><i class="fa fa-check"></i><b>1.6.1</b> College Rankings Systems</a></li>
<li class="chapter" data-level="1.6.2" data-path="intro.html"><a href="intro.html#trump-and-twitter"><i class="fa fa-check"></i><b>1.6.2</b> Trump and Twitter</a></li>
<li class="chapter" data-level="1.6.3" data-path="intro.html"><a href="intro.html#can-twitter-predict-election-results"><i class="fa fa-check"></i><b>1.6.3</b> Can Twitter Predict Election Results?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="visualization.html"><a href="visualization.html"><i class="fa fa-check"></i><b>2</b> Visualization</a><ul>
<li class="chapter" data-level="2.1" data-path="visualization.html"><a href="visualization.html#Sep10"><i class="fa fa-check"></i><b>2.1</b> 9/10/19 Agenda</a></li>
<li class="chapter" data-level="2.2" data-path="visualization.html"><a href="visualization.html#examples"><i class="fa fa-check"></i><b>2.2</b> Examples</a><ul>
<li class="chapter" data-level="2.2.1" data-path="visualization.html"><a href="visualization.html#cholera-via-tufte"><i class="fa fa-check"></i><b>2.2.1</b> Cholera via Tufte</a></li>
<li class="chapter" data-level="2.2.2" data-path="visualization.html"><a href="visualization.html#challenger-via-tufte"><i class="fa fa-check"></i><b>2.2.2</b> Challenger via Tufte</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="visualization.html"><a href="visualization.html#thoughts"><i class="fa fa-check"></i><b>2.3</b> Thoughts on Plotting</a><ul>
<li class="chapter" data-level="2.3.1" data-path="visualization.html"><a href="visualization.html#advice"><i class="fa fa-check"></i><b>2.3.1</b> Advice</a></li>
<li class="chapter" data-level="2.3.2" data-path="visualization.html"><a href="visualization.html#an-example-from-information-is-beautiful"><i class="fa fa-check"></i><b>2.3.2</b> An example from Information is Beautiful</a></li>
<li class="chapter" data-level="2.3.3" data-path="visualization.html"><a href="visualization.html#assessing-graphics-and-other-analyses"><i class="fa fa-check"></i><b>2.3.3</b> Assessing Graphics (and Other Analyses)</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="visualization.html"><a href="visualization.html#Sep12"><i class="fa fa-check"></i><b>2.4</b> 9/12/19 Agenda</a></li>
<li class="chapter" data-level="2.5" data-path="visualization.html"><a href="visualization.html#deconstruct"><i class="fa fa-check"></i><b>2.5</b> Deconstructing a graph</a><ul>
<li class="chapter" data-level="2.5.1" data-path="visualization.html"><a href="visualization.html#gg"><i class="fa fa-check"></i><b>2.5.1</b> The Grammar of Graphics (<code>gg</code>)</a></li>
<li class="chapter" data-level="2.5.2" data-path="visualization.html"><a href="visualization.html#ggplot2"><i class="fa fa-check"></i><b>2.5.2</b> <code>ggplot2</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="wrang.html"><a href="wrang.html"><i class="fa fa-check"></i><b>3</b> Data Wrangling</a><ul>
<li class="chapter" data-level="3.1" data-path="wrang.html"><a href="wrang.html#Sep17"><i class="fa fa-check"></i><b>3.1</b> 9/17/19 Agenda</a></li>
<li class="chapter" data-level="3.2" data-path="wrang.html"><a href="wrang.html#datastruc"><i class="fa fa-check"></i><b>3.2</b> Structure of Data</a><ul>
<li class="chapter" data-level="3.2.1" data-path="wrang.html"><a href="wrang.html#building-tidy-data"><i class="fa fa-check"></i><b>3.2.1</b> Building Tidy Data</a></li>
<li class="chapter" data-level="3.2.2" data-path="wrang.html"><a href="wrang.html#examples-of-chaining"><i class="fa fa-check"></i><b>3.2.2</b> Examples of Chaining</a></li>
<li class="chapter" data-level="3.2.3" data-path="wrang.html"><a href="wrang.html#data-verbs-on-single-data-frames"><i class="fa fa-check"></i><b>3.2.3</b> Data Verbs (on single data frames)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="wrang.html"><a href="wrang.html#r-examples-basic-verbs"><i class="fa fa-check"></i><b>3.3</b> R examples, basic verbs</a><ul>
<li class="chapter" data-level="3.3.1" data-path="wrang.html"><a href="wrang.html#datasets"><i class="fa fa-check"></i><b>3.3.1</b> Datasets</a></li>
<li class="chapter" data-level="3.3.2" data-path="wrang.html"><a href="wrang.html#examples-of-chaining-1"><i class="fa fa-check"></i><b>3.3.2</b> Examples of Chaining</a></li>
<li class="chapter" data-level="3.3.3" data-path="wrang.html"><a href="wrang.html#data-verbs"><i class="fa fa-check"></i><b>3.3.3</b> Data Verbs</a></li>
<li class="chapter" data-level="3.3.4" data-path="wrang.html"><a href="wrang.html#summarize-and-group_by"><i class="fa fa-check"></i><b>3.3.4</b> <code>summarize</code> and <code>group_by</code></a></li>
<li class="chapter" data-level="3.3.5" data-path="wrang.html"><a href="wrang.html#babynames"><i class="fa fa-check"></i><b>3.3.5</b> babynames</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="wrang.html"><a href="wrang.html#Sep19"><i class="fa fa-check"></i><b>3.4</b> 9/19/19 Agenda</a></li>
<li class="chapter" data-level="3.5" data-path="wrang.html"><a href="wrang.html#highverb"><i class="fa fa-check"></i><b>3.5</b> Higher Level Data Verbs</a></li>
<li class="chapter" data-level="3.6" data-path="wrang.html"><a href="wrang.html#r-examples-higher-level-verbs"><i class="fa fa-check"></i><b>3.6</b> R examples, higher level verbs</a><ul>
<li class="chapter" data-level="3.6.1" data-path="wrang.html"><a href="wrang.html#pivot_longer"><i class="fa fa-check"></i><b>3.6.1</b> <code>pivot_longer</code></a></li>
<li class="chapter" data-level="3.6.2" data-path="wrang.html"><a href="wrang.html#pivot_wider"><i class="fa fa-check"></i><b>3.6.2</b> <code>pivot_wider</code></a></li>
<li class="chapter" data-level="3.6.3" data-path="wrang.html"><a href="wrang.html#join-use-join-to-merge-two-datasets"><i class="fa fa-check"></i><b>3.6.3</b> <code>join</code> (use <code>join</code> to <strong>merge</strong> two datasets)</a></li>
<li class="chapter" data-level="3.6.4" data-path="wrang.html"><a href="wrang.html#lubridate"><i class="fa fa-check"></i><b>3.6.4</b> <code>lubridate</code></a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="wrang.html"><a href="wrang.html#reprex"><i class="fa fa-check"></i><b>3.7</b> <code>reprex</code></a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sims.html"><a href="sims.html"><i class="fa fa-check"></i><b>4</b> Simulating</a><ul>
<li class="chapter" data-level="4.1" data-path="sims.html"><a href="sims.html#Sep24"><i class="fa fa-check"></i><b>4.1</b> 9/24/19 Agenda</a></li>
<li class="chapter" data-level="4.2" data-path="sims.html"><a href="sims.html#simmodels"><i class="fa fa-check"></i><b>4.2</b> Simulating Complicated Models</a><ul>
<li class="chapter" data-level="4.2.1" data-path="sims.html"><a href="sims.html#goals-of-simulating-complicated-models"><i class="fa fa-check"></i><b>4.2.1</b> Goals of Simulating Complicated Models</a></li>
<li class="chapter" data-level="4.2.2" data-path="sims.html"><a href="sims.html#examples-of-pigs-and-blackjack"><i class="fa fa-check"></i><b>4.2.2</b> Examples of Pigs and Blackjack</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sims.html"><a href="sims.html#Sep26"><i class="fa fa-check"></i><b>4.3</b> 9/26/19 Agenda</a></li>
<li class="chapter" data-level="4.4" data-path="sims.html"><a href="sims.html#simsens"><i class="fa fa-check"></i><b>4.4</b> Simulating to Assess Sensitivity</a><ul>
<li class="chapter" data-level="4.4.1" data-path="sims.html"><a href="sims.html#biasmodels"><i class="fa fa-check"></i><b>4.4.1</b> Bias in Models</a></li>
<li class="chapter" data-level="4.4.2" data-path="sims.html"><a href="sims.html#technical-conditions"><i class="fa fa-check"></i><b>4.4.2</b> Technical Conditions</a></li>
<li class="chapter" data-level="4.4.3" data-path="sims.html"><a href="sims.html#generating-random-numbers"><i class="fa fa-check"></i><b>4.4.3</b> Generating random numbers</a></li>
<li class="chapter" data-level="4.4.4" data-path="sims.html"><a href="sims.html#generating-other-rvs-the-inverse-transform-method"><i class="fa fa-check"></i><b>4.4.4</b> Generating other RVs: <strong>The Inverse Transform Method</strong></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="permschp.html"><a href="permschp.html"><i class="fa fa-check"></i><b>5</b> Permutation Tests</a><ul>
<li class="chapter" data-level="5.1" data-path="permschp.html"><a href="permschp.html#Oct1"><i class="fa fa-check"></i><b>5.1</b> 10/1/19 Agenda</a></li>
<li class="chapter" data-level="5.2" data-path="permschp.html"><a href="permschp.html#algs"><i class="fa fa-check"></i><b>5.2</b> Inference Algorithms</a><ul>
<li class="chapter" data-level="5.2.1" data-path="permschp.html"><a href="permschp.html#hypothesis-test-algorithm"><i class="fa fa-check"></i><b>5.2.1</b> Hypothesis Test Algorithm</a></li>
<li class="chapter" data-level="5.2.2" data-path="permschp.html"><a href="permschp.html#permutation-tests-algorithm"><i class="fa fa-check"></i><b>5.2.2</b> Permutation Tests Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="permschp.html"><a href="permschp.html#Oct3"><i class="fa fa-check"></i><b>5.3</b> 10/3/19 Agenda</a></li>
<li class="chapter" data-level="5.4" data-path="permschp.html"><a href="permschp.html#perms"><i class="fa fa-check"></i><b>5.4</b> Permutation tests in practice</a><ul>
<li class="chapter" data-level="5.4.1" data-path="permschp.html"><a href="permschp.html#permutation-vs.randomization-tests"><i class="fa fa-check"></i><b>5.4.1</b> Permutation vs. Randomization Tests</a></li>
<li class="chapter" data-level="5.4.2" data-path="permschp.html"><a href="permschp.html#ci-from-permutation-tests"><i class="fa fa-check"></i><b>5.4.2</b> CI from Permutation Tests</a></li>
<li class="chapter" data-level="5.4.3" data-path="permschp.html"><a href="permschp.html#randomization-example"><i class="fa fa-check"></i><b>5.4.3</b> Randomization Example</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="permschp.html"><a href="permschp.html#r-examples"><i class="fa fa-check"></i><b>5.5</b> R examples</a><ul>
<li class="chapter" data-level="5.5.1" data-path="permschp.html"><a href="permschp.html#cloud-seeding-two-sample-test-computationally-very-difficult-to-do-a-randomization-test"><i class="fa fa-check"></i><b>5.5.1</b> Cloud Seeding (Two sample test – computationally very difficult to do a randomization test)</a></li>
<li class="chapter" data-level="5.5.2" data-path="permschp.html"><a href="permschp.html#macnell-teaching-evaluations-stratified-two-sample-t-test"><i class="fa fa-check"></i><b>5.5.2</b> MacNell Teaching Evaluations (Stratified two-sample t-test)</a></li>
<li class="chapter" data-level="5.5.3" data-path="permschp.html"><a href="permschp.html#income-and-health-f-like-test"><i class="fa fa-check"></i><b>5.5.3</b> Income and Health (F-like test)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="boot.html"><a href="boot.html"><i class="fa fa-check"></i><b>6</b> Bootstrapping</a><ul>
<li class="chapter" data-level="6.1" data-path="boot.html"><a href="boot.html#Oct8"><i class="fa fa-check"></i><b>6.1</b> 10/8/19 Agenda</a></li>
<li class="chapter" data-level="6.2" data-path="boot.html"><a href="boot.html#introduction"><i class="fa fa-check"></i><b>6.2</b> Introduction</a></li>
<li class="chapter" data-level="6.3" data-path="boot.html"><a href="boot.html#BSnotation"><i class="fa fa-check"></i><b>6.3</b> Basics &amp; Notation</a><ul>
<li class="chapter" data-level="6.3.1" data-path="boot.html"><a href="boot.html#the-plug-in-principle"><i class="fa fa-check"></i><b>6.3.1</b> The Plug-in Principle</a></li>
<li class="chapter" data-level="6.3.2" data-path="boot.html"><a href="boot.html#the-bootstrap-idea"><i class="fa fa-check"></i><b>6.3.2</b> The Bootstrap Idea</a></li>
<li class="chapter" data-level="6.3.3" data-path="boot.html"><a href="boot.html#bootstrap-procedure"><i class="fa fa-check"></i><b>6.3.3</b> Bootstrap Procedure</a></li>
<li class="chapter" data-level="6.3.4" data-path="boot.html"><a href="boot.html#bootstrap-notation"><i class="fa fa-check"></i><b>6.3.4</b> Bootstrap Notation</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="boot.html"><a href="boot.html#Oct10"><i class="fa fa-check"></i><b>6.4</b> 10/10/19 Agenda</a></li>
<li class="chapter" data-level="6.5" data-path="boot.html"><a href="boot.html#BSCI"><i class="fa fa-check"></i><b>6.5</b> Bootstrap Confidence Intervals</a><ul>
<li class="chapter" data-level="6.5.1" data-path="boot.html"><a href="boot.html#normal-standard-ci-with-bs-se-typenorm"><i class="fa fa-check"></i><b>6.5.1</b> Normal (standard) CI with BS SE: <code>type=&quot;norm&quot;</code></a></li>
<li class="chapter" data-level="6.5.2" data-path="boot.html"><a href="boot.html#bootstrap-t-confidence-intervals-typestud"><i class="fa fa-check"></i><b>6.5.2</b> Bootstrap-t Confidence Intervals: <code>type=&quot;stud&quot;</code></a></li>
<li class="chapter" data-level="6.5.3" data-path="boot.html"><a href="boot.html#percentile-confidence-intervals-typeperc"><i class="fa fa-check"></i><b>6.5.3</b> Percentile Confidence Intervals: <code>type=&quot;perc&quot;</code></a></li>
<li class="chapter" data-level="6.5.4" data-path="boot.html"><a href="boot.html#what-makes-a-ci-good"><i class="fa fa-check"></i><b>6.5.4</b> What makes a CI good?</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="boot.html"><a href="boot.html#r-example-heroin"><i class="fa fa-check"></i><b>6.6</b> R example: Heroin</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ethics.html"><a href="ethics.html"><i class="fa fa-check"></i><b>7</b> Ethics</a><ul>
<li class="chapter" data-level="7.1" data-path="ethics.html"><a href="ethics.html#Oct24"><i class="fa fa-check"></i><b>7.1</b> 10/24/19 Agenda</a></li>
<li class="chapter" data-level="7.2" data-path="ethics.html"><a href="ethics.html#doing-data-science"><i class="fa fa-check"></i><b>7.2</b> Doing Data Science</a></li>
<li class="chapter" data-level="7.3" data-path="ethics.html"><a href="ethics.html#graphics"><i class="fa fa-check"></i><b>7.3</b> Graphics</a></li>
<li class="chapter" data-level="7.4" data-path="ethics.html"><a href="ethics.html#p-hacking"><i class="fa fa-check"></i><b>7.4</b> p-hacking</a><ul>
<li class="chapter" data-level="7.4.1" data-path="ethics.html"><a href="ethics.html#multiple-studies"><i class="fa fa-check"></i><b>7.4.1</b> Multiple Studies</a></li>
<li class="chapter" data-level="7.4.2" data-path="ethics.html"><a href="ethics.html#p-values"><i class="fa fa-check"></i><b>7.4.2</b> p-values</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="ethics.html"><a href="ethics.html#human-subjects-research"><i class="fa fa-check"></i><b>7.5</b> Human Subjects Research</a></li>
<li class="chapter" data-level="7.6" data-path="ethics.html"><a href="ethics.html#authorship"><i class="fa fa-check"></i><b>7.6</b> Authorship</a></li>
<li class="chapter" data-level="7.7" data-path="ethics.html"><a href="ethics.html#algorithms"><i class="fa fa-check"></i><b>7.7</b> Algorithms</a><ul>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#sentencing"><i class="fa fa-check"></i>Sentencing</a></li>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#algorithmic-justice-league"><i class="fa fa-check"></i>Algorithmic Justice League</a></li>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#sentiment-analysis"><i class="fa fa-check"></i>Sentiment Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.8" data-path="ethics.html"><a href="ethics.html#guiding-ethical-principles"><i class="fa fa-check"></i><b>7.8</b> Guiding Ethical Principles</a><ul>
<li class="chapter" data-level="" data-path="ethics.html"><a href="ethics.html#asa-ethical-guidelines-for-statistical-practice"><i class="fa fa-check"></i>ASA Ethical Guidelines for Statistical Practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="class.html"><a href="class.html"><i class="fa fa-check"></i><b>8</b> Classification</a><ul>
<li class="chapter" data-level="8.1" data-path="class.html"><a href="class.html#Oct29"><i class="fa fa-check"></i><b>8.1</b> 10/29/19 Agenda</a></li>
<li class="chapter" data-level="8.2" data-path="class.html"><a href="class.html#cv"><i class="fa fa-check"></i><b>8.2</b> Cross Validation</a><ul>
<li class="chapter" data-level="8.2.1" data-path="class.html"><a href="class.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>8.2.1</b> Bias-variance trade-off</a></li>
<li class="chapter" data-level="8.2.2" data-path="class.html"><a href="class.html#implementing-cross-validation"><i class="fa fa-check"></i><b>8.2.2</b> Implementing Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="class.html"><a href="class.html#knn"><i class="fa fa-check"></i><b>8.3</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a><ul>
<li class="chapter" data-level="8.3.1" data-path="class.html"><a href="class.html#k-nn-algorithm"><i class="fa fa-check"></i><b>8.3.1</b> <span class="math inline">\(k\)</span>-NN algorithm</a></li>
<li class="chapter" data-level="8.3.2" data-path="class.html"><a href="class.html#r-knn-example"><i class="fa fa-check"></i><b>8.3.2</b> R knn Example</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="class.html"><a href="class.html#Oct31"><i class="fa fa-check"></i><b>8.4</b> 10/31/19 Agenda</a></li>
<li class="chapter" data-level="8.5" data-path="class.html"><a href="class.html#cart"><i class="fa fa-check"></i><b>8.5</b> CART</a><ul>
<li class="chapter" data-level="8.5.1" data-path="class.html"><a href="class.html#cart-algorithm"><i class="fa fa-check"></i><b>8.5.1</b> CART algorithm</a></li>
<li class="chapter" data-level="8.5.2" data-path="class.html"><a href="class.html#r-cart-example"><i class="fa fa-check"></i><b>8.5.2</b> R CART Example</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="class.html"><a href="class.html#Nov5"><i class="fa fa-check"></i><b>8.6</b> 11/5/19 Agenda</a></li>
<li class="chapter" data-level="8.7" data-path="class.html"><a href="class.html#bagging"><i class="fa fa-check"></i><b>8.7</b> Bagging</a><ul>
<li class="chapter" data-level="8.7.1" data-path="class.html"><a href="class.html#out-of-bag-oob-error-rate"><i class="fa fa-check"></i><b>8.7.1</b> Out Of Bag (OOB) error rate</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="class.html"><a href="class.html#Nov7"><i class="fa fa-check"></i><b>8.8</b> 11/7/19 Agenda</a></li>
<li class="chapter" data-level="8.9" data-path="class.html"><a href="class.html#rf"><i class="fa fa-check"></i><b>8.9</b> Random Forests</a><ul>
<li class="chapter" data-level="8.9.1" data-path="class.html"><a href="class.html#random-forest-algorithm"><i class="fa fa-check"></i><b>8.9.1</b> Random Forest algorithm</a></li>
<li class="chapter" data-level="8.9.2" data-path="class.html"><a href="class.html#r-rf-example"><i class="fa fa-check"></i><b>8.9.2</b> R RF Example</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="class.html"><a href="class.html#model-choices"><i class="fa fa-check"></i><b>8.10</b> Model Choices</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://st47s.com/Math154" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Computational Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="class" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Classification</h1>
<!--
Daniela Witten talking about inference in prediction: https://www.youtube.com/watch?v=Y4UJjzuYjfM 
R Unconference 2013

that f is a meaningful reflection of the true unknown process.


One of the most satisfying aspects of this unit is that you can now turn students loose on a massive data set. Past instances of the KDD Cup \url{http://www.sigkdd.org/kddcup/index.php} are an excellent source for such data sets. We explored data from the 2008 KDD Cup on breast cancer. Each of the n observations contained digitized data from an X-Ray image of a breast. Each observation corresponded to a small area of a particular breast, which may or may not depict a malignant tumor ï¿œï¿œthis provided the binary response variable.  In addition to a handful of well-defined variables ((x, y)-location, etc.), each observation has 117 nameless attributes, about which no information was provided. Knowing nothing about what these variables mean, students recognized the need to employ machine learning techniques to sift through them and find relationships. The size of the data and number of variables made manual exploration of the data impractical.

\textcolor{red}{See zissermanML.pdf for much more on regression trees, SVM, etc.}
\url{http://www.dabi.temple.edu/~hbling/8590.002/Montillo_RandomForests_4-2-2009.pdf}
-->
<div id="Oct29" class="section level2">
<h2><span class="header-section-number">8.1</span> 10/29/19 Agenda</h2>
<ol style="list-style-type: decimal">
<li>classification</li>
<li><span class="math inline">\(k\)</span>-Nearest Neighbors</li>
<li>bias-variance trade-off</li>
<li>cross validation</li>
</ol>
<p><strong>Important Note</strong>: For the majority of the classification and clustering methods, we will use the <code>caret</code> package in R. For more information see: <a href="http://topepo.github.io/caret/index.html" class="uri">http://topepo.github.io/caret/index.html</a></p>
<p>Also, check out the <code>caret</code> cheat sheet: <a href="https://github.com/rstudio/cheatsheets/raw/master/caret.pdf" class="uri">https://github.com/rstudio/cheatsheets/raw/master/caret.pdf</a></p>
<p><span class="citation">Baumer (<a href="#ref-Baumer15">2015</a>)</span> provides a concise explanation of how both statistics and data science work to enhance ideas of machine learning, one aspect of which is classification:</p>
<blockquote>
<p>In order to understand machine learning, one must recognize the differences between the mindset of the data miner and the statistician, notably characterized by <span class="citation">Breiman (<a href="#ref-brei01">2001</a>)</span>, who distinguished two types of models f for y, the response variable, and x, a vector of explanatory variables. One might consider a <em>data model</em> f such that y <span class="math inline">\(\sim\)</span> f(x), assess whether f could reasonably have been the process that generated y from x, and then make inferences about f. The goal here is to learn about the real process that generated y from x, and the conceit is</p>
</blockquote>
<blockquote>
<p>Alternatively, one might construct an <em>algorithmic model</em> f, such that <span class="math inline">\(y \sim f(x)\)</span>, and use f to predict unobserved values of y. If it can be determined that f does in fact do a good job of predicting values of y, one might not care to learn much about f. In the former case, since we want to learn about f, a simpler model may be preferred. Conversely, in the latter case, since we want to predict new values of y, we may be indifferent to model complexity (other than concerns about overfitting and scalability).</p>
</blockquote>
<p>Classification is a supervised learning technique to extract general patterns from the data in order to build a predictor for a new test or validation data set. That is, the model should <em>classify</em> new points into groups (or with a numerical response values) based on a model built from a set of data which provides known group membership for each value. For most of the methods below, we will consider classifying into categories (in fact, usually only two categories), but sometimes (e.g., support vector machines and linear regression) the goal is to predict a numeric variable.</p>
<p>Some examples of classification techniques include: linear regression, logistic regression, neural networks, <strong>classification trees</strong>, <strong>random forests</strong>, <strong>k-nearest neighbors</strong>, <strong>support vector machines</strong>, näive Bayes, and linear discriminant analysis. We will cover the methods in <strong>bold</strong>.</p>
<p><strong>Simple is Better</strong> (From <span class="citation">Fielding (<a href="#ref-field07">2007</a>)</span>, p. 87)</p>
<ol style="list-style-type: decimal">
<li>We want to avoid over-fitting the model (certainly, it is a bad idea to model the noise!)</li>
<li>Future prediction performance goes down with too many predictors.</li>
<li>Simple models provide better insight into causality and specific associations.</li>
<li>Fewer predictors implies fewer variables to collect in later studies.</li>
</ol>
<p>That said, the model should still represent the complexity of the data! We describe the trade-off above as the “bias-variance” trade-off. In order to fully understand that trade-off, let’s first cover the classification method known as <span class="math inline">\(k\)</span>-Nearest Neighbors.</p>
</div>
<div id="cv" class="section level2">
<h2><span class="header-section-number">8.2</span> Cross Validation</h2>
<div id="bias-variance-trade-off" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Bias-variance trade-off</h3>
<div id="excellent-resource" class="section level4 unnumbered">
<h4>Excellent resource</h4>
<p>for explaining the bias-variance trade-off: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="uri">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></p>
<ul>
<li><p><strong>Variance</strong> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training set. Generally, the closer the model fits the data, the more variable it will be (it’ll be different for each data set!). A model with many many explanatory variables will often fit the data too closely.</p></li>
<li><p><strong>Bias</strong> refers to the error that is introduced by approximating the “truth” by a model which is too simple. For example, we often use linear models to describe complex relationships, but it is unlikely that any real life situation actually has a <em>true</em> linear model. However, if the true relationship is close to linear, then the linear model will have a low bias.</p></li>
</ul>
<p>Generally, the simpler the model, the lower the variance. The more complicated the model, the lower the bias. In this class, cross validation will be used to assess model fit. [If time permits, Receiver Operating Characteristic (ROC) curves will also be covered.]</p>
<p><span class="math display">\[\begin{align}
\mbox{prediction error } = \mbox{ irreducible error } + \mbox{ bias } + \mbox{ variance}
\end{align}\]</span></p>
<ul>
<li><strong>irreducible error</strong> The irreducible error is the natural variability that comes with observations. No matter how good the model is, we will never be able to predict perfectly.</li>
<li><strong>bias</strong> The bias of the model represents the difference between the true model and a model which is too simple. That is, the more complicated the model (e.g., smaller <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN), the closer the points are to the prediction. As the model gets more complicated (e.g., as <span class="math inline">\(k\)</span> decreases), the bias goes down.</li>
<li><strong>variance</strong> The variance represents the variability of the model from sample to sample. That is, a simple model (big <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN) would not change a lot from sample to sample. The variance decreases as the model becomes more simple (e.g., as <span class="math inline">\(k\)</span> increases).</li>
</ul>
<p>Note the bias-variance trade-off. We want our prediction error to be small, so we choose a model that is medium with respect to both bias and variance. We cannot control the irreducible error.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-2"></span>
<img src="figs/varbias.png" alt="Test and training error as a function of model complexity.  Note that the error goes down monotonically only for the training data.  Be careful not to overfit!!  [@ESL]" width="100%" />
<p class="caption">
Figure 1.2: Test and training error as a function of model complexity. Note that the error goes down monotonically only for the training data. Be careful not to overfit!! <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-ESL">2001</a>)</span>
</p>
</div>
<p>The following visualization does an excellent job of communicating the trade-off between bias and variance as a function of a specific tuning parameter, here: minimum node size of a classification tree. <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-2/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-2/</a></p>
</div>
</div>
<div id="implementing-cross-validation" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Implementing Cross Validation</h3>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-3"></span>
<img src="figs/overfitting.jpg" alt="[@flach12]" width="100%" />
<p class="caption">
Figure 1.3: <span class="citation">(Flach <a href="#ref-flach12">2012</a>)</span>
</p>
</div>
<p>Cross validation is typically used in two ways.</p>
<ol style="list-style-type: decimal">
<li>To assess a model’s accuracy (<em>model assessment</em>).<br />
</li>
<li>To build a model (<em>model selection</em>).</li>
</ol>
<div id="different-ways-to-cv" class="section level4 unnumbered">
<h4>Different ways to CV</h4>
<p>Suppose that we build a classifier on a given data set. We’d like to know how well the model classifies observations, but if we test on the samples at hand, the error rate will be much lower than the model’s inherent accuracy rate. Instead, we’d like to predict <em>new</em> observations that were not used to create the model. There are various ways of creating <em>test</em> or <em>validation</em> sets of data:</p>
<ul>
<li>one training set, one test set [two drawbacks: estimate of error is highly variable because it depends on which points go into the training set; and because the training data set is smaller than the full data set, the error rate is biased in such a way that it overestimates the actual error rate of the modeling technique.]</li>
<li>leave one out cross validation (LOOCV)</li>
</ul>
<ol style="list-style-type: decimal">
<li>remove one observation</li>
<li>build the model using the remaining n-1 points</li>
<li>predict class membership for the observation which was removed</li>
<li>repeat by removing each observation one at a time</li>
</ol>
<ul>
<li><span class="math inline">\(k\)</span>-fold cross validation (<span class="math inline">\(k\)</span>-fold CV)
<ul>
<li>like LOOCV except that the algorithm is run <span class="math inline">\(k\)</span> times on each group (of approximately equal size) from a partition of the data set.]</li>
<li>LOOCV is a special case of <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k=n\)</span></li>
<li>advantage of <span class="math inline">\(k\)</span>-fold is computational</li>
<li><span class="math inline">\(k\)</span>-fold often has a better bias-variance trade-off [bias is lower with LOOCV. however, because LOOCV predicts <span class="math inline">\(n\)</span> observations from <span class="math inline">\(n\)</span> models which are basically the same, the variability will be higher (i.e., based on the <span class="math inline">\(n\)</span> data values). with <span class="math inline">\(k\)</span>-fold, prediction is on <span class="math inline">\(n\)</span> values from <span class="math inline">\(k\)</span> models which are much less correlated. the effect is to average out the predicted values in such a way that there will be less variability from data set to data set.]</li>
</ul></li>
</ul>
</div>
<div id="cv-for-model-assessment-10-fold" class="section level4 unnumbered">
<h4>CV for <strong>Model assessment</strong> 10-fold</h4>
<ol style="list-style-type: decimal">
<li>assume <span class="math inline">\(k\)</span> is given for <span class="math inline">\(k\)</span>-NN</li>
<li>remove 10% of the data</li>
<li>build the model using the remaining 90%</li>
<li>predict class membership / continuous response for the 10% of the observations which were removed</li>
<li>repeat by removing each decile one at a time</li>
<li>a good measure of the model’s ability to predict is the error rate associated with the predictions on the data which have been independently predicted</li>
</ol>
</div>
<div id="cv-for-model-selection-10-fold" class="section level4 unnumbered">
<h4>CV for <strong>Model selection</strong> 10-fold</h4>
<ol style="list-style-type: decimal">
<li>set <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-NN</li>
<li>build the model using the <span class="math inline">\(k\)</span> value set above:
<ol style="list-style-type: lower-alpha">
<li>remove 10% of the data</li>
<li>build the model using the remaining 90%</li>
<li>predict class membership / continuous response for the 10% of the observations which were removed</li>
<li>repeat by removing each decile one at a time</li>
</ol></li>
<li>measure the CV prediction error for the <span class="math inline">\(k\)</span> value at hand</li>
<li>repeat steps 1-3 and choose the <span class="math inline">\(k\)</span> for which the prediction error is lowest</li>
</ol>
</div>
<div id="cv-for-model-assessment-and-selection-10-fold" class="section level4 unnumbered">
<h4>CV for <strong>Model assessment and selection</strong> 10-fold</h4>
<p>To do both, one approach is to use test/training data <em>and</em> CV in order to both model assessment and selection. Note that CV could be used in both steps, but the algorithm is slightly more complicated.</p>
<ol style="list-style-type: decimal">
<li>split the data into training and test observations</li>
<li>set <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-NN</li>
<li>build the model using the <span class="math inline">\(k\)</span> value set above on <em>only the training data</em>:
<ol style="list-style-type: lower-alpha">
<li>remove 10% of the training data</li>
<li>build the model using the remaining 90% of the training data</li>
<li>predict class membership / continuous response for the 10% of the training observations which were removed</li>
<li>repeat by removing each decile one at a time from the training data</li>
</ol></li>
<li>measure the CV prediction error for the <span class="math inline">\(k\)</span> value at hand on the training data</li>
<li>repeat steps 2-4 and choose the <span class="math inline">\(k\)</span> for which the prediction error is lowest for the training data</li>
<li>using the <span class="math inline">\(k\)</span> value given in step 5, assess the prediction error on the test data</li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-4"></span>
<img src="figs/CV.jpg" alt="Nested cross-validation: two cross-validation loops are run one inside the other.  [@CVpaper]" width="100%" />
<p class="caption">
Figure 1.4: Nested cross-validation: two cross-validation loops are run one inside the other. <span class="citation">(Varoquaux et al. <a href="#ref-CVpaper">2017</a>)</span>
</p>
</div>
</div>
</div>
</div>
<div id="knn" class="section level2">
<h2><span class="header-section-number">8.3</span> <span class="math inline">\(k\)</span>-Nearest Neighbors</h2>
<p>The <span class="math inline">\(k\)</span>-Nearest Neighbor algorithm does exactly what it sounds like it does. The user decides on the integer value for <span class="math inline">\(k\)</span>, and a point is classified to be in the group for which the majority of the <span class="math inline">\(k\)</span> closest points in the training data.</p>
<div id="k-nn-algorithm" class="section level3">
<h3><span class="header-section-number">8.3.1</span> <span class="math inline">\(k\)</span>-NN algorithm</h3>
<ol style="list-style-type: decimal">
<li>Decide on a distance metric (e.g., Euclidean distance, 1 - correlation, etc.) and find the distances from each point in the test set to each point in the training set. The distance is measured in the feature space, that is, with respect to the explanatory variables (not the response variable).</li>
</ol>
<p>n.b. In most machine learning algorithms that use “distance” as a measure, the “distance” is not required to be a mathematical distance metric. Indeed, 1-correlation is a very common distance measure, and it fails the triangle inequality.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Consider a point in the test set. Find the <span class="math inline">\(k\)</span> closest points in the training set to the one test observation.</p></li>
<li><p>Using majority vote, find the dominate class of the <span class="math inline">\(k\)</span> closest points. Predict that class label to the test observation.</p></li>
</ol>
<p>Note: if the response variable is continuous (instead of categorical), find the average response variable of the <span class="math inline">\(k\)</span> training point to be the predicted response for the one test observation.</p>
<p><strong>Shortcomings of <span class="math inline">\(k\)</span>-NN</strong>:
* one class can dominate if it has a large majority
* Euclidean distance is dominated by scale
* it can be computationally unwieldy (and unneeded!!) to calculate all distances (there are algorithms to search smartly)
* the output doesn’t provide any information about which explanatory variables are informative.</p>
<p><strong>Strengths of <span class="math inline">\(k\)</span>-NN</strong>:
* it can easily work for any number of categories
* it can predict a quantitative response variable
* the bias of 1-NN is often low (but the variance is high)
* any distance metric can be used (so the algorithm models the data appropriately)
* the method is simple to implement / understand
* model is nonparametric (no distributional assumptions on the data)
* great model for imputing missing data</p>
<p><img src="figs/knnmodel.jpg" width="100%" style="display: block; margin: auto;" /><img src="figs/knnK.jpg" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="r-knn-example" class="section level3">
<h3><span class="header-section-number">8.3.2</span> R knn Example</h3>
<p>R code for using the <code>caret</code> package to cluster the <code>iris</code> data. The <code>caret</code> package vignette for <code>knn</code> is here: <a href="http://topepo.github.io/caret/miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function" class="uri">http://topepo.github.io/caret/miscellaneous-model-functions.html#yet-another-k-nearest-neighbor-function</a></p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1"><span class="kw">library</span>(GGally) <span class="co"># for plotting</span></a>
<a class="sourceLine" id="cb455-2" data-line-number="2"><span class="kw">library</span>(caret)  <span class="co"># for partitioning &amp; classification</span></a>
<a class="sourceLine" id="cb455-3" data-line-number="3"><span class="kw">data</span>(iris)</a></code></pre></div>
<div id="iris-data" class="section level4 unnumbered">
<h4>iris Data</h4>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb456-1" data-line-number="1"><span class="kw">ggpairs</span>(iris, <span class="dt">color=</span><span class="st">&quot;Species&quot;</span>, <span class="dt">alpha=</span>.<span class="dv">4</span>)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-7-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="knn-1" class="section level4 unnumbered">
<h4>kNN</h4>
<p>Without thinking about test / training data, a naive model is:</p>
<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">fitControl &lt;-caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>, <span class="dt">classProbs =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb457-2" data-line-number="2">tr.iris &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(Species <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb457-3" data-line-number="3">                        <span class="dt">data=</span>iris,</a>
<a class="sourceLine" id="cb457-4" data-line-number="4">                        <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,</a>
<a class="sourceLine" id="cb457-5" data-line-number="5">                        <span class="dt">trControl =</span> fitControl,</a>
<a class="sourceLine" id="cb457-6" data-line-number="6">                        <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">k=</span><span class="dv">3</span>))</a>
<a class="sourceLine" id="cb457-7" data-line-number="7"></a>
<a class="sourceLine" id="cb457-8" data-line-number="8">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(<span class="dt">data=</span><span class="kw">predict</span>(tr.iris, <span class="dt">newdata =</span> iris), </a>
<a class="sourceLine" id="cb457-9" data-line-number="9">                <span class="dt">reference =</span> iris<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         47         3
##   virginica       0          3        47
## 
## Overall Statistics
##                                          
##                Accuracy : 0.96           
##                  95% CI : (0.915, 0.9852)
##     No Information Rate : 0.3333         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.94           
##                                          
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9400           0.9400
## Specificity                 1.0000            0.9700           0.9700
## Pos Pred Value              1.0000            0.9400           0.9400
## Neg Pred Value              1.0000            0.9700           0.9700
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3133           0.3133
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9550           0.9550</code></pre>
</div>
<div id="why-naive" class="section level4 unnumbered">
<h4>Why naive?</h4>
<ol style="list-style-type: decimal">
<li>Not good to train and test on the same data set!</li>
<li>Assumed the knowledge of <span class="math inline">\(k\)</span> groups.</li>
<li>Was Euclidean distance the right thing to use? [The <code>knn</code> package in R only uses Euclidean distance.]</li>
</ol>
</div>
<div id="using-testtraining-data-sets." class="section level4 unnumbered">
<h4>Using test/training data sets.</h4>
<p>One of the common pieces to use in the <code>caret</code> package is creating test and training datasets for cross validation.</p>
<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb459-2" data-line-number="2">inTrain &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> iris<span class="op">$</span>Species, <span class="dt">p=</span><span class="fl">0.7</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb459-3" data-line-number="3">iris.train &lt;-<span class="st"> </span>iris[inTrain,]</a>
<a class="sourceLine" id="cb459-4" data-line-number="4">iris.test &lt;-<span class="st"> </span>iris[<span class="op">-</span><span class="kw">c</span>(inTrain),]</a>
<a class="sourceLine" id="cb459-5" data-line-number="5"></a>
<a class="sourceLine" id="cb459-6" data-line-number="6">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb459-7" data-line-number="7">tr.iris &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(Species <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb459-8" data-line-number="8">                        <span class="dt">data=</span>iris.train, </a>
<a class="sourceLine" id="cb459-9" data-line-number="9">                        <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb459-10" data-line-number="10">                        <span class="dt">trControl =</span> fitControl, </a>
<a class="sourceLine" id="cb459-11" data-line-number="11">                        <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">k=</span><span class="dv">5</span>))</a>
<a class="sourceLine" id="cb459-12" data-line-number="12"></a>
<a class="sourceLine" id="cb459-13" data-line-number="13">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(<span class="dt">data=</span><span class="kw">predict</span>(tr.iris, <span class="dt">newdata =</span> iris.test), </a>
<a class="sourceLine" id="cb459-14" data-line-number="14">                <span class="dt">reference =</span> iris.test<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         1
##   virginica       0          1        14
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9556          
##                  95% CI : (0.8485, 0.9946)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9333          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.9333
## Specificity                 1.0000            0.9667           0.9667
## Pos Pred Value              1.0000            0.9333           0.9333
## Neg Pred Value              1.0000            0.9667           0.9667
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.3111
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9500           0.9500</code></pre>
</div>
<div id="k-neighbors-cv-on-training-to-find-k" class="section level4 unnumbered">
<h4><span class="math inline">\(k\)</span> neighbors? CV on TRAINING to find <span class="math inline">\(k\)</span></h4>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">47</span>)</a>
<a class="sourceLine" id="cb461-2" data-line-number="2">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb461-3" data-line-number="3">tr.iris &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(Species <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb461-4" data-line-number="4">                        <span class="dt">data=</span>iris.train, </a>
<a class="sourceLine" id="cb461-5" data-line-number="5">                        <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>, </a>
<a class="sourceLine" id="cb461-6" data-line-number="6">                        <span class="dt">trControl =</span> fitControl, </a>
<a class="sourceLine" id="cb461-7" data-line-number="7">                        <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">k=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>,<span class="dv">9</span>,<span class="dv">11</span>)))</a>
<a class="sourceLine" id="cb461-8" data-line-number="8">tr.iris</a></code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 105 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 95, 93, 95, 95, 94, 95, ... 
## Resampling results across tuning parameters:
## 
##   k   Accuracy   Kappa    
##    1  0.9516667  0.9275521
##    3  0.9316667  0.8974822
##    5  0.9205556  0.8805824
##    7  0.9500000  0.9249006
##    9  0.9616667  0.9427036
##   11  0.9716667  0.9580882
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was k = 11.</code></pre>
</div>
<div id="then-measure-accuracy-by-testing-on-test-data" class="section level4 unnumbered">
<h4>Then measure accuracy by testing on test data!</h4>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" data-line-number="1">caret<span class="op">::</span><span class="kw">confusionMatrix</span>(<span class="dt">data=</span><span class="kw">predict</span>(tr.iris, <span class="dt">newdata =</span> iris.test), </a>
<a class="sourceLine" id="cb463-2" data-line-number="2">                       <span class="dt">reference =</span> iris.test<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         1
##   virginica       0          1        14
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9556          
##                  95% CI : (0.8485, 0.9946)
##     No Information Rate : 0.3333          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.9333          
##                                           
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.9333           0.9333
## Specificity                 1.0000            0.9667           0.9667
## Pos Pred Value              1.0000            0.9333           0.9333
## Neg Pred Value              1.0000            0.9667           0.9667
## Prevalence                  0.3333            0.3333           0.3333
## Detection Rate              0.3333            0.3111           0.3111
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.9500           0.9500</code></pre>
</div>
</div>
</div>
<div id="Oct31" class="section level2">
<h2><span class="header-section-number">8.4</span> 10/31/19 Agenda</h2>
<ol style="list-style-type: decimal">
<li>trees (CART)</li>
<li>building trees (binary recursive splitting)</li>
<li>homogeneity measures</li>
<li>pruning trees</li>
</ol>
</div>
<div id="cart" class="section level2">
<h2><span class="header-section-number">8.5</span> CART</h2>
<p>See the following (amazing!) demonstration for tree intuition: <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-1/</a></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-12"></span>
<img src="figs/ObamaClinton.jpg" alt="http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg Best information was whether or not the county was more than 20 percent black.   Then each successive node is split again on the best possible informative variable.  Note that the leaves on the tree are reasonably homogenous. NYT, April 16, 2008." width="100%" />
<p class="caption">
Figure 1.8: <a href="http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg" class="uri">http://graphics8.nytimes.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg</a> Best information was whether or not the county was more than 20 percent black. Then each successive node is split again on the best possible informative variable. Note that the leaves on the tree are reasonably homogenous. NYT, April 16, 2008.
</p>
</div>
<div id="cart-algorithm" class="section level3">
<h3><span class="header-section-number">8.5.1</span> CART algorithm</h3>
<p><strong>Basic Classification and Regression Trees (CART) Algorithm:</strong></p>
<ol style="list-style-type: decimal">
<li>Start with all observations in one group.</li>
<li>Find the variable/split that best separates the response variable (successive binary partitions based on the different predictors / explanatory variables).
<ul>
<li>Evaluation “homogeneity” within each group</li>
<li>Divide the data into two groups (“leaves”) on that split (“node”).</li>
<li>Within each split, find the best variable/split that separates the outcomes.</li>
</ul></li>
<li>Continue until the groups are too small or sufficiently “pure”.</li>
<li>Prune tree.</li>
</ol>
<p><strong>Shortcomings of CART:</strong></p>
<ul>
<li>Straight CART do not generally have the same predictive accuracy as other classification approaches. (we will improve the model - see random forests, boosting, bagging)</li>
<li>Difficult to write down / consider the CART “model”</li>
<li>Without proper pruning, the model can easily lead to overfitting</li>
<li>With lots of predictors, (even greedy) partitioning can become computationally unwieldy</li>
<li>Often, prediction performance is poor</li>
</ul>
<p><strong>Strengths of CART:</strong></p>
<ul>
<li>They are easy to explain; trees are easy to display graphically (which make them easy to interpret). (They mirror the typical human decision-making process.)</li>
<li>Can handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!).</li>
<li>Can handle more than 2 groups for categorical predictions</li>
<li>Easily ignore redundant variables.</li>
<li>Perform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables.</li>
<li>Data transformations may be less important (monotone transformations on the explanatory variables won’t change anything).</li>
</ul>
<div id="classification-trees" class="section level4">
<h4><span class="header-section-number">8.5.1.1</span> Classification Trees</h4>
<p>A <em>classification tree</em> is used to predict a categorical response variable (rather than a quantitative one). The end predicted value will be the one of the <em>most commonly occurring class</em> of training observations in the region to which it belongs. The goal is to create regions which are as homogeneous as possible with respect to the response variable - categories.</p>
<p><strong>measures of impurity</strong></p>
<ol style="list-style-type: decimal">
<li>Calculate the <em>classification error rate</em> as the fraction of the training observations in that region that do not belong to the most common class: <span class="math display">\[E_m = 1 - \max_k(\hat{p}_{mk})\]</span>
where <span class="math inline">\(\hat{p}_{mk}\)</span> represents the proportion of training observations in the <span class="math inline">\(m\)</span>th region that are from the <span class="math inline">\(k\)</span>th class. However, the classification error rate is not particularly sensitive to node purity, and so two additional measures are typically used to partition the regions.</li>
<li>Further, the <em>Gini index</em> is defined by <span class="math display">\[G_m= \sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})\]</span>
a measure of total variance across the <span class="math inline">\(K\)</span> classes. [Recall, the variance of a Bernoulli random variable with <span class="math inline">\(\pi\)</span> = P(success) is <span class="math inline">\(\pi(1-\pi)\)</span>.] Note that the Gini index takes on a small value if all of the <span class="math inline">\(\hat{p}_{mk}\)</span> values are close to zero or one. For this reason, the Gini index is referred to as a measure of node <em>purity</em> - a small value indicates that a node contains predominantly observations from a single class.</li>
<li>Last, the <em>cross-entropy</em> is defined as <span class="math display">\[D_m = - \sum_{k=1}^K \hat{p}_{mk} \log \hat{p}_{mk}\]</span>
Since <span class="math inline">\(0 \leq \hat{p}_{mk} \leq 1\)</span> it follows that <span class="math inline">\(0 \leq -\hat{p}_{mk} \log\hat{p}_{mk}\)</span>. One can show that the cross-entropy will take on a value near zero if the <span class="math inline">\(\hat{p}_{mk}\)</span> values are all near zero or all near one. Therefore, like the Gini index, the cross-entropy will take on a small value if the <span class="math inline">\(m\)</span>th node is pure.</li>
<li>To <em>build</em> the tree, typically the Gini index or the cross-entropy are used to evaluate a particular split.</li>
<li>To <em>prune</em> the tree, often classification error is used (if accuracy of the final pruned tree is the goal)</li>
</ol>
<p>Computationally, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a <em>top down</em> approach to the problem which is known as <em>recursive binary splitting</em> (<em>greedy</em> because we look only at the current split and not at the outcomes of the splits to come).</p>
<p><strong>Recursive Binary Splitting on Categories</strong> (for a given node)</p>
<ol style="list-style-type: decimal">
<li>Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\(\{X | X_j&lt; s\}\)</span> and <span class="math inline">\(\{X | X_j \geq s\}\)</span> lead to the greatest reduction in Gini index or cross-entropy.</li>
<li>For any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, define the pair of half-planes to be
<span class="math display">\[R_1(j,s) = \{X | X_j &lt; s\} \mbox{ and } R_2(j,s) = \{X | X_j \geq s\}\]</span>
and we seek the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that minimize the equation:
<span class="math display">\[\begin{align}
&amp; \sum_{i:x_i \in R_1(j,s)} \sum_{k=1}^K \hat{p}_{{R_1}k}(1-\hat{p}_{{R_1}k}) + \sum_{i:x_i \in R_2(j,s)} \sum_{k=1}^K \hat{p}_{{R_2}k}(1-\hat{p}_{{R_2}k})\\
\mbox{equivalently: } &amp; n_{R_1} \sum_{k=1}^K \hat{p}_{{R_1}k}(1-\hat{p}_{{R_1}k}) + n_{R_2} \sum_{k=1}^K \hat{p}_{{R_2}k}(1-\hat{p}_{{R_2}k})\\
\end{align}\]</span></li>
<li>Repeat the process, looking for the best predictor and best cutpoint <em>within</em> one of the previously identified regions (producing three regions, now).</li>
<li>Keep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations.</li>
</ol>
</div>
<div id="regression-trees" class="section level4">
<h4><span class="header-section-number">8.5.1.2</span> Regression Trees</h4>
<p>The goal of the algorithm in a <em>regression tree</em> is to split the set of possible value for the data into <span class="math inline">\(J\)</span> distinct and non-overlapping regions, <span class="math inline">\(R_1, R_2, \ldots, R_J\)</span>. For every observation that falls into the region <span class="math inline">\(R_J\)</span>, we make the same prediction - the mean of the response values for the training observations in <span class="math inline">\(R_J\)</span>. So how do we find the regions <span class="math inline">\(R_1, \ldots, R_J\)</span>?</p>
<p><span class="math inline">\(\Rightarrow\)</span> Minimize RSS, <span class="math display">\[RSS = \sum_{j=1}^J \sum_{i \in R_j} (y_i - \overline{y}_{R_j})^2\]</span>
where <span class="math inline">\(\overline{y}_{R_j}\)</span> is the mean response for the training observations within the <span class="math inline">\(j\)</span>th region.</p>
<p>(Note: in the chapter <span class="citation">(James et al. <a href="#ref-ISL">2013</a>)</span> they refer to MSE - mean squared error - in addition to RSS where MSE is simply RSS / n, see equation (2.5).)</p>
<p><span class="math display">\[ MSE = \frac{\sum_{i=1}^N (y_i - \overline{y}_i)^2}{N}\]</span></p>
<p>Again, it is usually infeasible to consider every possible partition of the observations. Instead of looking at all partitions, we perform a <em>top down</em> approach to the problem which is known as <em>recursive binary splitting</em> (<em>greedy</em> because we look only at the current split and not at the outcomes of the splits to come).</p>
<p><strong>Recursive Binary Splitting on Numerical Response</strong> (for a given node)</p>
<ol style="list-style-type: decimal">
<li>Select the predictor <span class="math inline">\(X_j\)</span> and the cutpoint <span class="math inline">\(s\)</span> such that splitting the predictor space into the regions <span class="math inline">\(\{X | X_j&lt; s\}\)</span> and <span class="math inline">\(\{X | X_j \geq s\}\)</span> lead to the greatest reduction in RSS.</li>
<li>For any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, define the pair of half-planes to be
<span class="math display">\[R_1(j,s) = \{X | X_j &lt; s\} \mbox{ and } R_2(j,s) = \{X | X_j \geq s\}\]</span>
and we see the value of <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span> that minimize the equation:
<span class="math display">\[\sum_{i:x_i \in R_1(j,s)} (y_i - \overline{y}_{R_1})^2 + \sum_{i:x_i \in R_2(j,s)} (y_i - \overline{y}_{R_2})^2\]</span>
where <span class="math inline">\(\overline{y}_{R_1}\)</span> is the mean response for the training observations in <span class="math inline">\(R_1(j,s)\)</span> and <span class="math inline">\(\overline{y}_{R_2}\)</span> is the mean response for training observations in <span class="math inline">\(R_2(j,s)\)</span>.</li>
<li>Repeat the process, looking for the best predictor and best cutpoint <em>within</em> one of the previously identified regions (producing three regions, now).</li>
<li>Keep repeating the process until a stopping criterion is reached - for example, until no region contains more than 5 observations.</li>
</ol>
</div>
<div id="avoiding-overfitting" class="section level4">
<h4><span class="header-section-number">8.5.1.3</span> (Avoiding) Overfitting</h4>
<p>Ideally, the tree would not overfit the training data. One could imagine how easy it would be to grow the tree over the training data so as to end up with terminal nodes which are completely homogeneous (but then don’t represent the test data).</p>
<p>See the following (amazing!) demonstration for intuition on model validation / overfitting: <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-2/" class="uri">http://www.r2d3.us/visual-intro-to-machine-learning-part-2/</a></p>
<p>One possible algorithm for building a tree is to split based on the reduction in RSS (or Gini index, etc.) exceeding some (presumably high) threshold. However, the strategy is known to be short sighted, as a split later down the tree may contain a large amount of information. A better strategy is to grow a very large tree <span class="math inline">\(T_0\)</span> and then prune it back in order to obtain a subtree. We use cross validation to build the subtree so as to not overfit the data.</p>
<hr />
<p><strong>Algorithm</strong>: Building a Regression Tree</p>
<hr />
<ol style="list-style-type: decimal">
<li>Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.</li>
<li>Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of <span class="math inline">\(\alpha\)</span>.</li>
<li>Use <span class="math inline">\(K\)</span>-fold cross-validation to choose <span class="math inline">\(\alpha\)</span>. That is, divide the training observations into <span class="math inline">\(K\)</span> folds. For each <span class="math inline">\(k=1, 2, \ldots, K\)</span>:
<ol style="list-style-type: lower-alpha">
<li>Repeat Steps 1 and 2 on all but the <span class="math inline">\(k\)</span>th fold of the training data.</li>
<li>Evaluate the mean squared prediction error on the data in the left-out <span class="math inline">\(k\)</span>th fold, as a function of <span class="math inline">\(\alpha\)</span>.
For each value of <span class="math inline">\(\alpha\)</span>, average the prediction error (either misclassification or RSS), and pick <span class="math inline">\(\alpha\)</span> to minimize the average error.</li>
</ol></li>
<li>Return the subtree from Step 2 that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>.</li>
</ol>
<hr />
</div>
<div id="cost-complexity-pruning" class="section level4">
<h4><span class="header-section-number">8.5.1.4</span> Cost Complexity Pruning</h4>
<p>Also known as <em>weakest link pruning</em>, the idea is to consider a sequence of trees indexed by a nonnegative tuning parameter <span class="math inline">\(\alpha\)</span> (instead of considering every single subtree). Generally, the idea is that there is a cost to having a larger (more complex!) tree. We define the cost complexity criterion (<span class="math inline">\(\alpha &gt; 0\)</span>):
<span class="math display">\[\begin{align}
\mbox{numerical: } C_\alpha(T) &amp;= \sum_{m=1}^{|T|} \sum_{i \in R_m} (y_i - \overline{y}_{R_m})^2 + \alpha|T|\\
\mbox{categorical: } C_\alpha(T) &amp;= \sum_{m=1}^{|T|} \sum_{i \in R_m} I(y_i \ne k(m)) + \alpha|T|
\end{align}\]</span>
where <span class="math inline">\(k(m)\)</span> is the class with the majority of observations in node <span class="math inline">\(m\)</span> and <span class="math inline">\(|T|\)</span> is the number of terminal nodes in the tree.</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> small: If <span class="math inline">\(\alpha\)</span> is set to be small, we are saying that the risk is more worrisome than the complexity and larger trees are favored because they reduce the risk.</li>
<li><span class="math inline">\(\alpha\)</span> large: If <span class="math inline">\(\alpha\)</span> is set to be large, then the complexity of the tree is more worrisome and smaller trees are favored.</li>
</ul>
<p>The way to think about cost complexity is to consider <span class="math inline">\(\alpha\)</span> increasing. As <span class="math inline">\(\alpha\)</span> gets bigger, the “best” tree will be smaller. But the test error will not be monotonically related to the size of the training tree.</p>
<p><img src="figs/treealpha.jpg" width="100%" style="display: block; margin: auto;" /></p>
<div id="variations-on-a-theme" class="section level5 unnumbered">
<h5>Variations on a theme</h5>
<p>The main ideas above are consistent throughout all CART algorithms. However, the exact details of implementation can change from function to function, and often times it is very difficult to decipher exactly which equation is being used. In the <code>tree</code> function in R, much of the decision making is done on <code>deviance</code> which is defined as:
<span class="math display">\[\begin{align}
\mbox{numerical: } \mbox{deviance} &amp;= \sum_{m=1}^{|T|}  \sum_{i \in R_m} (y_i - \overline{y}_{R_m})^2\\
\mbox{categorical: }  \mbox{deviance} &amp;= -2\sum_{m=1}^{|T|} \sum_{k=1}^K n_{mk} \log \hat{p}_{mk}\\
\end{align}\]</span></p>
<p>For the CART algorithm, minimize the deviance (for both types of variables). The categorical deviance will be small if most of the observations are in the majority group (with high proportion). Also, <span class="math inline">\(\lim_{\epsilon \rightarrow 0} \epsilon \log(\epsilon) = 0\)</span>. Additionally, methods of cross validation can also vary. In particular, if the number of variables is large, the tree algorithm can be slow and so the cross validation process - choice of <span class="math inline">\(\alpha\)</span> - needs to be efficient.</p>
</div>
<div id="cv-for-model-building-and-model-assessment" class="section level5 unnumbered">
<h5>CV for model building and model assessment</h5>
<p>Notice that CV is used for both model building and model assessment. It is possible (and practical, though quite computational!) to use both practices on the same classification model. The algorithm could be as follows.</p>
<hr />
<p><strong>Algorithm</strong>: CV for both <span class="math inline">\(k_1\)</span>-fold CV building and <span class="math inline">\(k_2\)</span>-fold CV assessment</p>
<hr />
<ol style="list-style-type: decimal">
<li>Partition the data in <span class="math inline">\(k_1\)</span> groups.</li>
<li>Remove the first group, and train the data on the remaining <span class="math inline">\(k_1-1\)</span> groups.</li>
<li>Use <span class="math inline">\(k_2\)</span>-fold cross-validation (on the <span class="math inline">\(k_1-1\)</span> groups) to choose <span class="math inline">\(\alpha\)</span>. That is, divide the training observations into <span class="math inline">\(k_2\)</span> folds and find <span class="math inline">\(\alpha\)</span> that minimizes the error.</li>
<li>Using the subtree that corresponds to the chosen value of <span class="math inline">\(\alpha\)</span>, predict the first of the <span class="math inline">\(k_1\)</span> hold out samples.</li>
<li>Repeat steps 2-4 using the remaining <span class="math inline">\(k_1 - 1\)</span> groups.</li>
</ol>
<hr />
</div>
</div>
</div>
<div id="r-cart-example" class="section level3">
<h3><span class="header-section-number">8.5.2</span> R CART Example</h3>
<p>There are multiple tree building options in R both in the <code>caret</code> package and <code>party</code>, <code>rpart</code>, and <code>tree</code> packages.</p>
<p>The Census Bureau divides the country up into “tracts” of approximately
equal population. For the 1990 Census, California was divided into 20640 tracts. One data sets (houses on <a href="http://lib.stat.cmu.edu/datasets/" class="uri">http://lib.stat.cmu.edu/datasets/</a>; <a href="http://lib.stat.cmu.edu/datasets/houses.zip" class="uri">http://lib.stat.cmu.edu/datasets/houses.zip</a>) records the following for each tract in California: Median house price, median house age, total number of rooms, total number of bedrooms, total number of occupants, total number of houses, median income (in thousands of dollars), latitude and longitude. It appeared in Pace and Barry (1997), “Sparse Spatial Autoregressions”, <strong>Statistics and Probability Letters</strong>.</p>
<div id="classification-and-regression-trees" class="section level4 unnumbered">
<h4>Classification and Regression Trees</h4>
<p><strong>Classification Trees</strong> are used to predict a response or class <span class="math inline">\(Y\)</span> from input <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. If it is a continuous response it’s called a regression tree, if it is categorical, it’s called a classification tree. At each node of the tree, we check the value of one the input <span class="math inline">\(X_i\)</span> and depending of the (binary) answer we continue to the left or to the right subbranch. When we reach a leaf we will find the prediction (usually it is a simple statistic of the dataset the leaf represents, like the most common value from the available classes).</p>
<p>Note on <code>maxdepth</code>: as you might expect, <code>maxdepth</code> indicates the longest length from the root of the tree to a terminal node. However, for <code>rpart</code> (in particular, using <code>rpart</code> or <code>rpart2</code> in <code>caret</code>), there are other default settings that keep the tree from growing all the way to singular nodes, even with a high <code>maxdepth</code>.</p>
</div>
<div id="regression-trees-1" class="section level4 unnumbered">
<h4>Regression Trees</h4>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" data-line-number="1">real.estate &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt&quot;</span>, </a>
<a class="sourceLine" id="cb465-2" data-line-number="2">                          <span class="dt">header=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb465-3" data-line-number="3"></a>
<a class="sourceLine" id="cb465-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb465-5" data-line-number="5">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb465-6" data-line-number="6">tr.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude, </a>
<a class="sourceLine" id="cb465-7" data-line-number="7">                         <span class="dt">data=</span>real.estate, </a>
<a class="sourceLine" id="cb465-8" data-line-number="8">                         <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>, </a>
<a class="sourceLine" id="cb465-9" data-line-number="9">                         <span class="dt">trControl =</span> fitControl, </a>
<a class="sourceLine" id="cb465-10" data-line-number="10">                         <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">5</span>))</a>
<a class="sourceLine" id="cb465-11" data-line-number="11"></a>
<a class="sourceLine" id="cb465-12" data-line-number="12">rpart.plot<span class="op">::</span><span class="kw">rpart.plot</span>(tr.house<span class="op">$</span>finalModel)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="scatterplot" class="section level4 unnumbered">
<h4>Scatterplot</h4>
<p>Compare the predictions with the dataset (darker is more expensive) which seem to capture the global price trend. Note that this plot uses the <code>tree</code> model (instead of the <code>rpart2</code> model) because the optimization is different.</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1">tree.model &lt;-<span class="st"> </span>tree<span class="op">::</span><span class="kw">tree</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude, </a>
<a class="sourceLine" id="cb466-2" data-line-number="2">                         <span class="dt">data=</span>real.estate)</a>
<a class="sourceLine" id="cb466-3" data-line-number="3"></a>
<a class="sourceLine" id="cb466-4" data-line-number="4">price.deciles &lt;-<span class="st"> </span><span class="kw">quantile</span>(real.estate<span class="op">$</span>MedianHouseValue, <span class="dv">0</span><span class="op">:</span><span class="dv">10</span><span class="op">/</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb466-5" data-line-number="5">cut.prices    &lt;-<span class="st"> </span><span class="kw">cut</span>(real.estate<span class="op">$</span>MedianHouseValue, </a>
<a class="sourceLine" id="cb466-6" data-line-number="6">                     price.deciles, </a>
<a class="sourceLine" id="cb466-7" data-line-number="7">                     <span class="dt">include.lowest=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb466-8" data-line-number="8"><span class="kw">plot</span>(real.estate<span class="op">$</span>Longitude, </a>
<a class="sourceLine" id="cb466-9" data-line-number="9">     real.estate<span class="op">$</span>Latitude, </a>
<a class="sourceLine" id="cb466-10" data-line-number="10">     <span class="dt">col=</span><span class="kw">grey</span>(<span class="dv">10</span><span class="op">:</span><span class="dv">2</span><span class="op">/</span><span class="dv">11</span>)[cut.prices], </a>
<a class="sourceLine" id="cb466-11" data-line-number="11">     <span class="dt">pch=</span><span class="dv">20</span>, </a>
<a class="sourceLine" id="cb466-12" data-line-number="12">     <span class="dt">xlab=</span><span class="st">&quot;Longitude&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Latitude&quot;</span>)</a>
<a class="sourceLine" id="cb466-13" data-line-number="13"></a>
<a class="sourceLine" id="cb466-14" data-line-number="14">tree<span class="op">::</span><span class="kw">partition.tree</span>(tree.model, </a>
<a class="sourceLine" id="cb466-15" data-line-number="15">                     <span class="dt">ordvars=</span><span class="kw">c</span>(<span class="st">&quot;Longitude&quot;</span>,<span class="st">&quot;Latitude&quot;</span>), </a>
<a class="sourceLine" id="cb466-16" data-line-number="16">                     <span class="dt">add=</span><span class="ot">TRUE</span>) </a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-15-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="finer-partition" class="section level4 unnumbered">
<h4>Finer partition</h4>
<pre><code>12) Latitude&gt;=34.7 2844  645.0 11.5 </code></pre>
<p>the node that splits at latitude greater than 34.7 has 2844 houses. 645 is the “deviance” which is the sum of squares value for that node. the predicted value is the average of the points in that node: 11.5. it is not a terminal node (no asterisk).</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb468-2" data-line-number="2">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb468-3" data-line-number="3">tr.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>Longitude <span class="op">+</span><span class="st"> </span>Latitude, </a>
<a class="sourceLine" id="cb468-4" data-line-number="4">                         <span class="dt">data=</span>real.estate, </a>
<a class="sourceLine" id="cb468-5" data-line-number="5">                         <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>,</a>
<a class="sourceLine" id="cb468-6" data-line-number="6">                         <span class="dt">trControl =</span> fitControl, </a>
<a class="sourceLine" id="cb468-7" data-line-number="7">                         <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">5</span>))</a>
<a class="sourceLine" id="cb468-8" data-line-number="8"></a>
<a class="sourceLine" id="cb468-9" data-line-number="9">tr.house<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## n= 20640 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 20640 6685.26300 12.08488  
##    2) Latitude&gt;=38.485 2061  383.26410 11.59422  
##      4) Latitude&gt;=39.355 674   65.51082 11.31630 *
##      5) Latitude&lt; 39.355 1387  240.39580 11.72928 *
##    3) Latitude&lt; 38.485 18579 5750.77400 12.13931  
##      6) Longitude&gt;=-121.655 13941 4395.52000 12.05527  
##       12) Latitude&gt;=34.675 2844  645.27310 11.51018  
##         24) Longitude&gt;=-120.275 1460  212.47730 11.28145 *
##         25) Longitude&lt; -120.275 1384  275.83120 11.75148 *
##       13) Latitude&lt; 34.675 11097 2688.68000 12.19497  
##         26) Longitude&gt;=-118.315 8384 1823.33000 12.08687  
##           52) Longitude&gt;=-117.545 2839  691.79800 11.87672 *
##           53) Longitude&lt; -117.545 5545  941.96340 12.19446 *
##         27) Longitude&lt; -118.315 2713  464.62720 12.52902 *
##      7) Longitude&lt; -121.655 4638  960.79250 12.39194  
##       14) Latitude&gt;=37.925 1063  177.59430 12.09533 *
##       15) Latitude&lt; 37.925 3575  661.87260 12.48013 *</code></pre>
</div>
<div id="more-variables" class="section level4 unnumbered">
<h4>More variables</h4>
<p>Including all the variables, not only the latitude and longitude:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb470-2" data-line-number="2">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>)</a>
<a class="sourceLine" id="cb470-3" data-line-number="3">tr.full.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb470-4" data-line-number="4">                              <span class="dt">data=</span>real.estate, </a>
<a class="sourceLine" id="cb470-5" data-line-number="5">                              <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>, </a>
<a class="sourceLine" id="cb470-6" data-line-number="6">                              <span class="dt">trControl =</span> fitControl, </a>
<a class="sourceLine" id="cb470-7" data-line-number="7">                              <span class="dt">tuneGrid=</span> <span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">5</span>))</a>
<a class="sourceLine" id="cb470-8" data-line-number="8"></a>
<a class="sourceLine" id="cb470-9" data-line-number="9">tr.full.house<span class="op">$</span>finalModel</a></code></pre></div>
<pre><code>## n= 20640 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 20640 6685.26300 12.08488  
##    2) MedianIncome&lt; 3.5471 10381 2662.31300 11.77174  
##      4) MedianIncome&lt; 2.51025 4842 1193.71700 11.57572  
##        8) Latitude&gt;=34.465 2520  557.77450 11.38771  
##         16) Longitude&gt;=-120.275 728   77.14396 11.08365 *
##         17) Longitude&lt; -120.275 1792  385.97890 11.51124  
##           34) Latitude&gt;=37.905 1103  150.31490 11.35795 *
##           35) Latitude&lt; 37.905 689  168.25420 11.75664 *
##        9) Latitude&lt; 34.465 2322  450.19880 11.77976  
##         18) Longitude&gt;=-117.775 878  144.15330 11.52580 *
##         19) Longitude&lt; -117.775 1444  214.98520 11.93418 *
##      5) MedianIncome&gt;=2.51025 5539 1119.89800 11.94310  
##       10) Latitude&gt;=37.925 1104  123.65980 11.68124 *
##       11) Latitude&lt; 37.925 4435  901.69050 12.00829  
##         22) Longitude&gt;=-122.235 4084  770.65270 11.96811  
##           44) Latitude&gt;=34.455 1270  284.66500 11.76617 *
##           45) Latitude&lt; 34.455 2814  410.82510 12.05924 *
##         23) Longitude&lt; -122.235 351   47.73002 12.47579 *
##    3) MedianIncome&gt;=3.5471 10259 1974.99300 12.40175  
##      6) MedianIncome&lt; 5.5892 7265 1156.09500 12.25720  
##       12) MedianHouseAge&lt; 38.5 5907  858.59850 12.20694 *
##       13) MedianHouseAge&gt;=38.5 1358  217.69860 12.47578 *
##      7) MedianIncome&gt;=5.5892 2994  298.73550 12.75251  
##       14) MedianIncome&lt; 7.393 2008  176.41530 12.64297 *
##       15) MedianIncome&gt;=7.393 986   49.16749 12.97557 *</code></pre>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1">rpart.plot<span class="op">::</span><span class="kw">rpart.plot</span>(tr.full.house<span class="op">$</span>finalModel)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="cross-validation-model-building" class="section level4 unnumbered">
<h4>Cross Validation (model building!)</h4>
<p>Turns out that the tree does “better” by being more complex – why is that? The tree with 14 nodes corresponds to the tree with the highest accuracy / lowest deviance.</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb473-1" data-line-number="1"><span class="co"># here, let&#39;s use all the variables and all the samples</span></a>
<a class="sourceLine" id="cb473-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb473-3" data-line-number="3">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>)</a>
<a class="sourceLine" id="cb473-4" data-line-number="4">tree.cv.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb473-5" data-line-number="5">                              <span class="dt">data=</span>real.estate, </a>
<a class="sourceLine" id="cb473-6" data-line-number="6">                              <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>,</a>
<a class="sourceLine" id="cb473-7" data-line-number="7">                              <span class="dt">trControl=</span>fitControl,</a>
<a class="sourceLine" id="cb473-8" data-line-number="8">                              <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span>),</a>
<a class="sourceLine" id="cb473-9" data-line-number="9">                              <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;gini&quot;</span>))</a>
<a class="sourceLine" id="cb473-10" data-line-number="10">  </a>
<a class="sourceLine" id="cb473-11" data-line-number="11">tree.cv.house  </a></code></pre></div>
<pre><code>## CART 
## 
## 20640 samples
##     8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 18576, 18576, 18576, 18575, 18576, 18576, ... 
## Resampling results across tuning parameters:
## 
##   maxdepth  RMSE       Rsquared   MAE      
##    1        0.4748682  0.3041572  0.3848606
##    2        0.4478756  0.3809354  0.3563586
##    3        0.4282733  0.4340116  0.3393296
##    4        0.4178448  0.4611563  0.3296215
##    5        0.4054431  0.4924175  0.3184901
##    6        0.3962472  0.5155365  0.3103266
##    7        0.3948428  0.5189584  0.3092563
##    8        0.3935306  0.5221099  0.3080369
##    9        0.3891254  0.5326804  0.3044392
##   10        0.3836652  0.5456808  0.3000226
##   11        0.3786873  0.5574177  0.2956848
##   12        0.3739131  0.5685161  0.2907504
##   13        0.3712711  0.5746216  0.2868830
##   14        0.3703641  0.5767271  0.2858720
##   15        0.3703641  0.5767271  0.2858720
##   16        0.3703641  0.5767271  0.2858720
##   17        0.3703641  0.5767271  0.2858720
##   18        0.3703641  0.5767271  0.2858720
##   19        0.3703641  0.5767271  0.2858720
##   20        0.3703641  0.5767271  0.2858720
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was maxdepth = 14.</code></pre>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb475-1" data-line-number="1">rpart.plot<span class="op">::</span><span class="kw">rpart.plot</span>(tree.cv.house<span class="op">$</span>finalModel)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-18-1.png" width="480" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1"><span class="kw">plot</span>(tree.cv.house)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-18-2.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="training-test-data-for-model-building-and-model-accuracy" class="section level4 unnumbered">
<h4>Training / test data for model building AND model accuracy</h4>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb477-1" data-line-number="1"><span class="co"># first create two datasets: one training, one test</span></a>
<a class="sourceLine" id="cb477-2" data-line-number="2">inTrain &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> real.estate<span class="op">$</span>MedianHouseValue, </a>
<a class="sourceLine" id="cb477-3" data-line-number="3">                                      <span class="dt">p=</span>.<span class="dv">8</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb477-4" data-line-number="4">house.train &lt;-<span class="st"> </span>real.estate[inTrain,]</a>
<a class="sourceLine" id="cb477-5" data-line-number="5">house.test &lt;-<span class="st"> </span>real.estate[<span class="op">-</span><span class="kw">c</span>(inTrain),]</a>
<a class="sourceLine" id="cb477-6" data-line-number="6"></a>
<a class="sourceLine" id="cb477-7" data-line-number="7"></a>
<a class="sourceLine" id="cb477-8" data-line-number="8"><span class="co"># then use CV on the training data to find the best maxdepth</span></a>
<a class="sourceLine" id="cb477-9" data-line-number="9"><span class="kw">set.seed</span>(<span class="dv">4747</span>)</a>
<a class="sourceLine" id="cb477-10" data-line-number="10">fitControl &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>)</a>
<a class="sourceLine" id="cb477-11" data-line-number="11">tree.cvtrain.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb477-12" data-line-number="12">                                   <span class="dt">data=</span>house.train, </a>
<a class="sourceLine" id="cb477-13" data-line-number="13">                                   <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>,</a>
<a class="sourceLine" id="cb477-14" data-line-number="14">                                   <span class="dt">trControl=</span>fitControl, </a>
<a class="sourceLine" id="cb477-15" data-line-number="15">                                   <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">1</span><span class="op">:</span><span class="dv">20</span>),</a>
<a class="sourceLine" id="cb477-16" data-line-number="16">                                   <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;gini&quot;</span>))</a>
<a class="sourceLine" id="cb477-17" data-line-number="17"></a>
<a class="sourceLine" id="cb477-18" data-line-number="18"></a>
<a class="sourceLine" id="cb477-19" data-line-number="19">tree.cvtrain.house</a></code></pre></div>
<pre><code>## CART 
## 
## 16513 samples
##     8 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 14862, 14862, 14862, 14862, 14862, 14861, ... 
## Resampling results across tuning parameters:
## 
##   maxdepth  RMSE       Rsquared   MAE      
##    1        0.4756049  0.2994314  0.3851775
##    2        0.4497958  0.3734197  0.3581148
##    3        0.4289837  0.4302083  0.3396992
##    4        0.4192079  0.4558965  0.3304520
##    5        0.4026435  0.4979229  0.3153937
##    6        0.3960649  0.5141665  0.3102682
##    7        0.3960649  0.5141665  0.3102682
##    8        0.3924388  0.5229961  0.3072433
##    9        0.3875832  0.5347306  0.3027262
##   10        0.3830783  0.5454004  0.2981715
##   11        0.3783297  0.5566766  0.2941443
##   12        0.3724797  0.5702362  0.2883089
##   13        0.3694837  0.5770987  0.2850918
##   14        0.3694837  0.5770987  0.2850918
##   15        0.3694837  0.5770987  0.2850918
##   16        0.3694837  0.5770987  0.2850918
##   17        0.3694837  0.5770987  0.2850918
##   18        0.3694837  0.5770987  0.2850918
##   19        0.3694837  0.5770987  0.2850918
##   20        0.3694837  0.5770987  0.2850918
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was maxdepth = 13.</code></pre>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb479-1" data-line-number="1">tree.train.house &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw">train</span>(<span class="kw">log</span>(MedianHouseValue) <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb479-2" data-line-number="2">                                 <span class="dt">data=</span>house.train, </a>
<a class="sourceLine" id="cb479-3" data-line-number="3">                                 <span class="dt">method=</span><span class="st">&quot;rpart2&quot;</span>,</a>
<a class="sourceLine" id="cb479-4" data-line-number="4">                                 <span class="dt">trControl=</span>caret<span class="op">::</span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;none&quot;</span>),</a>
<a class="sourceLine" id="cb479-5" data-line-number="5">                                 <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">maxdepth=</span><span class="dv">14</span>),</a>
<a class="sourceLine" id="cb479-6" data-line-number="6">                                 <span class="dt">parms=</span><span class="kw">list</span>(<span class="dt">split=</span><span class="st">&quot;gini&quot;</span>))</a></code></pre></div>
<p><strong>for classification results</strong> use <code>confusionMatrix</code> instead of <code>postResample</code></p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">test.pred &lt;-<span class="st"> </span><span class="kw">predict</span>(tree.train.house, house.test)</a>
<a class="sourceLine" id="cb480-2" data-line-number="2">caret<span class="op">::</span><span class="kw">postResample</span>(<span class="dt">pred =</span> test.pred, <span class="dt">obs=</span><span class="kw">log</span>(house.test<span class="op">$</span>MedianHouseValue))</a></code></pre></div>
<pre><code>##      RMSE  Rsquared       MAE 
## 0.3696649 0.5840583 0.2846395</code></pre>
</div>
<div id="other-tree-r-packages" class="section level4 unnumbered">
<h4>Other tree R packages</h4>
<ul>
<li><p><code>rpart</code> is faster than <code>tree</code></p></li>
<li><p><code>party</code> gives great plotting options</p></li>
<li><p><code>maptree</code> also gives trees from hierarchical clustering</p></li>
<li><p><code>randomForest</code> up next!</p></li>
</ul>
<p>Reference: slides built from <a href="http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf" class="uri">http://www.stat.cmu.edu/~cshalizi/350/lectures/22/lecture-22.pdf</a></p>
</div>
</div>
</div>
<div id="Nov5" class="section level2">
<h2><span class="header-section-number">8.6</span> 11/5/19 Agenda</h2>
<ol style="list-style-type: decimal">
<li>pruning</li>
<li>variable selection</li>
<li>bagging (no boosting)</li>
<li>OOB error rate</li>
</ol>
</div>
<div id="bagging" class="section level2">
<h2><span class="header-section-number">8.7</span> Bagging</h2>
<p>The tree based models given by CART are easy to understand and implement, but they suffer from <em>high variance</em>. That is, if we split the training data into two parts at random and fit a decision tree to both halves, the results that we get could be quite different (you might have seen this in your homework assignment!). We’d like a model that produces low variance - one for which if we ran it on different datasets, we’d get (close to) the same model every time.</p>
<p><strong>Bagging = Bootstrap Aggregating</strong>. The idea is that sometimes when you fit multiple models and aggregate those models together, you get a smoother model fit which will give you a better balance between bias in your fit and variance in your fit. Bagging can be applied to any classifier to reduce variability.</p>
<p style="color:red">
Recall that the variance of the sample mean is variance / n. So we’ve seen the idea that averaging an outcome gives reduced variability.
</p>
<p><strong>Shortcomings of Bagging:</strong></p>
<ul>
<li>Model is even harder to “write-down” (than CART)</li>
<li>With lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! (because of the number of trees grown for each bootstrap sample)</li>
</ul>
<p><strong>Strengths of Bagging:</strong></p>
<ul>
<li>Can handle categorical or numerical predictors or response variables (indeed, they can handle mixed predictors at the same time!).</li>
<li>Can handle more than 2 groups for categorical predictions</li>
<li>Easily ignore redundant variables.</li>
<li>Perform better than linear models in non-linear settings. Classification trees are non-linear models, so they immediately use interactions between variables.</li>
<li>Data transformations may be less important (monotone transformations on the explanatory variables won’t change anything).
<p style="color:red">
<em>Similar bias</em> to CART, but <em>reduced variance</em>
</p>
(can be proved).</li>
</ul>
<p>Basic Idea:</p>
<ul>
<li>Resample <em>cases</em> (observational units, not variables) and recalculate predictions. Choose <span class="math inline">\(N&#39; \leq N\)</span> for the number of observations in the new training set through random sampling with replacement. Almost always we use <span class="math inline">\(N&#39; = N\)</span> for a full bootstrap.</li>
<li>Build a tree on each new set of <span class="math inline">\(N&#39;\)</span> training observations.</li>
<li>Average (regression) or majority vote (classification).</li>
<li>Note that for every bootstrap sample, approximately 2/3 of the observations will be chosen and 1/3 of them will not be chosen.
<span class="math display">\[\begin{align}
P(\mbox{observation $n$ is not in the bootstrap sample}) &amp;= \bigg(1 - \frac{1}{n} \bigg)^n\\
\lim_{n \rightarrow \infty} \bigg(1 - \frac{1}{n} \bigg)^n = \frac{1}{e} \approx \frac{1}{3}
\end{align}\]</span></li>
</ul>
<p><strong>Notes on bagging:</strong></p>
<ul>
<li>Bagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped).</li>
<li>Note that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand.</li>
<li>Bagging gives a smoother decision boundary</li>
<li>Bagging can be done on <em>any</em> decision method (not just trees).</li>
<li>No need to prune or CV trees. The reason is that averaging keeps us from overfitting a particular few observations (think of averages in other contexts: law of large numbers). Pruning wouldn’t be a bad thing to do in terms of fit, but it is unnecessary for good predictions (and would add a lot to the complexity of the algorithm).</li>
</ul>
<div id="out-of-bag-oob-error-rate" class="section level3">
<h3><span class="header-section-number">8.7.1</span> Out Of Bag (OOB) error rate</h3>
<p>Additionally, with bagging, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error. It is estimated internally, during the run, as follows:</p>
<ul>
<li>Each tree is constructed using a different bootstrap sample from the original data. About one-third of the cases are left out of the bootstrap sample and not used in the construction of the kth tree.</li>
<li>Put each case left out in the construction of the kth tree down the kth tree to get a classification. In this way, a test set classification is obtained for each case in about one-third of the trees. At the end of the run, take j to be the class that got most of the votes every time case n was oob. The proportion of times that j is not equal to the true class of n averaged over all cases is the oob error estimate. This has proven to be unbiased in many tests.</li>
</ul>
<p>How does it work? Consider the following predictions for a silly toy data set of 9 observations. Recall that <span class="math inline">\(\sim 1/3\)</span> of the observations will be left out at each bootstrap sample. Those are the observations for which predictions will be made. In the table below, an X is given if there is a prediction made for that value.</p>
<table>
<thead>
<tr class="header">
<th align="center">obs</th>
<th align="center">tree1</th>
<th align="center">tree2</th>
<th align="center">tree3</th>
<th align="center">tree4</th>
<th align="center"><span class="math inline">\(\cdots\)</span></th>
<th align="center">tree100</th>
<th align="center">average</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center"></td>
<td align="center">X</td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\sum(pred)/2\)</span></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\sum(pred)/1\)</span></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"><span class="math inline">\(\sum(pred)/2\)</span></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\sum(pred)/1\)</span></td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">$(pred)/1 $</td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"><span class="math inline">\(\sum(pred)/2\)</span></td>
</tr>
<tr class="odd">
<td align="center">7</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"><span class="math inline">\(\sum(pred)/1\)</span></td>
</tr>
<tr class="even">
<td align="center">8</td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center">X</td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"><span class="math inline">\(\sum(pred)/3\)</span></td>
</tr>
<tr class="odd">
<td align="center">9</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">X</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\sum(pred)/1\)</span></td>
</tr>
</tbody>
</table>
<p>Let the OOB prediction for the <span class="math inline">\(i^{th}\)</span> observation to be <span class="math inline">\(\hat{y}_{(-i)}\)</span></p>
<p><span class="math display">\[\begin{align}
\mbox{OOB}_{\mbox{error}} &amp;= \frac{1}{n} \sum_{i=1}^n \textrm{I} (y_i \ne \hat{y}_{(-i)}) \ \ \ \ \ \ \ \  \mbox{classification}\\
\mbox{OOB}_{\mbox{error}} &amp;= \frac{1}{n} \sum_{i=1}^n  (y_i - \hat{y}_{(-i)})^2  \ \ \ \ \ \ \ \ \mbox{regression}\\
\end{align}\]</span></p>
</div>
</div>
<div id="Nov7" class="section level2">
<h2><span class="header-section-number">8.8</span> 11/7/19 Agenda</h2>
<ol style="list-style-type: decimal">
<li>OOB again</li>
<li>Random Forests</li>
<li>variable importance</li>
<li>R code / examples</li>
</ol>
</div>
<div id="rf" class="section level2">
<h2><span class="header-section-number">8.9</span> Random Forests</h2>
<p>Random Forests are an extension to bagging for regression trees (note: bagging can be done on any prediction method). Again, with the idea of infusing extra variability and then averaging over that variability, RFs use a <em>subset</em> of predictor variables at every node in the tree.</p>
<p><strong>Shortcomings of Random Forests:</strong>
* Model is even harder to “write-down” (than CART)
* With lots of predictors, (even greedy) partitioning can become computationally unwieldy - now computational task is even harder! … bagging the observations and</p>
<p><strong>Strengths of Random Forests:</strong>
* refinement of bagged trees; quite popular (random forests tries to improve on bagging by “de-correlating” the trees. Each tree has the same expectation, but the average will again reduce the variability.)
* subset of predictors makes random forests <em>much faster</em> to search through than all predictors
* creates a diverse set of trees that can be built. Note that by bootstrapping the samples and the predictor variables, we add another level of randomness over which we can average to again decrease the variability.
* random forests are quite accurate
* generally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (<span class="math inline">\(m\)</span>, node size, max number of nodes, etc.).</p>
<blockquote>
<p>“Random forests does not overfit. You can run as many trees as you want.” Brieman, <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" class="uri">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</blockquote>
<div id="random-forest-algorithm" class="section level3">
<h3><span class="header-section-number">8.9.1</span> Random Forest algorithm</h3>
<hr />
<p><strong>Algorithm</strong>: Random Forest</p>
<hr />
<ol style="list-style-type: decimal">
<li>Bootstrap sample from the training set.</li>
<li>Grow an un-pruned tree on this bootstrap sample.</li>
</ol>
<ul>
<li><em>At each split</em>, select <span class="math inline">\(m\)</span> variables and determine the best split using only these predictors.
Typically <span class="math inline">\(m = \sqrt{p}\)</span> or <span class="math inline">\(\log_2 p\)</span>, where <span class="math inline">\(p\)</span> is the number of features. Random forests are not overly sensitive to the value of <span class="math inline">\(m\)</span>. [splits are chosen as with trees: according to either squared error or gini index / cross entropy / classification error.]</li>
<li>Do <em>not</em> prune the tree. Save the tree as is!</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>For each tree grown on a bootstrap sample, predict the OOB samples. For each tree grown, <span class="math inline">\(~1/3\)</span> of the training samples won’t be in the bootstrap sample – those are called out of bootstrap (OOB) samples. OOB samples can be used as <em>test</em> data to estimate the error rate of the tree.</li>
<li>Combine the OOB predictions to create the “out-of-bag” error rate (either majority vote or average of predictions / class probabilities).</li>
<li>All trees together represent the model that is used for new predictions (either majority vote or average).</li>
</ol>
<hr />

<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-21"></span>
<img src="figs/zissermanRF.jpg" alt="Building multiple trees and then combining the outputs (predictions).  Note that this image makes the choice to average the tree probabilities instead of using majority vote.  Both are valid methods for creating a random forest prediction model.  http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf" width="100%" />
<p class="caption">
Figure 8.1: Building multiple trees and then combining the outputs (predictions). Note that this image makes the choice to average the tree probabilities instead of using majority vote. Both are valid methods for creating a random forest prediction model. <a href="http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf" class="uri">http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf</a>
</p>
</div>
<p><strong>Notes on random forests:</strong></p>
<ul>
<li>Bagging alone uses the full set of predictors to determine every tree (it is the observations that are bootstrapped). Random Forests use a subset of predictors.</li>
<li>Note that to predict for a particular observation, we start at the top, walk down the tree, and get the prediction. We average (or majority vote) the predictions to get one prediction for the observation at hand.</li>
<li>Bagging is a special case of random forest where <span class="math inline">\(m=k\)</span>.</li>
<li>generally, models do not overfit the data and CV is not needed. However, CV can be used to fit the tuning parameters (<span class="math inline">\(m\)</span>, node size, max number of nodes, etc.).</li>
</ul>
<blockquote>
<p>“Random forests does not overfit. You can run as many trees as you want.” Brieman, <a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" class="uri">http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</blockquote>
<div id="how-to-choose-parameters" class="section level4 unnumbered">
<h4>How to choose parameters?</h4>
<ul>
<li><strong><span class="math inline">\(\#\)</span> trees</strong>
Build trees until the error no longer decreases</li>
<li><strong><span class="math inline">\(m\)</span></strong>
Try the recommended defaults, half of them, and twice of them - pick the best (use CV to avoid overfitting).</li>
<li><strong><span class="math inline">\(N&#39;\)</span> samples</strong>
<span class="math inline">\(N&#39;\)</span> should be the same size as the training data.</li>
</ul>
</div>
<div id="variable-importance" class="section level4 unnumbered">
<h4>Variable Importance</h4>
<p>All learners are bad when there are too many noisy variables because the response is bound to correlate with some of them. We can measure the contribution of each additional variable in the model by how much the model accuracy decreased when the given variable was <em>excluded</em> from the model.</p>
<blockquote>
<p>importance = decrease in node impurity resulting from splits over that variable, averaged over all trees</p>
</blockquote>
<p>(“impurity” is defined as RSS for regression trees and deviance for classification trees).</p>
<p><strong>Variable importance</strong> is measured by two different metrics (from R help on <code>importance</code>):</p>
<ul>
<li>(permutation) <strong>accuracy:</strong> For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Within the oob values, permute the <span class="math inline">\(j^{th}\)</span> variable and recalculate the prediction error. The difference between the two are then averaged over all trees (with the <span class="math inline">\(j^{th}\)</span> variable) to give the importance for the <span class="math inline">\(j^{th}\)</span> variable.</li>
<li><strong>purity:</strong> The decrease (or increase, depending on the plot) in node purity: root sum of squares (RSS) [deviance/gini for classification trees]. That is, the amount of total decrease in RSS from splitting on <em>that</em> variable, averaged over all trees.</li>
</ul>
<p>If the number of variables is very large, forests can be run once with all the variables, then run again using only the most important variables from the first run.</p>
</div>
</div>
<div id="r-rf-example" class="section level3">
<h3><span class="header-section-number">8.9.2</span> R RF Example</h3>
<p>(“impurity” is defined as RSS for regression trees and deviance for classification trees). In R see <code>importance</code> within the <code>randomForest</code> function, and then <code>varImpPlot</code> to plot.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" data-line-number="1"><span class="kw">data</span>(iris)</a>
<a class="sourceLine" id="cb482-2" data-line-number="2"><span class="kw">library</span>(tidyverse)</a>
<a class="sourceLine" id="cb482-3" data-line-number="3"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb482-4" data-line-number="4"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb482-5" data-line-number="5"></a>
<a class="sourceLine" id="cb482-6" data-line-number="6">inTrain &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y =</span> iris<span class="op">$</span>Species, <span class="dt">p=</span><span class="fl">0.7</span>, <span class="dt">list=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb482-7" data-line-number="7">training &lt;-<span class="st"> </span>iris[inTrain,]</a>
<a class="sourceLine" id="cb482-8" data-line-number="8">testing &lt;-<span class="st"> </span>iris[<span class="op">-</span>inTrain,]</a></code></pre></div>
<p><code>prox=TRUE</code> gives us a little more extra information in the output</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb483-1" data-line-number="1">modFit &lt;-<span class="st"> </span><span class="kw">train</span>(Species <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb483-2" data-line-number="2">                <span class="dt">data=</span>training, </a>
<a class="sourceLine" id="cb483-3" data-line-number="3">                <span class="dt">method=</span><span class="st">&quot;rf&quot;</span>, </a>
<a class="sourceLine" id="cb483-4" data-line-number="4">                <span class="dt">prox=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb483-5" data-line-number="5">modFit</a></code></pre></div>
<pre><code>## Random Forest 
## 
## 105 samples
##   4 predictor
##   3 classes: &#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39; 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 105, 105, 105, 105, 105, 105, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   2     0.9518527  0.9265992
##   3     0.9508527  0.9250337
##   4     0.9489015  0.9221052
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 2.</code></pre>
<p>look at a specific tree</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" data-line-number="1"><span class="kw">getTree</span>(modFit<span class="op">$</span>finalModel, <span class="dt">k=</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>##   left daughter right daughter split var split point status prediction
## 1             2              3         4        0.80      1          0
## 2             0              0         0        0.00     -1          1
## 3             4              5         4        1.65      1          0
## 4             0              0         0        0.00     -1          2
## 5             6              7         4        1.75      1          0
## 6             8              9         2        2.75      1          0
## 7             0              0         0        0.00     -1          3
## 8             0              0         0        0.00     -1          3
## 9             0              0         0        0.00     -1          2</code></pre>
<p>can get class “centers”</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" data-line-number="1">irisP &lt;-<span class="st"> </span><span class="kw">classCenter</span>(training[,<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">4</span>)], </a>
<a class="sourceLine" id="cb487-2" data-line-number="2">                     training<span class="op">$</span>Species, </a>
<a class="sourceLine" id="cb487-3" data-line-number="3">                     modFit<span class="op">$</span>finalModel<span class="op">$</span>prox)</a>
<a class="sourceLine" id="cb487-4" data-line-number="4">irisP &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(irisP)</a>
<a class="sourceLine" id="cb487-5" data-line-number="5">irisP<span class="op">$</span>Species &lt;-<span class="st"> </span><span class="kw">rownames</span>(irisP)</a>
<a class="sourceLine" id="cb487-6" data-line-number="6"></a>
<a class="sourceLine" id="cb487-7" data-line-number="7"><span class="kw">ggplot</span>(training, <span class="kw">aes</span>(<span class="dt">x=</span>Petal.Width, <span class="dt">y=</span>Petal.Length, <span class="dt">col=</span>Species)) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb487-8" data-line-number="8"><span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">size=</span><span class="dv">5</span>, <span class="dt">shape=</span><span class="dv">4</span>)</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-25-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>testing predictions</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(modFit, testing)</a>
<a class="sourceLine" id="cb488-2" data-line-number="2">testing<span class="op">$</span>predRight &lt;-<span class="st"> </span>pred <span class="op">==</span><span class="st"> </span>testing<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb488-3" data-line-number="3"><span class="kw">table</span>(pred, testing<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>##             
## pred         setosa versicolor virginica
##   setosa         15          0         0
##   versicolor      0         14         2
##   virginica       0          1        13</code></pre>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="kw">ggplot</span>(testing, <span class="kw">aes</span>(<span class="dt">x=</span>Petal.Width, <span class="dt">y=</span>Petal.Length, <span class="dt">color=</span>predRight) ) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb490-2" data-line-number="2"><span class="st">    </span><span class="kw">geom_point</span>()</a></code></pre></div>
<p><img src="08-classification_files/figure-html/unnamed-chunk-26-1.png" width="480" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" data-line-number="1"><span class="co"># need a main label &quot;new data Predictions&quot;</span></a></code></pre></div>
</div>
</div>
<div id="model-choices" class="section level2">
<h2><span class="header-section-number">8.10</span> Model Choices</h2>
<p>There are <em>soooooo</em> many choices we’ve made along the way. The following list should make you realize that there is no <strong>truth</strong> with respect to any given model. Every choice will (could) lead to a different model.</p>
<table>
<thead>
<tr class="header">
<th>* explanatory variable choice</th>
<th>* k (kNN)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>* number of explanatory variables</td>
<td>* distance measure</td>
</tr>
<tr class="even">
<td>* functions/transformation of explanatory</td>
<td>* k (CV)</td>
</tr>
<tr class="odd">
<td>* transformation of response</td>
<td>* CV set.seed</td>
</tr>
<tr class="even">
<td>* response:continuous vs. categorical</td>
<td>* alpha prune</td>
</tr>
<tr class="odd">
<td>* how missing data is dealt with</td>
<td>* maxdepth prune</td>
</tr>
<tr class="even">
<td>* train/test split (set.seed)</td>
<td>* prune or not</td>
</tr>
<tr class="odd">
<td>* train/test proportion</td>
<td>* gini / entropy (split)</td>
</tr>
<tr class="even">
<td>* type of classification model</td>
<td>* # trees / # BS samples</td>
</tr>
<tr class="odd">
<td>* use of cost complexity / parameter</td>
<td>* grid search etc. for tuning</td>
</tr>
<tr class="even">
<td>* majority / average prob (tree error rate)</td>
<td>* value(s) of mtry</td>
</tr>
<tr class="odd">
<td>* accuracy vs sensitivity vs specificity</td>
<td>* OOB vs CV for tuning</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Baumer15">
<p>Baumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” <em>The American Statistician</em>.</p>
</div>
<div id="ref-brei01">
<p>Breiman, L. 2001. “Statistical Modeling: The Two Cultures.” <em>Statistical Science</em> 16 (3): 199–215.</p>
</div>
<div id="ref-field07">
<p>Fielding, Alan. 2007. <em>Cluster and Classification Techniques for the Biosciences</em>. Cambridge.</p>
</div>
<div id="ref-flach12">
<p>Flach, P. 2012. <em>Machine Learning</em>. Cambridge University Press.</p>
</div>
<div id="ref-ESL">
<p>Hastie, T., R. Tibshirani, and J. Friedman. 2001. <em>The Elements of Statistical Learning</em>. Springer.</p>
</div>
<div id="ref-ISL">
<p>James, Witten, Hastie, and Tibshirani. 2013. <em>An Introduction to Statistical Learning</em>. Springer. <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">http://faculty.marshall.usc.edu/gareth-james/ISL/</a>.</p>
</div>
<div id="ref-CVpaper">
<p>Varoquaux, G., P. Reddy Raamana, D. Engemann, A. Hoyos-Idrobo, Y. Schwartz, and B. Thirion. 2017. “Assessing and Tuning Brain Decoders: Cross-Validation, Caveats, and Guidelines.” <em>NeuroImage</em> 145: 166–79.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ethics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/08-classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Math-154-Notes.pdf", "Math-154-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
