[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 158 Pomona College: Computational Statistics. notes based extensively Introduction Statistical Learning (James et al. 2021); Applied Linear Statistical Models (Kutner et al. 2004). computing part class taken R Data Science (Wickham Grolemund 2017) Wickham Grolemund well Tidy Modeling R (Kuhn Silge 2021) Kuhn Silge.responsible reading relevant chapters texts. texts good & readable, use . make sure coming class also reading materials associated activities.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims populationWhat content Math 158?\nMath 158 course statistical linear models.goal Math 158 understand modeling linear statistical relationships explanatory / predictor (X) variables response (Y) variables.models grow sophistication semester including multiple linear regression, interaction terms, ridge regression, Lasso, smoothing.Throughout semester, continue talk good modeling practices, ideas extend beyond linear models types inference prediction.think carefully inferential modeling makes sense predictive modeling makes sense. neither type analysis done!math class go quickly, little calculus. , however, learn linear algebra.necessarily use linear models determine causation (need experimental design course discuss issues observational study vs. experiment). E.g., (1) thermostat versus actual temperature, (2) ice cream sales versus boating accidents.take Math 158?\nLinear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.prerequisites Math 158?\nLinear Models requires strong background statistics well algorithmic thinking. formal prerequisite introductory statistics course, AP Statistics, may find working hard first weeks class catch . taken lot mathematics, parts course come easily . However, mathematics degree substitute introductory statistics, taken introductory statistics, majority course work intuitive . must taken prior statistics course pre-requisite Math 158; computer science course helpful.Many derivations much notation course come linear algebra. taken linear algebra enormously helpful class, cover enough notation need linear algebra.worth noting concepts probability theory represent majority mathematical tools used statistics / modeling, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.overlap classes?\nStatistical Linear Models overlaps Econometrics (Econ 167) Applied Econometrics (107). Econometrics focuses probability theory matrix algebra (mathematics) lead derivation linear models. Applied Econometrics focuses tools analysis. Statistical Linear Models focuses use model, assumptions made, conclusions appropriate given results. Additionally, later topics Linear Models typically covered Econometrics.take Math 158?\nprerequisite Linear Models introduction statistics, course moves quickly covers tremendous amount material. ideally suited first year student coming straight AP Statistics. Instead, student focus taking mathematics, CS, interdisciplinary science, statistics courses. students taking Linear Models sophomores juniors.workload Math 158?\none homework assignment per week, two -class midterm exams, two take-home midterm exams, final end semester project. Many students report working 8-10 hours per week outside class.software use? real world applications? mathematics? CS?\nwork done R (using RStudio front end, called integrated development environment, IDE). need either download R RStudio (free) onto computer use Pomona’s server. assignments posted private repositories GitHub. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.assignments turned using R Markdown compiled pdf + pushed GitHub\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Linear Models ubiquitous incredibly powerful. Indeed, often times linear models appropriate (e.g., violating technical conditions) yet end giving almost identical solutions models appropriate. solid understanding linear model framework, however, requires strong foundation theory goes inferential thinking well predictive modeling. review ideas inference introductory statistics.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]","code":""},{"path":"intro.html","id":"simple-linear-regression","chapter":"1 Introduction","heading":"1.2.2 Simple Linear Regression","text":"simplest case, study first, suppose two variables. call one explanatory variable, response variable.Explanatory / Predictor variable: also known independent variable, numeric variable often known advance variable, thought possibly influence value.Response / outcome variable: also known dependent variable, also numeric, thought function predictor variable. n.b., don’t use word “dependent” don’t want send message ’ve measured anything causal model.goal ascertain relationship two. observe sample population interest, \\((x_i,y_i), =1,\\dots,n\\). \\(x_i\\) predictor, \\(y_i\\) response, \\(n\\) sample size.observe sample, actual population , best can hope estimate relationship two variables. resulting estimates give us idea relationship actually population. However, random quantities, depend random sample. , exact. theory hypothesis testing needed\ndetermine much can actually say population quantities, called parameters.Parameter: quantity describes population. Examples population mean, population standard deviation, say relationship two variables polynomial, coefficients function parameters.","code":""},{"path":"intro.html","id":"hypothesis-testing","chapter":"1 Introduction","heading":"1.2.3 Hypothesis Testing","text":"set : population big observe. ’d like know value specific parameters, say instance mean population. can’t however, calculate parameters directly.\nInstead, observe sample random population. Based sample, estimate parameters. However, estimated values exact. need technique uses estimated model say something population model.Null Hypothesis: Denoted \\(H_0\\), null hypothesis usually set believed unless evidence presented otherwise. specific, specifying parameter equal specific value.\n\\(H_0\\) true, theory tells us exactly estimate behaves.Alternative Hypothesis: Denoted \\(H_a\\), alternative hypothesis usually wish show true. general \\(H_0\\), usually form parameter somehow equal value used \\(H_0\\), without specifying exactly think value . result, don’t know estimate behaves, depends value parameter.","code":""},{"path":"intro.html","id":"what-really-is-an-alternative-hypothesis","chapter":"1 Introduction","heading":"What really is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEYIn video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?Since procedure potential wrong, search one makes bad errors infrequently. two types errors can made.Type error: Rejecting \\(H_0\\) \\(H_0\\) actually true. Usually considered worst error possible, thus find procedure makes type error small probability. probability denoted \\(\\alpha\\).Type II error: rejecting \\(H_0\\) \\(H_a\\) actually true. small type II error secondary concern (controlling type error). probability type II error denoted \\(\\beta\\). \\(1-\\beta\\)\nknown power.reality, true reason choose test small value \\(\\alpha\\) value know calculate. \\(\\beta\\) power possible calculate \\(H_a\\) doesn’t tell us value parameter .way hypothesis test carried via p-value, essentially tells us unusual observed data comparison \\(H_0\\). estimates consistent expected \\(H_0\\) true, conclusion \\(H_0\\) false: explanation data strange. definition p-value little tricky.p-value: probability, \\(H_0\\) true, observing data contradictory \\(H_0\\) repeat experiment .p-value .01, means data showed something happens 1 time 100 \\(H_0\\) true. Considering particular set data observed, reasonable conclusion \\(H_0\\) must \ntrue. rule : reject \\(H_0\\) p-value \\(< \\alpha\\). resulting test type error probability \\(\\alpha\\), value get specify. \\(\\alpha\\) often set .05.","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3 Reflection Questions","text":"difference sample population?experimental design issues influence conclusions?type error, type II error, power?p-value (careful, p-value probability \\(H_0\\) true!!!)?regression line ?linear regression always appropriate strategy?properties good fitting line ?line appropriately interpreted?","code":""},{"path":"intro.html","id":"r-reproduciblity","chapter":"1 Introduction","heading":"1.4 R: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.4.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.4.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.4.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"tools-git-github","chapter":"1 Introduction","heading":"Tools: Git & GitHub","text":"must submit assignments via GitHubFollow Jenny Bryan’s advice get set-: http://happygitwithr.com/Class specific instructions https://m158-comp-stats.netlify.app/github.htmlAdmittedly, steep learning curve Git. However, among tools likely use future endeavors, spending little time focusing concepts now may pay big time future. Beyond practicing working http://happygitwithr.com/, may want read little bit Git behind scenes. reference: Learn git concepts, commands good accessible.","code":""},{"path":"intro.html","id":"tools-a-github-merge-conflict-demo","chapter":"1 Introduction","heading":"Tools: a GitHub merge conflict (demo)","text":"GitHub (web) edit README document Commit message describing ., RStudio also edit README document different change.\nCommit changes\nTry push \\(\\rightarrow\\) ’ll get error!\nTry pulling\nResolve merge conflict commit push\nCommit changesTry push \\(\\rightarrow\\) ’ll get error!Try pullingResolve merge conflict commit pushAs work teams run merge conflicts, learning resolve properly important.\nFigure 1.7: https://xkcd.com/1597/\n","code":""},{"path":"intro.html","id":"steps-for-weekly-homework","chapter":"1 Introduction","heading":"Steps for weekly homework","text":"get link new assignment (clicking link create new private repo)Use R (within R Studio)\nNew Project, version control, Git\nClone repo using SSH\nNew Project, version control, GitClone repo using SSHIf exists, rename Rmd file ma158-hw#-lname-fname.RmdDo assignment\ncommit push every problem\ncommit push every problemAll necessary files must folder (e.g., data)","code":""},{"path":"wrang.html","id":"wrang","chapter":"2 Data Wrangling","heading":"2 Data Wrangling","text":"data visualization, data wrangling fundamental part able accurately, reproducibly, efficiently work data. approach taken following chapter based philosophy tidy data takes many precepts database theory. done much work SQL, functionality approach tidy data feel familiar. adept data wrangling, effective data analysis.Information want, data ’ve got. (Kaplan 2015)Embrace ways get help!cheat sheets: https://www.rstudio.com/resources/cheatsheets/tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/pivoting: https://tidyr.tidyverse.org/articles/pivot.htmlgoogle need include R tidy tidyverse","code":""},{"path":"wrang.html","id":"datastruc","chapter":"2 Data Wrangling","heading":"2.1 Structure of Data","text":"plotting, analyses, model building, etc., ’s important data structured particular way. Hadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy Data: rows (cases/observational units) columns (variables). key every row case *every} column variable. exceptions.Creating tidy data trivial. work objects (often data tables), functions, arguments (often variables).Active Duty data tidy! cases? data tidy? might data look like tidy form? Suppose case “individual armed forces.” variables use capture information following table?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794Problem: totals different sheetsBetter R: longer format columns - grade, gender, status, service, count (case still total pay grade)Case individual (?): grade, gender, status, service (count row counting)","code":""},{"path":"wrang.html","id":"building-tidy-data","chapter":"2 Data Wrangling","heading":"2.1.1 Building Tidy Data","text":"Within R (really within type computing language, Python, SQL, Java, etc.), need understand build data using patterns language. things consider:object_name = function_name(arguments) way using function create new object.object_name = data_table %>% function_name(arguments) uses chaining syntax extension ideas functions. chaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.\n* R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.\n* pipe syntax read , %>%.","code":"object_name = data_table %>%\nfunction_name(arguments) %>% \nfunction_name(arguments)"},{"path":"wrang.html","id":"examples-of-chaining","chapter":"2 Data Wrangling","heading":"2.1.2 Examples of Chaining","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)","code":""},{"path":"wrang.html","id":"little-bunny-foo-foo","chapter":"2 Data Wrangling","heading":"2.1.2.1 Little Bunny Foo Foo","text":"Hadley Wickham, think tidy data.Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThe nursery rhyme created series steps output step saved object along way.Another approach concatenate functions one output.even worse, one line:Instead, code can written using pipe order function evaluated:babynames year, US Social Security Administration publishes list popular names given babies. 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys.babynames data table babynames package comes Social Security Administration’s listing names givens babies year, number babies sex given name. (names 5 babies published SSA.)","code":"foo_foo <- little_bunny()\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)bop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))foo_foo %>%\n   hop(through = forest) %>%\n       scoop(up = field_mice) %>%\n           bop(on = head)"},{"path":"wrang.html","id":"data-verbs-on-single-data-frames","chapter":"2 Data Wrangling","heading":"2.1.3 Data Verbs (on single data frames)","text":"Super important resource: RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). use R package dplyr much data wrangling. list verbs helpful wrangling many different types data. See Data Wrangling cheat sheet RStudio additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfsample_n() take random row(s)sample_n() take random row(s)head() grab first rowshead() grab first rowstail() grab last rowstail() grab last rowsfilter() removes unwanted casesfilter() removes unwanted casesarrange() reorders casesarrange() reorders casesselect() removes unwanted variables (rename() )select() removes unwanted variables (rename() )distinct() returns unique values tabledistinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)mutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nsummarize() collapses data frame single row. functions used within summarize() include:min(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]","code":""},{"path":"wrang.html","id":"r-examples-basic-verbs","chapter":"2 Data Wrangling","heading":"2.2 R examples, basic verbs","text":"","code":""},{"path":"wrang.html","id":"datasets","chapter":"2 Data Wrangling","heading":"2.2.1 Datasets","text":"starwars dplyr , although originally SWAPI, Star Wars API, http://swapi.co/.NHANES ?NHANES: NHANES survey data collected US National Center Health Statistics (NCHS) conducted series health nutrition surveys since early 1960’s. Since 1999 approximately 5,000 individuals ages interviewed homes every year complete health examination component survey. health examination conducted mobile examination center (MEC).babynames year, US Social Security Administration publishes list popular names given babies. 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys. (names 5 babies published SSA.)","code":""},{"path":"wrang.html","id":"examples-of-chaining-1","chapter":"2 Data Wrangling","heading":"2.2.2 Examples of Chaining","text":"","code":"\nlibrary(babynames)\nbabynames %>% nrow()## [1] 1924665\nbabynames %>% names()## [1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\nbabynames %>% glimpse()## Rows: 1,924,665\n## Columns: 5\n## $ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n## $ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n## $ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n## $ n    <int> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n## $ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\nbabynames %>% head()## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\nbabynames %>% tail()## # A tibble: 6 × 5\n##    year sex   name       n       prop\n##   <dbl> <chr> <chr>  <int>      <dbl>\n## 1  2017 M     Zyhier     5 0.00000255\n## 2  2017 M     Zykai      5 0.00000255\n## 3  2017 M     Zykeem     5 0.00000255\n## 4  2017 M     Zylin      5 0.00000255\n## 5  2017 M     Zylis      5 0.00000255\n## 6  2017 M     Zyrie      5 0.00000255\nbabynames %>% sample_n(size=5)## # A tibble: 5 × 5\n##    year sex   name         n      prop\n##   <dbl> <chr> <chr>    <int>     <dbl>\n## 1  1946 M     Owen       392 0.000238 \n## 2  1953 M     Abelardo    34 0.0000170\n## 3  1996 F     Nicki       50 0.0000261\n## 4  1914 F     Sofia       38 0.0000477\n## 5  1970 M     Lorenza     45 0.0000236\nbabynames %>% mosaic::favstats(n ~ sex, data = .)##   sex min Q1 median Q3   max     mean       sd       n missing\n## 1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n## 2   M   5  7     12 33 94756 223.4940 1932.338  786372       0"},{"path":"wrang.html","id":"data-verbs","chapter":"2 Data Wrangling","heading":"2.2.3 Data Verbs","text":"Taken dplyr tutorial: http://dplyr.tidyverse.org/","code":""},{"path":"wrang.html","id":"starwars","chapter":"2 Data Wrangling","heading":"2.2.3.1 Starwars","text":"","code":"\nlibrary(dplyr)\n\nstarwars %>% dim()## [1] 87 14\nstarwars %>% names()##  [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n##  [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n## [11] \"species\"    \"films\"      \"vehicles\"   \"starships\"\nstarwars %>% head()## # A tibble: 6 × 14\n##   name     height  mass hair_color  skin_color eye_color birth_year sex   gender\n##   <chr>     <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Luke Sk…    172    77 blond       fair       blue            19   male  mascu…\n## 2 C-3PO       167    75 <NA>        gold       yellow         112   none  mascu…\n## 3 R2-D2        96    32 <NA>        white, bl… red             33   none  mascu…\n## 4 Darth V…    202   136 none        white      yellow          41.9 male  mascu…\n## 5 Leia Or…    150    49 brown       light      brown           19   fema… femin…\n## 6 Owen La…    178   120 brown, grey light      blue            52   male  mascu…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       8\n## 2 masculine  15 75     80 88.0 1358 106.14694 184.972677 49      17\nstarwars %>% \n  dplyr::filter(species == \"Droid\")## # A tibble: 6 × 14\n##   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n##   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n## 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculi…\n## 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculi…\n## 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculi…\n## 4 IG-88     200   140 none       metal       red               15 none  masculi…\n## 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n## 6 BB8        NA    NA none       none        black             NA none  masculi…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  dplyr::filter(species != \"Droid\") %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n## 2 masculine  15 77     80 88.0 1358 109.38222 192.397084 45      16\nstarwars %>% \n  dplyr::select(name, ends_with(\"color\"))## # A tibble: 87 × 4\n##    name               hair_color    skin_color  eye_color\n##    <chr>              <chr>         <chr>       <chr>    \n##  1 Luke Skywalker     blond         fair        blue     \n##  2 C-3PO              <NA>          gold        yellow   \n##  3 R2-D2              <NA>          white, blue red      \n##  4 Darth Vader        none          white       yellow   \n##  5 Leia Organa        brown         light       brown    \n##  6 Owen Lars          brown, grey   light       blue     \n##  7 Beru Whitesun lars brown         light       blue     \n##  8 R5-D4              <NA>          white, red  red      \n##  9 Biggs Darklighter  black         light       brown    \n## 10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n## # … with 77 more rows\nstarwars %>% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %>%\n  dplyr::select(name:mass, bmi)## # A tibble: 87 × 4\n##    name               height  mass   bmi\n##    <chr>               <int> <dbl> <dbl>\n##  1 Luke Skywalker        172    77  26.0\n##  2 C-3PO                 167    75  26.9\n##  3 R2-D2                  96    32  34.7\n##  4 Darth Vader           202   136  33.3\n##  5 Leia Organa           150    49  21.8\n##  6 Owen Lars             178   120  37.9\n##  7 Beru Whitesun lars    165    75  27.5\n##  8 R5-D4                  97    32  34.0\n##  9 Biggs Darklighter     183    84  25.1\n## 10 Obi-Wan Kenobi        182    77  23.2\n## # … with 77 more rows\nstarwars %>% \n  dplyr::arrange(desc(mass))## # A tibble: 87 × 14\n##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Jabba …    175  1358 <NA>        green-tan… orange         600   herm… mascu…\n##  2 Grievo…    216   159 none        brown, wh… green, y…       NA   male  mascu…\n##  3 IG-88      200   140 none        metal      red             15   none  mascu…\n##  4 Darth …    202   136 none        white      yellow          41.9 male  mascu…\n##  5 Tarfful    234   136 brown       brown      blue            NA   male  mascu…\n##  6 Owen L…    178   120 brown, grey light      blue            52   male  mascu…\n##  7 Bossk      190   113 none        green      red             53   male  mascu…\n##  8 Chewba…    228   112 brown       unknown    blue           200   male  mascu…\n##  9 Jek To…    180   110 brown       fair       blue            NA   male  mascu…\n## 10 Dexter…    198   102 none        brown      yellow          NA   male  mascu…\n## # … with 77 more rows, and 5 more variables: homeworld <chr>, species <chr>,\n## #   films <list>, vehicles <list>, starships <list>\nstarwars %>%\n  dplyr::group_by(species) %>%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  dplyr::filter(num > 1)## # A tibble: 9 × 3\n##   species    num  mass\n##   <chr>    <int> <dbl>\n## 1 Droid        6  69.8\n## 2 Gungan       3  74  \n## 3 Human       35  82.8\n## 4 Kaminoan     2  88  \n## 5 Mirialan     2  53.1\n## 6 Twi'lek      2  55  \n## 7 Wookiee      2 124  \n## 8 Zabrak       2  80  \n## 9 <NA>         4  48"},{"path":"wrang.html","id":"nhanes","chapter":"2 Data Wrangling","heading":"2.2.3.2 NHANES","text":"","code":"\nrequire(NHANES)\nnames(NHANES)##  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n##  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n##  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n## [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n## [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n## [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n## [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n## [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n## [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n## [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n## [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n## [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n## [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n## [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n## [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n## [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n## [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n## [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n## [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n# find the sleep variables\nNHANESsleep <- NHANES %>% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"\ndim(NHANESsleep)## [1] 10000    11\n# subset for college students\nNHANESsleep <- NHANESsleep %>% filter(Age %in% c(18:22)) %>% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"\ndim(NHANESsleep)## [1] 655  12\nNHANESsleep %>% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) "},{"path":"wrang.html","id":"summarize-and-group_by","chapter":"2 Data Wrangling","heading":"2.2.4 summarize and group_by","text":"","code":"\n# number of people (cases) in NHANES\nNHANES %>% summarize(n())## # A tibble: 1 × 1\n##   `n()`\n##   <int>\n## 1 10000\n# total weight of all the people in NHANES (silly)\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(sum(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `sum(Weightlb, na.rm = TRUE)`\n##                           <dbl>\n## 1                      1549419.\n# mean weight of all the people in NHANES\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `mean(Weightlb, na.rm = TRUE)`\n##                            <dbl>\n## 1                           156.\n# repeat the above but for groups\n\n# males versus females\nNHANES %>% group_by(Gender) %>% summarize(n())## # A tibble: 2 × 2\n##   Gender `n()`\n##   <fct>  <int>\n## 1 female  5020\n## 2 male    4980\nNHANES %>% group_by(Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 2 × 2\n##   Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                           <dbl>\n## 1 female                           146.\n## 2 male                             167.\n# smokers and non-smokers\nNHANES %>% group_by(SmokeNow) %>% summarize(n())## # A tibble: 3 × 2\n##   SmokeNow `n()`\n##   <fct>    <int>\n## 1 No        1745\n## 2 Yes       1466\n## 3 <NA>      6789\nNHANES %>% group_by(SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                 186.\n## 2 Yes                                177.\n## 3 <NA>                               144.\n# people with and without diabetes\nNHANES %>% group_by(Diabetes) %>% summarize(n())## # A tibble: 3 × 2\n##   Diabetes `n()`\n##   <fct>    <int>\n## 1 No        9098\n## 2 Yes        760\n## 3 <NA>       142\nNHANES %>% group_by(Diabetes) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   Diabetes `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                155. \n## 2 Yes                               202. \n## 3 <NA>                               21.6\n# break down the smokers versus non-smokers further, by sex\nNHANES %>% group_by(SmokeNow, Gender) %>% summarize(n())## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `n()`\n##   <fct>    <fct>  <int>\n## 1 No       female   764\n## 2 No       male     981\n## 3 Yes      female   638\n## 4 Yes      male     828\n## 5 <NA>     female  3618\n## 6 <NA>     male    3171\nNHANES %>% group_by(SmokeNow, Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                           <dbl>\n## 1 No       female                           167.\n## 2 No       male                             201.\n## 3 Yes      female                           167.\n## 4 Yes      male                             185.\n## 5 <NA>     female                           138.\n## 6 <NA>     male                             151.\n# break down the people with diabetes further, by smoking\nNHANES %>% group_by(Diabetes, SmokeNow) %>% summarize(n())## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `n()`\n##   <fct>    <fct>    <int>\n## 1 No       No        1476\n## 2 No       Yes       1360\n## 3 No       <NA>      6262\n## 4 Yes      No         267\n## 5 Yes      Yes        106\n## 6 Yes      <NA>       387\n## 7 <NA>     No           2\n## 8 <NA>     <NA>       140\nNHANES %>% group_by(Diabetes, SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                             <dbl>\n## 1 No       No                                183. \n## 2 No       Yes                               175. \n## 3 No       <NA>                              143. \n## 4 Yes      No                                204. \n## 5 Yes      Yes                               204. \n## 6 Yes      <NA>                              199. \n## 7 <NA>     No                                193. \n## 8 <NA>     <NA>                               19.1"},{"path":"wrang.html","id":"babynames","chapter":"2 Data Wrangling","heading":"2.2.5 babynames","text":"","code":"\nbabynames %>% group_by(sex) %>%\n  summarize(total=sum(n))## # A tibble: 2 × 2\n##   sex       total\n##   <chr>     <int>\n## 1 F     172371079\n## 2 M     175749438\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% head()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  1880 F            942\n## 2  1880 M           1058\n## 3  1881 F            938\n## 4  1881 M            997\n## 5  1882 F           1028\n## 6  1882 M           1099\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% tail()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  2015 F          19074\n## 2  2015 M          14024\n## 3  2016 F          18817\n## 4  2016 M          14162\n## 5  2017 F          18309\n## 6  2017 M          14160\nbabysamp <- babynames %>% sample_n(size=50)\nbabysamp %>% select(year) %>% distinct() %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    1    1    1    1    1\nbabysamp %>% distinct() %>% select(year) %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    2    2    1    1    3    1    1    1    1    1    2    2    1    2    2 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    3    1    2    1    1\nFrances <- babynames %>%\n  filter(name== \"Frances\") %>%\n  group_by(year, sex) %>%\n  summarize(yrTot = sum(n))\n\nFrances %>% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")"},{"path":"wrang.html","id":"highverb","chapter":"2 Data Wrangling","heading":"2.3 Higher Level Data Verbs","text":"complicated verbs may important sophisticated analyses. See RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.pivot_longer makes many columns 2 columns: pivot_longer(data, cols,  names_to = , value_to = )pivot_wider makes one column multiple columns: pivot_wider(data, names_from = , values_from = )left_join returns rows left table, rows matching keys right table.inner_join returns rows left table matching keys right table (.e., matching rows sets).full_join returns rows tables, join records left matching keys right table.Good practice: always specify argument joining data frames.ever need understand join right join , try find image lay function . found one quite good taken Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti","code":""},{"path":"wrang.html","id":"r-examples-higher-level-verbs","chapter":"2 Data Wrangling","heading":"2.4 R examples, higher level verbs","text":"tidyr 1.0.0 just released! new release means need update tidyr. know latest version following command works console (window ):familiar spread gather, acquaint pivot_longer() pivot_wider(). idea go wide dataframes long dataframes vice versa.","code":"?tidyr::pivot_longer"},{"path":"wrang.html","id":"pivot_longer","chapter":"2 Data Wrangling","heading":"2.4.1 pivot_longer()","text":"pivot military pay grade become longer?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#\ngid=1811988794Does graph tell us right? done wrong…?","code":"\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\nglimpse(navy_gs)## Rows: 38\n## Columns: 17\n## $ ...1                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ `Active Duty Family` <chr> NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n## $ ...3                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n## $ ...4                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n## $ ...5                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n## $ ...6                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n## $ ...7                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n## $ ...8                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n## $ ...9                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n## $ ...10                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n## $ ...11                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n## $ ...12                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n## $ ...13                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n## $ ...14                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n## $ ...15                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n## $ ...16                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n## $ ...17                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)## Rows: 30\n## Columns: 16\n## $ pay.grade       <chr> \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n## $ male.sing.wo    <dbl> 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n## $ female.sing.wo  <dbl> 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n## $ tot.sing.wo     <dbl> 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n## $ male.sing.w     <dbl> 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n## $ female.sing.w   <dbl> 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n## $ tot.sing.w      <dbl> 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n## $ male.joint.NA   <dbl> 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n## $ female.joint.NA <dbl> 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n## $ tot.joint.NA    <dbl> 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n## $ male.civ.NA     <dbl> 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n## $ female.civ.NA   <dbl> 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n## $ tot.civ.NA      <dbl> 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n## $ male.tot.NA     <dbl> 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n## $ female.tot.NA   <dbl> 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n## $ tot.tot.NA      <dbl> 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n# get rid of total columns & rows:\n\nnavyWR = navy %>% select(-contains(\"tot\")) %>%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %>%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %>%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %>% head()## # A tibble: 6 × 5\n##   pay.grade sex    marital kids  numPeople\n##   <chr>     <chr>  <chr>   <chr>     <dbl>\n## 1 E-1       male   sing    wo        31229\n## 2 E-1       female sing    wo         5717\n## 3 E-1       male   sing    w           563\n## 4 E-1       female sing    w           122\n## 5 E-1       male   joint   NA          139\n## 6 E-1       female joint   NA          141\nnavyWR %>% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))"},{"path":"wrang.html","id":"pivot_wider","chapter":"2 Data Wrangling","heading":"2.4.2 pivot_wider","text":"","code":"\nlibrary(babynames)\nbabynames %>% dplyr::select(-prop) %>%\n   tidyr::pivot_wider(names_from = sex, values_from = n) ## # A tibble: 1,756,284 × 4\n##     year name          F     M\n##    <dbl> <chr>     <int> <int>\n##  1  1880 Mary       7065    27\n##  2  1880 Anna       2604    12\n##  3  1880 Emma       2003    10\n##  4  1880 Elizabeth  1939     9\n##  5  1880 Minnie     1746     9\n##  6  1880 Margaret   1578    NA\n##  7  1880 Ida        1472     8\n##  8  1880 Alice      1414    NA\n##  9  1880 Bertha     1320    NA\n## 10  1880 Sarah      1288    NA\n## # … with 1,756,274 more rows\nbabynames %>% \n  select(-prop) %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(year), desc(M))## # A tibble: 168,381 × 4\n##     year name         F     M\n##    <dbl> <chr>    <int> <int>\n##  1  2017 Liam        36 18728\n##  2  2017 Noah       170 18326\n##  3  2017 William     18 14904\n##  4  2017 James       77 14232\n##  5  2017 Logan     1103 13974\n##  6  2017 Benjamin     8 13733\n##  7  2017 Mason       58 13502\n##  8  2017 Elijah      26 13268\n##  9  2017 Oliver      15 13141\n## 10  2017 Jacob       16 13106\n## # … with 168,371 more rows\nbabynames %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(prop))## # A tibble: 12 × 5\n##     year name            prop     F     M\n##    <dbl> <chr>          <dbl> <int> <int>\n##  1  1986 Marquette 0.0000130     24    25\n##  2  1996 Dariel    0.0000115     22    23\n##  3  2014 Laramie   0.0000108     21    22\n##  4  1939 Earnie    0.00000882    10    10\n##  5  1939 Vertis    0.00000882    10    10\n##  6  1921 Vernis    0.00000703     9     8\n##  7  1939 Alvia     0.00000529     6     6\n##  8  1939 Eudell    0.00000529     6     6\n##  9  1939 Ladell    0.00000529     6     6\n## 10  1939 Lory      0.00000529     6     6\n## 11  1939 Maitland  0.00000529     6     6\n## 12  1939 Delaney   0.00000441     5     5"},{"path":"wrang.html","id":"join-use-join-to-merge-two-datasets","chapter":"2 Data Wrangling","heading":"2.4.3 join (use join to merge two datasets)","text":"","code":""},{"path":"wrang.html","id":"first-get-the-data-gapminder","chapter":"2 Data Wrangling","heading":"2.4.3.1 First get the data (GapMinder)","text":"following datasets come GapMinder. first represents country, year, female literacy rate. second represents country, year, GDP (fixed 2000 US$).","code":"\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %>% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %>%\n  filter(!is.na(litRateF))\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %>% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %>%\n  filter(!is.na(gdp))\nhead(litF)## # A tibble: 6 × 3\n##   country     year  litRateF\n##   <chr>       <chr>    <dbl>\n## 1 Afghanistan 1979      4.99\n## 2 Afghanistan 2011     13   \n## 3 Albania     2001     98.3 \n## 4 Albania     2008     94.7 \n## 5 Albania     2011     95.7 \n## 6 Algeria     1987     35.8\nhead(GDP)## # A tibble: 6 × 3\n##   country year    gdp\n##   <chr>   <chr> <dbl>\n## 1 Albania 1980  1061.\n## 2 Albania 1981  1100.\n## 3 Albania 1982  1111.\n## 4 Albania 1983  1101.\n## 5 Albania 1984  1065.\n## 6 Albania 1985  1060.\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)## [1] 571   4\nsum(is.na(litGDPleft$gdp))## [1] 66\nhead(litGDPleft)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902.\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)## [1] 7988    4\nsum(is.na(litGDPright$gdp))## [1] 0\nhead(litGDPright)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)## [1] 505   4\nsum(is.na(litGDPinner$gdp))## [1] 0\nhead(litGDPinner)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)## [1] 8054    4\nsum(is.na(litGDPfull$gdp))## [1] 66\nhead(litGDPfull)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902."},{"path":"wrang.html","id":"lubridate","chapter":"2 Data Wrangling","heading":"2.4.4 lubridate","text":"lubridate another R package meant data wrangling (Grolemund Wickham 2011). particular, lubridate makes easy work days, times, dates. base idea start dates ymd (year month day) format transform information whatever want. linked table original paper provides many basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.Example https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html","code":""},{"path":"wrang.html","id":"if-anyone-drove-a-time-machine-they-would-crash","chapter":"2 Data Wrangling","heading":"2.4.4.1 If anyone drove a time machine, they would crash","text":"length months years change often arithmetic can unintuitive. Consider simple operation, January 31st + one month. answer :February 31st (doesn’t exist)March 4th (31 days January 31), orFebruary 28th (assuming leap year)basic property arithmetic + b - b = . solution 1 obeys mathematical property, invalid date. Wickham wants make lubridate consistent possible invoking following rule: adding subtracting month year creates invalid date, lubridate return NA.thought solution 2 3 useful, problem. can still get results clever arithmetic, using special %m+% %m-% operators. %m+% %m-% automatically roll dates back last day month, necessary.","code":""},{"path":"wrang.html","id":"r-examples-lubridate","chapter":"2 Data Wrangling","heading":"2.4.4.2 R examples, lubridate()","text":"","code":""},{"path":"wrang.html","id":"some-basics-in-lubridate","chapter":"2 Data Wrangling","heading":"Some basics in lubridate","text":"","code":"\nrequire(lubridate)\nrightnow <- now()\n\nday(rightnow)## [1] 11\nweek(rightnow)## [1] 2\nmonth(rightnow, label=FALSE)## [1] 1\nmonth(rightnow, label=TRUE)## [1] Jan\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nyear(rightnow)## [1] 2022\nminute(rightnow)## [1] 55\nhour(rightnow)## [1] 15\nyday(rightnow)## [1] 11\nmday(rightnow)## [1] 11\nwday(rightnow, label=FALSE)## [1] 3\nwday(rightnow, label=TRUE)## [1] Tue\n## Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"},{"path":"wrang.html","id":"but-how-do-i-create-a-date-object","chapter":"2 Data Wrangling","heading":"But how do I create a date object?","text":"","code":"\njan31 <- ymd(\"2021-01-31\")\njan31 + months(0:11)##  [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n##  [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n## [11] NA           \"2021-12-31\"\nfloor_date(jan31, \"month\") + months(0:11) + days(31)##  [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n##  [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n## [11] \"2021-12-02\" \"2022-01-01\"\njan31 + months(0:11) + days(31)##  [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n##  [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n## [11] NA           \"2022-01-31\"\njan31 %m+% months(0:11)##  [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n##  [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n## [11] \"2021-11-30\" \"2021-12-31\""},{"path":"wrang.html","id":"nyc-flights","chapter":"2 Data Wrangling","heading":"NYC flights","text":"","code":"\nlibrary(nycflights13)\nnames(flights)##  [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n##  [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n##  [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n## [13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n## [17] \"hour\"           \"minute\"         \"time_hour\"\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)## # A tibble: 6 × 22\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      517            515         2      830            819\n## 2  2013     1     1      533            529         4      850            830\n## 3  2013     1     1      542            540         2      923            850\n## 4  2013     1     1      544            545        -1     1004           1022\n## 5  2013     1     1      554            600        -6      812            837\n## 6  2013     1     1      554            558        -4      740            728\n## # … with 14 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, ymdday <date>, weekdy <ord>,\n## #   whichweek <dbl>\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %>% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %>%  \n   head()## # A tibble: 6 × 9\n##    year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n##   <int> <int> <int> <date>     <ord>      <dbl>    <int>    <int>    <dbl>\n## 1  2013     1     1 2013-01-01 Tue            1      517      830      227\n## 2  2013     1     1 2013-01-01 Tue            1      533      850      227\n## 3  2013     1     1 2013-01-01 Tue            1      542      923      160\n## 4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n## 5  2013     1     1 2013-01-01 Tue            1      554      812      116\n## 6  2013     1     1 2013-01-01 Tue            1      554      740      150"},{"path":"wrang.html","id":"purrr-for-functional-programming","chapter":"2 Data Wrangling","heading":"2.5 purrr for functional programming","text":"see R package purrr greater detail go, now, let’s get hint works.going focus map family functions get us started. Lots good purrr functions like pluck() accumulate().Much taken tutorial Rebecca Barter.map functions named output produce. example:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vectorNote first argument always data object second object always function want iteratively apply element input object.input map function always either vector (like column), list (can non-rectangular), dataframe (like rectangle).list way hold things might different shape:Consider following function:can map() add_ten() function across vector. Note output list (default).use different type input? default behavior still return list!want different type output? use different map() function, map_df(), example.Shorthand lets us get away pre-defining function (useful). Use tilde ~ indicate function:Mostly, tilde used functions already know:","code":"\na_list <- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)## $a_number\n## [1] 5\n## \n## $a_vector\n## [1] \"a\" \"b\" \"c\"\n## \n## $a_dataframe\n##   a b             c\n## 1 1 q       bananas\n## 2 2 b           are\n## 3 3 z so very great\nadd_ten <- function(x) {\n  return(x + 10)\n  }\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 15\n## \n## [[3]]\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map(add_ten)## $a\n## [1] 12\n## \n## $b\n## [1] 15\n## \n## $c\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(add_ten)## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(~{.x + 10})## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %>%\n  split(.$species) %>%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map_df(tidy)  # map(tidy)## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n## 2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n## 3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n## 4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n## 5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\npenguins %>%\n  group_by(species) %>%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map(tidy)  # map_df(tidy)## [[1]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 0.00948      \n## 2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n## \n## [[2]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -3037.     997.       -3.05 0.00333      \n## 2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n## \n## [[3]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 2 flipper_length_mm     54.6      5.03     10.9  1.33e-19"},{"path":"wrang.html","id":"reprex","chapter":"2 Data Wrangling","heading":"2.6 reprex()","text":"Help help youIn order create reproducible example …Step 1. Copy code onto clipboardStep 2. Type reprex() ConsoleStep 3. Look Viewer right. Copy Viewer output GitHub, Piazza, email, stackexchange, etc.places learn reprex includeA blog : https://teachdatascience.com/reprex/reprex vignette: https://reprex.tidyverse.org/index.htmlreprex dos donts: https://reprex.tidyverse.org/articles/reprex-dos--donts.htmlJenny Bryan webinar reprex: “Help help . Creating reproducible examples” https://resources.rstudio.com/webinars/help--help--creating-reproducible-examples-jenny-bryanSome advice: https://stackoverflow.com/help/minimal-reproducible-example","code":""},{"path":"wrang.html","id":"reprex-demo","chapter":"2 Data Wrangling","heading":"2.6.0.1 reprex demo","text":"multiple lines code:","code":"reprex(\n  jan31 + months(0:11) + days(31)\n)reprex({\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})reprex({\n  library(lubridate)\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})"},{"path":"viz.html","id":"viz","chapter":"3 Visualization","heading":"3 Visualization","text":"Data visualization integral understanding data models. Computational statistics data science sometimes focus models resulting predictions models. doubt structure format data key whether model appropriate good. good data analyst always spend lot time effort exploratory data analysis, much includes making many visualizations data possible.Depending introductory () statistics classes ’ve , instructor may focused less visualizations class. () may even said something like making visualizations incredibly important entire data analysis process. even buy perspective, don’t see good graphics analyses? Andrew Gelman (Gelman 2011) responds stating, “Good statistical graphics hard , much harder running regressions making tables.” goal create graphics visualizations convey statistical information.Nolan (Nolan Perrett 2016) describes three important ways graphics can used convey statistical information. “guiding principles” used way evaluating others’ figures well metric creating visualizations help statistical analysis.Make data stand outThe important idea find anything unusual data. patterns? Outliers? bounds variables? axes scaled? transformations warranted?Facilitate comparisonThe second item allows us consider research questions hand. important variables? emphasize ? variables plotted together? Can super-imposed? color, plotting character, size plot character help bring important relationships? aware plotting issues color blindness. http://colorbrewer2.org/Add informationPlots also add context comparison. Figure legends, axes scales, reference markers (e.g., line \\(y=x\\)) go long way toward helping reader understand message. Captions self-contained (assume user also read text) descriptive; summarize content figure conclusion related message want convey.Randy Pruim asks following question decide whether plot good: plot make comparisons interested …easily? andaccurately?Consider adding alt text allow screen readers parse image. DataViz Society/Nightingale way Amy Cesal article writing good alt text plots/graphs, Writing Alt Text Data Visualization.","code":""},{"path":"viz.html","id":"thoughts","chapter":"3 Visualization","heading":"3.1 Thoughts on Plotting","text":"","code":""},{"path":"viz.html","id":"advice","chapter":"3 Visualization","heading":"3.1.1 Advice","text":"Basic plotting\nAvoid graph elements interfere data\nUse visually prominent symbols\nAvoid -plotting (One way avoid plotting: Jitter values)\nDifferent values data may obscure \nInclude nearly data\nFill data region\nAvoid graph elements interfere dataUse visually prominent symbolsAvoid -plotting (One way avoid plotting: Jitter values)Different values data may obscure otherInclude nearly dataFill data regionEliminate superfluous material\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background images\nExtra tick marks grid lines\nUnnecessary text arrows\nDecimal places beyond measurement error level difference\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background imagesExtra tick marks grid linesUnnecessary text arrowsDecimal places beyond measurement error level differenceFacilitate Comparisons\nPut juxtaposed plots scale\nMake easy distinguish elements superposed plots (e.g. color)\nEmphasizes important difference\nComparison: volume, area, height (careful, volume can seem bigger mean )\nPut juxtaposed plots scaleMake easy distinguish elements superposed plots (e.g. color)Emphasizes important differenceComparison: volume, area, height (careful, volume can seem bigger mean )Choosing Scale (n.b., principles may go counter one another, use judgment.)\nKeep scales x y axes plots facilitate comparison\nZoom focus region contains bulk data\nKeep scale throughout plot (.e. don’t change mid-axis)\nOrigin need scale\nChoose scale improves resolution\nAvoid jiggling baseline\nKeep scales x y axes plots facilitate comparisonZoom focus region contains bulk dataKeep scale throughout plot (.e. don’t change mid-axis)Origin need scaleChoose scale improves resolutionAvoid jiggling baselineHow make plot information rich\nDescribe see caption\nAdd context reference markers (lines points) including text\nAdd legends labels\nUse color plotting symbols add information\nPlot thing different ways/scales\nReduce clutter\nDescribe see captionAdd context reference markers (lines points) including textAdd legends labelsUse color plotting symbols add informationPlot thing different ways/scalesReduce clutterCaptions \ncomprehensive\nSelf-contained\nDescribe graphed\nDraw attention important features\nDescribe conclusions drawn graph\ncomprehensiveSelf-containedDescribe graphedDraw attention important featuresDescribe conclusions drawn graphGood Plot Making Practice\nPut major conclusions graphical form\nProvide reference information\nProof read clarity consistency\nGraphing iterative process\nMultiplicity OK, .e. two plots variable may provide different messages\nMake plots data rich\nPut major conclusions graphical formProvide reference informationProof read clarity consistencyGraphing iterative processMultiplicity OK, .e. two plots variable may provide different messagesMake plots data richCreating statistical graphic iterative process discovery fine tuning. try model process creating visualizations course dedicating class time iterative creation plot. begin either plot screams correction, transform step--step, always thinking goal graph data rich presents clear vision important features data.","code":""},{"path":"viz.html","id":"fonts-matter","chapter":"3 Visualization","heading":"3.1.1.1 Fonts Matter","text":"RStudio::conf 2020, Glamour Graphics, Chase makes important points making good graphics matters. talk might summarized plot : fonts matter.","code":""},{"path":"viz.html","id":"deconstruct","chapter":"3 Visualization","heading":"3.2 Deconstructing a graph","text":"","code":""},{"path":"viz.html","id":"gg","chapter":"3 Visualization","heading":"3.2.1 The Grammar of Graphics (gg)","text":"Yau (2013) Wickham (2014) come taxonomy grammar thinking parts figure just like conceptualize parts body parts sentence.One great way thinking new process: longer necessary talk name graph (e.g., boxplot). Instead now think glyphs (geoms), can put whatever want plot. Note also transition leads passive consumer (need make plot XXX everyone else , just plug data) active participant (want data say? can put information onto graphic?)important questions can ask respect creating figures :want R ? (goal?)R need know?Yau (2013) gives us nine visual cues, Wickham (2014) translates language using ggplot2. (items Baumer, Kaplan, Horton (2021), chapter 2.)Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Coordinate System: rectangular, polar, geographic, etc.Coordinate System: rectangular, polar, geographic, etc.Scale: numeric (linear? logarithmic?), categorical (ordered?), timeScale: numeric (linear? logarithmic?), categorical (ordered?), timeContext: comparison (think back ideas Tufte)Context: comparison (think back ideas Tufte)","code":""},{"path":"viz.html","id":"order-matters","chapter":"3 Visualization","heading":"Order Matters","text":"","code":""},{"path":"viz.html","id":"cues-together","chapter":"3 Visualization","heading":"Cues Together","text":"","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-1","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-2","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"the-grammar-of-graphics-in-ggplot2","chapter":"3 Visualization","heading":"3.2.1.1 The grammar of graphics in ggplot2","text":"geom: geometric “shape” used display databar, point, line, ribbon, text, etc.aesthetic: attribute controlling geom displayed respect variablesx position, y position, color, fill, shape, size, etc.scale: adjust information aesthetic map onto plotparticular assignment colors, shapes, sizes, etc.; making axes continuous constrained particular range values.guide: helps user convert visual data back raw data (legends, axes)stat: transformation applied data geom gets itexample: histograms work binned data","code":""},{"path":"viz.html","id":"ggplot2","chapter":"3 Visualization","heading":"3.2.2 ggplot2","text":"ggplot2, aesthetic refers mapping variable information conveys plot. information plotting visualizing information given chapter 2 (Data visualization) Baumer, Kaplan, Horton (2021). Much data presentation represents births 1978 US: date, day year, number births.","code":""},{"path":"viz.html","id":"goals","chapter":"3 Visualization","heading":"Goals","text":"try dogive tour ggplot2give tour ggplot2explain think plots ggplot2 wayexplain think plots ggplot2 wayprepare/encourage learn laterprepare/encourage learn laterWhat can’t one sessionshow every bell whistleshow every bell whistlemake expert using ggplot2make expert using ggplot2","code":""},{"path":"viz.html","id":"getting-help","chapter":"3 Visualization","heading":"Getting help","text":"One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfLook end presentation. help options .Look end presentation. help options .","code":"\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)##         date births wday year month day_of_year day_of_month day_of_week\n## 1 1978-01-01   7701  Sun 1978     1           1            1           1\n## 2 1978-01-02   7527  Mon 1978     1           2            2           2\n## 3 1978-01-03   8825  Tue 1978     1           3            3           3"},{"path":"viz.html","id":"how-can-we-make-the-plot","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Two Questions:want R ? (goal?)want R ? (goal?)R need know?\ndata source: Births78\naesthetics:\ndate -> x\nbirths -> y\npoints (!)\n\nR need know?data source: Births78data source: Births78aesthetics:\ndate -> x\nbirths -> y\npoints (!)\naesthetics:date -> xbirths -> ypoints (!)Goal: scatterplot = plot points\nggplot() + geom_point()\nGoal: scatterplot = plot pointsggplot() + geom_point()R need know?\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\nR need know?data source: data = Births78data source: data = Births78aesthetics: aes(x = date, y = births)aesthetics: aes(x = date, y = births)","code":""},{"path":"viz.html","id":"how-can-we-make-the-plot-1","chapter":"3 Visualization","heading":"How can we make the plot?","text":"changed?new aesthetic: mapping color day week","code":""},{"path":"viz.html","id":"adding-day-of-week-to-the-data-set","chapter":"3 Visualization","heading":"Adding day of week to the data set","text":"wday() function lubridate package computes day week date.","code":"\nBirths78 <-  \n  Births78 %>% \n  mutate(wday = lubridate::wday(date, label=TRUE))\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-2","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now use lines instead dots","code":"\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-3","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now two layers, one points one \nlinesThe layers placed one top : points\nlines .layers placed one top : points\nlines .data aes specified ggplot() affect geomsdata aes specified ggplot() affect geoms","code":"\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"alternative-syntax","chapter":"3 Visualization","heading":"Alternative Syntax","text":"","code":"\nBirths78 %>% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"what-does-adding-the-color-argument-do","chapter":"3 Visualization","heading":"What does adding the color argument do?","text":"variable, mapped color aesthetic new variable one value (“navy”). dots get set color, ’s navy.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping","chapter":"3 Visualization","heading":"Setting vs. Mapping","text":"want set color navy dots, outside aesthetic, without dataset variable:Note color = \"navy\" now outside aesthetics list. ’s ggplot2 distinguishes mapping setting.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-4","chapter":"3 Visualization","heading":"How can we make the plot?","text":"ggplot() establishes default data aesthetics geoms, geom may change defaults.ggplot() establishes default data aesthetics geoms, geom may change defaults.good practice: put ggplot() things affect () layers; rest geom_blah()good practice: put ggplot() things affect () layers; rest geom_blah()","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping-again","chapter":"3 Visualization","heading":"Setting vs. Mapping (again)","text":"Information gets passed plot via:map variable information inside aes (aesthetic) commandmap variable information inside aes (aesthetic) commandset non-variable information outside aes (aesthetic) commandset non-variable information outside aes (aesthetic) command","code":""},{"path":"viz.html","id":"other-geoms","chapter":"3 Visualization","heading":"Other geoms","text":"help pages tell aesthetics, default stats, etc.","code":"\napropos(\"^geom_\") [1] \"geom_abline\"                  \"geom_area\"                   \n [3] \"geom_ash\"                     \"geom_bar\"                    \n [5] \"geom_barh\"                    \"geom_bin_2d\"                 \n [7] \"geom_bin2d\"                   \"geom_blank\"                  \n [9] \"geom_boxplot\"                 \"geom_boxploth\"               \n[11] \"geom_col\"                     \"geom_colh\"                   \n[13] \"geom_contour\"                 \"geom_contour_filled\"         \n[15] \"geom_count\"                   \"geom_crossbar\"               \n[17] \"geom_crossbarh\"               \"geom_curve\"                  \n[19] \"geom_density\"                 \"geom_density_2d\"             \n[21] \"geom_density_2d_filled\"       \"geom_density_line\"           \n[23] \"geom_density_ridges\"          \"geom_density_ridges_gradient\"\n[25] \"geom_density_ridges2\"         \"geom_density2d\"              \n[27] \"geom_density2d_filled\"        \"geom_dotplot\"                \n[29] \"geom_errorbar\"                \"geom_errorbarh\"              \n[31] \"geom_errorbarh\"               \"geom_freqpoly\"               \n[33] \"geom_function\"                \"geom_hex\"                    \n[35] \"geom_histogram\"               \"geom_histogramh\"             \n[37] \"geom_hline\"                   \"geom_jitter\"                 \n[39] \"geom_label\"                   \"geom_line\"                   \n[41] \"geom_linerange\"               \"geom_linerangeh\"             \n[43] \"geom_lm\"                      \"geom_map\"                    \n[45] \"geom_path\"                    \"geom_point\"                  \n[47] \"geom_pointrange\"              \"geom_pointrangeh\"            \n[49] \"geom_polygon\"                 \"geom_qq\"                     \n[51] \"geom_qq_line\"                 \"geom_quantile\"               \n[53] \"geom_raster\"                  \"geom_rect\"                   \n[55] \"geom_ribbon\"                  \"geom_ridgeline\"              \n[57] \"geom_ridgeline_gradient\"      \"geom_rug\"                    \n[59] \"geom_segment\"                 \"geom_sf\"                     \n[61] \"geom_sf_label\"                \"geom_sf_text\"                \n[63] \"geom_sina\"                    \"geom_smooth\"                 \n[65] \"geom_spline\"                  \"geom_spoke\"                  \n[67] \"geom_step\"                    \"geom_text\"                   \n[69] \"geom_tile\"                    \"geom_violin\"                 \n[71] \"geom_violinh\"                 \"geom_vline\"                  \n[73] \"geom_vridgeline\"             \n?geom_area             # for example"},{"path":"viz.html","id":"lets-try-geom_area","chapter":"3 Visualization","heading":"Let’s try geom_area","text":"Using area produce good plotover plotting hiding much dataextending y-axis 0 may may desirable.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"side-note-what-makes-a-plot-good","chapter":"3 Visualization","heading":"Side note: what makes a plot good?","text":"(?) graphics intended help us make comparisonsHow something change time?treatments matter? much?men women respond way?Key plot metric: plot make comparisons interested ineasily, andaccurately?","code":""},{"path":"viz.html","id":"time-for-some-different-data","chapter":"3 Visualization","heading":"Time for some different data","text":"HELPrct: Health Evaluation Linkage Primary care randomized clinical trialSubjects admitted treatment addiction one three substances.","code":"\nhead(HELPrct)##   age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n## 1  37            1    yes   49  3        177      225        0  NA      0\n## 2  37            1    yes   30 22          2       NA        0  NA      0\n## 3  26            1    yes   39  0          3      365       20  NA      0\n## 4  39            1    yes   15  2        189      343        0   1      1\n## 5  32            1    yes   39 12          2       57        0   1      0\n## 6  47            1    yes    6  1         31      365        0  NA      1\n##      sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n## 1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n## 2   male yes homeless 56 62  2     43         NA <NA> 26.670307 36.03694      1\n## 3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n## 4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n## 5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n## 6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n##   racegrp satreat sexrisk substance treat avg_drinks max_drinks\n## 1   black      no       4   cocaine   yes         13         26\n## 2   white      no       7   alcohol   yes         56         62\n## 3   black      no       2    heroin    no          0          0\n## 4   white     yes       4    heroin    no          5          5\n## 5   black      no       6   cocaine    no         10         13\n## 6   black      no       5   cocaine   yes          4          4\n##   hospitalizations\n## 1                3\n## 2               22\n## 3                0\n## 4                2\n## 5               12\n## 6                1"},{"path":"viz.html","id":"who-are-the-people-in-the-study","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"Hmm. ’s y?\nstat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.\nHmm. ’s y?stat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-1","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-2","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nlibrary(scales)\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-3","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study","chapter":"3 Visualization","heading":"How old are people in the HELP study?","text":"Notice messagesstat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.stat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.binwidth: default binwidth selected, really choose .binwidth: default binwidth selected, really choose .","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"viz.html","id":"setting-the-binwidth-manually","chapter":"3 Visualization","heading":"Setting the binwidth manually","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study-other-geoms","chapter":"3 Visualization","heading":"How old are people in the HELP study? – Other geoms","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every geom comes default statfor simple cases, stat stat_identity() nothingwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually-1","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every stat comes default geom, every geom default statwe can specify stats instead geom, preferwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"more-combinations","chapter":"3 Visualization","heading":"More combinations","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-much-do-they-drink-i1","chapter":"3 Visualization","heading":"How much do they drink? (i1)","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"covariates-adding-in-more-variables","chapter":"3 Visualization","heading":"Covariates: Adding in more variables","text":"Using color linetype:Using color facets","code":"\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"boxplots","chapter":"3 Visualization","heading":"Boxplots","text":"Boxplots use stat_quantile() computes five-number summary (roughly five quartiles data) uses define “box” “whiskers.”quantitative variable must y, must additional x variable.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"horizontal-boxplots","chapter":"3 Visualization","heading":"Horizontal boxplots","text":"Horizontal boxplots obtained flipping coordinate system:coord_flip() may used plots well reverse roles\nx y plot.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"axes-scaling-with-boxplots","chapter":"3 Visualization","heading":"Axes scaling with boxplots","text":"can scale continuous axis","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"give-me-some-space","chapter":"3 Visualization","heading":"Give me some space","text":"’ve triggered new feature: dodge (dodging things left/right). can control much set dodge manually.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"issues-with-bigger-data","chapter":"3 Visualization","heading":"Issues with bigger data","text":"Although can see generally positive association (expect), plotting may hiding information.","code":"\nrequire(NHANES)\ndim(NHANES)## [1] 10000    76\nNHANES %>%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"using-alpha-opacity","chapter":"3 Visualization","heading":"Using alpha (opacity)","text":"One way deal plotting set opacity low.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"geom_density2d","chapter":"3 Visualization","heading":"geom_density2d","text":"Alternatively (simultaneously) might prefer different geom altogether.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"multiple-layers","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"multiple-layers-1","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"things-i-havent-mentioned-much","chapter":"3 Visualization","heading":"Things I haven’t mentioned (much)","text":"coords (coord_flip() good know )coords (coord_flip() good know )themes (customizing appearance)themes (customizing appearance)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)transforming axestransforming axes","code":"\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"a-little-bit-of-everything","chapter":"3 Visualization","heading":"A little bit of everything","text":"","code":"\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"want-to-learn-more","chapter":"3 Visualization","heading":"Want to learn more?","text":"docs.ggplot2.org/docs.ggplot2.org/Winston Chang’s: R Graphics CookbookWinston Chang’s: R Graphics Cookbook","code":""},{"path":"viz.html","id":"what-else-can-we-do","chapter":"3 Visualization","heading":"What else can we do?","text":"shinyinteractive graphics / modelinginteractive graphics / modelinghttps://shiny.rstudio.com/https://shiny.rstudio.com/plotlyPlotly R package creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. plotly R library contains ggplotly function , convert ggplot2 figures Plotly object. Furthermore, option manipulating Plotly object style function.https://plot.ly/ggplot2/getting-started/Dynamic documentscombination RMarkdown, ggvis, shiny","code":""},{"path":"slr.html","id":"slr","chapter":"4 Simple Linear Regression","heading":"4 Simple Linear Regression","text":"","code":""},{"path":"slr.html","id":"a-linear-model","chapter":"4 Simple Linear Regression","heading":"4.1 A Linear Model","text":"Consider situation two variables, denote \\(x\\) \\(Y\\); \\(x\\) predictor variable, \\(Y\\) response. observe \\(n\\) observations, sample, denoted \\((x_i,y_i)\\). believe two variable related, namely \\[Y=f(x)+\\epsilon,\\] \\(\\epsilon\\) random error accounts fact know values \\(x\\) \\(f\\), still won’t know exactly \\(Y\\) .two variable case, assume \\(f(x)\\) linear function \\(x\\). , assume model \\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i.\\] attempts estimate function \\(f\\) now reduced trying estimate two numbers, intercept \\(\\beta_0\\) slope \\(\\beta_1\\): parameters.Consider following 4 models. Note differences statistics vs. parameters also individual observations vs. averages. Convince know use model.\\[\\begin{eqnarray*}\nE[Y_i|x_i] &=& \\beta_0 + \\beta_1 x_i \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n&& \\epsilon_i = y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\n&& e_i = y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"fitting-the-regression-line-least-squares","chapter":"4 Simple Linear Regression","heading":"4.1.1 Fitting the regression line: least squares","text":"fit regression line? Find \\(b_0\\) \\(b_1\\) minimize sum squared distance points line (called ordinary least squares):discusses previously, actually calculate \\(\\beta_0\\) \\(\\beta_1\\) need observe entire population. Instead, estimate quantities sample data . essentially trying find line fits data best. can thought line closest points sense. Given particular line particular point, think far point line? way think terms want model. Recall \\(x\\) predictor variable, \\(Y\\) response. linear regression context, set usually \\(x\\) something known beforehand, one goals predict response \\(Y\\). sense, way think good fitting line one vertical distance points line small.Residual: vertical distance point line. \\(^{th}\\) residual defined follows: \\[e_i=y_i-(b_0+b_1x_i)\\] \\(b_0\\) \\(b_1\\) intercept slope line consideration.","code":""},{"path":"slr.html","id":"notes","chapter":"4 Simple Linear Regression","heading":"Notes:","text":"vertical error measure reasonable predictor-response relationship. interested studying relationship height shoe size, vertical error idea doesn’t really exist (don’t think variables explanatory response - though certainly model approximate linear relationship). good fit might based perpendicular distance point line. reason vertical error model isn’t always ideal don’t always naturally consider one variables explanatory response. true, however, can (often ) model relationships variables don’t natural predictor - response relationship. , linear models cover, error (variable) term measured vertical direction.Now, find “best” fitting line, search line smallest residuals sense. particular, goal try find line minimizes following quantity: \\[Q=\\sum e_i^2 = \\sum (y_i-(b_0+b_1x_i))^2.\\]SSE: Sum squared errors (residuals), measure well line fits. SSE value \\(Q\\) optimal values \\(b_0\\) \\(b_1\\) given dataset.Finding \\(b_0\\) \\(b_1\\) minimize Q becomes calculus problem.\n\\[\\frac{dQ}{db_0}=-2\\sum (y_i-(b_0+b_1x_i)),\\qquad \\frac{dQ}{db_1}=-2\\sum\nx_i(y_i-(b_0+b_1x_i))\\] Setting equal 0 solving \\(b_0\\) \\(b_1\\) yields optimal values, denote \\(b_0\\) \\(b_1\\)Least Squares Estimates: \\(b_0\\) \\(b_1\\) (follows) minimize sum squared residuals, given :\n\\[b_0=\\bar{y}-b_1\\bar{x}, \\qquad b_1=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\n(x_i-\\bar{x})^2}\\] (Sometimes \\(b_0\\) \\(b_1\\) referred \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1.)\\) now can write \\[SSE=\\sum (y_i-(b_0+b_1x_i))^2\\] can note already knew discussion. switch roles \\(x\\) \\(Y\\), best fitting line different. relationship actually symmetric, switching roles \\(x\\) \\(Y\\) give slope \\(1/b_1\\), case.question might ask chose minimize \\(\\sum e_i^2\\) opposed perhaps \\(\\sum |e_i|\\), \\(|\\cdot|\\) denotes absolute value. original motivation math much easier first case, however, can shown “nice” situations, statistical properties better well.Consider good applet visualizing concept minimizing sums squares. Note applet also allows visualization line created minimizing sum absolute errors.Keep mind following notation:\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 x_i \\mbox{   true mean response}\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i \\mbox{   estimate mean response}\\\\\n &=& \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\ne_i &=& y_i - \\hat{y}_i\\\\\n\\epsilon_i &=& y_i - E[Y_i]\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"variance-sigma2","chapter":"4 Simple Linear Regression","heading":"4.1.2 Variance: \\(\\sigma^2\\)","text":"one variable (e.g., credit card balance), estimate variability response variable \\[\\mbox{sample variance} = \\frac{SSTO}{n-1} = \\frac{1}{(n-1)} \\sum_{=1}^n (y_i - \\overline{y})^2,\\] SSTO = sum squares total.regression setting, interested variance error terms (around line). particular, variance observed \\(y_i\\) around line given \\(\\sigma^2\\). estimate \\(\\sigma^2\\) using observed variability around line - residual. \\[SSE = \\sum_{=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{=1}^n e_i^2,\\] SSE sum squared errors (sometimes called sum square residuals). Note estimated two parameters, degrees freedom \\(df = n-2\\). best estimate \\(\\sigma^2\\) Mean Squared Error (MSE): \\[s^2 = MSE = \\frac{SSE}{n-2} = \\frac{1}{n-2} \\sum_{=1}^n (y_i - \\hat{y}_i)^2.\\] MSE = mean squared error.","code":""},{"path":"slr.html","id":"normal-errors-model","chapter":"4 Simple Linear Regression","heading":"4.2 Normal Errors Model","text":"least squares regression previous modeling hold probability model. However, inference easiest given normal probability model. Given linear model population:\n\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\]\n\\[\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\]","code":""},{"path":"slr.html","id":"important-features-of-the-model-with-normal-errors","chapter":"4 Simple Linear Regression","heading":"4.2.1 Important Features of the Model with Normal Errors","text":"\\(\\epsilon_i\\) random error term (, \\(\\epsilon_i\\) random variable).\\(Y_i\\) random variable \\(\\epsilon_i\\).assume \\(E[\\epsilon_i]=0\\), therefore \\(E[Y_i | x_i] = \\beta_0 + \\beta_1 x_i\\). (E expected value can thought long run average population mean.) , relationship explanatory response variables linear.\\(\\epsilon_i\\) amount \\(Y_i\\) values exceed fall short regression line.\\(var\\{Y_i | x_i\\} = \\sigma^2\\{Y_i | x_i \\} = \\sigma^2\\{\\epsilon_i\\} = \\sigma^2 \\rightarrow\\) constant variance around regression line.\\(\\sigma\\{Y_i, Y_j\\} = \\sigma\\{\\epsilon_i, \\epsilon_j \\} = 0 \\rightarrow\\) error terms uncorrelated.\\(\\epsilon_i \\sim N(0, \\sigma^2)\\), error terms normally distributed.\\(Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)\nFigure 1.2: Figs 1.6 Kutner et al. (2004).\n","code":""},{"path":"slr.html","id":"technical-conditions","chapter":"4 Simple Linear Regression","heading":"4.2.2 Technical Conditions","text":"Though use least squares regression criterion fit line observed data, setting series conditions allows us inference model. conditions crucial check whenever regression, satisfied, nothing data real meaning. , interpretations won’t make sense, inference won’t valid. conditions:Condition Linearity relationship actually linear:\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\] doesn’t make sense fit line data don’t believe relationship linear. \\(\\epsilon\\) term random variable, thus, two individuals measured value \\(x_i\\), \\(Y_i\\) , general, different. \\(Y_i\\) \nfunction \\(\\epsilon_i\\), \\(Y_i\\) also random variable.Furthermore, assuming mean \\(\\epsilon_i\\) 0. tells us fixed value \\(x_i\\), average value \\(Y_i\\) given \\(\\beta_0+\\beta_1 x_i\\). write \n\\[E[Y_i | x_i] = \\mu_{Y_i|x_i}=\\beta_0+\\beta_1 x_i\\] \\(\\mu\\) represents mean. fitted values estimates mean \\(Y_i\\) plugged-value \\(x_i\\).Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2\\{Y_i|x_i\\}=\\sigma^2.\\]Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2\\{Y_i|x_i\\}=\\sigma^2.\\]Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .","code":""},{"path":"slr.html","id":"maximum-likelihood","chapter":"4 Simple Linear Regression","heading":"4.2.3 Maximum Likelihood","text":"won’t cover details maximum likelihood. However, worth pointing maximum likelihood methods used many different areas statistics quite powerful. Additionally, simple linear regression case normal errors, maximum likelihood estimates turn exactly least squares estimates.","code":""},{"path":"slr.html","id":"reflection-questions-1","chapter":"4 Simple Linear Regression","heading":"4.3  Reflection Questions","text":"condition linear relationship appropriate?go estimating \\(\\beta_i, =0,1\\)?close estimates actual population values \\(\\beta_i, =0,1\\)?estimated function, actually interpret ?linear model conditions important inference?","code":""},{"path":"slr.html","id":"ethics-considerations","chapter":"4 Simple Linear Regression","heading":"4.4  Ethics Considerations","text":"matter technical conditions violated reporting analysis / model?technical conditions matter fitting line? Inference line? Neither? ?strong linear relationship predictor response variables found, mean predictor variables causes response?Simple Linear Regression called “simple?” model easy? (Spoiler: .)","code":""},{"path":"slr.html","id":"r-code-slr","chapter":"4 Simple Linear Regression","heading":"4.5 R code: SLR","text":"","code":""},{"path":"slr.html","id":"example-credit-scores","chapter":"4 Simple Linear Regression","heading":"4.5.1 Example: Credit Scores","text":"Consider dataset ISLR credit scores. don’t know sampling mechanism used collect data, unable generalize model results larger population. However, can look relationship variables build linear model. Notice lm() command always form: lm(response ~ explanatory).broom package three important functions:tidy() reports information based explanatory variableglance() reports information based overall modelaugment() reports information based observationWe can assume population model underlying relationship Limit average Balance. example, possible true (unknown) population model : \\[ E(Balance) = -300 + 0.2 \\cdot Limit.\\] Note consider Balance random Limit fixed. Also, note order inference, see error terms normally distributed around regression line constant variance values x.Consider someone $5,000 Limit credit card balance $1,000. population error term person : \\[\\epsilon_{5000 bal} = 1000 - [-300 + 0.2 \\cdot 5000] =  \\$700.\\]Consider someone $2,000 Limit credit card balance $50. population error term person : \\[\\epsilon_{2000 bal} = 50 - [-300 + 0.2 \\cdot 1000] =  -\\$50.\\]","code":"\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ISLR)\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) -293.     26.7         -11.0 1.18e- 24\n## 2 Limit          0.172   0.00507      33.9 2.53e-119\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.743         0.742  234.     1148. 2.53e-119     1 -2748. 5502. 5514.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  augment()## # A tibble: 400 × 8\n##    Balance Limit .fitted  .resid    .hat .sigma     .cooksd .std.resid\n##      <int> <int>   <dbl>   <dbl>   <dbl>  <dbl>       <dbl>      <dbl>\n##  1     333  3606    326.    6.87 0.00310   234. 0.00000135      0.0294\n##  2     903  6645    848.   55.3  0.00422   234. 0.000119        0.237 \n##  3     580  7075    922. -342.   0.00507   233. 0.00548        -1.47  \n##  4     964  9504   1338. -374.   0.0132    233. 0.0174         -1.61  \n##  5     331  4897    548. -217.   0.00251   234. 0.00109        -0.929 \n##  6    1151  8047   1088.   62.6  0.00766   234. 0.000280        0.269 \n##  7     203  3388    289.  -85.7  0.00335   234. 0.000227       -0.368 \n##  8     872  7114    928.  -56.2  0.00516   234. 0.000151       -0.241 \n##  9     279  3300    274.    5.39 0.00347   234. 0.000000929     0.0231\n## 10    1350  6819    878.  472.   0.00454   233. 0.00937         2.03  \n## # … with 390 more rows\nCredit %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) +\n  xlab(\"Credit limit (in $)\") +\n  ylab(\"Credit card balance (in $)\")"},{"path":"infslr.html","id":"infslr","chapter":"5 Inference on SLR Parameters","heading":"5 Inference on SLR Parameters","text":"normal error regression model important estimating line. computer happily minimize sum squares model choose. However, inference linear parameters, must normal error regression model. , unless otherwise stated, assume normal error regression model holds.\\[Y_i = \\beta_0 + \\beta_1 x_i  + \\epsilon_i\\] \\(x_i\\) known, \\(\\beta_0, \\beta_1\\) parameters, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently.Sampling Distribution: sampling distribution distribution statistic measured repeated random samples population. Consider sampling applet provides nice visual sampling distribution.","code":""},{"path":"infslr.html","id":"inference-on-beta_1","chapter":"5 Inference on SLR Parameters","heading":"5.1 Inference on \\(\\beta_1\\)","text":"recall: \\[b_1 = \\frac{\\sum(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum(x_i - \\overline{x})^2}\\]\nalso know ,\n\\[\\begin{eqnarray*}\nE[b_1] &=& \\beta_1\\\\\n\\sigma^2\\{b_1\\} &=& \\frac{\\sigma^2}{\\sum(x_i - \\overline{x})^2}\\\\\ns^2\\{ b_1 \\} &=& \\frac{MSE}{\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]\nvariability slope becomes smaller \\(x_i\\) values spread . make sense intuitively \\(x_i\\) values spread , slight deviations measurements (.e., different random samples) won’t change slope line much. \\(x_i\\) exist narrow range, easy get vastly different \\(b_1\\) values depending particular random sample.","code":""},{"path":"infslr.html","id":"distribution-of-b_1","chapter":"5 Inference on SLR Parameters","heading":"Distribution of \\(b_1\\)","text":"distribution \\(b_1\\) (condition \\(E[b_1] = \\beta_1\\) normal error regression model), seem familiar : \\[T = \\frac{b_1 - \\beta_1}{s\\{b_1\\}} \\sim t_{n-2}.\\]\nRecall use t distribution standardized variable dividing constant. Instead, divide standard error induces extra variability thus t distribution.Indeed, \\(H_0: \\beta_1=0\\) true, \n\\[\\begin{eqnarray*}\nT = \\frac{b_1 - 0}{s\\{b_1\\}} \\sim t_{n-2}\n\\end{eqnarray*}\\]\nNote degrees freedom now \\(n-2\\) estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). reject null hypothesis, \\(b_1\\) leads us t-statistic larger expect random chance (.e., p-value small).p-value: p-value probability observed data extreme , fact, null hypothesis true.","code":""},{"path":"infslr.html","id":"ci-for-beta_1","chapter":"5 Inference on SLR Parameters","heading":"CI for \\(\\beta_1\\)","text":"many statistics know standard error, can create intervals give us confidence statements (parameter) making. general, confidence intervals form:\\((1-\\alpha)100\\%\\) confidence interval slope parameter, \\(\\beta_1\\): \n\\[\\begin{eqnarray*}\nb_1 &\\pm& t^*_{\\alpha/2,n-2} s\\{b_1\\}\\\\\nb_1 &\\pm& t^*_{\\alpha/2, n-2} \\sqrt{MSE/\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]Remember \\(\\beta_1\\) random. randomness comes data, endpoints CI random. interpreting CI, give interval plausible values \\(\\beta_1\\) rejected done hypothesis test. Alternatively, think CI set values \\(\\beta_1\\) fairly certain contains \\(\\beta_1\\). repeat process many times, \\((1-\\alpha)100\\%\\) time interval captures true parameter., regression finding linear relationships. claim relationship explanatory response variables causative. randomized studies able find causative mechanisms.","code":""},{"path":"infslr.html","id":"parameter-interpretation","chapter":"5 Inference on SLR Parameters","heading":"Parameter Interpretation","text":"confidence interval gives us estimate parameter(s) model, goal understand interpretation quantities.Intercept \\(\\beta_0\\): average value \\(Y\\) \\(x\\) 0. Often, intercept interpretable . example, studying relationship height weight, \\(\\beta_0\\) average weight someone 0 inches tall. Nonsense. often \\(\\beta_0\\) placeholder, number needs specified interpretation.Slope \\(\\beta_1\\): \\(\\beta_1\\) can interpreted increase average \\(Y\\) \\(x\\) incremented single unit. \\[E[Y|x+1]-E[Y|x]=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1x)=\\beta_1\\]Variance \\(\\sigma^2\\): average squared deviation observation line.","code":""},{"path":"infslr.html","id":"estimating-a-response","chapter":"5 Inference on SLR Parameters","heading":"5.2 Estimating a response","text":"","code":""},{"path":"infslr.html","id":"interpolation-vs-extrapolation","chapter":"5 Inference on SLR Parameters","heading":"5.2.1 Interpolation vs Extrapolation","text":"One big problem using regression model condition linearity whether actually holds. result, attempting say something \\(Y\\) value \\(x^*\\) don’t typically data, can reasonably long \\(x^*\\) contained range data . called interpolation. Extrapolation hand trying use regression line outside range data. problem extrapolation evidence, ability check, whether linear condition holds beyond range data. Consider following example:interested amount crop produced single plant function amount compost added soil. take several plants, apply small, differing, amounts compost plants. see compost increases, output plant increases well, seemingly linear fashion. Naivete might lead us think , using regression line, can use 100 kilograms compost single plant receive huge amount crop return. truth, kill plant. evidence relationship linear anything outside range data, thus extrapolation mistake.interpolation, data support linear relationship, thus fine. trying say something happen particular values \\(x\\), call \\(x_h\\), need think exactly want say. know ahead time want say, can design experiment smart ways, see.","code":""},{"path":"infslr.html","id":"prediction-intervals-vs-confidence-intervals","chapter":"5 Inference on SLR Parameters","heading":"5.2.2 Prediction Intervals vs Confidence Intervals","text":"linear regression line gives us guess mean response individual particular value \\(x_h\\).conditions give us \\[E[Y|x]=\\beta_0+\\beta_1x\\]Plugging estimators, get \\[\\hat{y_i}=b_0+b_1 x_i\\] fitted value. However, accurate guess?confidence interval gives range plausible values \\(E[Y|x]\\). , mean response fixed value \\(x\\). confidence interval differs prediction interval, intended contain mean response, rather value response next individual observed value \\(x_h\\). result, prediction interval need larger confidence interval.","code":""},{"path":"infslr.html","id":"variability-of-point-estimates","chapter":"5 Inference on SLR Parameters","heading":"5.2.3 Variability of point estimates","text":"following variances \\(b_0\\), \\(b_1\\), fitted value \\(x_h\\): \\(\\hat{y}_{x_h}\\), new value \\(x_h\\): \\(\\hat{y}_{x_h(new)}\\).\\(\\sigma^2\\) variance errors.\\[\\begin{eqnarray*}\n\\mbox{var}(b_0)&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(b_1)&=&\\frac{\\sigma^2}{\\sum(x_i-\\bar{x})^2}\\\\\n\\mbox{var}(\\hat{y}_{x_h})&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(\\hat{y}_{x_h(new)})&=& \\sigma^2 + \\mbox(var)(\\hat{y}_{x_h}) = \\sigma^2\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\n\\end{eqnarray*}\\]quantities estimated replacing \\(\\sigma^2\\) guess, \\(MSE\\). \\(b_0\\) fitted value (\\(x_h=0\\)), variability \\(b_0\\) special case 3rd formula. difference last two one \\(\\sigma^2\\). make sense, variance next observation (point) variance mean (.e., predicted regression line) plus variance error top mean.","code":""},{"path":"infslr.html","id":"standard-error","chapter":"5 Inference on SLR Parameters","heading":"5.2.3.1 Standard Error:","text":"phrase “standard error” indicates variability (square root variance) statistic situation variability estimate (, MSE used instead \\(\\sigma^2\\)). SE quantities therefore given :\\[\\begin{eqnarray*}\n\\mbox{SE}(b_0)&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(b_1)&=& \\sqrt{\\frac{MSE}{\\sum(x_i-\\bar{x})^2}}\\\\\n\\mbox{SE}(\\hat{y}_{x_h})&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(\\hat{y}_{x_h(new)})&=& \\sqrt{MSE + \\mbox(SE)(\\hat{y}_{x_h})^2} = \\sqrt{MSE\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\n\\end{eqnarray*}\\]Confidence intervals essentially best guess plus minus two standard errors. exact, instead 2, use \\(qt(.975,n-p)\\) \\(p\\) number coefficients estimated (SLR \\(p=2\\)). idea smaller standard errors, narrower confidence intervals , information .","code":""},{"path":"infslr.html","id":"notes-1","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"prediction future (mean) response accurate \\(x_h = \\overline{x}\\). Think behavior regression line away \\(\\overline{x}\\). line much variable extremes.Confidence limits (.e., % coverage) \\(E[Y_h], \\beta_0, \\beta_1\\) sensitive departures normality. Especially large sample sizes (central limit theorem).Coverage percentages \\(\\hat{y}_h\\) sensitive departures normality. central limit theorem estimating average.Confidence limits apply one confidence interval (.e., entire line). won’t simultaneously 95% confident lots different intervals. ’ll address issue later.","code":""},{"path":"infslr.html","id":"anova-approach-to-regression","chapter":"5 Inference on SLR Parameters","heading":"5.3 ANOVA approach to regression","text":"mentioned previously, can think variability \\(Y\\) total variability. measure variability without knowing anything explanatory variable(s). , knowledge explanatory variables helps us predict \\(Y\\), , remove variability associated response. Consider following terms\\[\\begin{eqnarray*}\nSSTO &=& \\sum (y_i - \\overline{y})^2\\\\\nSSE &=& \\sum (y_i - \\hat{y}_i)^2\\\\\nSSR &=& \\sum (\\hat{y}_i - \\overline{y})^2\n\\end{eqnarray*}\\]SSTO sum squares total; SSE sum squared errors; SSR sum squares regression line. Unsquared, residuals nice relationship:\\[\\begin{eqnarray*}\ny_i - \\overline{y} &=& \\hat{y}_i - \\overline{y} + y_i - \\hat{y}_i\\\\\n\\mbox{total deviation} &=& \\mbox{dev reg around mean} + \\mbox{dev around line}\n\\end{eqnarray*}\\]\nresiduals squared, obvious \\[SSTO = SSR + SSE.\\] However, can derive relationship using algebra:\n\\[\\begin{eqnarray*}\n\\sum(y_i - \\overline{y})^2 &=& \\sum [ (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)]^2\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2 + 2 \\sum (\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2\\\\\n\\end{eqnarray*}\\]\nlast term zeroed using equations (1.17) (1.20) ALSM (pgs 23-24).","code":""},{"path":"infslr.html","id":"mean-squares","chapter":"5 Inference on SLR Parameters","heading":"5.3.1 Mean Squares","text":"Sums squares increasing number data values. accommodate number observations, use mean squares instead sums square","code":""},{"path":"infslr.html","id":"notes-2","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"MSE estimates \\(\\sigma^2\\) regardless whether \\(\\beta_1 = 0\\).\\(\\beta_1=0\\), MSR also estimates \\(\\sigma^2\\).comparison MSR MSE seem indicate whether \\(\\beta_1=0\\).\nNote can think MSR variability regression line around line \\(\\overline{y}\\). \\(\\beta_1=0\\), regression line varies MSR measuring natural variability error terms (\\(\\sigma^2\\)). \\(\\beta_1 \\ne 0\\), \\(b_1\\) values still vary naturally PLUS bit difference line \\(\\beta_1\\) line \\(\\mu_Y\\).","code":""},{"path":"infslr.html","id":"f-test-of-beta_1-0-versus-beta_1-ne-0","chapter":"5 Inference on SLR Parameters","heading":"5.3.2 F test of \\(\\beta_1 = 0\\) versus \\(\\beta_1 \\ne 0\\)","text":"\\[\\begin{eqnarray*}\nH_0: \\beta_1 = 0\\\\\nH_a: \\beta_1 \\ne 0\n\\end{eqnarray*}\\]\ntest statistic \\[F = \\frac{MSR}{MSE} = \\frac{\\sum(\\hat{y}_i - \\overline{y})^2}{\\sum(y_i - \\hat{y}_i)^2 / (n-2)}.\\] Large values \\(F\\) support \\(H_a\\), values \\(F\\) close 1 support \\(H_0\\). \\(H_0\\) true, \\[F \\sim F_{1,n-2}.\\] Note F-test always one-sided test (meaning reject BIG values \\(F\\)), though assessing two-sided hypothesis.Notice anova() output includes information sums squares F-test calculate \\(R^2\\). Also, tidy() function broom output dataframe easy work .","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA"},{"path":"infslr.html","id":"equivalence-of-f-and-t-tests","chapter":"5 Inference on SLR Parameters","heading":"5.3.3 Equivalence of F and t-tests","text":"\\[F = \\frac{SSR}{SSE/(n-2)} = \\frac{b_1^2 \\sum(x_i - \\overline{x})^2}{MSE} = \\frac{b_1^2}{MSE/\\sum(x_i - \\overline{x})^2} = \\bigg(\\frac{b_1}{s\\{b_1\\} }\\bigg)^2 = (T)^2\\]\n’re going continue use test models get complicated. general strategy always :Fit full model: \\(SSE_{full} = \\sum(y_i - b_0 - b_1 x_i)^2 = \\sum(y_i - \\hat{y}_i)^2\\)Fit reduced model (\\(H_0\\)): \\(SSE_{reduced} = \\sum(y_i - b_0)^2 = \\sum(y_i - \\overline{y})^2 = SSTO\\)\\(F = \\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}} \\div \\frac{SSE_{full}}{df_{full}} = \\frac{MSR}{MSE}\\)","code":""},{"path":"infslr.html","id":"descriptive-measures-of-linear-association","chapter":"5 Inference on SLR Parameters","heading":"5.4 Descriptive Measures of Linear Association","text":"discuss r (correlation) \\(R^2\\) (coefficient determination) descriptive measures linear association typically interested estimating parameter. Instead, \\(r\\) \\(R^2\\) tell us well linear model fits data.","code":""},{"path":"infslr.html","id":"correlation","chapter":"5 Inference on SLR Parameters","heading":"5.4.1 Correlation","text":"Consider scatterplot, variability directions: \\((x_i - \\overline{x}) \\ \\& \\ (y_i - \\overline{y})\\). data shown represent crop types taken World Data part Tidy Tuesday. point plot different country. x y variables represent proportion total yield last 50 years due crop type.\nFigure 5.1: % total yield different crops (across last 50 years). point represents country. Now lines average x average y values superimposed onto plots.\nred dot (plot), consider distance observation \\(\\overline{X}\\) line \\(\\overline{Y}\\) line. observation (red dot) ? ? one ?particular red dot (observation) contribute correlation? positive way (make \\(r\\) bigger)? negative way (make \\(r\\) smaller)?Positive Relationship: \\(x\\) increases, \\(Y\\) also tends increase, two variables said positive relationship (example: shoe size height).Negative Relationship: \\(x\\) increases, \\(Y\\) tends decrease, two variables said negative relationship (example: outside temperature heating oil used).variables positive relationship, \\(r=\\sqrt{R^2}\\). variables negative relationship, \\(r=-\\sqrt{R^2}\\).\\(r\\) can calculated directly well, given following formula:\\[\\begin{eqnarray*}\n\\mbox{sample covariance}&&\\\\\ncov(x,y) &=& \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\\n\\mbox{sample correlation}&&\\\\\nr(x,y) &=& \\frac{cov(x,y)}{s_x s_y}\\\\\n&=& \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\\n&=& \\frac{\\sum[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\end{eqnarray*}\\]numerator correlation describes relationship \\(x\\) \\(y\\). , \\(x\\) tends large (mean), \\(y\\) also tends large (mean), product two positive numbers, positive. Likewise, \\(x\\) small, \\(y\\) also tends small, product two negative numbers, positive. positive relationship result lot positive numbers summed numerator, thus \\(r\\) positive.relationship negative, one tend negative positive, thus sum involve lot negative terms causing \\(r\\) negative. denominator always positive.One thing note \\(r\\) affected choice label predictor label response. roles \\(x\\) \\(y\\) switched, \\(r\\) remain unaffected. Thus, \\(r\\), unlike value line, symmetric.\\(-1 \\leq r \\leq 1\\).\\(b_1 = r \\frac{s_y}{s_x}\\)\\(r=0, b_1=0\\)\\(r=1, b_1 > 0\\) can anything!\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)","code":""},{"path":"infslr.html","id":"coefficient-of-determination","chapter":"5 Inference on SLR Parameters","heading":"5.4.2 Coefficient of Determination","text":"coefficient determination, given \\(R^2\\), nice interpretation, proportion variability \\(y\\) explained variable \\(x\\). Recall, \\(SSE\\), sum squared errors (residuals) measure amount variable remaining data accounting information \\(x\\). \\(SSTO\\) total sums squares, measured total amount variability variable \\(y\\). coefficient determination given \n\\[R^2=1-\\frac{SSE}{SSTO}\\]\n, defining \\(SSR\\) regression sum squares (amount variation explained regression) \\[R^2=\\frac{SSR}{SSTO}.\\]Limitations:\n1. High \\(R^2\\) necessarily produce good “predictions.” lot variability around line (.e., within data), can wide prediction intervals response.\n2. High \\(R^2\\) necessarily mean line good fit. Quadratic () relationships can sometimes lead high \\(R^2\\). Additionally, one outlier can huge effect value \\(R^2\\).\n3. \\(R^2 \\approx 0\\) mean relationship \\(x\\) \\(y\\). Instead, \\(x\\) \\(y\\) might perfect quadratic relationship.","code":""},{"path":"infslr.html","id":"reflection-questions-2","chapter":"5 Inference on SLR Parameters","heading":"5.5  Reflection Questions","text":"different ways use inference model parameters?difference prediction confidence interval? used?sums squares broken meaningful pieces? differences SSTO, SSE, SSR?interpretation \\(R^2\\)?","code":""},{"path":"infslr.html","id":"ethics-considerations-1","chapter":"5 Inference on SLR Parameters","heading":"5.6  Ethics Considerations","text":"technical conditions violated, CI tell us slope parameter? normality condition?technical conditions violated PI tell us predicted values? normality condition?confounding variables might exist link Limit Balance positively correlated causal?population might ames data representative ? population might Credit data representative ? (Hint: look data documentation typing ?ames ?Credit.)","code":""},{"path":"infslr.html","id":"ames-inf","chapter":"5 Inference on SLR Parameters","heading":"5.7 R: SLR Inference","text":"","code":""},{"path":"infslr.html","id":"cis","chapter":"5 Inference on SLR Parameters","heading":"5.7.1 CIs","text":"Also available broom CIs coefficients. Consider ames dataset available openintro package.Data set contains information Ames Assessor’s Office used computing assessed values individual residential properties sold Ames, IA 2006 2010.reasons discuss coming chapters, ’ll consider \\(\\ln\\) price home, also consider homes less 3000 square feet.R t-test automatically, done hand using provided SE (available column called std.error).\\[\\begin{eqnarray*}\nH_0:&& \\beta_1 = 0\\\\\nH_a:&& \\beta_1 \\ne 0\\\\\nt &=& \\frac{0.000613 - 0}{0.0000114} = 53.86\\\\\np-value &=& 2 P(t_{2902} \\geq 53.86) = 2*(1-pt(53.86, 2902)) = \\mbox{small}\\\\\n\\end{eqnarray*}\\]p-value small, reject null hypothesis: model linear relationship area price_ln, slope coefficient must different zero (greater zero one-sided test). Note knowing positive relationship tell us price_ln result area. , reason believe causative mechanism.90% confidence interval slope coefficient, \\(\\beta_1\\) (0.000595, 0.000632). true population slope model area price_ln somewhere (0.000595, 0.000632). Note even though values seem small, significantly (necessarily substantially) away zero. tempted confuse small zero, magnitude slope coefficient depends heavily units measurement variables.","code":"\nlibrary(openintro)\names_inf <- ames %>%\n  filter(area <= 3000) %>%\n  mutate(price_ln = log(price))\names_inf %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\names_inf %>%\n  lm(price_ln ~ area, data = .) %>% \n  tidy(conf.int = TRUE, conf.level = 0.9)## # A tibble: 2 × 7\n##   term         estimate std.error statistic p.value  conf.low conf.high\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0 11.1      11.1     \n## 2 area         0.000614 0.0000114      53.9       0  0.000595  0.000632"},{"path":"infslr.html","id":"predictions","chapter":"5 Inference on SLR Parameters","heading":"5.7.2 Predictions","text":"","code":""},{"path":"infslr.html","id":"predicting-ames","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.1 Predicting Ames","text":"Fortunately, R allows creating mean prediction intervals. need create new data set variable name predictor, value interested , might call new_ames. use augment() give either confidence prediction interval, follows., create intervals entire range explanatory variables:, turns , easier ways computer find confidence prediction intervals:Also, graphs can made, ’ll need keep output (use entire dataset instead new_ames).","code":"\n# store the linear model object so that we can use it later.\names_lm <- lm(price_ln ~ area, data = ames_inf)\n\n# create a new dataframe\nnew_ames <- data.frame(area = c(1000, 1500, 2000))\n\n# get df from the model\names_df <- ames_lm %>% glance() %>% select(df.residual) %>% pull()\names_df## [1] 2902\n# new data predictions\names_pred <- ames_lm %>% \n  augment(newdata = new_ames, type.predict = \"response\", se_fit = TRUE)\names_pred## # A tibble: 3 × 3\n##    area .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000    11.7 0.00760\n## 2  1500    12.0 0.00527\n## 3  2000    12.3 0.00792\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, ames_df)\ncrit_val## [1] 1.96\n# SE of the mean response\nse_fit <- ames_pred %>% select(.se.fit) %>% pull()\nse_fit##       1       2       3 \n## 0.00760 0.00527 0.00792\n# esimate of the overall variability, sigma\names_sig <- ames_lm %>% glance() %>% select(sigma) %>% pull()\names_sig## [1] 0.284\n# calculate the SE of the predictions\nse_pred <- sqrt(ames_sig^2 + se_fit^2)\nse_pred##     1     2     3 \n## 0.284 0.284 0.284\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\names_pred <- ames_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\names_pred## # A tibble: 3 × 7\n##    area .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000    11.7 0.00760     11.2     12.3     11.7     11.7\n## 2  1500    12.0 0.00527     11.5     12.6     12.0     12.0\n## 3  2000    12.3 0.00792     11.8     12.9     12.3     12.3\names_pred_all <- ames_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(ames_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\names_pred_all %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")\names_lm %>% \n  augment(newdata = new_ames, interval = \"confidence\")## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.7   11.7\n## 2  1500    12.0   12.0   12.0\n## 3  2000    12.3   12.3   12.3\names_lm %>% \n  augment(newdata = new_ames, interval = \"prediction\")  ## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.2   12.3\n## 2  1500    12.0   11.5   12.6\n## 3  2000    12.3   11.8   12.9\names_conf <- ames_lm %>% \n  augment(interval = \"confidence\")\n\names_pred <- ames_lm %>% \n  augment(interval = \"prediction\")  \n\names_pred %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2) + \n  geom_ribbon(ames_conf, mapping = aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"predicting-credit","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.2 Predicting Credit","text":"contrast ames data, note predictions (particulary CI around line) drastically changes sample size small., create intervals entire range explanatory variables:","code":"\nlibrary(ISLR)  # source of the Credit data\n# store the linear model object so that we can use it later.\ncredit_lm <- lm(Balance ~ Limit, data = Credit)\n\n# create a new dataframe\nnew_credit <- data.frame(Limit = c(1000, 3000, 7000))\n\n# get df from the model\ncredit_df <- credit_lm %>% glance() %>% select(df.residual) %>% pull()\ncredit_df## [1] 398\n# new data predictions\ncredit_pred <- credit_lm %>% \n  augment(newdata = new_credit, type.predict = \"response\", se_fit = TRUE)\ncredit_pred## # A tibble: 3 × 3\n##   Limit .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000   -121.    22.2\n## 2  3000    222.    14.6\n## 3  7000    909.    16.4\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, credit_df)\ncrit_val## [1] 1.97\n# SE of the mean response\nse_fit <- credit_pred %>% select(.se.fit) %>% pull()\nse_fit##    1    2    3 \n## 22.2 14.6 16.4\n# esimate of the overall variability, $\\sigma$\ncredit_sig <- credit_lm %>% glance() %>% select(sigma) %>% pull()\ncredit_sig## [1] 234\n# calculate the SE of the predictions\nse_pred <- sqrt(credit_sig^2 + se_fit^2)\nse_pred##   1   2   3 \n## 235 234 234\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\ncredit_pred <- credit_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\ncredit_pred## # A tibble: 3 × 7\n##   Limit .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000   -121.    22.2    -582.     340.    -165.    -77.4\n## 2  3000    222.    14.6    -238.     682.     193.    251. \n## 3  7000    909.    16.4     448.    1369.     876.    941.\ncredit_pred_all <- credit_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(credit_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\ncredit_pred_all %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"anova-output","chapter":"5 Inference on SLR Parameters","heading":"5.7.3 ANOVA output","text":"Note tidy() creates dataframe slighlty easier work , regardless tidy() anova() function provides exact output.","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() ## Analysis of Variance Table\n## \n## Response: price_ln\n##             Df Sum Sq Mean Sq F value Pr(>F)    \n## area         1    233   233.4    2901 <2e-16 ***\n## Residuals 2902    234     0.1                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA\names_inf %>%\n  mutate(bedrooms = case_when(\n    Bedroom.AbvGr <=1 ~ \"1\",\n    Bedroom.AbvGr <=2 ~ \"2\",\n    Bedroom.AbvGr <=3 ~ \"3\",\n    TRUE ~ \"4+\"\n  )) %>%\n  ggplot(aes(x = area, y = price_ln, color = bedrooms)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)"},{"path":"diag1.html","id":"diag1","chapter":"6 Diagnostic Measures I","heading":"6 Diagnostic Measures I","text":"","code":""},{"path":"diag1.html","id":"model-conditions","chapter":"6 Diagnostic Measures I","heading":"6.1 Model Conditions","text":"linear relationshipconstant varianceindependent errorsnormal errorsno outliers (’s part normality condition)","code":""},{"path":"diag1.html","id":"notes-3","chapter":"6 Diagnostic Measures I","heading":"Notes:","text":"required conditions explanatory / predictor variable.explanatory variable binary, SLR becomes two-sided t-test.\nRemember, \\(x_i\\) constants, don’t think distribution. Also, allowed construct whatever \\(x_i\\) want. said,larger range \\(x_i\\) produce less variable predictions.However, outliers x-direction can influential.","code":""},{"path":"diag1.html","id":"residuals","chapter":"6 Diagnostic Measures I","heading":"6.1.1 Residuals","text":"\\[\\begin{eqnarray*}\n\\mbox{residual: } e_i &=& y_i -  \\hat{y}_i \\ \\ \\ \\mbox{can measure}\\\\\n\\mbox{error term: } \\epsilon_i &=& y_i - E[Y_i] \\ \\ \\ \\mbox{measure}\n\\end{eqnarray*}\\]mean: \\(\\overline{e} = 0\\) (definition!). Therefore, average residuals provides information whether \\(E[\\epsilon]=0\\).\n(note: \\(\\sum e_i = \\sum(y_i - b_0 - b_1 x_i) = 0\\) \\(\\frac{\\delta Q}{\\delta \\beta_0} = 0\\).)variance: \\[s^2 = \\frac{1}{(n-2)} \\sum (y_i - \\hat{y}_i)^2 = \\frac{1}{(n-2)} \\sum (e_i)^2 = \\frac{1}{(n-2)} \\sum (e_i - \\overline{e})^2 = \\frac{SSE}{(n-2)} = MSE\\]\nnon-independent: \\(\\sum e_i=0\\), residuals independent. \\(\\epsilon_i\\), errors, assume independent.","code":""},{"path":"diag1.html","id":"semistudentized-residuals","chapter":"6 Diagnostic Measures I","heading":"semistudentized residuals","text":"\\[e_i^* = \\frac{e_i - \\overline{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\\] MSE isn’t quite variance \\(e_i\\). actually estimate variance \\(\\epsilon_i\\). good enough right now.","code":""},{"path":"diag1.html","id":"diagnostic-plots-of-residuals","chapter":"6 Diagnostic Measures I","heading":"6.2 Diagnostic Plots of Residuals","text":"[1] Plots residuals vs. predictor variables / explanatory variables.\n[3] Plots residuals vs. fitted values.\n[6] Box plot (histogram) residuals.plots show conditions don’t hold??\n1. Abandon regression model use something appropriate. interested coefficients: nonparametrics, time series, random forests. interested prediction, maybe okay conditions don’t hold? maybe (e.g., independence key!). prediction models include: random forests, support vector machines, neural networks, loess (smoothing).\n2. Transform variables model hold.","code":""},{"path":"diag1.html","id":"violating-linearity","chapter":"6 Diagnostic Measures I","heading":"Violating Linearity","text":"see: pattern scatterplot isn’t linearYou :\n- include \\(x^2\\) term function \\(x\\).\n- can linearize non-linear function, interpretation can get complicated.","code":""},{"path":"diag1.html","id":"violating-constant-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Constant Errors","text":"see:\n- typically see errors increase \\(x\\) increases (sometimes easier see absolute value residuals).\n- hugely serious problem (less efficient estimates, variance estimates correct):\n- direct approach use weighted least squares (won’t talk )\n- often transformations stabilize variance","code":""},{"path":"diag1.html","id":"violating-independent-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Independent Errors","text":"see:\n- typically due another variable (time, geographic location, etc.)\n- residual plotted variable.:\n- work model accounts correlated error structure (e.g., time series)","code":""},{"path":"diag1.html","id":"violating-normal-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Normal Errors","text":"see:\n- residual plots symmetric\n- general empirical rule: \\(68\\% \\pm \\sqrt{MSE}\\), \\(95\\% \\pm 2\\sqrt{MSE}\\) (concerned grossly different)\n- won’t cover normal probability plots (also called q-q plots):\n- non-normality non-constant variance often related, transformations typically fix .","code":""},{"path":"diag1.html","id":"having-outliers","chapter":"6 Diagnostic Measures I","heading":"Having Outliers","text":"see:\n- typically easiest see standardized studentized residuals\n- SLR resistant outliers\n- expect 95% studentized residuals within \\(\\pm 2\\).:\n- outliers can seriously deform least squares estimate. good reason keep value(s), might consider nonparametric method places less weight point.","code":""},{"path":"diag1.html","id":"why-do-we-plot-resid-vs.-fitted-and-not-vs.-observed","chapter":"6 Diagnostic Measures I","heading":"Why do we plot resid vs. fitted and not vs. observed?","text":"know \\(e_i\\) \\(\\hat{y}_i\\) uncorrelated (can shown using linear algebra, also note \\(\\sum e_i \\hat{y}_i = 0\\)). , go resid vs. fitted scatterplot resid vs. observed scatter plot, shift point x-direction () amount equal residual. residual negative, point shift left. residual positive, point shift right. thus create positively correlated relationship (resid observed). degree shift depend relative magnitudes residuals predicted values.\\(\\Rightarrow e_i \\mbox{ } y_i\\) correlated therefore independent. Consider two examples . examples, residual correlated response variable. However, easier see correlation residual also responsible relationship response variable explanatory variable.\nFigure 1.3: correlated data, hard see dependence response variable residuals. However, careful look third plot shows slightly stronger correlating response variable residuals fitted values residuals.\n\nFigure 1.4: uncorrelated data, much easier see dependence response variable residuals.\n","code":""},{"path":"diag1.html","id":"transformations","chapter":"6 Diagnostic Measures I","heading":"6.3 Transformations","text":"Important note!! idea behind transformations make model appropriate possible data hand. want find correct linear model; want conditions hold. trying find significant model big \\(R^2\\).\nFigure 6.1: Taken Applied Linear Statistical Models, 5th ed. Kutner et al. Figures 3.13 3.15.\n","code":""},{"path":"diag1.html","id":"correcting-condition-violations","chapter":"6 Diagnostic Measures I","heading":"6.4 Correcting Condition Violations","text":"noticed non-linear relationship, certainly think fit non-linear trend line. time, might consider fitting exponential\nrelationship, functional form. still called linear regression, transform data \nfitting linear model \\(f(y)\\) \\(g(x)\\). way, techniques theory still follow, least squares etc. \ninstance, might think fitting linear relationship new variables \\(y^*=\\sqrt{y}\\) \\(x\\), possibly \\(y\\) \\(x^*=x^2\\). Occasionally, might think transforming variables. One general rule follows:initial fit shows violation linear condition , best transform \\(x\\).initial fit shows violation linear condition well normality issues heteroscedasticity, transform \\(y\\) considered. hopefully transformation correct problems ., changing \\(x\\) changes shape relationship. Changing \\(Y\\) changes error structure (also often shape).Note: Formal testing violations model conditions usually good idea. ? Multiple testing problems arise (extra type errors); additional tests typically sensitive technical conditions; lose power detect differences interest.","code":""},{"path":"diag1.html","id":"gdp-example","chapter":"6 Diagnostic Measures I","heading":"6.4.1 GDP Example","text":"Consider following data collected World Bank 20201. data include GDP % Urban Population. description variables defined World Bank provided .2GDP: “GDP per capita gross domestic product divided midyear population. GDP sum gross value added resident producers economy plus product taxes minus subsidies included value products. calculated without making deductions depreciation fabricated assets depletion degradation natural resources. Data current U.S. dollars.”Urban Population (% total): “Urban population refers people living urban areas defined national statistical offices. calculated using World Bank population estimates urban ratios United Nations World Urbanization Prospects.”\nFigure 6.2: seems though original data don’t meet LINE conditions needed inference linear model.\nLet’s try transform variables get model seems conform LINE technical conditions.\nFigure 6.3: ln(urban) vs gdp: seems like taking natural log urban makes relationship worse.\nAlas, really seems like gdp variable problem urban variable. Let’s transform gdp instead.\nFigure 1.5: urban vs gdp^2: squaring gdp also makes relationship worse.\nneeded transformation spread small gdp values shrink large gdp values.\nFigure 1.6: urban vs sqrt(gdp): square root gdp seems help!\nnatural log stronger function square root (, shrink large values even .).\nFigure 1.7: urban vs ln(gdp): natural log gdp creates residual plot seems follow LINE technical conditions.\nBox-Cox transformations class transformations. Generally, \\(\\ln, \\exp\\), square root, polynomial transformations sufficient fit linear model satisfies necessary technical conditions. won’t spend time learning Box-Cox, can read learn transforming variables.","code":""},{"path":"diag1.html","id":"interpreting-regression-coefficients","chapter":"6 Diagnostic Measures I","heading":"Interpreting Regression Coefficients","text":"Example 1: Transforming \\(x\\): \\(x' = \\ln(x)\\) \\[E[Y] = \\beta_0  + \\beta_1 \\ln(x)\\] can interpret \\(\\beta_1\\) following way: every increase 1 unit \\(\\ln(x)\\), \\(E[Y]\\) increases \\(\\beta_1\\). isn’t meaningful statement. Instead, consider \\[E[Y| \\ln(2x)] - E[Y|\\ln(x)] = \\beta_1 \\ln(2).\\] can interpreted doubling \\(x\\) gives additive increase E[Y] \\(\\beta_1 \\ln(2)\\) units.Example 2: Transforming \\(Y\\): \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 x\\] Note also \\[ E[\\ln(Y)] = median(\\ln(Y))\\] distribution \\(\\ln(Y)\\) symmetric around regression line.\\[\\begin{eqnarray*}\nmedian(\\ln(Y)) = \\beta_0 + \\beta_1 x\\\\\nmedian(Y) = e^{\\beta_0} e^{\\beta_1 x}\\\\\n\\frac{median(Y| x+1)}{median(Y| x)} = e^{\\beta_1}\\\\\n\\end{eqnarray*}\\]\nincrease 1 unit x associated multiplicative change \\(e^{\\beta_1}\\) median(\\(Y\\)). Important keep mind \n\\[\\begin{eqnarray*}\nE[\\ln(Y)] \\ne \\ln(E[Y])\\\\\nmedian(\\ln(Y)) = \\ln(median(Y))\n\\end{eqnarray*}\\]Example 3: Transforming \\(x\\) \\(Y\\): \\(x' = \\ln(x)\\) \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 \\ln(x)\\]\n\\[\\begin{eqnarray*}\n\\frac{median(Y|2x)}{median(Y|x)} &=& \\frac{e^{\\beta_0 + \\beta_1 \\ln(2x)}}{e^{\\beta_0 + \\beta_1 \\ln(x)}}\\\\\n&=& e^{\\beta_1 (\\ln(2x) - \\ln(x))}\\\\\n&=& e^{\\beta_1 \\ln(2)} = 2^{\\beta_1}\n\\end{eqnarray*}\\]\ndoubling \\(x\\) associated multiplicative change \\(2^{\\beta_1}\\) median Y.Note model regressing GDP % urban used log transformation GDP (response, Y variable). know :\\[\\begin{eqnarray*}\n\\hat{\\ln(Y)} &=& \\mbox{median}(\\ln(Y)) = b_0 + b_1 \\cdot x\\\\\n\\mbox{median}(Y) &=& \\exp(b_0 + b_1 \\cdot x)\n\\end{eqnarray*}\\]using coefficients output linear model regressing ln_gdp urban, can find model predicts median gdp (transformed!) function urban.\\[\\begin{eqnarray*}\n\\mbox{median}(\\verb;ln_gdp;) &=& \\exp(6.11 +  0.0425 \\cdot \\verb;urban;)\n\\end{eqnarray*}\\]\nFigure 6.4: red exponential line represents median GDP particular value % urban.\n","code":"\nGDP %>%\n  lm(ln_gdp ~ urban, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   6.11     0.202        30.3 1.18e-74\n## 2 urban         0.0425   0.00308      13.8 2.29e-30\nGDP %>%\n  ggplot() + \n  geom_point(aes(x = urban, y = gdp)) + \n  geom_line(aes(x = urban, y = exp(6.11 + 0.0425*urban)), color = \"red\")"},{"path":"diag1.html","id":"reflection-questions-3","chapter":"6 Diagnostic Measures I","heading":"6.5  Reflection Questions","text":"","code":""},{"path":"diag1.html","id":"residuals-1","chapter":"6 Diagnostic Measures I","heading":"6.5.1 Residuals","text":"know model conditions hold?model conditions don’t hold?log transformation, slope coefficient interpreted?happens want add lots explanatory variables model?add quadratic term?","code":""},{"path":"diag1.html","id":"ethics-considerations-2","chapter":"6 Diagnostic Measures I","heading":"6.6  Ethics Considerations","text":"GDP model country observational unit (192 countries). quick Google shows UN currently defines 193 countries Leaving aside might one short, technical conditions need apply entire population? Can think country data 2020 representative (unknown?) population?mammals dataset collected 1976. reasons 50 year old dataset might represent accurate model population today?","code":""},{"path":"diag1.html","id":"r-slr-inference","chapter":"6 Diagnostic Measures I","heading":"6.7 R: SLR Inference","text":"Consider mammals dataset MASS package (careful MASS, overwrite dplyr functions filter select) representing average brain (g) body (kg) weights 62 species land mammals3.first glance linear model, doesn’t seem like really linear relationship ! certainly residuals don’t look like ’d hoped LINE technical conditions.Let’s try log transformations (square root transformations might also make sense).\nFigure 6.5: Taking natural log body weight doesn’t seem create model linear shape.\n\nFigure 6.6: Taking natural log brain weight also doesn’t seem create model linear shape.\n\nFigure 6.7: Taking natural log brain body weight seem create model linear shape!\n","code":"\nlibrary(MASS)  # be careful with MASS, it messes up filter and select\ndata(mammals)\n\nmammals %>% \n  ggplot(aes(x = body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\nmammals %>%\n  lm(brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\nmammals <- mammals %>%\n  mutate(ln_body = log(body),\n         ln_brain = log(brain))\n\nlibrary(patchwork)  # to get the plots next to one another\n\np1 <- mammals %>% \n  ggplot(aes(x = ln_body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np2 <- mammals %>%\n  lm(brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np1 + p2\np3 <- mammals %>% \n  ggplot(aes(x = body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np4 <- mammals %>%\n  lm(ln_brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np3 + p4\np5 <- mammals %>% \n  ggplot(aes(x = ln_body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np6 <- mammals %>%\n  lm(ln_brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np5 + p6"},{"path":"simult.html","id":"simult","chapter":"7 Simultaneous Inference","heading":"7 Simultaneous Inference","text":"(sections 4.1, 4.2, 4.3 ALSM)Spring 2022: cover joint estimation \\(\\beta_0\\) \\(\\beta_1\\), cover simultaneous estimation mean response simultaneous prediction intervals new observations.","code":""},{"path":"simult.html","id":"joint-estimation-of-beta_0-and-beta_1","chapter":"7 Simultaneous Inference","heading":"7.1 Joint Estimation of \\(\\beta_0\\) and \\(\\beta_1\\)","text":"Note inference \\(\\beta_1\\) can performed \\(\\beta_0\\) slight modifications formula SE. Although \\(\\beta_1\\) typically related research question hand, \\(\\beta_0\\) can also informative right. Indeed, interested CI either one, calculate following:\\[b_0 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_1\\}\\]Clearly many parameters many intervals, wouldn’t think cover true parameters probability \\((1-\\alpha)\\). figure probability cover true parameter, let’s define\n\\[\\begin{eqnarray*}\nA_1 &=& \\mbox{first CI cover } \\beta_0\\\\\nP(A_1) &=& \\alpha\\\\\nA_2 &=& \\mbox{second CI cover } \\beta_1\\\\\nP(A_2) &=& \\alpha\\\\\n\\end{eqnarray*}\\]want bound probability intervals cover true parameter.\n\\[\\begin{eqnarray*}\nP(\\mbox{CIs cover}) &=& 1- P(\\mbox{least one CI cover})\\\\\n&=& 1 - P( A_1 \\mbox{ } A_2)\\\\\n&&\\\\\nP( A_1 \\mbox{ } A_2) &=& P(A_1) + P(A_2) - P(A_1 \\mbox{ } A_2)\\\\\n&=& \\alpha + \\alpha - ???\\\\\n& \\leq& 2 \\alpha \\\\\nP(\\mbox{least one CI cover}) &\\leq& 2 \\alpha\\\\\nP(\\mbox{CIs cover}) &\\geq& 1- 2 \\alpha\\\\\n\\end{eqnarray*}\\]Note make two 95% CIs, Bonferroni inequality tells us cover respective parameters 90% repeated samples (.e., 90% confidence).\\(g\\) CIs, letting multiplier level significance given \\(\\alpha/g\\) create familywise confidence intervals level significance \\(\\alpha\\).\\(\\beta_0\\) \\(\\beta_1\\), let \\(B = t_{(1-\\alpha/4), n-2}\\). intervals \\(1-\\alpha\\) familywise confidence limits :\\[b_0 \\pm B \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm B \\cdot s\\{b_1\\}\\]Note interpretations depend heavily number intervals (true multiple comparison adjustments).Bonferroni extremely conservative, therefore low power.Bonferroni easily extends number comparisons.Bonferroni can adjust multiple comparisons even comparing different types analyses (e.g., CI slope parameter predicted response).","code":""},{"path":"simult.html","id":"simultaneous-estimation-of-a-mean-response","chapter":"7 Simultaneous Inference","heading":"7.2 Simultaneous Estimation of a Mean Response","text":"intervals parameters, creating intervals multiple mean responses leads problems multiple comparisons. , goal adjust intervals probability getting dataset lead intervals covering mean responses \\(1-\\alpha\\).(Note reason simultaneous inference combination natural sampling variability \\(b_0\\) \\(b_1\\) lead mean responses correct \\(E[Y_h]\\) range \\(x\\) values incorrect different range \\(x\\) values.)","code":""},{"path":"simult.html","id":"working-hotelling-procedure","chapter":"7 Simultaneous Inference","heading":"Working-Hotelling Procedure","text":"Working-Hotelling procedure gives confidence band entire range \\(x\\) values. family confidence interval simultaneous intervals \\(1-\\alpha\\). Note multiplier determined bound entire line complete range \\(x\\) values.\\[\\hat{y}_h \\pm W s \\{ \\hat{y}_h \\}\\]\n\\(W^2 = 2 F_{(1-\\alpha; 2, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B \\cdot s \\{ \\hat{y}_h \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"simultaneous-prediction-intervals-for-new-observations","chapter":"7 Simultaneous Inference","heading":"7.3 Simultaneous Prediction Intervals for New Observations","text":"Just estimating mean response, interval predicting new observations can adjusted total range observations contained appropriate intervals probability \\(1-\\alpha\\). prediction intervals, necessary specify number \\(g\\) intervals interest.","code":""},{"path":"simult.html","id":"scheffé-procedure","chapter":"7 Simultaneous Inference","heading":"Scheffé Procedure","text":"Scheffé procedure gives confidence band set \\(x\\) values, \\(g\\) . Using Scheffé procedure, family confidence interval simultaneous intervals \\(1-\\alpha\\).\\[\\hat{y}_h \\pm S \\cdot s \\{ \\mbox{pred} \\}\\]\n\\(S^2 = g F_{(1-\\alpha; g, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure-1","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B s \\{ \\mbox{pred} \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"more","chapter":"7 Simultaneous Inference","heading":"7.4 More","text":"Note ALSM sections 4.6 (Inverse Predictions) 4.7 (Choice \\(x\\) values) provide additional information good model building. sections worth reading .","code":""},{"path":"simult.html","id":"reflection-questions-4","chapter":"7 Simultaneous Inference","heading":"7.5  Reflection Questions","text":"simultaneous CIs worry us (error perspective)?difference Bonferroni, Working-Hotelling, Scheffé adjustments?Bonferroni intervals larger two? smaller?say Bonferroni procedure general multiple comparisons procedures?\\(g\\) large, Working-Hotelling procedure preferred Bonferroni?","code":""},{"path":"simult.html","id":"ethics-considerations-3","chapter":"7 Simultaneous Inference","heading":"7.6  Ethics Considerations","text":"interval estimates often valuable report p-values?mean one adjustments “conservative?”sample representative population, can interval estimate misleading?words can used distinguish mean confidence intervals individual prediction intervals order better communication results?","code":""},{"path":"simult.html","id":"r-simultaneous-inference","chapter":"7 Simultaneous Inference","heading":"7.6.1 R: Simultaneous inference","text":"R packages simultaneous inference automatically. However, purposes covering, ’ll focus changing multiplier order control type error rate.Consider regression ames housing data. regression ln_price area home (sqft). See full analysis section @ref{ames-inf}.Let’s say want CI intercept slope parameters. case, two intervals created. Bonferroni controls type error dividing alpha error number intervals. multiplier t value adjusted alpha level degrees freedom linear model (n-2).intervals can created directly output tidy() function.Similarly, intervals mean response (confidence interval) individual response (prediction interval) use appropriate standard errors new multiplier. code Working-Hotelling multiplier Scheffe multiplier, 10 new observations.","code":"\names_lm <- ames_inf %>%\n  lm(price_ln ~ area, data = .) \n\names_lm %>%\n  tidy()## # A tibble: 2 × 5\n##   term         estimate std.error statistic p.value\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0\n## 2 area         0.000614 0.0000114      53.9       0\names_lm %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.500         0.500 0.284     2901.       0     1  -461.  928.  946.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\names_df <- ames_lm %>%\n  glance() %>%\n  select(df.residual) %>%\n  pull()\n\names_df## [1] 2902\nnum_int_param <- 2  # for beta0 and beta1\n\n# Bonferroni:\ncrit_Bonf <- qt((1-.975)/num_int_param, ames_df)\ncrit_Bonf## [1] -2.24\names_lm %>%\n  tidy() %>%\n  select(term, estimate, std.error) %>%\n  mutate(lower.ci = estimate + crit_Bonf*std.error,\n         upper.ci = estimate - crit_Bonf*std.error)## # A tibble: 2 × 5\n##   term         estimate std.error  lower.ci  upper.ci\n##   <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177    11.1      11.1     \n## 2 area         0.000614 0.0000114  0.000588  0.000639\n# Working-Hotelling\ncrit_WH <- sqrt(2*qf(.95, 2, ames_df))\ncrit_WH## [1] 2.45\n# Scheffe\nnum_int_pred <- 10 # if 10 new observations for prediction\ncrit_Sch <- sqrt(num_int_pred*qf(0.95, num_int_pred, ames_df))\ncrit_Sch## [1] 4.28"},{"path":"la.html","id":"la","chapter":"8 Regression using Matrices","heading":"8 Regression using Matrices","text":"Everything ’ve done far can written matrix form. Though might seem efficient use matrices simple linear regression, become clear multiple linear regression, matrices can powerful. ALSM chapter 5 contains lot matrix theory; main take away points chapter matrix theory applied regression setting. Please make sure read chapters / examples regression examples.","code":""},{"path":"la.html","id":"special-matrices","chapter":"8 Regression using Matrices","heading":"8.1 Special Matrices","text":"","code":""},{"path":"la.html","id":"regression-model","chapter":"8 Regression using Matrices","heading":"8.2 Regression Model","text":"","code":""},{"path":"la.html","id":"matrix-addition","chapter":"8 Regression using Matrices","heading":"8.2.1 Matrix Addition","text":"\\[\\begin{eqnarray*}\nY_i &=& E[Y_i] + \\epsilon_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& E[\\underline{Y}] + \\underline{\\epsilon}\\\\\n\\end{eqnarray*}\\]\nSimilarly,\n\\[\\begin{eqnarray*}\nY_i &=& \\hat{Y}_i + e_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} \\hat{Y}_1\\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} +\n\\begin{pmatrix} e_1\\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& \\underline{\\hat{Y}} + \\underline{e}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-multiplication","chapter":"8 Regression using Matrices","heading":"8.3 Matrix Multiplication","text":"","code":""},{"path":"la.html","id":"example","chapter":"8 Regression using Matrices","heading":"8.3.1 Example","text":"Consider multiplying \\(r \\times c\\) matrix \\(c \\times s\\) matrix. interior dimensions must always . resulting matrix always \\(r \\times s\\). element \\(^{th}\\) row \\(j^{th}\\) column given :\n\\[\\begin{eqnarray*}\n\\sum_{k=1}^c a_{ik} b_{kj}\\\\\nAB &=& \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{pmatrix}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} a_{11} b_{11} + a_{12} b_{21} + a_{13}b_{31} & a_{11} b_{12} + a_{12} b_{22} + a_{13}b_{32} \\\\ a_{21} b_{11} + a_{22} b_{21} + a_{23}b_{31} & a_{21} b_{12} + a_{22} b_{22} + a_{23}b_{32}\n\\end{pmatrix}\\\\\n&&\\\\\nAB &=& \\begin{pmatrix} 3 & -1 & 0 \\\\ 0 & 1 & 1 \\\\ -2 & 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 1\\\\ -1 & 2 \\\\ 0 & -1 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} 4 & 1\\\\ -1 & 1 \\\\ -2 & -3 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"and-in-the-linear-regression-context","chapter":"8 Regression using Matrices","heading":"and in the linear regression context…","text":"\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} &=&\n\\begin{pmatrix} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots \\\\ 1 & X_n \\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\\\\n&&\\\\\nE[\\underline{Y}] &=& X \\underline{\\beta}\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\underline{Y}^t \\underline{Y} &=& \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\sum_{=1}^n Y_i^2\\\\\n&&\\\\\nX^t X &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix}= \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\nX^t \\underline{Y} &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-inverses-in-the-regression-context","chapter":"8 Regression using Matrices","heading":"8.4 Matrix Inverses in the Regression Context","text":"","code":""},{"path":"la.html","id":"matrix-inverses","chapter":"8 Regression using Matrices","heading":"8.4.1 Matrix Inverses","text":"\\(n \\times n\\) matrix \\(\\) called invertible exists \\(n \\times n\\) matrix \\(B\\) \n\\[\\begin{eqnarray*}\nB = B = I_n\n\\end{eqnarray*}\\]\n\n\\(B\\) called inverse \\(\\) typically denoted \\(B = ^{-1}\\). (Note, inverses exist square matrices non-zero determinants.)","code":""},{"path":"la.html","id":"example-5","chapter":"8 Regression using Matrices","heading":"8.4.2 Example","text":"\\[= \\begin{pmatrix}  &  b \\\\ c  &  d \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} d / (ad - bc)  &  -b / (ad - bc) \\\\ -c / (ad - bc)  &  / (ad - bc) \\end{pmatrix}\\]determinant given \\(D = ad - bc\\).\n\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} d / (ad - bc) & -b / (ad - bc) \\\\ -c / (ad - bc) & / (ad - bc) \\end{pmatrix} \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (ad - bc) / (ad - bc) & (bd - bd) / (ad - bc) \\\\ (-ac + ac) / (ad - bc) & (ad - bc) / (ad - bc) \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]\\[= \\begin{pmatrix} 3  &  2 \\\\ 1  &  6 \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} 6 / 16  &  -2 / 16 \\\\ -1 / 16  &  3 / 16 \\end{pmatrix} \\]\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} 6 / 16 & -2 / 16 \\\\ -1 / 16 & 3 / 16 \\end{pmatrix}  \\begin{pmatrix} 3 & 2 \\\\ 1 & 6 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (18-2)/16 & (12-12)/16 \\\\ (-3+3)/16 & (-2+18)/16 \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"variance-of-coefficients","chapter":"8 Regression using Matrices","heading":"8.4.3 Variance of Coefficients","text":"","code":""},{"path":"la.html","id":"what-is-a-variance-matrix","chapter":"8 Regression using Matrices","heading":"What is a Variance Matrix?","text":"Consider two random variables, \\(U\\) \\(W\\). ’d typically \\(n\\) observations variable, resulting data vectors \\(\\underline{U}\\) \\(\\underline{W}\\). variance-covariance matrix describing variability relationship \\(U\\) \\(W\\) given :\n\\[\\begin{eqnarray*}\n\\mbox{var}\\{\\underline{U},\\underline{W} \\} &=& \\begin{pmatrix} \\sigma_U^2  & \\sigma_{U,W} \\\\ \\sigma_{W,U} & \\sigma_W^2 \\end{pmatrix}\\\\\n\\mbox{} \\sigma_U^2 &=& \\mbox{ variance } U  \\mbox{ (number)}\\\\\n\\sigma_W^2 &=& \\mbox{ variance } V  \\mbox{ (number)}\\\\\n\\sigma_{U,W} &=& \\sigma_{W,U} = \\mbox{ covariance } U \\mbox{ } W  \\mbox{ (number)} \\\\\n\\end{eqnarray*}\\]\norder find variance matrix regression coefficients, use matrix notation.\n\\[\\begin{eqnarray*}\n\\mbox{Recall:    }  X^t X &=&  \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{,    } D &=& n \\sum X_i^2 - (\\sum X_i)^2 = n \\sum(X_i - \\overline{X})^2\\\\\n&&\\\\\n(X^t X)^{-1} &=&  \\begin{pmatrix} \\frac{\\sum X_i^2}{n \\sum(X_i - \\overline{X})^2} & \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} & \\frac{n}{n \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=&  \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b}\\} &=& \\sigma^2  \\cdot (X^t X)^{-1}\\\\\n&=& \\begin{pmatrix} \\sigma^2_{b_0} & \\sigma_{b_0, b_1} \\\\ \\sigma_{b_1, b_0} & \\sigma^2_{b_1} \\end{pmatrix}\\\\\nSE^2\\{\\underline{b}\\} &=& MSE  \\cdot (X^t X)^{-1}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"estimating-coefficients","chapter":"8 Regression using Matrices","heading":"Estimating Coefficients","text":"Recall equations come differentiating sum squared residuals respect \\(\\beta_0\\) \\(\\beta_1\\):\\[\\begin{eqnarray*}\nn b_0 + b_1 \\sum X_i &=& \\sum Y_i\\\\\nb_0 \\sum X_i + b_1 \\sum X_i^2 &=& \\sum X_i Y_i\\\\\n&&\\\\\n\\begin{pmatrix} n & \\sum X_i \\\\ \\sum X_i & \\sum X_i^2 \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix} &=& \\begin{pmatrix} \\sum Y_i \\\\ \\sum X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n(X^t X) \\underline{b} &=& X^t \\underline{Y}\\\\\n\\underline{b} &=& (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b} \\} &=& (X^t X)^{-1} X^t \\sigma^2 X (X^t X)^{-1}\\\\\n&=& \\sigma^2 \\cdot (X^t X)^{-1}\\\\\n&&\\\\\n\\mbox{checking:}&&\\\\\n(X^t X)^{-1} (X^t \\underline{Y}) &=& \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix} \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\frac{\\sum Y_i}{n} + \\frac{\\sum Y_i \\overline{X}^2}{\\sum(X_i - \\overline{X})^2} + \\frac{-\\sum X_i Y_i (\\overline{X})}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum Y_i (\\overline{X})}{ \\sum(X_i - \\overline{X})^2} + \\frac{\\sum X_i Y_i}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\overline{Y} - b_1 \\overline{X} \\\\ \\frac{\\sum Y_i (X_i - \\overline{X})}{\\sum(X_i - \\overline{X})^2} \\end{pmatrix} = \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"fitted-values","chapter":"8 Regression using Matrices","heading":"8.5 Fitted Values","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_i &=& b_0 + b_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} &=& \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\\\\\n&&\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n &=& X (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&=& H \\underline{Y}\\\\\n\\mbox{\"hat\" matrix: } H &=& X (X^t X)^{-1} X^t\n\\end{eqnarray*}\\]\ncall \\(H\\) hat matrix takes \\(\\underline{Y}\\) puts hat . Note predicted values simply linear combinations response variable (\\(Y\\)) coefficients explanatory variables (\\(X\\)).","code":""},{"path":"la.html","id":"residuals-2","chapter":"8 Regression using Matrices","heading":"8.6 Residuals","text":"\\[\\begin{eqnarray*}\ne_i &=& Y_i - \\hat{Y}_i\\\\\n\\begin{pmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix} &=& \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\Y_n \\end{pmatrix} - \\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix}\\\\\n\\underline{e} &=& \\underline{Y} - \\hat{\\underline{Y}}\\\\\n&=& \\underline{Y} - X \\underline{b}\\\\\n&=& \\underline{Y} - H \\underline{Y}\\\\\n&=& (- H) \\underline{Y}\\\\\n&&\\\\\n\\mbox{var}\\{ \\underline{e} \\} &=& \\mbox{var}\\{ (-H) \\underline{Y} \\}\\\\\n&=& (- H) \\mbox{var}\\{ \\underline{Y} \\}\\\\\n&=& (- H) \\cdot \\sigma^2 \\cdot (- H)^t\\\\\n&=& \\sigma^2 \\cdot (- H) (-H^t)\\\\\n&=& \\sigma^2 \\cdot (- H - H^t + HH^t)\\\\\n&=&  \\sigma^2 \\cdot (-H)\\\\\nSE^2(\\underline{e}) &=& MSE \\cdot (-H)\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"analysis-of-variance","chapter":"8 Regression using Matrices","heading":"8.7 ANalysis Of VAriance","text":"\\[\\begin{eqnarray*}\nSSTO &=& \\underline{Y}^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} 1 & 1 & \\cdots &  1\\\\ 1 & 1 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 1 & \\cdots & 1   \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix}  \\begin{pmatrix} \\sum Y_i \\\\ \\sum Y_i \\\\ \\vdots \\\\ \\sum Y_i \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\sum Y_i \\sum Y_i\\\\\n&=& \\sum Y_i ^2 - n \\overline{Y}^2\\\\\n\\mbox{note: } \\sum(Y_i - \\overline{Y})^2 &=& \\sum (Y_i^2 - 2Y_i \\overline{Y} + \\overline{Y}^2)\\\\\n&=& \\sum Y_i^2 -2\\overline{Y}\\sum Y_i + n \\overline{Y}^2\\\\\n&=& \\sum Y_i^2 - n \\overline{Y}^2\\\\\n&&\\\\\nSSE &=& \\underline{Y}^t \\underline{Y} - \\underline{b}^t X^t \\underline{Y}\\\\\n&&\\\\\nSSR &=& \\underline{b}^t X^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"prediction-of-new-observations","chapter":"8 Regression using Matrices","heading":"8.8 Prediction of New Observations","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_h &=& \\underline{X}^t_h \\underline{b}\\\\\n\\mbox{var}\\{ \\hat{Y}_h \\} &=& \\underline{X}_h^t \\mbox{var}\\{\\underline{b} \\} \\underline{X}_h\\\\\n&=& \\sigma^2 \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\nSE^2\\{\\hat{Y}_h \\} &=& MSE \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\n&&\\\\\n&&\\\\\nSE^2 \\{\\mbox{pred} \\} = SE^2\\{\\hat{Y}_{h(new)} \\} &=& MSE \\cdot (1 + \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"reflection-questions-5","chapter":"8 Regression using Matrices","heading":"8.9  Reflection Questions","text":"normal errors linear model written matrix form?X matrix added column ones?Using matrix notation, normal equations used solve least squares estimates \\(\\beta_0\\) \\(\\beta_1\\)?covariance term? mean variables correlated (e.g., \\(b_0\\) \\(b_1\\))?“hat” matrix (\\(H\\)) named?","code":""},{"path":"la.html","id":"ethics-considerations-4","chapter":"8 Regression using Matrices","heading":"8.10  Ethics Considerations","text":"sampling distributions \\(b_0\\) \\(b_1\\) correlated?ways writing linear model matrix notation make extension explanatory variables easier?fundamental differences SLR model written matrix notation (opposed indexing elements equation)?","code":""},{"path":"la.html","id":"r-matrices","chapter":"8 Regression using Matrices","heading":"8.11 R: Matrices","text":"","code":""},{"path":"la.html","id":"addition","chapter":"8 Regression using Matrices","heading":"8.11.1 Addition","text":"Adding matrices gives ’d likely expect.","code":"\nmatrix1 <- matrix(c(1:12),ncol=4, byrow=T)\nmatrix2 <- matrix(seq(2,24,by=2),ncol=4, byrow=T)\n\nmatrix1##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\nmatrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    4    6    8\n## [2,]   10   12   14   16\n## [3,]   18   20   22   24\nmatrix1 + matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    3    6    9   12\n## [2,]   15   18   21   24\n## [3,]   27   30   33   36"},{"path":"la.html","id":"multiplication","chapter":"8 Regression using Matrices","heading":"8.11.2 Multiplication","text":"* produces element element multiplication.%*% matrix multiplication. Note error can’t multiply \\(3 \\times 4\\) \\(3 \\times4\\) matrix.either matrices transposed, conformable, able multiply .Note products symmetric matrix1 = \\(2 \\cdot\\) matrix2.","code":"\nmatrix1 * matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    8   18   32\n## [2,]   50   72   98  128\n## [3,]  162  200  242  288\nmatrix1 %*% matrix2## Error in matrix1 %*% matrix2: non-conformable arguments\nmatrix1 %*% t(matrix2)##      [,1] [,2] [,3]\n## [1,]   60  140  220\n## [2,]  140  348  556\n## [3,]  220  556  892\nt(matrix1) %*% matrix2##      [,1] [,2] [,3] [,4]\n## [1,]  214  244  274  304\n## [2,]  244  280  316  352\n## [3,]  274  316  358  400\n## [4,]  304  352  400  448"},{"path":"la.html","id":"taking-inverses","chapter":"8 Regression using Matrices","heading":"8.11.3 Taking Inverses","text":"function inverting matrices R solve(). Remember solve() can works square matrices non-zero determinants.Multiplying matrix inverse results identity matrix.","code":"\nmatrix3 <- matrix(c(5,7,1,4,3,6,2,0,8), ncol=3, byrow=T)\nmatrix3##      [,1] [,2] [,3]\n## [1,]    5    7    1\n## [2,]    4    3    6\n## [3,]    2    0    8\nsolve(matrix3)##        [,1]   [,2] [,3]\n## [1,] -0.923  2.154 -1.5\n## [2,]  0.769 -1.462  1.0\n## [3,]  0.231 -0.538  0.5\nmatrix3 %*% solve(matrix3)##          [,1]      [,2]     [,3]\n## [1,] 1.00e+00 -2.22e-16 1.11e-16\n## [2,] 4.44e-16  1.00e+00 0.00e+00\n## [3,] 4.44e-16  0.00e+00 1.00e+00"},{"path":"la.html","id":"concatenating-1s-for-the-intercept","chapter":"8 Regression using Matrices","heading":"8.11.4 Concatenating 1s for the intercept","text":"Let’s say explanatory variable called xvar","code":"\nset.seed(7447)\nxvar <- rnorm(12, 47, 3)\nxvar##  [1] 47.3 43.1 47.1 48.0 49.7 45.2 47.7 47.1 47.9 48.9 43.4 44.1\nrep(1,12)  # repeats the number 1, 12 times##  [1] 1 1 1 1 1 1 1 1 1 1 1 1\nXmatrix <- cbind(int = rep(1,12), xvar)\nXmatrix##       int xvar\n##  [1,]   1 47.3\n##  [2,]   1 43.1\n##  [3,]   1 47.1\n##  [4,]   1 48.0\n##  [5,]   1 49.7\n##  [6,]   1 45.2\n##  [7,]   1 47.7\n##  [8,]   1 47.1\n##  [9,]   1 47.9\n## [10,]   1 48.9\n## [11,]   1 43.4\n## [12,]   1 44.1"},{"path":"mlr.html","id":"mlr","chapter":"9 Multiple Linear Regression","heading":"9 Multiple Linear Regression","text":"get , use function bestregsubsets().Consider new dataset. data collected Michael Ernst St. Cloud University Minnesota (Polar Vortex January 2019).late fall early winter, temperature dropped (tends MN), Michael started get suspicious thermometer wasn’t entirely accurate. , put another thermometer trusted outside near new one read temperature window. wrote temperature every throughout December January.two variables: Temp, actual temperature (based trusted thermometer), Reading, reading suspect thermometer.\nFigure 9.1: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.2: scatterplot looks linear… residual plot doesn’t!\nHopefully, transforming data help. figure seems like square root Reading log Temp might help. Let’s try .\nFigure 9.3: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.4: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.5: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.6: scatterplot looks linear… residual plot doesn’t!\nDoesn’t seem like transformations going work. square term added? still linear model? (Yes!) residuals better? (Yes!)","code":"\nlibrary(openintro)\names_inf <- ames %>%\n  filter(area <= 3000) %>%\n  mutate(price_ln = log(price))\names_inf %>%\n  mutate(bedrooms = case_when(\n    Bedroom.AbvGr <=1 ~ \"1\",\n    Bedroom.AbvGr <=2 ~ \"2\",\n    Bedroom.AbvGr <=3 ~ \"3\",\n    TRUE ~ \"4+\"\n  )) %>%\n  ggplot(aes(x = area, y = price_ln, color = bedrooms)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\ntemperature <- readr::read_csv(\"Temperature.csv\")\ntemperature %>%\n  ggplot(aes(x = Temp, y = Reading)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\ntemperature %>%\n  lm(Reading ~ Temp, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\ntemperature <- temperature %>%\n  mutate(sqrt_read = sqrt(Reading),\n         ln_temp = log(Temp))\n    \ntemperature %>%\n  ggplot(aes(x = Temp, y = sqrt_read)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\ntemperature %>%\n  lm(sqrt_read ~ Temp, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\ntemperature %>%\n  ggplot(aes(x = ln_temp, y = Reading)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\ntemperature %>%\n  lm(Reading ~ ln_temp, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)"},{"path":"build.html","id":"build","chapter":"10 Model Building","heading":"10 Model Building","text":"","code":""},{"path":"diag2.html","id":"diag2","chapter":"11 Diagnostics II","heading":"11 Diagnostics II","text":"","code":""},{"path":"shrink.html","id":"shrink","chapter":"12 Shrinkage Methods","heading":"12 Shrinkage Methods","text":"","code":""},{"path":"smooth.html","id":"smooth","chapter":"13 Smoothing Methods","heading":"13 Smoothing Methods","text":"","code":""},{"path":"anova.html","id":"anova","chapter":"14 ANOVA","heading":"14 ANOVA","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
