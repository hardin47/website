[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 158 Pomona College: Computational Statistics. notes based extensively Introduction Statistical Learning (James et al. 2021); Applied Linear Statistical Models (Kutner et al. 2004). computing part class taken R Data Science (Wickham Grolemund 2017) Wickham Grolemund well Tidy Modeling R (Kuhn Silge 2021) Kuhn Silge.responsible reading relevant chapters texts. texts good & readable, use . make sure coming class also reading materials associated activities.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims populationWhat content Math 158?\nMath 158 course statistical linear models.goal Math 158 understand modeling linear statistical relationships explanatory / predictor (X) variables response (Y) variables.models grow sophistication semester including multiple linear regression, interaction terms, ridge regression, Lasso, smoothing.Throughout semester, continue talk good modeling practices, ideas extend beyond linear models types inference prediction.think carefully inferential modeling makes sense predictive modeling makes sense. neither type analysis done!math class go quickly, little calculus. , however, learn linear algebra.necessarily use linear models determine causation (need experimental design course discuss issues observational study vs. experiment). E.g., (1) thermostat versus actual temperature, (2) ice cream sales versus boating accidents.take Math 158?\nLinear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.prerequisites Math 158?\nLinear Models requires strong background statistics well algorithmic thinking. formal prerequisite introductory statistics course, AP Statistics, may find working hard first weeks class catch . taken lot mathematics, parts course come easily . However, mathematics degree substitute introductory statistics, taken introductory statistics, majority course work intuitive . must taken prior statistics course pre-requisite Math 158; computer science course helpful.Many derivations much notation course come linear algebra. taken linear algebra enormously helpful class, cover enough notation need linear algebra.worth noting concepts probability theory represent majority mathematical tools used statistics / modeling, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.overlap classes?\nStatistical Linear Models overlaps Econometrics (Econ 167) Applied Econometrics (107). Econometrics focuses probability theory matrix algebra (mathematics) lead derivation linear models. Applied Econometrics focuses tools analysis. Statistical Linear Models focuses use model, assumptions made, conclusions appropriate given results. Additionally, later topics Linear Models typically covered Econometrics.take Math 158?\nprerequisite Linear Models introduction statistics, course moves quickly covers tremendous amount material. ideally suited first year student coming straight AP Statistics. Instead, student focus taking mathematics, CS, interdisciplinary science, statistics courses. students taking Linear Models sophomores juniors.workload Math 158?\none homework assignment per week, two -class midterm exams, two take-home midterm exams, final end semester project. Many students report working 8-10 hours per week outside class.software use? real world applications? mathematics? CS?\nwork done R (using RStudio front end, called integrated development environment, IDE). need either download R RStudio (free) onto computer use Pomona’s server. assignments posted private repositories GitHub. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.assignments turned using R Markdown compiled pdf + pushed GitHub\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Linear Models ubiquitous incredibly powerful. Indeed, often times linear models appropriate (e.g., violating technical conditions) yet end giving almost identical solutions models appropriate. solid understanding linear model framework, however, requires strong foundation theory goes inferential thinking well predictive modeling. review ideas inference introductory statistics.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]","code":""},{"path":"intro.html","id":"simple-linear-regression","chapter":"1 Introduction","heading":"1.2.2 Simple Linear Regression","text":"simplest case, study first, suppose two variables. call one explanatory variable, response variable.Explanatory / Predictor variable: also known independent variable, numeric variable often known advance variable, thought possibly influence value.Response / outcome variable: also known dependent variable, also numeric, thought function predictor variable. n.b., don’t use word “dependent” don’t want send message ’ve measured anything causal model.goal ascertain relationship two. observe sample population interest, \\((x_i,y_i), =1,\\dots,n\\). \\(x_i\\) predictor, \\(y_i\\) response, \\(n\\) sample size.observe sample, actual population , best can hope estimate relationship two variables. resulting estimates give us idea relationship actually population. However, random quantities, depend random sample. , exact. theory hypothesis testing needed\ndetermine much can actually say population quantities, called parameters.Parameter: quantity describes population. Examples population mean, population standard deviation, say relationship two variables polynomial, coefficients function parameters.","code":""},{"path":"intro.html","id":"hypothesis-testing","chapter":"1 Introduction","heading":"1.2.3 Hypothesis Testing","text":"set : population big observe. ’d like know value specific parameters, say instance mean population. can’t however, calculate parameters directly.\nInstead, observe sample random population. Based sample, estimate parameters. However, estimated values exact. need technique uses estimated model say something population model.Null Hypothesis: Denoted \\(H_0\\), null hypothesis usually set believed unless evidence presented otherwise. specific, specifying parameter equal specific value.\n\\(H_0\\) true, theory tells us exactly estimate behaves.Alternative Hypothesis: Denoted \\(H_a\\), alternative hypothesis usually wish show true. general \\(H_0\\), usually form parameter somehow equal value used \\(H_0\\), without specifying exactly think value . result, don’t know estimate behaves, depends value parameter.","code":""},{"path":"intro.html","id":"what-really-is-an-alternative-hypothesis","chapter":"1 Introduction","heading":"What really is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEYIn video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?Since procedure potential wrong, search one makes bad errors infrequently. two types errors can made.Type error: Rejecting \\(H_0\\) \\(H_0\\) actually true. Usually considered worst error possible, thus find procedure makes type error small probability. probability denoted \\(\\alpha\\).Type II error: rejecting \\(H_0\\) \\(H_a\\) actually true. small type II error secondary concern (controlling type error). probability type II error denoted \\(\\beta\\). \\(1-\\beta\\)\nknown power.reality, true reason choose test small value \\(\\alpha\\) value know calculate. \\(\\beta\\) power possible calculate \\(H_a\\) doesn’t tell us value parameter .way hypothesis test carried via p-value, essentially tells us unusual observed data comparison \\(H_0\\). estimates consistent expected \\(H_0\\) true, conclusion \\(H_0\\) false: explanation data strange. definition p-value little tricky.p-value: probability, \\(H_0\\) true, observing data contradictory \\(H_0\\) repeat experiment .p-value .01, means data showed something happens 1 time 100 \\(H_0\\) true. Considering particular set data observed, reasonable conclusion \\(H_0\\) must \ntrue. rule : reject \\(H_0\\) p-value \\(< \\alpha\\). resulting test type error probability \\(\\alpha\\), value get specify. \\(\\alpha\\) often set .05.","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3 Reflection Questions","text":"difference sample population?experimental design issues influence conclusions?type error, type II error, power?p-value (careful, p-value probability \\(H_0\\) true!!!)?regression line ?linear regression always appropriate strategy?properties good fitting line ?line appropriately interpreted?","code":""},{"path":"intro.html","id":"r-reproduciblity","chapter":"1 Introduction","heading":"1.4 R: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.4.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.4.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.4.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"tools-git-github","chapter":"1 Introduction","heading":"Tools: Git & GitHub","text":"must submit assignments via GitHubFollow Jenny Bryan’s advice get set-: http://happygitwithr.com/Class specific instructions https://m158-comp-stats.netlify.app/github.htmlAdmittedly, steep learning curve Git. However, among tools likely use future endeavors, spending little time focusing concepts now may pay big time future. Beyond practicing working http://happygitwithr.com/, may want read little bit Git behind scenes. reference: Learn git concepts, commands good accessible.","code":""},{"path":"intro.html","id":"tools-a-github-merge-conflict-demo","chapter":"1 Introduction","heading":"Tools: a GitHub merge conflict (demo)","text":"GitHub (web) edit README document Commit message describing ., RStudio also edit README document different change.\nCommit changes\nTry push \\(\\rightarrow\\) ’ll get error!\nTry pulling\nResolve merge conflict commit push\nCommit changesTry push \\(\\rightarrow\\) ’ll get error!Try pullingResolve merge conflict commit pushAs work teams run merge conflicts, learning resolve properly important.\nFigure 1.7: https://xkcd.com/1597/\n","code":""},{"path":"intro.html","id":"steps-for-weekly-homework","chapter":"1 Introduction","heading":"Steps for weekly homework","text":"get link new assignment (clicking link create new private repo)Use R (within R Studio)\nNew Project, version control, Git\nClone repo using SSH\nNew Project, version control, GitClone repo using SSHIf exists, rename Rmd file ma158-hw#-lname-fname.RmdDo assignment\ncommit push every problem\ncommit push every problemAll necessary files must folder (e.g., data)","code":""},{"path":"wrang.html","id":"wrang","chapter":"2 Data Wrangling","heading":"2 Data Wrangling","text":"data visualization, data wrangling fundamental part able accurately, reproducibly, efficiently work data. approach taken following chapter based philosophy tidy data takes many precepts database theory. done much work SQL, functionality approach tidy data feel familiar. adept data wrangling, effective data analysis.Information want, data ’ve got. (Kaplan 2015)Embrace ways get help!cheat sheets: https://www.rstudio.com/resources/cheatsheets/tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/pivoting: https://tidyr.tidyverse.org/articles/pivot.htmlgoogle need include R tidy tidyverse","code":""},{"path":"wrang.html","id":"datastruc","chapter":"2 Data Wrangling","heading":"2.1 Structure of Data","text":"plotting, analyses, model building, etc., ’s important data structured particular way. Hadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy Data: rows (cases/observational units) columns (variables). key every row case *every} column variable. exceptions.Creating tidy data trivial. work objects (often data tables), functions, arguments (often variables).Active Duty data tidy! cases? data tidy? might data look like tidy form? Suppose case “individual armed forces.” variables use capture information following table?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794Problem: totals different sheetsBetter R: longer format columns - grade, gender, status, service, count (case still total pay grade)Case individual (?): grade, gender, status, service (count row counting)","code":""},{"path":"wrang.html","id":"building-tidy-data","chapter":"2 Data Wrangling","heading":"2.1.1 Building Tidy Data","text":"Within R (really within type computing language, Python, SQL, Java, etc.), need understand build data using patterns language. things consider:object_name = function_name(arguments) way using function create new object.object_name = data_table %>% function_name(arguments) uses chaining syntax extension ideas functions. chaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.\n* R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.\n* pipe syntax read , %>%.","code":"object_name = data_table %>%\nfunction_name(arguments) %>% \nfunction_name(arguments)"},{"path":"wrang.html","id":"examples-of-chaining","chapter":"2 Data Wrangling","heading":"2.1.2 Examples of Chaining","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)","code":""},{"path":"wrang.html","id":"little-bunny-foo-foo","chapter":"2 Data Wrangling","heading":"2.1.2.1 Little Bunny Foo Foo","text":"Hadley Wickham, think tidy data.Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThe nursery rhyme created series steps output step saved object along way.Another approach concatenate functions one output.even worse, one line:Instead, code can written using pipe order function evaluated:babynames year, US Social Security Administration publishes list popular names given babies. 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys.babynames data table babynames package comes Social Security Administration’s listing names givens babies year, number babies sex given name. (names 5 babies published SSA.)","code":"foo_foo <- little_bunny()\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)bop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))foo_foo %>%\n   hop(through = forest) %>%\n       scoop(up = field_mice) %>%\n           bop(on = head)"},{"path":"wrang.html","id":"data-verbs-on-single-data-frames","chapter":"2 Data Wrangling","heading":"2.1.3 Data Verbs (on single data frames)","text":"Super important resource: RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). use R package dplyr much data wrangling. list verbs helpful wrangling many different types data. See Data Wrangling cheat sheet RStudio additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfsample_n() take random row(s)sample_n() take random row(s)head() grab first rowshead() grab first rowstail() grab last rowstail() grab last rowsfilter() removes unwanted casesfilter() removes unwanted casesarrange() reorders casesarrange() reorders casesselect() removes unwanted variables (rename() )select() removes unwanted variables (rename() )distinct() returns unique values tabledistinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)mutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nsummarize() collapses data frame single row. functions used within summarize() include:min(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]","code":""},{"path":"wrang.html","id":"r-examples-basic-verbs","chapter":"2 Data Wrangling","heading":"2.2 R examples, basic verbs","text":"","code":""},{"path":"wrang.html","id":"datasets","chapter":"2 Data Wrangling","heading":"2.2.1 Datasets","text":"starwars dplyr , although originally SWAPI, Star Wars API, http://swapi.co/.NHANES ?NHANES: NHANES survey data collected US National Center Health Statistics (NCHS) conducted series health nutrition surveys since early 1960’s. Since 1999 approximately 5,000 individuals ages interviewed homes every year complete health examination component survey. health examination conducted mobile examination center (MEC).babynames year, US Social Security Administration publishes list popular names given babies. 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys. (names 5 babies published SSA.)","code":""},{"path":"wrang.html","id":"examples-of-chaining-1","chapter":"2 Data Wrangling","heading":"2.2.2 Examples of Chaining","text":"","code":"\nlibrary(babynames)\nbabynames %>% nrow()## [1] 1924665\nbabynames %>% names()## [1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\nbabynames %>% glimpse()## Rows: 1,924,665\n## Columns: 5\n## $ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n## $ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n## $ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n## $ n    <int> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n## $ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\nbabynames %>% head()## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\nbabynames %>% tail()## # A tibble: 6 × 5\n##    year sex   name       n       prop\n##   <dbl> <chr> <chr>  <int>      <dbl>\n## 1  2017 M     Zyhier     5 0.00000255\n## 2  2017 M     Zykai      5 0.00000255\n## 3  2017 M     Zykeem     5 0.00000255\n## 4  2017 M     Zylin      5 0.00000255\n## 5  2017 M     Zylis      5 0.00000255\n## 6  2017 M     Zyrie      5 0.00000255\nbabynames %>% sample_n(size=5)## # A tibble: 5 × 5\n##    year sex   name         n      prop\n##   <dbl> <chr> <chr>    <int>     <dbl>\n## 1  1946 M     Owen       392 0.000238 \n## 2  1953 M     Abelardo    34 0.0000170\n## 3  1996 F     Nicki       50 0.0000261\n## 4  1914 F     Sofia       38 0.0000477\n## 5  1970 M     Lorenza     45 0.0000236\nbabynames %>% mosaic::favstats(n ~ sex, data = .)##   sex min Q1 median Q3   max     mean       sd       n missing\n## 1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n## 2   M   5  7     12 33 94756 223.4940 1932.338  786372       0"},{"path":"wrang.html","id":"data-verbs","chapter":"2 Data Wrangling","heading":"2.2.3 Data Verbs","text":"Taken dplyr tutorial: http://dplyr.tidyverse.org/","code":""},{"path":"wrang.html","id":"starwars","chapter":"2 Data Wrangling","heading":"2.2.3.1 Starwars","text":"","code":"\nlibrary(dplyr)\n\nstarwars %>% dim()## [1] 87 14\nstarwars %>% names()##  [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n##  [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n## [11] \"species\"    \"films\"      \"vehicles\"   \"starships\"\nstarwars %>% head()## # A tibble: 6 × 14\n##   name     height  mass hair_color  skin_color eye_color birth_year sex   gender\n##   <chr>     <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Luke Sk…    172    77 blond       fair       blue            19   male  mascu…\n## 2 C-3PO       167    75 <NA>        gold       yellow         112   none  mascu…\n## 3 R2-D2        96    32 <NA>        white, bl… red             33   none  mascu…\n## 4 Darth V…    202   136 none        white      yellow          41.9 male  mascu…\n## 5 Leia Or…    150    49 brown       light      brown           19   fema… femin…\n## 6 Owen La…    178   120 brown, grey light      blue            52   male  mascu…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       8\n## 2 masculine  15 75     80 88.0 1358 106.14694 184.972677 49      17\nstarwars %>% \n  dplyr::filter(species == \"Droid\")## # A tibble: 6 × 14\n##   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n##   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n## 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculi…\n## 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculi…\n## 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculi…\n## 4 IG-88     200   140 none       metal       red               15 none  masculi…\n## 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n## 6 BB8        NA    NA none       none        black             NA none  masculi…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  dplyr::filter(species != \"Droid\") %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n## 2 masculine  15 77     80 88.0 1358 109.38222 192.397084 45      16\nstarwars %>% \n  dplyr::select(name, ends_with(\"color\"))## # A tibble: 87 × 4\n##    name               hair_color    skin_color  eye_color\n##    <chr>              <chr>         <chr>       <chr>    \n##  1 Luke Skywalker     blond         fair        blue     \n##  2 C-3PO              <NA>          gold        yellow   \n##  3 R2-D2              <NA>          white, blue red      \n##  4 Darth Vader        none          white       yellow   \n##  5 Leia Organa        brown         light       brown    \n##  6 Owen Lars          brown, grey   light       blue     \n##  7 Beru Whitesun lars brown         light       blue     \n##  8 R5-D4              <NA>          white, red  red      \n##  9 Biggs Darklighter  black         light       brown    \n## 10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n## # … with 77 more rows\nstarwars %>% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %>%\n  dplyr::select(name:mass, bmi)## # A tibble: 87 × 4\n##    name               height  mass   bmi\n##    <chr>               <int> <dbl> <dbl>\n##  1 Luke Skywalker        172    77  26.0\n##  2 C-3PO                 167    75  26.9\n##  3 R2-D2                  96    32  34.7\n##  4 Darth Vader           202   136  33.3\n##  5 Leia Organa           150    49  21.8\n##  6 Owen Lars             178   120  37.9\n##  7 Beru Whitesun lars    165    75  27.5\n##  8 R5-D4                  97    32  34.0\n##  9 Biggs Darklighter     183    84  25.1\n## 10 Obi-Wan Kenobi        182    77  23.2\n## # … with 77 more rows\nstarwars %>% \n  dplyr::arrange(desc(mass))## # A tibble: 87 × 14\n##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Jabba …    175  1358 <NA>        green-tan… orange         600   herm… mascu…\n##  2 Grievo…    216   159 none        brown, wh… green, y…       NA   male  mascu…\n##  3 IG-88      200   140 none        metal      red             15   none  mascu…\n##  4 Darth …    202   136 none        white      yellow          41.9 male  mascu…\n##  5 Tarfful    234   136 brown       brown      blue            NA   male  mascu…\n##  6 Owen L…    178   120 brown, grey light      blue            52   male  mascu…\n##  7 Bossk      190   113 none        green      red             53   male  mascu…\n##  8 Chewba…    228   112 brown       unknown    blue           200   male  mascu…\n##  9 Jek To…    180   110 brown       fair       blue            NA   male  mascu…\n## 10 Dexter…    198   102 none        brown      yellow          NA   male  mascu…\n## # … with 77 more rows, and 5 more variables: homeworld <chr>, species <chr>,\n## #   films <list>, vehicles <list>, starships <list>\nstarwars %>%\n  dplyr::group_by(species) %>%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  dplyr::filter(num > 1)## # A tibble: 9 × 3\n##   species    num  mass\n##   <chr>    <int> <dbl>\n## 1 Droid        6  69.8\n## 2 Gungan       3  74  \n## 3 Human       35  82.8\n## 4 Kaminoan     2  88  \n## 5 Mirialan     2  53.1\n## 6 Twi'lek      2  55  \n## 7 Wookiee      2 124  \n## 8 Zabrak       2  80  \n## 9 <NA>         4  48"},{"path":"wrang.html","id":"nhanes","chapter":"2 Data Wrangling","heading":"2.2.3.2 NHANES","text":"","code":"\nrequire(NHANES)\nnames(NHANES)##  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n##  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n##  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n## [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n## [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n## [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n## [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n## [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n## [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n## [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n## [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n## [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n## [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n## [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n## [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n## [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n## [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n## [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n## [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n# find the sleep variables\nNHANESsleep <- NHANES %>% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"\ndim(NHANESsleep)## [1] 10000    11\n# subset for college students\nNHANESsleep <- NHANESsleep %>% filter(Age %in% c(18:22)) %>% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"\ndim(NHANESsleep)## [1] 655  12\nNHANESsleep %>% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) "},{"path":"wrang.html","id":"summarize-and-group_by","chapter":"2 Data Wrangling","heading":"2.2.4 summarize and group_by","text":"","code":"\n# number of people (cases) in NHANES\nNHANES %>% summarize(n())## # A tibble: 1 × 1\n##   `n()`\n##   <int>\n## 1 10000\n# total weight of all the people in NHANES (silly)\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(sum(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `sum(Weightlb, na.rm = TRUE)`\n##                           <dbl>\n## 1                      1549419.\n# mean weight of all the people in NHANES\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `mean(Weightlb, na.rm = TRUE)`\n##                            <dbl>\n## 1                           156.\n# repeat the above but for groups\n\n# males versus females\nNHANES %>% group_by(Gender) %>% summarize(n())## # A tibble: 2 × 2\n##   Gender `n()`\n##   <fct>  <int>\n## 1 female  5020\n## 2 male    4980\nNHANES %>% group_by(Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 2 × 2\n##   Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                           <dbl>\n## 1 female                           146.\n## 2 male                             167.\n# smokers and non-smokers\nNHANES %>% group_by(SmokeNow) %>% summarize(n())## # A tibble: 3 × 2\n##   SmokeNow `n()`\n##   <fct>    <int>\n## 1 No        1745\n## 2 Yes       1466\n## 3 <NA>      6789\nNHANES %>% group_by(SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                 186.\n## 2 Yes                                177.\n## 3 <NA>                               144.\n# people with and without diabetes\nNHANES %>% group_by(Diabetes) %>% summarize(n())## # A tibble: 3 × 2\n##   Diabetes `n()`\n##   <fct>    <int>\n## 1 No        9098\n## 2 Yes        760\n## 3 <NA>       142\nNHANES %>% group_by(Diabetes) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   Diabetes `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                155. \n## 2 Yes                               202. \n## 3 <NA>                               21.6\n# break down the smokers versus non-smokers further, by sex\nNHANES %>% group_by(SmokeNow, Gender) %>% summarize(n())## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `n()`\n##   <fct>    <fct>  <int>\n## 1 No       female   764\n## 2 No       male     981\n## 3 Yes      female   638\n## 4 Yes      male     828\n## 5 <NA>     female  3618\n## 6 <NA>     male    3171\nNHANES %>% group_by(SmokeNow, Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                           <dbl>\n## 1 No       female                           167.\n## 2 No       male                             201.\n## 3 Yes      female                           167.\n## 4 Yes      male                             185.\n## 5 <NA>     female                           138.\n## 6 <NA>     male                             151.\n# break down the people with diabetes further, by smoking\nNHANES %>% group_by(Diabetes, SmokeNow) %>% summarize(n())## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `n()`\n##   <fct>    <fct>    <int>\n## 1 No       No        1476\n## 2 No       Yes       1360\n## 3 No       <NA>      6262\n## 4 Yes      No         267\n## 5 Yes      Yes        106\n## 6 Yes      <NA>       387\n## 7 <NA>     No           2\n## 8 <NA>     <NA>       140\nNHANES %>% group_by(Diabetes, SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                             <dbl>\n## 1 No       No                                183. \n## 2 No       Yes                               175. \n## 3 No       <NA>                              143. \n## 4 Yes      No                                204. \n## 5 Yes      Yes                               204. \n## 6 Yes      <NA>                              199. \n## 7 <NA>     No                                193. \n## 8 <NA>     <NA>                               19.1"},{"path":"wrang.html","id":"babynames","chapter":"2 Data Wrangling","heading":"2.2.5 babynames","text":"","code":"\nbabynames %>% group_by(sex) %>%\n  summarize(total=sum(n))## # A tibble: 2 × 2\n##   sex       total\n##   <chr>     <int>\n## 1 F     172371079\n## 2 M     175749438\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% head()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  1880 F            942\n## 2  1880 M           1058\n## 3  1881 F            938\n## 4  1881 M            997\n## 5  1882 F           1028\n## 6  1882 M           1099\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% tail()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  2015 F          19074\n## 2  2015 M          14024\n## 3  2016 F          18817\n## 4  2016 M          14162\n## 5  2017 F          18309\n## 6  2017 M          14160\nbabysamp <- babynames %>% sample_n(size=50)\nbabysamp %>% select(year) %>% distinct() %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    1    1    1    1    1\nbabysamp %>% distinct() %>% select(year) %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    2    2    1    1    3    1    1    1    1    1    2    2    1    2    2 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    3    1    2    1    1\nFrances <- babynames %>%\n  filter(name== \"Frances\") %>%\n  group_by(year, sex) %>%\n  summarize(yrTot = sum(n))\n\nFrances %>% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")"},{"path":"wrang.html","id":"highverb","chapter":"2 Data Wrangling","heading":"2.3 Higher Level Data Verbs","text":"complicated verbs may important sophisticated analyses. See RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.pivot_longer makes many columns 2 columns: pivot_longer(data, cols,  names_to = , value_to = )pivot_wider makes one column multiple columns: pivot_wider(data, names_from = , values_from = )left_join returns rows left table, rows matching keys right table.inner_join returns rows left table matching keys right table (.e., matching rows sets).full_join returns rows tables, join records left matching keys right table.Good practice: always specify argument joining data frames.ever need understand join right join , try find image lay function . found one quite good taken Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti","code":""},{"path":"wrang.html","id":"r-examples-higher-level-verbs","chapter":"2 Data Wrangling","heading":"2.4 R examples, higher level verbs","text":"tidyr 1.0.0 just released! new release means need update tidyr. know latest version following command works console (window ):familiar spread gather, acquaint pivot_longer() pivot_wider(). idea go wide dataframes long dataframes vice versa.","code":"?tidyr::pivot_longer"},{"path":"wrang.html","id":"pivot_longer","chapter":"2 Data Wrangling","heading":"2.4.1 pivot_longer()","text":"pivot military pay grade become longer?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#\ngid=1811988794Does graph tell us right? done wrong…?","code":"\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\nglimpse(navy_gs)## Rows: 38\n## Columns: 17\n## $ ...1                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ `Active Duty Family` <chr> NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n## $ ...3                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n## $ ...4                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n## $ ...5                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n## $ ...6                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n## $ ...7                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n## $ ...8                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n## $ ...9                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n## $ ...10                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n## $ ...11                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n## $ ...12                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n## $ ...13                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n## $ ...14                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n## $ ...15                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n## $ ...16                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n## $ ...17                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)## Rows: 30\n## Columns: 16\n## $ pay.grade       <chr> \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n## $ male.sing.wo    <dbl> 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n## $ female.sing.wo  <dbl> 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n## $ tot.sing.wo     <dbl> 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n## $ male.sing.w     <dbl> 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n## $ female.sing.w   <dbl> 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n## $ tot.sing.w      <dbl> 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n## $ male.joint.NA   <dbl> 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n## $ female.joint.NA <dbl> 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n## $ tot.joint.NA    <dbl> 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n## $ male.civ.NA     <dbl> 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n## $ female.civ.NA   <dbl> 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n## $ tot.civ.NA      <dbl> 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n## $ male.tot.NA     <dbl> 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n## $ female.tot.NA   <dbl> 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n## $ tot.tot.NA      <dbl> 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n# get rid of total columns & rows:\n\nnavyWR = navy %>% select(-contains(\"tot\")) %>%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %>%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %>%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %>% head()## # A tibble: 6 × 5\n##   pay.grade sex    marital kids  numPeople\n##   <chr>     <chr>  <chr>   <chr>     <dbl>\n## 1 E-1       male   sing    wo        31229\n## 2 E-1       female sing    wo         5717\n## 3 E-1       male   sing    w           563\n## 4 E-1       female sing    w           122\n## 5 E-1       male   joint   NA          139\n## 6 E-1       female joint   NA          141\nnavyWR %>% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))"},{"path":"wrang.html","id":"pivot_wider","chapter":"2 Data Wrangling","heading":"2.4.2 pivot_wider","text":"","code":"\nlibrary(babynames)\nbabynames %>% dplyr::select(-prop) %>%\n   tidyr::pivot_wider(names_from = sex, values_from = n) ## # A tibble: 1,756,284 × 4\n##     year name          F     M\n##    <dbl> <chr>     <int> <int>\n##  1  1880 Mary       7065    27\n##  2  1880 Anna       2604    12\n##  3  1880 Emma       2003    10\n##  4  1880 Elizabeth  1939     9\n##  5  1880 Minnie     1746     9\n##  6  1880 Margaret   1578    NA\n##  7  1880 Ida        1472     8\n##  8  1880 Alice      1414    NA\n##  9  1880 Bertha     1320    NA\n## 10  1880 Sarah      1288    NA\n## # … with 1,756,274 more rows\nbabynames %>% \n  select(-prop) %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(year), desc(M))## # A tibble: 168,381 × 4\n##     year name         F     M\n##    <dbl> <chr>    <int> <int>\n##  1  2017 Liam        36 18728\n##  2  2017 Noah       170 18326\n##  3  2017 William     18 14904\n##  4  2017 James       77 14232\n##  5  2017 Logan     1103 13974\n##  6  2017 Benjamin     8 13733\n##  7  2017 Mason       58 13502\n##  8  2017 Elijah      26 13268\n##  9  2017 Oliver      15 13141\n## 10  2017 Jacob       16 13106\n## # … with 168,371 more rows\nbabynames %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(prop))## # A tibble: 12 × 5\n##     year name            prop     F     M\n##    <dbl> <chr>          <dbl> <int> <int>\n##  1  1986 Marquette 0.0000130     24    25\n##  2  1996 Dariel    0.0000115     22    23\n##  3  2014 Laramie   0.0000108     21    22\n##  4  1939 Earnie    0.00000882    10    10\n##  5  1939 Vertis    0.00000882    10    10\n##  6  1921 Vernis    0.00000703     9     8\n##  7  1939 Alvia     0.00000529     6     6\n##  8  1939 Eudell    0.00000529     6     6\n##  9  1939 Ladell    0.00000529     6     6\n## 10  1939 Lory      0.00000529     6     6\n## 11  1939 Maitland  0.00000529     6     6\n## 12  1939 Delaney   0.00000441     5     5"},{"path":"wrang.html","id":"join-use-join-to-merge-two-datasets","chapter":"2 Data Wrangling","heading":"2.4.3 join (use join to merge two datasets)","text":"","code":""},{"path":"wrang.html","id":"first-get-the-data-gapminder","chapter":"2 Data Wrangling","heading":"2.4.3.1 First get the data (GapMinder)","text":"following datasets come GapMinder. first represents country, year, female literacy rate. second represents country, year, GDP (fixed 2000 US$).","code":"\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %>% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %>%\n  filter(!is.na(litRateF))\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %>% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %>%\n  filter(!is.na(gdp))\nhead(litF)## # A tibble: 6 × 3\n##   country     year  litRateF\n##   <chr>       <chr>    <dbl>\n## 1 Afghanistan 1979      4.99\n## 2 Afghanistan 2011     13   \n## 3 Albania     2001     98.3 \n## 4 Albania     2008     94.7 \n## 5 Albania     2011     95.7 \n## 6 Algeria     1987     35.8\nhead(GDP)## # A tibble: 6 × 3\n##   country year    gdp\n##   <chr>   <chr> <dbl>\n## 1 Albania 1980  1061.\n## 2 Albania 1981  1100.\n## 3 Albania 1982  1111.\n## 4 Albania 1983  1101.\n## 5 Albania 1984  1065.\n## 6 Albania 1985  1060.\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)## [1] 571   4\nsum(is.na(litGDPleft$gdp))## [1] 66\nhead(litGDPleft)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902.\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)## [1] 7988    4\nsum(is.na(litGDPright$gdp))## [1] 0\nhead(litGDPright)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)## [1] 505   4\nsum(is.na(litGDPinner$gdp))## [1] 0\nhead(litGDPinner)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)## [1] 8054    4\nsum(is.na(litGDPfull$gdp))## [1] 66\nhead(litGDPfull)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902."},{"path":"wrang.html","id":"lubridate","chapter":"2 Data Wrangling","heading":"2.4.4 lubridate","text":"lubridate another R package meant data wrangling (Grolemund Wickham 2011). particular, lubridate makes easy work days, times, dates. base idea start dates ymd (year month day) format transform information whatever want. linked table original paper provides many basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.Example https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html","code":""},{"path":"wrang.html","id":"if-anyone-drove-a-time-machine-they-would-crash","chapter":"2 Data Wrangling","heading":"2.4.4.1 If anyone drove a time machine, they would crash","text":"length months years change often arithmetic can unintuitive. Consider simple operation, January 31st + one month. answer :February 31st (doesn’t exist)March 4th (31 days January 31), orFebruary 28th (assuming leap year)basic property arithmetic + b - b = . solution 1 obeys mathematical property, invalid date. Wickham wants make lubridate consistent possible invoking following rule: adding subtracting month year creates invalid date, lubridate return NA.thought solution 2 3 useful, problem. can still get results clever arithmetic, using special %m+% %m-% operators. %m+% %m-% automatically roll dates back last day month, necessary.","code":""},{"path":"wrang.html","id":"r-examples-lubridate","chapter":"2 Data Wrangling","heading":"2.4.4.2 R examples, lubridate()","text":"","code":""},{"path":"wrang.html","id":"some-basics-in-lubridate","chapter":"2 Data Wrangling","heading":"Some basics in lubridate","text":"","code":"\nrequire(lubridate)\nrightnow <- now()\n\nday(rightnow)## [1] 11\nweek(rightnow)## [1] 2\nmonth(rightnow, label=FALSE)## [1] 1\nmonth(rightnow, label=TRUE)## [1] Jan\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nyear(rightnow)## [1] 2022\nminute(rightnow)## [1] 55\nhour(rightnow)## [1] 15\nyday(rightnow)## [1] 11\nmday(rightnow)## [1] 11\nwday(rightnow, label=FALSE)## [1] 3\nwday(rightnow, label=TRUE)## [1] Tue\n## Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"},{"path":"wrang.html","id":"but-how-do-i-create-a-date-object","chapter":"2 Data Wrangling","heading":"But how do I create a date object?","text":"","code":"\njan31 <- ymd(\"2021-01-31\")\njan31 + months(0:11)##  [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n##  [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n## [11] NA           \"2021-12-31\"\nfloor_date(jan31, \"month\") + months(0:11) + days(31)##  [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n##  [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n## [11] \"2021-12-02\" \"2022-01-01\"\njan31 + months(0:11) + days(31)##  [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n##  [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n## [11] NA           \"2022-01-31\"\njan31 %m+% months(0:11)##  [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n##  [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n## [11] \"2021-11-30\" \"2021-12-31\""},{"path":"wrang.html","id":"nyc-flights","chapter":"2 Data Wrangling","heading":"NYC flights","text":"","code":"\nlibrary(nycflights13)\nnames(flights)##  [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n##  [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n##  [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n## [13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n## [17] \"hour\"           \"minute\"         \"time_hour\"\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)## # A tibble: 6 × 22\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      517            515         2      830            819\n## 2  2013     1     1      533            529         4      850            830\n## 3  2013     1     1      542            540         2      923            850\n## 4  2013     1     1      544            545        -1     1004           1022\n## 5  2013     1     1      554            600        -6      812            837\n## 6  2013     1     1      554            558        -4      740            728\n## # … with 14 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, ymdday <date>, weekdy <ord>,\n## #   whichweek <dbl>\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %>% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %>%  \n   head()## # A tibble: 6 × 9\n##    year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n##   <int> <int> <int> <date>     <ord>      <dbl>    <int>    <int>    <dbl>\n## 1  2013     1     1 2013-01-01 Tue            1      517      830      227\n## 2  2013     1     1 2013-01-01 Tue            1      533      850      227\n## 3  2013     1     1 2013-01-01 Tue            1      542      923      160\n## 4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n## 5  2013     1     1 2013-01-01 Tue            1      554      812      116\n## 6  2013     1     1 2013-01-01 Tue            1      554      740      150"},{"path":"wrang.html","id":"purrr-for-functional-programming","chapter":"2 Data Wrangling","heading":"2.5 purrr for functional programming","text":"see R package purrr greater detail go, now, let’s get hint works.going focus map family functions get us started. Lots good purrr functions like pluck() accumulate().Much taken tutorial Rebecca Barter.map functions named output produce. example:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vectorNote first argument always data object second object always function want iteratively apply element input object.input map function always either vector (like column), list (can non-rectangular), dataframe (like rectangle).list way hold things might different shape:Consider following function:can map() add_ten() function across vector. Note output list (default).use different type input? default behavior still return list!want different type output? use different map() function, map_df(), example.Shorthand lets us get away pre-defining function (useful). Use tilde ~ indicate function:Mostly, tilde used functions already know:","code":"\na_list <- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)## $a_number\n## [1] 5\n## \n## $a_vector\n## [1] \"a\" \"b\" \"c\"\n## \n## $a_dataframe\n##   a b             c\n## 1 1 q       bananas\n## 2 2 b           are\n## 3 3 z so very great\nadd_ten <- function(x) {\n  return(x + 10)\n  }\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 15\n## \n## [[3]]\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map(add_ten)## $a\n## [1] 12\n## \n## $b\n## [1] 15\n## \n## $c\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(add_ten)## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(~{.x + 10})## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %>%\n  split(.$species) %>%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map_df(tidy)  # map(tidy)## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n## 2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n## 3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n## 4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n## 5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\npenguins %>%\n  group_by(species) %>%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map(tidy)  # map_df(tidy)## [[1]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 0.00948      \n## 2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n## \n## [[2]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -3037.     997.       -3.05 0.00333      \n## 2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n## \n## [[3]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 2 flipper_length_mm     54.6      5.03     10.9  1.33e-19"},{"path":"wrang.html","id":"reprex","chapter":"2 Data Wrangling","heading":"2.6 reprex()","text":"Help help youIn order create reproducible example …Step 1. Copy code onto clipboardStep 2. Type reprex() ConsoleStep 3. Look Viewer right. Copy Viewer output GitHub, Piazza, email, stackexchange, etc.places learn reprex includeA blog : https://teachdatascience.com/reprex/reprex vignette: https://reprex.tidyverse.org/index.htmlreprex dos donts: https://reprex.tidyverse.org/articles/reprex-dos--donts.htmlJenny Bryan webinar reprex: “Help help . Creating reproducible examples” https://resources.rstudio.com/webinars/help--help--creating-reproducible-examples-jenny-bryanSome advice: https://stackoverflow.com/help/minimal-reproducible-example","code":""},{"path":"wrang.html","id":"reprex-demo","chapter":"2 Data Wrangling","heading":"2.6.0.1 reprex demo","text":"multiple lines code:","code":"reprex(\n  jan31 + months(0:11) + days(31)\n)reprex({\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})reprex({\n  library(lubridate)\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})"},{"path":"viz.html","id":"viz","chapter":"3 Visualization","heading":"3 Visualization","text":"Data visualization integral understanding data models. Computational statistics data science sometimes focus models resulting predictions models. doubt structure format data key whether model appropriate good. good data analyst always spend lot time effort exploratory data analysis, much includes making many visualizations data possible.Depending introductory () statistics classes ’ve , instructor may focused less visualizations class. () may even said something like making visualizations incredibly important entire data analysis process. even buy perspective, don’t see good graphics analyses? Andrew Gelman (Gelman 2011) responds stating, “Good statistical graphics hard , much harder running regressions making tables.” goal create graphics visualizations convey statistical information.Nolan (Nolan Perrett 2016) describes three important ways graphics can used convey statistical information. “guiding principles” used way evaluating others’ figures well metric creating visualizations help statistical analysis.Make data stand outThe important idea find anything unusual data. patterns? Outliers? bounds variables? axes scaled? transformations warranted?Facilitate comparisonThe second item allows us consider research questions hand. important variables? emphasize ? variables plotted together? Can super-imposed? color, plotting character, size plot character help bring important relationships? aware plotting issues color blindness. http://colorbrewer2.org/Add informationPlots also add context comparison. Figure legends, axes scales, reference markers (e.g., line \\(y=x\\)) go long way toward helping reader understand message. Captions self-contained (assume user also read text) descriptive; summarize content figure conclusion related message want convey.Randy Pruim asks following question decide whether plot good: plot make comparisons interested …easily? andaccurately?Consider adding alt text allow screen readers parse image. DataViz Society/Nightingale way Amy Cesal article writing good alt text plots/graphs, Writing Alt Text Data Visualization.","code":""},{"path":"viz.html","id":"thoughts","chapter":"3 Visualization","heading":"3.1 Thoughts on Plotting","text":"","code":""},{"path":"viz.html","id":"advice","chapter":"3 Visualization","heading":"3.1.1 Advice","text":"Basic plotting\nAvoid graph elements interfere data\nUse visually prominent symbols\nAvoid -plotting (One way avoid plotting: Jitter values)\nDifferent values data may obscure \nInclude nearly data\nFill data region\nAvoid graph elements interfere dataUse visually prominent symbolsAvoid -plotting (One way avoid plotting: Jitter values)Different values data may obscure otherInclude nearly dataFill data regionEliminate superfluous material\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background images\nExtra tick marks grid lines\nUnnecessary text arrows\nDecimal places beyond measurement error level difference\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background imagesExtra tick marks grid linesUnnecessary text arrowsDecimal places beyond measurement error level differenceFacilitate Comparisons\nPut juxtaposed plots scale\nMake easy distinguish elements superposed plots (e.g. color)\nEmphasizes important difference\nComparison: volume, area, height (careful, volume can seem bigger mean )\nPut juxtaposed plots scaleMake easy distinguish elements superposed plots (e.g. color)Emphasizes important differenceComparison: volume, area, height (careful, volume can seem bigger mean )Choosing Scale (n.b., principles may go counter one another, use judgment.)\nKeep scales x y axes plots facilitate comparison\nZoom focus region contains bulk data\nKeep scale throughout plot (.e. don’t change mid-axis)\nOrigin need scale\nChoose scale improves resolution\nAvoid jiggling baseline\nKeep scales x y axes plots facilitate comparisonZoom focus region contains bulk dataKeep scale throughout plot (.e. don’t change mid-axis)Origin need scaleChoose scale improves resolutionAvoid jiggling baselineHow make plot information rich\nDescribe see caption\nAdd context reference markers (lines points) including text\nAdd legends labels\nUse color plotting symbols add information\nPlot thing different ways/scales\nReduce clutter\nDescribe see captionAdd context reference markers (lines points) including textAdd legends labelsUse color plotting symbols add informationPlot thing different ways/scalesReduce clutterCaptions \ncomprehensive\nSelf-contained\nDescribe graphed\nDraw attention important features\nDescribe conclusions drawn graph\ncomprehensiveSelf-containedDescribe graphedDraw attention important featuresDescribe conclusions drawn graphGood Plot Making Practice\nPut major conclusions graphical form\nProvide reference information\nProof read clarity consistency\nGraphing iterative process\nMultiplicity OK, .e. two plots variable may provide different messages\nMake plots data rich\nPut major conclusions graphical formProvide reference informationProof read clarity consistencyGraphing iterative processMultiplicity OK, .e. two plots variable may provide different messagesMake plots data richCreating statistical graphic iterative process discovery fine tuning. try model process creating visualizations course dedicating class time iterative creation plot. begin either plot screams correction, transform step--step, always thinking goal graph data rich presents clear vision important features data.","code":""},{"path":"viz.html","id":"fonts-matter","chapter":"3 Visualization","heading":"3.1.1.1 Fonts Matter","text":"RStudio::conf 2020, Glamour Graphics, Chase makes important points making good graphics matters. talk might summarized plot : fonts matter.","code":""},{"path":"viz.html","id":"deconstruct","chapter":"3 Visualization","heading":"3.2 Deconstructing a graph","text":"","code":""},{"path":"viz.html","id":"gg","chapter":"3 Visualization","heading":"3.2.1 The Grammar of Graphics (gg)","text":"Yau (2013) Wickham (2014) come taxonomy grammar thinking parts figure just like conceptualize parts body parts sentence.One great way thinking new process: longer necessary talk name graph (e.g., boxplot). Instead now think glyphs (geoms), can put whatever want plot. Note also transition leads passive consumer (need make plot XXX everyone else , just plug data) active participant (want data say? can put information onto graphic?)important questions can ask respect creating figures :want R ? (goal?)R need know?Yau (2013) gives us nine visual cues, Wickham (2014) translates language using ggplot2. (items Baumer, Kaplan, Horton (2021), chapter 2.)Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Coordinate System: rectangular, polar, geographic, etc.Coordinate System: rectangular, polar, geographic, etc.Scale: numeric (linear? logarithmic?), categorical (ordered?), timeScale: numeric (linear? logarithmic?), categorical (ordered?), timeContext: comparison (think back ideas Tufte)Context: comparison (think back ideas Tufte)","code":""},{"path":"viz.html","id":"order-matters","chapter":"3 Visualization","heading":"Order Matters","text":"","code":""},{"path":"viz.html","id":"cues-together","chapter":"3 Visualization","heading":"Cues Together","text":"","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-1","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-2","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"the-grammar-of-graphics-in-ggplot2","chapter":"3 Visualization","heading":"3.2.1.1 The grammar of graphics in ggplot2","text":"geom: geometric “shape” used display databar, point, line, ribbon, text, etc.aesthetic: attribute controlling geom displayed respect variablesx position, y position, color, fill, shape, size, etc.scale: adjust information aesthetic map onto plotparticular assignment colors, shapes, sizes, etc.; making axes continuous constrained particular range values.guide: helps user convert visual data back raw data (legends, axes)stat: transformation applied data geom gets itexample: histograms work binned data","code":""},{"path":"viz.html","id":"ggplot2","chapter":"3 Visualization","heading":"3.2.2 ggplot2","text":"ggplot2, aesthetic refers mapping variable information conveys plot. information plotting visualizing information given chapter 2 (Data visualization) Baumer, Kaplan, Horton (2021). Much data presentation represents births 1978 US: date, day year, number births.","code":""},{"path":"viz.html","id":"goals","chapter":"3 Visualization","heading":"Goals","text":"try dogive tour ggplot2give tour ggplot2explain think plots ggplot2 wayexplain think plots ggplot2 wayprepare/encourage learn laterprepare/encourage learn laterWhat can’t one sessionshow every bell whistleshow every bell whistlemake expert using ggplot2make expert using ggplot2","code":""},{"path":"viz.html","id":"getting-help","chapter":"3 Visualization","heading":"Getting help","text":"One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfLook end presentation. help options .Look end presentation. help options .","code":"\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)##         date births wday year month day_of_year day_of_month day_of_week\n## 1 1978-01-01   7701  Sun 1978     1           1            1           1\n## 2 1978-01-02   7527  Mon 1978     1           2            2           2\n## 3 1978-01-03   8825  Tue 1978     1           3            3           3"},{"path":"viz.html","id":"how-can-we-make-the-plot","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Two Questions:want R ? (goal?)want R ? (goal?)R need know?\ndata source: Births78\naesthetics:\ndate -> x\nbirths -> y\npoints (!)\n\nR need know?data source: Births78data source: Births78aesthetics:\ndate -> x\nbirths -> y\npoints (!)\naesthetics:date -> xbirths -> ypoints (!)Goal: scatterplot = plot points\nggplot() + geom_point()\nGoal: scatterplot = plot pointsggplot() + geom_point()R need know?\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\nR need know?data source: data = Births78data source: data = Births78aesthetics: aes(x = date, y = births)aesthetics: aes(x = date, y = births)","code":""},{"path":"viz.html","id":"how-can-we-make-the-plot-1","chapter":"3 Visualization","heading":"How can we make the plot?","text":"changed?new aesthetic: mapping color day week","code":""},{"path":"viz.html","id":"adding-day-of-week-to-the-data-set","chapter":"3 Visualization","heading":"Adding day of week to the data set","text":"wday() function lubridate package computes day week date.","code":"\nBirths78 <-  \n  Births78 %>% \n  mutate(wday = lubridate::wday(date, label=TRUE))\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-2","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now use lines instead dots","code":"\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-3","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now two layers, one points one \nlinesThe layers placed one top : points\nlines .layers placed one top : points\nlines .data aes specified ggplot() affect geomsdata aes specified ggplot() affect geoms","code":"\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"alternative-syntax","chapter":"3 Visualization","heading":"Alternative Syntax","text":"","code":"\nBirths78 %>% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"what-does-adding-the-color-argument-do","chapter":"3 Visualization","heading":"What does adding the color argument do?","text":"variable, mapped color aesthetic new variable one value (“navy”). dots get set color, ’s navy.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping","chapter":"3 Visualization","heading":"Setting vs. Mapping","text":"want set color navy dots, outside aesthetic, without dataset variable:Note color = \"navy\" now outside aesthetics list. ’s ggplot2 distinguishes mapping setting.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-4","chapter":"3 Visualization","heading":"How can we make the plot?","text":"ggplot() establishes default data aesthetics geoms, geom may change defaults.ggplot() establishes default data aesthetics geoms, geom may change defaults.good practice: put ggplot() things affect () layers; rest geom_blah()good practice: put ggplot() things affect () layers; rest geom_blah()","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping-again","chapter":"3 Visualization","heading":"Setting vs. Mapping (again)","text":"Information gets passed plot via:map variable information inside aes (aesthetic) commandmap variable information inside aes (aesthetic) commandset non-variable information outside aes (aesthetic) commandset non-variable information outside aes (aesthetic) command","code":""},{"path":"viz.html","id":"other-geoms","chapter":"3 Visualization","heading":"Other geoms","text":"help pages tell aesthetics, default stats, etc.","code":"\napropos(\"^geom_\") [1] \"geom_abline\"                  \"geom_area\"                   \n [3] \"geom_ash\"                     \"geom_bar\"                    \n [5] \"geom_barh\"                    \"geom_bin_2d\"                 \n [7] \"geom_bin2d\"                   \"geom_blank\"                  \n [9] \"geom_boxplot\"                 \"geom_boxploth\"               \n[11] \"geom_col\"                     \"geom_colh\"                   \n[13] \"geom_contour\"                 \"geom_contour_filled\"         \n[15] \"geom_count\"                   \"geom_crossbar\"               \n[17] \"geom_crossbarh\"               \"geom_curve\"                  \n[19] \"geom_density\"                 \"geom_density_2d\"             \n[21] \"geom_density_2d_filled\"       \"geom_density_line\"           \n[23] \"geom_density_ridges\"          \"geom_density_ridges_gradient\"\n[25] \"geom_density_ridges2\"         \"geom_density2d\"              \n[27] \"geom_density2d_filled\"        \"geom_dotplot\"                \n[29] \"geom_errorbar\"                \"geom_errorbarh\"              \n[31] \"geom_errorbarh\"               \"geom_freqpoly\"               \n[33] \"geom_function\"                \"geom_hex\"                    \n[35] \"geom_histogram\"               \"geom_histogramh\"             \n[37] \"geom_hline\"                   \"geom_jitter\"                 \n[39] \"geom_label\"                   \"geom_line\"                   \n[41] \"geom_linerange\"               \"geom_linerangeh\"             \n[43] \"geom_lm\"                      \"geom_map\"                    \n[45] \"geom_path\"                    \"geom_point\"                  \n[47] \"geom_pointrange\"              \"geom_pointrangeh\"            \n[49] \"geom_polygon\"                 \"geom_qq\"                     \n[51] \"geom_qq_line\"                 \"geom_quantile\"               \n[53] \"geom_raster\"                  \"geom_rect\"                   \n[55] \"geom_ribbon\"                  \"geom_ridgeline\"              \n[57] \"geom_ridgeline_gradient\"      \"geom_rug\"                    \n[59] \"geom_segment\"                 \"geom_sf\"                     \n[61] \"geom_sf_label\"                \"geom_sf_text\"                \n[63] \"geom_sina\"                    \"geom_smooth\"                 \n[65] \"geom_spline\"                  \"geom_spoke\"                  \n[67] \"geom_step\"                    \"geom_text\"                   \n[69] \"geom_tile\"                    \"geom_violin\"                 \n[71] \"geom_violinh\"                 \"geom_vline\"                  \n[73] \"geom_vridgeline\"             \n?geom_area             # for example"},{"path":"viz.html","id":"lets-try-geom_area","chapter":"3 Visualization","heading":"Let’s try geom_area","text":"Using area produce good plotover plotting hiding much dataextending y-axis 0 may may desirable.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"side-note-what-makes-a-plot-good","chapter":"3 Visualization","heading":"Side note: what makes a plot good?","text":"(?) graphics intended help us make comparisonsHow something change time?treatments matter? much?men women respond way?Key plot metric: plot make comparisons interested ineasily, andaccurately?","code":""},{"path":"viz.html","id":"time-for-some-different-data","chapter":"3 Visualization","heading":"Time for some different data","text":"HELPrct: Health Evaluation Linkage Primary care randomized clinical trialSubjects admitted treatment addiction one three substances.","code":"\nhead(HELPrct)##   age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n## 1  37            1    yes   49  3        177      225        0  NA      0\n## 2  37            1    yes   30 22          2       NA        0  NA      0\n## 3  26            1    yes   39  0          3      365       20  NA      0\n## 4  39            1    yes   15  2        189      343        0   1      1\n## 5  32            1    yes   39 12          2       57        0   1      0\n## 6  47            1    yes    6  1         31      365        0  NA      1\n##      sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n## 1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n## 2   male yes homeless 56 62  2     43         NA <NA> 26.670307 36.03694      1\n## 3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n## 4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n## 5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n## 6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n##   racegrp satreat sexrisk substance treat avg_drinks max_drinks\n## 1   black      no       4   cocaine   yes         13         26\n## 2   white      no       7   alcohol   yes         56         62\n## 3   black      no       2    heroin    no          0          0\n## 4   white     yes       4    heroin    no          5          5\n## 5   black      no       6   cocaine    no         10         13\n## 6   black      no       5   cocaine   yes          4          4\n##   hospitalizations\n## 1                3\n## 2               22\n## 3                0\n## 4                2\n## 5               12\n## 6                1"},{"path":"viz.html","id":"who-are-the-people-in-the-study","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"Hmm. ’s y?\nstat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.\nHmm. ’s y?stat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-1","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-2","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nlibrary(scales)\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-3","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study","chapter":"3 Visualization","heading":"How old are people in the HELP study?","text":"Notice messagesstat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.stat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.binwidth: default binwidth selected, really choose .binwidth: default binwidth selected, really choose .","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"viz.html","id":"setting-the-binwidth-manually","chapter":"3 Visualization","heading":"Setting the binwidth manually","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study-other-geoms","chapter":"3 Visualization","heading":"How old are people in the HELP study? – Other geoms","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every geom comes default statfor simple cases, stat stat_identity() nothingwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually-1","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every stat comes default geom, every geom default statwe can specify stats instead geom, preferwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"more-combinations","chapter":"3 Visualization","heading":"More combinations","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-much-do-they-drink-i1","chapter":"3 Visualization","heading":"How much do they drink? (i1)","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"covariates-adding-in-more-variables","chapter":"3 Visualization","heading":"Covariates: Adding in more variables","text":"Using color linetype:Using color facets","code":"\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"boxplots","chapter":"3 Visualization","heading":"Boxplots","text":"Boxplots use stat_quantile() computes five-number summary (roughly five quartiles data) uses define “box” “whiskers.”quantitative variable must y, must additional x variable.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"horizontal-boxplots","chapter":"3 Visualization","heading":"Horizontal boxplots","text":"Horizontal boxplots obtained flipping coordinate system:coord_flip() may used plots well reverse roles\nx y plot.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"axes-scaling-with-boxplots","chapter":"3 Visualization","heading":"Axes scaling with boxplots","text":"can scale continuous axis","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"give-me-some-space","chapter":"3 Visualization","heading":"Give me some space","text":"’ve triggered new feature: dodge (dodging things left/right). can control much set dodge manually.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"issues-with-bigger-data","chapter":"3 Visualization","heading":"Issues with bigger data","text":"Although can see generally positive association (expect), plotting may hiding information.","code":"\nrequire(NHANES)\ndim(NHANES)## [1] 10000    76\nNHANES %>%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"using-alpha-opacity","chapter":"3 Visualization","heading":"Using alpha (opacity)","text":"One way deal plotting set opacity low.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"geom_density2d","chapter":"3 Visualization","heading":"geom_density2d","text":"Alternatively (simultaneously) might prefer different geom altogether.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"multiple-layers","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"multiple-layers-1","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"things-i-havent-mentioned-much","chapter":"3 Visualization","heading":"Things I haven’t mentioned (much)","text":"coords (coord_flip() good know )coords (coord_flip() good know )themes (customizing appearance)themes (customizing appearance)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)transforming axestransforming axes","code":"\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"a-little-bit-of-everything","chapter":"3 Visualization","heading":"A little bit of everything","text":"","code":"\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"want-to-learn-more","chapter":"3 Visualization","heading":"Want to learn more?","text":"docs.ggplot2.org/docs.ggplot2.org/Winston Chang’s: R Graphics CookbookWinston Chang’s: R Graphics Cookbook","code":""},{"path":"viz.html","id":"what-else-can-we-do","chapter":"3 Visualization","heading":"What else can we do?","text":"shinyinteractive graphics / modelinginteractive graphics / modelinghttps://shiny.rstudio.com/https://shiny.rstudio.com/plotlyPlotly R package creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. plotly R library contains ggplotly function , convert ggplot2 figures Plotly object. Furthermore, option manipulating Plotly object style function.https://plot.ly/ggplot2/getting-started/Dynamic documentscombination RMarkdown, ggvis, shiny","code":""},{"path":"slr.html","id":"slr","chapter":"4 Simple Linear Regression","heading":"4 Simple Linear Regression","text":"","code":""},{"path":"slr.html","id":"a-linear-model","chapter":"4 Simple Linear Regression","heading":"4.1 A Linear Model","text":"Consider situation two variables, denote \\(x\\) \\(Y\\); \\(x\\) predictor variable, \\(Y\\) response. observe \\(n\\) observations, sample, denoted \\((x_i,y_i)\\). believe two variable related, namely \\[Y=f(x)+\\epsilon,\\] \\(\\epsilon\\) random error accounts fact know values \\(x\\) \\(f\\), still won’t know exactly \\(Y\\) .two variable case, assume \\(f(x)\\) linear function \\(x\\). , assume model \\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i.\\] attempts estimate function \\(f\\) now reduced trying estimate two numbers, intercept \\(\\beta_0\\) slope \\(\\beta_1\\): parameters.Consider following 4 models. Note differences statistics vs. parameters also individual observations vs. averages. Convince know use model.\\[\\begin{eqnarray*}\nE[Y_i|x_i] &=& \\beta_0 + \\beta_1 x_i \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n&& \\epsilon_i = y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\n&& e_i = y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"fitting-the-regression-line-least-squares","chapter":"4 Simple Linear Regression","heading":"4.1.1 Fitting the regression line: least squares","text":"fit regression line? Find \\(b_0\\) \\(b_1\\) minimize sum squared distance points line (called ordinary least squares):discusses previously, actually calculate \\(\\beta_0\\) \\(\\beta_1\\) need observe entire population. Instead, estimate quantities sample data . essentially trying find line fits data best. can thought line closest points sense. Given particular line particular point, think far point line? way think terms want model. Recall \\(x\\) predictor variable, \\(Y\\) response. linear regression context, set usually \\(x\\) something known beforehand, one goals predict response \\(Y\\). sense, way think good fitting line one vertical distance points line small.Residual: vertical distance point line. \\(^{th}\\) residual defined follows: \\[e_i=y_i-(b_0+b_1x_i)\\] \\(b_0\\) \\(b_1\\) intercept slope line consideration.","code":""},{"path":"slr.html","id":"notes","chapter":"4 Simple Linear Regression","heading":"Notes:","text":"vertical error measure reasonable predictor-response relationship. interested studying relationship height shoe size, vertical error idea doesn’t really exist (don’t think variables explanatory response - though certainly model approximate linear relationship). good fit might based perpendicular distance point line. reason vertical error model isn’t always ideal don’t always naturally consider one variables explanatory response. true, however, can (often ) model relationships variables don’t natural predictor - response relationship. , linear models cover, error (variable) term measured vertical direction.Now, find “best” fitting line, search line smallest residuals sense. particular, goal try find line minimizes following quantity: \\[Q=\\sum e_i^2 = \\sum (y_i-(b_0+b_1x_i))^2.\\]SSE: Sum squared errors (residuals), measure well line fits. SSE value \\(Q\\) optimal values \\(b_0\\) \\(b_1\\) given dataset.Finding \\(b_0\\) \\(b_1\\) minimize Q becomes calculus problem.\n\\[\\frac{dQ}{db_0}=-2\\sum (y_i-(b_0+b_1x_i)),\\qquad \\frac{dQ}{db_1}=-2\\sum\nx_i(y_i-(b_0+b_1x_i))\\] Setting equal 0 solving \\(b_0\\) \\(b_1\\) yields optimal values, denote \\(b_0\\) \\(b_1\\)Least Squares Estimates: \\(b_0\\) \\(b_1\\) (follows) minimize sum squared residuals, given :\n\\[b_0=\\bar{y}-b_1\\bar{x}, \\qquad b_1=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\n(x_i-\\bar{x})^2}\\] (Sometimes \\(b_0\\) \\(b_1\\) referred \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1.)\\) now can write \\[SSE=\\sum (y_i-(b_0+b_1x_i))^2\\] can note already knew discussion. switch roles \\(x\\) \\(Y\\), best fitting line different. relationship actually symmetric, switching roles \\(x\\) \\(Y\\) give slope \\(1/b_1\\), case.question might ask chose minimize \\(\\sum e_i^2\\) opposed perhaps \\(\\sum |e_i|\\), \\(|\\cdot|\\) denotes absolute value. original motivation math much easier first case, however, can shown “nice” situations, statistical properties better well.Consider good applet visualizing concept minimizing sums squares. Note applet also allows visualization line created minimizing sum absolute errors.Keep mind following notation:\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 x_i \\mbox{   true mean response}\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i \\mbox{   estimate mean response}\\\\\n &=& \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\ne_i &=& y_i - \\hat{y}_i\\\\\n\\epsilon_i &=& y_i - E[Y_i]\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"variance-sigma2","chapter":"4 Simple Linear Regression","heading":"4.1.2 Variance: \\(\\sigma^2\\)","text":"one variable (e.g., credit card balance), estimate variability response variable \\[\\mbox{sample variance} = \\frac{SSTO}{n-1} = \\frac{1}{(n-1)} \\sum_{=1}^n (y_i - \\overline{y})^2,\\] SSTO = sum squares total.regression setting, interested variance error terms (around line). particular, variance observed \\(y_i\\) around line given \\(\\sigma^2\\). estimate \\(\\sigma^2\\) using observed variability around line - residual. \\[SSE = \\sum_{=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{=1}^n e_i^2,\\] SSE sum squared errors (sometimes called sum square residuals). Note estimated two parameters, degrees freedom \\(df = n-2\\). best estimate \\(\\sigma^2\\) Mean Squared Error (MSE): \\[s^2 = MSE = \\frac{SSE}{n-2} = \\frac{1}{n-2} \\sum_{=1}^n (y_i - \\hat{y}_i)^2.\\] MSE = mean squared error.","code":""},{"path":"slr.html","id":"normal-errors-model","chapter":"4 Simple Linear Regression","heading":"4.2 Normal Errors Model","text":"least squares regression previous modeling hold probability model. However, inference easiest given normal probability model. Given linear model population:\n\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\]\n\\[\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\]","code":""},{"path":"slr.html","id":"important-features-of-the-model-with-normal-errors","chapter":"4 Simple Linear Regression","heading":"4.2.1 Important Features of the Model with Normal Errors","text":"\\(\\epsilon_i\\) random error term (, \\(\\epsilon_i\\) random variable).\\(Y_i\\) random variable \\(\\epsilon_i\\).assume \\(E[\\epsilon_i]=0\\), therefore \\(E[Y_i | x_i] = \\beta_0 + \\beta_1 x_i\\). (E expected value can thought long run average population mean.) , relationship explanatory response variables linear.\\(\\epsilon_i\\) amount \\(Y_i\\) values exceed fall short regression line.\\(var\\{Y_i | x_i\\} = var\\{\\epsilon_i\\} = \\sigma^2 \\rightarrow\\) constant variance around regression line.\\(SD\\{Y_i, Y_j\\} = SD\\{\\epsilon_i, \\epsilon_j \\} = 0 \\rightarrow\\) error terms uncorrelated.\\(\\epsilon_i \\sim N(0, \\sigma^2)\\), error terms normally distributed.\\(Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)\nFigure 1.2: Figs 1.6 Kutner et al. (2004).\n","code":""},{"path":"slr.html","id":"technical-conditions","chapter":"4 Simple Linear Regression","heading":"4.2.2 Technical Conditions","text":"Though use least squares regression criterion fit line observed data, setting series conditions allows us inference model. conditions crucial check whenever regression, satisfied, nothing data real meaning. , interpretations won’t make sense, inference won’t valid. conditions:Condition Linearity relationship actually linear:\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\] doesn’t make sense fit line data don’t believe relationship linear. \\(\\epsilon\\) term random variable, thus, two individuals measured value \\(x_i\\), \\(Y_i\\) , general, different. \\(Y_i\\) \nfunction \\(\\epsilon_i\\), \\(Y_i\\) also random variable.Furthermore, assuming mean \\(\\epsilon_i\\) 0. tells us fixed value \\(x_i\\), average value \\(Y_i\\) given \\(\\beta_0+\\beta_1 x_i\\). write \n\\[E[Y_i | x_i] = \\mu_{Y_i|x_i}=\\beta_0+\\beta_1 x_i\\] \\(\\mu\\) represents mean. fitted values estimates mean \\(Y_i\\) plugged-value \\(x_i\\).Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2.\\]Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2.\\]Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .","code":""},{"path":"slr.html","id":"maximum-likelihood","chapter":"4 Simple Linear Regression","heading":"4.2.3 Maximum Likelihood","text":"won’t cover details maximum likelihood. However, worth pointing maximum likelihood methods used many different areas statistics quite powerful. Additionally, simple linear regression case normal errors, maximum likelihood estimates turn exactly least squares estimates.","code":""},{"path":"slr.html","id":"reflection-questions-1","chapter":"4 Simple Linear Regression","heading":"4.3  Reflection Questions","text":"condition linear relationship appropriate?go estimating \\(\\beta_i, =0,1\\)?close estimates actual population values \\(\\beta_i, =0,1\\)?estimated function, actually interpret ?linear model conditions important inference?","code":""},{"path":"slr.html","id":"ethics-considerations","chapter":"4 Simple Linear Regression","heading":"4.4  Ethics Considerations","text":"matter technical conditions violated reporting analysis / model?technical conditions matter fitting line? Inference line? Neither? ?strong linear relationship predictor response variables found, mean predictor variables causes response?Simple Linear Regression called “simple?” model easy? (Spoiler: .)","code":""},{"path":"slr.html","id":"r-code-slr","chapter":"4 Simple Linear Regression","heading":"4.5 R code: SLR","text":"","code":""},{"path":"slr.html","id":"example-credit-scores","chapter":"4 Simple Linear Regression","heading":"4.5.1 Example: Credit Scores","text":"Consider dataset ISLR credit scores. don’t know sampling mechanism used collect data, unable generalize model results larger population. However, can look relationship variables build linear model. Notice lm() command always form: lm(response ~ explanatory).broom package three important functions:tidy() reports information based explanatory variableglance() reports information based overall modelaugment() reports information based observationWe can assume population model underlying relationship Limit average Balance. example, possible true (unknown) population model : \\[ E(Balance) = -300 + 0.2 \\cdot Limit.\\] Note consider Balance random Limit fixed. Also, note order inference, see error terms normally distributed around regression line constant variance values x.Consider someone $5,000 Limit credit card balance $1,000. population error term person : \\[\\epsilon_{5000 bal} = 1000 - [-300 + 0.2 \\cdot 5000] =  \\$700.\\]Consider someone $2,000 Limit credit card balance $50. population error term person : \\[\\epsilon_{2000 bal} = 50 - [-300 + 0.2 \\cdot 1000] =  -\\$50.\\]","code":"\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ISLR)\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) -293.     26.7         -11.0 1.18e- 24\n## 2 Limit          0.172   0.00507      33.9 2.53e-119\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.743         0.742  234.     1148. 2.53e-119     1 -2748. 5502. 5514.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  augment()## # A tibble: 400 × 8\n##    Balance Limit .fitted  .resid    .hat .sigma     .cooksd .std.resid\n##      <int> <int>   <dbl>   <dbl>   <dbl>  <dbl>       <dbl>      <dbl>\n##  1     333  3606    326.    6.87 0.00310   234. 0.00000135      0.0294\n##  2     903  6645    848.   55.3  0.00422   234. 0.000119        0.237 \n##  3     580  7075    922. -342.   0.00507   233. 0.00548        -1.47  \n##  4     964  9504   1338. -374.   0.0132    233. 0.0174         -1.61  \n##  5     331  4897    548. -217.   0.00251   234. 0.00109        -0.929 \n##  6    1151  8047   1088.   62.6  0.00766   234. 0.000280        0.269 \n##  7     203  3388    289.  -85.7  0.00335   234. 0.000227       -0.368 \n##  8     872  7114    928.  -56.2  0.00516   234. 0.000151       -0.241 \n##  9     279  3300    274.    5.39 0.00347   234. 0.000000929     0.0231\n## 10    1350  6819    878.  472.   0.00454   233. 0.00937         2.03  \n## # … with 390 more rows\nCredit %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) +\n  xlab(\"Credit limit (in $)\") +\n  ylab(\"Credit card balance (in $)\")"},{"path":"infslr.html","id":"infslr","chapter":"5 Inference on SLR Parameters","heading":"5 Inference on SLR Parameters","text":"normal error regression model important estimating line. computer happily minimize sum squares model choose. However, inference linear parameters, must normal error regression model. , unless otherwise stated, assume normal error regression model holds.\\[Y_i = \\beta_0 + \\beta_1 x_i  + \\epsilon_i\\] \\(x_i\\) known, \\(\\beta_0, \\beta_1\\) parameters, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently.Sampling Distribution: sampling distribution distribution statistic measured repeated random samples population. Consider sampling applet provides nice visual sampling distribution.","code":""},{"path":"infslr.html","id":"inference-on-beta_1","chapter":"5 Inference on SLR Parameters","heading":"5.1 Inference on \\(\\beta_1\\)","text":"recall: \\[b_1 = \\frac{\\sum(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum(x_i - \\overline{x})^2}\\]\nalso know ,\n\\[\\begin{eqnarray*}\nE[b_1] &=& \\beta_1\\\\\nVar\\{b_1\\} &=& \\frac{\\sigma^2}{\\sum(x_i - \\overline{x})^2}\\\\\nSE^2\\{ b_1 \\} &=& \\frac{MSE}{\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]\nvariability slope becomes smaller \\(x_i\\) values spread . make sense intuitively \\(x_i\\) values spread , slight deviations measurements (.e., different random samples) won’t change slope line much. \\(x_i\\) exist narrow range, easy get vastly different \\(b_1\\) values depending particular random sample.","code":""},{"path":"infslr.html","id":"distribution-of-b_1","chapter":"5 Inference on SLR Parameters","heading":"Distribution of \\(b_1\\)","text":"distribution \\(b_1\\) (condition \\(E[b_1] = \\beta_1\\) normal error regression model), seem familiar : \\[T = \\frac{b_1 - \\beta_1}{s\\{b_1\\}} \\sim t_{n-2}.\\]\nRecall use t distribution standardized variable dividing constant. Instead, divide standard error induces extra variability thus t distribution.Indeed, \\(H_0: \\beta_1=0\\) true, \n\\[\\begin{eqnarray*}\nT = \\frac{b_1 - 0}{s\\{b_1\\}} \\sim t_{n-2}\n\\end{eqnarray*}\\]\nNote degrees freedom now \\(n-2\\) estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). reject null hypothesis, \\(b_1\\) leads us t-statistic larger expect random chance (.e., p-value small).p-value: p-value probability observed data extreme , fact, null hypothesis true.","code":""},{"path":"infslr.html","id":"ci-for-beta_1","chapter":"5 Inference on SLR Parameters","heading":"CI for \\(\\beta_1\\)","text":"many statistics know standard error, can create intervals give us confidence statements (parameter) making. general, confidence intervals form:\\((1-\\alpha)100\\%\\) confidence interval slope parameter, \\(\\beta_1\\): \n\\[\\begin{eqnarray*}\nb_1 &\\pm& t^*_{\\alpha/2,n-2} s\\{b_1\\}\\\\\nb_1 &\\pm& t^*_{\\alpha/2, n-2} \\sqrt{MSE/\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]Remember \\(\\beta_1\\) random. randomness comes data, endpoints CI random. interpreting CI, give interval plausible values \\(\\beta_1\\) rejected done hypothesis test. Alternatively, think CI set values \\(\\beta_1\\) fairly certain contains \\(\\beta_1\\). repeat process many times, \\((1-\\alpha)100\\%\\) time interval captures true parameter., regression finding linear relationships. claim relationship explanatory response variables causative. randomized studies able find causative mechanisms.","code":""},{"path":"infslr.html","id":"parameter-interpretation","chapter":"5 Inference on SLR Parameters","heading":"Parameter Interpretation","text":"confidence interval gives us estimate parameter(s) model, goal understand interpretation quantities.Intercept \\(\\beta_0\\): average value \\(Y\\) \\(x\\) 0. Often, intercept interpretable . example, studying relationship height weight, \\(\\beta_0\\) average weight someone 0 inches tall. Nonsense. often \\(\\beta_0\\) placeholder, number needs specified interpretation.Slope \\(\\beta_1\\): \\(\\beta_1\\) can interpreted increase average \\(Y\\) \\(x\\) incremented single unit. \\[E[Y|x+1]-E[Y|x]=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1x)=\\beta_1\\]Variance \\(\\sigma^2\\): average squared deviation observation line.","code":""},{"path":"infslr.html","id":"estimating-a-response","chapter":"5 Inference on SLR Parameters","heading":"5.2 Estimating a response","text":"","code":""},{"path":"infslr.html","id":"interpolation-vs-extrapolation","chapter":"5 Inference on SLR Parameters","heading":"5.2.1 Interpolation vs Extrapolation","text":"One big problem using regression model condition linearity whether actually holds. result, attempting say something \\(Y\\) value \\(x^*\\) don’t typically data, can reasonably long \\(x^*\\) contained range data . called interpolation. Extrapolation hand trying use regression line outside range data. problem extrapolation evidence, ability check, whether linear condition holds beyond range data. Consider following example:interested amount crop produced single plant function amount compost added soil. take several plants, apply small, differing, amounts compost plants. see compost increases, output plant increases well, seemingly linear fashion. Naivete might lead us think , using regression line, can use 100 kilograms compost single plant receive huge amount crop return. truth, kill plant. evidence relationship linear anything outside range data, thus extrapolation mistake.interpolation, data support linear relationship, thus fine. trying say something happen particular values \\(x\\), call \\(x_h\\), need think exactly want say. know ahead time want say, can design experiment smart ways, see.","code":""},{"path":"infslr.html","id":"prediction-intervals-vs-confidence-intervals","chapter":"5 Inference on SLR Parameters","heading":"5.2.2 Prediction Intervals vs Confidence Intervals","text":"linear regression line gives us guess mean response individual particular value \\(x_h\\).conditions give us \\[E[Y|x]=\\beta_0+\\beta_1x\\]Plugging estimators, get \\[\\hat{y_i}=b_0+b_1 x_i\\] fitted value. However, accurate guess?confidence interval gives range plausible values \\(E[Y|x]\\). , mean response fixed value \\(x\\). confidence interval differs prediction interval, intended contain mean response, rather value response next individual observed value \\(x_h\\). result, prediction interval need larger confidence interval.","code":""},{"path":"infslr.html","id":"variability-of-point-estimates","chapter":"5 Inference on SLR Parameters","heading":"5.2.3 Variability of point estimates","text":"following variances \\(b_0\\), \\(b_1\\), fitted value \\(x_h\\): \\(\\hat{y}_{x_h}\\), new value \\(x_h\\): \\(\\hat{y}_{x_h(new)}\\).\\(\\sigma^2\\) variance errors.\\[\\begin{eqnarray*}\n\\mbox{var}(b_0)&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(b_1)&=&\\frac{\\sigma^2}{\\sum(x_i-\\bar{x})^2}\\\\\n\\mbox{var}(\\hat{y}_{x_h})&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(\\hat{y}_{x_h(new)})&=& \\sigma^2 + \\mbox(var)(\\hat{y}_{x_h}) = \\sigma^2\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\n\\end{eqnarray*}\\]quantities estimated replacing \\(\\sigma^2\\) guess, \\(MSE\\). \\(b_0\\) fitted value (\\(x_h=0\\)), variability \\(b_0\\) special case 3rd formula. difference last two one \\(\\sigma^2\\). make sense, variance next observation (point) variance mean (.e., predicted regression line) plus variance error top mean.","code":""},{"path":"infslr.html","id":"standard-error","chapter":"5 Inference on SLR Parameters","heading":"5.2.3.1 Standard Error:","text":"phrase “standard error” indicates variability (square root variance) statistic situation variability estimate (, MSE used instead \\(\\sigma^2\\)). SE quantities therefore given :\\[\\begin{eqnarray*}\n\\mbox{SE}(b_0)&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(b_1)&=& \\sqrt{\\frac{MSE}{\\sum(x_i-\\bar{x})^2}}\\\\\n\\mbox{SE}(\\hat{y}_{x_h})&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(\\hat{y}_{x_h(new)})&=& \\sqrt{MSE + \\mbox(SE)(\\hat{y}_{x_h})^2} = \\sqrt{MSE\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\n\\end{eqnarray*}\\]Confidence intervals essentially best guess plus minus two standard errors. exact, instead 2, use \\(qt(.975,n-p)\\) \\(p\\) number coefficients estimated (SLR \\(p=2\\)). idea smaller standard errors, narrower confidence intervals , information .","code":""},{"path":"infslr.html","id":"notes-1","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"prediction future (mean) response accurate \\(x_h = \\overline{x}\\). Think behavior regression line away \\(\\overline{x}\\). line much variable extremes.Confidence limits (.e., % coverage) \\(E[Y_h], \\beta_0, \\beta_1\\) sensitive departures normality. Especially large sample sizes (central limit theorem).Coverage percentages \\(\\hat{y}_h\\) sensitive departures normality. central limit theorem estimating average.Confidence limits apply one confidence interval (.e., entire line). won’t simultaneously 95% confident lots different intervals. ’ll address issue later.","code":""},{"path":"infslr.html","id":"anova-approach-to-regression","chapter":"5 Inference on SLR Parameters","heading":"5.3 ANOVA approach to regression","text":"mentioned previously, can think variability \\(Y\\) total variability. measure variability without knowing anything explanatory variable(s). , knowledge explanatory variables helps us predict \\(Y\\), , remove variability associated response. Consider following terms\\[\\begin{eqnarray*}\nSSTO &=& \\sum (y_i - \\overline{y})^2\\\\\nSSE &=& \\sum (y_i - \\hat{y}_i)^2\\\\\nSSR &=& \\sum (\\hat{y}_i - \\overline{y})^2\n\\end{eqnarray*}\\]SSTO sum squares total; SSE sum squared errors; SSR sum squares regression line. Unsquared, residuals nice relationship:\\[\\begin{eqnarray*}\ny_i - \\overline{y} &=& \\hat{y}_i - \\overline{y} + y_i - \\hat{y}_i\\\\\n\\mbox{total deviation} &=& \\mbox{dev reg around mean} + \\mbox{dev around line}\n\\end{eqnarray*}\\]\nresiduals squared, obvious \\[SSTO = SSR + SSE.\\] However, can derive relationship using algebra:\n\\[\\begin{eqnarray*}\n\\sum(y_i - \\overline{y})^2 &=& \\sum [ (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)]^2\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2 + 2 \\sum (\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2\\\\\n\\end{eqnarray*}\\]\nlast term zeroed using equations (1.17) (1.20) ALSM (pgs 23-24).","code":""},{"path":"infslr.html","id":"mean-squares","chapter":"5 Inference on SLR Parameters","heading":"5.3.1 Mean Squares","text":"Sums squares increasing number data values. accommodate number observations, use mean squares instead sums square","code":""},{"path":"infslr.html","id":"notes-2","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"MSE estimates \\(\\sigma^2\\) regardless whether \\(\\beta_1 = 0\\).\\(\\beta_1=0\\), MSR also estimates \\(\\sigma^2\\).comparison MSR MSE seem indicate whether \\(\\beta_1=0\\).\nNote can think MSR variability regression line around line \\(\\overline{y}\\). \\(\\beta_1=0\\), regression line varies MSR measuring natural variability error terms (\\(\\sigma^2\\)). \\(\\beta_1 \\ne 0\\), \\(b_1\\) values still vary naturally PLUS bit difference line \\(\\beta_1\\) line \\(\\mu_Y\\).","code":""},{"path":"infslr.html","id":"f-test-of-beta_1-0-versus-beta_1-ne-0","chapter":"5 Inference on SLR Parameters","heading":"5.3.2 F test of \\(\\beta_1 = 0\\) versus \\(\\beta_1 \\ne 0\\)","text":"\\[\\begin{eqnarray*}\nH_0: \\beta_1 = 0\\\\\nH_a: \\beta_1 \\ne 0\n\\end{eqnarray*}\\]\ntest statistic \\[F = \\frac{MSR}{MSE} = \\frac{\\sum(\\hat{y}_i - \\overline{y})^2}{\\sum(y_i - \\hat{y}_i)^2 / (n-2)}.\\] Large values \\(F\\) support \\(H_a\\), values \\(F\\) close 1 support \\(H_0\\). \\(H_0\\) true, \\[F \\sim F_{1,n-2}.\\] Note F-test always one-sided test (meaning reject BIG values \\(F\\)), though assessing two-sided hypothesis.Notice anova() output includes information sums squares F-test calculate \\(R^2\\). Also, tidy() function broom output dataframe easy work .","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA"},{"path":"infslr.html","id":"equivalence-of-f-and-t-tests","chapter":"5 Inference on SLR Parameters","heading":"5.3.3 Equivalence of F and t-tests","text":"\\[F = \\frac{SSR}{SSE/(n-2)} = \\frac{b_1^2 \\sum(x_i - \\overline{x})^2}{MSE} = \\frac{b_1^2}{MSE/\\sum(x_i - \\overline{x})^2} = \\bigg(\\frac{b_1}{s\\{b_1\\} }\\bigg)^2 = (T)^2\\]\n’re going continue use test models get complicated. general strategy always :Fit full model: \\(SSE_{full} = \\sum(y_i - b_0 - b_1 x_i)^2 = \\sum(y_i - \\hat{y}_i)^2\\)Fit reduced model (\\(H_0\\)): \\(SSE_{reduced} = \\sum(y_i - b_0)^2 = \\sum(y_i - \\overline{y})^2 = SSTO\\)\\(F = \\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}} \\div \\frac{SSE_{full}}{df_{full}} = \\frac{MSR}{MSE}\\)","code":""},{"path":"infslr.html","id":"descriptive-measures-of-linear-association","chapter":"5 Inference on SLR Parameters","heading":"5.4 Descriptive Measures of Linear Association","text":"discuss r (correlation) \\(R^2\\) (coefficient determination) descriptive measures linear association typically interested estimating parameter. Instead, \\(r\\) \\(R^2\\) tell us well linear model fits data.","code":""},{"path":"infslr.html","id":"correlation","chapter":"5 Inference on SLR Parameters","heading":"5.4.1 Correlation","text":"Consider scatterplot, variability directions: \\((x_i - \\overline{x}) \\ \\& \\ (y_i - \\overline{y})\\). data shown represent crop types taken World Data part Tidy Tuesday. point plot different country. x y variables represent proportion total yield last 50 years due crop type.\nFigure 5.1: % total yield different crops (across last 50 years). point represents country. Now lines average x average y values superimposed onto plots.\nred dot (plot), consider distance observation \\(\\overline{X}\\) line \\(\\overline{Y}\\) line. observation (red dot) ? ? one ?particular red dot (observation) contribute correlation? positive way (make \\(r\\) bigger)? negative way (make \\(r\\) smaller)?Positive Relationship: \\(x\\) increases, \\(Y\\) also tends increase, two variables said positive relationship (example: shoe size height).Negative Relationship: \\(x\\) increases, \\(Y\\) tends decrease, two variables said negative relationship (example: outside temperature heating oil used).variables positive relationship, \\(r=\\sqrt{R^2}\\). variables negative relationship, \\(r=-\\sqrt{R^2}\\).\\(r\\) can calculated directly well, given following formula:\\[\\begin{eqnarray*}\n\\mbox{sample covariance}&&\\\\\ncov(x,y) &=& \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\\n\\mbox{sample correlation}&&\\\\\nr(x,y) &=& \\frac{cov(x,y)}{s_x s_y}\\\\\n&=& \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\\n&=& \\frac{\\sum[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\end{eqnarray*}\\]numerator correlation describes relationship \\(x\\) \\(y\\). , \\(x\\) tends large (mean), \\(y\\) also tends large (mean), product two positive numbers, positive. Likewise, \\(x\\) small, \\(y\\) also tends small, product two negative numbers, positive. positive relationship result lot positive numbers summed numerator, thus \\(r\\) positive.relationship negative, one tend negative positive, thus sum involve lot negative terms causing \\(r\\) negative. denominator always positive.One thing note \\(r\\) affected choice label predictor label response. roles \\(x\\) \\(y\\) switched, \\(r\\) remain unaffected. Thus, \\(r\\), unlike value line, symmetric.\\(-1 \\leq r \\leq 1\\).\\(b_1 = r \\frac{s_y}{s_x}\\)\\(r=0, b_1=0\\)\\(r=1, b_1 > 0\\) can anything!\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)","code":""},{"path":"infslr.html","id":"coefficient-of-determination","chapter":"5 Inference on SLR Parameters","heading":"5.4.2 Coefficient of Determination","text":"coefficient determination, given \\(R^2\\), nice interpretation, proportion variability \\(y\\) explained variable \\(x\\). Recall, \\(SSE\\), sum squared errors (residuals) measure amount variable remaining data accounting information \\(x\\). \\(SSTO\\) total sums squares, measured total amount variability variable \\(y\\). coefficient determination given \n\\[R^2=1-\\frac{SSE}{SSTO}\\]\n, defining \\(SSR\\) regression sum squares (amount variation explained regression) \\[R^2=\\frac{SSR}{SSTO}.\\]Limitations:\n1. High \\(R^2\\) necessarily produce good “predictions.” lot variability around line (.e., within data), can wide prediction intervals response.\n2. High \\(R^2\\) necessarily mean line good fit. Quadratic () relationships can sometimes lead high \\(R^2\\). Additionally, one outlier can huge effect value \\(R^2\\).\n3. \\(R^2 \\approx 0\\) mean relationship \\(x\\) \\(y\\). Instead, \\(x\\) \\(y\\) might perfect quadratic relationship.","code":""},{"path":"infslr.html","id":"reflection-questions-2","chapter":"5 Inference on SLR Parameters","heading":"5.5  Reflection Questions","text":"different ways use inference model parameters?difference prediction confidence interval? used?sums squares broken meaningful pieces? differences SSTO, SSE, SSR?interpretation \\(R^2\\)?","code":""},{"path":"infslr.html","id":"ethics-considerations-1","chapter":"5 Inference on SLR Parameters","heading":"5.6  Ethics Considerations","text":"technical conditions violated, CI tell us slope parameter? normality condition?technical conditions violated PI tell us predicted values? normality condition?confounding variables might exist link Limit Balance positively correlated causal?population might ames data representative ? population might Credit data representative ? (Hint: look data documentation typing ?ames ?Credit.)","code":""},{"path":"infslr.html","id":"ames-inf","chapter":"5 Inference on SLR Parameters","heading":"5.7 R: SLR Inference","text":"","code":""},{"path":"infslr.html","id":"cis","chapter":"5 Inference on SLR Parameters","heading":"5.7.1 CIs","text":"Also available broom CIs coefficients. Consider ames dataset available openintro package.Data set contains information Ames Assessor’s Office used computing assessed values individual residential properties sold Ames, IA 2006 2010.reasons discuss coming chapters, ’ll consider \\(\\ln\\) price home, also consider homes less 3000 square feet.R t-test automatically, done hand using provided SE (available column called std.error).\\[\\begin{eqnarray*}\nH_0:&& \\beta_1 = 0\\\\\nH_a:&& \\beta_1 \\ne 0\\\\\nt &=& \\frac{0.000613 - 0}{0.0000114} = 53.86\\\\\np-value &=& 2 P(t_{2902} \\geq 53.86) = 2*(1-pt(53.86, 2902)) = \\mbox{small}\\\\\n\\end{eqnarray*}\\]p-value small, reject null hypothesis: model linear relationship area price_ln, slope coefficient must different zero (greater zero one-sided test). Note knowing positive relationship tell us price_ln result area. , reason believe causative mechanism.90% confidence interval slope coefficient, \\(\\beta_1\\) (0.000595, 0.000632). true population slope model area price_ln somewhere (0.000595, 0.000632). Note even though values seem small, significantly (necessarily substantially) away zero. tempted confuse small zero, magnitude slope coefficient depends heavily units measurement variables.","code":"\nlibrary(openintro)\names_inf <- ames %>%\n  filter(area <= 3000) %>%\n  mutate(price_ln = log(price))\names_inf %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\names_inf %>%\n  lm(price_ln ~ area, data = .) %>% \n  tidy(conf.int = TRUE, conf.level = 0.9)## # A tibble: 2 × 7\n##   term         estimate std.error statistic p.value  conf.low conf.high\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0 11.1      11.1     \n## 2 area         0.000614 0.0000114      53.9       0  0.000595  0.000632"},{"path":"infslr.html","id":"predictions","chapter":"5 Inference on SLR Parameters","heading":"5.7.2 Predictions","text":"","code":""},{"path":"infslr.html","id":"predicting-ames","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.1 Predicting Ames","text":"Fortunately, R allows creating mean prediction intervals. need create new data set variable name predictor, value interested , might call new_ames. use augment() give either confidence prediction interval, follows., create intervals entire range explanatory variables:, turns , easier ways computer find confidence prediction intervals:Also, graphs can made, ’ll need keep output (use entire dataset instead new_ames).","code":"\n# store the linear model object so that we can use it later.\names_lm <- lm(price_ln ~ area, data = ames_inf)\n\n# create a new dataframe\nnew_ames <- data.frame(area = c(1000, 1500, 2000))\n\n# get df from the model\names_df <- ames_lm %>% glance() %>% select(df.residual) %>% pull()\names_df## [1] 2902\n# new data predictions\names_pred <- ames_lm %>% \n  augment(newdata = new_ames, type.predict = \"response\", se_fit = TRUE)\names_pred## # A tibble: 3 × 3\n##    area .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000    11.7 0.00760\n## 2  1500    12.0 0.00527\n## 3  2000    12.3 0.00792\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, ames_df)\ncrit_val## [1] 1.96\n# SE of the mean response\nse_fit <- ames_pred %>% select(.se.fit) %>% pull()\nse_fit##       1       2       3 \n## 0.00760 0.00527 0.00792\n# esimate of the overall variability, sigma\names_sig <- ames_lm %>% glance() %>% select(sigma) %>% pull()\names_sig## [1] 0.284\n# calculate the SE of the predictions\nse_pred <- sqrt(ames_sig^2 + se_fit^2)\nse_pred##     1     2     3 \n## 0.284 0.284 0.284\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\names_pred <- ames_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\names_pred## # A tibble: 3 × 7\n##    area .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000    11.7 0.00760     11.2     12.3     11.7     11.7\n## 2  1500    12.0 0.00527     11.5     12.6     12.0     12.0\n## 3  2000    12.3 0.00792     11.8     12.9     12.3     12.3\names_pred_all <- ames_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(ames_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\names_pred_all %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")\names_lm %>% \n  augment(newdata = new_ames, interval = \"confidence\")## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.7   11.7\n## 2  1500    12.0   12.0   12.0\n## 3  2000    12.3   12.3   12.3\names_lm %>% \n  augment(newdata = new_ames, interval = \"prediction\")  ## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.2   12.3\n## 2  1500    12.0   11.5   12.6\n## 3  2000    12.3   11.8   12.9\names_conf <- ames_lm %>% \n  augment(interval = \"confidence\")\n\names_pred <- ames_lm %>% \n  augment(interval = \"prediction\")  \n\names_pred %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2) + \n  geom_ribbon(ames_conf, mapping = aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"predicting-credit","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.2 Predicting Credit","text":"contrast ames data, note predictions (particulary CI around line) drastically changes sample size small., create intervals entire range explanatory variables:","code":"\nlibrary(ISLR)  # source of the Credit data\n# store the linear model object so that we can use it later.\ncredit_lm <- lm(Balance ~ Limit, data = Credit)\n\n# create a new dataframe\nnew_credit <- data.frame(Limit = c(1000, 3000, 7000))\n\n# get df from the model\ncredit_df <- credit_lm %>% glance() %>% select(df.residual) %>% pull()\ncredit_df## [1] 398\n# new data predictions\ncredit_pred <- credit_lm %>% \n  augment(newdata = new_credit, type.predict = \"response\", se_fit = TRUE)\ncredit_pred## # A tibble: 3 × 3\n##   Limit .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000   -121.    22.2\n## 2  3000    222.    14.6\n## 3  7000    909.    16.4\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, credit_df)\ncrit_val## [1] 1.97\n# SE of the mean response\nse_fit <- credit_pred %>% select(.se.fit) %>% pull()\nse_fit##    1    2    3 \n## 22.2 14.6 16.4\n# esimate of the overall variability, $\\sigma$\ncredit_sig <- credit_lm %>% glance() %>% select(sigma) %>% pull()\ncredit_sig## [1] 234\n# calculate the SE of the predictions\nse_pred <- sqrt(credit_sig^2 + se_fit^2)\nse_pred##   1   2   3 \n## 235 234 234\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\ncredit_pred <- credit_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\ncredit_pred## # A tibble: 3 × 7\n##   Limit .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000   -121.    22.2    -582.     340.    -165.    -77.4\n## 2  3000    222.    14.6    -238.     682.     193.    251. \n## 3  7000    909.    16.4     448.    1369.     876.    941.\ncredit_pred_all <- credit_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(credit_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\ncredit_pred_all %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"anova-output","chapter":"5 Inference on SLR Parameters","heading":"5.7.3 ANOVA output","text":"Note tidy() creates dataframe slighlty easier work , regardless tidy() anova() function provides exact output.","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() ## Analysis of Variance Table\n## \n## Response: price_ln\n##             Df Sum Sq Mean Sq F value Pr(>F)    \n## area         1    233   233.4    2901 <2e-16 ***\n## Residuals 2902    234     0.1                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA\names_inf %>%\n  mutate(bedrooms = case_when(\n    Bedroom.AbvGr <=1 ~ \"1\",\n    Bedroom.AbvGr <=2 ~ \"2\",\n    Bedroom.AbvGr <=3 ~ \"3\",\n    TRUE ~ \"4+\"\n  )) %>%\n  ggplot(aes(x = area, y = price_ln, color = bedrooms)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)"},{"path":"diag1.html","id":"diag1","chapter":"6 Diagnostic Measures I","heading":"6 Diagnostic Measures I","text":"","code":""},{"path":"diag1.html","id":"model-conditions","chapter":"6 Diagnostic Measures I","heading":"6.1 Model Conditions","text":"linear relationshipconstant varianceindependent errorsnormal errorsno outliers (’s part normality condition)","code":""},{"path":"diag1.html","id":"notes-3","chapter":"6 Diagnostic Measures I","heading":"Notes:","text":"required conditions explanatory / predictor variable.explanatory variable binary, SLR becomes two-sided t-test.\nRemember, \\(x_i\\) constants, don’t think distribution. Also, allowed construct whatever \\(x_i\\) want. said,larger range \\(x_i\\) produce less variable predictions.However, outliers x-direction can influential.","code":""},{"path":"diag1.html","id":"residuals","chapter":"6 Diagnostic Measures I","heading":"6.1.1 Residuals","text":"\\[\\begin{eqnarray*}\n\\mbox{residual: } e_i &=& Y_i -  \\hat{Y}_i \\ \\ \\ \\mbox{can measure}\\\\\n\\mbox{error term: } \\epsilon_i &=& Y_i - E[Y_i] \\ \\ \\ \\mbox{measure}\n\\end{eqnarray*}\\]mean: \\(\\overline{e} = 0\\) (definition!). Therefore, average residuals provides information whether \\(E[\\epsilon]=0\\).\n(note: \\(\\sum e_i = \\sum(Y_i - b_0 - b_1 x_i) = 0\\) \\(\\frac{\\delta Q}{\\delta \\beta_0} = 0\\).)variance: \\[s^2 = \\frac{1}{(n-2)} \\sum (Y_i - \\hat{Y}_i)^2 = \\frac{1}{(n-2)} \\sum (e_i)^2 = \\frac{1}{(n-2)} \\sum (e_i - \\overline{e})^2 = \\frac{SSE}{(n-2)} = MSE\\]\nnon-independent: \\(\\sum e_i=0\\), residuals independent. \\(\\epsilon_i\\), errors, assume independent.","code":""},{"path":"diag1.html","id":"semistudentized-residuals","chapter":"6 Diagnostic Measures I","heading":"semistudentized residuals","text":"\\[e_i^* = \\frac{e_i - \\overline{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\\] MSE isn’t quite variance \\(e_i\\). actually estimate variance \\(\\epsilon_i\\). good enough right now.","code":""},{"path":"diag1.html","id":"diagnostic-plots-of-residuals","chapter":"6 Diagnostic Measures I","heading":"6.2 Diagnostic Plots of Residuals","text":"[1] Plots residuals vs. predictor variables / explanatory variables.\n[3] Plots residuals vs. fitted values.\n[6] Box plot (histogram) residuals.plots show conditions don’t hold??\n1. Abandon regression model use something appropriate. interested coefficients: nonparametrics, time series, random forests. interested prediction, maybe okay conditions don’t hold? maybe (e.g., independence key!). prediction models include: random forests, support vector machines, neural networks, loess (smoothing).\n2. Transform variables model hold.","code":""},{"path":"diag1.html","id":"violating-linearity","chapter":"6 Diagnostic Measures I","heading":"Violating Linearity","text":"see: pattern scatterplot isn’t linearYou :\n- include \\(x^2\\) term function \\(x\\).\n- can linearize non-linear function, interpretation can get complicated.","code":""},{"path":"diag1.html","id":"violating-constant-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Constant Errors","text":"see:\n- typically see errors increase \\(x\\) increases (sometimes easier see absolute value residuals).\n- hugely serious problem (less efficient estimates, variance estimates correct):\n- direct approach use weighted least squares (won’t talk )\n- often transformations stabilize variance","code":""},{"path":"diag1.html","id":"violating-independent-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Independent Errors","text":"see:\n- typically due another variable (time, geographic location, etc.)\n- residual plotted variable.:\n- work model accounts correlated error structure (e.g., time series)","code":""},{"path":"diag1.html","id":"violating-normal-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Normal Errors","text":"see:\n- residual plots symmetric\n- general empirical rule: \\(68\\% \\pm \\sqrt{MSE}\\), \\(95\\% \\pm 2\\sqrt{MSE}\\) (concerned grossly different)\n- won’t cover normal probability plots (also called q-q plots):\n- non-normality non-constant variance often related, transformations typically fix .","code":""},{"path":"diag1.html","id":"having-outliers","chapter":"6 Diagnostic Measures I","heading":"Having Outliers","text":"see:\n- typically easiest see standardized studentized residuals\n- SLR resistant outliers\n- expect 95% studentized residuals within \\(\\pm 2\\).:\n- outliers can seriously deform least squares estimate. good reason keep value(s), might consider nonparametric method places less weight point.","code":""},{"path":"diag1.html","id":"why-do-we-plot-resid-vs.-fitted-and-not-vs.-observed","chapter":"6 Diagnostic Measures I","heading":"Why do we plot resid vs. fitted and not vs. observed?","text":"know \\(e_i\\) \\(\\hat{Y}_i\\) uncorrelated (can shown using linear algebra, also note \\(\\sum e_i \\hat{Y}_i = 0\\)). , go resid vs. fitted scatterplot resid vs. observed scatter plot, shift point x-direction () amount equal residual. residual negative, point shift left. residual positive, point shift right. thus create positively correlated relationship (resid observed). degree shift depend relative magnitudes residuals predicted values.\\(\\Rightarrow e_i \\mbox{ } Y_i\\) correlated therefore independent. Consider two examples . examples, residual correlated response variable. However, easier see correlation residual also responsible relationship response variable explanatory variable.\nFigure 1.3: correlated data, hard see dependence response variable residuals. However, careful look third plot shows slightly stronger correlating response variable residuals fitted values residuals.\n\nFigure 1.4: uncorrelated data, much easier see dependence response variable residuals.\n","code":""},{"path":"diag1.html","id":"transformations","chapter":"6 Diagnostic Measures I","heading":"6.3 Transformations","text":"Important note!! idea behind transformations make model appropriate possible data hand. want find correct linear model; want conditions hold. trying find significant model big \\(R^2\\).\nFigure 6.1: Taken Applied Linear Statistical Models, 5th ed. Kutner et al. Figures 3.13 3.15.\n","code":""},{"path":"diag1.html","id":"correcting-condition-violations","chapter":"6 Diagnostic Measures I","heading":"6.4 Correcting Condition Violations","text":"noticed non-linear relationship, certainly think fit non-linear trend line. time, might consider fitting exponential\nrelationship, functional form. still called linear regression, transform data \nfitting linear model \\(f(y)\\) \\(g(x)\\). way, techniques theory still follow, least squares etc. \ninstance, might think fitting linear relationship new variables \\(y^*=\\sqrt{y}\\) \\(x\\), possibly \\(y\\) \\(x^*=x^2\\). Occasionally, might think transforming variables. One general rule follows:initial fit shows violation linear condition , best transform \\(x\\).initial fit shows violation linear condition well normality issues heteroscedasticity, transform \\(y\\) considered. hopefully transformation correct problems ., changing \\(x\\) changes shape relationship. Changing \\(Y\\) changes error structure (also often shape).Note: Formal testing violations model conditions usually good idea. ? Multiple testing problems arise (extra type errors); additional tests typically sensitive technical conditions; lose power detect differences interest.","code":""},{"path":"diag1.html","id":"gdp-example","chapter":"6 Diagnostic Measures I","heading":"6.4.1 GDP Example","text":"Consider following data collected World Bank 20201. data include GDP % Urban Population. description variables defined World Bank provided .2GDP: “GDP per capita gross domestic product divided midyear population. GDP sum gross value added resident producers economy plus product taxes minus subsidies included value products. calculated without making deductions depreciation fabricated assets depletion degradation natural resources. Data current U.S. dollars.”Urban Population (% total): “Urban population refers people living urban areas defined national statistical offices. calculated using World Bank population estimates urban ratios United Nations World Urbanization Prospects.”\nFigure 6.2: seems though original data don’t meet LINE conditions needed inference linear model.\nLet’s try transform variables get model seems conform LINE technical conditions.\nFigure 6.3: ln(urban) vs gdp: seems like taking natural log urban makes relationship worse.\nAlas, really seems like gdp variable problem urban variable. Let’s transform gdp instead.\nFigure 1.5: urban vs gdp^2: squaring gdp also makes relationship worse.\nneeded transformation spread small gdp values shrink large gdp values.\nFigure 1.6: urban vs sqrt(gdp): square root gdp seems help!\nnatural log stronger function square root (, shrink large values even .).\nFigure 1.7: urban vs ln(gdp): natural log gdp creates residual plot seems follow LINE technical conditions.\nBox-Cox transformations class transformations. Generally, \\(\\ln, \\exp\\), square root, polynomial transformations sufficient fit linear model satisfies necessary technical conditions. won’t spend time learning Box-Cox, can read learn transforming variables.","code":""},{"path":"diag1.html","id":"interpreting-regression-coefficients","chapter":"6 Diagnostic Measures I","heading":"Interpreting Regression Coefficients","text":"Example 1: Transforming \\(x\\): \\(x' = \\ln(x)\\) \\[E[Y] = \\beta_0  + \\beta_1 \\ln(x)\\] can interpret \\(\\beta_1\\) following way: every increase 1 unit \\(\\ln(x)\\), \\(E[Y]\\) increases \\(\\beta_1\\). isn’t meaningful statement. Instead, consider \\[E[Y| \\ln(2x)] - E[Y|\\ln(x)] = \\beta_1 \\ln(2).\\] can interpreted doubling \\(x\\) gives additive increase E[Y] \\(\\beta_1 \\ln(2)\\) units.Example 2: Transforming \\(Y\\): \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 x\\] Note also \\[ E[\\ln(Y)] = median(\\ln(Y))\\] distribution \\(\\ln(Y)\\) symmetric around regression line.\\[\\begin{eqnarray*}\nmedian(\\ln(Y)) = \\beta_0 + \\beta_1 x\\\\\nmedian(Y) = e^{\\beta_0} e^{\\beta_1 x}\\\\\n\\frac{median(Y| x+1)}{median(Y| x)} = e^{\\beta_1}\\\\\n\\end{eqnarray*}\\]\nincrease 1 unit x associated multiplicative change \\(e^{\\beta_1}\\) median(\\(Y\\)). Important keep mind \n\\[\\begin{eqnarray*}\nE[\\ln(Y)] \\ne \\ln(E[Y])\\\\\nmedian(\\ln(Y)) = \\ln(median(Y))\n\\end{eqnarray*}\\]Example 3: Transforming \\(x\\) \\(Y\\): \\(x' = \\ln(x)\\) \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 \\ln(x)\\]\n\\[\\begin{eqnarray*}\n\\frac{median(Y|2x)}{median(Y|x)} &=& \\frac{e^{\\beta_0 + \\beta_1 \\ln(2x)}}{e^{\\beta_0 + \\beta_1 \\ln(x)}}\\\\\n&=& e^{\\beta_1 (\\ln(2x) - \\ln(x))}\\\\\n&=& e^{\\beta_1 \\ln(2)} = 2^{\\beta_1}\n\\end{eqnarray*}\\]\ndoubling \\(x\\) associated multiplicative change \\(2^{\\beta_1}\\) median Y.Note model regressing GDP % urban used log transformation GDP (response, Y variable). know :\\[\\begin{eqnarray*}\n\\widehat{\\ln(Y)} &=& \\mbox{median}(\\ln(Y)) = b_0 + b_1 \\cdot x\\\\\n\\mbox{median}(Y) &=& \\exp(b_0 + b_1 \\cdot x)\n\\end{eqnarray*}\\]using coefficients output linear model regressing ln_gdp urban, can find model predicts median gdp (transformed!) function urban.\\[\\begin{eqnarray*}\n\\mbox{median}(\\verb;gdp;) &=& \\exp(6.11 +  0.0425 \\cdot \\verb;urban;)\n\\end{eqnarray*}\\]\nFigure 6.4: blue exponential line represents median GDP particular value % urban.\n","code":"\nGDP %>%\n  lm(ln_gdp ~ urban, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   6.11     0.202        30.3 1.18e-74\n## 2 urban         0.0425   0.00308      13.8 2.29e-30\nGDP %>%\n  ggplot() + \n  geom_point(aes(x = urban, y = gdp)) + \n  geom_line(aes(x = urban, y = exp(6.11 + 0.0425*urban)), color = \"blue\")"},{"path":"diag1.html","id":"reflection-questions-3","chapter":"6 Diagnostic Measures I","heading":"6.5  Reflection Questions","text":"know model conditions hold?model conditions don’t hold?log transformation, slope coefficient interpreted?happens want add lots explanatory variables model?add quadratic term?","code":""},{"path":"diag1.html","id":"ethics-considerations-2","chapter":"6 Diagnostic Measures I","heading":"6.6  Ethics Considerations","text":"GDP model country observational unit (192 countries). quick Google shows UN currently defines 193 countries Leaving aside might one short, technical conditions need apply entire population? Can think country data 2020 representative (unknown?) population?mammals dataset collected 1976. reasons 50 year old dataset might represent accurate model population today?","code":""},{"path":"diag1.html","id":"r-slr-inference","chapter":"6 Diagnostic Measures I","heading":"6.7 R: SLR Inference","text":"Consider mammals dataset MASS package (careful MASS, overwrite dplyr functions filter select) representing average brain (g) body (kg) weights 62 species land mammals3.first glance linear model, doesn’t seem like really linear relationship ! certainly residuals don’t look like ’d hoped LINE technical conditions.Let’s try log transformations (square root transformations might also make sense).\nFigure 6.5: Taking natural log body weight doesn’t seem create model linear shape.\n\nFigure 6.6: Taking natural log brain weight also doesn’t seem create model linear shape.\n\nFigure 6.7: Taking natural log brain body weight seem create model linear shape!\n","code":"\nlibrary(MASS)  # be careful with MASS, it messes up filter and select\ndata(mammals)\n\nmammals %>% \n  ggplot(aes(x = body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\nmammals %>%\n  lm(brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\nmammals <- mammals %>%\n  mutate(ln_body = log(body),\n         ln_brain = log(brain))\n\nlibrary(patchwork)  # to get the plots next to one another\n\np1 <- mammals %>% \n  ggplot(aes(x = ln_body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np2 <- mammals %>%\n  lm(brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np1 + p2\np3 <- mammals %>% \n  ggplot(aes(x = body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np4 <- mammals %>%\n  lm(ln_brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np3 + p4\np5 <- mammals %>% \n  ggplot(aes(x = ln_body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np6 <- mammals %>%\n  lm(ln_brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np5 + p6"},{"path":"simult.html","id":"simult","chapter":"7 Simultaneous Inference","heading":"7 Simultaneous Inference","text":"(sections 4.1, 4.2, 4.3 ALSM)Spring 2022: cover joint estimation \\(\\beta_0\\) \\(\\beta_1\\), cover simultaneous estimation mean response simultaneous prediction intervals new observations.","code":""},{"path":"simult.html","id":"joint-estimation-of-beta_0-and-beta_1","chapter":"7 Simultaneous Inference","heading":"7.1 Joint Estimation of \\(\\beta_0\\) and \\(\\beta_1\\)","text":"Note inference \\(\\beta_1\\) can performed \\(\\beta_0\\) slight modifications formula SE. Although \\(\\beta_1\\) typically related research question hand, \\(\\beta_0\\) can also informative right. Indeed, interested CI either one, calculate following:\\[b_0 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_1\\}\\]Clearly many parameters many intervals, wouldn’t think cover true parameters probability \\((1-\\alpha)\\). figure probability cover true parameter, let’s define\n\\[\\begin{eqnarray*}\nA_1 &=& \\mbox{first CI cover } \\beta_0\\\\\nP(A_1) &=& \\alpha\\\\\nA_2 &=& \\mbox{second CI cover } \\beta_1\\\\\nP(A_2) &=& \\alpha\\\\\n\\end{eqnarray*}\\]want bound probability intervals cover true parameter.\n\\[\\begin{eqnarray*}\nP(\\mbox{CIs cover}) &=& 1- P(\\mbox{least one CI cover})\\\\\n&=& 1 - P( A_1 \\mbox{ } A_2)\\\\\n&&\\\\\nP( A_1 \\mbox{ } A_2) &=& P(A_1) + P(A_2) - P(A_1 \\mbox{ } A_2)\\\\\n&=& \\alpha + \\alpha - ???\\\\\n& \\leq& 2 \\alpha \\\\\nP(\\mbox{least one CI cover}) &\\leq& 2 \\alpha\\\\\nP(\\mbox{CIs cover}) &\\geq& 1- 2 \\alpha\\\\\n\\end{eqnarray*}\\]Note make two 95% CIs, Bonferroni inequality tells us cover respective parameters 90% repeated samples (.e., 90% confidence).\\(g\\) CIs, letting multiplier level significance given \\(\\alpha/g\\) create familywise confidence intervals level significance \\(\\alpha\\).\\(\\beta_0\\) \\(\\beta_1\\), let \\(B = t_{(1-\\alpha/4), n-2}\\). intervals \\(1-\\alpha\\) familywise confidence limits :\\[b_0 \\pm B \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm B \\cdot s\\{b_1\\}\\]Note interpretations depend heavily number intervals (true multiple comparison adjustments).Bonferroni extremely conservative, therefore low power.Bonferroni easily extends number comparisons.Bonferroni can adjust multiple comparisons even comparing different types analyses (e.g., CI slope parameter predicted response).","code":""},{"path":"simult.html","id":"simultaneous-estimation-of-a-mean-response","chapter":"7 Simultaneous Inference","heading":"7.2 Simultaneous Estimation of a Mean Response","text":"intervals parameters, creating intervals multiple mean responses leads problems multiple comparisons. , goal adjust intervals probability getting dataset lead intervals covering mean responses \\(1-\\alpha\\).(Note reason simultaneous inference combination natural sampling variability \\(b_0\\) \\(b_1\\) lead mean responses correct \\(E[Y_h]\\) range \\(x\\) values incorrect different range \\(x\\) values.)","code":""},{"path":"simult.html","id":"working-hotelling-procedure","chapter":"7 Simultaneous Inference","heading":"Working-Hotelling Procedure","text":"Working-Hotelling procedure gives confidence band entire range \\(x\\) values. family confidence interval simultaneous intervals \\(1-\\alpha\\). Note multiplier determined bound entire line complete range \\(x\\) values.\\[\\hat{y}_h \\pm W s \\{ \\hat{y}_h \\}\\]\n\\(W^2 = 2 F_{(1-\\alpha; 2, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B \\cdot s \\{ \\hat{y}_h \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"simultaneous-prediction-intervals-for-new-observations","chapter":"7 Simultaneous Inference","heading":"7.3 Simultaneous Prediction Intervals for New Observations","text":"Just estimating mean response, interval predicting new observations can adjusted total range observations contained appropriate intervals probability \\(1-\\alpha\\). prediction intervals, necessary specify number \\(g\\) intervals interest.","code":""},{"path":"simult.html","id":"scheffé-procedure","chapter":"7 Simultaneous Inference","heading":"Scheffé Procedure","text":"Scheffé procedure gives confidence band set \\(x\\) values, \\(g\\) . Using Scheffé procedure, family confidence interval simultaneous intervals \\(1-\\alpha\\).\\[\\hat{y}_h \\pm S \\cdot s \\{ \\mbox{pred} \\}\\]\n\\(S^2 = g F_{(1-\\alpha; g, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure-1","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B s \\{ \\mbox{pred} \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"more","chapter":"7 Simultaneous Inference","heading":"7.4 More","text":"Note ALSM sections 4.6 (Inverse Predictions) 4.7 (Choice \\(x\\) values) provide additional information good model building. sections worth reading .","code":""},{"path":"simult.html","id":"reflection-questions-4","chapter":"7 Simultaneous Inference","heading":"7.5  Reflection Questions","text":"simultaneous CIs worry us (error perspective)?difference Bonferroni, Working-Hotelling, Scheffé adjustments?Bonferroni intervals larger two? smaller?say Bonferroni procedure general multiple comparisons procedures?\\(g\\) large, Working-Hotelling procedure preferred Bonferroni?","code":""},{"path":"simult.html","id":"ethics-considerations-3","chapter":"7 Simultaneous Inference","heading":"7.6  Ethics Considerations","text":"interval estimates often valuable report p-values?mean one adjustments “conservative?”sample representative population, can interval estimate misleading?words can used distinguish mean confidence intervals individual prediction intervals order better communication results?","code":""},{"path":"simult.html","id":"r-simultaneous-inference","chapter":"7 Simultaneous Inference","heading":"7.6.1 R: Simultaneous inference","text":"R packages simultaneous inference automatically. However, purposes covering, ’ll focus changing multiplier order control type error rate.Consider regression ames housing data. regression ln_price area home (sqft). See full analysis section @ref{ames-inf}.Let’s say want CI intercept slope parameters. case, two intervals created. Bonferroni controls type error dividing alpha error number intervals. multiplier t value adjusted alpha level degrees freedom linear model (n-2).intervals can created directly output tidy() function.Similarly, intervals mean response (confidence interval) individual response (prediction interval) use appropriate standard errors new multiplier. code Working-Hotelling multiplier Scheffe multiplier, 10 new observations.","code":"\names_lm <- ames_inf %>%\n  lm(price_ln ~ area, data = .) \n\names_lm %>%\n  tidy()## # A tibble: 2 × 5\n##   term         estimate std.error statistic p.value\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0\n## 2 area         0.000614 0.0000114      53.9       0\names_lm %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.500         0.500 0.284     2901.       0     1  -461.  928.  946.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\names_df <- ames_lm %>%\n  glance() %>%\n  select(df.residual) %>%\n  pull()\n\names_df## [1] 2902\nnum_int_param <- 2  # for beta0 and beta1\n\n# Bonferroni:\ncrit_Bonf <- qt((1-.975)/num_int_param, ames_df)\ncrit_Bonf## [1] -2.24\names_lm %>%\n  tidy() %>%\n  select(term, estimate, std.error) %>%\n  mutate(lower.ci = estimate + crit_Bonf*std.error,\n         upper.ci = estimate - crit_Bonf*std.error)## # A tibble: 2 × 5\n##   term         estimate std.error  lower.ci  upper.ci\n##   <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177    11.1      11.1     \n## 2 area         0.000614 0.0000114  0.000588  0.000639\n# Working-Hotelling\ncrit_WH <- sqrt(2*qf(.95, 2, ames_df))\ncrit_WH## [1] 2.45\n# Scheffe\nnum_int_pred <- 10 # if 10 new observations for prediction\ncrit_Sch <- sqrt(num_int_pred*qf(0.95, num_int_pred, ames_df))\ncrit_Sch## [1] 4.28"},{"path":"la.html","id":"la","chapter":"8 Regression using Matrices","heading":"8 Regression using Matrices","text":"Everything ’ve done far can written matrix form. Though might seem efficient use matrices simple linear regression, become clear multiple linear regression, matrices can powerful. ALSM chapter 5 contains lot matrix theory; main take away points chapter matrix theory applied regression setting. Please make sure read chapters / examples regression examples.","code":""},{"path":"la.html","id":"special-matrices","chapter":"8 Regression using Matrices","heading":"8.1 Special Matrices","text":"","code":""},{"path":"la.html","id":"regression-model","chapter":"8 Regression using Matrices","heading":"8.2 Regression Model","text":"","code":""},{"path":"la.html","id":"matrix-addition","chapter":"8 Regression using Matrices","heading":"8.2.1 Matrix Addition","text":"\\[\\begin{eqnarray*}\nY_i &=& E[Y_i] + \\epsilon_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& E[\\underline{Y}] + \\underline{\\epsilon}\\\\\n\\end{eqnarray*}\\]\nSimilarly,\n\\[\\begin{eqnarray*}\nY_i &=& \\hat{Y}_i + e_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} \\hat{Y}_1\\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} +\n\\begin{pmatrix} e_1\\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& \\underline{\\hat{Y}} + \\underline{e}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-multiplication","chapter":"8 Regression using Matrices","heading":"8.3 Matrix Multiplication","text":"","code":""},{"path":"la.html","id":"example","chapter":"8 Regression using Matrices","heading":"8.3.1 Example","text":"Consider multiplying \\(r \\times c\\) matrix \\(c \\times s\\) matrix. interior dimensions must always . resulting matrix always \\(r \\times s\\). element \\(^{th}\\) row \\(j^{th}\\) column given :\n\\[\\begin{eqnarray*}\n\\sum_{k=1}^c a_{ik} b_{kj}\\\\\nAB &=& \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{pmatrix}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} a_{11} b_{11} + a_{12} b_{21} + a_{13}b_{31} & a_{11} b_{12} + a_{12} b_{22} + a_{13}b_{32} \\\\ a_{21} b_{11} + a_{22} b_{21} + a_{23}b_{31} & a_{21} b_{12} + a_{22} b_{22} + a_{23}b_{32}\n\\end{pmatrix}\\\\\n&&\\\\\nAB &=& \\begin{pmatrix} 3 & -1 & 0 \\\\ 0 & 1 & 1 \\\\ -2 & 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 1\\\\ -1 & 2 \\\\ 0 & -1 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} 4 & 1\\\\ -1 & 1 \\\\ -2 & -3 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"and-in-the-linear-regression-context","chapter":"8 Regression using Matrices","heading":"and in the linear regression context…","text":"\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} &=&\n\\begin{pmatrix} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots \\\\ 1 & X_n \\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\\\\n&&\\\\\nE[\\underline{Y}] &=& X \\underline{\\beta}\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\underline{Y}^t \\underline{Y} &=& \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\sum_{=1}^n Y_i^2\\\\\n&&\\\\\nX^t X &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix}= \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\nX^t \\underline{Y} &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-inverses-in-the-regression-context","chapter":"8 Regression using Matrices","heading":"8.4 Matrix Inverses in the Regression Context","text":"","code":""},{"path":"la.html","id":"matrix-inverses","chapter":"8 Regression using Matrices","heading":"8.4.1 Matrix Inverses","text":"\\(n \\times n\\) matrix \\(\\) called invertible exists \\(n \\times n\\) matrix \\(B\\) \n\\[\\begin{eqnarray*}\nB = B = I_n\n\\end{eqnarray*}\\]\n\n\\(B\\) called inverse \\(\\) typically denoted \\(B = ^{-1}\\). (Note, inverses exist square matrices non-zero determinants.)","code":""},{"path":"la.html","id":"example-5","chapter":"8 Regression using Matrices","heading":"8.4.2 Example","text":"\\[= \\begin{pmatrix}  &  b \\\\ c  &  d \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} d / (ad - bc)  &  -b / (ad - bc) \\\\ -c / (ad - bc)  &  / (ad - bc) \\end{pmatrix}\\]determinant given \\(D = ad - bc\\).\n\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} d / (ad - bc) & -b / (ad - bc) \\\\ -c / (ad - bc) & / (ad - bc) \\end{pmatrix} \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (ad - bc) / (ad - bc) & (bd - bd) / (ad - bc) \\\\ (-ac + ac) / (ad - bc) & (ad - bc) / (ad - bc) \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]\\[= \\begin{pmatrix} 3  &  2 \\\\ 1  &  6 \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} 6 / 16  &  -2 / 16 \\\\ -1 / 16  &  3 / 16 \\end{pmatrix} \\]\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} 6 / 16 & -2 / 16 \\\\ -1 / 16 & 3 / 16 \\end{pmatrix}  \\begin{pmatrix} 3 & 2 \\\\ 1 & 6 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (18-2)/16 & (12-12)/16 \\\\ (-3+3)/16 & (-2+18)/16 \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"variance-of-coefficients","chapter":"8 Regression using Matrices","heading":"8.4.3 Variance of Coefficients","text":"","code":""},{"path":"la.html","id":"what-is-a-variance-matrix","chapter":"8 Regression using Matrices","heading":"What is a Variance Matrix?","text":"Consider two random variables, \\(U\\) \\(W\\). ’d typically \\(n\\) observations variable, resulting data vectors \\(\\underline{U}\\) \\(\\underline{W}\\). variance-covariance matrix describing variability relationship \\(U\\) \\(W\\) given :\n\\[\\begin{eqnarray*}\n\\mbox{var}\\{\\underline{U},\\underline{W} \\} &=& \\begin{pmatrix} \\sigma_U^2  & \\sigma_{U,W} \\\\ \\sigma_{W,U} & \\sigma_W^2 \\end{pmatrix}\\\\\n\\mbox{} \\sigma_U^2 &=& \\mbox{ variance } U  \\mbox{ (number)}\\\\\n\\sigma_W^2 &=& \\mbox{ variance } V  \\mbox{ (number)}\\\\\n\\sigma_{U,W} &=& \\sigma_{W,U} = \\mbox{ covariance } U \\mbox{ } W  \\mbox{ (number)} \\\\\n\\end{eqnarray*}\\]\norder find variance matrix regression coefficients, use matrix notation.\n\\[\\begin{eqnarray*}\n\\mbox{Recall:    }  X^t X &=&  \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{,    } D &=& n \\sum X_i^2 - (\\sum X_i)^2 = n \\sum(X_i - \\overline{X})^2\\\\\n&&\\\\\n(X^t X)^{-1} &=&  \\begin{pmatrix} \\frac{\\sum X_i^2}{n \\sum(X_i - \\overline{X})^2} & \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} & \\frac{n}{n \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=&  \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b}\\} &=& \\sigma^2  \\cdot (X^t X)^{-1}\\\\\n&=& \\begin{pmatrix} \\sigma^2_{b_0} & \\sigma_{b_0, b_1} \\\\ \\sigma_{b_1, b_0} & \\sigma^2_{b_1} \\end{pmatrix}\\\\\nSE^2\\{\\underline{b}\\} &=& MSE  \\cdot (X^t X)^{-1}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"estimating-coefficients","chapter":"8 Regression using Matrices","heading":"Estimating Coefficients","text":"Recall equations come differentiating sum squared residuals respect \\(\\beta_0\\) \\(\\beta_1\\):\\[\\begin{eqnarray*}\nn b_0 + b_1 \\sum X_i &=& \\sum Y_i\\\\\nb_0 \\sum X_i + b_1 \\sum X_i^2 &=& \\sum X_i Y_i\\\\\n&&\\\\\n\\begin{pmatrix} n & \\sum X_i \\\\ \\sum X_i & \\sum X_i^2 \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix} &=& \\begin{pmatrix} \\sum Y_i \\\\ \\sum X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n(X^t X) \\underline{b} &=& X^t \\underline{Y}\\\\\n\\underline{b} &=& (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b} \\} &=& (X^t X)^{-1} X^t \\sigma^2 X (X^t X)^{-1}\\\\\n&=& \\sigma^2 \\cdot (X^t X)^{-1}\\\\\n&&\\\\\n\\mbox{checking:}&&\\\\\n(X^t X)^{-1} (X^t \\underline{Y}) &=& \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix} \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\frac{\\sum Y_i}{n} + \\frac{\\sum Y_i \\overline{X}^2}{\\sum(X_i - \\overline{X})^2} + \\frac{-\\sum X_i Y_i (\\overline{X})}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum Y_i (\\overline{X})}{ \\sum(X_i - \\overline{X})^2} + \\frac{\\sum X_i Y_i}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\overline{Y} - b_1 \\overline{X} \\\\ \\frac{\\sum Y_i (X_i - \\overline{X})}{\\sum(X_i - \\overline{X})^2} \\end{pmatrix} = \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"fitted-values","chapter":"8 Regression using Matrices","heading":"8.5 Fitted Values","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_i &=& b_0 + b_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} &=& \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\\\\\n&&\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n &=& X (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&=& H \\underline{Y}\\\\\n\\mbox{\"hat\" matrix: } H &=& X (X^t X)^{-1} X^t\n\\end{eqnarray*}\\]\ncall \\(H\\) hat matrix takes \\(\\underline{Y}\\) puts hat . Note predicted values simply linear combinations response variable (\\(Y\\)) coefficients explanatory variables (\\(X\\)).","code":""},{"path":"la.html","id":"residuals-1","chapter":"8 Regression using Matrices","heading":"8.6 Residuals","text":"\\[\\begin{eqnarray*}\ne_i &=& Y_i - \\hat{Y}_i\\\\\n\\begin{pmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix} &=& \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\Y_n \\end{pmatrix} - \\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix}\\\\\n\\underline{e} &=& \\underline{Y} - \\hat{\\underline{Y}}\\\\\n&=& \\underline{Y} - X \\underline{b}\\\\\n&=& \\underline{Y} - H \\underline{Y}\\\\\n&=& (- H) \\underline{Y}\\\\\n&&\\\\\n\\mbox{var}\\{ \\underline{e} \\} &=& \\mbox{var}\\{ (-H) \\underline{Y} \\}\\\\\n&=& (- H) \\mbox{var}\\{ \\underline{Y} \\}\\\\\n&=& (- H) \\cdot \\sigma^2 \\cdot (- H)^t\\\\\n&=& \\sigma^2 \\cdot (- H) (-H^t)\\\\\n&=& \\sigma^2 \\cdot (- H - H^t + HH^t)\\\\\n&=&  \\sigma^2 \\cdot (-H)\\\\\nSE^2(\\underline{e}) &=& MSE \\cdot (-H)\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"analysis-of-variance","chapter":"8 Regression using Matrices","heading":"8.7 ANalysis Of VAriance","text":"\\[\\begin{eqnarray*}\nSSTO &=& \\underline{Y}^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} 1 & 1 & \\cdots &  1\\\\ 1 & 1 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 1 & \\cdots & 1   \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix}  \\begin{pmatrix} \\sum Y_i \\\\ \\sum Y_i \\\\ \\vdots \\\\ \\sum Y_i \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\sum Y_i \\sum Y_i\\\\\n&=& \\sum Y_i ^2 - n \\overline{Y}^2\\\\\n\\mbox{note: } \\sum(Y_i - \\overline{Y})^2 &=& \\sum (Y_i^2 - 2Y_i \\overline{Y} + \\overline{Y}^2)\\\\\n&=& \\sum Y_i^2 -2\\overline{Y}\\sum Y_i + n \\overline{Y}^2\\\\\n&=& \\sum Y_i^2 - n \\overline{Y}^2\\\\\n&&\\\\\nSSE &=& \\underline{Y}^t \\underline{Y} - \\underline{b}^t X^t \\underline{Y}\\\\\n&&\\\\\nSSR &=& \\underline{b}^t X^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"prediction-of-new-observations","chapter":"8 Regression using Matrices","heading":"8.8 Prediction of New Observations","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_h &=& \\underline{X}^t_h \\underline{b}\\\\\n\\mbox{var}\\{ \\hat{Y}_h \\} &=& \\underline{X}_h^t \\mbox{var}\\{\\underline{b} \\} \\underline{X}_h\\\\\n&=& \\sigma^2 \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\nSE^2\\{\\hat{Y}_h \\} &=& MSE \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\n&&\\\\\n&&\\\\\nSE^2 \\{\\mbox{pred} \\} = SE^2\\{\\hat{Y}_{h(new)} \\} &=& MSE \\cdot (1 + \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"reflection-questions-5","chapter":"8 Regression using Matrices","heading":"8.9  Reflection Questions","text":"normal errors linear model written matrix form?X matrix added column ones?Using matrix notation, normal equations used solve least squares estimates \\(\\beta_0\\) \\(\\beta_1\\)?covariance term? mean variables correlated (e.g., \\(b_0\\) \\(b_1\\))?“hat” matrix (\\(H\\)) named?","code":""},{"path":"la.html","id":"ethics-considerations-4","chapter":"8 Regression using Matrices","heading":"8.10  Ethics Considerations","text":"sampling distributions \\(b_0\\) \\(b_1\\) correlated?ways writing linear model matrix notation make extension explanatory variables easier?fundamental differences SLR model written matrix notation (opposed indexing elements equation)?","code":""},{"path":"la.html","id":"r-matrices","chapter":"8 Regression using Matrices","heading":"8.11 R: Matrices","text":"","code":""},{"path":"la.html","id":"addition","chapter":"8 Regression using Matrices","heading":"8.11.1 Addition","text":"Adding matrices gives ’d likely expect.","code":"\nmatrix1 <- matrix(c(1:12),ncol=4, byrow=T)\nmatrix2 <- matrix(seq(2,24,by=2),ncol=4, byrow=T)\n\nmatrix1##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\nmatrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    4    6    8\n## [2,]   10   12   14   16\n## [3,]   18   20   22   24\nmatrix1 + matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    3    6    9   12\n## [2,]   15   18   21   24\n## [3,]   27   30   33   36"},{"path":"la.html","id":"multiplication","chapter":"8 Regression using Matrices","heading":"8.11.2 Multiplication","text":"* produces element element multiplication.%*% matrix multiplication. Note error can’t multiply \\(3 \\times 4\\) \\(3 \\times4\\) matrix.either matrices transposed, conformable, able multiply .Note products symmetric matrix1 = \\(2 \\cdot\\) matrix2.","code":"\nmatrix1 * matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    8   18   32\n## [2,]   50   72   98  128\n## [3,]  162  200  242  288\nmatrix1 %*% matrix2## Error in matrix1 %*% matrix2: non-conformable arguments\nmatrix1 %*% t(matrix2)##      [,1] [,2] [,3]\n## [1,]   60  140  220\n## [2,]  140  348  556\n## [3,]  220  556  892\nt(matrix1) %*% matrix2##      [,1] [,2] [,3] [,4]\n## [1,]  214  244  274  304\n## [2,]  244  280  316  352\n## [3,]  274  316  358  400\n## [4,]  304  352  400  448"},{"path":"la.html","id":"taking-inverses","chapter":"8 Regression using Matrices","heading":"8.11.3 Taking Inverses","text":"function inverting matrices R solve(). Remember solve() can works square matrices non-zero determinants.Multiplying matrix inverse results identity matrix.","code":"\nmatrix3 <- matrix(c(5,7,1,4,3,6,2,0,8), ncol=3, byrow=T)\nmatrix3##      [,1] [,2] [,3]\n## [1,]    5    7    1\n## [2,]    4    3    6\n## [3,]    2    0    8\nsolve(matrix3)##        [,1]   [,2] [,3]\n## [1,] -0.923  2.154 -1.5\n## [2,]  0.769 -1.462  1.0\n## [3,]  0.231 -0.538  0.5\nmatrix3 %*% solve(matrix3)##          [,1]      [,2]     [,3]\n## [1,] 1.00e+00 -2.22e-16 1.11e-16\n## [2,] 4.44e-16  1.00e+00 0.00e+00\n## [3,] 4.44e-16  0.00e+00 1.00e+00"},{"path":"la.html","id":"concatenating-1s-for-the-intercept","chapter":"8 Regression using Matrices","heading":"8.11.4 Concatenating 1s for the intercept","text":"Let’s say explanatory variable called xvar","code":"\nset.seed(7447)\nxvar <- rnorm(12, 47, 3)\nxvar##  [1] 47.3 43.1 47.1 48.0 49.7 45.2 47.7 47.1 47.9 48.9 43.4 44.1\nrep(1,12)  # repeats the number 1, 12 times##  [1] 1 1 1 1 1 1 1 1 1 1 1 1\nXmatrix <- cbind(int = rep(1,12), xvar)\nXmatrix##       int xvar\n##  [1,]   1 47.3\n##  [2,]   1 43.1\n##  [3,]   1 47.1\n##  [4,]   1 48.0\n##  [5,]   1 49.7\n##  [6,]   1 45.2\n##  [7,]   1 47.7\n##  [8,]   1 47.1\n##  [9,]   1 47.9\n## [10,]   1 48.9\n## [11,]   1 43.4\n## [12,]   1 44.1"},{"path":"mlr.html","id":"mlr","chapter":"9 Multiple Linear Regression","heading":"9 Multiple Linear Regression","text":"","code":""},{"path":"mlr.html","id":"basic-model-set-up","chapter":"9 Multiple Linear Regression","heading":"9.1 Basic Model Set-Up","text":"looking volume riders bike trail, can use data collected understand relationships deeper level. trail Massachusetts, expected relationship positive: higher temp riders expect. However, volume also related whether weekend. Indeed, think volume riders function temperature day week. Thus two predictor variables model.","code":""},{"path":"mlr.html","id":"notation","chapter":"9 Multiple Linear Regression","heading":"9.1.1 Notation","text":"Consider \\(n\\) observations. response variable \\(^{th}\\) individual, denoted \\(Y_i\\) , observed. variation remaining \\(Y_i\\) isn’t explained predictors also remain , denoted \\(\\epsilon_i\\) called random error. Since now one predictor, additional subscript added \\(X\\), denoting value \\(k^{th}\\) predictor variable \\(^{th}\\) individual \\(X_{ik}\\). Thus model now:\n\\[\\begin{eqnarray*}\nY_i&=&\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}+ \\cdots + \\beta_{p-1}X_{,p-1} + \\epsilon_i\\\\\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+ \\cdots + \\beta_{p-1}X_{p-1}\\\\\nY_i&=&b_0+b_1X_{i1}+b_2X_{i2}+ \\cdots + b_{p-1}X_{,p-1} + e_i\\\\\n\\hat{Y}&=&b_0+b_1X_{1}+b_2X_{2}+ \\cdots + b_{p-1}X_{p-1}\\\\\n&&\\\\\nE[\\underline{Y}] &=& X \\underline{\\beta}\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n\\end{eqnarray*}\\]\nRail Trails example, \\(X_{i1}\\) might denote volume riders \\(^{th}\\) day, \\(X_{i2}\\) denote indicator variable whether day weekend week day.","code":""},{"path":"mlr.html","id":"fitting-the-model","chapter":"9 Multiple Linear Regression","heading":"9.1.2 Fitting the Model","text":"estimate coefficients, use principle , least\nsquares. , minimize\n\\[\\sum_{=1}^n(Y_i-(\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2} + \\cdots + \\beta_{p-1}X_{,p-1}))^2\\]\ninterested finding least squares estimates parameters model \\(b_i\\). , something looks like\n\\[(\\underline{Y}-\\mathbf{X}\\underline{\\beta})^t(\\underline{Y}-\\mathbf{X}\\underline{\\beta})\\]\ntrying minimize (sum squared residuals). Calculus gives:\n\\[\\mathbf{X}^t\\underline{Y}-\\mathbf{X}^t\\mathbf{X}\\mathbf{\\beta}=0,\\]\nsolving unknown \\(\\underline{\\beta}\\) gives:\n\\[\\underline{b}=(\\mathbf{X}^t\\mathbf{X})^{-1}(\\mathbf{X}^t\\underline{Y}).\\]\ntranspose concession multiplication division work slightly differently matrix context. inverse, denoted power -1, provides way solve equation.\nThus,\n\\[\\hat{\\underline{Y}}=\\mathbf{X}\\underline{b}=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\underline{Y}:=\\mathbf{H}\\underline{Y}\\]\n\n\\[\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\]\nhat matrix (multiplying H \n\\(\\underline{Y}\\) puts hat ).\nhat matrix come learning leverage.","code":""},{"path":"mlr.html","id":"types-of-multiple-regression","chapter":"9 Multiple Linear Regression","heading":"9.1.3 Types of Multiple Regression","text":"multiple linear regression model useful variety situations. discussed . example first type:\\(p-1\\) predictor variables: say \\(p-1\\) instead \\(p\\) \nincluding intercept \\(p\\) parameters need \nestimation.\nprevious example, \\(p=3\\), order estimate \\(\\beta_0\\),\n\\(\\beta_1\\) \\(\\beta_2\\). two independent variables, high temperature weekday status.Qualitative Predictor Variables: including categorical variable model, must written one () binary variables. example,\\[\\begin{eqnarray*}X_2=\\begin{aligned} &0& \\quad &\\mbox{ weekend}&\\\\\n&1&\\quad &\\mbox{ weekday}&\\end{aligned}\\end{eqnarray*}\\]general, qualitative variable \\(k\\) levels, \\(k-1\\) \"dummy’’ (.e., binary) variables must included. instance, variables school year, (2013, 2014, 2015, 2016), 3 variables (coded , example: 1=2013, 0=2013). given observation, three dummy variables zero, model know observation took place school year coded .Transformed Variables: simple linear regression, often good idea transform variables ensure model technical conditions hold.Interaction Effects: model “additive” means response function predictors added together. variables interact . interaction model can form\n\\[E[Y]=\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+\\beta_3X_{1}X_{2}\\]\ninteraction multiplicative. Often idea fitting interaction comes sort knowledge variables .\nAssume \\(E[Y]\\) average volume riders days \\(X_{1}\\) high temp, \\(X_{2}\\) whether weekday. interaction model provides way break model particular groups, model provides different linear model weekdays weekends.Weekday=0\n\\[\\begin{eqnarray*}\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2 0+\\beta_3X_{1} 0\\\\\n&=&\\beta_0+\\beta_1X_{1}\\\\\n\\end{eqnarray*}\\]Weekday=1\n\\[\\begin{eqnarray*}\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2 1+\\beta_3X_{1} 1\\\\\n&=&(\\beta_0 + \\beta_2) +(\\beta_1 + \\beta_3) X_{1}\\\\\n\\end{eqnarray*}\\]additive model (.e., interactions) states dummy variables move line . volume ridership different different days depend high temp, additive component included. day week changes relationship high temp volume, interaction term included.\nVariables interact effect one predictor variable depends levels predictor variables.\nAnother way think interaction whether change \\(E[Y]\\) change one variables (e.g., \\(X_1\\)) mediated another variable (e.g., \\(X_2\\)).\\[\\begin{eqnarray*}\nE[Y]=\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+\\beta_3X_{1}X_{2}\\\\\n\\frac{\\partial E[Y]}{\\partial X_1} = \\beta_1 + \\beta_3 X_2\\\\\n\\end{eqnarray*}\\]Polynomial Regression: response variable might function polynomial predictor giving rise polynomial model:\n\\[Y_i=\\beta_0+\\beta_1X_i+\\beta_2X_i^2+\\epsilon_i\\]\nrepresents \\(Y\\) quadratic function \\(X\\).term linear model therefore refer response surface, rather fact model linear parameters. Though fitting hyper-plane data, think surface looks like terms original variables, may highly non-linear due transformations \nforth.","code":""},{"path":"mlr.html","id":"example-thermometers","chapter":"9 Multiple Linear Regression","heading":"Example: thermometers","text":"Consider new dataset. data collected Michael Ernst St. Cloud University Minnesota (Polar Vortex January 2019).late fall early winter, temperature dropped (tends MN), Michael started get suspicious thermometer wasn’t entirely accurate. , put another thermometer trusted outside near new one read temperature window. wrote temperature every throughout December January.two variables: Temp, actual temperature (based trusted thermometer), Reading, reading suspect thermometer.\nFigure 9.1: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.2: scatterplot looks linear… residual plot doesn’t!\nHopefully, transforming data help. figure seems like square root Reading log Temp might help. Let’s try , first ’ll shift (get rid negative numbers) arbitrarily 35 degrees.\nFigure 9.3: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.4: scatterplot looks linear… residual plot doesn’t!\nDoesn’t seem like transformations going work. square term added? still linear model? (Yes!) residuals better? (Yes!)quadratic term alone (without linear term) doesn’t help model fit model forces linear coefficient zero. making linear part zero, force vertex (parabola) X=0 doesn’t make sense model fit. Indeed, typically quadratic term without linear term good idea () curved relationship constant errors vertex plot, /(b) really believe reason \\(Y\\) (linear) function \\(X^2\\).","code":"\ntemperature %>%\n  ggplot(aes(x = Temp, y = Reading)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\ntemperature %>%\n  lm(Reading ~ Temp, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\nlin_mod <- temperature %>%\n  lm(Reading ~ Temp, data = .)\n\nquad_mod_1 <- temperature %>%\n  lm(Reading ~ temp_sq, data = .)\n\nquad_mod_2 <- temperature %>%\n  lm(Reading ~ Temp + temp_sq, data = .)\n\nlin_mod %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    -4.93   0.138       -35.7 8.50e-46\n## 2 Temp            1.19   0.00867     137.  7.94e-85\nquad_mod_1 %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic    p.value\n##   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n## 1 (Intercept) -14.4      2.79        -5.16 0.00000229\n## 2 temp_sq       0.0241   0.00727      3.32 0.00147\nquad_mod_2 %>% tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) -4.12     0.144       -28.7  2.33e-39\n## 2 Temp         1.21     0.00690     176.   5.36e-91\n## 3 temp_sq     -0.00295  0.000374     -7.90 3.65e-11"},{"path":"mlr.html","id":"revisiting-other-important-definitions","chapter":"9 Multiple Linear Regression","heading":"9.1.4 Revisiting Other Important Definitions","text":"solving regression coefficients \\((\\underline{b})\\) involves exact matrix algebra, pieces involved linear regression model given using matrix notation.\n\\[\\begin{eqnarray*}\n\\underline{b} &=& (X^t X)^{-1} X^t \\underline{Y}\\\\\n\\underline{e} &=& \\underline{Y} - \\hat{\\underline{Y}}\\\\\n&=& \\underline{Y} - X \\underline{b}\\\\\n&=& (- H) \\underline{Y}\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n &=& X (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&=& H \\underline{Y}\\\\\ns^2(\\underline{e}) &=& MSE \\cdot (-H)\\\\\n\\sigma^2(\\underline{e}) &=& \\sigma^2 \\cdot (-H)\\\\\ns^2(\\underline{\\epsilon}) &=& MSE \\cdot \\\\\n\\sigma^2(\\underline{\\epsilon}) &=& \\sigma^2 \\\\\n\\end{eqnarray*}\\]Equivalently, components ANOVA table remain , slight change now degrees freedom generalized account fact estimating \\(p\\) parameters.\\[\\begin{eqnarray*}\nSSR &=& \\sum (\\hat{Y}_i - \\overline{Y})^2 = \\underline{b}^t X^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\nSSE &=& \\sum (Y_i - \\hat{Y}_i)^2 = \\underline{Y}^t \\underline{Y} - \\underline{b}^t X^t \\underline{Y}\\\\\nSSTO &=& \\sum (Y_i - \\overline{Y})^2 = \\underline{Y}^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\n\\end{eqnarray*}\\]Note (\\(p=3\\), two explanatory variables):\n\\[\\begin{eqnarray*}\nE[MSE] &=& \\sigma^2\\\\\nE[MSR] &=& \\sigma^2 + 0.5[\\beta_1^2 \\sum(X_{i1} - \\overline{X}_1)^2 + \\beta_2^2 \\sum(X_{i2} - \\overline{X}_2)^2 + 2 \\beta_1 \\beta_2 \\sum(X_{i1} - \\overline{X}_1)(X_{i2} - \\overline{X}_2)]\n\\end{eqnarray*}\\]","code":""},{"path":"mlr.html","id":"inference","chapter":"9 Multiple Linear Regression","heading":"9.2 Inference","text":"","code":""},{"path":"mlr.html","id":"f-test","chapter":"9 Multiple Linear Regression","heading":"9.2.1 F-test","text":"F-test simple linear regression, can now generalized test addressing whether non-intercept coefficients simultaneously zero. test asks whether explanatory variables set add anything model terms predicting response variable.\\[\\begin{eqnarray*}\nH_0:&& \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0\\\\\nH_a:&& \\mbox{ } \\beta_k = 0 \\mbox{ (still might zero)}\n\\end{eqnarray*}\\], measure ratio MSR MSE decide whether regression error components measuring quantity (residual error).\n\\[\\begin{eqnarray*}\nF = \\frac{MSR}{MSE} \\sim F_{(p-1, n-p)}  \\mbox{ $H_0$ true (!)}\n\\end{eqnarray*}\\]\nRemember MSE always estimates \\(\\sigma^2\\), MSR estimates \\(\\sigma^2\\) \\(\\beta_k\\) coefficients simultaneously equal zero.","code":""},{"path":"mlr.html","id":"coefficient-of-multiple-determination","chapter":"9 Multiple Linear Regression","heading":"9.2.2 Coefficient of Multiple Determination","text":"Recall measured proportion variability explained linear model using \\(R^2\\). interpretation now \\(p-1\\) predictors. \\[ R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\] :\n\\[\\begin{eqnarray*}\nR^2 &=& 0 \\mbox{ } b_k = 0 \\ \\ \\forall k=1, \\ldots, p-1\\\\\nR^2 &=& 1 \\mbox{ } \\hat{Y}_i = Y_i \\ \\ \\forall \n\\end{eqnarray*}\\]now situation adding variables always increases \\(R^2\\). , add explanatory variables model, \\(R^2\\) value increases.\\(\\beta_k=0 \\ \\ \\forall k\\), \\(E[MSR] = \\sigma^2 \\rightarrow E[SSR] = \\sigma^2(p-1)\\), also \\(E[SSTO] = \\sigma^2(n-1) \\rightarrow R^2 \\approx (p-1)/(n-1)\\).","code":""},{"path":"mlr.html","id":"adjusted-r2","chapter":"9 Multiple Linear Regression","heading":"Adjusted \\(R^2\\)","text":"account problem \\(R^2\\) increasing number variables, compare mean squares instead sums squares. Now, value longer increasing number variables trade-reducing errors (SSE) losing degree freedom. \\[R^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)}{(n-p)} \\frac{SSE}{SSTO}\\]","code":""},{"path":"mlr.html","id":"inference-about-regression-parameters","chapter":"9 Multiple Linear Regression","heading":"9.2.3 Inference about Regression Parameters","text":"","code":""},{"path":"mlr.html","id":"coefficients","chapter":"9 Multiple Linear Regression","heading":"Coefficients","text":"know \n\\[\\begin{eqnarray*}\nvar\\{ \\underline{b} \\} &=& \\sigma^2 (X^t X)^{-1}\\\\\nSE^2\\{ \\underline{b} \\} &=& MSE (X^t X)^{-1}\\\\\n\\end{eqnarray*}\\]\ncan use estimate SE create test statistic t distribution null hypothesis true (note now estimating \\(p\\) parameters, degrees freedom \\(n-p\\)).\n\\[\\begin{eqnarray*}\n\\frac{b_k - \\beta_k}{SE\\{b_k\\}} \\sim t_{(n-p)}\n\\end{eqnarray*}\\]\n\\((1-\\alpha)100\\%\\) CI \\(\\beta_k\\) given \\[b_k \\pm t_{(1-\\alpha/2, n-p)} s\\{b_k\\}\\]\nNote t-test done separately \\(\\beta\\) coefficient. say estimating MSE variables model. test asks effect removing variable hand. testing interpretation regression coefficients done keeping variables constant.","code":""},{"path":"mlr.html","id":"linear-combinations-of-coefficients","chapter":"9 Multiple Linear Regression","heading":"Linear Combinations of Coefficients","text":"Periodically, question interest related linear combination coefficients. example, might interested testing whether coefficient spring statistically different coefficient fall [\\(H_0: \\beta_1 = \\beta_2\\)]. Let\n\\[\\begin{eqnarray*}\n\\gamma &=& c_0 \\beta_0 + c_1 \\beta_1 + \\ldots + c_p \\beta_p\\\\\ng &=& c_0 b_0 + c_1 b_1 + \\ldots + c_p b_p\\\\\nvar(g) &=& c_0^2 var\\{b_0\\} + c_1^2 var\\{b_1\\} + \\ldots + c_p^2 var\\{b_p\\} + 2c_0c_1 cov(b_0, b_1) + 2 c_0 c_2 cov(b_0, b_2) + \\ldots + 2c_{p-1}c_p cov(b_{p-1}, b_p)\\\\\n\\end{eqnarray*}\\]\nestimate difference SE, t-statistic (create CI) provides formal inference coefficients. Note function vcov() estimates variances covariance coefficients.\n\\[\\begin{eqnarray*}\n\\hat{var}(b_1 - b_2) &=& (1)^2 SE^2\\{b_1\\} + (-1)^2 SE^2\\{ b_2\\} + 2(1)(-1)\\hat{cov}(b_1, b_2)\\\\\n&=& 889 + 1862 -  2*604 = 1543\\\\\nH_0: && \\beta_1 = \\beta_2\\\\\nt-stat &=& \\frac{(-50.1 - (-126.8)) - 0}{ \\sqrt{1543}} = 1.952\\\\\np-value &=& 2 * P(t_{87} \\geq 1.952) = 0.054\n\\end{eqnarray*}\\]\np-value borderline, certainly strong evidence say fall spring significantly different model. tidy() output shows fall significantly different summer (baseline) spring may may different (p-value < 0.1). Note: days measured winter.","code":"\nRailTrail %>%\n  lm(volume ~ spring + fall, data = .) %>%\n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    422.       24.6     17.2  1.13e-29\n## 2 spring         -50.2      29.8     -1.68 9.57e- 2\n## 3 fall          -127.       43.2     -2.94 4.23e- 3\nRailTrail %>%\n  lm(volume ~ spring + fall, data = .) %>%\n  vcov()##             (Intercept) spring fall\n## (Intercept)         604   -604 -604\n## spring             -604    889  604\n## fall               -604    604 1862"},{"path":"mlr.html","id":"mean-response","chapter":"9 Multiple Linear Regression","heading":"Mean Response","text":", using linear algebra simple linear regression:\\[\\begin{eqnarray*}\nvar\\{\\hat{Y}_h\\} &=& \\sigma^2 X_h^t (X^t X)^{-1} X_h\\\\\ns^2\\{\\hat{Y}_h\\} &=& MSE X_h^t (X^t X)^{-1} X_h\n\\end{eqnarray*}\\]\nallows us create \\((1-\\alpha)100\\%\\) CI \\(E[Y_h] = X_h^t \\underline{\\beta}\\): \\[\\hat{Y}_h \\pm t_{(1-\\alpha/2, n-p)} s\\{\\hat{Y}_h\\}\\]\n\n\\[\\begin{eqnarray*}\nX_h^t &=& (1 \\ \\ X_{h 1} \\ \\ X_{h  2} \\ \\ldots \\ X_{h  p-1})\\\\\n\\hat{Y}_h &=& X_h^t \\underline{b}\n\\end{eqnarray*}\\]","code":""},{"path":"mlr.html","id":"future-predicted-response","chapter":"9 Multiple Linear Regression","heading":"Future / Predicted Response","text":"new observation uses derivation mean response, add variability observations around line.\n\\[\\begin{eqnarray*}\nvar\\{ \\mbox{pred} \\} = var\\{\\hat{Y}_{h(new)}\\} &=& \\sigma^2 (1+ X_h^t (X^t X)^{-1} X_h)\\\\\nSE^2\\{ \\mbox{pred} \\} = SE^2\\{\\hat{Y}_{h(new)}\\} &=& MSE (1+ X_h^t (X^t X)^{-1} X_h)\\\\\n&=& MSE + SE^2\\{\\hat{Y}_h\\}\n\\end{eqnarray*}\\]\nallows us create \\((1-\\alpha)100\\%\\) prediction interval response \\(X_h\\): \\[\\hat{Y}_h \\pm t_{(1-\\alpha/2, n-p)} s\\{\\hat{Y}_{h(new)}\\}\\]\nNote can interpret interval say \\((1-\\alpha)100\\%\\) response values \\(X_h\\) fall interval.","code":""},{"path":"mlr.html","id":"skipping","chapter":"9 Multiple Linear Regression","heading":"Skipping","text":"text skip:surface predictionsimultaneous confidence intervalsprediction one valueor one \\(X_h\\)formal hypothesis tests normality / error variance / constant variance / lack fit (use residual plots instead)","code":""},{"path":"mlr.html","id":"criteria-for-evaluating-models","chapter":"9 Multiple Linear Regression","heading":"9.2.4 Criteria for Evaluating Models","text":"idea good model find balance small residuals many predictors. , MSE small, \\(p\\) small well. Recall \\(R^2_a\\) balance MSE \\(p\\), one specific way. ideas given . Later, models built using p-values F-tests. However, myriad criteria optimize given model (.e., variables best include). Consider using criteria compare models differing number variables (response, model structure). following defined :\\[\\begin{eqnarray*}\nSSE_p &=& SSE \\mbox{ model $p$ parameters}\\\\\nSSE_{full} &=& SSE \\mbox{  model possible parameters}\n\\end{eqnarray*}\\]\\(C_p\\): \\[C_p = \\frac{SSE_p}{MSE_{full}} - (n-2p)\\]\\(AIC_p\\): \\[AIC_p = n \\ln(SSE_p) - n \\ln(n) + 2p\\]\\(SBC_p = BIC_p\\): \\[SBC_p = n \\ln(SSE_p) - n \\ln(n) + \\ln(n) p\\]\n\\(AIC_p\\) \\(SBC_p\\) measure likelihood data (\\(-2 \\ln(likelihood)\\)) given particular model \\(p-1\\) explanatory variables. choose model smallest \\(AIC_p\\) \\(SBC_p\\).\nNote can use criteria build model either adding one variable time; starting full model (variables) subtracting one variable time; combination adding subtracting variables.","code":""},{"path":"mlr.html","id":"c_p-aic-and-sbc-in-r","chapter":"9 Multiple Linear Regression","heading":"\\(C_p\\), AIC, and SBC in R","text":"Ideally, chosen model minimize three criteria. three trade-offs SSE (want small) \\(p\\) (want small).\\(C_p\\) also measures trade-bias variance.\n\\[\\begin{eqnarray*}\nBias(\\hat{Y}_i) &=& E[\\hat{Y}_i] - E[Y_i]\\\\\nMSE(\\hat{Y}_i) &=& [Bias(\\hat{Y}_i)]^2 + Var(\\hat{Y}_i)\\\\\n\\Gamma_p &=& \\frac{1}{\\sigma^2} [Bias(\\hat{Y}_i)]^2 + Var(\\hat{Y}_i)\\\\\n\\Gamma_p &=& p  \\mbox{ bias model}\n\\end{eqnarray*}\\]\nestimate \\(\\Gamma_p\\)? know population variance, \\(\\sigma^2\\), can estimate \\(\\Gamma_p\\) using:\n\\[\\begin{eqnarray*}\n C_p &=& p + \\frac{(MSE_p - \\sigma^2)(n-p)}{ \\sigma^2}\\\\\n \\end{eqnarray*}\\]\nEstimating \\(\\sigma^2\\) using \\(MSE_{full}\\) gives\n\\[\\begin{eqnarray*}\n C_p &=& p + \\frac{MSE_p - MSE_{full})(n-p)}{ MSE_{full}} = \\frac{SSE_p}{MSE_{full}} - (n-2p)\\\\\n \\end{eqnarray*}\\]\\(C_p\\) estimates quantity “total MSE divided \\(\\sigma^2\\).” can shown \\(\\sum_{=1}^n Var(\\hat{Y}_i) = \\sigma^2 p\\). want \\(C_p\\) small \\(\\approx p\\). See comments page 359 Kutner et al.\nNote calculating \\(C_P\\) full model (P parameters), get\n\\[\\begin{eqnarray*}\nC_P &=& \\frac{SSE_P}{MSE_P} - (n-2P)\\\\\n&=& (n-P) - n + 2P\\\\\n&=& P\\\\\n\\end{eqnarray*}\\]Estimating \\(\\sigma^2\\) using \\(MSE_{full}\\) assumes biases full model predictors, assumption may may valid, can’t tested without additional information (least important predictors involved).AIC SBC based maximum likelihood estimates model parameters. idea maximum likelihood find parameters produce largest likelihood function given available data. likelihood number 0 1. variety reasons, unimportant , common take log likelihood (action change function maximized) multiply likelihood -2. linear regression, parameter estimates found least squares maximum likelihood identical. However, using least squares versus maximum likelihood, difference estimating \\(\\sigma^2\\). using unbiased estimate \\(\\sigma^2\\) MSE = SSE / (n-p). maximum likelihood estimate \\(\\sigma^2\\) SSE/n. MLE slight negative bias, also smaller variance. Note estimating \\(p\\) regression coefficients \\(\\sigma^2\\), actually estimating \\(p+1\\) parameters. short, full AIC given following.\\[\\begin{eqnarray}\nE[AIC] &=& -2 \\ln(L) + 2(p+1)\\\\\\\n&=& -2 \\ln \\bigg[ \\prod_{=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp( -(Y_i - E[Y_i])^2/ 2\\sigma_i^2) \\bigg] + 2(p+1) \\tag{9.1}\\\\\n&=& -2 \\ln \\bigg[ (2\\pi)^{-(n/2)} \\sigma^{-(2n/2)} \\exp(-\\sum_{=1}^n (Y_i - E[Y_i])^2 / 2 \\sigma^2) \\bigg] + 2(p+1) \\tag{9.2}\\\\\nAIC &=& -2 \\ln \\bigg[ (2\\pi)^{-(n/2)} (SSE_p/n)^{-(n/2)} \\exp(-SSE_p / (2 SSE_p/n)) \\bigg] + 2(p+1) \\tag{9.3}\\\\\n&=& 2 (n/2) \\ln(2 \\pi) - 2(-n/2) \\ln(SSE_p/n) + n + 2(p+1) \\nonumber \\\\\n&=& n \\ln(2 \\pi) + n\\ln(SSE_p/n) + n + 2(p+1) \\nonumber \\\\\n&=& n \\ln(2 \\pi) + n\\ln(SSE_p)  - n\\ln(n) + n + 2(p+1) \\nonumber \\\\\n&=& n\\ln(SSE_p) - n\\ln(n) + 2p + constant \\nonumber\n\\end{eqnarray}\\]go (9.1) (9.2) assume \\(\\sigma^2 = \\sigma_i^2\\); , variance constant individuals. go (9.2) (9.3) approximate \\(\\sigma^2\\) (\\(E[Y_i]\\)) using maximum likelihood estimates \\(\\sigma^2 = SSE / n\\) \\(\\beta_k\\).\nSBC (BIC) uses posterior likelihood similar derivation. SBC can given following.\n\\[\\begin{eqnarray*}\nSBC &=& -2 \\ln(L_{posterior}) + \\ln(n) (p+1)\\\\\n&=& n + n\\ln(2\\pi) + n \\ln(SSE_p / n) + \\ln(n)(p+1)\\\\\n&=& n \\ln(SSE_p) - n \\ln(n) + \\ln(n) p + constant\n\\end{eqnarray*}\\]\nNote AIC SBC don’t consider constant term models compared data (\\(n\\) ).","code":""},{"path":"mlr.html","id":"aic-bic","chapter":"9 Multiple Linear Regression","heading":"AIC & BIC","text":"Estimators prediction error relative quality models:Akaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] Schwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]\\[\n\\begin{aligned} \n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1) \n\\end{aligned}\n\\]First Term: Decreases p increases\\[\n\\begin{aligned} \n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1) \n\\end{aligned}\n\\]Second Term: Fixed given sample size n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)} \n\\end{aligned}\n\\]Third Term: Increases p increases","code":""},{"path":"mlr.html","id":"using-aic-bic","chapter":"9 Multiple Linear Regression","heading":"Using AIC & BIC","text":"\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)} \n\\end{aligned}\n\\]Choose model smaller value AIC BICChoose model smaller value AIC BICIf \\(n \\geq 8\\), penalty BIC larger AIC, BIC tends favor parsimonious models (.e. models fewer terms)\\(n \\geq 8\\), penalty BIC larger AIC, BIC tends favor parsimonious models (.e. models fewer terms)","code":""},{"path":"mlr.html","id":"reflection-questions-6","chapter":"9 Multiple Linear Regression","heading":"9.3  Reflection Questions","text":"model change multiple variables?interaction mean? model ? interpret ? R code?considerations associated quadratic term?test whether variables significant?test whether individual variables significant?assess linear combination coefficients?difference \\(R^2\\) \\(R^2_a\\)?prediction mean confidence intervals created multiple explanatory variables?","code":""},{"path":"mlr.html","id":"ethics-considerations-5","chapter":"9 Multiple Linear Regression","heading":"9.4  Ethics Considerations","text":"mean “keeping variables constant” interpreting single coefficient multiple regression model? interpretation important?important include variables interest model?Can including variables change relationships variables?big question next: options, decide include include?","code":""},{"path":"mlr.html","id":"r-mlr-with-rail-trails","chapter":"9 Multiple Linear Regression","heading":"9.5 R: MLR with Rail Trails","text":"variables used following analysis hightemp, volume, precip weekday. description data given :always good idea graph data look numerical summaries. Sometimes ’ll find important artifacts mistakes.Table 9.1: Data summaryVariable type: characterVariable type: logicalVariable type: numericWe’re interested predicting volume riders high temperature (F) given day.happens weekday included binary indicator variable?Note F p-value longer equal p-value(s) associated t-test coefficients. Also, degrees freedom now (2, 87) model estimates 3 parameters.Write estimated regression model separately weekdays weekends, sketch lines onto scatterplot.new coefficients (\\(b_0, b_1, b_2\\)) interpreted?coefficient hightemp change?\\(R^2\\) change? MSE change?say weekdayTRUE instead weekday?hightemp weekday interact?Note F p-value longer equal t-stat p-value(s). Now degrees freedom (3, 86) model estimates 4 parameters.Write estimated regression model separately weekdays non-weekdays, sketch lines onto scatterplot.interpret new coefficients (\\(b_0, b_1, b_2, b_3\\))?happened significance? coefficient weekday change?\\(R^2\\) change? MSE change?happens model additional quantitative variable?Note p-values, parameter estimates, \\(R^2\\), MSE, F-stat, df, F-stat p-values.","code":"library(mosiacData)\n?RailTrail\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.4) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  skim_without_charts()\nRailTrail %>%\n  lm(volume ~ hightemp, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic       p.value\n##   <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)   -17.1     59.4      -0.288 0.774        \n## 2 hightemp        5.70     0.848     6.72  0.00000000171\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp + weekday, data = .) %>%\n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic      p.value\n##   <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept)    42.8     64.3       0.665 0.508       \n## 2 hightemp        5.35     0.846     6.32  0.0000000109\n## 3 weekdayTRUE   -51.6     23.7      -2.18  0.0321\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume, color = weekday)) + \n  geom_point(alpha = 0.4) +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp * weekday, data = .) %>%\n  tidy()## # A tibble: 4 × 5\n##   term                 estimate std.error statistic p.value\n##   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)            135.      108.        1.25 0.215  \n## 2 hightemp                 4.07      1.47      2.78 0.00676\n## 3 weekdayTRUE           -186.      129.       -1.44 0.153  \n## 4 hightemp:weekdayTRUE     1.91      1.80      1.06 0.292\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume, color = weekday)) + \n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp + weekday + precip, data = .) %>%\n  tidy()## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    19.3     60.3       0.320 7.50e- 1\n## 2 hightemp        5.80     0.799     7.26  1.59e-10\n## 3 weekdayTRUE   -43.1     22.2      -1.94  5.52e- 2\n## 4 precip       -146.      38.9      -3.74  3.27e- 4"},{"path":"mlr.html","id":"r-model-comparison-with-restaurant-tips","chapter":"9 Multiple Linear Regression","heading":"9.6 R: Model comparison with Restaurant tips4","text":"student collected data restaurant waitress (Dahlquist Dong 2011). student interested learning conditions waitress can expect largest tips—example: dinner time late night? younger older patrons? patrons receiving free meals? patrons drinking alcohol? patrons tipping cash credit? tip amount measured total dollar amount percentage?variables help us predict amount customers tip restaurant? model best?Instead jumping predictions immediately, let’s look data . wrangling can data order make model accurate easy communicate?","code":"## # A tibble: 169 × 4\n##      Tip Party Meal   Age   \n##    <dbl> <dbl> <chr>  <chr> \n##  1  2.99     1 Dinner Yadult\n##  2  2        1 Dinner Yadult\n##  3  5        1 Dinner SenCit\n##  4  4        3 Dinner Middle\n##  5 10.3      2 Dinner SenCit\n##  6  4.85     2 Dinner Middle\n##  7  5        4 Dinner Yadult\n##  8  4        3 Dinner Middle\n##  9  5        2 Dinner Middle\n## 10  1.58     1 Dinner SenCit\n## # … with 159 more rows"},{"path":"mlr.html","id":"variables","chapter":"9 Multiple Linear Regression","heading":"9.6.1 Variables","text":"Predictors / Explanatory:Party: Number people partyMeal: Time day (Lunch, Dinner, Late Night)Age: Age category person paying bill (Yadult, Middle, SenCit)Outcome / Response: Tip: Amount tip","code":""},{"path":"mlr.html","id":"response-tip","chapter":"9 Multiple Linear Regression","heading":"Response: Tip","text":"","code":""},{"path":"mlr.html","id":"explanatory","chapter":"9 Multiple Linear Regression","heading":"Explanatory","text":"","code":""},{"path":"mlr.html","id":"relevel-categorical-explanatory","chapter":"9 Multiple Linear Regression","heading":"Relevel categorical explanatory","text":"","code":"\ntips <- tips %>%\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"},{"path":"mlr.html","id":"explanatory-again","chapter":"9 Multiple Linear Regression","heading":"Explanatory, again","text":"","code":""},{"path":"mlr.html","id":"response-vs.-predictors","chapter":"9 Multiple Linear Regression","heading":"Response vs. predictors","text":"","code":""},{"path":"mlr.html","id":"fit-and-summarize-model","chapter":"9 Multiple Linear Regression","heading":"9.6.2 Fit and summarize model","text":"","code":"\ntip_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit, conf.int = TRUE) %>%\n  kable(digits = 3)"},{"path":"mlr.html","id":"r-squared-r2","chapter":"9 Multiple Linear Regression","heading":"R-squared, \\(R^2\\)","text":"Recall: \\(R^2\\) proportion variation response variable explained regression model.\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]","code":"\nglance(tip_fit)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.641         0.635  2.04      98.3 1.56e-36     3  -358.  726.  742.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"},{"path":"mlr.html","id":"model-comparison","chapter":"9 Multiple Linear Regression","heading":"9.6.3 Model comparison","text":"","code":""},{"path":"mlr.html","id":"r-squared-r2-1","chapter":"9 Multiple Linear Regression","heading":"R-squared, \\(R^2\\)","text":"\\(R^2\\) always increase add variables model + add enough variables, can always achieve \\(R^2=100\\%\\)use \\(R^2\\) choose best fit model, prone choose model predictor variables","code":""},{"path":"mlr.html","id":"adjusted-r2-1","chapter":"9 Multiple Linear Regression","heading":"Adjusted \\(R^2\\)","text":"Adjusted \\(R^2\\): measure includes penalty unnecessary predictor variablesSimilar \\(R^2\\), measure amount variation response explained regression modelDiffers \\(R^2\\) using mean squares rather sums squares therefore adjusting number predictor variables\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]Adjusted \\(R^2\\) can used quick assessment compare fit multiple models; however, assessment!Use \\(R^2\\) describing relationship response predictor variables","code":"\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age +  Meal, data = tips)\n\nglance(tip_fit_1) %>% \n  select(r.squared, adj.r.squared)## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.674         0.664\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal + Day, data = tips)\n\nglance(tip_fit_2) %>% \n  select(r.squared, adj.r.squared)## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.683         0.662"},{"path":"mlr.html","id":"comparing-models-with-aic-and-bic","chapter":"9 Multiple Linear Regression","heading":"Comparing models with AIC and BIC","text":"","code":"\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal, data = tips)\n\nglance(tip_fit_1) %>% \n  select(AIC, BIC)## # A tibble: 1 × 2\n##     AIC   BIC\n##   <dbl> <dbl>\n## 1  714.  736.\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal + Day, data = tips)\n\nglance(tip_fit_2) %>% \n  select(AIC, BIC)## # A tibble: 1 × 2\n##     AIC   BIC\n##   <dbl> <dbl>\n## 1  720.  757."},{"path":"mlr.html","id":"commonalities-between-criteria","chapter":"9 Multiple Linear Regression","heading":"9.6.4 Commonalities between criteria","text":"\\(R^2_{adj}\\), AIC, BIC apply penalty predictorsThe penalty added model complexity attempts strike balance underfitting (predictors model) overfitting (many predictors model)Goal: Parsimony","code":""},{"path":"mlr.html","id":"parsimony-and-occams-razor","chapter":"9 Multiple Linear Regression","heading":"Parsimony and Occam’s razor","text":"principle parsimony attributed William Occam (early 14th-century English nominalist philosopher), insisted , given set equally good explanations given phenomenon, correct explanation simplest explanation5The principle parsimony attributed William Occam (early 14th-century English nominalist philosopher), insisted , given set equally good explanations given phenomenon, correct explanation simplest explanation5Called Occam’s razor “shaved” explanations bare minimumCalled Occam’s razor “shaved” explanations bare minimumParsimony modeling:Parsimony modeling:models parameters possiblemodels parameters possiblelinear models preferred non-linear modelslinear models preferred non-linear modelsexperiments relying assumptions preferred relying manyexperiments relying assumptions preferred relying manymodels pared minimal adequatemodels pared minimal adequatesimple explanations preferred complex explanationssimple explanations preferred complex explanations","code":""},{"path":"mlr.html","id":"in-pursuit-of-occams-razor","chapter":"9 Multiple Linear Regression","heading":"In pursuit of Occam’s razor","text":"Occam’s razor states among competing hypotheses predict equally well, one fewest assumptions selectedOccam’s razor states among competing hypotheses predict equally well, one fewest assumptions selectedModel selection follows principleModel selection follows principleWe want add another variable model addition variable brings something valuable terms predictive power modelWe want add another variable model addition variable brings something valuable terms predictive power modelIn words, prefer simplest best model, .e. parsimonious modelIn words, prefer simplest best model, .e. parsimonious model","code":""},{"path":"mlr.html","id":"alternate-views","chapter":"9 Multiple Linear Regression","heading":"Alternate views","text":"Sometimes simple model outperform complex model .\n. . Nevertheless, believe deliberately limiting complexity model fruitful problem evidently complex.\nInstead, simple model found outperforms particular complex model, appropriate response define different complex model captures whatever aspect problem led simple model performing well.Radford Neal - Bayesian Learning Neural Networks[Suggested blog post: Occam Andrew Gelman]","code":""},{"path":"mlr.html","id":"other-concerns-with-the-approach","chapter":"9 Multiple Linear Regression","heading":"Other concerns with the approach","text":"criteria considered model comparison require making predictions data uses prediction error (\\(SS_{Error}\\)) somewhere formulaBut ’re making prediction data used build model (estimate coefficients), can lead overfittingInstead \nsplit data testing training sets\n“train” model training data pick models ’re genuinely considering potentially good models\ntest models testing set\nsplit data testing training sets“train” model training data pick models ’re genuinely considering potentially good modelstest models testing set","code":""},{"path":"process.html","id":"process","chapter":"10 Modeling as a Process6","heading":"10 Modeling as a Process6","text":"","code":""},{"path":"process.html","id":"comparing-models","chapter":"10 Modeling as a Process6","heading":"10.1 Comparing Models","text":"many models, little time. Understanding interpret multiple linear regression model half modeling process. real-life situations, many variables can used predict describe response variable hand. choose combination subset variables best?, ’ll walk many big ideas surrounding modeling building process. next chapter consider technical ideas behind statistical inference modeling building. even using formal inference, ideas always inform larger process analysis conclusions.Leo Breiman among giants machine learning helped bridge ideas statistics computer science. Trained statistician, spent career University California, Berkeley developed Classification Regression Trees (CART), bagging, Random Forests.Among important insights idea two cultures can motivate (linear) modeling:Think data generated black box vector \ninput variables x (independent variables) go one side, side response variables y come . Inside black box, nature functions associate predictor variables response variables.Data Modeling: analysis culture starts assuming stochastic data model inside black box…values parameters estimated data model used information /predictionAlgorithmic Modeling: analysis culture considers inside box complex unknown. [] approach find function f(x) — algorithm operates x predict responses y.Chapter 10, focus algorithmic modeling developing models optimally predictive response variable hand.","code":""},{"path":"process.html","id":"worth-a-comment","chapter":"10 Modeling as a Process6","heading":"Worth a comment","text":"Notice R code gotten interesting now. fun!! R code help process! R package tidymodels includes tools facilitate, particular, feature engineering cross validation.","code":""},{"path":"process.html","id":"feature-engineering","chapter":"10 Modeling as a Process6","heading":"10.2 Feature Engineering","text":"example used consider feature engineering, data come data.world, way TidyTuesday. now exist schrute R package.Can IMDB rating (imdb_rating) Office predicted variables dataset? model best?Instead jumping predictions immediately, let’s look data . wrangling can data order take advantage information variables? ’d also like make model accurate easy communicate.","code":"## # A tibble: 188 × 6\n##    season episode title             imdb_rating total_votes air_date  \n##     <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>    \n##  1      1       1 Pilot                     7.6        3706 2005-03-24\n##  2      1       2 Diversity Day             8.3        3566 2005-03-29\n##  3      1       3 Health Care               7.9        2983 2005-04-05\n##  4      1       4 The Alliance              8.1        2886 2005-04-12\n##  5      1       5 Basketball                8.4        3179 2005-04-19\n##  6      1       6 Hot Girl                  7.8        2852 2005-04-26\n##  7      2       1 The Dundies               8.7        3213 2005-09-20\n##  8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n##  9      2       3 Office Olympics           8.4        2742 2005-10-04\n## 10      2       4 The Fire                  8.4        2713 2005-10-11\n## # … with 178 more rows"},{"path":"process.html","id":"imdb-ratings","chapter":"10 Modeling as a Process6","heading":"IMDB ratings","text":"","code":""},{"path":"process.html","id":"imdb-ratings-vs.-number-of-votes","chapter":"10 Modeling as a Process6","heading":"IMDB ratings vs. number of votes","text":"","code":""},{"path":"process.html","id":"outliers","chapter":"10 Modeling as a Process6","heading":"Outliers?","text":"","code":""},{"path":"process.html","id":"aside","chapter":"10 Modeling as a Process6","heading":"Aside…","text":"like Dinner Party episode, highly recommend “oral history” episode published Rolling Stone magazine.","code":""},{"path":"process.html","id":"rating-vs.-air-date","chapter":"10 Modeling as a Process6","heading":"Rating vs. air date","text":"","code":""},{"path":"process.html","id":"imdb-ratings-vs.-seasons","chapter":"10 Modeling as a Process6","heading":"IMDB ratings vs. seasons","text":"","code":""},{"path":"process.html","id":"building-a-model","chapter":"10 Modeling as a Process6","heading":"10.2.1 Building a Model","text":"idea build model can predict IMDB ratings, need way see (end). Indeed, important us put data pocket (called “test” data) doesn’t see modeling process. ’ll use test data end assess whether predictions good.","code":""},{"path":"process.html","id":"train-test","chapter":"10 Modeling as a Process6","heading":"Train / test","text":"Step 1: Create initial split:Step 2: Save training dataStep 3: Save testing data","code":"\nset.seed(123)\noffice_split <- initial_split(office_ratings) # prop = 3/4 by default\noffice_train <- training(office_split)\ndim(office_train)## [1] 141   6\noffice_test  <- testing(office_split)\ndim(office_test)## [1] 47  6"},{"path":"process.html","id":"using-the-training-data","chapter":"10 Modeling as a Process6","heading":"Using the training data","text":"","code":"\noffice_train## # A tibble: 141 × 6\n##    season episode title               imdb_rating total_votes air_date  \n##     <dbl>   <dbl> <chr>                     <dbl>       <dbl> <date>    \n##  1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n##  2      9      14 Vandalism                   7.6        1402 2013-01-31\n##  3      2       8 Performance Review          8.2        2416 2005-11-15\n##  4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n##  5      3      22 Beach Games                 9.1        2783 2007-05-10\n##  6      7       1 Nepotism                    8.4        1897 2010-09-23\n##  7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n##  8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n##  9      9      18 Promos                      8          1445 2013-04-04\n## 10      8      12 Pool Party                  8          1612 2012-01-19\n## # … with 131 more rows"},{"path":"process.html","id":"feature-engineering-1","chapter":"10 Modeling as a Process6","heading":"10.2.2 Feature engineering","text":"prefer simple models possible, parsimony mean sacrificing accuracy (predictive performance) interest simplicity.prefer simple models possible, parsimony mean sacrificing accuracy (predictive performance) interest simplicity.Variables go model represented just critical success model.Variables go model represented just critical success model.Feature engineering allows us get creative predictors effort make useful model (increase predictive performance).Feature engineering allows us get creative predictors effort make useful model (increase predictive performance).","code":""},{"path":"process.html","id":"feature-engineering-with-dplyr","chapter":"10 Modeling as a Process6","heading":"Feature engineering with dplyr","text":"can use dplyr (tidyverse) , example, mutate() function create new variables use models.Can identify potential problems approach?One problems mutate() approach test training data get formatted separately might inconsistencies test data predicted end.","code":"\noffice_train %>%\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )## # A tibble: 141 × 8\n##   season episode title            imdb_rating total_votes air_date   month  wday\n##   <fct>    <dbl> <chr>                  <dbl>       <dbl> <date>     <dbl> <dbl>\n## 1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n## 2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n## 3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n## 4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n## 5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n## 6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n## # … with 135 more rows"},{"path":"process.html","id":"modeling-workflow","chapter":"10 Modeling as a Process6","heading":"Modeling workflow","text":"Ideally, feature engineering happens part workflow. , part modeling process. way, training data used fit model, feature engineering happens. test data used come predictions, feature engineering also happens.Create recipe feature engineering steps applied training dataCreate recipe feature engineering steps applied training dataFit model training data steps appliedFit model training data steps appliedUsing model estimates training data, predict outcomes test dataUsing model estimates training data, predict outcomes test dataEvaluate performance model test dataEvaluate performance model test data","code":""},{"path":"process.html","id":"specifying-a-model","chapter":"10 Modeling as a Process6","heading":"10.2.3 Specifying a model","text":"Instead using lm() command, ’re going use tidymodels framework specify model. Math 158 always use “lm,” take applied statistics classes, ’ll use different model specifications feature engineering modeling fitting steps.","code":"\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"building-a-recipe","chapter":"10 Modeling as a Process6","heading":"10.2.4 Building a recipe","text":"steps building recipe done sequentially format variable desired model. seen Section (@ref{sec:wflow}), recipe steps can happen sequence using pipe (%>%) function.However, work single pipeline, recipe effects data aren’t seen, can unsettling. can look happen ultimately apply recipe data using functions prep() bake().Note: Using prep() bake() shown demonstrative purposes. need part pipeline. find assuring, however, can see effects recipe steps recipe built.","code":""},{"path":"process.html","id":"initiate-a-recipe","chapter":"10 Modeling as a Process6","heading":"Initiate a recipe","text":"","code":"\noffice_rec <- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          5"},{"path":"process.html","id":"step-1-alter-roles","chapter":"10 Modeling as a Process6","heading":"Step 1: Alter roles","text":"title isn’t predictor, might want keep around ID.update_role() alters existing role recipe assigns initial role variables yet declared role.","code":"\noffice_rec <- office_rec %>%\n  update_role(title, new_role = \"ID\")\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 6\n## $ season      <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2, 9…\n## $ episode     <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20, 8,…\n## $ title       <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review\", …\n## $ total_votes <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1612…\n## $ air_date    <date> 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-05-1…\n## $ imdb_rating <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, 8.9…"},{"path":"process.html","id":"step-2-add-features","chapter":"10 Modeling as a Process6","heading":"Step 2: Add features","text":"New features day week month. , air_date variable specified keep separate information day week month information.step_date() creates specification recipe step convert date data one factor numeric variables.","code":"\noffice_rec <- office_rec %>%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 8\n## $ season         <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2…\n## $ episode        <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,…\n## $ title          <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review…\n## $ total_votes    <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1…\n## $ air_date       <date> 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0…\n## $ imdb_rating    <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, …\n## $ air_date_dow   <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, …\n## $ air_date_month <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, …"},{"path":"process.html","id":"step-3-add-more-features","chapter":"10 Modeling as a Process6","heading":"Step 3: Add more features","text":"Identify holidays air_date, remove air_date.step_holiday() creates specification recipe step convert date data one binary indicator variables common holidays.","code":"\noffice_rec <- office_rec %>%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 11\n## $ season                     <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_dow               <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n## $ air_date_month             <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-4-convert-numbers-to-factors","chapter":"10 Modeling as a Process6","heading":"Step 4: Convert numbers to factors","text":"Convert season factor.step_num2factor() convert one numeric vectors factors (ordered unordered). can useful categories encoded integers.","code":"\noffice_rec <- office_rec %>%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 11\n## $ season                     <fct> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_dow               <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n## $ air_date_month             <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-5-make-dummy-variables","chapter":"10 Modeling as a Process6","heading":"Step 5: Make dummy variables","text":"Convert nominal (categorical) predictors factors.step_dummy() creates specification recipe step convert nominal data (e.g. character factors) one numeric binary model terms levels original data.","code":"\noffice_rec <- office_rec %>%\n  step_dummy(all_nominal_predictors())\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 33\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X2                  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X3                  <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X4                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X5                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0…\n## $ season_X6                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n## $ season_X7                  <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n## $ season_X8                  <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n## $ season_X9                  <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0…\n## $ air_date_dow_Mon           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Tue           <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Wed           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Thu           <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ air_date_dow_Fri           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Sat           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Feb         <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Mar         <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Apr         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1…\n## $ air_date_month_May         <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0…\n## $ air_date_month_Jun         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Jul         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Aug         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Sep         <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n## $ air_date_month_Oct         <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Nov         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Dec         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-6-remove-zero-variance-predictors","chapter":"10 Modeling as a Process6","heading":"Step 6: Remove zero variance predictors","text":"Remove predictors contain single value. “zero variance” means variability entire column, literally every value . variables won’t ever help prediction.step_zv() creates specification recipe step remove variables contain single value.","code":"\noffice_rec <- office_rec %>%\n  step_zv(all_predictors())\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\n## Zero variance filter on all_predictors()\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 22\n## $ episode            <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,…\n## $ title              <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Re…\n## $ total_votes        <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144…\n## $ imdb_rating        <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8…\n## $ season_X2          <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X3          <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X4          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X5          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n## $ season_X6          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, …\n## $ season_X7          <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ season_X8          <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X9          <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n## $ air_date_dow_Tue   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_dow_Thu   <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ air_date_month_Feb <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Mar <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Apr <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ air_date_month_May <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n## $ air_date_month_Sep <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n## $ air_date_month_Oct <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n## $ air_date_month_Nov <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n## $ air_date_month_Dec <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"},{"path":"process.html","id":"putting-it-altogether","chapter":"10 Modeling as a Process6","heading":"Putting it altogether","text":"Turns feature engineering steps can using pipe function (%>%) concatenate functions.","code":"\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %>%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %>%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %>%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %>%\n  # remove zero variance predictors\n  step_zv(all_predictors())\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\n## Zero variance filter on all_predictors()"},{"path":"process.html","id":"building-workflows","chapter":"10 Modeling as a Process6","heading":"10.2.5 Building workflows","text":"Workflows bring together models recipes can easily applied training test data.","code":""},{"path":"process.html","id":"specify-model","chapter":"10 Modeling as a Process6","heading":"Specify model","text":"workflow: Notice two important parts workflows model specification feature engineering recipe information.","code":"\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\noffice_wflow <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec)\n\noffice_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_date()\n## • step_holiday()\n## • step_num2factor()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"fit-model-to-training-data","chapter":"10 Modeling as a Process6","heading":"Fit model to training data","text":"workflow hand, model can now fit training data. Although, wow… many predictors!","code":"\noffice_fit <- office_wflow %>%\n  fit(data = office_train)\n\noffice_fit %>% tidy() %>% print(n = 21)## # A tibble: 21 × 5\n##    term                estimate std.error statistic  p.value\n##    <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)         6.40     0.510        12.5   1.51e-23\n##  2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n##  3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n##  4 season_X2           0.811    0.327         2.48  1.44e- 2\n##  5 season_X3           1.04     0.343         3.04  2.91e- 3\n##  6 season_X4           1.09     0.295         3.70  3.32e- 4\n##  7 season_X5           1.08     0.348         3.11  2.34e- 3\n##  8 season_X6           1.00     0.367         2.74  7.18e- 3\n##  9 season_X7           1.02     0.352         2.89  4.52e- 3\n## 10 season_X8           0.497    0.348         1.43  1.55e- 1\n## 11 season_X9           0.621    0.345         1.80  7.41e- 2\n## 12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n## 13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n## 14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n## 15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n## 16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n## 17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n## 18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n## 19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n## 20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n## 21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"},{"path":"process.html","id":"evaluate-the-model","chapter":"10 Modeling as a Process6","heading":"10.2.6 Evaluate the model","text":"","code":""},{"path":"process.html","id":"predictions-for-training-data","chapter":"10 Modeling as a Process6","heading":"Predictions for training data","text":"","code":"\noffice_train_pred <- predict(office_fit, office_train) %>%\n  bind_cols(office_train %>% select(imdb_rating, title))\n\noffice_train_pred## # A tibble: 141 × 3\n##    .pred imdb_rating title              \n##    <dbl>       <dbl> <chr>              \n##  1  7.57         7.8 Last Day in Florida\n##  2  7.77         7.6 Vandalism          \n##  3  8.31         8.2 Performance Review \n##  4  7.67         7.1 Here Comes Treble  \n##  5  8.84         9.1 Beach Games        \n##  6  8.33         8.4 Nepotism           \n##  7  8.46         8.3 Phyllis' Wedding   \n##  8  8.14         8.9 Livin' the Dream   \n##  9  7.87         8   Promos             \n## 10  7.74         8   Pool Party         \n## # … with 131 more rows"},{"path":"process.html","id":"r-squared","chapter":"10 Modeling as a Process6","heading":"R-squared","text":"Percentage variability IMDB ratings explained model.models high low \\(R^2\\) preferable?","code":"\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.670"},{"path":"process.html","id":"rmse","chapter":"10 Modeling as a Process6","heading":"RMSE","text":"alternative model performance statistic: root mean square error.\\[RMSE = \\sqrt{\\frac{\\sum_{= 1}^n (y_i - \\hat{y}_i)^2}{n}}\\]models high low RMSE preferable?RMSE considered low high?Depends…, really…cares predictions training data?","code":"\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.302\noffice_train %>%\n  summarise(min = min(imdb_rating), \n            max = max(imdb_rating))## # A tibble: 1 × 2\n##     min   max\n##   <dbl> <dbl>\n## 1   6.7   9.7"},{"path":"process.html","id":"predictions-for-testing-data","chapter":"10 Modeling as a Process6","heading":"Predictions for testing data","text":"","code":"\noffice_test_pred <- predict(office_fit, office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title))\n\noffice_test_pred## # A tibble: 47 × 3\n##    .pred imdb_rating title              \n##    <dbl>       <dbl> <chr>              \n##  1  8.03         8.3 Diversity Day      \n##  2  7.98         7.9 Health Care        \n##  3  8.41         8.4 The Fire           \n##  4  8.35         8.2 Halloween          \n##  5  8.35         8.4 E-Mail Surveillance\n##  6  8.68         9   The Injury         \n##  7  8.32         7.9 The Carpet         \n##  8  8.93         9.3 Casino Night       \n##  9  8.80         8.9 Gay Witch Hunt     \n## 10  8.37         8.2 Initiation         \n## # … with 37 more rows"},{"path":"process.html","id":"evaluate-performance-for-testing-data","chapter":"10 Modeling as a Process6","heading":"Evaluate performance for testing data","text":"\\(R^2\\) model fit testing dataRMSE model fit testing data","code":"\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.468\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.411"},{"path":"process.html","id":"training-vs.-testing","chapter":"10 Modeling as a Process6","heading":"Training vs. testing","text":"","code":"\nrmse_train <- rmse(office_train_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrsq_train <- rsq(office_train_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrmse_test <- rmse(office_test_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrsq_test <- rsq(office_test_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)"},{"path":"process.html","id":"evaluating-performance-on-training-data","chapter":"10 Modeling as a Process6","heading":"Evaluating performance on training data","text":"training set capacity good arbiter performance.training set capacity good arbiter performance.independent piece information; predicting training set can reflect model already knows.independent piece information; predicting training set can reflect model already knows.Suppose give class test, give answers, provide test.\nstudent scores second test accurately reflect know subject; scores probably higher results first test.Suppose give class test, give answers, provide test.\nstudent scores second test accurately reflect know subject; scores probably higher results first test.","code":""},{"path":"process.html","id":"cross-validation","chapter":"10 Modeling as a Process6","heading":"10.3 Cross Validation","text":"get details cross validation, let’s set scenario need cross validation. Recall use test data assess model . haven’t yet thought use data build particular model.example, let’s set scenario compare two different models. first model one built recipe . second model use air_date variable .Model 1:Model 2:","code":"\noffice_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"ID\") %>%\n  step_date(air_date, features = c(\"dow\", \"month\")) %>%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %>%\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\nprep(office_rec1) %>%\nbake(office_train) %>%\n  glimpse()## Rows: 141\n## Columns: 22\n## $ episode            <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,…\n## $ title              <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Re…\n## $ total_votes        <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144…\n## $ imdb_rating        <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8…\n## $ season_X2          <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X3          <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X4          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X5          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n## $ season_X6          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, …\n## $ season_X7          <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ season_X8          <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X9          <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n## $ air_date_dow_Tue   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_dow_Thu   <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ air_date_month_Feb <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Mar <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Apr <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ air_date_month_May <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n## $ air_date_month_Sep <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n## $ air_date_month_Oct <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n## $ air_date_month_Nov <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n## $ air_date_month_Dec <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\noffice_rec2 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"id\") %>%\n  # delete the air_date variable\n  step_rm(air_date) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\nprep(office_rec2) %>%\nbake(office_train) %>%\n  glimpse()## Rows: 141\n## Columns: 5\n## $ season      <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2, 9…\n## $ episode     <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20, 8,…\n## $ title       <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review\", …\n## $ total_votes <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1612…\n## $ imdb_rating <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, 8.9…"},{"path":"process.html","id":"creating-workflows","chapter":"10 Modeling as a Process6","heading":"Creating workflows","text":"Using separate recipes, different workflows set :Model 1:Model 2:","code":"\noffice_wflow1 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec1)\n\noffice_wflow1## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_date()\n## • step_holiday()\n## • step_num2factor()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\noffice_wflow2 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec2)\n\noffice_wflow2## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 3 Recipe Steps\n## \n## • step_rm()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"fit-the-models-to-the-training-data","chapter":"10 Modeling as a Process6","heading":"Fit the models to the training data","text":"WAIT, fast!","code":""},{"path":"process.html","id":"fit-the-models-using-cross-validation","chapter":"10 Modeling as a Process6","heading":"10.3.1 Fit the models using cross validation","text":"","code":""},{"path":"process.html","id":"spending-the-data","chapter":"10 Modeling as a Process6","heading":"“Spending” the data","text":"already established idea data spending test set recommended obtaining unbiased estimate performance.However, usually need understand effectiveness model using test set.Typically can’t decide final model take test set without making model assessments.Remedy: Resampling make model assessments training data way can generalize new data.","code":""},{"path":"process.html","id":"resampling-for-model-assessment","chapter":"10 Modeling as a Process6","heading":"Resampling for model assessment","text":"Resampling conducted training set.\ntest set involved.\niteration resampling, data partitioned two subsamples:model fit analysis set.model evaluated assessment set.\nFigure 10.1: Repeated samples taken training data, resample observations used build model observations used estimate performance. Source: (Kuhn Silge 2022)\nAside: “re” “resamples” repeated samples. confused repeated samples taken bootstrapping replacement. cross validation, repeated samples taken without replacement.","code":""},{"path":"process.html","id":"analysis-and-assessment-sets","chapter":"10 Modeling as a Process6","heading":"Analysis and assessment sets","text":"Analysis set analogous training set.Assessment set analogous test set.terms analysis assessment avoids confusion initial split data.data sets mutually exclusive.","code":""},{"path":"process.html","id":"cross-validation-1","chapter":"10 Modeling as a Process6","heading":"Cross validation","text":"specifically, v-fold cross validation – commonly used resampling technique:Randomly split training data v partitionsUse 1 partition assessment, remaining v-1 partitions analysisRepeat v times, updating partition used assessment timeLet’s give example v = 3…","code":""},{"path":"process.html","id":"cross-validation-step-1","chapter":"10 Modeling as a Process6","heading":"Cross validation, step 1","text":"Consider example training data randomly split 3 partitions:\nFigure 10.2: Splitting data partition v=3 groups. Source: (Kuhn Silge 2022)\nNote three repeated samples (“resamples”) taken without replacement original dataset.","code":"\nset.seed(345)\nfolds <- vfold_cv(office_train, v = 3)\nfolds## #  3-fold cross-validation \n## # A tibble: 3 × 2\n##   splits          id   \n##   <list>          <chr>\n## 1 <split [94/47]> Fold1\n## 2 <split [94/47]> Fold2\n## 3 <split [94/47]> Fold3"},{"path":"process.html","id":"cross-validation-steps-2-and-3","chapter":"10 Modeling as a Process6","heading":"Cross validation, steps 2 and 3","text":"Use 1 partition assessment, remaining v-1 partitions analysisRepeat v times, updating partition used assessment time\nFigure 10.3: data split three groups, can see 2/3 observations used fit model 1/3 observations used estimate performance model. Source: (Kuhn Silge 2022)\n","code":""},{"path":"process.html","id":"fit-resamples","chapter":"10 Modeling as a Process6","heading":"Fit resamples","text":"data split v (3) resamples, can fit two models interest.Model 1:Model 2:","code":"\nset.seed(456)\n\noffice_fit_rs1 <- office_wflow1 %>%\n  fit_resamples(folds)\n\noffice_fit_rs1## # Resampling results\n## # 3-fold cross-validation \n## # A tibble: 3 × 4\n##   splits          id    .metrics         .notes          \n##   <list>          <chr> <list>           <list>          \n## 1 <split [94/47]> Fold1 <tibble [2 × 4]> <tibble [0 × 1]>\n## 2 <split [94/47]> Fold2 <tibble [2 × 4]> <tibble [0 × 1]>\n## 3 <split [94/47]> Fold3 <tibble [2 × 4]> <tibble [1 × 1]>\nset.seed(456)\n\noffice_fit_rs2 <- office_wflow2 %>%\n  fit_resamples(folds)\n\noffice_fit_rs2## # Resampling results\n## # 3-fold cross-validation \n## # A tibble: 3 × 4\n##   splits          id    .metrics         .notes          \n##   <list>          <chr> <list>           <list>          \n## 1 <split [94/47]> Fold1 <tibble [2 × 4]> <tibble [0 × 1]>\n## 2 <split [94/47]> Fold2 <tibble [2 × 4]> <tibble [0 × 1]>\n## 3 <split [94/47]> Fold3 <tibble [2 × 4]> <tibble [0 × 1]>"},{"path":"process.html","id":"cross-validation-now-what","chapter":"10 Modeling as a Process6","heading":"Cross validation, now what?","text":"’ve fit bunch modelsNow ’s time use collect metrics (e.g., R-squared, RMSE) model use evaluate model fit varies across folds","code":""},{"path":"process.html","id":"collect-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Collect CV metrics","text":"Model 1:Model 2:","code":"\ncollect_metrics(office_fit_rs1)## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.402     3  0.0329 Preprocessor1_Model1\n## 2 rsq     standard   0.524     3  0.0410 Preprocessor1_Model1\ncollect_metrics(office_fit_rs1)## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.402     3  0.0329 Preprocessor1_Model1\n## 2 rsq     standard   0.524     3  0.0410 Preprocessor1_Model1"},{"path":"process.html","id":"deeper-look-into-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Deeper look into CV metrics","text":"Model 1:Model 2:","code":"\ncv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1## # A tibble: 6 × 5\n##   id    .metric .estimator .estimate .config             \n##   <chr> <chr>   <chr>          <dbl> <chr>               \n## 1 Fold1 rmse    standard       0.364 Preprocessor1_Model1\n## 2 Fold1 rsq     standard       0.590 Preprocessor1_Model1\n## 3 Fold2 rmse    standard       0.376 Preprocessor1_Model1\n## 4 Fold2 rsq     standard       0.449 Preprocessor1_Model1\n## 5 Fold3 rmse    standard       0.468 Preprocessor1_Model1\n## 6 Fold3 rsq     standard       0.534 Preprocessor1_Model1\ncv_metrics2 <- collect_metrics(office_fit_rs2, summarize = FALSE) \n\ncv_metrics2## # A tibble: 6 × 5\n##   id    .metric .estimator .estimate .config             \n##   <chr> <chr>   <chr>          <dbl> <chr>               \n## 1 Fold1 rmse    standard       0.388 Preprocessor1_Model1\n## 2 Fold1 rsq     standard       0.529 Preprocessor1_Model1\n## 3 Fold2 rmse    standard       0.421 Preprocessor1_Model1\n## 4 Fold2 rsq     standard       0.266 Preprocessor1_Model1\n## 5 Fold3 rmse    standard       0.411 Preprocessor1_Model1\n## 6 Fold3 rsq     standard       0.518 Preprocessor1_Model1"},{"path":"process.html","id":"better-tabulation-of-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Better tabulation of CV metrics","text":"","code":""},{"path":"process.html","id":"how-does-rmse-compare-to-y","chapter":"10 Modeling as a Process6","heading":"How does RMSE compare to y?","text":"Recall RMSE calculated original units response variable.\\[RMSE_{\\mbox{training}} = \\sqrt{\\frac{\\sum_{= 1}^n (y_i - \\hat{y}_i)^2}{n}}\\]CV RMSE uses 2/3 observations build model (\\(\\hat{y}_i\\)) 1/3 observations test . equation “” 1/3. Note \\(\\hat{y}_{\\tiny\\mbox{2/3 fold 1}, \\normalsize }\\) indicates \\(^{th}\\) row predicted using model built 2/3^{rds} observations.\\[RMSE_{\\mbox{fold 1}} = \\sqrt{\\frac{\\sum_{\\\\tiny \\mbox{1/3 fold 1}} \\normalsize (y_i - \\hat{y}_{\\tiny\\mbox{2/3 fold 1}, \\normalsize })^2}{n/3}}\\]comparing model better, CV RMSE provides information well model predicting 1/3 hold sample. Indeed, RMSE error (read: variability) continues exist even model built.can compare model RMSE original variability seen imbd_rating variable.Model 1:Model 2:Training data IMDB score stats:original variability (measured standard deviation) ratings 0.538. running Model 1, remaining variability (measured RMSE averaged folds) 0.482; running Model 2, remaining variability (measured RMSE averaged folds) 0.52.Conclusions:seems though linear model reduce variability response variable (though much).seems though linear model includes air_date variable (21 coefficients!) (slightly) better model variable include air_date (4 coefficients).","code":"\ncv_metrics1 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )## # A tibble: 1 × 4\n##     min   max  mean     sd\n##   <dbl> <dbl> <dbl>  <dbl>\n## 1 0.364 0.468 0.402 0.0569\ncv_metrics2 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )## # A tibble: 1 × 4\n##     min   max  mean     sd\n##   <dbl> <dbl> <dbl>  <dbl>\n## 1 0.388 0.421 0.407 0.0168\noffice_ratings %>%\n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )## # A tibble: 1 × 4\n##     min   max  mean    sd\n##   <dbl> <dbl> <dbl> <dbl>\n## 1   6.7   9.7  8.26 0.538"},{"path":"process.html","id":"cross-validation-jargon","chapter":"10 Modeling as a Process6","heading":"Cross validation jargon","text":"Referred v-fold k-fold cross validationAlso commonly abbreviated CV","code":""},{"path":"process.html","id":"cross-validation-redux","chapter":"10 Modeling as a Process6","heading":"Cross validation, redux","text":"illustrate CV works, used v = 3:illustrate CV works, used v = 3:Analysis sets 2/3 training setAnalysis sets 2/3 training setEach assessment set distinct 1/3Each assessment set distinct 1/3The final resampling estimate performance averages 3 replicatesThe final resampling estimate performance averages 3 replicatesIt useful illustrative purposes, v = 3 poor choice practiceIt useful illustrative purposes, v = 3 poor choice practiceValues v often 5 10; generally prefer 10-fold cross-validation defaultValues v often 5 10; generally prefer 10-fold cross-validation default","code":""},{"path":"process.html","id":"final-model-assessment","chapter":"10 Modeling as a Process6","heading":"10.4 Final model assessment","text":"Now Model 1 chosen better model, test data finally brought measure well Model 1 predict data wild (additional wild episodes Office, …).Model 1:seen , test \\(R^2\\) test data 0.468 (46.8% variability imdb_rating test data explained model training data). Additionally, test RMSE 0.411. expected, RMSE lower training test; \\(R^2\\) higher training test.","code":"\noffice_preds1 <- office_wflow1 %>%\n  fit(data = office_train) %>%\n  predict(office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title)) \n\noffice_preds1 %>%\n  rsq(truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.468\noffice_preds1 %>%\n  rmse(truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.411"},{"path":"build.html","id":"build","chapter":"11 Model Building","heading":"11 Model Building","text":"get , use function bestregsubsets().best model explain variation tips?","code":""},{"path":"build.html","id":"another-model-summary","chapter":"11 Model Building","heading":"11.1 Another model summary7","text":"","code":"\nanova(tip_fit$fit) %>%\n  tidy() %>%\n  kable(digits = 2)"},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"12 Analysis of variance (ANOVA)","heading":"12 Analysis of variance (ANOVA)","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova-1","chapter":"12 Analysis of variance (ANOVA)","heading":"12.1 Analysis of variance (ANOVA)","text":"Main Idea: Decompose total variation outcome :\nvariation can explained variables model\nvariation can’t explained model (left residuals)\nvariation can explained variables modelthe variation can explained variables modelthe variation can’t explained model (left residuals)variation can’t explained model (left residuals)variation can explained variables model greater variation residuals, signals model might “valuable” (least one \\(\\beta\\)s equal 0)","code":""},{"path":"analysis-of-variance-anova.html","id":"anova-output-1","chapter":"12 Analysis of variance (ANOVA)","heading":"12.2 ANOVA output","text":"","code":"\nanova(tip_fit$fit) %>%\n  tidy() %>%\n  kable(digits = 2)"},{"path":"analysis-of-variance-anova.html","id":"anova-output-with-totals","chapter":"12 Analysis of variance (ANOVA)","heading":"12.3 ANOVA output, with totals","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"sum-of-squares","chapter":"12 Analysis of variance (ANOVA)","heading":"12.4 Sum of squares","text":"\\(SS_{Total}\\): Total sum squares, variability outcome, \\(\\sum_{= 1}^n (y_i - \\bar{y})^2\\)\\(SS_{Error}\\): Residual sum squares, variability residuals, \\(\\sum_{= 1}^n (y_i - \\hat{y})^2\\)\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained model","code":""},{"path":"analysis-of-variance-anova.html","id":"sec:nestF","chapter":"12 Analysis of variance (ANOVA)","heading":"12.5 Testing Sets of Coefficients","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"model-selection","chapter":"12 Analysis of variance (ANOVA)","heading":"12.6 Model Selection","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"other-ways-for-comparing-models","chapter":"12 Analysis of variance (ANOVA)","heading":"12.7 Other ways for comparing models","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"getting-the-variables-right","chapter":"12 Analysis of variance (ANOVA)","heading":"12.8 Getting the Variables Right","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"one-model-building-strategy","chapter":"12 Analysis of variance (ANOVA)","heading":"12.9 One Model Building Strategy","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"thoughts-on-model-selection","chapter":"12 Analysis of variance (ANOVA)","heading":"12.9.1 Thoughts on Model Selection…","text":"Question: females receive lower starting salaries males? [Statistical Sleuth Ramsey Schafer]model: y = log(salary), x’s: seniority, age, experience, education, sex.Sleuth, first find good model using seniority, age, experience education (including considerations interactions/quadratics). find suitable model (Model 1), add sex variable model determine significant. (H0: Model 1 vs HA: Model 1 + sex) regression texts, models considered include sex variable beginning, work , always keeping sex variable . pluses/minuses approaches?Response seems possible, even likely, sex associated variables, depending model selection starts sex included done, entirely possible choose model includes sex one variables, sex significant. however, variables included, sex might explain significant amount variation beyond others. Whereas model selection doesn’t start sex likely include associated covariates start .\nOne nice aspect methods end sex model; one difficulty model selection procedure ends removing variable interest people claim variable interest doesn’t matter. However, often advantageous avoid model selection much possible. model answers different question, ideally good decide ahead time question interest .\ncase two questions interest; differences (univariate model), differences accounting covariates (multivariate model)? differences get smaller adjusting covariates, leads interesting question , whether differences also part sex discrimination. Consider explanation wage gap men women due men higher-paying jobs, really, ’s part problem, jobs women pay less. :( point, though, one model may sufficient particular situation, looking one “best” model can misleading.","code":""},{"path":"diag2.html","id":"diag2","chapter":"13 Diagnostics II","heading":"13 Diagnostics II","text":"","code":""},{"path":"shrink.html","id":"shrink","chapter":"14 Shrinkage Methods","heading":"14 Shrinkage Methods","text":"","code":""},{"path":"smooth.html","id":"smooth","chapter":"15 Smoothing Methods","heading":"15 Smoothing Methods","text":"","code":""},{"path":"anova.html","id":"anova","chapter":"16 ANOVA","heading":"16 ANOVA","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
