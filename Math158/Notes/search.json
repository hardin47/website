[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 158 Pomona College: Computational Statistics. notes based extensively Introduction Statistical Learning (James et al. 2021); Applied Linear Statistical Models (Kutner et al. 2004). computing part class taken R Data Science (Wickham Grolemund 2017) Wickham Grolemund well Tidy Modeling R (Kuhn Silge 2021) Kuhn Silge.responsible reading relevant chapters texts. texts good & readable, use . make sure coming class also reading materials associated activities.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims populationWhat content Math 158?\nMath 158 course statistical linear models.goal Math 158 understand modeling linear statistical relationships explanatory / predictor (X) variables response (Y) variables.models grow sophistication semester including multiple linear regression, interaction terms, ridge regression, Lasso, smoothing.Throughout semester, continue talk good modeling practices, ideas extend beyond linear models types inference prediction.think carefully inferential modeling makes sense predictive modeling makes sense. neither type analysis done!math class go quickly, little calculus. , however, learn linear algebra.necessarily use linear models determine causation (need experimental design course discuss issues observational study vs. experiment). E.g., (1) thermostat versus actual temperature, (2) ice cream sales versus boating accidents.take Math 158?\nLinear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.prerequisites Math 158?\nLinear Models requires strong background statistics well algorithmic thinking. formal prerequisite introductory statistics course, AP Statistics, may find working hard first weeks class catch . taken lot mathematics, parts course come easily . However, mathematics degree substitute introductory statistics, taken introductory statistics, majority course work intuitive . must taken prior statistics course pre-requisite Math 158; computer science course helpful.Many derivations much notation course come linear algebra. taken linear algebra enormously helpful class, cover enough notation need linear algebra.worth noting concepts probability theory represent majority mathematical tools used statistics / modeling, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.overlap classes?\nStatistical Linear Models overlaps Econometrics (Econ 167) Applied Econometrics (107). Econometrics focuses probability theory matrix algebra (mathematics) lead derivation linear models. Applied Econometrics focuses tools analysis. Statistical Linear Models focuses use model, assumptions made, conclusions appropriate given results. Additionally, later topics Linear Models typically covered Econometrics.take Math 158?\nprerequisite Linear Models introduction statistics, course moves quickly covers tremendous amount material. ideally suited first year student coming straight AP Statistics. Instead, student focus taking mathematics, CS, interdisciplinary science, statistics courses. students taking Linear Models sophomores juniors.workload Math 158?\none homework assignment per week, two -class midterm exams, two take-home midterm exams, final end semester project. Many students report working 8-10 hours per week outside class.software use? real world applications? mathematics? CS?\nwork done R (using RStudio front end, called integrated development environment, IDE). need either download R RStudio (free) onto computer use Pomona’s server. assignments posted private repositories GitHub. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.assignments turned using R Markdown compiled pdf + pushed GitHub\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Linear Models ubiquitous incredibly powerful. Indeed, often times linear models appropriate (e.g., violating technical conditions) yet end giving almost identical solutions models appropriate. solid understanding linear model framework, however, requires strong foundation theory goes inferential thinking well predictive modeling. review ideas inference introductory statistics.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]","code":""},{"path":"intro.html","id":"simple-linear-regression","chapter":"1 Introduction","heading":"1.2.2 Simple Linear Regression","text":"simplest case, study first, suppose two variables. call one explanatory variable, response variable.Explanatory / Predictor variable: also known independent variable, numeric variable often known advance variable, thought possibly influence value.Response / outcome variable: also known dependent variable, also numeric, thought function predictor variable. n.b., don’t use word “dependent” don’t want send message ’ve measured anything causal model.goal ascertain relationship two. observe sample population interest, \\((x_i,y_i), =1,\\dots,n\\). \\(x_i\\) predictor, \\(y_i\\) response, \\(n\\) sample size.observe sample, actual population , best can hope estimate relationship two variables. resulting estimates give us idea relationship actually population. However, random quantities, depend random sample. , exact. theory hypothesis testing needed\ndetermine much can actually say population quantities, called parameters.Parameter: quantity describes population. Examples population mean, population standard deviation, say relationship two variables polynomial, coefficients function parameters.","code":""},{"path":"intro.html","id":"hypothesis-testing","chapter":"1 Introduction","heading":"1.2.3 Hypothesis Testing","text":"set : population big observe. ’d like know value specific parameters, say instance mean population. can’t however, calculate parameters directly.\nInstead, observe sample random population. Based sample, estimate parameters. However, estimated values exact. need technique uses estimated model say something population model.Null Hypothesis: Denoted \\(H_0\\), null hypothesis usually set believed unless evidence presented otherwise. specific, specifying parameter equal specific value.\n\\(H_0\\) true, theory tells us exactly estimate behaves.Alternative Hypothesis: Denoted \\(H_a\\), alternative hypothesis usually wish show true. general \\(H_0\\), usually form parameter somehow equal value used \\(H_0\\), without specifying exactly think value . result, don’t know estimate behaves, depends value parameter.","code":""},{"path":"intro.html","id":"what-really-is-an-alternative-hypothesis","chapter":"1 Introduction","heading":"What really is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEYIn video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?Since procedure potential wrong, search one makes bad errors infrequently. two types errors can made.Type error: Rejecting \\(H_0\\) \\(H_0\\) actually true. Usually considered worst error possible, thus find procedure makes type error small probability. probability denoted \\(\\alpha\\).Type II error: rejecting \\(H_0\\) \\(H_a\\) actually true. small type II error secondary concern (controlling type error). probability type II error denoted \\(\\beta\\). \\(1-\\beta\\)\nknown power.reality, true reason choose test small value \\(\\alpha\\) value know calculate. \\(\\beta\\) power possible calculate \\(H_a\\) doesn’t tell us value parameter .way hypothesis test carried via p-value, essentially tells us unusual observed data comparison \\(H_0\\). estimates consistent expected \\(H_0\\) true, conclusion \\(H_0\\) false: explanation data strange. definition p-value little tricky.p-value: probability, \\(H_0\\) true, observing data contradictory \\(H_0\\) repeat experiment .p-value .01, means data showed something happens 1 time 100 \\(H_0\\) true. Considering particular set data observed, reasonable conclusion \\(H_0\\) must \ntrue. rule : reject \\(H_0\\) p-value \\(< \\alpha\\). resulting test type error probability \\(\\alpha\\), value get specify. \\(\\alpha\\) often set .05.","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3 Reflection Questions","text":"difference sample population?experimental design issues influence conclusions?type error, type II error, power?p-value (careful, p-value probability \\(H_0\\) true!!!)?regression line ?linear regression always appropriate strategy?properties good fitting line ?line appropriately interpreted?","code":""},{"path":"intro.html","id":"r-reproduciblity","chapter":"1 Introduction","heading":"1.4 R: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.4.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.4.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.4.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"tools-git-github","chapter":"1 Introduction","heading":"Tools: Git & GitHub","text":"must submit assignments via GitHubFollow Jenny Bryan’s advice get set-: http://happygitwithr.com/Class specific instructions https://m158-comp-stats.netlify.app/github.htmlAdmittedly, steep learning curve Git. However, among tools likely use future endeavors, spending little time focusing concepts now may pay big time future. Beyond practicing working http://happygitwithr.com/, may want read little bit Git behind scenes. reference: Learn git concepts, commands good accessible.","code":""},{"path":"intro.html","id":"tools-a-github-merge-conflict-demo","chapter":"1 Introduction","heading":"Tools: a GitHub merge conflict (demo)","text":"GitHub (web) edit README document Commit message describing ., RStudio also edit README document different change.\nCommit changes\nTry push \\(\\rightarrow\\) ’ll get error!\nTry pulling\nResolve merge conflict commit push\nCommit changesTry push \\(\\rightarrow\\) ’ll get error!Try pullingResolve merge conflict commit pushAs work teams run merge conflicts, learning resolve properly important.\nFigure 1.7: https://xkcd.com/1597/\n","code":""},{"path":"intro.html","id":"steps-for-weekly-homework","chapter":"1 Introduction","heading":"Steps for weekly homework","text":"get link new assignment (clicking link create new private repo)Use R (within R Studio)\nNew Project, version control, Git\nClone repo using SSH\nNew Project, version control, GitClone repo using SSHIf exists, rename Rmd file ma158-hw#-lname-fname.RmdDo assignment\ncommit push every problem\ncommit push every problemAll necessary files must folder (e.g., data)","code":""},{"path":"wrang.html","id":"wrang","chapter":"2 Data Wrangling","heading":"2 Data Wrangling","text":"data visualization, data wrangling fundamental part able accurately, reproducibly, efficiently work data. approach taken following chapter based philosophy tidy data takes many precepts database theory. done much work SQL, functionality approach tidy data feel familiar. adept data wrangling, effective data analysis.Information want, data ’ve got. (Kaplan 2015)Embrace ways get help!cheat sheets: https://www.rstudio.com/resources/cheatsheets/tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/pivoting: https://tidyr.tidyverse.org/articles/pivot.htmlgoogle need include R tidy tidyverse","code":""},{"path":"wrang.html","id":"datastruc","chapter":"2 Data Wrangling","heading":"2.1 Structure of Data","text":"plotting, analyses, model building, etc., ’s important data structured particular way. Hadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy Data: rows (cases/observational units) columns (variables). key every row case *every} column variable. exceptions.Creating tidy data trivial. work objects (often data tables), functions, arguments (often variables).Active Duty data tidy! cases? data tidy? might data look like tidy form? Suppose case “individual armed forces.” variables use capture information following table?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794Problem: totals different sheetsBetter R: longer format columns - grade, gender, status, service, count (case still total pay grade)Case individual (?): grade, gender, status, service (count row counting)","code":""},{"path":"wrang.html","id":"building-tidy-data","chapter":"2 Data Wrangling","heading":"2.1.1 Building Tidy Data","text":"Within R (really within type computing language, Python, SQL, Java, etc.), need understand build data using patterns language. things consider:object_name = function_name(arguments) way using function create new object.object_name = data_table %>% function_name(arguments) uses chaining syntax extension ideas functions. chaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.\n* R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.\n* pipe syntax read , %>%.","code":"object_name = data_table %>%\nfunction_name(arguments) %>% \nfunction_name(arguments)"},{"path":"wrang.html","id":"examples-of-chaining","chapter":"2 Data Wrangling","heading":"2.1.2 Examples of Chaining","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)","code":""},{"path":"wrang.html","id":"little-bunny-foo-foo","chapter":"2 Data Wrangling","heading":"2.1.2.1 Little Bunny Foo Foo","text":"Hadley Wickham, think tidy data.Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThe nursery rhyme created series steps output step saved object along way.Another approach concatenate functions one output.even worse, one line:Instead, code can written using pipe order function evaluated:babynames year, US Social Security Administration publishes list popular names given babies. 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys.babynames data table babynames package comes Social Security Administration’s listing names givens babies year, number babies sex given name. (names 5 babies published SSA.)","code":"foo_foo <- little_bunny()\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)bop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))foo_foo %>%\n   hop(through = forest) %>%\n       scoop(up = field_mice) %>%\n           bop(on = head)"},{"path":"wrang.html","id":"data-verbs-on-single-data-frames","chapter":"2 Data Wrangling","heading":"2.1.3 Data Verbs (on single data frames)","text":"Super important resource: RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). use R package dplyr much data wrangling. list verbs helpful wrangling many different types data. See Data Wrangling cheat sheet RStudio additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfsample_n() take random row(s)sample_n() take random row(s)head() grab first rowshead() grab first rowstail() grab last rowstail() grab last rowsfilter() removes unwanted casesfilter() removes unwanted casesarrange() reorders casesarrange() reorders casesselect() removes unwanted variables (rename() )select() removes unwanted variables (rename() )distinct() returns unique values tabledistinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)mutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nsummarize() collapses data frame single row. functions used within summarize() include:min(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]","code":""},{"path":"wrang.html","id":"r-examples-basic-verbs","chapter":"2 Data Wrangling","heading":"2.2 R examples, basic verbs","text":"","code":""},{"path":"wrang.html","id":"datasets","chapter":"2 Data Wrangling","heading":"2.2.1 Datasets","text":"starwars dplyr , although originally SWAPI, Star Wars API, http://swapi.co/.NHANES ?NHANES: NHANES survey data collected US National Center Health Statistics (NCHS) conducted series health nutrition surveys since early 1960’s. Since 1999 approximately 5,000 individuals ages interviewed homes every year complete health examination component survey. health examination conducted mobile examination center (MEC).babynames year, US Social Security Administration publishes list popular names given babies. 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys. (names 5 babies published SSA.)","code":""},{"path":"wrang.html","id":"examples-of-chaining-1","chapter":"2 Data Wrangling","heading":"2.2.2 Examples of Chaining","text":"","code":"\nlibrary(babynames)\nbabynames %>% nrow()## [1] 1924665\nbabynames %>% names()## [1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\nbabynames %>% glimpse()## Rows: 1,924,665\n## Columns: 5\n## $ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n## $ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n## $ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n## $ n    <int> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n## $ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\nbabynames %>% head()## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\nbabynames %>% tail()## # A tibble: 6 × 5\n##    year sex   name       n       prop\n##   <dbl> <chr> <chr>  <int>      <dbl>\n## 1  2017 M     Zyhier     5 0.00000255\n## 2  2017 M     Zykai      5 0.00000255\n## 3  2017 M     Zykeem     5 0.00000255\n## 4  2017 M     Zylin      5 0.00000255\n## 5  2017 M     Zylis      5 0.00000255\n## 6  2017 M     Zyrie      5 0.00000255\nbabynames %>% sample_n(size=5)## # A tibble: 5 × 5\n##    year sex   name         n      prop\n##   <dbl> <chr> <chr>    <int>     <dbl>\n## 1  1946 M     Owen       392 0.000238 \n## 2  1953 M     Abelardo    34 0.0000170\n## 3  1996 F     Nicki       50 0.0000261\n## 4  1914 F     Sofia       38 0.0000477\n## 5  1970 M     Lorenza     45 0.0000236\nbabynames %>% mosaic::favstats(n ~ sex, data = .)##   sex min Q1 median Q3   max     mean       sd       n missing\n## 1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n## 2   M   5  7     12 33 94756 223.4940 1932.338  786372       0"},{"path":"wrang.html","id":"data-verbs","chapter":"2 Data Wrangling","heading":"2.2.3 Data Verbs","text":"Taken dplyr tutorial: http://dplyr.tidyverse.org/","code":""},{"path":"wrang.html","id":"starwars","chapter":"2 Data Wrangling","heading":"2.2.3.1 Starwars","text":"","code":"\nlibrary(dplyr)\n\nstarwars %>% dim()## [1] 87 14\nstarwars %>% names()##  [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n##  [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n## [11] \"species\"    \"films\"      \"vehicles\"   \"starships\"\nstarwars %>% head()## # A tibble: 6 × 14\n##   name     height  mass hair_color  skin_color eye_color birth_year sex   gender\n##   <chr>     <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Luke Sk…    172    77 blond       fair       blue            19   male  mascu…\n## 2 C-3PO       167    75 <NA>        gold       yellow         112   none  mascu…\n## 3 R2-D2        96    32 <NA>        white, bl… red             33   none  mascu…\n## 4 Darth V…    202   136 none        white      yellow          41.9 male  mascu…\n## 5 Leia Or…    150    49 brown       light      brown           19   fema… femin…\n## 6 Owen La…    178   120 brown, grey light      blue            52   male  mascu…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       8\n## 2 masculine  15 75     80 88.0 1358 106.14694 184.972677 49      17\nstarwars %>% \n  dplyr::filter(species == \"Droid\")## # A tibble: 6 × 14\n##   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n##   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n## 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculi…\n## 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculi…\n## 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculi…\n## 4 IG-88     200   140 none       metal       red               15 none  masculi…\n## 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n## 6 BB8        NA    NA none       none        black             NA none  masculi…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  dplyr::filter(species != \"Droid\") %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n## 2 masculine  15 77     80 88.0 1358 109.38222 192.397084 45      16\nstarwars %>% \n  dplyr::select(name, ends_with(\"color\"))## # A tibble: 87 × 4\n##    name               hair_color    skin_color  eye_color\n##    <chr>              <chr>         <chr>       <chr>    \n##  1 Luke Skywalker     blond         fair        blue     \n##  2 C-3PO              <NA>          gold        yellow   \n##  3 R2-D2              <NA>          white, blue red      \n##  4 Darth Vader        none          white       yellow   \n##  5 Leia Organa        brown         light       brown    \n##  6 Owen Lars          brown, grey   light       blue     \n##  7 Beru Whitesun lars brown         light       blue     \n##  8 R5-D4              <NA>          white, red  red      \n##  9 Biggs Darklighter  black         light       brown    \n## 10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n## # … with 77 more rows\nstarwars %>% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %>%\n  dplyr::select(name:mass, bmi)## # A tibble: 87 × 4\n##    name               height  mass   bmi\n##    <chr>               <int> <dbl> <dbl>\n##  1 Luke Skywalker        172    77  26.0\n##  2 C-3PO                 167    75  26.9\n##  3 R2-D2                  96    32  34.7\n##  4 Darth Vader           202   136  33.3\n##  5 Leia Organa           150    49  21.8\n##  6 Owen Lars             178   120  37.9\n##  7 Beru Whitesun lars    165    75  27.5\n##  8 R5-D4                  97    32  34.0\n##  9 Biggs Darklighter     183    84  25.1\n## 10 Obi-Wan Kenobi        182    77  23.2\n## # … with 77 more rows\nstarwars %>% \n  dplyr::arrange(desc(mass))## # A tibble: 87 × 14\n##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Jabba …    175  1358 <NA>        green-tan… orange         600   herm… mascu…\n##  2 Grievo…    216   159 none        brown, wh… green, y…       NA   male  mascu…\n##  3 IG-88      200   140 none        metal      red             15   none  mascu…\n##  4 Darth …    202   136 none        white      yellow          41.9 male  mascu…\n##  5 Tarfful    234   136 brown       brown      blue            NA   male  mascu…\n##  6 Owen L…    178   120 brown, grey light      blue            52   male  mascu…\n##  7 Bossk      190   113 none        green      red             53   male  mascu…\n##  8 Chewba…    228   112 brown       unknown    blue           200   male  mascu…\n##  9 Jek To…    180   110 brown       fair       blue            NA   male  mascu…\n## 10 Dexter…    198   102 none        brown      yellow          NA   male  mascu…\n## # … with 77 more rows, and 5 more variables: homeworld <chr>, species <chr>,\n## #   films <list>, vehicles <list>, starships <list>\nstarwars %>%\n  dplyr::group_by(species) %>%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  dplyr::filter(num > 1)## # A tibble: 9 × 3\n##   species    num  mass\n##   <chr>    <int> <dbl>\n## 1 Droid        6  69.8\n## 2 Gungan       3  74  \n## 3 Human       35  82.8\n## 4 Kaminoan     2  88  \n## 5 Mirialan     2  53.1\n## 6 Twi'lek      2  55  \n## 7 Wookiee      2 124  \n## 8 Zabrak       2  80  \n## 9 <NA>         4  48"},{"path":"wrang.html","id":"nhanes","chapter":"2 Data Wrangling","heading":"2.2.3.2 NHANES","text":"","code":"\nrequire(NHANES)\nnames(NHANES)##  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n##  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n##  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n## [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n## [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n## [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n## [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n## [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n## [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n## [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n## [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n## [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n## [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n## [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n## [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n## [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n## [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n## [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n## [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n# find the sleep variables\nNHANESsleep <- NHANES %>% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"\ndim(NHANESsleep)## [1] 10000    11\n# subset for college students\nNHANESsleep <- NHANESsleep %>% filter(Age %in% c(18:22)) %>% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"\ndim(NHANESsleep)## [1] 655  12\nNHANESsleep %>% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) "},{"path":"wrang.html","id":"summarize-and-group_by","chapter":"2 Data Wrangling","heading":"2.2.4 summarize and group_by","text":"","code":"\n# number of people (cases) in NHANES\nNHANES %>% summarize(n())## # A tibble: 1 × 1\n##   `n()`\n##   <int>\n## 1 10000\n# total weight of all the people in NHANES (silly)\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(sum(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `sum(Weightlb, na.rm = TRUE)`\n##                           <dbl>\n## 1                      1549419.\n# mean weight of all the people in NHANES\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `mean(Weightlb, na.rm = TRUE)`\n##                            <dbl>\n## 1                           156.\n# repeat the above but for groups\n\n# males versus females\nNHANES %>% group_by(Gender) %>% summarize(n())## # A tibble: 2 × 2\n##   Gender `n()`\n##   <fct>  <int>\n## 1 female  5020\n## 2 male    4980\nNHANES %>% group_by(Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 2 × 2\n##   Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                           <dbl>\n## 1 female                           146.\n## 2 male                             167.\n# smokers and non-smokers\nNHANES %>% group_by(SmokeNow) %>% summarize(n())## # A tibble: 3 × 2\n##   SmokeNow `n()`\n##   <fct>    <int>\n## 1 No        1745\n## 2 Yes       1466\n## 3 <NA>      6789\nNHANES %>% group_by(SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                 186.\n## 2 Yes                                177.\n## 3 <NA>                               144.\n# people with and without diabetes\nNHANES %>% group_by(Diabetes) %>% summarize(n())## # A tibble: 3 × 2\n##   Diabetes `n()`\n##   <fct>    <int>\n## 1 No        9098\n## 2 Yes        760\n## 3 <NA>       142\nNHANES %>% group_by(Diabetes) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   Diabetes `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                155. \n## 2 Yes                               202. \n## 3 <NA>                               21.6\n# break down the smokers versus non-smokers further, by sex\nNHANES %>% group_by(SmokeNow, Gender) %>% summarize(n())## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `n()`\n##   <fct>    <fct>  <int>\n## 1 No       female   764\n## 2 No       male     981\n## 3 Yes      female   638\n## 4 Yes      male     828\n## 5 <NA>     female  3618\n## 6 <NA>     male    3171\nNHANES %>% group_by(SmokeNow, Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                           <dbl>\n## 1 No       female                           167.\n## 2 No       male                             201.\n## 3 Yes      female                           167.\n## 4 Yes      male                             185.\n## 5 <NA>     female                           138.\n## 6 <NA>     male                             151.\n# break down the people with diabetes further, by smoking\nNHANES %>% group_by(Diabetes, SmokeNow) %>% summarize(n())## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `n()`\n##   <fct>    <fct>    <int>\n## 1 No       No        1476\n## 2 No       Yes       1360\n## 3 No       <NA>      6262\n## 4 Yes      No         267\n## 5 Yes      Yes        106\n## 6 Yes      <NA>       387\n## 7 <NA>     No           2\n## 8 <NA>     <NA>       140\nNHANES %>% group_by(Diabetes, SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                             <dbl>\n## 1 No       No                                183. \n## 2 No       Yes                               175. \n## 3 No       <NA>                              143. \n## 4 Yes      No                                204. \n## 5 Yes      Yes                               204. \n## 6 Yes      <NA>                              199. \n## 7 <NA>     No                                193. \n## 8 <NA>     <NA>                               19.1"},{"path":"wrang.html","id":"babynames","chapter":"2 Data Wrangling","heading":"2.2.5 babynames","text":"","code":"\nbabynames %>% group_by(sex) %>%\n  summarize(total=sum(n))## # A tibble: 2 × 2\n##   sex       total\n##   <chr>     <int>\n## 1 F     172371079\n## 2 M     175749438\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% head()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  1880 F            942\n## 2  1880 M           1058\n## 3  1881 F            938\n## 4  1881 M            997\n## 5  1882 F           1028\n## 6  1882 M           1099\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% tail()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  2015 F          19074\n## 2  2015 M          14024\n## 3  2016 F          18817\n## 4  2016 M          14162\n## 5  2017 F          18309\n## 6  2017 M          14160\nbabysamp <- babynames %>% sample_n(size=50)\nbabysamp %>% select(year) %>% distinct() %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    1    1    1    1    1\nbabysamp %>% distinct() %>% select(year) %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    2    2    1    1    3    1    1    1    1    1    2    2    1    2    2 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    3    1    2    1    1\nFrances <- babynames %>%\n  filter(name== \"Frances\") %>%\n  group_by(year, sex) %>%\n  summarize(yrTot = sum(n))\n\nFrances %>% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")"},{"path":"wrang.html","id":"highverb","chapter":"2 Data Wrangling","heading":"2.3 Higher Level Data Verbs","text":"complicated verbs may important sophisticated analyses. See RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.pivot_longer makes many columns 2 columns: pivot_longer(data, cols,  names_to = , value_to = )pivot_wider makes one column multiple columns: pivot_wider(data, names_from = , values_from = )left_join returns rows left table, rows matching keys right table.inner_join returns rows left table matching keys right table (.e., matching rows sets).full_join returns rows tables, join records left matching keys right table.Good practice: always specify argument joining data frames.ever need understand join right join , try find image lay function . found one quite good taken Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti","code":""},{"path":"wrang.html","id":"r-examples-higher-level-verbs","chapter":"2 Data Wrangling","heading":"2.4 R examples, higher level verbs","text":"tidyr 1.0.0 just released! new release means need update tidyr. know latest version following command works console (window ):familiar spread gather, acquaint pivot_longer() pivot_wider(). idea go wide dataframes long dataframes vice versa.","code":"?tidyr::pivot_longer"},{"path":"wrang.html","id":"pivot_longer","chapter":"2 Data Wrangling","heading":"2.4.1 pivot_longer()","text":"pivot military pay grade become longer?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#\ngid=1811988794Does graph tell us right? done wrong…?","code":"\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\nglimpse(navy_gs)## Rows: 38\n## Columns: 17\n## $ ...1                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ `Active Duty Family` <chr> NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n## $ ...3                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n## $ ...4                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n## $ ...5                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n## $ ...6                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n## $ ...7                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n## $ ...8                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n## $ ...9                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n## $ ...10                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n## $ ...11                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n## $ ...12                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n## $ ...13                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n## $ ...14                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n## $ ...15                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n## $ ...16                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n## $ ...17                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)## Rows: 30\n## Columns: 16\n## $ pay.grade       <chr> \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n## $ male.sing.wo    <dbl> 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n## $ female.sing.wo  <dbl> 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n## $ tot.sing.wo     <dbl> 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n## $ male.sing.w     <dbl> 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n## $ female.sing.w   <dbl> 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n## $ tot.sing.w      <dbl> 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n## $ male.joint.NA   <dbl> 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n## $ female.joint.NA <dbl> 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n## $ tot.joint.NA    <dbl> 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n## $ male.civ.NA     <dbl> 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n## $ female.civ.NA   <dbl> 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n## $ tot.civ.NA      <dbl> 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n## $ male.tot.NA     <dbl> 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n## $ female.tot.NA   <dbl> 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n## $ tot.tot.NA      <dbl> 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n# get rid of total columns & rows:\n\nnavyWR = navy %>% select(-contains(\"tot\")) %>%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %>%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %>%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %>% head()## # A tibble: 6 × 5\n##   pay.grade sex    marital kids  numPeople\n##   <chr>     <chr>  <chr>   <chr>     <dbl>\n## 1 E-1       male   sing    wo        31229\n## 2 E-1       female sing    wo         5717\n## 3 E-1       male   sing    w           563\n## 4 E-1       female sing    w           122\n## 5 E-1       male   joint   NA          139\n## 6 E-1       female joint   NA          141\nnavyWR %>% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))"},{"path":"wrang.html","id":"pivot_wider","chapter":"2 Data Wrangling","heading":"2.4.2 pivot_wider","text":"","code":"\nlibrary(babynames)\nbabynames %>% dplyr::select(-prop) %>%\n   tidyr::pivot_wider(names_from = sex, values_from = n) ## # A tibble: 1,756,284 × 4\n##     year name          F     M\n##    <dbl> <chr>     <int> <int>\n##  1  1880 Mary       7065    27\n##  2  1880 Anna       2604    12\n##  3  1880 Emma       2003    10\n##  4  1880 Elizabeth  1939     9\n##  5  1880 Minnie     1746     9\n##  6  1880 Margaret   1578    NA\n##  7  1880 Ida        1472     8\n##  8  1880 Alice      1414    NA\n##  9  1880 Bertha     1320    NA\n## 10  1880 Sarah      1288    NA\n## # … with 1,756,274 more rows\nbabynames %>% \n  select(-prop) %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(year), desc(M))## # A tibble: 168,381 × 4\n##     year name         F     M\n##    <dbl> <chr>    <int> <int>\n##  1  2017 Liam        36 18728\n##  2  2017 Noah       170 18326\n##  3  2017 William     18 14904\n##  4  2017 James       77 14232\n##  5  2017 Logan     1103 13974\n##  6  2017 Benjamin     8 13733\n##  7  2017 Mason       58 13502\n##  8  2017 Elijah      26 13268\n##  9  2017 Oliver      15 13141\n## 10  2017 Jacob       16 13106\n## # … with 168,371 more rows\nbabynames %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(prop))## # A tibble: 12 × 5\n##     year name            prop     F     M\n##    <dbl> <chr>          <dbl> <int> <int>\n##  1  1986 Marquette 0.0000130     24    25\n##  2  1996 Dariel    0.0000115     22    23\n##  3  2014 Laramie   0.0000108     21    22\n##  4  1939 Earnie    0.00000882    10    10\n##  5  1939 Vertis    0.00000882    10    10\n##  6  1921 Vernis    0.00000703     9     8\n##  7  1939 Alvia     0.00000529     6     6\n##  8  1939 Eudell    0.00000529     6     6\n##  9  1939 Ladell    0.00000529     6     6\n## 10  1939 Lory      0.00000529     6     6\n## 11  1939 Maitland  0.00000529     6     6\n## 12  1939 Delaney   0.00000441     5     5"},{"path":"wrang.html","id":"join-use-join-to-merge-two-datasets","chapter":"2 Data Wrangling","heading":"2.4.3 join (use join to merge two datasets)","text":"","code":""},{"path":"wrang.html","id":"first-get-the-data-gapminder","chapter":"2 Data Wrangling","heading":"2.4.3.1 First get the data (GapMinder)","text":"following datasets come GapMinder. first represents country, year, female literacy rate. second represents country, year, GDP (fixed 2000 US$).","code":"\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %>% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %>%\n  filter(!is.na(litRateF))\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %>% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %>%\n  filter(!is.na(gdp))\nhead(litF)## # A tibble: 6 × 3\n##   country     year  litRateF\n##   <chr>       <chr>    <dbl>\n## 1 Afghanistan 1979      4.99\n## 2 Afghanistan 2011     13   \n## 3 Albania     2001     98.3 \n## 4 Albania     2008     94.7 \n## 5 Albania     2011     95.7 \n## 6 Algeria     1987     35.8\nhead(GDP)## # A tibble: 6 × 3\n##   country year    gdp\n##   <chr>   <chr> <dbl>\n## 1 Albania 1980  1061.\n## 2 Albania 1981  1100.\n## 3 Albania 1982  1111.\n## 4 Albania 1983  1101.\n## 5 Albania 1984  1065.\n## 6 Albania 1985  1060.\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)## [1] 571   4\nsum(is.na(litGDPleft$gdp))## [1] 66\nhead(litGDPleft)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902.\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)## [1] 7988    4\nsum(is.na(litGDPright$gdp))## [1] 0\nhead(litGDPright)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)## [1] 505   4\nsum(is.na(litGDPinner$gdp))## [1] 0\nhead(litGDPinner)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)## [1] 8054    4\nsum(is.na(litGDPfull$gdp))## [1] 66\nhead(litGDPfull)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902."},{"path":"wrang.html","id":"lubridate","chapter":"2 Data Wrangling","heading":"2.4.4 lubridate","text":"lubridate another R package meant data wrangling (Grolemund Wickham 2011). particular, lubridate makes easy work days, times, dates. base idea start dates ymd (year month day) format transform information whatever want. linked table original paper provides many basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.Example https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html","code":""},{"path":"wrang.html","id":"if-anyone-drove-a-time-machine-they-would-crash","chapter":"2 Data Wrangling","heading":"2.4.4.1 If anyone drove a time machine, they would crash","text":"length months years change often arithmetic can unintuitive. Consider simple operation, January 31st + one month. answer :February 31st (doesn’t exist)March 4th (31 days January 31), orFebruary 28th (assuming leap year)basic property arithmetic + b - b = . solution 1 obeys mathematical property, invalid date. Wickham wants make lubridate consistent possible invoking following rule: adding subtracting month year creates invalid date, lubridate return NA.thought solution 2 3 useful, problem. can still get results clever arithmetic, using special %m+% %m-% operators. %m+% %m-% automatically roll dates back last day month, necessary.","code":""},{"path":"wrang.html","id":"r-examples-lubridate","chapter":"2 Data Wrangling","heading":"2.4.4.2 R examples, lubridate()","text":"","code":""},{"path":"wrang.html","id":"some-basics-in-lubridate","chapter":"2 Data Wrangling","heading":"Some basics in lubridate","text":"","code":"\nrequire(lubridate)\nrightnow <- now()\n\nday(rightnow)## [1] 11\nweek(rightnow)## [1] 2\nmonth(rightnow, label=FALSE)## [1] 1\nmonth(rightnow, label=TRUE)## [1] Jan\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nyear(rightnow)## [1] 2022\nminute(rightnow)## [1] 55\nhour(rightnow)## [1] 15\nyday(rightnow)## [1] 11\nmday(rightnow)## [1] 11\nwday(rightnow, label=FALSE)## [1] 3\nwday(rightnow, label=TRUE)## [1] Tue\n## Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"},{"path":"wrang.html","id":"but-how-do-i-create-a-date-object","chapter":"2 Data Wrangling","heading":"But how do I create a date object?","text":"","code":"\njan31 <- ymd(\"2021-01-31\")\njan31 + months(0:11)##  [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n##  [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n## [11] NA           \"2021-12-31\"\nfloor_date(jan31, \"month\") + months(0:11) + days(31)##  [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n##  [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n## [11] \"2021-12-02\" \"2022-01-01\"\njan31 + months(0:11) + days(31)##  [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n##  [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n## [11] NA           \"2022-01-31\"\njan31 %m+% months(0:11)##  [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n##  [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n## [11] \"2021-11-30\" \"2021-12-31\""},{"path":"wrang.html","id":"nyc-flights","chapter":"2 Data Wrangling","heading":"NYC flights","text":"","code":"\nlibrary(nycflights13)\nnames(flights)##  [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n##  [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n##  [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n## [13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n## [17] \"hour\"           \"minute\"         \"time_hour\"\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)## # A tibble: 6 × 22\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      517            515         2      830            819\n## 2  2013     1     1      533            529         4      850            830\n## 3  2013     1     1      542            540         2      923            850\n## 4  2013     1     1      544            545        -1     1004           1022\n## 5  2013     1     1      554            600        -6      812            837\n## 6  2013     1     1      554            558        -4      740            728\n## # … with 14 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, ymdday <date>, weekdy <ord>,\n## #   whichweek <dbl>\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %>% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %>%  \n   head()## # A tibble: 6 × 9\n##    year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n##   <int> <int> <int> <date>     <ord>      <dbl>    <int>    <int>    <dbl>\n## 1  2013     1     1 2013-01-01 Tue            1      517      830      227\n## 2  2013     1     1 2013-01-01 Tue            1      533      850      227\n## 3  2013     1     1 2013-01-01 Tue            1      542      923      160\n## 4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n## 5  2013     1     1 2013-01-01 Tue            1      554      812      116\n## 6  2013     1     1 2013-01-01 Tue            1      554      740      150"},{"path":"wrang.html","id":"purrr-for-functional-programming","chapter":"2 Data Wrangling","heading":"2.5 purrr for functional programming","text":"see R package purrr greater detail go, now, let’s get hint works.going focus map family functions get us started. Lots good purrr functions like pluck() accumulate().Much taken tutorial Rebecca Barter.map functions named output produce. example:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vectorNote first argument always data object second object always function want iteratively apply element input object.input map function always either vector (like column), list (can non-rectangular), dataframe (like rectangle).list way hold things might different shape:Consider following function:can map() add_ten() function across vector. Note output list (default).use different type input? default behavior still return list!want different type output? use different map() function, map_df(), example.Shorthand lets us get away pre-defining function (useful). Use tilde ~ indicate function:Mostly, tilde used functions already know:","code":"\na_list <- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)## $a_number\n## [1] 5\n## \n## $a_vector\n## [1] \"a\" \"b\" \"c\"\n## \n## $a_dataframe\n##   a b             c\n## 1 1 q       bananas\n## 2 2 b           are\n## 3 3 z so very great\nadd_ten <- function(x) {\n  return(x + 10)\n  }\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 15\n## \n## [[3]]\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map(add_ten)## $a\n## [1] 12\n## \n## $b\n## [1] 15\n## \n## $c\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(add_ten)## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(~{.x + 10})## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %>%\n  split(.$species) %>%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map_df(tidy)  # map(tidy)## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n## 2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n## 3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n## 4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n## 5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\npenguins %>%\n  group_by(species) %>%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map(tidy)  # map_df(tidy)## [[1]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 0.00948      \n## 2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n## \n## [[2]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -3037.     997.       -3.05 0.00333      \n## 2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n## \n## [[3]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 2 flipper_length_mm     54.6      5.03     10.9  1.33e-19"},{"path":"wrang.html","id":"reprex","chapter":"2 Data Wrangling","heading":"2.6 reprex()","text":"Help help youIn order create reproducible example …Step 1. Copy code onto clipboardStep 2. Type reprex() ConsoleStep 3. Look Viewer right. Copy Viewer output GitHub, Piazza, email, stackexchange, etc.places learn reprex includeA blog : https://teachdatascience.com/reprex/reprex vignette: https://reprex.tidyverse.org/index.htmlreprex dos donts: https://reprex.tidyverse.org/articles/reprex-dos--donts.htmlJenny Bryan webinar reprex: “Help help . Creating reproducible examples” https://resources.rstudio.com/webinars/help--help--creating-reproducible-examples-jenny-bryanSome advice: https://stackoverflow.com/help/minimal-reproducible-example","code":""},{"path":"wrang.html","id":"reprex-demo","chapter":"2 Data Wrangling","heading":"2.6.0.1 reprex demo","text":"multiple lines code:","code":"reprex(\n  jan31 + months(0:11) + days(31)\n)reprex({\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})reprex({\n  library(lubridate)\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})"},{"path":"viz.html","id":"viz","chapter":"3 Visualization","heading":"3 Visualization","text":"Data visualization integral understanding data models. Computational statistics data science sometimes focus models resulting predictions models. doubt structure format data key whether model appropriate good. good data analyst always spend lot time effort exploratory data analysis, much includes making many visualizations data possible.Depending introductory () statistics classes ’ve , instructor may focused less visualizations class. () may even said something like making visualizations incredibly important entire data analysis process. even buy perspective, don’t see good graphics analyses? Andrew Gelman (Gelman 2011) responds stating, “Good statistical graphics hard , much harder running regressions making tables.” goal create graphics visualizations convey statistical information.Nolan (Nolan Perrett 2016) describes three important ways graphics can used convey statistical information. “guiding principles” used way evaluating others’ figures well metric creating visualizations help statistical analysis.Make data stand outThe important idea find anything unusual data. patterns? Outliers? bounds variables? axes scaled? transformations warranted?Facilitate comparisonThe second item allows us consider research questions hand. important variables? emphasize ? variables plotted together? Can super-imposed? color, plotting character, size plot character help bring important relationships? aware plotting issues color blindness. http://colorbrewer2.org/Add informationPlots also add context comparison. Figure legends, axes scales, reference markers (e.g., line \\(y=x\\)) go long way toward helping reader understand message. Captions self-contained (assume user also read text) descriptive; summarize content figure conclusion related message want convey.Randy Pruim asks following question decide whether plot good: plot make comparisons interested …easily? andaccurately?Consider adding alt text allow screen readers parse image. DataViz Society/Nightingale way Amy Cesal article writing good alt text plots/graphs, Writing Alt Text Data Visualization.","code":""},{"path":"viz.html","id":"thoughts","chapter":"3 Visualization","heading":"3.1 Thoughts on Plotting","text":"","code":""},{"path":"viz.html","id":"advice","chapter":"3 Visualization","heading":"3.1.1 Advice","text":"Basic plotting\nAvoid graph elements interfere data\nUse visually prominent symbols\nAvoid -plotting (One way avoid plotting: Jitter values)\nDifferent values data may obscure \nInclude nearly data\nFill data region\nAvoid graph elements interfere dataUse visually prominent symbolsAvoid -plotting (One way avoid plotting: Jitter values)Different values data may obscure otherInclude nearly dataFill data regionEliminate superfluous material\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background images\nExtra tick marks grid lines\nUnnecessary text arrows\nDecimal places beyond measurement error level difference\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background imagesExtra tick marks grid linesUnnecessary text arrowsDecimal places beyond measurement error level differenceFacilitate Comparisons\nPut juxtaposed plots scale\nMake easy distinguish elements superposed plots (e.g. color)\nEmphasizes important difference\nComparison: volume, area, height (careful, volume can seem bigger mean )\nPut juxtaposed plots scaleMake easy distinguish elements superposed plots (e.g. color)Emphasizes important differenceComparison: volume, area, height (careful, volume can seem bigger mean )Choosing Scale (n.b., principles may go counter one another, use judgment.)\nKeep scales x y axes plots facilitate comparison\nZoom focus region contains bulk data\nKeep scale throughout plot (.e. don’t change mid-axis)\nOrigin need scale\nChoose scale improves resolution\nAvoid jiggling baseline\nKeep scales x y axes plots facilitate comparisonZoom focus region contains bulk dataKeep scale throughout plot (.e. don’t change mid-axis)Origin need scaleChoose scale improves resolutionAvoid jiggling baselineHow make plot information rich\nDescribe see caption\nAdd context reference markers (lines points) including text\nAdd legends labels\nUse color plotting symbols add information\nPlot thing different ways/scales\nReduce clutter\nDescribe see captionAdd context reference markers (lines points) including textAdd legends labelsUse color plotting symbols add informationPlot thing different ways/scalesReduce clutterCaptions \ncomprehensive\nSelf-contained\nDescribe graphed\nDraw attention important features\nDescribe conclusions drawn graph\ncomprehensiveSelf-containedDescribe graphedDraw attention important featuresDescribe conclusions drawn graphGood Plot Making Practice\nPut major conclusions graphical form\nProvide reference information\nProof read clarity consistency\nGraphing iterative process\nMultiplicity OK, .e. two plots variable may provide different messages\nMake plots data rich\nPut major conclusions graphical formProvide reference informationProof read clarity consistencyGraphing iterative processMultiplicity OK, .e. two plots variable may provide different messagesMake plots data richCreating statistical graphic iterative process discovery fine tuning. try model process creating visualizations course dedicating class time iterative creation plot. begin either plot screams correction, transform step--step, always thinking goal graph data rich presents clear vision important features data.","code":""},{"path":"viz.html","id":"fonts-matter","chapter":"3 Visualization","heading":"3.1.1.1 Fonts Matter","text":"RStudio::conf 2020, Glamour Graphics, Chase makes important points making good graphics matters. talk might summarized plot : fonts matter.","code":""},{"path":"viz.html","id":"deconstruct","chapter":"3 Visualization","heading":"3.2 Deconstructing a graph","text":"","code":""},{"path":"viz.html","id":"gg","chapter":"3 Visualization","heading":"3.2.1 The Grammar of Graphics (gg)","text":"Yau (2013) Wickham (2014) come taxonomy grammar thinking parts figure just like conceptualize parts body parts sentence.One great way thinking new process: longer necessary talk name graph (e.g., boxplot). Instead now think glyphs (geoms), can put whatever want plot. Note also transition leads passive consumer (need make plot XXX everyone else , just plug data) active participant (want data say? can put information onto graphic?)important questions can ask respect creating figures :want R ? (goal?)R need know?Yau (2013) gives us nine visual cues, Wickham (2014) translates language using ggplot2. (items Baumer, Kaplan, Horton (2021), chapter 2.)Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Coordinate System: rectangular, polar, geographic, etc.Coordinate System: rectangular, polar, geographic, etc.Scale: numeric (linear? logarithmic?), categorical (ordered?), timeScale: numeric (linear? logarithmic?), categorical (ordered?), timeContext: comparison (think back ideas Tufte)Context: comparison (think back ideas Tufte)","code":""},{"path":"viz.html","id":"order-matters","chapter":"3 Visualization","heading":"Order Matters","text":"","code":""},{"path":"viz.html","id":"cues-together","chapter":"3 Visualization","heading":"Cues Together","text":"","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-1","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-2","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"the-grammar-of-graphics-in-ggplot2","chapter":"3 Visualization","heading":"3.2.1.1 The grammar of graphics in ggplot2","text":"geom: geometric “shape” used display databar, point, line, ribbon, text, etc.aesthetic: attribute controlling geom displayed respect variablesx position, y position, color, fill, shape, size, etc.scale: adjust information aesthetic map onto plotparticular assignment colors, shapes, sizes, etc.; making axes continuous constrained particular range values.guide: helps user convert visual data back raw data (legends, axes)stat: transformation applied data geom gets itexample: histograms work binned data","code":""},{"path":"viz.html","id":"ggplot2","chapter":"3 Visualization","heading":"3.2.2 ggplot2","text":"ggplot2, aesthetic refers mapping variable information conveys plot. information plotting visualizing information given chapter 2 (Data visualization) Baumer, Kaplan, Horton (2021). Much data presentation represents births 1978 US: date, day year, number births.","code":""},{"path":"viz.html","id":"goals","chapter":"3 Visualization","heading":"Goals","text":"try dogive tour ggplot2give tour ggplot2explain think plots ggplot2 wayexplain think plots ggplot2 wayprepare/encourage learn laterprepare/encourage learn laterWhat can’t one sessionshow every bell whistleshow every bell whistlemake expert using ggplot2make expert using ggplot2","code":""},{"path":"viz.html","id":"getting-help","chapter":"3 Visualization","heading":"Getting help","text":"One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfLook end presentation. help options .Look end presentation. help options .","code":"\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)##         date births wday year month day_of_year day_of_month day_of_week\n## 1 1978-01-01   7701  Sun 1978     1           1            1           1\n## 2 1978-01-02   7527  Mon 1978     1           2            2           2\n## 3 1978-01-03   8825  Tue 1978     1           3            3           3"},{"path":"viz.html","id":"how-can-we-make-the-plot","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Two Questions:want R ? (goal?)want R ? (goal?)R need know?\ndata source: Births78\naesthetics:\ndate -> x\nbirths -> y\npoints (!)\n\nR need know?data source: Births78data source: Births78aesthetics:\ndate -> x\nbirths -> y\npoints (!)\naesthetics:date -> xbirths -> ypoints (!)Goal: scatterplot = plot points\nggplot() + geom_point()\nGoal: scatterplot = plot pointsggplot() + geom_point()R need know?\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\nR need know?data source: data = Births78data source: data = Births78aesthetics: aes(x = date, y = births)aesthetics: aes(x = date, y = births)","code":""},{"path":"viz.html","id":"how-can-we-make-the-plot-1","chapter":"3 Visualization","heading":"How can we make the plot?","text":"changed?new aesthetic: mapping color day week","code":""},{"path":"viz.html","id":"adding-day-of-week-to-the-data-set","chapter":"3 Visualization","heading":"Adding day of week to the data set","text":"wday() function lubridate package computes day week date.","code":"\nBirths78 <-  \n  Births78 %>% \n  mutate(wday = lubridate::wday(date, label=TRUE))\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-2","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now use lines instead dots","code":"\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-3","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now two layers, one points one \nlinesThe layers placed one top : points\nlines .layers placed one top : points\nlines .data aes specified ggplot() affect geomsdata aes specified ggplot() affect geoms","code":"\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"alternative-syntax","chapter":"3 Visualization","heading":"Alternative Syntax","text":"","code":"\nBirths78 %>% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"what-does-adding-the-color-argument-do","chapter":"3 Visualization","heading":"What does adding the color argument do?","text":"variable, mapped color aesthetic new variable one value (“navy”). dots get set color, ’s navy.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping","chapter":"3 Visualization","heading":"Setting vs. Mapping","text":"want set color navy dots, outside aesthetic, without dataset variable:Note color = \"navy\" now outside aesthetics list. ’s ggplot2 distinguishes mapping setting.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-4","chapter":"3 Visualization","heading":"How can we make the plot?","text":"ggplot() establishes default data aesthetics geoms, geom may change defaults.ggplot() establishes default data aesthetics geoms, geom may change defaults.good practice: put ggplot() things affect () layers; rest geom_blah()good practice: put ggplot() things affect () layers; rest geom_blah()","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping-again","chapter":"3 Visualization","heading":"Setting vs. Mapping (again)","text":"Information gets passed plot via:map variable information inside aes (aesthetic) commandmap variable information inside aes (aesthetic) commandset non-variable information outside aes (aesthetic) commandset non-variable information outside aes (aesthetic) command","code":""},{"path":"viz.html","id":"other-geoms","chapter":"3 Visualization","heading":"Other geoms","text":"help pages tell aesthetics, default stats, etc.","code":"\napropos(\"^geom_\") [1] \"geom_abline\"                  \"geom_area\"                   \n [3] \"geom_ash\"                     \"geom_bar\"                    \n [5] \"geom_barh\"                    \"geom_bin_2d\"                 \n [7] \"geom_bin2d\"                   \"geom_blank\"                  \n [9] \"geom_boxplot\"                 \"geom_boxploth\"               \n[11] \"geom_col\"                     \"geom_colh\"                   \n[13] \"geom_contour\"                 \"geom_contour_filled\"         \n[15] \"geom_count\"                   \"geom_crossbar\"               \n[17] \"geom_crossbarh\"               \"geom_curve\"                  \n[19] \"geom_density\"                 \"geom_density_2d\"             \n[21] \"geom_density_2d_filled\"       \"geom_density_line\"           \n[23] \"geom_density_ridges\"          \"geom_density_ridges_gradient\"\n[25] \"geom_density_ridges2\"         \"geom_density2d\"              \n[27] \"geom_density2d_filled\"        \"geom_dotplot\"                \n[29] \"geom_errorbar\"                \"geom_errorbarh\"              \n[31] \"geom_errorbarh\"               \"geom_freqpoly\"               \n[33] \"geom_function\"                \"geom_hex\"                    \n[35] \"geom_histogram\"               \"geom_histogramh\"             \n[37] \"geom_hline\"                   \"geom_jitter\"                 \n[39] \"geom_label\"                   \"geom_line\"                   \n[41] \"geom_linerange\"               \"geom_linerangeh\"             \n[43] \"geom_lm\"                      \"geom_map\"                    \n[45] \"geom_path\"                    \"geom_point\"                  \n[47] \"geom_pointrange\"              \"geom_pointrangeh\"            \n[49] \"geom_polygon\"                 \"geom_qq\"                     \n[51] \"geom_qq_line\"                 \"geom_quantile\"               \n[53] \"geom_raster\"                  \"geom_rect\"                   \n[55] \"geom_ribbon\"                  \"geom_ridgeline\"              \n[57] \"geom_ridgeline_gradient\"      \"geom_rug\"                    \n[59] \"geom_segment\"                 \"geom_sf\"                     \n[61] \"geom_sf_label\"                \"geom_sf_text\"                \n[63] \"geom_sina\"                    \"geom_smooth\"                 \n[65] \"geom_spline\"                  \"geom_spoke\"                  \n[67] \"geom_step\"                    \"geom_text\"                   \n[69] \"geom_tile\"                    \"geom_violin\"                 \n[71] \"geom_violinh\"                 \"geom_vline\"                  \n[73] \"geom_vridgeline\"             \n?geom_area             # for example"},{"path":"viz.html","id":"lets-try-geom_area","chapter":"3 Visualization","heading":"Let’s try geom_area","text":"Using area produce good plotover plotting hiding much dataextending y-axis 0 may may desirable.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"side-note-what-makes-a-plot-good","chapter":"3 Visualization","heading":"Side note: what makes a plot good?","text":"(?) graphics intended help us make comparisonsHow something change time?treatments matter? much?men women respond way?Key plot metric: plot make comparisons interested ineasily, andaccurately?","code":""},{"path":"viz.html","id":"time-for-some-different-data","chapter":"3 Visualization","heading":"Time for some different data","text":"HELPrct: Health Evaluation Linkage Primary care randomized clinical trialSubjects admitted treatment addiction one three substances.","code":"\nhead(HELPrct)##   age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n## 1  37            1    yes   49  3        177      225        0  NA      0\n## 2  37            1    yes   30 22          2       NA        0  NA      0\n## 3  26            1    yes   39  0          3      365       20  NA      0\n## 4  39            1    yes   15  2        189      343        0   1      1\n## 5  32            1    yes   39 12          2       57        0   1      0\n## 6  47            1    yes    6  1         31      365        0  NA      1\n##      sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n## 1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n## 2   male yes homeless 56 62  2     43         NA <NA> 26.670307 36.03694      1\n## 3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n## 4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n## 5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n## 6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n##   racegrp satreat sexrisk substance treat avg_drinks max_drinks\n## 1   black      no       4   cocaine   yes         13         26\n## 2   white      no       7   alcohol   yes         56         62\n## 3   black      no       2    heroin    no          0          0\n## 4   white     yes       4    heroin    no          5          5\n## 5   black      no       6   cocaine    no         10         13\n## 6   black      no       5   cocaine   yes          4          4\n##   hospitalizations\n## 1                3\n## 2               22\n## 3                0\n## 4                2\n## 5               12\n## 6                1"},{"path":"viz.html","id":"who-are-the-people-in-the-study","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"Hmm. ’s y?\nstat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.\nHmm. ’s y?stat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-1","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-2","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nlibrary(scales)\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-3","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study","chapter":"3 Visualization","heading":"How old are people in the HELP study?","text":"Notice messagesstat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.stat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.binwidth: default binwidth selected, really choose .binwidth: default binwidth selected, really choose .","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"viz.html","id":"setting-the-binwidth-manually","chapter":"3 Visualization","heading":"Setting the binwidth manually","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study-other-geoms","chapter":"3 Visualization","heading":"How old are people in the HELP study? – Other geoms","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every geom comes default statfor simple cases, stat stat_identity() nothingwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually-1","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every stat comes default geom, every geom default statwe can specify stats instead geom, preferwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"more-combinations","chapter":"3 Visualization","heading":"More combinations","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-much-do-they-drink-i1","chapter":"3 Visualization","heading":"How much do they drink? (i1)","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"covariates-adding-in-more-variables","chapter":"3 Visualization","heading":"Covariates: Adding in more variables","text":"Using color linetype:Using color facets","code":"\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"boxplots","chapter":"3 Visualization","heading":"Boxplots","text":"Boxplots use stat_quantile() computes five-number summary (roughly five quartiles data) uses define “box” “whiskers.”quantitative variable must y, must additional x variable.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"horizontal-boxplots","chapter":"3 Visualization","heading":"Horizontal boxplots","text":"Horizontal boxplots obtained flipping coordinate system:coord_flip() may used plots well reverse roles\nx y plot.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"axes-scaling-with-boxplots","chapter":"3 Visualization","heading":"Axes scaling with boxplots","text":"can scale continuous axis","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"give-me-some-space","chapter":"3 Visualization","heading":"Give me some space","text":"’ve triggered new feature: dodge (dodging things left/right). can control much set dodge manually.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"issues-with-bigger-data","chapter":"3 Visualization","heading":"Issues with bigger data","text":"Although can see generally positive association (expect), plotting may hiding information.","code":"\nrequire(NHANES)\ndim(NHANES)## [1] 10000    76\nNHANES %>%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"using-alpha-opacity","chapter":"3 Visualization","heading":"Using alpha (opacity)","text":"One way deal plotting set opacity low.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"geom_density2d","chapter":"3 Visualization","heading":"geom_density2d","text":"Alternatively (simultaneously) might prefer different geom altogether.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"multiple-layers","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"multiple-layers-1","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"things-i-havent-mentioned-much","chapter":"3 Visualization","heading":"Things I haven’t mentioned (much)","text":"coords (coord_flip() good know )coords (coord_flip() good know )themes (customizing appearance)themes (customizing appearance)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)transforming axestransforming axes","code":"\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"a-little-bit-of-everything","chapter":"3 Visualization","heading":"A little bit of everything","text":"","code":"\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"want-to-learn-more","chapter":"3 Visualization","heading":"Want to learn more?","text":"docs.ggplot2.org/docs.ggplot2.org/Winston Chang’s: R Graphics CookbookWinston Chang’s: R Graphics Cookbook","code":""},{"path":"viz.html","id":"what-else-can-we-do","chapter":"3 Visualization","heading":"What else can we do?","text":"shinyinteractive graphics / modelinginteractive graphics / modelinghttps://shiny.rstudio.com/https://shiny.rstudio.com/plotlyPlotly R package creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. plotly R library contains ggplotly function , convert ggplot2 figures Plotly object. Furthermore, option manipulating Plotly object style function.https://plot.ly/ggplot2/getting-started/Dynamic documentscombination RMarkdown, ggvis, shiny","code":""},{"path":"slr.html","id":"slr","chapter":"4 Simple Linear Regression","heading":"4 Simple Linear Regression","text":"","code":""},{"path":"slr.html","id":"a-linear-model","chapter":"4 Simple Linear Regression","heading":"4.1 A Linear Model","text":"Consider situation two variables, denote \\(x\\) \\(Y\\); \\(x\\) predictor variable, \\(Y\\) response. observe \\(n\\) observations, sample, denoted \\((x_i,y_i)\\). believe two variable related, namely \\[Y=f(x)+\\epsilon,\\] \\(\\epsilon\\) random error accounts fact know values \\(x\\) \\(f\\), still won’t know exactly \\(Y\\) .two variable case, assume \\(f(x)\\) linear function \\(x\\). , assume model \\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i.\\] attempts estimate function \\(f\\) now reduced trying estimate two numbers, intercept \\(\\beta_0\\) slope \\(\\beta_1\\): parameters.Consider following 4 models. Note differences statistics vs. parameters also individual observations vs. averages. Convince know use model.\\[\\begin{eqnarray*}\nE[Y_i|x_i] &=& \\beta_0 + \\beta_1 x_i \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n&& \\epsilon_i = y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\n&& e_i = y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"fitting-the-regression-line-least-squares","chapter":"4 Simple Linear Regression","heading":"4.1.1 Fitting the regression line: least squares","text":"fit regression line? Find \\(b_0\\) \\(b_1\\) minimize sum squared distance points line (called ordinary least squares):discusses previously, actually calculate \\(\\beta_0\\) \\(\\beta_1\\) need observe entire population. Instead, estimate quantities sample data . essentially trying find line fits data best. can thought line closest points sense. Given particular line particular point, think far point line? way think terms want model. Recall \\(x\\) predictor variable, \\(Y\\) response. linear regression context, set usually \\(x\\) something known beforehand, one goals predict response \\(Y\\). sense, way think good fitting line one vertical distance points line small.Residual: vertical distance point line. \\(^{th}\\) residual defined follows: \\[e_i=y_i-(b_0+b_1x_i)\\] \\(b_0\\) \\(b_1\\) intercept slope line consideration.","code":""},{"path":"slr.html","id":"notes","chapter":"4 Simple Linear Regression","heading":"Notes:","text":"vertical error measure reasonable predictor-response relationship. interested studying relationship height shoe size, vertical error idea doesn’t really exist (don’t think variables explanatory response - though certainly model approximate linear relationship). good fit might based perpendicular distance point line. reason vertical error model isn’t always ideal don’t always naturally consider one variables explanatory response. true, however, can (often ) model relationships variables don’t natural predictor - response relationship. , linear models cover, error (variable) term measured vertical direction.Now, find “best” fitting line, search line smallest residuals sense. particular, goal try find line minimizes following quantity: \\[Q=\\sum e_i^2 = \\sum (y_i-(b_0+b_1x_i))^2.\\]SSE: Sum squared errors (residuals), measure well line fits. SSE value \\(Q\\) optimal values \\(b_0\\) \\(b_1\\) given dataset.Finding \\(b_0\\) \\(b_1\\) minimize Q becomes calculus problem.\n\\[\\frac{dQ}{db_0}=-2\\sum (y_i-(b_0+b_1x_i)),\\qquad \\frac{dQ}{db_1}=-2\\sum\nx_i(y_i-(b_0+b_1x_i))\\] Setting equal 0 solving \\(b_0\\) \\(b_1\\) yields optimal values, denote \\(b_0\\) \\(b_1\\)Least Squares Estimates: \\(b_0\\) \\(b_1\\) (follows) minimize sum squared residuals, given :\n\\[b_0=\\bar{y}-b_1\\bar{x}, \\qquad b_1=\\frac{\\sum (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum\n(x_i-\\bar{x})^2}\\] (Sometimes \\(b_0\\) \\(b_1\\) referred \\(\\hat{\\beta}_0\\) \\(\\hat{\\beta}_1.)\\) now can write \\[SSE=\\sum (y_i-(b_0+b_1x_i))^2\\] can note already knew discussion. switch roles \\(x\\) \\(Y\\), best fitting line different. relationship actually symmetric, switching roles \\(x\\) \\(Y\\) give slope \\(1/b_1\\), case.question might ask chose minimize \\(\\sum e_i^2\\) opposed perhaps \\(\\sum |e_i|\\), \\(|\\cdot|\\) denotes absolute value. original motivation math much easier first case, however, can shown “nice” situations, statistical properties better well.Consider good applet visualizing concept minimizing sums squares. Note applet also allows visualization line created minimizing sum absolute errors.Keep mind following notation:\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 x_i \\mbox{   true mean response}\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i \\mbox{   estimate mean response}\\\\\n &=& \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\\\\ne_i &=& y_i - \\hat{y}_i\\\\\n\\epsilon_i &=& y_i - E[Y_i]\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"slr.html","id":"variance-sigma2","chapter":"4 Simple Linear Regression","heading":"4.1.2 Variance: \\(\\sigma^2\\)","text":"one variable (e.g., credit card balance), estimate variability response variable \\[\\mbox{sample variance} = \\frac{SSTO}{n-1} = \\frac{1}{(n-1)} \\sum_{=1}^n (y_i - \\overline{y})^2,\\] SSTO = sum squares total.regression setting, interested variance error terms (around line). particular, variance observed \\(y_i\\) around line given \\(\\sigma^2\\). estimate \\(\\sigma^2\\) using observed variability around line - residual. \\[SSE = \\sum_{=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{=1}^n e_i^2,\\] SSE sum squared errors (sometimes called sum square residuals). Note estimated two parameters, degrees freedom \\(df = n-2\\). best estimate \\(\\sigma^2\\) Mean Squared Error (MSE): \\[s^2 = MSE = \\frac{SSE}{n-2} = \\frac{1}{n-2} \\sum_{=1}^n (y_i - \\hat{y}_i)^2.\\] MSE = mean squared error.","code":""},{"path":"slr.html","id":"normal-errors-model","chapter":"4 Simple Linear Regression","heading":"4.2 Normal Errors Model","text":"least squares regression previous modeling hold probability model. However, inference easiest given normal probability model. Given linear model population:\n\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\]\n\\[\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\]","code":""},{"path":"slr.html","id":"important-features-of-the-model-with-normal-errors","chapter":"4 Simple Linear Regression","heading":"4.2.1 Important Features of the Model with Normal Errors","text":"\\(\\epsilon_i\\) random error term (, \\(\\epsilon_i\\) random variable).\\(Y_i\\) random variable \\(\\epsilon_i\\).assume \\(E[\\epsilon_i]=0\\), therefore \\(E[Y_i | x_i] = \\beta_0 + \\beta_1 x_i\\). (E expected value can thought long run average population mean.) , relationship explanatory response variables linear.\\(\\epsilon_i\\) amount \\(Y_i\\) values exceed fall short regression line.\\(var\\{Y_i | x_i\\} = var\\{\\epsilon_i\\} = \\sigma^2 \\rightarrow\\) constant variance around regression line.\\(SD\\{Y_i, Y_j\\} = SD\\{\\epsilon_i, \\epsilon_j \\} = 0 \\rightarrow\\) error terms uncorrelated.\\(\\epsilon_i \\sim N(0, \\sigma^2)\\), error terms normally distributed.\\(Y_i \\sim N(\\beta_0 + \\beta_1 x_i, \\sigma^2)\\)\nFigure 1.2: Figs 1.6 Kutner et al. (2004).\n","code":""},{"path":"slr.html","id":"technical-conditions","chapter":"4 Simple Linear Regression","heading":"4.2.2 Technical Conditions","text":"Though use least squares regression criterion fit line observed data, setting series conditions allows us inference model. conditions crucial check whenever regression, satisfied, nothing data real meaning. , interpretations won’t make sense, inference won’t valid. conditions:Condition Linearity relationship actually linear:\\[Y_i=\\beta_0+\\beta_1 x_i+\\epsilon_i\\] doesn’t make sense fit line data don’t believe relationship linear. \\(\\epsilon\\) term random variable, thus, two individuals measured value \\(x_i\\), \\(Y_i\\) , general, different. \\(Y_i\\) \nfunction \\(\\epsilon_i\\), \\(Y_i\\) also random variable.Furthermore, assuming mean \\(\\epsilon_i\\) 0. tells us fixed value \\(x_i\\), average value \\(Y_i\\) given \\(\\beta_0+\\beta_1 x_i\\). write \n\\[E[Y_i | x_i] = \\mu_{Y_i|x_i}=\\beta_0+\\beta_1 x_i\\] \\(\\mu\\) represents mean. fitted values estimates mean \\(Y_i\\) plugged-value \\(x_i\\).Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Independence individual observations independent . assuming data random sample population interest. contrast condition, suppose interested number pieces jigsaw puzzle time takes complete . data come one person (e.g., multiple puzzles), happens good jigsaw puzzles, estimate line much lower , person finish puzzles quickly, .e. small values \\(y_i\\). However, data independent, chance also getting someone bad jigsaw puzzles things even get unbiased estimate line.Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2.\\]Condition Constant Variance error terms, addition mean 0, assumed variance \\(\\sigma^2\\) depend value \\(x_i\\). assumed \nlooking point equal importance. Suppose knew particular value \\(x_i\\), variance \\(\\epsilon_i\\) 0. observed value \\(y_i\\) actually \\(\\mu_y\\), \nthus force line go point, since true line goes point. extreme case, case non-constant variance, regard values\nobserved smaller variation higher importance, tend accurate. denote variance condition \\[Var(Y_i|x_i)=\\sigma^2.\\]Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .Condition Normal Distribution Lastly, assume distribution error terms normal, common distribution. reason normal condition theoretic, techniques using say something \\(\\beta_i\\) based \\(b_i\\) data assumes normal distribution, easy work .","code":""},{"path":"slr.html","id":"maximum-likelihood","chapter":"4 Simple Linear Regression","heading":"4.2.3 Maximum Likelihood","text":"won’t cover details maximum likelihood. However, worth pointing maximum likelihood methods used many different areas statistics quite powerful. Additionally, simple linear regression case normal errors, maximum likelihood estimates turn exactly least squares estimates.","code":""},{"path":"slr.html","id":"reflection-questions-1","chapter":"4 Simple Linear Regression","heading":"4.3  Reflection Questions","text":"condition linear relationship appropriate?go estimating \\(\\beta_i, =0,1\\)?close estimates actual population values \\(\\beta_i, =0,1\\)?estimated function, actually interpret ?linear model conditions important inference?","code":""},{"path":"slr.html","id":"ethics-considerations","chapter":"4 Simple Linear Regression","heading":"4.4  Ethics Considerations","text":"matter technical conditions violated reporting analysis / model?technical conditions matter fitting line? Inference line? Neither? ?strong linear relationship predictor response variables found, mean predictor variables causes response?Simple Linear Regression called “simple?” model easy? (Spoiler: .)","code":""},{"path":"slr.html","id":"r-code-slr","chapter":"4 Simple Linear Regression","heading":"4.5 R code: SLR","text":"","code":""},{"path":"slr.html","id":"example-credit-scores","chapter":"4 Simple Linear Regression","heading":"4.5.1 Example: Credit Scores","text":"Consider dataset ISLR credit scores. don’t know sampling mechanism used collect data, unable generalize model results larger population. However, can look relationship variables build linear model. Notice lm() command always form: lm(response ~ explanatory).broom package three important functions:tidy() reports information based explanatory variableglance() reports information based overall modelaugment() reports information based observationWe can assume population model underlying relationship Limit average Balance. example, possible true (unknown) population model : \\[ E(Balance) = -300 + 0.2 \\cdot Limit.\\] Note consider Balance random Limit fixed. Also, note order inference, see error terms normally distributed around regression line constant variance values x.Consider someone $5,000 Limit credit card balance $1,000. population error term person : \\[\\epsilon_{5000 bal} = 1000 - [-300 + 0.2 \\cdot 5000] =  \\$700.\\]Consider someone $2,000 Limit credit card balance $50. population error term person : \\[\\epsilon_{2000 bal} = 50 - [-300 + 0.2 \\cdot 1000] =  -\\$50.\\]","code":"\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ISLR)\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) -293.     26.7         -11.0 1.18e- 24\n## 2 Limit          0.172   0.00507      33.9 2.53e-119\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>     <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.743         0.742  234.     1148. 2.53e-119     1 -2748. 5502. 5514.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nCredit %>%\n  lm(Balance ~ Limit, data = .) %>%\n  augment()## # A tibble: 400 × 8\n##    Balance Limit .fitted  .resid    .hat .sigma     .cooksd .std.resid\n##      <int> <int>   <dbl>   <dbl>   <dbl>  <dbl>       <dbl>      <dbl>\n##  1     333  3606    326.    6.87 0.00310   234. 0.00000135      0.0294\n##  2     903  6645    848.   55.3  0.00422   234. 0.000119        0.237 \n##  3     580  7075    922. -342.   0.00507   233. 0.00548        -1.47  \n##  4     964  9504   1338. -374.   0.0132    233. 0.0174         -1.61  \n##  5     331  4897    548. -217.   0.00251   234. 0.00109        -0.929 \n##  6    1151  8047   1088.   62.6  0.00766   234. 0.000280        0.269 \n##  7     203  3388    289.  -85.7  0.00335   234. 0.000227       -0.368 \n##  8     872  7114    928.  -56.2  0.00516   234. 0.000151       -0.241 \n##  9     279  3300    274.    5.39 0.00347   234. 0.000000929     0.0231\n## 10    1350  6819    878.  472.   0.00454   233. 0.00937         2.03  \n## # … with 390 more rows\nCredit %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE) +\n  xlab(\"Credit limit (in $)\") +\n  ylab(\"Credit card balance (in $)\")"},{"path":"infslr.html","id":"infslr","chapter":"5 Inference on SLR Parameters","heading":"5 Inference on SLR Parameters","text":"normal error regression model important estimating line. computer happily minimize sum squares model choose. However, inference linear parameters, must normal error regression model. , unless otherwise stated, assume normal error regression model holds.\\[Y_i = \\beta_0 + \\beta_1 x_i  + \\epsilon_i\\] \\(x_i\\) known, \\(\\beta_0, \\beta_1\\) parameters, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) independently.Sampling Distribution: sampling distribution distribution statistic measured repeated random samples population. Consider sampling applet provides nice visual sampling distribution.","code":""},{"path":"infslr.html","id":"inference-on-beta_1","chapter":"5 Inference on SLR Parameters","heading":"5.1 Inference on \\(\\beta_1\\)","text":"recall: \\[b_1 = \\frac{\\sum(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum(x_i - \\overline{x})^2}\\]\nalso know ,\n\\[\\begin{eqnarray*}\nE[b_1] &=& \\beta_1\\\\\nVar\\{b_1\\} &=& \\frac{\\sigma^2}{\\sum(x_i - \\overline{x})^2}\\\\\nSE^2\\{ b_1 \\} &=& \\frac{MSE}{\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]\nvariability slope becomes smaller \\(x_i\\) values spread . make sense intuitively \\(x_i\\) values spread , slight deviations measurements (.e., different random samples) won’t change slope line much. \\(x_i\\) exist narrow range, easy get vastly different \\(b_1\\) values depending particular random sample.","code":""},{"path":"infslr.html","id":"distribution-of-b_1","chapter":"5 Inference on SLR Parameters","heading":"Distribution of \\(b_1\\)","text":"distribution \\(b_1\\) (condition \\(E[b_1] = \\beta_1\\) normal error regression model), seem familiar : \\[T = \\frac{b_1 - \\beta_1}{s\\{b_1\\}} \\sim t_{n-2}.\\]\nRecall use t distribution standardized variable dividing constant. Instead, divide standard error induces extra variability thus t distribution.Indeed, \\(H_0: \\beta_1=0\\) true, \n\\[\\begin{eqnarray*}\nT = \\frac{b_1 - 0}{s\\{b_1\\}} \\sim t_{n-2}\n\\end{eqnarray*}\\]\nNote degrees freedom now \\(n-2\\) estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). reject null hypothesis, \\(b_1\\) leads us t-statistic larger expect random chance (.e., p-value small).p-value: p-value probability observed data extreme , fact, null hypothesis true.","code":""},{"path":"infslr.html","id":"ci-for-beta_1","chapter":"5 Inference on SLR Parameters","heading":"CI for \\(\\beta_1\\)","text":"many statistics know standard error, can create intervals give us confidence statements (parameter) making. general, confidence intervals form:\\((1-\\alpha)100\\%\\) confidence interval slope parameter, \\(\\beta_1\\): \n\\[\\begin{eqnarray*}\nb_1 &\\pm& t^*_{\\alpha/2,n-2} s\\{b_1\\}\\\\\nb_1 &\\pm& t^*_{\\alpha/2, n-2} \\sqrt{MSE/\\sum(x_i - \\overline{x})^2}\\\\\n\\end{eqnarray*}\\]Remember \\(\\beta_1\\) random. randomness comes data, endpoints CI random. interpreting CI, give interval plausible values \\(\\beta_1\\) rejected done hypothesis test. Alternatively, think CI set values \\(\\beta_1\\) fairly certain contains \\(\\beta_1\\). repeat process many times, \\((1-\\alpha)100\\%\\) time interval captures true parameter., regression finding linear relationships. claim relationship explanatory response variables causative. randomized studies able find causative mechanisms.","code":""},{"path":"infslr.html","id":"parameter-interpretation","chapter":"5 Inference on SLR Parameters","heading":"Parameter Interpretation","text":"confidence interval gives us estimate parameter(s) model, goal understand interpretation quantities.Intercept \\(\\beta_0\\): average value \\(Y\\) \\(x\\) 0. Often, intercept interpretable . example, studying relationship height weight, \\(\\beta_0\\) average weight someone 0 inches tall. Nonsense. often \\(\\beta_0\\) placeholder, number needs specified interpretation.Slope \\(\\beta_1\\): \\(\\beta_1\\) can interpreted increase average \\(Y\\) \\(x\\) incremented single unit. \\[E[Y|x+1]-E[Y|x]=\\beta_0+\\beta_1(x+1)-(\\beta_0+\\beta_1x)=\\beta_1\\]Variance \\(\\sigma^2\\): average squared deviation observation line.","code":""},{"path":"infslr.html","id":"estimating-a-response","chapter":"5 Inference on SLR Parameters","heading":"5.2 Estimating a response","text":"","code":""},{"path":"infslr.html","id":"interpolation-vs-extrapolation","chapter":"5 Inference on SLR Parameters","heading":"5.2.1 Interpolation vs Extrapolation","text":"One big problem using regression model condition linearity whether actually holds. result, attempting say something \\(Y\\) value \\(x^*\\) don’t typically data, can reasonably long \\(x^*\\) contained range data . called interpolation. Extrapolation hand trying use regression line outside range data. problem extrapolation evidence, ability check, whether linear condition holds beyond range data. Consider following example:interested amount crop produced single plant function amount compost added soil. take several plants, apply small, differing, amounts compost plants. see compost increases, output plant increases well, seemingly linear fashion. Naivete might lead us think , using regression line, can use 100 kilograms compost single plant receive huge amount crop return. truth, kill plant. evidence relationship linear anything outside range data, thus extrapolation mistake.interpolation, data support linear relationship, thus fine. trying say something happen particular values \\(x\\), call \\(x_h\\), need think exactly want say. know ahead time want say, can design experiment smart ways, see.","code":""},{"path":"infslr.html","id":"prediction-intervals-vs-confidence-intervals","chapter":"5 Inference on SLR Parameters","heading":"5.2.2 Prediction Intervals vs Confidence Intervals","text":"linear regression line gives us guess mean response individual particular value \\(x_h\\).conditions give us \\[E[Y|x]=\\beta_0+\\beta_1x\\]Plugging estimators, get \\[\\hat{y_i}=b_0+b_1 x_i\\] fitted value. However, accurate guess?confidence interval gives range plausible values \\(E[Y|x]\\). , mean response fixed value \\(x\\). confidence interval differs prediction interval, intended contain mean response, rather value response next individual observed value \\(x_h\\). result, prediction interval need larger confidence interval.","code":""},{"path":"infslr.html","id":"variability-of-point-estimates","chapter":"5 Inference on SLR Parameters","heading":"5.2.3 Variability of point estimates","text":"following variances \\(b_0\\), \\(b_1\\), fitted value \\(x_h\\): \\(\\hat{y}_{x_h}\\), new value \\(x_h\\): \\(\\hat{y}_{x_h(new)}\\).\\(\\sigma^2\\) variance errors.\\[\\begin{eqnarray*}\n\\mbox{var}(b_0)&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(b_1)&=&\\frac{\\sigma^2}{\\sum(x_i-\\bar{x})^2}\\\\\n\\mbox{var}(\\hat{y}_{x_h})&=&\\sigma^2\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\\\\\n\\mbox{var}(\\hat{y}_{x_h(new)})&=& \\sigma^2 + \\mbox(var)(\\hat{y}_{x_h}) = \\sigma^2\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]\n\\end{eqnarray*}\\]quantities estimated replacing \\(\\sigma^2\\) guess, \\(MSE\\). \\(b_0\\) fitted value (\\(x_h=0\\)), variability \\(b_0\\) special case 3rd formula. difference last two one \\(\\sigma^2\\). make sense, variance next observation (point) variance mean (.e., predicted regression line) plus variance error top mean.","code":""},{"path":"infslr.html","id":"standard-error","chapter":"5 Inference on SLR Parameters","heading":"5.2.3.1 Standard Error:","text":"phrase “standard error” indicates variability (square root variance) statistic situation variability estimate (, MSE used instead \\(\\sigma^2\\)). SE quantities therefore given :\\[\\begin{eqnarray*}\n\\mbox{SE}(b_0)&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(b_1)&=& \\sqrt{\\frac{MSE}{\\sum(x_i-\\bar{x})^2}}\\\\\n\\mbox{SE}(\\hat{y}_{x_h})&=& \\sqrt{MSE\\left[\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\\\\\n\\mbox{SE}(\\hat{y}_{x_h(new)})&=& \\sqrt{MSE + \\mbox(SE)(\\hat{y}_{x_h})^2} = \\sqrt{MSE\\left[1+\\frac{1}{n}+\\frac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right]}\n\\end{eqnarray*}\\]Confidence intervals essentially best guess plus minus two standard errors. exact, instead 2, use \\(qt(.975,n-p)\\) \\(p\\) number coefficients estimated (SLR \\(p=2\\)). idea smaller standard errors, narrower confidence intervals , information .","code":""},{"path":"infslr.html","id":"notes-1","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"prediction future (mean) response accurate \\(x_h = \\overline{x}\\). Think behavior regression line away \\(\\overline{x}\\). line much variable extremes.Confidence limits (.e., % coverage) \\(E[Y_h], \\beta_0, \\beta_1\\) sensitive departures normality. Especially large sample sizes (central limit theorem).Coverage percentages \\(\\hat{y}_h\\) sensitive departures normality. central limit theorem estimating average.Confidence limits apply one confidence interval (.e., entire line). won’t simultaneously 95% confident lots different intervals. ’ll address issue later.","code":""},{"path":"infslr.html","id":"anova-approach-to-regression","chapter":"5 Inference on SLR Parameters","heading":"5.3 ANOVA approach to regression","text":"mentioned previously, can think variability \\(Y\\) total variability. measure variability without knowing anything explanatory variable(s). , knowledge explanatory variables helps us predict \\(Y\\), , remove variability associated response. Consider following terms\\[\\begin{eqnarray*}\nSSTO &=& \\sum (y_i - \\overline{y})^2\\\\\nSSE &=& \\sum (y_i - \\hat{y}_i)^2\\\\\nSSR &=& \\sum (\\hat{y}_i - \\overline{y})^2\n\\end{eqnarray*}\\]SSTO sum squares total; SSE sum squared errors; SSR sum squares regression line. Unsquared, residuals nice relationship:\\[\\begin{eqnarray*}\ny_i - \\overline{y} &=& \\hat{y}_i - \\overline{y} + y_i - \\hat{y}_i\\\\\n\\mbox{total deviation} &=& \\mbox{dev reg around mean} + \\mbox{dev around line}\n\\end{eqnarray*}\\]\nresiduals squared, obvious \\[SSTO = SSR + SSE.\\] However, can derive relationship using algebra:\n\\[\\begin{eqnarray*}\n\\sum(y_i - \\overline{y})^2 &=& \\sum [ (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)]^2\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2 + 2 \\sum (\\hat{y}_i - \\overline{y})(y_i - \\hat{y}_i)\\\\\n&=& \\sum(\\hat{y}_i - \\overline{y})^2 + \\sum(y_i - \\hat{y}_i)^2\\\\\n\\end{eqnarray*}\\]\nlast term zeroed using equations (1.17) (1.20) ALSM (pgs 23-24).","code":""},{"path":"infslr.html","id":"mean-squares","chapter":"5 Inference on SLR Parameters","heading":"5.3.1 Mean Squares","text":"Sums squares increasing number data values. accommodate number observations, use mean squares instead sums square","code":""},{"path":"infslr.html","id":"notes-2","chapter":"5 Inference on SLR Parameters","heading":"Notes:","text":"MSE estimates \\(\\sigma^2\\) regardless whether \\(\\beta_1 = 0\\).\\(\\beta_1=0\\), MSR also estimates \\(\\sigma^2\\).comparison MSR MSE seem indicate whether \\(\\beta_1=0\\).\nNote can think MSR variability regression line around line \\(\\overline{y}\\). \\(\\beta_1=0\\), regression line varies MSR measuring natural variability error terms (\\(\\sigma^2\\)). \\(\\beta_1 \\ne 0\\), \\(b_1\\) values still vary naturally PLUS bit difference line \\(\\beta_1\\) line \\(\\mu_Y\\).","code":""},{"path":"infslr.html","id":"f-test-of-beta_1-0-versus-beta_1-ne-0","chapter":"5 Inference on SLR Parameters","heading":"5.3.2 F test of \\(\\beta_1 = 0\\) versus \\(\\beta_1 \\ne 0\\)","text":"\\[\\begin{eqnarray*}\nH_0: \\beta_1 = 0\\\\\nH_a: \\beta_1 \\ne 0\n\\end{eqnarray*}\\]\ntest statistic \\[F = \\frac{MSR}{MSE} = \\frac{\\sum(\\hat{y}_i - \\overline{y})^2}{\\sum(y_i - \\hat{y}_i)^2 / (n-2)}.\\] Large values \\(F\\) support \\(H_a\\), values \\(F\\) close 1 support \\(H_0\\). \\(H_0\\) true, \\[F \\sim F_{1,n-2}.\\] Note F-test always one-sided test (meaning reject BIG values \\(F\\)), though assessing two-sided hypothesis.Notice anova() output includes information sums squares F-test calculate \\(R^2\\). Also, tidy() function broom output dataframe easy work .","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA"},{"path":"infslr.html","id":"equivalence-of-f-and-t-tests","chapter":"5 Inference on SLR Parameters","heading":"5.3.3 Equivalence of F and t-tests","text":"\\[F = \\frac{SSR}{SSE/(n-2)} = \\frac{b_1^2 \\sum(x_i - \\overline{x})^2}{MSE} = \\frac{b_1^2}{MSE/\\sum(x_i - \\overline{x})^2} = \\bigg(\\frac{b_1}{s\\{b_1\\} }\\bigg)^2 = (T)^2\\]\n’re going continue use test models get complicated. general strategy always :Fit full model: \\(SSE_{full} = \\sum(y_i - b_0 - b_1 x_i)^2 = \\sum(y_i - \\hat{y}_i)^2\\)Fit reduced model (\\(H_0\\)): \\(SSE_{reduced} = \\sum(y_i - b_0)^2 = \\sum(y_i - \\overline{y})^2 = SSTO\\)\\(F = \\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}} \\div \\frac{SSE_{full}}{df_{full}} = \\frac{MSR}{MSE}\\)","code":""},{"path":"infslr.html","id":"descriptive-measures-of-linear-association","chapter":"5 Inference on SLR Parameters","heading":"5.4 Descriptive Measures of Linear Association","text":"discuss r (correlation) \\(R^2\\) (coefficient determination) descriptive measures linear association typically interested estimating parameter. Instead, \\(r\\) \\(R^2\\) tell us well linear model fits data.","code":""},{"path":"infslr.html","id":"correlation","chapter":"5 Inference on SLR Parameters","heading":"5.4.1 Correlation","text":"Consider scatterplot, variability directions: \\((x_i - \\overline{x}) \\ \\& \\ (y_i - \\overline{y})\\). data shown represent crop types taken World Data part Tidy Tuesday. point plot different country. x y variables represent proportion total yield last 50 years due crop type.\nFigure 5.1: % total yield different crops (across last 50 years). point represents country. Now lines average x average y values superimposed onto plots.\nred dot (plot), consider distance observation \\(\\overline{X}\\) line \\(\\overline{Y}\\) line. observation (red dot) ? ? one ?particular red dot (observation) contribute correlation? positive way (make \\(r\\) bigger)? negative way (make \\(r\\) smaller)?Positive Relationship: \\(x\\) increases, \\(Y\\) also tends increase, two variables said positive relationship (example: shoe size height).Negative Relationship: \\(x\\) increases, \\(Y\\) tends decrease, two variables said negative relationship (example: outside temperature heating oil used).variables positive relationship, \\(r=\\sqrt{R^2}\\). variables negative relationship, \\(r=-\\sqrt{R^2}\\).\\(r\\) can calculated directly well, given following formula:\\[\\begin{eqnarray*}\n\\mbox{sample covariance}&&\\\\\ncov(x,y) &=& \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\\n\\mbox{sample correlation}&&\\\\\nr(x,y) &=& \\frac{cov(x,y)}{s_x s_y}\\\\\n&=& \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\\n&=& \\frac{\\sum[(x_i-\\bar{x})(y_i-\\bar{y})]}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\end{eqnarray*}\\]numerator correlation describes relationship \\(x\\) \\(y\\). , \\(x\\) tends large (mean), \\(y\\) also tends large (mean), product two positive numbers, positive. Likewise, \\(x\\) small, \\(y\\) also tends small, product two negative numbers, positive. positive relationship result lot positive numbers summed numerator, thus \\(r\\) positive.relationship negative, one tend negative positive, thus sum involve lot negative terms causing \\(r\\) negative. denominator always positive.One thing note \\(r\\) affected choice label predictor label response. roles \\(x\\) \\(y\\) switched, \\(r\\) remain unaffected. Thus, \\(r\\), unlike value line, symmetric.\\(-1 \\leq r \\leq 1\\).\\(b_1 = r \\frac{s_y}{s_x}\\)\\(r=0, b_1=0\\)\\(r=1, b_1 > 0\\) can anything!\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)","code":""},{"path":"infslr.html","id":"coefficient-of-determination","chapter":"5 Inference on SLR Parameters","heading":"5.4.2 Coefficient of Determination","text":"coefficient determination, given \\(R^2\\), nice interpretation, proportion variability \\(y\\) explained variable \\(x\\). Recall, \\(SSE\\), sum squared errors (residuals) measure amount variable remaining data accounting information \\(x\\). \\(SSTO\\) total sums squares, measured total amount variability variable \\(y\\). coefficient determination given \n\\[R^2=1-\\frac{SSE}{SSTO}\\]\n, defining \\(SSR\\) regression sum squares (amount variation explained regression) \\[R^2=\\frac{SSR}{SSTO}.\\]Limitations:\n1. High \\(R^2\\) necessarily produce good “predictions.” lot variability around line (.e., within data), can wide prediction intervals response.\n2. High \\(R^2\\) necessarily mean line good fit. Quadratic () relationships can sometimes lead high \\(R^2\\). Additionally, one outlier can huge effect value \\(R^2\\).\n3. \\(R^2 \\approx 0\\) mean relationship \\(x\\) \\(y\\). Instead, \\(x\\) \\(y\\) might perfect quadratic relationship.","code":""},{"path":"infslr.html","id":"reflection-questions-2","chapter":"5 Inference on SLR Parameters","heading":"5.5  Reflection Questions","text":"different ways use inference model parameters?difference prediction confidence interval? used?sums squares broken meaningful pieces? differences SSTO, SSE, SSR?interpretation \\(R^2\\)?","code":""},{"path":"infslr.html","id":"ethics-considerations-1","chapter":"5 Inference on SLR Parameters","heading":"5.6  Ethics Considerations","text":"technical conditions violated, CI tell us slope parameter? normality condition?technical conditions violated PI tell us predicted values? normality condition?confounding variables might exist link Limit Balance positively correlated causal?population might ames data representative ? population might Credit data representative ? (Hint: look data documentation typing ?ames ?Credit.)","code":""},{"path":"infslr.html","id":"ames-inf","chapter":"5 Inference on SLR Parameters","heading":"5.7 R: SLR Inference","text":"","code":""},{"path":"infslr.html","id":"cis","chapter":"5 Inference on SLR Parameters","heading":"5.7.1 CIs","text":"Also available broom CIs coefficients. Consider ames dataset available openintro package.Data set contains information Ames Assessor’s Office used computing assessed values individual residential properties sold Ames, IA 2006 2010.reasons discuss coming chapters, ’ll consider \\(\\ln\\) price home, also consider homes less 3000 square feet.R t-test automatically, done hand using provided SE (available column called std.error).\\[\\begin{eqnarray*}\nH_0:&& \\beta_1 = 0\\\\\nH_a:&& \\beta_1 \\ne 0\\\\\nt &=& \\frac{0.000613 - 0}{0.0000114} = 53.86\\\\\np-value &=& 2 P(t_{2902} \\geq 53.86) = 2*(1-pt(53.86, 2902)) = \\mbox{small}\\\\\n\\end{eqnarray*}\\]p-value small, reject null hypothesis: model linear relationship area price_ln, slope coefficient must different zero (greater zero one-sided test). Note knowing positive relationship tell us price_ln result area. , reason believe causative mechanism.90% confidence interval slope coefficient, \\(\\beta_1\\) (0.000595, 0.000632). true population slope model area price_ln somewhere (0.000595, 0.000632). Note even though values seem small, significantly (necessarily substantially) away zero. tempted confuse small zero, magnitude slope coefficient depends heavily units measurement variables.","code":"\nlibrary(openintro)\names_inf <- ames %>%\n  filter(area <= 3000) %>%\n  mutate(price_ln = log(price))\names_inf %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)\names_inf %>%\n  lm(price_ln ~ area, data = .) %>% \n  tidy(conf.int = TRUE, conf.level = 0.9)## # A tibble: 2 × 7\n##   term         estimate std.error statistic p.value  conf.low conf.high\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0 11.1      11.1     \n## 2 area         0.000614 0.0000114      53.9       0  0.000595  0.000632"},{"path":"infslr.html","id":"predictions","chapter":"5 Inference on SLR Parameters","heading":"5.7.2 Predictions","text":"","code":""},{"path":"infslr.html","id":"predicting-ames","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.1 Predicting Ames","text":"Fortunately, R allows creating mean prediction intervals. need create new data set variable name predictor, value interested , might call new_ames. use augment() give either confidence prediction interval, follows., create intervals entire range explanatory variables:, turns , easier ways computer find confidence prediction intervals:Also, graphs can made, ’ll need keep output (use entire dataset instead new_ames).","code":"\n# store the linear model object so that we can use it later.\names_lm <- lm(price_ln ~ area, data = ames_inf)\n\n# create a new dataframe\nnew_ames <- data.frame(area = c(1000, 1500, 2000))\n\n# get df from the model\names_df <- ames_lm %>% glance() %>% select(df.residual) %>% pull()\names_df## [1] 2902\n# new data predictions\names_pred <- ames_lm %>% \n  augment(newdata = new_ames, type.predict = \"response\", se_fit = TRUE)\names_pred## # A tibble: 3 × 3\n##    area .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000    11.7 0.00760\n## 2  1500    12.0 0.00527\n## 3  2000    12.3 0.00792\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, ames_df)\ncrit_val## [1] 1.96\n# SE of the mean response\nse_fit <- ames_pred %>% select(.se.fit) %>% pull()\nse_fit##       1       2       3 \n## 0.00760 0.00527 0.00792\n# esimate of the overall variability, sigma\names_sig <- ames_lm %>% glance() %>% select(sigma) %>% pull()\names_sig## [1] 0.284\n# calculate the SE of the predictions\nse_pred <- sqrt(ames_sig^2 + se_fit^2)\nse_pred##     1     2     3 \n## 0.284 0.284 0.284\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\names_pred <- ames_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\names_pred## # A tibble: 3 × 7\n##    area .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000    11.7 0.00760     11.2     12.3     11.7     11.7\n## 2  1500    12.0 0.00527     11.5     12.6     12.0     12.0\n## 3  2000    12.3 0.00792     11.8     12.9     12.3     12.3\names_pred_all <- ames_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(ames_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\names_pred_all %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")\names_lm %>% \n  augment(newdata = new_ames, interval = \"confidence\")## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.7   11.7\n## 2  1500    12.0   12.0   12.0\n## 3  2000    12.3   12.3   12.3\names_lm %>% \n  augment(newdata = new_ames, interval = \"prediction\")  ## # A tibble: 3 × 4\n##    area .fitted .lower .upper\n##   <dbl>   <dbl>  <dbl>  <dbl>\n## 1  1000    11.7   11.2   12.3\n## 2  1500    12.0   11.5   12.6\n## 3  2000    12.3   11.8   12.9\names_conf <- ames_lm %>% \n  augment(interval = \"confidence\")\n\names_pred <- ames_lm %>% \n  augment(interval = \"prediction\")  \n\names_pred %>%\n  ggplot(aes(x = area, y = price_ln)) + \n  geom_point() +\n  geom_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2) + \n  geom_ribbon(ames_conf, mapping = aes(ymin = .lower, ymax = .upper), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"predicting-credit","chapter":"5 Inference on SLR Parameters","heading":"5.7.2.2 Predicting Credit","text":"contrast ames data, note predictions (particulary CI around line) drastically changes sample size small., create intervals entire range explanatory variables:","code":"\nlibrary(ISLR)  # source of the Credit data\n# store the linear model object so that we can use it later.\ncredit_lm <- lm(Balance ~ Limit, data = Credit)\n\n# create a new dataframe\nnew_credit <- data.frame(Limit = c(1000, 3000, 7000))\n\n# get df from the model\ncredit_df <- credit_lm %>% glance() %>% select(df.residual) %>% pull()\ncredit_df## [1] 398\n# new data predictions\ncredit_pred <- credit_lm %>% \n  augment(newdata = new_credit, type.predict = \"response\", se_fit = TRUE)\ncredit_pred## # A tibble: 3 × 3\n##   Limit .fitted .se.fit\n##   <dbl>   <dbl>   <dbl>\n## 1  1000   -121.    22.2\n## 2  3000    222.    14.6\n## 3  7000    909.    16.4\n# get the multiplier / critical value for creating intervals\ncrit_val <- qt(0.975, credit_df)\ncrit_val## [1] 1.97\n# SE of the mean response\nse_fit <- credit_pred %>% select(.se.fit) %>% pull()\nse_fit##    1    2    3 \n## 22.2 14.6 16.4\n# esimate of the overall variability, $\\sigma$\ncredit_sig <- credit_lm %>% glance() %>% select(sigma) %>% pull()\ncredit_sig## [1] 234\n# calculate the SE of the predictions\nse_pred <- sqrt(credit_sig^2 + se_fit^2)\nse_pred##   1   2   3 \n## 235 234 234\n# calculating both confidence intervals for the mean responses and\n# prediction intervals for the individual responses\n\ncredit_pred <- credit_pred %>%\n  mutate(lower_PI = .fitted - crit_val * se_pred,\n         upper_PI = .fitted + crit_val * se_pred,\n         lower_CI = .fitted - crit_val * se_fit,\n         upper_CI = .fitted + crit_val * se_fit)\n\ncredit_pred## # A tibble: 3 × 7\n##   Limit .fitted .se.fit lower_PI upper_PI lower_CI upper_CI\n##   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n## 1  1000   -121.    22.2    -582.     340.    -165.    -77.4\n## 2  3000    222.    14.6    -238.     682.     193.    251. \n## 3  7000    909.    16.4     448.    1369.     876.    941.\ncredit_pred_all <- credit_lm %>% \n  augment(type.predict = \"response\", se_fit = TRUE) %>%\n  mutate(.se.pred = sqrt(credit_sig^2 + .se.fit^2)) %>%\n  mutate(lower_PI = .fitted - crit_val * .se.pred,\n         upper_PI = .fitted + crit_val * .se.pred,\n         lower_CI = .fitted - crit_val * .se.fit,\n         upper_CI = .fitted + crit_val * .se.fit)\n\ncredit_pred_all %>%\n  ggplot(aes(x = Limit, y = Balance)) + \n  geom_point() +\n  stat_smooth(method = lm, se = FALSE) +\n  geom_ribbon(aes(ymin = lower_PI, ymax = upper_PI), \n              alpha = 0.2) + \n  geom_ribbon(aes(ymin = lower_CI, ymax = upper_CI), \n              alpha = 0.2, fill = \"red\")"},{"path":"infslr.html","id":"anova-output","chapter":"5 Inference on SLR Parameters","heading":"5.7.3 ANOVA output","text":"Note tidy() creates dataframe slighlty easier work , regardless tidy() anova() function provides exact output.","code":"\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() ## Analysis of Variance Table\n## \n## Response: price_ln\n##             Df Sum Sq Mean Sq F value Pr(>F)    \n## area         1    233   233.4    2901 <2e-16 ***\n## Residuals 2902    234     0.1                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\names_inf %>%\n  lm(price_ln ~ area, data = .) %>%\n  anova() %>%\n  tidy()## # A tibble: 2 × 6\n##   term         df sumsq   meansq statistic p.value\n##   <chr>     <int> <dbl>    <dbl>     <dbl>   <dbl>\n## 1 area          1  233. 233.         2901.       0\n## 2 Residuals  2902  234.   0.0805       NA       NA\names_inf %>%\n  mutate(bedrooms = case_when(\n    Bedroom.AbvGr <=1 ~ \"1\",\n    Bedroom.AbvGr <=2 ~ \"2\",\n    Bedroom.AbvGr <=3 ~ \"3\",\n    TRUE ~ \"4+\"\n  )) %>%\n  ggplot(aes(x = area, y = price_ln, color = bedrooms)) + \n  geom_point() + \n  geom_smooth(method = lm, se = FALSE)"},{"path":"diag1.html","id":"diag1","chapter":"6 Diagnostic Measures I","heading":"6 Diagnostic Measures I","text":"","code":""},{"path":"diag1.html","id":"model-conditions","chapter":"6 Diagnostic Measures I","heading":"6.1 Model Conditions","text":"linear relationshipconstant varianceindependent errorsnormal errorsno outliers (’s part normality condition)","code":""},{"path":"diag1.html","id":"notes-3","chapter":"6 Diagnostic Measures I","heading":"Notes:","text":"required conditions explanatory / predictor variable.explanatory variable binary, SLR becomes two-sided t-test.\nRemember, \\(x_i\\) constants, don’t think distribution. Also, allowed construct whatever \\(x_i\\) want. said,larger range \\(x_i\\) produce less variable predictions.However, outliers x-direction can influential.","code":""},{"path":"diag1.html","id":"residuals","chapter":"6 Diagnostic Measures I","heading":"6.1.1 Residuals","text":"\\[\\begin{eqnarray*}\n\\mbox{residual: } e_i &=& Y_i -  \\hat{Y}_i \\ \\ \\ \\mbox{can measure}\\\\\n\\mbox{error term: } \\epsilon_i &=& Y_i - E[Y_i] \\ \\ \\ \\mbox{measure}\n\\end{eqnarray*}\\]mean: \\(\\overline{e} = 0\\) (definition!). Therefore, average residuals provides information whether \\(E[\\epsilon]=0\\).\n(note: \\(\\sum e_i = \\sum(Y_i - b_0 - b_1 x_i) = 0\\) \\(\\frac{\\delta Q}{\\delta \\beta_0} = 0\\).)variance: \\[s^2 = \\frac{1}{(n-2)} \\sum (Y_i - \\hat{Y}_i)^2 = \\frac{1}{(n-2)} \\sum (e_i)^2 = \\frac{1}{(n-2)} \\sum (e_i - \\overline{e})^2 = \\frac{SSE}{(n-2)} = MSE\\]\nnon-independent: \\(\\sum e_i=0\\), residuals independent. \\(\\epsilon_i\\), errors, assume independent.","code":""},{"path":"diag1.html","id":"semistudentized-residuals","chapter":"6 Diagnostic Measures I","heading":"semistudentized residuals","text":"\\[e_i^* = \\frac{e_i - \\overline{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\\] MSE isn’t quite variance \\(e_i\\). actually estimate variance \\(\\epsilon_i\\). good enough right now.","code":""},{"path":"diag1.html","id":"diagnostic-plots-of-residuals","chapter":"6 Diagnostic Measures I","heading":"6.2 Diagnostic Plots of Residuals","text":"[1] Plots residuals vs. predictor variables / explanatory variables.\n[3] Plots residuals vs. fitted values.\n[6] Box plot (histogram) residuals.plots show conditions don’t hold??\n1. Abandon regression model use something appropriate. interested coefficients: nonparametrics, time series, random forests. interested prediction, maybe okay conditions don’t hold? maybe (e.g., independence key!). prediction models include: random forests, support vector machines, neural networks, loess (smoothing).\n2. Transform variables model hold.","code":""},{"path":"diag1.html","id":"violating-linearity","chapter":"6 Diagnostic Measures I","heading":"Violating Linearity","text":"see: pattern scatterplot isn’t linearYou :\n- include \\(x^2\\) term function \\(x\\).\n- can linearize non-linear function, interpretation can get complicated.","code":""},{"path":"diag1.html","id":"violating-constant-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Constant Errors","text":"see:\n- typically see errors increase \\(x\\) increases (sometimes easier see absolute value residuals).\n- hugely serious problem (less efficient estimates, variance estimates correct):\n- direct approach use weighted least squares (won’t talk )\n- often transformations stabilize variance","code":""},{"path":"diag1.html","id":"violating-independent-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Independent Errors","text":"see:\n- typically due another variable (time, geographic location, etc.)\n- residual plotted variable.:\n- work model accounts correlated error structure (e.g., time series)","code":""},{"path":"diag1.html","id":"violating-normal-errors","chapter":"6 Diagnostic Measures I","heading":"Violating Normal Errors","text":"see:\n- residual plots symmetric\n- general empirical rule: \\(68\\% \\pm \\sqrt{MSE}\\), \\(95\\% \\pm 2\\sqrt{MSE}\\) (concerned grossly different)\n- won’t cover normal probability plots (also called q-q plots):\n- non-normality non-constant variance often related, transformations typically fix .","code":""},{"path":"diag1.html","id":"having-outliers","chapter":"6 Diagnostic Measures I","heading":"Having Outliers","text":"see:\n- typically easiest see standardized studentized residuals\n- SLR resistant outliers\n- expect 95% studentized residuals within \\(\\pm 2\\).:\n- outliers can seriously deform least squares estimate. good reason keep value(s), might consider nonparametric method places less weight point.","code":""},{"path":"diag1.html","id":"why-do-we-plot-resid-vs.-fitted-and-not-vs.-observed","chapter":"6 Diagnostic Measures I","heading":"Why do we plot resid vs. fitted and not vs. observed?","text":"know \\(e_i\\) \\(\\hat{Y}_i\\) uncorrelated (can shown using linear algebra, also note \\(\\sum e_i \\hat{Y}_i = 0\\)). , go resid vs. fitted scatterplot resid vs. observed scatter plot, shift point x-direction () amount equal residual. residual negative, point shift left. residual positive, point shift right. thus create positively correlated relationship (resid observed). degree shift depend relative magnitudes residuals predicted values.\\(\\Rightarrow e_i \\mbox{ } Y_i\\) correlated therefore independent. Consider two examples . examples, residual correlated response variable. However, easier see correlation residual also responsible relationship response variable explanatory variable.\nFigure 1.3: correlated data, hard see dependence response variable residuals. However, careful look third plot shows slightly stronger correlating response variable residuals fitted values residuals.\n\nFigure 1.4: uncorrelated data, much easier see dependence response variable residuals.\n","code":""},{"path":"diag1.html","id":"transformations","chapter":"6 Diagnostic Measures I","heading":"6.3 Transformations","text":"Important note!! idea behind transformations make model appropriate possible data hand. want find correct linear model; want conditions hold. trying find significant model big \\(R^2\\).\nFigure 6.1: Taken Applied Linear Statistical Models, 5th ed. Kutner et al. Figures 3.13 3.15.\n","code":""},{"path":"diag1.html","id":"correcting-condition-violations","chapter":"6 Diagnostic Measures I","heading":"6.4 Correcting Condition Violations","text":"noticed non-linear relationship, certainly think fit non-linear trend line. time, might consider fitting exponential\nrelationship, functional form. still called linear regression, transform data \nfitting linear model \\(f(y)\\) \\(g(x)\\). way, techniques theory still follow, least squares etc. \ninstance, might think fitting linear relationship new variables \\(y^*=\\sqrt{y}\\) \\(x\\), possibly \\(y\\) \\(x^*=x^2\\). Occasionally, might think transforming variables. One general rule follows:initial fit shows violation linear condition , best transform \\(x\\).initial fit shows violation linear condition well normality issues heteroscedasticity, transform \\(y\\) considered. hopefully transformation correct problems ., changing \\(x\\) changes shape relationship. Changing \\(Y\\) changes error structure (also often shape).Note: Formal testing violations model conditions usually good idea. ? Multiple testing problems arise (extra type errors); additional tests typically sensitive technical conditions; lose power detect differences interest.","code":""},{"path":"diag1.html","id":"gdp-example","chapter":"6 Diagnostic Measures I","heading":"6.4.1 GDP Example","text":"Consider following data collected World Bank 20201. data include GDP % Urban Population. description variables defined World Bank provided .2GDP: “GDP per capita gross domestic product divided midyear population. GDP sum gross value added resident producers economy plus product taxes minus subsidies included value products. calculated without making deductions depreciation fabricated assets depletion degradation natural resources. Data current U.S. dollars.”Urban Population (% total): “Urban population refers people living urban areas defined national statistical offices. calculated using World Bank population estimates urban ratios United Nations World Urbanization Prospects.”\nFigure 6.2: seems though original data don’t meet LINE conditions needed inference linear model.\nLet’s try transform variables get model seems conform LINE technical conditions.\nFigure 6.3: ln(urban) vs gdp: seems like taking natural log urban makes relationship worse.\nAlas, really seems like gdp variable problem urban variable. Let’s transform gdp instead.\nFigure 1.5: urban vs gdp^2: squaring gdp also makes relationship worse.\nneeded transformation spread small gdp values shrink large gdp values.\nFigure 1.6: urban vs sqrt(gdp): square root gdp seems help!\nnatural log stronger function square root (, shrink large values even .).\nFigure 1.7: urban vs ln(gdp): natural log gdp creates residual plot seems follow LINE technical conditions.\nBox-Cox transformations class transformations. Generally, \\(\\ln, \\exp\\), square root, polynomial transformations sufficient fit linear model satisfies necessary technical conditions. won’t spend time learning Box-Cox, can read learn transforming variables.","code":""},{"path":"diag1.html","id":"interpreting-regression-coefficients","chapter":"6 Diagnostic Measures I","heading":"Interpreting Regression Coefficients","text":"Example 1: Transforming \\(x\\): \\(x' = \\ln(x)\\) \\[E[Y] = \\beta_0  + \\beta_1 \\ln(x)\\] can interpret \\(\\beta_1\\) following way: every increase 1 unit \\(\\ln(x)\\), \\(E[Y]\\) increases \\(\\beta_1\\). isn’t meaningful statement. Instead, consider \\[E[Y| \\ln(2x)] - E[Y|\\ln(x)] = \\beta_1 \\ln(2).\\] can interpreted doubling \\(x\\) gives additive increase E[Y] \\(\\beta_1 \\ln(2)\\) units.Example 2: Transforming \\(Y\\): \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 x\\] Note also \\[ E[\\ln(Y)] = median(\\ln(Y))\\] distribution \\(\\ln(Y)\\) symmetric around regression line.\\[\\begin{eqnarray*}\nmedian(\\ln(Y)) = \\beta_0 + \\beta_1 x\\\\\nmedian(Y) = e^{\\beta_0} e^{\\beta_1 x}\\\\\n\\frac{median(Y| x+1)}{median(Y| x)} = e^{\\beta_1}\\\\\n\\end{eqnarray*}\\]\nincrease 1 unit x associated multiplicative change \\(e^{\\beta_1}\\) median(\\(Y\\)). Important keep mind \n\\[\\begin{eqnarray*}\nE[\\ln(Y)] \\ne \\ln(E[Y])\\\\\nmedian(\\ln(Y)) = \\ln(median(Y))\n\\end{eqnarray*}\\]Example 3: Transforming \\(x\\) \\(Y\\): \\(x' = \\ln(x)\\) \\(Y' = \\ln(Y)\\) \\[E[\\ln(Y)] = \\beta_0  + \\beta_1 \\ln(x)\\]\n\\[\\begin{eqnarray*}\n\\frac{median(Y|2x)}{median(Y|x)} &=& \\frac{e^{\\beta_0 + \\beta_1 \\ln(2x)}}{e^{\\beta_0 + \\beta_1 \\ln(x)}}\\\\\n&=& e^{\\beta_1 (\\ln(2x) - \\ln(x))}\\\\\n&=& e^{\\beta_1 \\ln(2)} = 2^{\\beta_1}\n\\end{eqnarray*}\\]\ndoubling \\(x\\) associated multiplicative change \\(2^{\\beta_1}\\) median Y.Note model regressing GDP % urban used log transformation GDP (response, Y variable). know :\\[\\begin{eqnarray*}\n\\widehat{\\ln(Y)} &=& \\mbox{median}(\\ln(Y)) = b_0 + b_1 \\cdot x\\\\\n\\mbox{median}(Y) &=& \\exp(b_0 + b_1 \\cdot x)\n\\end{eqnarray*}\\]using coefficients output linear model regressing ln_gdp urban, can find model predicts median gdp (transformed!) function urban.\\[\\begin{eqnarray*}\n\\mbox{median}(\\verb;gdp;) &=& \\exp(6.11 +  0.0425 \\cdot \\verb;urban;)\n\\end{eqnarray*}\\]\nFigure 6.4: blue exponential line represents median GDP particular value % urban.\n","code":"\nGDP %>%\n  lm(ln_gdp ~ urban, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   6.11     0.202        30.3 1.18e-74\n## 2 urban         0.0425   0.00308      13.8 2.29e-30\nGDP %>%\n  ggplot() + \n  geom_point(aes(x = urban, y = gdp)) + \n  geom_line(aes(x = urban, y = exp(6.11 + 0.0425*urban)), color = \"blue\")"},{"path":"diag1.html","id":"reflection-questions-3","chapter":"6 Diagnostic Measures I","heading":"6.5  Reflection Questions","text":"know model conditions hold?model conditions don’t hold?log transformation, slope coefficient interpreted?happens want add lots explanatory variables model?add quadratic term?","code":""},{"path":"diag1.html","id":"ethics-considerations-2","chapter":"6 Diagnostic Measures I","heading":"6.6  Ethics Considerations","text":"GDP model country observational unit (192 countries). quick Google shows UN currently defines 193 countries Leaving aside might one short, technical conditions need apply entire population? Can think country data 2020 representative (unknown?) population?mammals dataset collected 1976. reasons 50 year old dataset might represent accurate model population today?","code":""},{"path":"diag1.html","id":"r-slr-inference","chapter":"6 Diagnostic Measures I","heading":"6.7 R: SLR Inference","text":"Consider mammals dataset MASS package (careful MASS, overwrite dplyr functions filter select) representing average brain (g) body (kg) weights 62 species land mammals3.first glance linear model, doesn’t seem like really linear relationship ! certainly residuals don’t look like ’d hoped LINE technical conditions.Let’s try log transformations (square root transformations might also make sense).\nFigure 6.5: Taking natural log body weight doesn’t seem create model linear shape.\n\nFigure 6.6: Taking natural log brain weight also doesn’t seem create model linear shape.\n\nFigure 6.7: Taking natural log brain body weight seem create model linear shape!\n","code":"\nlibrary(MASS)  # be careful with MASS, it messes up filter and select\ndata(mammals)\n\nmammals %>% \n  ggplot(aes(x = body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\nmammals %>%\n  lm(brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\nmammals <- mammals %>%\n  mutate(ln_body = log(body),\n         ln_brain = log(brain))\n\nlibrary(patchwork)  # to get the plots next to one another\n\np1 <- mammals %>% \n  ggplot(aes(x = ln_body, y = brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np2 <- mammals %>%\n  lm(brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np1 + p2\np3 <- mammals %>% \n  ggplot(aes(x = body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np4 <- mammals %>%\n  lm(ln_brain ~ body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np3 + p4\np5 <- mammals %>% \n  ggplot(aes(x = ln_body, y = ln_brain)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\np6 <- mammals %>%\n  lm(ln_brain ~ ln_body, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\n\np5 + p6"},{"path":"simult.html","id":"simult","chapter":"7 Simultaneous Inference","heading":"7 Simultaneous Inference","text":"(sections 4.1, 4.2, 4.3 ALSM)Spring 2022: cover joint estimation \\(\\beta_0\\) \\(\\beta_1\\), cover simultaneous estimation mean response simultaneous prediction intervals new observations.","code":""},{"path":"simult.html","id":"joint-estimation-of-beta_0-and-beta_1","chapter":"7 Simultaneous Inference","heading":"7.1 Joint Estimation of \\(\\beta_0\\) and \\(\\beta_1\\)","text":"Note inference \\(\\beta_1\\) can performed \\(\\beta_0\\) slight modifications formula SE. Although \\(\\beta_1\\) typically related research question hand, \\(\\beta_0\\) can also informative right. Indeed, interested CI either one, calculate following:\\[b_0 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm t_{(1-\\alpha/2), n-2} \\cdot s\\{b_1\\}\\]Clearly many parameters many intervals, wouldn’t think cover true parameters probability \\((1-\\alpha)\\). figure probability cover true parameter, let’s define\n\\[\\begin{eqnarray*}\nA_1 &=& \\mbox{first CI cover } \\beta_0\\\\\nP(A_1) &=& \\alpha\\\\\nA_2 &=& \\mbox{second CI cover } \\beta_1\\\\\nP(A_2) &=& \\alpha\\\\\n\\end{eqnarray*}\\]want bound probability intervals cover true parameter.\n\\[\\begin{eqnarray*}\nP(\\mbox{CIs cover}) &=& 1- P(\\mbox{least one CI cover})\\\\\n&=& 1 - P( A_1 \\mbox{ } A_2)\\\\\n&&\\\\\nP( A_1 \\mbox{ } A_2) &=& P(A_1) + P(A_2) - P(A_1 \\mbox{ } A_2)\\\\\n&=& \\alpha + \\alpha - ???\\\\\n& \\leq& 2 \\alpha \\\\\nP(\\mbox{least one CI cover}) &\\leq& 2 \\alpha\\\\\nP(\\mbox{CIs cover}) &\\geq& 1- 2 \\alpha\\\\\n\\end{eqnarray*}\\]Note make two 95% CIs, Bonferroni inequality tells us cover respective parameters 90% repeated samples (.e., 90% confidence).\\(g\\) CIs, letting multiplier level significance given \\(\\alpha/g\\) create familywise confidence intervals level significance \\(\\alpha\\).\\(\\beta_0\\) \\(\\beta_1\\), let \\(B = t_{(1-\\alpha/4), n-2}\\). intervals \\(1-\\alpha\\) familywise confidence limits :\\[b_0 \\pm B \\cdot s\\{b_0\\}\\]\n\\[b_1 \\pm B \\cdot s\\{b_1\\}\\]Note interpretations depend heavily number intervals (true multiple comparison adjustments).Bonferroni extremely conservative, therefore low power.Bonferroni easily extends number comparisons.Bonferroni can adjust multiple comparisons even comparing different types analyses (e.g., CI slope parameter predicted response).","code":""},{"path":"simult.html","id":"simultaneous-estimation-of-a-mean-response","chapter":"7 Simultaneous Inference","heading":"7.2 Simultaneous Estimation of a Mean Response","text":"intervals parameters, creating intervals multiple mean responses leads problems multiple comparisons. , goal adjust intervals probability getting dataset lead intervals covering mean responses \\(1-\\alpha\\).(Note reason simultaneous inference combination natural sampling variability \\(b_0\\) \\(b_1\\) lead mean responses correct \\(E[Y_h]\\) range \\(x\\) values incorrect different range \\(x\\) values.)","code":""},{"path":"simult.html","id":"working-hotelling-procedure","chapter":"7 Simultaneous Inference","heading":"Working-Hotelling Procedure","text":"Working-Hotelling procedure gives confidence band entire range \\(x\\) values. family confidence interval simultaneous intervals \\(1-\\alpha\\). Note multiplier determined bound entire line complete range \\(x\\) values.\\[\\hat{y}_h \\pm W s \\{ \\hat{y}_h \\}\\]\n\\(W^2 = 2 F_{(1-\\alpha; 2, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B \\cdot s \\{ \\hat{y}_h \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"simultaneous-prediction-intervals-for-new-observations","chapter":"7 Simultaneous Inference","heading":"7.3 Simultaneous Prediction Intervals for New Observations","text":"Just estimating mean response, interval predicting new observations can adjusted total range observations contained appropriate intervals probability \\(1-\\alpha\\). prediction intervals, necessary specify number \\(g\\) intervals interest.","code":""},{"path":"simult.html","id":"scheffé-procedure","chapter":"7 Simultaneous Inference","heading":"Scheffé Procedure","text":"Scheffé procedure gives confidence band set \\(x\\) values, \\(g\\) . Using Scheffé procedure, family confidence interval simultaneous intervals \\(1-\\alpha\\).\\[\\hat{y}_h \\pm S \\cdot s \\{ \\mbox{pred} \\}\\]\n\\(S^2 = g F_{(1-\\alpha; g, n-2)}\\).","code":""},{"path":"simult.html","id":"bonferroni-procedure-1","chapter":"7 Simultaneous Inference","heading":"Bonferroni Procedure","text":"also produced Bonferroni intervals, determined using number \\(g\\) intervals interest (opposed Working-Hotelling cover entire range \\(x\\) values).\\[\\hat{y}_h \\pm B s \\{ \\mbox{pred} \\}\\]\n\\(B = t_{(1-\\alpha/2g; n-2)}\\).","code":""},{"path":"simult.html","id":"more","chapter":"7 Simultaneous Inference","heading":"7.4 More","text":"Note ALSM sections 4.6 (Inverse Predictions) 4.7 (Choice \\(x\\) values) provide additional information good model building. sections worth reading .","code":""},{"path":"simult.html","id":"reflection-questions-4","chapter":"7 Simultaneous Inference","heading":"7.5  Reflection Questions","text":"simultaneous CIs worry us (error perspective)?difference Bonferroni, Working-Hotelling, Scheffé adjustments?Bonferroni intervals larger two? smaller?say Bonferroni procedure general multiple comparisons procedures?\\(g\\) large, Working-Hotelling procedure preferred Bonferroni?","code":""},{"path":"simult.html","id":"ethics-considerations-3","chapter":"7 Simultaneous Inference","heading":"7.6  Ethics Considerations","text":"interval estimates often valuable report p-values?mean one adjustments “conservative?”sample representative population, can interval estimate misleading?words can used distinguish mean confidence intervals individual prediction intervals order better communication results?","code":""},{"path":"simult.html","id":"r-simultaneous-inference","chapter":"7 Simultaneous Inference","heading":"7.6.1 R: Simultaneous inference","text":"R packages simultaneous inference automatically. However, purposes covering, ’ll focus changing multiplier order control type error rate.Consider regression ames housing data. regression ln_price area home (sqft). See full analysis section @ref{ames-inf}.Let’s say want CI intercept slope parameters. case, two intervals created. Bonferroni controls type error dividing alpha error number intervals. multiplier t value adjusted alpha level degrees freedom linear model (n-2).intervals can created directly output tidy() function.Similarly, intervals mean response (confidence interval) individual response (prediction interval) use appropriate standard errors new multiplier. code Working-Hotelling multiplier Scheffe multiplier, 10 new observations.","code":"\names_lm <- ames_inf %>%\n  lm(price_ln ~ area, data = .) \n\names_lm %>%\n  tidy()## # A tibble: 2 × 5\n##   term         estimate std.error statistic p.value\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept) 11.1      0.0177        628.        0\n## 2 area         0.000614 0.0000114      53.9       0\names_lm %>%\n  glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.500         0.500 0.284     2901.       0     1  -461.  928.  946.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\names_df <- ames_lm %>%\n  glance() %>%\n  select(df.residual) %>%\n  pull()\n\names_df## [1] 2902\nnum_int_param <- 2  # for beta0 and beta1\n\n# Bonferroni:\ncrit_Bonf <- qt((1-.975)/num_int_param, ames_df)\ncrit_Bonf## [1] -2.24\names_lm %>%\n  tidy() %>%\n  select(term, estimate, std.error) %>%\n  mutate(lower.ci = estimate + crit_Bonf*std.error,\n         upper.ci = estimate - crit_Bonf*std.error)## # A tibble: 2 × 5\n##   term         estimate std.error  lower.ci  upper.ci\n##   <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) 11.1      0.0177    11.1      11.1     \n## 2 area         0.000614 0.0000114  0.000588  0.000639\n# Working-Hotelling\ncrit_WH <- sqrt(2*qf(.95, 2, ames_df))\ncrit_WH## [1] 2.45\n# Scheffe\nnum_int_pred <- 10 # if 10 new observations for prediction\ncrit_Sch <- sqrt(num_int_pred*qf(0.95, num_int_pred, ames_df))\ncrit_Sch## [1] 4.28"},{"path":"la.html","id":"la","chapter":"8 Regression using Matrices","heading":"8 Regression using Matrices","text":"Everything ’ve done far can written matrix form. Though might seem efficient use matrices simple linear regression, become clear multiple linear regression, matrices can powerful. ALSM chapter 5 contains lot matrix theory; main take away points chapter matrix theory applied regression setting. Please make sure read chapters / examples regression examples.","code":""},{"path":"la.html","id":"special-matrices","chapter":"8 Regression using Matrices","heading":"8.1 Special Matrices","text":"","code":""},{"path":"la.html","id":"regression-model","chapter":"8 Regression using Matrices","heading":"8.2 Regression Model","text":"","code":""},{"path":"la.html","id":"matrix-addition","chapter":"8 Regression using Matrices","heading":"8.2.1 Matrix Addition","text":"\\[\\begin{eqnarray*}\nY_i &=& E[Y_i] + \\epsilon_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} +\n\\begin{pmatrix} \\epsilon_1\\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& E[\\underline{Y}] + \\underline{\\epsilon}\\\\\n\\end{eqnarray*}\\]\nSimilarly,\n\\[\\begin{eqnarray*}\nY_i &=& \\hat{Y}_i + e_i\\\\\n&&\\\\\n\\begin{pmatrix} Y_1\\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} &=&\n\\begin{pmatrix} \\hat{Y}_1\\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} +\n\\begin{pmatrix} e_1\\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix}\\\\\n&&\\\\\n\\underline{Y} &=& \\underline{\\hat{Y}} + \\underline{e}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-multiplication","chapter":"8 Regression using Matrices","heading":"8.3 Matrix Multiplication","text":"","code":""},{"path":"la.html","id":"example","chapter":"8 Regression using Matrices","heading":"8.3.1 Example","text":"Consider multiplying \\(r \\times c\\) matrix \\(c \\times s\\) matrix. interior dimensions must always . resulting matrix always \\(r \\times s\\). element \\(^{th}\\) row \\(j^{th}\\) column given :\n\\[\\begin{eqnarray*}\n\\sum_{k=1}^c a_{ik} b_{kj}\\\\\nAB &=& \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{pmatrix}\n\\begin{pmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} a_{11} b_{11} + a_{12} b_{21} + a_{13}b_{31} & a_{11} b_{12} + a_{12} b_{22} + a_{13}b_{32} \\\\ a_{21} b_{11} + a_{22} b_{21} + a_{23}b_{31} & a_{21} b_{12} + a_{22} b_{22} + a_{23}b_{32}\n\\end{pmatrix}\\\\\n&&\\\\\nAB &=& \\begin{pmatrix} 3 & -1 & 0 \\\\ 0 & 1 & 1 \\\\ -2 & 0 & 1 \\end{pmatrix}\n\\begin{pmatrix} 1 & 1\\\\ -1 & 2 \\\\ 0 & -1 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} 4 & 1\\\\ -1 & 1 \\\\ -2 & -3 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"and-in-the-linear-regression-context","chapter":"8 Regression using Matrices","heading":"and in the linear regression context…","text":"\\[\\begin{eqnarray*}\nE[Y_i] &=& \\beta_0 + \\beta_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} E[Y_1]\\\\ E[Y_2] \\\\ \\vdots \\\\ E[Y_n] \\end{pmatrix} &=&\n\\begin{pmatrix} 1 & X_1\\\\ 1 & X_2 \\\\ \\vdots \\\\ 1 & X_n \\end{pmatrix}\n\\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\\\\\n&&\\\\\nE[\\underline{Y}] &=& X \\underline{\\beta}\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\underline{Y}^t \\underline{Y} &=& \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\sum_{=1}^n Y_i^2\\\\\n&&\\\\\nX^t X &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix}= \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\nX^t \\underline{Y} &=& \\begin{pmatrix} 1 & 1 & \\cdots & 1\\\\ X_1 & X_2 & \\cdots & X_n \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"matrix-inverses-in-the-regression-context","chapter":"8 Regression using Matrices","heading":"8.4 Matrix Inverses in the Regression Context","text":"","code":""},{"path":"la.html","id":"matrix-inverses","chapter":"8 Regression using Matrices","heading":"8.4.1 Matrix Inverses","text":"\\(n \\times n\\) matrix \\(\\) called invertible exists \\(n \\times n\\) matrix \\(B\\) \n\\[\\begin{eqnarray*}\nB = B = I_n\n\\end{eqnarray*}\\]\n\n\\(B\\) called inverse \\(\\) typically denoted \\(B = ^{-1}\\). (Note, inverses exist square matrices non-zero determinants.)","code":""},{"path":"la.html","id":"example-5","chapter":"8 Regression using Matrices","heading":"8.4.2 Example","text":"\\[= \\begin{pmatrix}  &  b \\\\ c  &  d \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} d / (ad - bc)  &  -b / (ad - bc) \\\\ -c / (ad - bc)  &  / (ad - bc) \\end{pmatrix}\\]determinant given \\(D = ad - bc\\).\n\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} d / (ad - bc) & -b / (ad - bc) \\\\ -c / (ad - bc) & / (ad - bc) \\end{pmatrix} \\begin{pmatrix} & b \\\\ c & d \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (ad - bc) / (ad - bc) & (bd - bd) / (ad - bc) \\\\ (-ac + ac) / (ad - bc) & (ad - bc) / (ad - bc) \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]\\[= \\begin{pmatrix} 3  &  2 \\\\ 1  &  6 \\end{pmatrix}\\]\\[ ^{-1} = \\begin{pmatrix} 6 / 16  &  -2 / 16 \\\\ -1 / 16  &  3 / 16 \\end{pmatrix} \\]\\[\\begin{eqnarray*}\n^{-1} &=& \\begin{pmatrix} 6 / 16 & -2 / 16 \\\\ -1 / 16 & 3 / 16 \\end{pmatrix}  \\begin{pmatrix} 3 & 2 \\\\ 1 & 6 \\end{pmatrix}\\\\\n&=& \\begin{pmatrix} (18-2)/16 & (12-12)/16 \\\\ (-3+3)/16 & (-2+18)/16 \\end{pmatrix}\\\\\n&=& I_2\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"variance-of-coefficients","chapter":"8 Regression using Matrices","heading":"8.4.3 Variance of Coefficients","text":"","code":""},{"path":"la.html","id":"what-is-a-variance-matrix","chapter":"8 Regression using Matrices","heading":"What is a Variance Matrix?","text":"Consider two random variables, \\(U\\) \\(W\\). ’d typically \\(n\\) observations variable, resulting data vectors \\(\\underline{U}\\) \\(\\underline{W}\\). variance-covariance matrix describing variability relationship \\(U\\) \\(W\\) given :\n\\[\\begin{eqnarray*}\n\\mbox{var}\\{\\underline{U},\\underline{W} \\} &=& \\begin{pmatrix} \\sigma_U^2  & \\sigma_{U,W} \\\\ \\sigma_{W,U} & \\sigma_W^2 \\end{pmatrix}\\\\\n\\mbox{} \\sigma_U^2 &=& \\mbox{ variance } U  \\mbox{ (number)}\\\\\n\\sigma_W^2 &=& \\mbox{ variance } V  \\mbox{ (number)}\\\\\n\\sigma_{U,W} &=& \\sigma_{W,U} = \\mbox{ covariance } U \\mbox{ } W  \\mbox{ (number)} \\\\\n\\end{eqnarray*}\\]\norder find variance matrix regression coefficients, use matrix notation.\n\\[\\begin{eqnarray*}\n\\mbox{Recall:    }  X^t X &=&  \\begin{pmatrix} n & \\sum_{=1}^n X_i \\\\ \\sum_{=1}^n X_i & \\sum_{=1}^n X_i^2 \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{,    } D &=& n \\sum X_i^2 - (\\sum X_i)^2 = n \\sum(X_i - \\overline{X})^2\\\\\n&&\\\\\n(X^t X)^{-1} &=&  \\begin{pmatrix} \\frac{\\sum X_i^2}{n \\sum(X_i - \\overline{X})^2} & \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum_{=1}^n X_i}{n \\sum(X_i - \\overline{X})^2} & \\frac{n}{n \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=&  \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b}\\} &=& \\sigma^2  \\cdot (X^t X)^{-1}\\\\\n&=& \\begin{pmatrix} \\sigma^2_{b_0} & \\sigma_{b_0, b_1} \\\\ \\sigma_{b_1, b_0} & \\sigma^2_{b_1} \\end{pmatrix}\\\\\nSE^2\\{\\underline{b}\\} &=& MSE  \\cdot (X^t X)^{-1}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"estimating-coefficients","chapter":"8 Regression using Matrices","heading":"Estimating Coefficients","text":"Recall equations come differentiating sum squared residuals respect \\(\\beta_0\\) \\(\\beta_1\\):\\[\\begin{eqnarray*}\nn b_0 + b_1 \\sum X_i &=& \\sum Y_i\\\\\nb_0 \\sum X_i + b_1 \\sum X_i^2 &=& \\sum X_i Y_i\\\\\n&&\\\\\n\\begin{pmatrix} n & \\sum X_i \\\\ \\sum X_i & \\sum X_i^2 \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix} &=& \\begin{pmatrix} \\sum Y_i \\\\ \\sum X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n(X^t X) \\underline{b} &=& X^t \\underline{Y}\\\\\n\\underline{b} &=& (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&&\\\\\n\\mbox{var}\\{\\underline{b} \\} &=& (X^t X)^{-1} X^t \\sigma^2 X (X^t X)^{-1}\\\\\n&=& \\sigma^2 \\cdot (X^t X)^{-1}\\\\\n&&\\\\\n\\mbox{checking:}&&\\\\\n(X^t X)^{-1} (X^t \\underline{Y}) &=& \\begin{pmatrix} \\frac{1}{n} + \\frac{\\overline{X}^2}{\\sum(X_i - \\overline{X})^2} & \\frac{-\\overline{X}}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\overline{X}}{ \\sum(X_i - \\overline{X})^2} & \\frac{1}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix} \\begin{pmatrix} \\sum_{=1}^n Y_i \\\\ \\sum_{=1}^n X_i Y_i \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\frac{\\sum Y_i}{n} + \\frac{\\sum Y_i \\overline{X}^2}{\\sum(X_i - \\overline{X})^2} + \\frac{-\\sum X_i Y_i (\\overline{X})}{\\sum(X_i - \\overline{X})^2} \\\\ \\frac{-\\sum Y_i (\\overline{X})}{ \\sum(X_i - \\overline{X})^2} + \\frac{\\sum X_i Y_i}{ \\sum(X_i - \\overline{X})^2} \\end{pmatrix}\\\\\n&&\\\\\n&=& \\begin{pmatrix} \\overline{Y} - b_1 \\overline{X} \\\\ \\frac{\\sum Y_i (X_i - \\overline{X})}{\\sum(X_i - \\overline{X})^2} \\end{pmatrix} = \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"fitted","chapter":"8 Regression using Matrices","heading":"8.5 Fitted Values","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_i &=& b_0 + b_1 X_i\\\\\n&&\\\\\n\\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix} &=& \\begin{pmatrix} 1 & X_1 \\\\ 1 & X_2 \\\\ \\vdots & \\vdots \\\\ 1 & X_n \\end{pmatrix} \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}\\\\\n&&\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n &=& X (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&=& H \\underline{Y}\\\\\n\\mbox{\"hat\" matrix: } H &=& X (X^t X)^{-1} X^t\n\\end{eqnarray*}\\]\ncall \\(H\\) hat matrix takes \\(\\underline{Y}\\) puts hat . Note predicted values simply linear combinations response variable (\\(Y\\)) coefficients explanatory variables (\\(X\\)).","code":""},{"path":"la.html","id":"residuals-1","chapter":"8 Regression using Matrices","heading":"8.6 Residuals","text":"\\[\\begin{eqnarray*}\ne_i &=& Y_i - \\hat{Y}_i\\\\\n\\begin{pmatrix} e_1 \\\\ e_2 \\\\ \\vdots \\\\ e_n \\end{pmatrix} &=& \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\Y_n \\end{pmatrix} - \\begin{pmatrix} \\hat{Y}_1 \\\\ \\hat{Y}_2 \\\\ \\vdots \\\\ \\hat{Y}_n \\end{pmatrix}\\\\\n\\underline{e} &=& \\underline{Y} - \\hat{\\underline{Y}}\\\\\n&=& \\underline{Y} - X \\underline{b}\\\\\n&=& \\underline{Y} - H \\underline{Y}\\\\\n&=& (- H) \\underline{Y}\\\\\n&&\\\\\n\\mbox{var}\\{ \\underline{e} \\} &=& \\mbox{var}\\{ (-H) \\underline{Y} \\}\\\\\n&=& (- H) \\mbox{var}\\{ \\underline{Y} \\}\\\\\n&=& (- H) \\cdot \\sigma^2 \\cdot (- H)^t\\\\\n&=& \\sigma^2 \\cdot (- H) (-H^t)\\\\\n&=& \\sigma^2 \\cdot (- H - H^t + HH^t)\\\\\n&=&  \\sigma^2 \\cdot (-H)\\\\\nSE^2(\\underline{e}) &=& MSE \\cdot (-H)\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"analysis-of-variance","chapter":"8 Regression using Matrices","heading":"8.7 ANalysis Of VAriance","text":"\\[\\begin{eqnarray*}\nSSTO &=& \\underline{Y}^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix} \\begin{pmatrix} 1 & 1 & \\cdots &  1\\\\ 1 & 1 & \\cdots & 1 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & 1 & \\cdots & 1   \\end{pmatrix} \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\begin{pmatrix} Y_1 & Y_2 & \\cdots & Y_n \\end{pmatrix}  \\begin{pmatrix} \\sum Y_i \\\\ \\sum Y_i \\\\ \\vdots \\\\ \\sum Y_i \\end{pmatrix} \\\\\n&=& \\sum Y_i ^2 - \\frac{1}{n} \\sum Y_i \\sum Y_i\\\\\n&=& \\sum Y_i ^2 - n \\overline{Y}^2\\\\\n\\mbox{note: } \\sum(Y_i - \\overline{Y})^2 &=& \\sum (Y_i^2 - 2Y_i \\overline{Y} + \\overline{Y}^2)\\\\\n&=& \\sum Y_i^2 -2\\overline{Y}\\sum Y_i + n \\overline{Y}^2\\\\\n&=& \\sum Y_i^2 - n \\overline{Y}^2\\\\\n&&\\\\\nSSE &=& \\underline{Y}^t \\underline{Y} - \\underline{b}^t X^t \\underline{Y}\\\\\n&&\\\\\nSSR &=& \\underline{b}^t X^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"prediction-of-new-observations","chapter":"8 Regression using Matrices","heading":"8.8 Prediction of New Observations","text":"\\[\\begin{eqnarray*}\n\\hat{Y}_h &=& \\underline{X}^t_h \\underline{b}\\\\\n\\mbox{var}\\{ \\hat{Y}_h \\} &=& \\underline{X}_h^t \\mbox{var}\\{\\underline{b} \\} \\underline{X}_h\\\\\n&=& \\sigma^2 \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\nSE^2\\{\\hat{Y}_h \\} &=& MSE \\cdot \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h\\\\\n&&\\\\\n&&\\\\\nSE^2 \\{\\mbox{pred} \\} = SE^2\\{\\hat{Y}_{h(new)} \\} &=& MSE \\cdot (1 + \\underline{X}_h^t (X^t X)^{-1} \\underline{X}_h)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"la.html","id":"reflection-questions-5","chapter":"8 Regression using Matrices","heading":"8.9  Reflection Questions","text":"normal errors linear model written matrix form?X matrix added column ones?Using matrix notation, normal equations used solve least squares estimates \\(\\beta_0\\) \\(\\beta_1\\)?covariance term? mean variables correlated (e.g., \\(b_0\\) \\(b_1\\))?“hat” matrix (\\(H\\)) named?","code":""},{"path":"la.html","id":"ethics-considerations-4","chapter":"8 Regression using Matrices","heading":"8.10  Ethics Considerations","text":"sampling distributions \\(b_0\\) \\(b_1\\) correlated?ways writing linear model matrix notation make extension explanatory variables easier?fundamental differences SLR model written matrix notation (opposed indexing elements equation)?","code":""},{"path":"la.html","id":"r-matrices","chapter":"8 Regression using Matrices","heading":"8.11 R: Matrices","text":"","code":""},{"path":"la.html","id":"addition","chapter":"8 Regression using Matrices","heading":"8.11.1 Addition","text":"Adding matrices gives ’d likely expect.","code":"\nmatrix1 <- matrix(c(1:12),ncol=4, byrow=T)\nmatrix2 <- matrix(seq(2,24,by=2),ncol=4, byrow=T)\n\nmatrix1##      [,1] [,2] [,3] [,4]\n## [1,]    1    2    3    4\n## [2,]    5    6    7    8\n## [3,]    9   10   11   12\nmatrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    4    6    8\n## [2,]   10   12   14   16\n## [3,]   18   20   22   24\nmatrix1 + matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    3    6    9   12\n## [2,]   15   18   21   24\n## [3,]   27   30   33   36"},{"path":"la.html","id":"multiplication","chapter":"8 Regression using Matrices","heading":"8.11.2 Multiplication","text":"* produces element element multiplication.%*% matrix multiplication. Note error can’t multiply \\(3 \\times 4\\) \\(3 \\times4\\) matrix.either matrices transposed, conformable, able multiply .Note products symmetric matrix1 = \\(2 \\cdot\\) matrix2.","code":"\nmatrix1 * matrix2##      [,1] [,2] [,3] [,4]\n## [1,]    2    8   18   32\n## [2,]   50   72   98  128\n## [3,]  162  200  242  288\nmatrix1 %*% matrix2## Error in matrix1 %*% matrix2: non-conformable arguments\nmatrix1 %*% t(matrix2)##      [,1] [,2] [,3]\n## [1,]   60  140  220\n## [2,]  140  348  556\n## [3,]  220  556  892\nt(matrix1) %*% matrix2##      [,1] [,2] [,3] [,4]\n## [1,]  214  244  274  304\n## [2,]  244  280  316  352\n## [3,]  274  316  358  400\n## [4,]  304  352  400  448"},{"path":"la.html","id":"taking-inverses","chapter":"8 Regression using Matrices","heading":"8.11.3 Taking Inverses","text":"function inverting matrices R solve(). Remember solve() can works square matrices non-zero determinants.Multiplying matrix inverse results identity matrix.","code":"\nmatrix3 <- matrix(c(5,7,1,4,3,6,2,0,8), ncol=3, byrow=T)\nmatrix3##      [,1] [,2] [,3]\n## [1,]    5    7    1\n## [2,]    4    3    6\n## [3,]    2    0    8\nsolve(matrix3)##        [,1]   [,2] [,3]\n## [1,] -0.923  2.154 -1.5\n## [2,]  0.769 -1.462  1.0\n## [3,]  0.231 -0.538  0.5\nmatrix3 %*% solve(matrix3)##          [,1]      [,2]     [,3]\n## [1,] 1.00e+00 -2.22e-16 1.11e-16\n## [2,] 4.44e-16  1.00e+00 0.00e+00\n## [3,] 4.44e-16  0.00e+00 1.00e+00"},{"path":"la.html","id":"concatenating-1s-for-the-intercept","chapter":"8 Regression using Matrices","heading":"8.11.4 Concatenating 1s for the intercept","text":"Let’s say explanatory variable called xvar","code":"\nset.seed(7447)\nxvar <- rnorm(12, 47, 3)\nxvar##  [1] 47.3 43.1 47.1 48.0 49.7 45.2 47.7 47.1 47.9 48.9 43.4 44.1\nrep(1,12)  # repeats the number 1, 12 times##  [1] 1 1 1 1 1 1 1 1 1 1 1 1\nXmatrix <- cbind(int = rep(1,12), xvar)\nXmatrix##       int xvar\n##  [1,]   1 47.3\n##  [2,]   1 43.1\n##  [3,]   1 47.1\n##  [4,]   1 48.0\n##  [5,]   1 49.7\n##  [6,]   1 45.2\n##  [7,]   1 47.7\n##  [8,]   1 47.1\n##  [9,]   1 47.9\n## [10,]   1 48.9\n## [11,]   1 43.4\n## [12,]   1 44.1"},{"path":"mlr.html","id":"mlr","chapter":"9 Multiple Linear Regression","heading":"9 Multiple Linear Regression","text":"","code":""},{"path":"mlr.html","id":"basic-model-set-up","chapter":"9 Multiple Linear Regression","heading":"9.1 Basic Model Set-Up","text":"looking volume riders bike trail, can use data collected understand relationships deeper level. trail Massachusetts, expected relationship positive: higher temp riders expect. However, volume also related whether weekend. Indeed, think volume riders function temperature day week. Thus two predictor variables model.","code":""},{"path":"mlr.html","id":"notation","chapter":"9 Multiple Linear Regression","heading":"9.1.1 Notation","text":"Consider \\(n\\) observations. response variable \\(^{th}\\) individual, denoted \\(Y_i\\) , observed. variation remaining \\(Y_i\\) isn’t explained predictors also remain , denoted \\(\\epsilon_i\\) called random error. Since now one predictor, additional subscript added \\(X\\), denoting value \\(k^{th}\\) predictor variable \\(^{th}\\) individual \\(X_{ik}\\). Thus model now:\n\\[\\begin{eqnarray*}\nY_i&=&\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}+ \\cdots + \\beta_{p-1}X_{,p-1} + \\epsilon_i\\\\\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+ \\cdots + \\beta_{p-1}X_{p-1}\\\\\nY_i&=&b_0+b_1X_{i1}+b_2X_{i2}+ \\cdots + b_{p-1}X_{,p-1} + e_i\\\\\n\\hat{Y}&=&b_0+b_1X_{1}+b_2X_{2}+ \\cdots + b_{p-1}X_{p-1}\\\\\n&&\\\\\nE[\\underline{Y}] &=& X \\underline{\\beta}\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n\\end{eqnarray*}\\]\nRail Trails example, \\(X_{i1}\\) might denote volume riders \\(^{th}\\) day, \\(X_{i2}\\) denote indicator variable whether day weekend week day.","code":""},{"path":"mlr.html","id":"fitting-the-model","chapter":"9 Multiple Linear Regression","heading":"9.1.2 Fitting the Model","text":"estimate coefficients, use principle , least\nsquares. , minimize\n\\[\\sum_{=1}^n(Y_i-(\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2} + \\cdots + \\beta_{p-1}X_{,p-1}))^2\\]\ninterested finding least squares estimates parameters model \\(b_i\\). , something looks like\n\\[(\\underline{Y}-\\mathbf{X}\\underline{\\beta})^t(\\underline{Y}-\\mathbf{X}\\underline{\\beta})\\]\ntrying minimize (sum squared residuals). Calculus gives:\n\\[\\mathbf{X}^t\\underline{Y}-\\mathbf{X}^t\\mathbf{X}\\mathbf{\\beta}=0,\\]\nsolving unknown \\(\\underline{\\beta}\\) gives:\n\\[\\underline{b}=(\\mathbf{X}^t\\mathbf{X})^{-1}(\\mathbf{X}^t\\underline{Y}).\\]\ntranspose concession multiplication division work slightly differently matrix context. inverse, denoted power -1, provides way solve equation.\nThus,\n\\[\\hat{\\underline{Y}}=\\mathbf{X}\\underline{b}=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\underline{Y}:=\\mathbf{H}\\underline{Y}\\]\n\n\\[\\mathbf{H}=\\mathbf{X}(\\mathbf{X}^t\\mathbf{X})^{-1}\\mathbf{X}^t\\]\nhat matrix (multiplying H \n\\(\\underline{Y}\\) puts hat ).\nhat matrix come learning leverage.","code":""},{"path":"mlr.html","id":"types-of-multiple-regression","chapter":"9 Multiple Linear Regression","heading":"9.1.3 Types of Multiple Regression","text":"multiple linear regression model useful variety situations. discussed . example first type:\\(p-1\\) predictor variables: say \\(p-1\\) instead \\(p\\) \nincluding intercept \\(p\\) parameters need \nestimation.\nprevious example, \\(p=3\\), order estimate \\(\\beta_0\\),\n\\(\\beta_1\\) \\(\\beta_2\\). two independent variables, high temperature weekday status.Qualitative Predictor Variables: including categorical variable model, must written one () binary variables. example,\\[\\begin{eqnarray*}X_2=\\begin{aligned} &0& \\quad &\\mbox{ weekend}&\\\\\n&1&\\quad &\\mbox{ weekday}&\\end{aligned}\\end{eqnarray*}\\]general, qualitative variable \\(k\\) levels, \\(k-1\\) “dummy’’ (.e., binary) variables must included. instance, variables school year, (2013, 2014, 2015, 2016), 3 variables (coded , example: 1=2013, 0=2013). given observation, three dummy variables zero, model know observation took place school year coded .Transformed Variables: simple linear regression, often good idea transform variables ensure model technical conditions hold.Interaction Effects: model “additive” means response function predictors added together. variables interact . interaction model can form\n\\[E[Y]=\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+\\beta_3X_{1}X_{2}\\]\ninteraction multiplicative. Often idea fitting interaction comes sort knowledge variables .\nAssume \\(E[Y]\\) average volume riders days \\(X_{1}\\) high temp, \\(X_{2}\\) whether weekday. interaction model provides way break model particular groups, model provides different linear model weekdays weekends.Weekday=0\n\\[\\begin{eqnarray*}\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2 0+\\beta_3X_{1} 0\\\\\n&=&\\beta_0+\\beta_1X_{1}\\\\\n\\end{eqnarray*}\\]Weekday=1\n\\[\\begin{eqnarray*}\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2 1+\\beta_3X_{1} 1\\\\\n&=&(\\beta_0 + \\beta_2) +(\\beta_1 + \\beta_3) X_{1}\\\\\n\\end{eqnarray*}\\]additive model (.e., interactions) states dummy variables move line . volume ridership different different days depend high temp, additive component included. day week changes relationship high temp volume, interaction term included.\nVariables interact effect one predictor variable depends levels predictor variables.\nAnother way think interaction whether change \\(E[Y]\\) change one variables (e.g., \\(X_1\\)) mediated another variable (e.g., \\(X_2\\)).\\[\\begin{eqnarray*}\nE[Y]=\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+\\beta_3X_{1}X_{2}\\\\\n\\frac{\\partial E[Y]}{\\partial X_1} = \\beta_1 + \\beta_3 X_2\\\\\n\\end{eqnarray*}\\]Polynomial Regression: response variable might function polynomial predictor giving rise polynomial model:\n\\[Y_i=\\beta_0+\\beta_1X_i+\\beta_2X_i^2+\\epsilon_i\\]\nrepresents \\(Y\\) quadratic function \\(X\\).term linear model therefore refer response surface, rather fact model linear parameters. Though fitting hyper-plane data, think surface looks like terms original variables, may highly non-linear due transformations \nforth.","code":""},{"path":"mlr.html","id":"example-thermometers","chapter":"9 Multiple Linear Regression","heading":"Example: thermometers","text":"Consider new dataset. data collected Michael Ernst St. Cloud University Minnesota (Polar Vortex January 2019).late fall early winter, temperature dropped (tends MN), Michael started get suspicious thermometer wasn’t entirely accurate. , put another thermometer trusted outside near new one read temperature window. wrote temperature every throughout December January.two variables: Temp, actual temperature (based trusted thermometer), Reading, reading suspect thermometer.\nFigure 9.1: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.2: scatterplot looks linear… residual plot doesn’t!\nHopefully, transforming data help. figure seems like square root Reading log Temp might help. Let’s try , first ’ll shift (get rid negative numbers) arbitrarily 35 degrees.\nFigure 9.3: scatterplot looks linear… residual plot doesn’t!\n\nFigure 9.4: scatterplot looks linear… residual plot doesn’t!\nDoesn’t seem like transformations going work. square term added? still linear model? (Yes!) residuals better? (Yes!)quadratic term alone (without linear term) doesn’t help model fit model forces linear coefficient zero. making linear part zero, force vertex (parabola) X=0 doesn’t make sense model fit. Indeed, typically quadratic term without linear term good idea () curved relationship constant errors vertex plot, /(b) really believe reason \\(Y\\) (linear) function \\(X^2\\).","code":"\ntemperature %>%\n  ggplot(aes(x = Temp, y = Reading)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\ntemperature %>%\n  lm(Reading ~ Temp, data = .) %>%\n  augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept = 0)\nlin_mod <- temperature %>%\n  lm(Reading ~ Temp, data = .)\n\nquad_mod_1 <- temperature %>%\n  lm(Reading ~ temp_sq, data = .)\n\nquad_mod_2 <- temperature %>%\n  lm(Reading ~ Temp + temp_sq, data = .)\n\nlin_mod %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    -4.93   0.138       -35.7 8.50e-46\n## 2 Temp            1.19   0.00867     137.  7.94e-85\nquad_mod_1 %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic    p.value\n##   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n## 1 (Intercept) -14.4      2.79        -5.16 0.00000229\n## 2 temp_sq       0.0241   0.00727      3.32 0.00147\nquad_mod_2 %>% tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) -4.12     0.144       -28.7  2.33e-39\n## 2 Temp         1.21     0.00690     176.   5.36e-91\n## 3 temp_sq     -0.00295  0.000374     -7.90 3.65e-11"},{"path":"mlr.html","id":"revisiting-other-important-definitions","chapter":"9 Multiple Linear Regression","heading":"9.1.4 Revisiting Other Important Definitions","text":"solving regression coefficients \\((\\underline{b})\\) involves exact matrix algebra, pieces involved linear regression model given using matrix notation.\n\\[\\begin{eqnarray*}\n\\underline{b} &=& (X^t X)^{-1} X^t \\underline{Y}\\\\\n\\underline{e} &=& \\underline{Y} - \\hat{\\underline{Y}}\\\\\n&=& \\underline{Y} - X \\underline{b}\\\\\n&=& (- H) \\underline{Y}\\\\\n\\underline{\\hat{Y}} &=& X \\underline{b}\\\\\n &=& X (X^t X)^{-1} (X^t \\underline{Y})\\\\\n&=& H \\underline{Y}\\\\\ns^2(\\underline{e}) &=& MSE \\cdot (-H)\\\\\n\\sigma^2(\\underline{e}) &=& \\sigma^2 \\cdot (-H)\\\\\ns^2(\\underline{\\epsilon}) &=& MSE \\cdot \\\\\n\\sigma^2(\\underline{\\epsilon}) &=& \\sigma^2 \\\\\n\\end{eqnarray*}\\]Equivalently, components ANOVA table remain , slight change now degrees freedom generalized account fact estimating \\(p\\) parameters.\\[\\begin{eqnarray*}\nSSR &=& \\sum (\\hat{Y}_i - \\overline{Y})^2 = \\underline{b}^t X^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\nSSE &=& \\sum (Y_i - \\hat{Y}_i)^2 = \\underline{Y}^t \\underline{Y} - \\underline{b}^t X^t \\underline{Y}\\\\\nSSTO &=& \\sum (Y_i - \\overline{Y})^2 = \\underline{Y}^t \\underline{Y} - \\bigg(\\frac{1}{n} \\bigg) \\underline{Y}^t J \\underline{Y}\\\\\n\\end{eqnarray*}\\]Note (\\(p=3\\), two explanatory variables):\n\\[\\begin{eqnarray*}\nE[MSE] &=& \\sigma^2\\\\\nE[MSR] &=& \\sigma^2 + 0.5[\\beta_1^2 \\sum(X_{i1} - \\overline{X}_1)^2 + \\beta_2^2 \\sum(X_{i2} - \\overline{X}_2)^2 + 2 \\beta_1 \\beta_2 \\sum(X_{i1} - \\overline{X}_1)(X_{i2} - \\overline{X}_2)]\n\\end{eqnarray*}\\]","code":""},{"path":"mlr.html","id":"inference","chapter":"9 Multiple Linear Regression","heading":"9.2 Inference","text":"","code":""},{"path":"mlr.html","id":"f-test","chapter":"9 Multiple Linear Regression","heading":"9.2.1 F-test","text":"F-test simple linear regression, can now generalized test addressing whether non-intercept coefficients simultaneously zero. test asks whether explanatory variables set add anything model terms predicting response variable.\\[\\begin{eqnarray*}\nH_0:&& \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0\\\\\nH_a:&& \\mbox{ } \\beta_k = 0 \\mbox{ (still might zero)}\n\\end{eqnarray*}\\], measure ratio MSR MSE decide whether regression error components measuring quantity (residual error).\n\\[\\begin{eqnarray*}\nF = \\frac{MSR}{MSE} \\sim F_{(p-1, n-p)}  \\mbox{ $H_0$ true (!)}\n\\end{eqnarray*}\\]\nRemember MSE always estimates \\(\\sigma^2\\), MSR estimates \\(\\sigma^2\\) \\(\\beta_k\\) coefficients simultaneously equal zero.","code":""},{"path":"mlr.html","id":"coefficient-of-multiple-determination","chapter":"9 Multiple Linear Regression","heading":"9.2.2 Coefficient of Multiple Determination","text":"Recall measured proportion variability explained linear model using \\(R^2\\). interpretation now \\(p-1\\) predictors. \\[ R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\] :\n\\[\\begin{eqnarray*}\nR^2 &=& 0 \\mbox{ } b_k = 0 \\ \\ \\forall k=1, \\ldots, p-1\\\\\nR^2 &=& 1 \\mbox{ } \\hat{Y}_i = Y_i \\ \\ \\forall \n\\end{eqnarray*}\\]now situation adding variables always increases \\(R^2\\). , add explanatory variables model, \\(R^2\\) value increases.\\(\\beta_k=0 \\ \\ \\forall k\\), \\(E[MSR] = \\sigma^2 \\rightarrow E[SSR] = \\sigma^2(p-1)\\), also \\(E[SSTO] = \\sigma^2(n-1) \\rightarrow R^2 \\approx (p-1)/(n-1)\\).","code":""},{"path":"mlr.html","id":"adjusted-r2","chapter":"9 Multiple Linear Regression","heading":"Adjusted \\(R^2\\)","text":"account problem \\(R^2\\) increasing number variables, compare mean squares instead sums squares. Now, value longer increasing number variables trade-reducing errors (SSE) losing degree freedom. \\[R^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)}{(n-p)} \\frac{SSE}{SSTO}\\]","code":""},{"path":"mlr.html","id":"inference-about-regression-parameters","chapter":"9 Multiple Linear Regression","heading":"9.2.3 Inference about Regression Parameters","text":"","code":""},{"path":"mlr.html","id":"coefficients","chapter":"9 Multiple Linear Regression","heading":"Coefficients","text":"know \n\\[\\begin{eqnarray*}\nvar\\{ \\underline{b} \\} &=& \\sigma^2 (X^t X)^{-1}\\\\\nSE^2\\{ \\underline{b} \\} &=& MSE (X^t X)^{-1}\\\\\n\\end{eqnarray*}\\]\ncan use estimate SE create test statistic t distribution null hypothesis true (note now estimating \\(p\\) parameters, degrees freedom \\(n-p\\)).\n\\[\\begin{eqnarray*}\n\\frac{b_k - \\beta_k}{SE\\{b_k\\}} \\sim t_{(n-p)}\n\\end{eqnarray*}\\]\n\\((1-\\alpha)100\\%\\) CI \\(\\beta_k\\) given \\[b_k \\pm t_{(1-\\alpha/2, n-p)} s\\{b_k\\}\\]\nNote t-test done separately \\(\\beta\\) coefficient. say estimating MSE variables model. test asks effect removing variable hand. testing interpretation regression coefficients done keeping variables constant.","code":""},{"path":"mlr.html","id":"linear-combinations-of-coefficients","chapter":"9 Multiple Linear Regression","heading":"Linear Combinations of Coefficients","text":"Periodically, question interest related linear combination coefficients. example, might interested testing whether coefficient spring statistically different coefficient fall [\\(H_0: \\beta_1 = \\beta_2\\)]. Let\n\\[\\begin{eqnarray*}\n\\gamma &=& c_0 \\beta_0 + c_1 \\beta_1 + \\ldots + c_p \\beta_p\\\\\ng &=& c_0 b_0 + c_1 b_1 + \\ldots + c_p b_p\\\\\nvar(g) &=& c_0^2 var\\{b_0\\} + c_1^2 var\\{b_1\\} + \\ldots + c_p^2 var\\{b_p\\} + 2c_0c_1 cov(b_0, b_1) + 2 c_0 c_2 cov(b_0, b_2) + \\ldots + 2c_{p-1}c_p cov(b_{p-1}, b_p)\\\\\n\\end{eqnarray*}\\]\nestimate difference SE, t-statistic (create CI) provides formal inference coefficients. Note function vcov() estimates variances covariance coefficients.\n\\[\\begin{eqnarray*}\n\\hat{var}(b_1 - b_2) &=& (1)^2 SE^2\\{b_1\\} + (-1)^2 SE^2\\{ b_2\\} + 2(1)(-1)\\hat{cov}(b_1, b_2)\\\\\n&=& 889 + 1862 -  2*604 = 1543\\\\\nH_0: && \\beta_1 = \\beta_2\\\\\nt-stat &=& \\frac{(-50.1 - (-126.8)) - 0}{ \\sqrt{1543}} = 1.952\\\\\np-value &=& 2 * P(t_{87} \\geq 1.952) = 0.054\n\\end{eqnarray*}\\]\np-value borderline, certainly strong evidence say fall spring significantly different model. tidy() output shows fall significantly different summer (baseline) spring may may different (p-value < 0.1). Note: days measured winter.","code":"\nRailTrail %>%\n  lm(volume ~ spring + fall, data = .) %>%\n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    422.       24.6     17.2  1.13e-29\n## 2 spring         -50.2      29.8     -1.68 9.57e- 2\n## 3 fall          -127.       43.2     -2.94 4.23e- 3\nRailTrail %>%\n  lm(volume ~ spring + fall, data = .) %>%\n  vcov()##             (Intercept) spring fall\n## (Intercept)         604   -604 -604\n## spring             -604    889  604\n## fall               -604    604 1862"},{"path":"mlr.html","id":"mean-response","chapter":"9 Multiple Linear Regression","heading":"Mean Response","text":", using linear algebra simple linear regression:\\[\\begin{eqnarray*}\nvar\\{\\hat{Y}_h\\} &=& \\sigma^2 X_h^t (X^t X)^{-1} X_h\\\\\ns^2\\{\\hat{Y}_h\\} &=& MSE X_h^t (X^t X)^{-1} X_h\n\\end{eqnarray*}\\]\nallows us create \\((1-\\alpha)100\\%\\) CI \\(E[Y_h] = X_h^t \\underline{\\beta}\\): \\[\\hat{Y}_h \\pm t_{(1-\\alpha/2, n-p)} s\\{\\hat{Y}_h\\}\\]\n\n\\[\\begin{eqnarray*}\nX_h^t &=& (1 \\ \\ X_{h 1} \\ \\ X_{h  2} \\ \\ldots \\ X_{h  p-1})\\\\\n\\hat{Y}_h &=& X_h^t \\underline{b}\n\\end{eqnarray*}\\]","code":""},{"path":"mlr.html","id":"future-predicted-response","chapter":"9 Multiple Linear Regression","heading":"Future / Predicted Response","text":"new observation uses derivation mean response, add variability observations around line.\n\\[\\begin{eqnarray*}\nvar\\{ \\mbox{pred} \\} = var\\{\\hat{Y}_{h(new)}\\} &=& \\sigma^2 (1+ X_h^t (X^t X)^{-1} X_h)\\\\\nSE^2\\{ \\mbox{pred} \\} = SE^2\\{\\hat{Y}_{h(new)}\\} &=& MSE (1+ X_h^t (X^t X)^{-1} X_h)\\\\\n&=& MSE + SE^2\\{\\hat{Y}_h\\}\n\\end{eqnarray*}\\]\nallows us create \\((1-\\alpha)100\\%\\) prediction interval response \\(X_h\\): \\[\\hat{Y}_h \\pm t_{(1-\\alpha/2, n-p)} s\\{\\hat{Y}_{h(new)}\\}\\]\nNote can interpret interval say \\((1-\\alpha)100\\%\\) response values \\(X_h\\) fall interval.","code":""},{"path":"mlr.html","id":"skipping","chapter":"9 Multiple Linear Regression","heading":"Skipping","text":"text skip:surface predictionsimultaneous confidence intervalsprediction one valueor one \\(X_h\\)formal hypothesis tests normality / error variance / constant variance / lack fit (use residual plots instead)","code":""},{"path":"mlr.html","id":"criteria-for-evaluating-models","chapter":"9 Multiple Linear Regression","heading":"9.2.4 Criteria for Evaluating Models","text":"idea good model find balance small residuals many predictors. , MSE small, \\(p\\) small well. Recall \\(R^2_a\\) balance MSE \\(p\\), one specific way. ideas given . Later, models built using p-values F-tests. However, myriad criteria optimize given model (.e., variables best include). Consider using criteria compare models differing number variables (response, model structure). following defined :\\[\\begin{eqnarray*}\nSSE_p &=& SSE \\mbox{ model $p$ parameters}\\\\\nSSE_{full} &=& SSE \\mbox{  model possible parameters}\n\\end{eqnarray*}\\]\\(C_p\\): \\[C_p = \\frac{SSE_p}{MSE_{full}} - (n-2p)\\]\\(AIC_p\\): \\[AIC_p = n \\ln(SSE_p) - n \\ln(n) + 2p\\]\\(SBC_p = BIC_p\\): \\[BIC_p = n \\ln(SSE_p) - n \\ln(n) + \\ln(n) p\\]\\(AIC_p\\) \\(BIC_p\\) measure likelihood data (\\(-2 \\ln(likelihood)\\)) given particular model \\(p-1\\) explanatory variables. choose model smallest \\(AIC_p\\) \\(BIC_p\\).\nNote can use criteria build model either adding one variable time; starting full model (variables) subtracting one variable time; combination adding subtracting variables.","code":""},{"path":"mlr.html","id":"c_p-aic-and-bic-in-r","chapter":"9 Multiple Linear Regression","heading":"\\(C_p\\), AIC, and BIC in R","text":"Ideally, chosen model minimize three criteria. three trade-offs SSE (want small) \\(p\\) (want small).\\(C_p\\) also measures trade-bias variance.\n\\[\\begin{eqnarray*}\nBias(\\hat{Y}_i) &=& E[\\hat{Y}_i] - E[Y_i]\\\\\nMSE(\\hat{Y}_i) &=& [Bias(\\hat{Y}_i)]^2 + Var(\\hat{Y}_i)\\\\\n\\Gamma_p &=& \\frac{1}{\\sigma^2} [Bias(\\hat{Y}_i)]^2 + Var(\\hat{Y}_i)\\\\\n\\Gamma_p &=& p  \\mbox{ bias model}\n\\end{eqnarray*}\\]\nestimate \\(\\Gamma_p\\)? know population variance, \\(\\sigma^2\\), can estimate \\(\\Gamma_p\\) using:\n\\[\\begin{eqnarray*}\n C_p &=& p + \\frac{(MSE_p - \\sigma^2)(n-p)}{ \\sigma^2}\\\\\n \\end{eqnarray*}\\]\nEstimating \\(\\sigma^2\\) using \\(MSE_{full}\\) gives\n\\[\\begin{eqnarray*}\n C_p &=& p + \\frac{(MSE_p - MSE_{full})(n-p)}{ MSE_{full}} = \\frac{SSE_p}{MSE_{full}} - (n-2p)\\\\\n \\end{eqnarray*}\\]\\(C_p\\) estimates quantity “total MSE divided \\(\\sigma^2\\).” can shown : \\(\\sum_{=1}^n Var(\\hat{Y}_i) = \\sigma^2 p\\). want \\(C_p\\) small \\(\\approx p\\). See comments page 359 Kutner et al. (2004).\nNote calculating \\(C_P\\) full model (P parameters), get\n\\[\\begin{eqnarray*}\nC_P &=& \\frac{SSE_P}{MSE_P} - (n-2P)\\\\\n&=& (n-P) - n + 2P\\\\\n&=& P\\\\\n\\end{eqnarray*}\\]Estimating \\(\\sigma^2\\) using \\(MSE_{full}\\) assumes biases full model predictors, assumption may may valid, can’t tested without additional information (least important predictors involved).AIC BIC based maximum likelihood estimates model parameters. idea maximum likelihood find parameters produce largest likelihood function given available data. likelihood number 0 1. variety reasons, unimportant , common take log likelihood (action change function maximized) multiply likelihood -2. linear regression, parameter estimates found least squares maximum likelihood identical. However, using least squares versus maximum likelihood, difference estimating \\(\\sigma^2\\). using unbiased estimate \\(\\sigma^2\\) MSE = SSE / (n-p). maximum likelihood estimate \\(\\sigma^2\\) SSE/n. MLE slight negative bias, also smaller variance. Note estimating \\(p\\) regression coefficients \\(\\sigma^2\\), actually estimating \\(p+1\\) parameters. short, full AIC given following.\\[\n\\begin{eqnarray}\nE[AIC] &=& -2 \\ln(L) + 2(p+1)\\\\\\\n&=& -2 \\ln \\bigg[ \\prod_{=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\exp( -(Y_i - E[Y_i])^2/ 2\\sigma_i^2) \\bigg] + 2(p+1) \\tag{9.1}\\\\\n&=& -2 \\ln \\bigg[ (2\\pi)^{-(n/2)} \\sigma^{-(2n/2)} \\exp(-\\sum_{=1}^n (Y_i - E[Y_i])^2 / 2 \\sigma^2) \\bigg] + 2(p+1) \\tag{9.2}\\\\\nAIC &=& -2 \\ln \\bigg[ (2\\pi)^{-(n/2)} (SSE_p/n)^{-(n/2)} \\exp(-SSE_p / (2 SSE_p/n)) \\bigg] + 2(p+1) \\tag{9.3}\\\\\n&=& 2 (n/2) \\ln(2 \\pi) - 2(-n/2) \\ln(SSE_p/n) + n + 2(p+1) \\nonumber \\\\\n&=& n \\ln(2 \\pi) + n\\ln(SSE_p/n) + n + 2(p+1) \\nonumber \\\\\n&=& n \\ln(2 \\pi) + n\\ln(SSE_p)  - n\\ln(n) + n + 2(p+1) \\nonumber \\\\\n&=& n\\ln(SSE_p) - n\\ln(n) + 2p + constant \\nonumber\n\\end{eqnarray}\n\\]go (9.1) (9.2) assume \\(\\sigma^2 = \\sigma_i^2\\); , variance constant individuals. go (9.2) (9.3) approximate \\(\\sigma^2\\) (\\(E[Y_i]\\)) using maximum likelihood estimates \\(\\sigma^2 = SSE / n\\) \\(\\beta_k\\).\nBIC (SBC) uses posterior likelihood similar derivation. BIC can given following.\n\\[\\begin{eqnarray*}\nBIC &=& -2 \\ln(L_{posterior}) + \\ln(n) (p)\\\\\n&=& n + n\\ln(2\\pi) + n \\ln(SSE_p / n) + \\ln(n)(p)\\\\\n&=& n \\ln(SSE_p) - n \\ln(n) + \\ln(n) p + constant\n\\end{eqnarray*}\\]\nNote AIC BIC don’t consider constant term models compared data (\\(n\\) ).","code":""},{"path":"mlr.html","id":"aic-bic","chapter":"9 Multiple Linear Regression","heading":"AIC & BIC","text":"Estimators prediction error relative quality models:Akaike’s Information Criterion (AIC): \\[AIC = n\\log(SSE) - n \\log(n) + 2(p)\\] Schwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SSE) - n\\log(n) + log(n)\\times(p)\\]Comparison AIC BIC\\[\n\\begin{eqnarray*} \n&& AIC = \\color{blue}{n\\log(SSE)} - n \\log(n) + 2(p) \\\\\n&& BIC = \\color{blue}{n\\log(SSE)} - n\\log(n) + \\log(n)\\times(p) \n\\end{eqnarray*}\n\\]First Term: Decreases p increases\\[\\begin{eqnarray*} \n&& AIC = n\\log(SSE) - \\color{blue}{n \\log(n)} + 2(p) \\\\\n&& BIC = n\\log(SSE) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p) \n\\end{eqnarray*}\\]Second Term: Fixed given sample size n\\[\\begin{eqnarray*} && AIC = n\\log(SSE) - n\\log(n) + \\color{blue}{2(p)} \\\\\n&& BIC = n\\log(SSE) - n\\log(n) + \\color{blue}{\\log(n)\\times(p)} \n\\end{eqnarray*}\\]Third Term: Increases p increases","code":""},{"path":"mlr.html","id":"using-aic-bic","chapter":"9 Multiple Linear Regression","heading":"Using AIC & BIC","text":"\\[\\begin{eqnarray*} && AIC = n\\log(SSE) - n \\log(n) + \\color{red}{2(p)} \\\\\n&& BIC = n\\log(SSE) - n\\log(n) + \\color{red}{\\log(n)\\times(p)} \n\\end{eqnarray*}\\]Choose model smaller value AIC BICChoose model smaller value AIC BICIf \\(n \\geq 8\\), penalty BIC larger AIC, BIC tends favor parsimonious models (.e. models fewer terms)\\(n \\geq 8\\), penalty BIC larger AIC, BIC tends favor parsimonious models (.e. models fewer terms)","code":""},{"path":"mlr.html","id":"reflection-questions-6","chapter":"9 Multiple Linear Regression","heading":"9.3  Reflection Questions","text":"model change multiple variables?interaction mean? model ? interpret ? R code?considerations associated quadratic term?test whether variables significant?test whether individual variables significant?assess linear combination coefficients?difference \\(R^2\\) \\(R^2_a\\)?prediction mean confidence intervals created multiple explanatory variables?","code":""},{"path":"mlr.html","id":"ethics-considerations-5","chapter":"9 Multiple Linear Regression","heading":"9.4  Ethics Considerations","text":"mean “keeping variables constant” interpreting single coefficient multiple regression model? interpretation important?important include variables interest model?Can including variables change relationships variables?big question next: options, decide include include?","code":""},{"path":"mlr.html","id":"r-mlr-with-rail-trails","chapter":"9 Multiple Linear Regression","heading":"9.5 R: MLR with Rail Trails","text":"variables used following analysis hightemp, volume, precip weekday. description data given :always good idea graph data look numerical summaries. Sometimes ’ll find important artifacts mistakes.Table 9.1: Data summaryVariable type: characterVariable type: logicalVariable type: numericWe’re interested predicting volume riders high temperature (F) given day.happens weekday included binary indicator variable?Note F p-value longer equal p-value(s) associated t-test coefficients. Also, degrees freedom now (2, 87) model estimates 3 parameters.Write estimated regression model separately weekdays weekends, sketch lines onto scatterplot.new coefficients (\\(b_0, b_1, b_2\\)) interpreted?coefficient hightemp change?\\(R^2\\) change? MSE change?say weekdayTRUE instead weekday?hightemp weekday interact?Note F p-value longer equal t-stat p-value(s). Now degrees freedom (3, 86) model estimates 4 parameters.Write estimated regression model separately weekdays non-weekdays, sketch lines onto scatterplot.interpret new coefficients (\\(b_0, b_1, b_2, b_3\\))?happened significance? coefficient weekday change?\\(R^2\\) change? MSE change?happens model additional quantitative variable?Note p-values, parameter estimates, \\(R^2\\), MSE, F-stat, df, F-stat p-values.","code":"library(mosiacData)\n?RailTrail\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.4) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  skim_without_charts()\nRailTrail %>%\n  lm(volume ~ hightemp, data = .) %>%\n  tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic       p.value\n##   <chr>          <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)   -17.1     59.4      -0.288 0.774        \n## 2 hightemp        5.70     0.848     6.72  0.00000000171\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp + weekday, data = .) %>%\n  tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic      p.value\n##   <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n## 1 (Intercept)    42.8     64.3       0.665 0.508       \n## 2 hightemp        5.35     0.846     6.32  0.0000000109\n## 3 weekdayTRUE   -51.6     23.7      -2.18  0.0321\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume, color = weekday)) + \n  geom_point(alpha = 0.4) +\n  moderndive::geom_parallel_slopes(se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp * weekday, data = .) %>%\n  tidy()## # A tibble: 4 × 5\n##   term                 estimate std.error statistic p.value\n##   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)            135.      108.        1.25 0.215  \n## 2 hightemp                 4.07      1.47      2.78 0.00676\n## 3 weekdayTRUE           -186.      129.       -1.44 0.153  \n## 4 hightemp:weekdayTRUE     1.91      1.80      1.06 0.292\nRailTrail %>%\n  ggplot(aes(x = hightemp, y = volume, color = weekday)) + \n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  xlab(\"high temp (F)\") + \n  ylab(\"number of riders\")\nRailTrail %>%\n  lm(volume ~ hightemp + weekday + precip, data = .) %>%\n  tidy()## # A tibble: 4 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    19.3     60.3       0.320 7.50e- 1\n## 2 hightemp        5.80     0.799     7.26  1.59e-10\n## 3 weekdayTRUE   -43.1     22.2      -1.94  5.52e- 2\n## 4 precip       -146.      38.9      -3.74  3.27e- 4"},{"path":"mlr.html","id":"tips","chapter":"9 Multiple Linear Regression","heading":"9.6 R: Model comparison with Restaurant tips4","text":"student collected data restaurant waitress (Dahlquist Dong 2011). student interested learning conditions waitress can expect largest tips—example: dinner time late night? younger older patrons? patrons receiving free meals? patrons drinking alcohol? patrons tipping cash credit? tip amount measured total dollar amount percentage?variables help us predict amount customers tip restaurant? model best?Instead jumping predictions immediately, let’s look data . wrangling can data order make model accurate easy communicate?","code":"## # A tibble: 169 × 4\n##      Tip Party Meal   Age   \n##    <dbl> <dbl> <chr>  <chr> \n##  1  2.99     1 Dinner Yadult\n##  2  2        1 Dinner Yadult\n##  3  5        1 Dinner SenCit\n##  4  4        3 Dinner Middle\n##  5 10.3      2 Dinner SenCit\n##  6  4.85     2 Dinner Middle\n##  7  5        4 Dinner Yadult\n##  8  4        3 Dinner Middle\n##  9  5        2 Dinner Middle\n## 10  1.58     1 Dinner SenCit\n## # … with 159 more rows"},{"path":"mlr.html","id":"variables","chapter":"9 Multiple Linear Regression","heading":"9.6.1 Variables","text":"Predictors / Explanatory:Party: Number people partyMeal: Time day (Lunch, Dinner, Late Night)Age: Age category person paying bill (Yadult, Middle, SenCit)Outcome / Response: Tip: Amount tip","code":""},{"path":"mlr.html","id":"response-tip","chapter":"9 Multiple Linear Regression","heading":"Response: Tip","text":"","code":""},{"path":"mlr.html","id":"explanatory","chapter":"9 Multiple Linear Regression","heading":"Explanatory","text":"","code":""},{"path":"mlr.html","id":"relevel-categorical-explanatory","chapter":"9 Multiple Linear Regression","heading":"Relevel categorical explanatory","text":"","code":"\ntips <- tips %>%\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"},{"path":"mlr.html","id":"explanatory-again","chapter":"9 Multiple Linear Regression","heading":"Explanatory, again","text":"","code":""},{"path":"mlr.html","id":"response-vs.-predictors","chapter":"9 Multiple Linear Regression","heading":"Response vs. predictors","text":"","code":""},{"path":"mlr.html","id":"fit-and-summarize-model","chapter":"9 Multiple Linear Regression","heading":"9.6.2 Fit and summarize model","text":"","code":"\ntip_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit, conf.int = TRUE) %>%\n  kable(digits = 3)"},{"path":"mlr.html","id":"r-squared-r2","chapter":"9 Multiple Linear Regression","heading":"R-squared, \\(R^2\\)","text":"Recall: \\(R^2\\) proportion variation response variable explained regression model.\\[\nR^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]","code":"\nglance(tip_fit)## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.641         0.635  2.04      98.3 1.56e-36     3  -358.  726.  742.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"},{"path":"mlr.html","id":"model-comparison","chapter":"9 Multiple Linear Regression","heading":"9.6.3 Model comparison","text":"","code":""},{"path":"mlr.html","id":"r-squared-r2-1","chapter":"9 Multiple Linear Regression","heading":"R-squared, \\(R^2\\)","text":"\\(R^2\\) always increase add variables model + add enough variables, can always achieve \\(R^2=100\\%\\)use \\(R^2\\) choose best fit model, prone choose model predictor variables","code":""},{"path":"mlr.html","id":"adjusted-r2-1","chapter":"9 Multiple Linear Regression","heading":"Adjusted \\(R^2\\)","text":"Adjusted \\(R^2\\): measure includes penalty unnecessary predictor variablesSimilar \\(R^2\\), measure amount variation response explained regression modelDiffers \\(R^2\\) using mean squares rather sums squares therefore adjusting number predictor variables\\[R^2 = \\frac{SSR}{SSTO} = 1 - \\frac{SSE}{SSTO}\\]\\[R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)}\\]Adjusted \\(R^2\\) can used quick assessment compare fit multiple models; however, assessment!Use \\(R^2\\) describing relationship response predictor variables","code":"\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age +  Meal, data = tips)\n\nglance(tip_fit_1) %>% \n  select(r.squared, adj.r.squared)## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.674         0.664\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal + Day, data = tips)\n\nglance(tip_fit_2) %>% \n  select(r.squared, adj.r.squared)## # A tibble: 1 × 2\n##   r.squared adj.r.squared\n##       <dbl>         <dbl>\n## 1     0.683         0.662"},{"path":"mlr.html","id":"comparing-models-with-aic-and-bic","chapter":"9 Multiple Linear Regression","heading":"Comparing models with AIC and BIC","text":"","code":"\ntip_fit_1 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal, data = tips)\n\nglance(tip_fit_1) %>% \n  select(AIC, BIC)## # A tibble: 1 × 2\n##     AIC   BIC\n##   <dbl> <dbl>\n## 1  714.  736.\ntip_fit_2 <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age + Meal + Day, data = tips)\n\nglance(tip_fit_2) %>% \n  select(AIC, BIC)## # A tibble: 1 × 2\n##     AIC   BIC\n##   <dbl> <dbl>\n## 1  720.  757."},{"path":"mlr.html","id":"commonalities-between-criteria","chapter":"9 Multiple Linear Regression","heading":"9.6.4 Commonalities between criteria","text":"\\(R^2_{adj}\\), AIC, BIC apply penalty predictorsThe penalty added model complexity attempts strike balance underfitting (predictors model) overfitting (many predictors model)Goal: Parsimony","code":""},{"path":"mlr.html","id":"parsimony-and-occams-razor","chapter":"9 Multiple Linear Regression","heading":"Parsimony and Occam’s razor","text":"principle parsimony attributed William Occam (early 14th-century English nominalist philosopher), insisted , given set equally good explanations given phenomenon, correct explanation simplest explanation5The principle parsimony attributed William Occam (early 14th-century English nominalist philosopher), insisted , given set equally good explanations given phenomenon, correct explanation simplest explanation5Called Occam’s razor “shaved” explanations bare minimumCalled Occam’s razor “shaved” explanations bare minimumParsimony modeling:Parsimony modeling:models parameters possiblemodels parameters possiblelinear models preferred non-linear modelslinear models preferred non-linear modelsexperiments relying assumptions preferred relying manyexperiments relying assumptions preferred relying manymodels pared minimal adequatemodels pared minimal adequatesimple explanations preferred complex explanationssimple explanations preferred complex explanations","code":""},{"path":"mlr.html","id":"in-pursuit-of-occams-razor","chapter":"9 Multiple Linear Regression","heading":"In pursuit of Occam’s razor","text":"Occam’s razor states among competing hypotheses predict equally well, one fewest assumptions selectedOccam’s razor states among competing hypotheses predict equally well, one fewest assumptions selectedModel selection follows principleModel selection follows principleWe want add another variable model addition variable brings something valuable terms predictive power modelWe want add another variable model addition variable brings something valuable terms predictive power modelIn words, prefer simplest best model, .e. parsimonious modelIn words, prefer simplest best model, .e. parsimonious model","code":""},{"path":"mlr.html","id":"alternate-views","chapter":"9 Multiple Linear Regression","heading":"Alternate views","text":"Sometimes simple model outperform complex model .\n. . Nevertheless, believe deliberately limiting complexity model fruitful problem evidently complex.\nInstead, simple model found outperforms particular complex model, appropriate response define different complex model captures whatever aspect problem led simple model performing well.Radford Neal - Bayesian Learning Neural Networks[Suggested blog post: Occam Andrew Gelman]","code":""},{"path":"mlr.html","id":"other-concerns-with-the-approach","chapter":"9 Multiple Linear Regression","heading":"Other concerns with the approach","text":"criteria considered model comparison require making predictions data uses prediction error (\\(SSE\\)) somewhere formulaBut ’re making prediction data used build model (estimate coefficients), can lead overfittingInstead \nsplit data testing training sets\n“train” model training data pick models ’re genuinely considering potentially good models\ntest models testing set\nsplit data testing training sets“train” model training data pick models ’re genuinely considering potentially good modelstest models testing set","code":""},{"path":"process.html","id":"process","chapter":"10 Modeling as a Process6","heading":"10 Modeling as a Process6","text":"","code":""},{"path":"process.html","id":"comparing-models","chapter":"10 Modeling as a Process6","heading":"10.1 Comparing Models","text":"many models, little time. Understanding interpret multiple linear regression model half modeling process. real-life situations, many variables can used predict describe response variable hand. choose combination subset variables best?, ’ll walk many big ideas surrounding modeling building process. next chapter consider technical ideas behind statistical inference modeling building. even using formal inference, ideas always inform larger process analysis conclusions.Leo Breiman among giants machine learning helped bridge ideas statistics computer science. Trained statistician, spent career University California, Berkeley developed Classification Regression Trees (CART), bagging, Random Forests.Among important insights idea two cultures can motivate (linear) modeling:Think data generated black box vector \ninput variables x (independent variables) go one side, side response variables y come . Inside black box, nature functions associate predictor variables response variables.Data Modeling: analysis culture starts assuming stochastic data model inside black box…values parameters estimated data model used information /predictionAlgorithmic Modeling: analysis culture considers inside box complex unknown. [] approach find function f(x) — algorithm operates x predict responses y.Chapter 10, focus algorithmic modeling developing models optimally predictive response variable hand.Spoiler:Chapter 11, model building use hypothesis testing p-values.Chapter 14, model building use mathematical optimization.isn’t right way model. fact, good data analysis hard. building tool box. Make sure notice every tool , just one hammer use. Sometimes tools can work together form even better models.","code":""},{"path":"process.html","id":"worth-a-comment","chapter":"10 Modeling as a Process6","heading":"Worth a comment","text":"Notice R code gotten interesting now. fun!! R code help process! R package tidymodels includes tools facilitate, particular, feature engineering cross validation.","code":""},{"path":"process.html","id":"bias-variance-trade-off","chapter":"10 Modeling as a Process6","heading":"Bias-variance trade-off","text":"Excellent resourcefor explaining bias-variance trade-: http://scott.fortmann-roe.com/docs/BiasVariance.htmlVariance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Variance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Generally, simpler model, lower variance. complicated model, lower bias. class, cross validation used assess model fit. [time permits, Receiver Operating Characteristic (ROC) curves also covered.]\\[\\begin{align}\n\\mbox{prediction error } = \\mbox{ irreducible error } + \\mbox{ bias } + \\mbox{ variance}\n\\end{align}\\]irreducible error irreducible error natural variability comes observations. matter good model , never able predict perfectly.bias bias model represents difference true model model simple. , complicated model (e.g., variables), closer points prediction. model gets complicated (e.g., variables added), bias goes .variance variance represents variability model sample sample. , simple model (variables) change lot sample sample. variance decreases model becomes simple (e.g., variables removed).Note bias-variance trade-. want prediction error small, choose model medium respect bias variance. control irreducible error.\nFigure 1.3: Test training error function model complexity. Note error goes monotonically training data. careful overfit!! (T. Hastie, Tibshirani, Friedman 2001)\n","code":""},{"path":"process.html","id":"feature-engineering","chapter":"10 Modeling as a Process6","heading":"10.2 Feature Engineering","text":"example used consider feature engineering, data come data.world, way TidyTuesday. now exist schrute R package.Can IMDB rating (imdb_rating) Office predicted variables dataset? model best?Instead jumping predictions immediately, let’s look data . wrangling can data order take advantage information variables? ’d also like make model accurate easy communicate.","code":"## # A tibble: 188 × 6\n##    season episode title             imdb_rating total_votes air_date  \n##     <dbl>   <dbl> <chr>                   <dbl>       <dbl> <date>    \n##  1      1       1 Pilot                     7.6        3706 2005-03-24\n##  2      1       2 Diversity Day             8.3        3566 2005-03-29\n##  3      1       3 Health Care               7.9        2983 2005-04-05\n##  4      1       4 The Alliance              8.1        2886 2005-04-12\n##  5      1       5 Basketball                8.4        3179 2005-04-19\n##  6      1       6 Hot Girl                  7.8        2852 2005-04-26\n##  7      2       1 The Dundies               8.7        3213 2005-09-20\n##  8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n##  9      2       3 Office Olympics           8.4        2742 2005-10-04\n## 10      2       4 The Fire                  8.4        2713 2005-10-11\n## # … with 178 more rows"},{"path":"process.html","id":"imdb-ratings","chapter":"10 Modeling as a Process6","heading":"IMDB ratings","text":"","code":""},{"path":"process.html","id":"imdb-ratings-vs.-number-of-votes","chapter":"10 Modeling as a Process6","heading":"IMDB ratings vs. number of votes","text":"","code":""},{"path":"process.html","id":"outliers","chapter":"10 Modeling as a Process6","heading":"Outliers?","text":"","code":""},{"path":"process.html","id":"aside","chapter":"10 Modeling as a Process6","heading":"Aside…","text":"like Dinner Party episode, highly recommend “oral history” episode published Rolling Stone magazine.","code":""},{"path":"process.html","id":"rating-vs.-air-date","chapter":"10 Modeling as a Process6","heading":"Rating vs. air date","text":"","code":""},{"path":"process.html","id":"imdb-ratings-vs.-seasons","chapter":"10 Modeling as a Process6","heading":"IMDB ratings vs. seasons","text":"","code":""},{"path":"process.html","id":"building-a-model","chapter":"10 Modeling as a Process6","heading":"10.2.1 Building a Model","text":"idea build model can predict IMDB ratings, need way see (end). Indeed, important us put data pocket (called “test” data) doesn’t see modeling process. ’ll use test data end assess whether predictions good.","code":""},{"path":"process.html","id":"train-test","chapter":"10 Modeling as a Process6","heading":"Train / test","text":"Step 1: Create initial split:Step 2: Save training dataStep 3: Save testing data","code":"\nset.seed(123)\noffice_split <- initial_split(office_ratings) # prop = 3/4 by default\noffice_train <- training(office_split)\ndim(office_train)## [1] 141   6\noffice_test  <- testing(office_split)\ndim(office_test)## [1] 47  6"},{"path":"process.html","id":"using-the-training-data","chapter":"10 Modeling as a Process6","heading":"Using the training data","text":"","code":"\noffice_train## # A tibble: 141 × 6\n##    season episode title               imdb_rating total_votes air_date  \n##     <dbl>   <dbl> <chr>                     <dbl>       <dbl> <date>    \n##  1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n##  2      9      14 Vandalism                   7.6        1402 2013-01-31\n##  3      2       8 Performance Review          8.2        2416 2005-11-15\n##  4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n##  5      3      22 Beach Games                 9.1        2783 2007-05-10\n##  6      7       1 Nepotism                    8.4        1897 2010-09-23\n##  7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n##  8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n##  9      9      18 Promos                      8          1445 2013-04-04\n## 10      8      12 Pool Party                  8          1612 2012-01-19\n## # … with 131 more rows"},{"path":"process.html","id":"feature-engineering-1","chapter":"10 Modeling as a Process6","heading":"10.2.2 Feature engineering","text":"prefer simple models possible, parsimony mean sacrificing accuracy (predictive performance) interest simplicity.prefer simple models possible, parsimony mean sacrificing accuracy (predictive performance) interest simplicity.Variables go model represented just critical success model.Variables go model represented just critical success model.Feature engineering allows us get creative predictors effort make useful model (increase predictive performance).Feature engineering allows us get creative predictors effort make useful model (increase predictive performance).Feature engineering process transforming raw data features (variables) better predictors (model hand).","code":""},{"path":"process.html","id":"feature-engineering-with-dplyr","chapter":"10 Modeling as a Process6","heading":"Feature engineering with dplyr","text":"can use dplyr (tidyverse) , example, mutate() function create new variables use models.Can identify potential problems approach?One problems mutate() approach test training data get formatted separately might inconsistencies test data predicted end.","code":"\noffice_train %>%\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )## # A tibble: 141 × 8\n##   season episode title            imdb_rating total_votes air_date   month  wday\n##   <fct>    <dbl> <chr>                  <dbl>       <dbl> <date>     <dbl> <dbl>\n## 1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n## 2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n## 3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n## 4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n## 5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n## 6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n## # … with 135 more rows"},{"path":"process.html","id":"modeling-workflow","chapter":"10 Modeling as a Process6","heading":"Modeling workflow","text":"Ideally, feature engineering happens part workflow. , part modeling process. way, training data used fit model, feature engineering happens. test data used come predictions, feature engineering also happens.Create recipe feature engineering steps applied training dataCreate recipe feature engineering steps applied training dataFit model training data steps appliedFit model training data steps appliedUsing model estimates training data, predict outcomes test dataUsing model estimates training data, predict outcomes test dataEvaluate performance model test dataEvaluate performance model test data","code":""},{"path":"process.html","id":"specifying-a-model","chapter":"10 Modeling as a Process6","heading":"10.2.3 Specifying a model","text":"Instead using lm() command, ’re going use tidymodels framework specify model. Math 158 always use “lm” engine, take applied statistics classes, ’ll use different model specifications feature engineering modeling fitting steps.","code":"\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"building-a-recipe","chapter":"10 Modeling as a Process6","heading":"10.2.4 Building a recipe","text":"steps building recipe done sequentially format variable desired model. seen Section (@ref{sec:wflow}), recipe steps can happen sequence using pipe (%>%) function.However, work single pipeline, recipe effects data aren’t seen, can unsettling. can look happen ultimately apply recipe data using functions prep() bake().Note: Using prep() bake() shown demonstrative purposes. need part pipeline. find assuring, however, can see effects recipe steps recipe built.","code":""},{"path":"process.html","id":"initiate-a-recipe","chapter":"10 Modeling as a Process6","heading":"Initiate a recipe","text":"","code":"\noffice_rec <- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##    outcome          1\n##  predictor          5"},{"path":"process.html","id":"step-1-alter-roles","chapter":"10 Modeling as a Process6","heading":"Step 1: Alter roles","text":"title isn’t predictor, might want keep around ID.update_role() alters existing role recipe assigns initial role variables yet declared role.","code":"\noffice_rec <- office_rec %>%\n  update_role(title, new_role = \"ID\")\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 6\n## $ season      <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2, 9…\n## $ episode     <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20, 8,…\n## $ title       <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review\", …\n## $ total_votes <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1612…\n## $ air_date    <date> 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-05-1…\n## $ imdb_rating <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, 8.9…"},{"path":"process.html","id":"step-2-add-features","chapter":"10 Modeling as a Process6","heading":"Step 2: Add features","text":"New features day week month. , air_date variable specified keep separate information day week month information.step_date() creates specification recipe step convert date data one factor numeric variables.","code":"\noffice_rec <- office_rec %>%\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 8\n## $ season         <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2…\n## $ episode        <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,…\n## $ title          <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review…\n## $ total_votes    <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1…\n## $ air_date       <date> 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0…\n## $ imdb_rating    <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, …\n## $ air_date_dow   <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, …\n## $ air_date_month <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, …"},{"path":"process.html","id":"step-3-add-more-features","chapter":"10 Modeling as a Process6","heading":"Step 3: Add more features","text":"Identify holidays air_date, remove air_date.step_holiday() creates specification recipe step convert date data one binary indicator variables common holidays.","code":"\noffice_rec <- office_rec %>%\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 11\n## $ season                     <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_dow               <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n## $ air_date_month             <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-4-convert-numbers-to-factors","chapter":"10 Modeling as a Process6","heading":"Step 4: Convert numbers to factors","text":"Convert season factor.step_num2factor() convert one numeric vectors factors (ordered unordered). can useful categories encoded integers.","code":"\noffice_rec <- office_rec %>%\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 11\n## $ season                     <fct> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_dow               <fct> Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n## $ air_date_month             <fct> Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-5-make-dummy-variables","chapter":"10 Modeling as a Process6","heading":"Step 5: Make dummy variables","text":"Convert nominal (categorical) predictors factors.step_dummy() creates specification recipe step convert nominal data (e.g. character factors) one numeric binary model terms levels original data.","code":"\noffice_rec <- office_rec %>%\n  step_dummy(all_nominal_predictors())\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 33\n## $ episode                    <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n## $ title                      <fct> \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n## $ total_votes                <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n## $ imdb_rating                <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n## $ air_date_USThanksgivingDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USChristmasDay    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USNewYearsDay     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_USIndependenceDay <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X2                  <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X3                  <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X4                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X5                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0…\n## $ season_X6                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n## $ season_X7                  <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n## $ season_X8                  <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n## $ season_X9                  <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0…\n## $ air_date_dow_Mon           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Tue           <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Wed           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Thu           <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ air_date_dow_Fri           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_dow_Sat           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Feb         <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Mar         <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Apr         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1…\n## $ air_date_month_May         <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0…\n## $ air_date_month_Jun         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Jul         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Aug         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Sep         <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n## $ air_date_month_Oct         <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Nov         <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ air_date_month_Dec         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"},{"path":"process.html","id":"step-6-remove-zero-variance-predictors","chapter":"10 Modeling as a Process6","heading":"Step 6: Remove zero variance predictors","text":"Remove predictors contain single value. “zero variance” means variability entire column, literally every value . variables won’t ever help prediction.step_zv() creates specification recipe step remove variables contain single value.","code":"\noffice_rec <- office_rec %>%\n  step_zv(all_predictors())\n\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\n## Zero variance filter on all_predictors()\noffice_rec_trained <- prep(office_rec)\n\nbake(office_rec_trained, office_train) %>%\n  glimpse## Rows: 141\n## Columns: 22\n## $ episode            <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,…\n## $ title              <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Re…\n## $ total_votes        <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144…\n## $ imdb_rating        <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8…\n## $ season_X2          <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X3          <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X4          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X5          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n## $ season_X6          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, …\n## $ season_X7          <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ season_X8          <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n## $ season_X9          <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n## $ air_date_dow_Tue   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_dow_Thu   <dbl> 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n## $ air_date_month_Feb <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Mar <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n## $ air_date_month_Apr <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n## $ air_date_month_May <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n## $ air_date_month_Sep <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n## $ air_date_month_Oct <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n## $ air_date_month_Nov <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n## $ air_date_month_Dec <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"},{"path":"process.html","id":"putting-it-altogether","chapter":"10 Modeling as a Process6","heading":"Putting it altogether","text":"Turns feature engineering steps can using pipe function (%>%) concatenate functions.step_ functionsFor information: https://recipes.tidymodels.org/reference/index.html","code":"\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  # make title's role ID\n  update_role(title, new_role = \"ID\") %>%\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) %>%\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) %>%\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) %>%\n  # remove zero variance predictors\n  step_zv(all_predictors())\noffice_rec## Data Recipe\n## \n## Inputs:\n## \n##       role #variables\n##         ID          1\n##    outcome          1\n##  predictor          4\n## \n## Operations:\n## \n## Date features from air_date\n## Holiday features from air_date\n## Factor variables from season\n## Dummy variables from all_nominal_predictors()\n## Zero variance filter on all_predictors()\nls(pattern = '^step_', env = as.environment('package:recipes'))##  [1] \"step_arrange\"       \"step_bagimpute\"     \"step_bin2factor\"   \n##  [4] \"step_BoxCox\"        \"step_bs\"            \"step_center\"       \n##  [7] \"step_classdist\"     \"step_corr\"          \"step_count\"        \n## [10] \"step_cut\"           \"step_date\"          \"step_depth\"        \n## [13] \"step_discretize\"    \"step_downsample\"    \"step_dummy\"        \n## [16] \"step_factor2string\" \"step_filter\"        \"step_geodist\"      \n## [19] \"step_holiday\"       \"step_hyperbolic\"    \"step_ica\"          \n## [22] \"step_impute_bag\"    \"step_impute_knn\"    \"step_impute_linear\"\n## [25] \"step_impute_lower\"  \"step_impute_mean\"   \"step_impute_median\"\n## [28] \"step_impute_mode\"   \"step_impute_roll\"   \"step_indicate_na\"  \n## [31] \"step_integer\"       \"step_interact\"      \"step_intercept\"    \n## [34] \"step_inverse\"       \"step_invlogit\"      \"step_isomap\"       \n## [37] \"step_knnimpute\"     \"step_kpca\"          \"step_kpca_poly\"    \n## [40] \"step_kpca_rbf\"      \"step_lag\"           \"step_lincomb\"      \n## [43] \"step_log\"           \"step_logit\"         \"step_lowerimpute\"  \n## [46] \"step_meanimpute\"    \"step_medianimpute\"  \"step_modeimpute\"   \n## [49] \"step_mutate\"        \"step_mutate_at\"     \"step_naomit\"       \n## [52] \"step_nnmf\"          \"step_normalize\"     \"step_novel\"        \n## [55] \"step_ns\"            \"step_num2factor\"    \"step_nzv\"          \n## [58] \"step_ordinalscore\"  \"step_other\"         \"step_pca\"          \n## [61] \"step_pls\"           \"step_poly\"          \"step_profile\"      \n## [64] \"step_range\"         \"step_ratio\"         \"step_regex\"        \n## [67] \"step_relevel\"       \"step_relu\"          \"step_rename\"       \n## [70] \"step_rename_at\"     \"step_rm\"            \"step_rollimpute\"   \n## [73] \"step_sample\"        \"step_scale\"         \"step_select\"       \n## [76] \"step_shuffle\"       \"step_slice\"         \"step_spatialsign\"  \n## [79] \"step_sqrt\"          \"step_string2factor\" \"step_unknown\"      \n## [82] \"step_unorder\"       \"step_upsample\"      \"step_window\"       \n## [85] \"step_YeoJohnson\"    \"step_zv\""},{"path":"process.html","id":"building-workflows","chapter":"10 Modeling as a Process6","heading":"10.2.5 Building workflows","text":"Workflows bring together models recipes can easily applied training test data.","code":""},{"path":"process.html","id":"specify-model","chapter":"10 Modeling as a Process6","heading":"Specify model","text":"workflow: Notice two important parts workflows model specification feature engineering recipe information.","code":"\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")\n\noffice_spec## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\noffice_wflow <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec)\n\noffice_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 5 Recipe Steps\n## \n## • step_date()\n## • step_holiday()\n## • step_num2factor()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"fit-model-to-training-data","chapter":"10 Modeling as a Process6","heading":"Fit model to training data","text":"workflow hand, model can now fit training data. Although, wow… many predictors!","code":"\noffice_fit <- office_wflow %>%\n  fit(data = office_train)\n\noffice_fit %>% tidy() %>% print(n = 21)## # A tibble: 21 × 5\n##    term                estimate std.error statistic  p.value\n##    <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)         6.40     0.510        12.5   1.51e-23\n##  2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n##  3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n##  4 season_X2           0.811    0.327         2.48  1.44e- 2\n##  5 season_X3           1.04     0.343         3.04  2.91e- 3\n##  6 season_X4           1.09     0.295         3.70  3.32e- 4\n##  7 season_X5           1.08     0.348         3.11  2.34e- 3\n##  8 season_X6           1.00     0.367         2.74  7.18e- 3\n##  9 season_X7           1.02     0.352         2.89  4.52e- 3\n## 10 season_X8           0.497    0.348         1.43  1.55e- 1\n## 11 season_X9           0.621    0.345         1.80  7.41e- 2\n## 12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n## 13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n## 14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n## 15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n## 16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n## 17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n## 18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n## 19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n## 20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n## 21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"},{"path":"process.html","id":"evaluate-the-model","chapter":"10 Modeling as a Process6","heading":"10.2.6 Evaluate the model","text":"","code":""},{"path":"process.html","id":"predictions-for-training-data","chapter":"10 Modeling as a Process6","heading":"Predictions for training data","text":"","code":"\noffice_train_pred <- predict(office_fit, office_train) %>%\n  bind_cols(office_train %>% select(imdb_rating, title))\n\noffice_train_pred## # A tibble: 141 × 3\n##    .pred imdb_rating title              \n##    <dbl>       <dbl> <chr>              \n##  1  7.57         7.8 Last Day in Florida\n##  2  7.77         7.6 Vandalism          \n##  3  8.31         8.2 Performance Review \n##  4  7.67         7.1 Here Comes Treble  \n##  5  8.84         9.1 Beach Games        \n##  6  8.33         8.4 Nepotism           \n##  7  8.46         8.3 Phyllis' Wedding   \n##  8  8.14         8.9 Livin' the Dream   \n##  9  7.87         8   Promos             \n## 10  7.74         8   Pool Party         \n## # … with 131 more rows"},{"path":"process.html","id":"r-squared","chapter":"10 Modeling as a Process6","heading":"R-squared","text":"Percentage variability IMDB ratings explained model.models high low \\(R^2\\) preferable?","code":"\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.670"},{"path":"process.html","id":"rmse","chapter":"10 Modeling as a Process6","heading":"RMSE","text":"alternative model performance statistic: root mean square error.\\[RMSE = \\sqrt{\\frac{\\sum_{= 1}^n (y_i - \\hat{y}_i)^2}{n}}\\]\nNote: RMSE computed type statistics machine learning model (.e., far typical observations predicted value). change denominator means MSE \\(n\\) won’t estimate \\(\\sigma^2\\), also means metric allows us compare different kinds models (e.g., linear regression vs. neural network vs. random forest) scale. yes, MSE decrease number parameters increases use cross validation F-tests constrain model.models high low RMSE preferable?RMSE considered low high?Depends…, really…cares predictions training data?","code":"\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.302\noffice_train %>%\n  summarise(min = min(imdb_rating), \n            max = max(imdb_rating))## # A tibble: 1 × 2\n##     min   max\n##   <dbl> <dbl>\n## 1   6.7   9.7"},{"path":"process.html","id":"predictions-for-testing-data","chapter":"10 Modeling as a Process6","heading":"Predictions for testing data","text":"","code":"\noffice_test_pred <- predict(office_fit, office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title))\n\noffice_test_pred## # A tibble: 47 × 3\n##    .pred imdb_rating title              \n##    <dbl>       <dbl> <chr>              \n##  1  8.03         8.3 Diversity Day      \n##  2  7.98         7.9 Health Care        \n##  3  8.41         8.4 The Fire           \n##  4  8.35         8.2 Halloween          \n##  5  8.35         8.4 E-Mail Surveillance\n##  6  8.68         9   The Injury         \n##  7  8.32         7.9 The Carpet         \n##  8  8.93         9.3 Casino Night       \n##  9  8.80         8.9 Gay Witch Hunt     \n## 10  8.37         8.2 Initiation         \n## # … with 37 more rows"},{"path":"process.html","id":"evaluate-performance-for-testing-data","chapter":"10 Modeling as a Process6","heading":"Evaluate performance for testing data","text":"\\(R^2\\) model fit testing dataRMSE model fit testing data","code":"\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.468\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.411"},{"path":"process.html","id":"training-vs.-testing","chapter":"10 Modeling as a Process6","heading":"Training vs. testing","text":"","code":"\nrmse_train <- rmse(office_train_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrsq_train <- rsq(office_train_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrmse_test <- rmse(office_test_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)\n\nrsq_test <- rsq(office_test_pred, truth = imdb_rating, estimate = .pred) %>%\n  pull(.estimate) %>%\n  round(3)"},{"path":"process.html","id":"evaluating-performance-on-training-data","chapter":"10 Modeling as a Process6","heading":"Evaluating performance on training data","text":"training set capacity good arbiter performance.training set capacity good arbiter performance.independent piece information; predicting training set can reflect model already knows.independent piece information; predicting training set can reflect model already knows.Suppose give class test, give answers, provide test.\nstudent scores second test accurately reflect know subject; scores probably higher results first test.Suppose give class test, give answers, provide test.\nstudent scores second test accurately reflect know subject; scores probably higher results first test.","code":""},{"path":"process.html","id":"cross-validation","chapter":"10 Modeling as a Process6","heading":"10.3 Cross Validation","text":"get details cross validation, let’s set scenario need cross validation. Recall use test data assess model . haven’t yet thought use data build particular model.example, let’s set scenario compare two different models. first model doesn’t use air_date otherwise similar model . second model also use air_date variable considers season numeric variable.Model 1:Model 2:","code":"\noffice_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"ID\") %>%\n  # delete the air_date variable\n  step_rm(air_date) %>%\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\nprep(office_rec1) %>%\nbake(office_train) %>%\n  glimpse()## Rows: 141\n## Columns: 12\n## $ episode     <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20, 8,…\n## $ title       <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review\", …\n## $ total_votes <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1612…\n## $ imdb_rating <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, 8.9…\n## $ season_X2   <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0…\n## $ season_X3   <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X4   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X5   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0…\n## $ season_X6   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…\n## $ season_X7   <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n## $ season_X8   <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ season_X9   <dbl> 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1…\noffice_rec2 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"id\") %>%\n  # delete the air_date variable\n  step_rm(air_date) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\nprep(office_rec2) %>%\nbake(office_train) %>%\n  glimpse()## Rows: 141\n## Columns: 5\n## $ season      <dbl> 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2, 9…\n## $ episode     <dbl> 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20, 8,…\n## $ title       <fct> \"Last Day in Florida\", \"Vandalism\", \"Performance Review\", …\n## $ total_votes <dbl> 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1612…\n## $ imdb_rating <dbl> 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, 8.9…"},{"path":"process.html","id":"creating-workflows","chapter":"10 Modeling as a Process6","heading":"Creating workflows","text":"Using separate recipes, different workflows set :Model 1:Model 2:","code":"\noffice_wflow1 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec1)\n\noffice_wflow1## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_rm()\n## • step_num2factor()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\noffice_wflow2 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec2)\n\noffice_wflow2## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 3 Recipe Steps\n## \n## • step_rm()\n## • step_dummy()\n## • step_zv()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"process.html","id":"fit-the-models-to-the-training-data","chapter":"10 Modeling as a Process6","heading":"Fit the models to the training data","text":"WAIT, fast!","code":""},{"path":"process.html","id":"implementing-cross-validation","chapter":"10 Modeling as a Process6","heading":"Implementing Cross Validation","text":"\nFigure 10.1: (Flach 2012)\nCross validation typically used two ways.assess model’s accuracy (model assessment).build model (model selection).","code":""},{"path":"process.html","id":"different-ways-to-cv","chapter":"10 Modeling as a Process6","heading":"10.3.1 Different ways to CV","text":"Suppose build classifier given data set. ’d like know well model classifies observations, test samples hand, error rate much lower model’s inherent accuracy rate. Instead, ’d like predict new observations used create model. various ways creating test validation sets data:one training set, one test set [two drawbacks: estimate error highly variable depends points go training set; training data set smaller full data set, error rate biased way overestimates actual error rate modeling technique.]one training set, one test set [two drawbacks: estimate error highly variable depends points go training set; training data set smaller full data set, error rate biased way overestimates actual error rate modeling technique.]leave one cross validation (LOOCV)leave one cross validation (LOOCV)remove one observationbuild model using remaining n-1 pointspredict class membership observation removedrepeat removing observation one time\\(V\\)-fold cross validation (\\(V\\)-fold CV)\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]\nLOOCV special case \\(V\\)-fold CV \\(V=n\\)\nadvantage \\(V\\)-fold computational\n\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]LOOCV special case \\(V\\)-fold CV \\(V=n\\)advantage \\(V\\)-fold computational\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]","code":""},{"path":"process.html","id":"cv-for-model-assessment-10-fold","chapter":"10 Modeling as a Process6","heading":"CV for Model assessment 10-fold","text":"assume variables setremove 10% databuild model using remaining 90%predict response 10% observations removedrepeat removing decile one timea good measure model’s ability predict error rate associated predictions data independently predicted","code":""},{"path":"process.html","id":"cv-for-model-selection-10-fold","chapter":"10 Modeling as a Process6","heading":"CV for Model selection 10-fold","text":"set variablesbuild model using variables set :\nremove 10% data\nbuild model using remaining 90%\npredict response 10% observations removed\nrepeat removing decile one time\nremove 10% databuild model using remaining 90%predict response 10% observations removedrepeat removing decile one timemeasure CV prediction error \\(k\\) value handrepeat steps 1-3 choose variables prediction error lowest","code":""},{"path":"process.html","id":"cv-for-model-assessment-and-selection-10-fold","chapter":"10 Modeling as a Process6","heading":"CV for Model assessment and selection 10-fold","text":", one approach use test/training data CV order model assessment selection. Note CV used steps, algorithm slightly complicated.split data training test observationsset \\(k\\) \\(k\\)-NNbuild model using \\(k\\) value set training data:\nremove 10% training data\nbuild model using remaining 90% training data\npredict class membership / continuous response 10% training observations removed\nrepeat removing decile one time training data\nremove 10% training databuild model using remaining 90% training datapredict class membership / continuous response 10% training observations removedrepeat removing decile one time training datameasure CV prediction error \\(k\\) value hand training datarepeat steps 2-4 choose \\(k\\) prediction error lowest training datausing \\(k\\) value given step 5, assess prediction error test data\nFigure 10.2: Nested cross-validation: two cross-validation loops run one inside . (Varoquaux et al. 2017)\n","code":""},{"path":"process.html","id":"fit-the-models-using-cross-validation","chapter":"10 Modeling as a Process6","heading":"10.3.2 Fit the models using cross validation","text":"","code":""},{"path":"process.html","id":"spending-the-data","chapter":"10 Modeling as a Process6","heading":"“Spending” the data","text":"already established idea data spending test set recommended obtaining unbiased estimate performance.However, need decide model choose using test set.Typically can’t decide final model take test set without making model assessments.Remedy: Resampling make model assessments training data way can generalize new data.","code":""},{"path":"process.html","id":"resampling-for-model-assessment","chapter":"10 Modeling as a Process6","heading":"Resampling for model assessment","text":"Resampling conducted training set.\ntest set involved.\niteration resampling, data partitioned two subsamples:model fit analysis set.model evaluated assessment set.\nFigure 10.3: Repeated samples taken training data, resample observations used build model observations used estimate performance. Source: (Kuhn Silge 2022)\nAside: “re” “resamples” repeated samples. confused repeated samples taken bootstrapping replacement. cross validation, repeated samples taken without replacement.","code":""},{"path":"process.html","id":"analysis-and-assessment-sets","chapter":"10 Modeling as a Process6","heading":"Analysis and assessment sets","text":"Analysis set analogous training set.Assessment set analogous test set.terms analysis assessment avoids confusion initial split data.data sets mutually exclusive.","code":""},{"path":"process.html","id":"cross-validation-1","chapter":"10 Modeling as a Process6","heading":"Cross validation","text":"specifically, v-fold cross validation – commonly used resampling technique:Randomly split training data v partitionsUse 1 partition assessment, remaining v-1 partitions analysisRepeat v times, updating partition used assessment timeLet’s give example v = 3…","code":""},{"path":"process.html","id":"cross-validation-step-1","chapter":"10 Modeling as a Process6","heading":"Cross validation, step 1","text":"Consider example training data randomly split 3 partitions:\nFigure 10.4: Splitting data partition v=3 groups. Source: (Kuhn Silge 2022)\nNote three repeated samples (“resamples”) taken without replacement original dataset.","code":"\nset.seed(345)\nfolds <- vfold_cv(office_train, v = 3)\nfolds## #  3-fold cross-validation \n## # A tibble: 3 × 2\n##   splits          id   \n##   <list>          <chr>\n## 1 <split [94/47]> Fold1\n## 2 <split [94/47]> Fold2\n## 3 <split [94/47]> Fold3"},{"path":"process.html","id":"cross-validation-steps-2-and-3","chapter":"10 Modeling as a Process6","heading":"Cross validation, steps 2 and 3","text":"Use 1 partition assessment, remaining v-1 partitions analysisRepeat v times, updating partition used assessment time\nFigure 10.5: data split three groups, can see 2/3 observations used fit model 1/3 observations used estimate performance model. Source: (Kuhn Silge 2022)\n","code":""},{"path":"process.html","id":"fit-resamples","chapter":"10 Modeling as a Process6","heading":"Fit resamples","text":"data split v (3) resamples, can fit two models interest.Model 1:Model 2:","code":"\nset.seed(456)\n\noffice_fit_rs1 <- office_wflow1 %>%\n  fit_resamples(folds)\n\noffice_fit_rs1## # Resampling results\n## # 3-fold cross-validation \n## # A tibble: 3 × 4\n##   splits          id    .metrics         .notes          \n##   <list>          <chr> <list>           <list>          \n## 1 <split [94/47]> Fold1 <tibble [2 × 4]> <tibble [0 × 1]>\n## 2 <split [94/47]> Fold2 <tibble [2 × 4]> <tibble [0 × 1]>\n## 3 <split [94/47]> Fold3 <tibble [2 × 4]> <tibble [0 × 1]>\nset.seed(456)\n\noffice_fit_rs2 <- office_wflow2 %>%\n  fit_resamples(folds)\n\noffice_fit_rs2## # Resampling results\n## # 3-fold cross-validation \n## # A tibble: 3 × 4\n##   splits          id    .metrics         .notes          \n##   <list>          <chr> <list>           <list>          \n## 1 <split [94/47]> Fold1 <tibble [2 × 4]> <tibble [0 × 1]>\n## 2 <split [94/47]> Fold2 <tibble [2 × 4]> <tibble [0 × 1]>\n## 3 <split [94/47]> Fold3 <tibble [2 × 4]> <tibble [0 × 1]>"},{"path":"process.html","id":"cross-validation-now-what","chapter":"10 Modeling as a Process6","heading":"Cross validation, now what?","text":"’ve fit bunch modelsNow ’s time use collect metrics (e.g., R-squared, RMSE) model use evaluate model fit varies across folds","code":""},{"path":"process.html","id":"collect-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Collect CV metrics","text":"Model 1:Model 2:","code":"\ncollect_metrics(office_fit_rs1)## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.373     3  0.0324 Preprocessor1_Model1\n## 2 rsq     standard   0.574     3  0.0614 Preprocessor1_Model1\ncollect_metrics(office_fit_rs1)## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.373     3  0.0324 Preprocessor1_Model1\n## 2 rsq     standard   0.574     3  0.0614 Preprocessor1_Model1"},{"path":"process.html","id":"deeper-look-into-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Deeper look into CV metrics","text":"Model 1:Model 2:","code":"\ncv_metrics1 <- collect_metrics(office_fit_rs1, summarize = FALSE) \n\ncv_metrics1## # A tibble: 6 × 5\n##   id    .metric .estimator .estimate .config             \n##   <chr> <chr>   <chr>          <dbl> <chr>               \n## 1 Fold1 rmse    standard       0.320 Preprocessor1_Model1\n## 2 Fold1 rsq     standard       0.687 Preprocessor1_Model1\n## 3 Fold2 rmse    standard       0.368 Preprocessor1_Model1\n## 4 Fold2 rsq     standard       0.476 Preprocessor1_Model1\n## 5 Fold3 rmse    standard       0.432 Preprocessor1_Model1\n## 6 Fold3 rsq     standard       0.558 Preprocessor1_Model1\ncv_metrics2 <- collect_metrics(office_fit_rs2, summarize = FALSE) \n\ncv_metrics2## # A tibble: 6 × 5\n##   id    .metric .estimator .estimate .config             \n##   <chr> <chr>   <chr>          <dbl> <chr>               \n## 1 Fold1 rmse    standard       0.388 Preprocessor1_Model1\n## 2 Fold1 rsq     standard       0.529 Preprocessor1_Model1\n## 3 Fold2 rmse    standard       0.421 Preprocessor1_Model1\n## 4 Fold2 rsq     standard       0.266 Preprocessor1_Model1\n## 5 Fold3 rmse    standard       0.411 Preprocessor1_Model1\n## 6 Fold3 rsq     standard       0.518 Preprocessor1_Model1"},{"path":"process.html","id":"better-tabulation-of-cv-metrics","chapter":"10 Modeling as a Process6","heading":"Better tabulation of CV metrics","text":"","code":""},{"path":"process.html","id":"how-does-rmse-compare-to-y","chapter":"10 Modeling as a Process6","heading":"How does RMSE compare to y?","text":"Recall RMSE calculated original units response variable.\\[RMSE_{\\mbox{training}} = \\sqrt{\\frac{\\sum_{= 1}^n (y_i - \\hat{y}_i)^2}{n}}\\]CV RMSE uses 2/3 observations build model (\\(\\hat{y}_i\\)) 1/3 observations test . equation “” 1/3. Note \\(\\hat{y}_{\\tiny\\mbox{2/3 fold 1}, \\normalsize }\\) indicates \\(^{th}\\) row predicted using model built 2/3^{rds} observations.\\[RMSE_{\\mbox{fold 1}} = \\sqrt{\\frac{\\sum_{\\\\tiny \\mbox{1/3 fold 1}} \\normalsize (y_i - \\hat{y}_{\\tiny\\mbox{2/3 fold 1}, \\normalsize })^2}{n/3}}\\]comparing model better, CV RMSE provides information well model predicting 1/3 hold sample. Indeed, RMSE error (read: variability) continues exist even model built.can compare model RMSE original variability seen imbd_rating variable.Model 1:Model 2:Training data IMDB score stats:original variability (measured standard deviation) ratings 0.538. running Model 1, remaining variability (measured RMSE averaged folds) 0.466; running Model 2, remaining variability (measured RMSE averaged folds) 0.52.Conclusions:seems though linear model reduce variability response variable (though much).seems though linear model includes season factor variable (slightly) better model one uses season numeric.","code":"\ncv_metrics1 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )## # A tibble: 1 × 4\n##     min   max  mean     sd\n##   <dbl> <dbl> <dbl>  <dbl>\n## 1 0.320 0.432 0.373 0.0562\ncv_metrics2 %>%\n  filter(.metric == \"rmse\") %>%\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )## # A tibble: 1 × 4\n##     min   max  mean     sd\n##   <dbl> <dbl> <dbl>  <dbl>\n## 1 0.388 0.421 0.407 0.0168\noffice_ratings %>%\n  summarise(\n    min = min(imdb_rating),\n    max = max(imdb_rating),\n    mean = mean(imdb_rating),\n    sd = sd(imdb_rating)\n  )## # A tibble: 1 × 4\n##     min   max  mean    sd\n##   <dbl> <dbl> <dbl> <dbl>\n## 1   6.7   9.7  8.26 0.538"},{"path":"process.html","id":"cross-validation-jargon","chapter":"10 Modeling as a Process6","heading":"Cross validation jargon","text":"Referred v-fold k-fold cross validationAlso commonly abbreviated CV","code":""},{"path":"process.html","id":"cross-validation-redux","chapter":"10 Modeling as a Process6","heading":"Cross validation, redux","text":"illustrate CV works, used v = 3:illustrate CV works, used v = 3:Analysis sets 2/3 training setAnalysis sets 2/3 training setEach assessment set distinct 1/3Each assessment set distinct 1/3The final resampling estimate performance averages 3 replicatesThe final resampling estimate performance averages 3 replicatesIt useful illustrative purposes, v = 3 poor choice practiceIt useful illustrative purposes, v = 3 poor choice practiceValues v often 5 10; generally prefer 10-fold cross-validation defaultValues v often 5 10; generally prefer 10-fold cross-validation default","code":""},{"path":"process.html","id":"final-model-assessment","chapter":"10 Modeling as a Process6","heading":"10.4 Final model assessment","text":"Now Model 1 chosen better model, test data finally brought measure well Model 1 predict data wild (additional wild episodes Office, …).Model 1:\\(R^2\\) test data 0.562 (56.2% variability imdb_rating test data explained model training data). Additionally, test RMSE 0.415. expected, RMSE lower training test; \\(R^2\\) higher training test.","code":"\noffice_preds1 <- office_wflow1 %>%\n  fit(data = office_train) %>%\n  predict(office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title)) \n\noffice_preds1 %>%\n  rsq(truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.476\noffice_preds1 %>%\n  rmse(truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.412"},{"path":"process.html","id":"reflection-questions-7","chapter":"10 Modeling as a Process6","heading":"10.5  Reflection Questions","text":"steps workflow / pipeline?types feature engineering can done step_ functions?different model building model assessment?examples, cross validation used model building model assessment?","code":""},{"path":"process.html","id":"ethics-considerations-6","chapter":"10 Modeling as a Process6","heading":"10.6  Ethics Considerations","text":"important use different parts dataset two different tasks model building model assessment?cross validation help keep model overfitting data hand?Additionally, consider example given Aaron Roth blog titled Algorithmic Unfairness Without Bias Baked . ’ve written full example.take away :two populations different feature distributions, learning single classifier (prohibited discriminating based population) fit bigger two populationsdepending nature distribution difference, can either benefit detriment minority populationdepending nature distribution difference, can either benefit detriment minority populationno explicit human bias, either part algorithm designer data gathering processno explicit human bias, either part algorithm designer data gathering processthe problem exacerbated artificially force algorithm group blindthe problem exacerbated artificially force algorithm group blindwell intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes account can actually make things less fair less accurate timewell intentioned “fairness” regulations prohibiting decision makers form taking sensitive attributes account can actually make things less fair less accurate time","code":""},{"path":"process.html","id":"r-full-pipeline-with-cv-assessment","chapter":"10 Modeling as a Process6","heading":"10.7 R: Full pipeline with CV + assessment","text":"Using example , process synthesized essential aspects / code.","code":""},{"path":"process.html","id":"the-data","chapter":"10 Modeling as a Process6","heading":"10.7.1 The data","text":"Break data test training sets.","code":"\nset.seed(123)\noffice_split <- initial_split(office_ratings) # prop = 3/4 by default\noffice_train <- training(office_split)\noffice_test  <- testing(office_split)"},{"path":"process.html","id":"the-model-enigne","chapter":"10 Modeling as a Process6","heading":"10.7.2 The model / enigne","text":"Tell computer run linear regression using lm() function.","code":"\noffice_spec <- linear_reg() %>%\n  set_engine(\"lm\")"},{"path":"process.html","id":"the-recipes","chapter":"10 Modeling as a Process6","heading":"10.7.3 The recipe(s)","text":"Set variables interest (formula) perform necessary feature engineering.","code":"\noffice_rec1 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"id\") %>%\n  step_rm(air_date) %>%\n  step_num2factor(season, levels = as.character(1:9)) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())\n\noffice_rec2 <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(title, new_role = \"id\") %>%\n  step_rm(air_date) %>%\n  step_dummy(all_nominal_predictors()) %>%\n  step_zv(all_predictors())"},{"path":"process.html","id":"the-workflows","chapter":"10 Modeling as a Process6","heading":"10.7.4 The workflow(s)","text":"","code":"\noffice_wflow1 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec1)\n\noffice_wflow2 <- workflow() %>%\n  add_model(office_spec) %>%\n  add_recipe(office_rec2)"},{"path":"process.html","id":"choice-fit-the-model-cross-validate-to-decide-between-models","chapter":"10 Modeling as a Process6","heading":"10.7.5 Choice: fit the model? cross validate to decide between models?","text":"","code":""},{"path":"process.html","id":"fit-the-model","chapter":"10 Modeling as a Process6","heading":"Fit the model","text":"fit object output lm() used working :","code":"\noffice_fit <- office_wflow %>%\n  fit(data = office_train)\noffice_fit %>% tidy()## # A tibble: 21 × 5\n##    term         estimate std.error statistic  p.value\n##    <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)  6.40     0.510        12.5   1.51e-23\n##  2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n##  3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n##  4 season_X2    0.811    0.327         2.48  1.44e- 2\n##  5 season_X3    1.04     0.343         3.04  2.91e- 3\n##  6 season_X4    1.09     0.295         3.70  3.32e- 4\n##  7 season_X5    1.08     0.348         3.11  2.34e- 3\n##  8 season_X6    1.00     0.367         2.74  7.18e- 3\n##  9 season_X7    1.02     0.352         2.89  4.52e- 3\n## 10 season_X8    0.497    0.348         1.43  1.55e- 1\n## # … with 11 more rows\noffice_fit %>% glance()## # A tibble: 1 × 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.670         0.615 0.327      12.2 2.10e-20    20  -31.2  106.  171.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"},{"path":"process.html","id":"cross-validate-to-choose-between-models","chapter":"10 Modeling as a Process6","heading":"Cross validate to choose between models","text":"cross validating, model needs fit separately one hold folds CV model. , v=5 used, different v=3 used simplify explanation. Typically, number folds 5 10.Note objects now similar ’ve worked previously (output lm()), contain separate fit (.e., linear model) CV folds.","code":"\nset.seed(47)\nfolds <- vfold_cv(office_train, v = 5)\n\noffice_fit_rs1 <- office_wflow1 %>%\n  fit_resamples(folds)\n\noffice_fit_rs2 <- office_wflow2 %>%\n  fit_resamples(folds)"},{"path":"process.html","id":"assess-the-fit","chapter":"10 Modeling as a Process6","heading":"10.7.6 Assess the fit","text":"","code":""},{"path":"process.html","id":"on-the-test-data","chapter":"10 Modeling as a Process6","heading":"On the test data","text":"Note ’d assess fit test data done model building process ’ve chosen model want move forward ., \\(R^2\\) RMSE calculated general sense (.e., RMSE uses \\(n\\) denominator, \\(n-p\\), agree, confusing!).first lines don’t make sense, run pieces. , run predict(office_fit, office_test) run office_test %>% select(imdb_rating, title) think mean bind columns together.","code":"\noffice_test_pred <- predict(office_fit, office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title))\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.468\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.411"},{"path":"process.html","id":"on-the-cv-folds","chapter":"10 Modeling as a Process6","heading":"On the CV folds","text":"Note difference information. want values per fold, don’t summarize. want overall information, summarize.Note variables Model 1 perform better using cross validation variables Model 2, choose Model 1 report :","code":"\noffice_fit_rs1 %>% collect_metrics()## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.355     5  0.0133 Preprocessor1_Model1\n## 2 rsq     standard   0.579     5  0.0338 Preprocessor1_Model1\noffice_fit_rs2 %>% collect_metrics()## # A tibble: 2 × 6\n##   .metric .estimator  mean     n std_err .config             \n##   <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.392     5  0.0284 Preprocessor1_Model1\n## 2 rsq     standard   0.500     5  0.0563 Preprocessor1_Model1\noffice_fit_rs1 %>% collect_metrics(summarize = FALSE)## # A tibble: 10 × 5\n##    id    .metric .estimator .estimate .config             \n##    <chr> <chr>   <chr>          <dbl> <chr>               \n##  1 Fold1 rmse    standard       0.316 Preprocessor1_Model1\n##  2 Fold1 rsq     standard       0.545 Preprocessor1_Model1\n##  3 Fold2 rmse    standard       0.337 Preprocessor1_Model1\n##  4 Fold2 rsq     standard       0.608 Preprocessor1_Model1\n##  5 Fold3 rmse    standard       0.382 Preprocessor1_Model1\n##  6 Fold3 rsq     standard       0.672 Preprocessor1_Model1\n##  7 Fold4 rmse    standard       0.356 Preprocessor1_Model1\n##  8 Fold4 rsq     standard       0.598 Preprocessor1_Model1\n##  9 Fold5 rmse    standard       0.386 Preprocessor1_Model1\n## 10 Fold5 rsq     standard       0.470 Preprocessor1_Model1\noffice_fit_rs2 %>% collect_metrics(summarize = FALSE)## # A tibble: 10 × 5\n##    id    .metric .estimator .estimate .config             \n##    <chr> <chr>   <chr>          <dbl> <chr>               \n##  1 Fold1 rmse    standard       0.307 Preprocessor1_Model1\n##  2 Fold1 rsq     standard       0.607 Preprocessor1_Model1\n##  3 Fold2 rmse    standard       0.357 Preprocessor1_Model1\n##  4 Fold2 rsq     standard       0.572 Preprocessor1_Model1\n##  5 Fold3 rmse    standard       0.458 Preprocessor1_Model1\n##  6 Fold3 rsq     standard       0.471 Preprocessor1_Model1\n##  7 Fold4 rmse    standard       0.387 Preprocessor1_Model1\n##  8 Fold4 rsq     standard       0.555 Preprocessor1_Model1\n##  9 Fold5 rmse    standard       0.450 Preprocessor1_Model1\n## 10 Fold5 rsq     standard       0.293 Preprocessor1_Model1\noffice_test_pred_1 <- office_wflow1 %>%\n  fit(office_train) %>%\n  predict(office_test) %>%\n  bind_cols(office_test %>% select(imdb_rating, title))\n\nrsq(office_test_pred_1, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rsq     standard       0.476\nrmse(office_test_pred_1, truth = imdb_rating, estimate = .pred)## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.412"},{"path":"build.html","id":"build","chapter":"11 Statistical Model Building","heading":"11 Statistical Model Building","text":"One main tools statistical building multiple regression models nested F test. Two models nested parameters smaller model subset parameters larger model. two models must contain exact observations (careful missing values variables others!).\\[\\begin{eqnarray*}\nSSR &=& \\sum (\\hat{Y}_i - \\overline{Y})^2 \\\\\nSSE &=& \\sum (Y_i - \\hat{Y}_i)^2 \\\\\nSSTO &=& \\sum (Y_i - \\overline{Y})^2 \\\\\nSSTO &=& SSE + SSR\n\\end{eqnarray*}\\]\nConvince \\(SSE(X_1) > SSE(X_1, X_2)\\) (variables parentheses indicate variables included model). calculus least squares says variables produce smaller SSE (otherwise \\(b_2\\) estimated zero).Let \\[SSR(X_2 | X_1 ) = SSE(X_1) - SSE(X_1, X_2).\\] call \\(SSR(X_2 | X_1 )\\) extra sum squares. marginal reduction error sum squares one () explanatory variable(s) added model (given explanatory variables already model). know SSTO change number variables (make sure sample size \\(n\\) doesn’t change due missing observations!), can write SSR variety ways.\n\\[\\begin{eqnarray*}\nSSR(X_3| X_1, X_2) &=& SSE(X_1, X_2) - SSE(X_1, X_2, X_3)\\\\\n&=& SSR(X_1, X_2, X_3) - SSR(X_1, X_2)\\\\\nSSR(X_1, X_2 | X_3 ) &=& SSE(X_3) - SSE(X_1, X_2, X_3)\\\\\n&=& SSR(X_1, X_2, X_3) - SSR(X_3)\\\\\n\\end{eqnarray*}\\]\nConsider two nested models:\nModel 1: \\(E[Y] = \\beta_0 + \\beta_1 X_1\\) \\[SSTO = SSR(X_1) + SSE(X_1)\\]\nModel 2: \\(E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\)\\[\\begin{eqnarray*}\nSSTO &=& SSR(X_1, X_2) + SSE(X_1, X_2)\\\\\nSSR(X_1, X_2) &=& SSR(X_1) + SSR(X_2 | X_1) \\\\\nSSTO &=& SSR(X_1) + SSR(X_2 | X_1) + SSE(X_1, X_2)\\\\\n\\end{eqnarray*}\\]\n() \\(SSR(X_1)\\) measures contribution \\(X_1\\) alone.\n(b) \\(SSR(X_2 | X_1)\\) measures contribution \\(X_2\\) given \\(X_1\\) model.\ntypical ANOVA table model three explanatory variables look something like table . Note hierarchical structure adding variables:","code":""},{"path":"build.html","id":"nestF","chapter":"11 Statistical Model Building","heading":"11.1 Testing Sets of Coefficients","text":"Previously, covered t-tests \\(H_0: \\beta_k = 0\\) using full model F-test test whether coefficients non-significant. fact, can also use full reduced models compare mean squares test nested models. Recall:\n1. Fit full model obtain SSE(full model)\n2. Fit reduced model \\(H_0\\) get SSE(reduced)\n3. Use \\(F^* = \\frac{SSE(reduced) - SSE(full)}{df_{reduced} - df_{full}} \\div \\frac{SSE(full)}{df_{full}}\\)\nNote 1: best estimate \\(\\sigma^2\\) come MSE full model. MSE unbiased estimate \\(\\sigma^2\\), always use denominator F test-statistic.\nNote 2: previous F test learned (\\(H_0: \\beta_k = 0 \\ \\ \\forall \\ \\  k \\ne 0\\)) SSTO = SSE(reduced) \\(\\beta_k=0\\), SSR(reduced) = 0. , testing \\(\\beta_k = 0 \\ \\ \\forall \\ \\  k \\ne 0\\): SSE(reduced) - SSE(full) = SSR(full).Consider testing following hypotheses\\(H_0: \\beta_2 = \\beta_3 = 0\\)\\(H_a:\\) zero\nFull model: \\(E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\)\nReduced model: \\(E[Y] = \\beta_0 + \\beta_1 X_1\\)\\[\\begin{eqnarray*}\nSSE(full) &=& SSE(X_1, X_2, X_3)\\\\\nSSE(reduced) &=& SSE(X_1)\\\\\nF^* &=& \\frac{SSE(X_1) - SSE(X_1, X_2, X_3)}{(n-2) - (n-4)} \\div \\frac{SSE(X_1, X_2, X_3)}{n-4}\\\\\n&=& \\frac{SSR(X_1, X_2, X_3) - SSR(X_1)}{(n-2) - (n-4)} \\div \\frac{SSE(X_1, X_2, X_3)}{n-4}\\\\\n&=& \\frac{SSR(X_2 | X_1) + SSR(X_3 | X_1, X_2)}{2} \\div \\frac{SSE(X_1, X_2, X_3)}{n-4}\\\\\n&=& \\frac{SSR(X_2, X_3 | X_1)}{2} \\div \\frac{SSE(X_1, X_2, X_3)}{n-4}\\\\\n&=& \\frac{MSR(X_2, X_3 | X_1)}{MSE(X_1, X_2, X_3)}\n\\end{eqnarray*}\\]","code":""},{"path":"build.html","id":"examples","chapter":"11 Statistical Model Building","heading":"11.1.1 Examples","text":"considering output , let’s say ’d like test whether smoking mother’s age needed model. full model : gained, smoke, mage model; reduced model model gained .\\[\\begin{eqnarray*}\nF &=& \\frac{(1485.09 - 1453.37) / (939 - 937)}{1453.37 / 937} = 10.22\n\\end{eqnarray*}\\]p-value low, reject \\(H_0: \\beta_2 = \\beta_3 = 0\\), claim least one smoking habit mother’s age, mage, needed model (possibly ).One reason use nested F-test (lieu t-test), example, ’d like know whether term important variable model (see model 8 way R code). noted , term factor variable responsible 2 separate coefficients. order test whether term significant, fit model without term, null hypothesis testing \\(H_0: \\beta_2 = \\beta_3 = 0\\).One reason use nested F-test (lieu t-test), example, ’d like know whether term important variable model (see model 8 way R code). noted , term factor variable responsible 2 separate coefficients. order test whether term significant, fit model without term, null hypothesis testing \\(H_0: \\beta_2 = \\beta_3 = 0\\).Another reason use nested F-test want simultaneously determine interaction needed model. might 4 explanatory variables, ’d \\({4\\choose2} = 6\\) pairwise interactions consider. test interaction coefficients simultaneously fitting model additive effects (reduced) model interaction effects (full). nesting , don’t need test interaction coefficient one time.Another reason use nested F-test want simultaneously determine interaction needed model. might 4 explanatory variables, ’d \\({4\\choose2} = 6\\) pairwise interactions consider. test interaction coefficients simultaneously fitting model additive effects (reduced) model interaction effects (full). nesting , don’t need test interaction coefficient one time.Let’s say want test \\(H_0: \\beta_1 = \\beta_2\\). Note form full reduced models.Let’s say want test \\(H_0: \\beta_1 = \\beta_2\\). Note form full reduced models.\\[\\begin{eqnarray*}\n\\mbox{full model}:&& E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\\\\n\\mbox{reduced model}: && E[Y] = \\beta_0 + \\beta_c (X_1 + X_2) + \\beta_3 X_3\\\\\n\\end{eqnarray*}\\]reduced model specific form full model, two models nested. can reformat data (.e., add first two variables), run linear model R. can get SSE full model reduced model calculate F statistic hand.Let’s say want test \\(H_0: \\beta_3 = 47\\). , note form full reduced models.\\[\\begin{eqnarray*}\n\\mbox{full model}:&& E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\\\\\n\\mbox{reduced model}: && E[Y] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + 47 * X_3\\\\\n\\end{eqnarray*}\\]\ndon’t want find coefficient \\(X_3\\), subtract \\(47*X_3\\) Y value:\n\\[\\begin{eqnarray*}\n\\mbox{reduced model}: && E[Y - 47* X_3] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\\\\n\\end{eqnarray*}\\]\n, reduced model specific form full model, two models nested. Using reformatted data, run linear models full reduced models calculate F statistic hand.","code":"\noz_g_lm <- lm(weight ~ gained, data = births14)\noz_ghm_lm <- lm(weight ~ gained + habit + mage, data = births14)\n\nanova(oz_g_lm)## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9    21.4 4.2e-06 ***\n## Residuals 939   1485     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(oz_ghm_lm)## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9   21.83 3.4e-06 ***\n## habit       1     25    25.3   16.31 5.8e-05 ***\n## mage        1      6     6.4    4.14   0.042 *  \n## Residuals 937   1453     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(oz_g_lm, oz_ghm_lm)## Analysis of Variance Table\n## \n## Model 1: weight ~ gained\n## Model 2: weight ~ gained + habit + mage\n##   Res.Df  RSS Df Sum of Sq    F  Pr(>F)    \n## 1    939 1485                              \n## 2    937 1453  2      31.7 10.2 4.1e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"build.html","id":"coefficient-of-partial-determination","chapter":"11 Statistical Model Building","heading":"11.1.2 Coefficient of Partial Determination","text":"Just \\(R^2\\) measures proportion reduction variation \\(Y\\) achieved set explanatory variables, coefficient partial determination measures marginal contribution one X variable (set X variables) already others model.\\[\\begin{eqnarray*}\nR^2_{Y 2|1} = \\frac{SSE(X_1) - SSE(X_1, X_2)}{SSE(X_1)} = \\frac{SSR(X_2|X_1)}{SSE(X_1)}\n\\end{eqnarray*}\\]coefficient partial determination measures marginal contribution one X variable others already included model.\\(R^2_{Y2|1}\\) measures proportionate reduction “variation \\(Y\\) remaining \\(X_1\\) included model” gained also including \\(X_2\\) model.","code":""},{"path":"build.html","id":"multicollinearity","chapter":"11 Statistical Model Building","heading":"11.2 Multicollinearity","text":"Consider multiple regression model:\n\\[\\begin{eqnarray*}\nE[Y] &=& \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2\\\\\nY &=& \\mbox{amount money pocket}\\\\\nX_1 &=& \\# \\mbox{ coins pocket}\\\\\nX_2 &=& \\# \\mbox{ pennies, nickels, dimes pocket}\n\\end{eqnarray*}\\]\nUsing completely non-random sample, got following data:things notice output:3 variables positively (pairwise correlated).effect number low coins positive amount , negative amount number coins model.number low coins significant , significant number coins model.\\(R^2_{Y1} = 63.363/(63.363 + 28.208 + 21.366) = 0.561\\)\\(R^2_{Y2|1} = 28.208 / (28.208 + 21.366) = 0.569\\). see number low coins added model already contains number coins, SSE reduced 56.9%. [ \\(SSR(X_1) = 63.363, SSR(X_2|X_1) = 28.208, SSE(X_1, X_2) = 21.366, SSE(X_1) = SSTO - SSR(X_1) = 28.208 + 21.366\\) ]","code":"\namount <- c(1.37, 1.01, 1.5, 0.56, 0.61, 3.06, 5.42, 1.75, 5.4, 0.56,\n               0.34, 2.33, 3.34)\nnum.coins <- c(9,10,3,5,10,37,28,9,11,4,6,17,15)\nnum.lowcoins <- c(4,8,0,4,9,34,9,3,2,2,5,12,11)\nlm(amount ~ num.coins) %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)    0.732    0.668       1.10  0.297 \n## 2 num.coins      0.108    0.0424      2.55  0.0269\nlm(amount ~ num.lowcoins) %>% tidy()## # A tibble: 2 × 5\n##   term         estimate std.error statistic p.value\n##   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)    1.73      0.680      2.54   0.0272\n## 2 num.lowcoins   0.0462    0.0591     0.781  0.451\nlm(amount ~ num.coins + num.lowcoins) %>% tidy()## # A tibble: 3 × 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     0.307    0.466      0.660 0.524   \n## 2 num.coins       0.296    0.0578     5.13  0.000443\n## 3 num.lowcoins   -0.246    0.0656    -3.75  0.00376\nlm(amount ~ num.coins + num.lowcoins) %>% anova()## Analysis of Variance Table\n## \n## Response: amount\n##              Df Sum Sq Mean Sq F value Pr(>F)   \n## num.coins     1  13.64   13.64    14.3 0.0036 **\n## num.lowcoins  1  13.48   13.48    14.1 0.0038 **\n## Residuals    10   9.57    0.96                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"build.html","id":"effects-of-multicollinearity","chapter":"11 Statistical Model Building","heading":"Effects of Multicollinearity","text":"reality, always degree correlation explanatory variables (pg 283 Kutner et al. (2004)). regression models, important understand entire context model, particularly correlated variables.Regardless degree multicollinearity, ability obtain good fit make predictions (mean individual) inhibited.variables highly correlated, many different linear combinations produce equally good fits. , different samples population may produce wildly different estimated coefficients. reason, variability associated coefficients can quite high. Additionally, explanatory variables can statistically significant even though definite relationship exists response set predictors.can longer interpret coefficient mean “change response variable increases one unit others held constant” may impossible hold variables constant. regression coefficients reflect inherent effect particular predictor variable response rather marginal partial effect given whatever correlated predictor variables included model.Recall \\(SE^2(\\underline{b}) = MSE\\cdot (X^t X)^{-1}\\). \\(X^t X\\) determinant close zero, taking inverse akin dividing zero. say, often SE b coefficients can large sampling variability.investigate multicollinearity depth Chapter 10 Variance Inflation Factor (VIF).Note: section ALSM 7.5 Chapter 8 [Although good stuff ! Section 7.5 discusses standardize variables – action can sometimes crucially important.]","code":""},{"path":"build.html","id":"model-selection","chapter":"11 Statistical Model Building","heading":"11.3 Model Selection","text":"need come something clever find model going use. need figure variables going enter model, going appear model (transformations, polynomials, interaction terms, etc), whether need also transform response variable. hard problem?\nSuppose first agree criterion “best model.” Maybe think best model lowest adjusted \\(R^2\\).\n(Recall \\(R_a^2=1-\\frac{n-1}{n-p}\\frac{SSE}{SSTO}\\)). Suppose also agree set variables enter model, suppose \\(m\\) . many models need look ?\nsmallest model predictors , .e. \\[E[Y]=\\beta_0+\\epsilon\\]\nlargest model \\(m\\) , .e. \\[E[Y]=\\beta_0+\\sum_{j=1}^m \\beta_jX_j+\\epsilon\\]\neverything .\ncan think like tree, two choices regarding first variable, either include , , two choices second, forth. \\(2^m\\) possible models look . Suppose \\(m=20\\), isn’t unusual. 20 variables, 1.05 million different possible models. 30 variables, 1.07 billion models. going search ? might \\(m\\) particularly small. \\(m=3\\) gives 8 possible models, quite feasible. search called subsets. otherwise, need something clever.Algorithm: Best subset selection (ISLR)Let \\(M_0\\) denote null model, contains predictors. null model predicts sample mean response variable observation.\\(k = 1, 2, \\ldots m\\):\nFit \\({m\\choose k}\\) models contain exactly \\(k\\) predictors (explanatory variables).\nPick best among \\({m\\choose k}\\) models, call \\(M_k\\). best7 defined smallest SSE, equivalently, largest \\(R^2\\).\nFit \\({m\\choose k}\\) models contain exactly \\(k\\) predictors (explanatory variables).Pick best among \\({m\\choose k}\\) models, call \\(M_k\\). best7 defined smallest SSE, equivalently, largest \\(R^2\\).Select single best model among \\(M_0, \\ldots ,M_{m}\\) using cross-validated prediction error, \\(C_p\\), AIC, BIC, adjusted \\(R^2\\).","code":""},{"path":"build.html","id":"forward-selection","chapter":"11 Statistical Model Building","heading":"11.3.1 Forward Selection","text":"start empty model add best available variable iteration, checking needed transformations. (also look interactions might suspect. However, looking possible interactions (2-way interactions, also consider 3-way interactions etc.), things can get hand quickly.) Generally forward selection:\n1. start response variable versus variables find best predictor. many, might just look correlation matrix. However, may miss variables good predictors aren’t linearly related. Therefore, possible, scatter plot matrix best.\n2. locate best variable, regress response variable .\n3. variable seems useful, keep move looking second.\n4. , stop.Algorithm: Forward stepwise subset selection (ISLR)Let \\(M_0\\) denote null model, contains predictors. null model predicts sample mean response variable observation.\\(k = 0, 1, \\ldots m-1\\):\nConsider \\(m - k\\) models augment predictors \\(M_k\\) one additional predictor.\nChoose best among \\(m - k\\) models, call \\(M_{k+1}\\). best defined smallest SSE highest \\(R^2\\).\nConsider \\(m - k\\) models augment predictors \\(M_k\\) one additional predictor.Choose best among \\(m - k\\) models, call \\(M_{k+1}\\). best defined smallest SSE highest \\(R^2\\).Select single best model among \\(M_0, \\ldots ,M_{m}\\) using cross-validated prediction error, \\(C_p\\), AIC, BIC, adjusted \\(R^2\\).Algorithm: Forward selection F testsLet \\(M_0\\) denote null model, contains predictors. null model predicts sample mean response variable observation.Let \\(k = 0\\):\nConsider \\(m - k\\) models augment predictors \\(M_k\\) one additional predictor.\nChoose best among \\(m - k\\) models, call \\(M_{k+1}\\). best defined smallest p-value including \\((k+1)^{th}\\) variable given \\(k\\) variables already model.\np-value step b. less \\(\\alpha_e\\), consider \\(M_{k+1}\\) augment \\(k\\) 1. Go back step . p-value step b. larger \\(\\alpha_e\\), report model \\(M_k\\) stop algorithm.\nConsider \\(m - k\\) models augment predictors \\(M_k\\) one additional predictor.Choose best among \\(m - k\\) models, call \\(M_{k+1}\\). best defined smallest p-value including \\((k+1)^{th}\\) variable given \\(k\\) variables already model.p-value step b. less \\(\\alpha_e\\), consider \\(M_{k+1}\\) augment \\(k\\) 1. Go back step . p-value step b. larger \\(\\alpha_e\\), report model \\(M_k\\) stop algorithm.","code":""},{"path":"build.html","id":"it-is-doing-exactly-what-we-want-right","chapter":"11 Statistical Model Building","heading":"It is doing exactly what we want, right???","text":"Suppose take exam covers 100 different topics, know . rules, however, state can bring two classmates consultants. Suppose also know topics classmates familiar . bring one consultant, easy figure bring: one knows topics (variable associated answer). Let’s say Kelly knows 85 topics.two consultants might choose Kelly first, second option, seems reasonable choose second knowledgeable classmate (second highly associated variable), example Jamie, knows 75 topics. problem strategy may 75 subjects Jamie knows already included 85 Kelly knows, therefore, Jamie provide knowledge beyond Kelly.better strategy select second considering knows regarding entire agenda, looking person knows topics first know (variable best explains residual equation variables entered). may even happen best pair consultants knowledgeable, may two complement perfectly way one knows 55 topics knows remaining 45, knowledgeable complement anybody.8","code":"Consider people A, B, C, D who know the following topics:\n\nA: {1, 2, 3, 4, 5, 6, 7}\n\nB: {8, 9, 10}\n\nC: {1, 2, 3, 4, 8, 10}\n\nD: {5, 6, 7, 9, 11}\n\n\nForward, choose A  and then B (and you'd know topics 1-10).  \nBackward (& best subsets), choose C and D (and you'd know topics 1-11)."},{"path":"build.html","id":"forward-stepwise-selection-using-f-tests","chapter":"11 Statistical Model Building","heading":"Forward Stepwise Selection using F-tests","text":"method follows way Forward Regression, new variable enters model, check see variables already model can now removed. done specifying two values, \\(\\alpha_e\\) \\(\\alpha\\) level needed enter model, \\(\\alpha_l\\) \\(\\alpha\\) level needed leave model. require \\(\\alpha_e<\\alpha_l\\), otherwise, algorithm cycle, add variable, immediately decide delete , continuing ad infinitum. bad.start empty model, add best predictor, assuming p-value associated smaller \\(\\alpha_e\\).Now, find best remaining variables, add p-value smaller \\(\\alpha_e\\). add , also check see first variable can dropped, calculating p-value associated (different first time, now two variables model). p-value greater \\(\\alpha_l\\), remove variable.continue process variables meet either requirements. many situations, help us stopping less desirable model.choose \\(\\alpha\\) values? set \\(\\alpha_e\\) small, might walk away variables model, least many. set large, wander around , good thing, explore models, may end variables model aren’t necessary.","code":""},{"path":"build.html","id":"backward-selection","chapter":"11 Statistical Model Building","heading":"11.3.2 Backward Selection","text":"Start full model including every term (possibly every interaction, etc.).Remove variable least significant (biggest p-value) model.Continue removing variables variables significant chosen \\(\\alpha\\) level.Algorithm: Backward stepwise selection (ISLR)Let \\(M_{full}\\) denote full model, contains \\(m\\) predictors.\\(k = m, m-1, \\ldots, 1\\):\nConsider k models contain one predictors \\(M_k\\) (including total \\(k - 1\\) predictors).\nChoose best among \\(k\\) models, call \\(M_{k-1}\\). best defined smallest SSE highest \\(R^2\\).\nConsider k models contain one predictors \\(M_k\\) (including total \\(k - 1\\) predictors).Choose best among \\(k\\) models, call \\(M_{k-1}\\). best defined smallest SSE highest \\(R^2\\).Select single best model among \\(M_0, \\ldots ,M_{m}\\) using cross-validated prediction error, \\(C_p\\), AIC, BIC, adjusted \\(R^2\\).Algorithm: Backward selection F testsLet \\(M_{full}\\) denote full model, contains \\(m\\) predictors.Let \\(k = m\\):\nConsider k models contain one predictors \\(M_k\\) (including total \\(k - 1\\) predictors).\nChoose best among \\(k\\) models, call \\(M_{k-1}\\). best defined removing largest p-value including \\((k)^{th}\\) variable given \\(k-1\\) variables already model.\np-value step b. larger \\(\\alpha_r\\), consider \\(M_{k}\\) decrease \\(k\\) 1. Go back step . p-value step b. smaller \\(\\alpha_r\\), report model \\(M_k\\) stop algorithm.\nConsider k models contain one predictors \\(M_k\\) (including total \\(k - 1\\) predictors).Choose best among \\(k\\) models, call \\(M_{k-1}\\). best defined removing largest p-value including \\((k)^{th}\\) variable given \\(k-1\\) variables already model.p-value step b. larger \\(\\alpha_r\\), consider \\(M_{k}\\) decrease \\(k\\) 1. Go back step . p-value step b. smaller \\(\\alpha_r\\), report model \\(M_k\\) stop algorithm.methods represent fool-proof strategy fitting model? , start. Remember, important always check residuals logical interpretation model.","code":""},{"path":"build.html","id":"other-ways-for-comparing-models","chapter":"11 Statistical Model Building","heading":"11.4 Other ways for comparing models","text":"\nFigure 6.4: strategy data analysis using statistical models. Source: Ramsey Schafer (2012)\n","code":""},{"path":"build.html","id":"analysis-of-appropriateness-of-model","chapter":"11 Statistical Model Building","heading":"11.4.1 Analysis of Appropriateness of Model","text":"","code":""},{"path":"build.html","id":"scatter-plot-matrix","chapter":"11 Statistical Model Building","heading":"Scatter Plot Matrix","text":"Plots variables others. Gives indications need consider transformations predictors. Also shows predictors highly correlated predictors, thus possibly needing included model.","code":""},{"path":"build.html","id":"correlation-matrix","chapter":"11 Statistical Model Building","heading":"Correlation Matrix","text":"numerical version scatterplot matrix, correlation matrix computes correlations groups variables. want predictor variables highly correlated response, need careful predictor variables highly correlated .","code":""},{"path":"build.html","id":"residual-plot","chapter":"11 Statistical Model Building","heading":"Residual Plot","text":"Plots fitted values residuals. , see trend constant variance.\nResiduals also plotted variables individually, including variables left model, well possible interactions.\ntrends residual plots variables model, might consider transformation adding polynomial term. trends residual plots variables left model, might consider adding variables model. trends residual plot interaction terms (like \\(X_1X_2\\)), might consider adding interaction term model.","code":""},{"path":"build.html","id":"getting-the-variables-right","chapter":"11 Statistical Model Building","heading":"11.5 Getting the Variables Right","text":"terms selecting variables model particular response, four things can happen:regression model correct!regression model underspecified.regression model contains extraneous variables.regression model overspecified.","code":""},{"path":"build.html","id":"underspecified","chapter":"11 Statistical Model Building","heading":"Underspecified","text":"regression model underspecified missing one important predictor variables. underspecified worst case scenario model ends biased predictions wrong virtually every observation. Additionally, estimate MSE tends big yields larger confidence intervals estimates (less chance significance).Consider another SAT dataset. see don’t stratify fraction students state took SAT (0-22%, 22-49%, 49-81%). much changes! slopes negative large group positive subgroups. Additionally, \\(R^2\\) value goes 0.193 0.806!! model without fraction students underspecified quite biased. doesn’t matter many observations collect, model always wrong. Underspecifying model worst possible things can happen.","code":"\nlibrary(mosaic)\nlibrary(mosaicData)\n\nSAT <- SAT %>%\n  mutate(SAT, frac_group = cut(frac, breaks=c(0, 22, 49, 81),\n                            labels=c(\"low fraction\", \n                                     \"medium fraction\", \n                                     \"high fraction\")))\n\nSAT %>%\n  ggplot(aes(x = salary, y = sat)) + \n  geom_point(aes(shape = frac_group, color = frac_group)) +\n  geom_smooth(method = \"lm\", se = FALSE)\nSAT %>%\n  ggplot(aes(x = salary, y = sat, group = frac_group, color = frac_group)) + \n  geom_point(aes(shape = frac_group)) +\n  geom_smooth(method = \"lm\", se = FALSE, fullrange = TRUE)"},{"path":"build.html","id":"extraneous","chapter":"11 Statistical Model Building","heading":"Extraneous","text":"third type variable situation comes extra variables included model variables neither related response correlated explanatory variables. Generally, extraneous variables problematic produce models unbiased coefficient estimators, unbiased predictions, unbiased MSE. worst thing happens error degrees freedom lowered makes confidence intervals wider p-values bigger (lower power). Also problematic model becomes unnecessarily complicated harder interpret.","code":""},{"path":"build.html","id":"overspecified","chapter":"11 Statistical Model Building","heading":"Overspecified","text":"model overspecified, one redundant variables. , variables contain information variables (.e., correlated!). ’ve seen, correlated variables cause trouble inflate variance coefficient estimates. correlated variables still possible get unbiased prediction estimates, coefficients variable interpreted (can inference easily performed).Generally: idea use model building strategy criteria (F-tests, AIC, BIC, Adjusted \\(R^2\\), \\(C_p\\), LASSO, Ridge regression) find middle ground underspecified model overspecified model.","code":""},{"path":"build.html","id":"a-model-building-strategy","chapter":"11 Statistical Model Building","heading":"11.6 A Model Building Strategy9","text":"Model building definitely art. Unsurprisingly, many approaches model building, one strategy, consisting seven steps, commonly used building regression model.","code":""},{"path":"build.html","id":"the-first-step","chapter":"11 Statistical Model Building","heading":"The first step","text":"Decide type model needed order achieve goals study. general, five reasons one might want build regression model. :predictive reasons - , model used predict response variable chosen set predictors.theoretical reasons - , researcher wants estimate model based known theoretical relationship response predictors.control purposes - , model used control response variable manipulating values predictor variables.inferential reasons - , model used explore strength relationships response predictors.data summary reasons - , model used merely way summarize large set data single equation.","code":""},{"path":"build.html","id":"the-second-step","chapter":"11 Statistical Model Building","heading":"The second step","text":"Decide explanatory variables response variable collect data. Collect data.","code":""},{"path":"build.html","id":"the-third-step","chapter":"11 Statistical Model Building","heading":"The third step","text":"Explore data. :univariate basis, check outliers, gross data errors, missing values.Study bivariate relationships reveal outliers, suggest possible transformations, identify possible multicollinearities.can’t possibly -emphasize data exploration step. ’s data analyst hasn’t made mistake skipping step later regretting data point found error, thereby nullifying hours work.","code":""},{"path":"build.html","id":"the-fourth-step","chapter":"11 Statistical Model Building","heading":"The fourth step","text":"(fourth step good modeling practice. gives sense whether ’ve overfit model building process.) Randomly divide data training set validation set:training set, least 15-20 error degrees freedom, used estimate model.validation set used cross-validation fitted model.","code":""},{"path":"build.html","id":"the-fifth-step","chapter":"11 Statistical Model Building","heading":"The fifth step","text":"Using training set, identify several candidate models:Use best subsets regression.Use subsets, stepwise, forward, backward selection regression. Using different alpha--remove alpha--enter values can lead variety models.","code":""},{"path":"build.html","id":"the-sixth-step","chapter":"11 Statistical Model Building","heading":"The sixth step","text":"Select evaluate “good” models:Select models based criteria learned, well number nature predictors.Evaluate selected models violation model conditions.none models provide satisfactory fit, try something else, collecting data, identifying different predictors, formulating different type model.","code":""},{"path":"build.html","id":"the-seventh-and-final-step","chapter":"11 Statistical Model Building","heading":"The seventh and final step","text":"Select final model:small mean square prediction error (larger cross-validation \\(R^2\\)) validation data good predictive model (population interest).Consider residual plots, outliers, parsimony, relevance, ease measurement predictors., , don’t forget necessarily one good model given set data. might equally satisfactory models.","code":""},{"path":"build.html","id":"thoughts-on-model-selection","chapter":"11 Statistical Model Building","heading":"11.6.1 Thoughts on Model Selection…","text":"Question: females receive lower starting salaries males?10model:\ny: log(salary)\nx’s: seniority, age, experience, education, sexIn Ramsey Schafer (2012), first find good model using seniority, age, experience, education (including considerations interactions/quadratics). find suitable model (Model 1), add sex variable model determine significant. (H0: Model 1 vs HA: Model 1 + sex) regression texts, models considered include sex variable beginning, work , always keeping sex variable . pluses/minuses approaches?Response seems possible, even likely, sex associated variables, depending model selection starts sex included done, entirely possible choose model includes sex one variables, sex significant. however, variables included, sex might explain significant amount variation beyond others. Whereas model selection doesn’t start sex likely include associated covariates start .One nice aspect methods end sex model; one difficulty model selection procedure ends removing variable interest people claim variable interest doesn’t matter. However, often advantageous avoid model selection much possible. model answers different question, ideally good decide ahead time question interest .case two questions interest; differences (univariate model sex), differences accounting covariates (multivariate model)? differences get smaller adjusting covariates, leads interesting question , whether differences also part sex discrimination. Consider explanation wage gap men women due men higher-paying jobs, really, ’s part problem, jobs women pay less. :( point, though, one model may sufficient particular situation, looking one “best” model can misleading.","code":""},{"path":"build.html","id":"reflection-questions-8","chapter":"11 Statistical Model Building","heading":"11.7  Reflection Questions","text":"","code":""},{"path":"build.html","id":"decomposing-sums-of-squares","chapter":"11 Statistical Model Building","heading":"11.7.1 Decomposing sums of squares","text":"SSE, SSR, SSTO change variables added model?\\(SSR(X_2 | X_1)\\) really mean? defined?break SSR ANOVA table? Note: order variables maters!tests sets coefficients aren’t right order? equal constant? equal ?nested F-test without anova() output, find \\(F^*\\) critical value? (Hint: use qf() R)","code":""},{"path":"build.html","id":"multicollinearity-1","chapter":"11 Statistical Model Building","heading":"11.7.2 Multicollinearity","text":"multicollinearity mean?effects multicollinearity various aspects regression analyses?effects correlated predictors various aspects regression analyses?(ALSM chapter 10: variance inflation factors, use help detect multicollinearity)can multicollinearity problems reduced analysis?big regression pitfalls? (including extrapolation, non-constant variance, autocorrelation (e.g., time series), overfitting, excluding important predictor variables, missing data, power sample size.)","code":""},{"path":"build.html","id":"building-models","chapter":"11 Statistical Model Building","heading":"11.7.3 Building models","text":"impact four different kinds models respect “correctness”: correctly specified, underspecified, overspecified, correct extraneous predictors?conduct stepwise regression “hand” (using either F tests one criteria)?limitations stepwise regression?can choose optimal model based \\(R^2\\) value, \\(R^2_{adj}\\), MSE, AIC, BIC, \\(C_p\\) criterion?seven steps good model building strategy?","code":""},{"path":"build.html","id":"ethics-considerations-7","chapter":"11 Statistical Model Building","heading":"11.8  Ethics Considerations","text":"Consider following example Building Compassion Modelling Variables Heather Krause Count.example describes hypothetical budget app whose goal help user save money. explanatory variable whether app used; response variable amount money saved.Heather suggests three strategies model variables interest three strategies privilege different groups people. three strategies, randomly assign budgeting app individuals study order determine causal mechanism budgeting app amount money saved.additional interest variable describes proportion individual’s income disposable income (amount left paying food, shelter, heat, etc.).key blog define technical details work different types variables (confounder, moderator, mediator). point blog recognize different ways model covariates, model change results privilege.","code":""},{"path":"build.html","id":"strategy-1-fix-disposable-income-proportion-confounder","chapter":"11 Statistical Model Building","heading":"Strategy 1: fix disposable income proportion (confounder)","text":"first strategy include proportion disposable income individuals model covariate. , interpretation budgeting app coefficient given “variables constant.” model can something straightforward linear model,maybe doesn’t make sense whole point app change proportion disposable income, turn affects amount savings.Using example blog, :isolate effect budgeting app regardless portion income disposable. run model turns app isn’t much effect whether people saved least $175 extra dollars. Sorry guys, say meeting.","code":""},{"path":"build.html","id":"strategy-2-disposable-income-proportion-interacting-moderator","chapter":"11 Statistical Model Building","heading":"Strategy 2: disposable income proportion interacting (moderator)","text":"amount disposable income changes impact budgeting app amount money saved, say disposable income budgeting app interact. , reasonably flush, maybe app can really help save lot money. finances already tight, probably aren’t spending lot money non-essentials. budgeting app may help low disposable income proportion.use model, assuming success app depends entirely whether can adjust disposable income, giving voice people across spectrum experience, positive negative. ’s really important care helping people already budgeted within inch life.Sure app might prettier, faster way , might worth 175 dollars someone counting .","code":""},{"path":"build.html","id":"strategy-3-disposable-income-proportion-as-a-mediator","chapter":"11 Statistical Model Building","heading":"Strategy 3: disposable income proportion as a mediator","text":"mediator variable one causal pathway \\(X\\) \\(Y\\). modeling strategy involves causal inference, possibly modeling directed acyclic graph (DAG). Alternatively, first model whether \\(X\\) predicts mediator whether mediator predicts \\(Y\\) (Baron Kenny 1986). Lots information .model shows app performs assuming position adjust expenses, expected mechanism change. say “app works” based model privileging position benefit works. case people financial leeway .","code":""},{"path":"build.html","id":"r-anova-decompisition---tips","chapter":"11 Statistical Model Building","heading":"11.9 R: ANOVA decompisition - tips","text":"return tips data used Section 9.611.best model explain variation tips?Note: single dataset used build fit model, penalize fitting (think \\(R^2_{adj}\\)). different data used, .e., CV, need use metric penalization.","code":"\ntip_fit <- linear_reg() %>%\n  set_engine(\"lm\") %>%\n  fit(Tip ~ Party + Age, data = tips)"},{"path":"build.html","id":"analysis-of-variance-anova","chapter":"11 Statistical Model Building","heading":"11.9.1 Analysis of variance (ANOVA)","text":"Main Idea: Decompose total variation outcome :\nvariation can explained variables model\nvariation can’t explained model (left residuals)\nvariation can explained variables modelthe variation can explained variables modelthe variation can’t explained model (left residuals)variation can’t explained model (left residuals)variation can explained variables model greater variation residuals, signals model might “valuable” (least one \\(\\beta\\)s equal 0)","code":"\nanova(tip_fit$fit) %>%\n  tidy() %>%\n  kable(digits = 2)"},{"path":"build.html","id":"anova-output-with-totals","chapter":"11 Statistical Model Building","heading":"ANOVA output, with totals","text":"","code":""},{"path":"build.html","id":"sum-of-squares","chapter":"11 Statistical Model Building","heading":"Sum of squares","text":"\\(SS_{Total}\\): Total sum squares, variability outcome, \\(\\sum_{= 1}^n (y_i - \\bar{y})^2\\)\\(SS_{Error}\\): Residual sum squares, variability residuals, \\(\\sum_{= 1}^n (y_i - \\hat{y})^2\\)\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained model","code":""},{"path":"build.html","id":"r-nested-f-test---births","chapter":"11 Statistical Model Building","heading":"11.10 R: nested F test - births","text":"Every year, US releases public large data set containing information births recorded country. data set interest medical researchers studying relation habits practices expectant mothers birth children. random sample 1,000 cases data set released 2014. Data description .Notice variable names ’ll use mage, weight, gained, habit.Interested predicting baby’s weight pounds mother gained.smoking habit included variable?smoking habit weight gained interact?happens model another quantitative variable, mother’s age mage, added?happens model another quantitative variable added interaction?F-test see mage added model interaction? (borderline, p-value mage 0.0423 \\(H_0: \\beta_{mage}=0\\))Can interaction mage dropped? (Yes, p-value reasonably large indicating evidence either coefficients different zero. Now test \\(H_0: \\beta_{gainxsmk} = \\beta_{mage} = 0\\).)Calculate coefficient partial determination mage given gained habit.variability (ounces baby weight) remaining modeling gained habit reduced 0.4% additionally adding mage.happens variables entered model different order?different note… variable term three levels (“early,” “full,” “late”). Notice variable can model model model uses two degrees freedom two coefficients estimate. term interacts gained additional two coefficients estimate (interaction coefficients model).","code":"\nlibrary(openintro)\ndata(births14)\nbirths14 <- births14 %>%\n  select(-fage, -visits) %>%\n  drop_na() %>%\n  mutate(term = case_when(\n    weeks <= 38 ~ \"early\",\n    weeks <= 40 ~ \"full\",\n    TRUE ~ \"late\"\n  ))\n\nnames(births14)##  [1] \"mage\"           \"mature\"         \"weeks\"          \"premie\"        \n##  [5] \"gained\"         \"weight\"         \"lowbirthweight\" \"sex\"           \n##  [9] \"habit\"          \"marital\"        \"whitemom\"       \"term\"\noz_g_lm <- lm(weight ~ gained, data=births14)\n\noz_g_lm %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic    p.value\n##   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n## 1 (Intercept)   6.83     0.0912      74.9  0         \n## 2 gained        0.0124   0.00267      4.63 0.00000423\noz_g_lm %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9    21.4 4.2e-06 ***\n## Residuals 939   1485     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noz_gh_lm <- lm(weight ~ gained + habit, data=births14)\n\noz_gh_lm %>% tidy()## # A tibble: 3 × 5\n##   term        estimate std.error statistic    p.value\n##   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n## 1 (Intercept)   6.91     0.0924      74.8  0         \n## 2 gained        0.0119   0.00266      4.49 0.00000813\n## 3 habitsmoker  -0.507    0.126       -4.03 0.0000598\noz_gh_lm %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9    21.8 3.5e-06 ***\n## habit       1     25    25.3    16.3 6.0e-05 ***\n## Residuals 938   1460     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noz_gih_lm <- lm(weight ~ gained * habit, data=births14)\n\noz_gih_lm %>% tidy()## # A tibble: 4 × 5\n##   term               estimate std.error statistic   p.value\n##   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)         6.89      0.0978     70.5   0        \n## 2 gained              0.0124    0.00285     4.35  0.0000153\n## 3 habitsmoker        -0.400     0.259      -1.55  0.123    \n## 4 gained:habitsmoker -0.00368   0.00782    -0.471 0.638\noz_gih_lm %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##               Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained         1     34    33.9   21.74 3.6e-06 ***\n## habit          1     25    25.3   16.24 6.0e-05 ***\n## gained:habit   1      0     0.3    0.22    0.64    \n## Residuals    937   1459     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noz_ghm_lm <- lm(weight ~ gained + habit + mage, data=births14)\n\noz_ghm_lm %>% tidy()## # A tibble: 4 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   6.49     0.225       28.8  2.94e-131\n## 2 gained        0.0122   0.00265      4.59 5.11e-  6\n## 3 habitsmoker  -0.494    0.126       -3.93 9.07e-  5\n## 4 mage          0.0144   0.00707      2.03 4.22e-  2\noz_ghm_lm %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9   21.83 3.4e-06 ***\n## habit       1     25    25.3   16.31 5.8e-05 ***\n## mage        1      6     6.4    4.14   0.042 *  \n## Residuals 937   1453     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noz_gihm_lm <- lm(weight ~ gained * habit + mage, data=births14)\n\noz_gihm_lm %>% tidy()## # A tibble: 5 × 5\n##   term               estimate std.error statistic   p.value\n##   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)         6.48      0.227      28.5   2.27e-129\n## 2 gained              0.0126    0.00285     4.43  1.07e-  5\n## 3 habitsmoker        -0.397     0.259      -1.54  1.25e-  1\n## 4 mage                0.0143    0.00707     2.02  4.33e-  2\n## 5 gained:habitsmoker -0.00335   0.00781    -0.430 6.68e-  1\noz_gihm_lm %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##               Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained         1     34    33.9   21.81 3.4e-06 ***\n## habit          1     25    25.3   16.30 5.9e-05 ***\n## mage           1      6     6.4    4.13   0.042 *  \n## gained:habit   1      0     0.3    0.18   0.668    \n## Residuals    936   1453     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(oz_gih_lm, oz_gihm_lm)## Analysis of Variance Table\n## \n## Model 1: weight ~ gained * habit\n## Model 2: weight ~ gained * habit + mage\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)  \n## 1    937 1459                           \n## 2    936 1453  1      6.36 4.09  0.043 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nanova(oz_gh_lm, oz_gihm_lm)## Analysis of Variance Table\n## \n## Model 1: weight ~ gained + habit\n## Model 2: weight ~ gained * habit + mage\n##   Res.Df  RSS Df Sum of Sq    F Pr(>F)\n## 1    938 1460                         \n## 2    936 1453  2       6.7 2.16   0.12\nanova(oz_ghm_lm)## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9   21.83 3.4e-06 ***\n## habit       1     25    25.3   16.31 5.8e-05 ***\n## mage        1      6     6.4    4.14   0.042 *  \n## Residuals 937   1453     1.6                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n6.42 / (6.42 + 1453.37)## [1] 0.0044\nlm(weight ~ marital + mage, data=births14) %>% tidy()## # A tibble: 3 × 5\n##   term               estimate std.error statistic   p.value\n##   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)         7.08      0.245      28.9   4.87e-132\n## 2 maritalnot married -0.199     0.0926     -2.15  3.22e-  2\n## 3 mage                0.00725   0.00787     0.920 3.58e-  1\nlm(weight ~ mage + marital, data=births14) %>% tidy()## # A tibble: 3 × 5\n##   term               estimate std.error statistic   p.value\n##   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)         7.08      0.245      28.9   4.87e-132\n## 2 mage                0.00725   0.00787     0.920 3.58e-  1\n## 3 maritalnot married -0.199     0.0926     -2.15  3.22e-  2\nlm(weight ~ marital + mage, data=births14) %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value Pr(>F)   \n## marital     1     12   12.33    7.69 0.0057 **\n## mage        1      1    1.36    0.85 0.3576   \n## Residuals 938   1505    1.60                  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nlm(weight ~ mage + marital, data=births14) %>% anova()## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value Pr(>F)  \n## mage        1      6    6.31    3.93  0.048 *\n## marital     1      7    7.38    4.60  0.032 *\n## Residuals 938   1505    1.60                 \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\noz_gtm_lm <- lm(weight ~ gained + term + mage, data=births14)\noz_gitm_lm <- lm(weight ~ gained * term + mage, data=births14)\n\ntidy(oz_gtm_lm)## # A tibble: 5 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   5.72     0.213       26.9  2.32e-118\n## 2 gained        0.0109   0.00246      4.44 9.93e-  6\n## 3 termfull      1.02     0.0833      12.3  3.99e- 32\n## 4 termlate      1.07     0.113        9.49 1.85e- 20\n## 5 mage          0.0171   0.00654      2.61 9.08e-  3\nanova(oz_gtm_lm)## Analysis of Variance Table\n## \n## Response: weight\n##            Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained      1     34    33.9   25.40 5.6e-07 ***\n## term        2    228   114.2   85.67 < 2e-16 ***\n## mage        1      9     9.1    6.84  0.0091 ** \n## Residuals 936   1248     1.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ntidy(oz_gitm_lm)## # A tibble: 7 × 5\n##   term             estimate std.error statistic   p.value\n##   <chr>               <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)      5.77       0.236     24.5    1.26e-102\n## 2 gained           0.00929    0.00430    2.16   3.11e-  2\n## 3 termfull         0.911      0.187      4.88   1.28e-  6\n## 4 termlate         1.09       0.247      4.42   1.08e-  5\n## 5 mage             0.0171     0.00655    2.61   9.27e-  3\n## 6 gained:termfull  0.00365    0.00559    0.652  5.14e-  1\n## 7 gained:termlate -0.000469   0.00703   -0.0667 9.47e-  1\nanova(oz_gitm_lm)## Analysis of Variance Table\n## \n## Response: weight\n##              Df Sum Sq Mean Sq F value  Pr(>F)    \n## gained        1     34    33.9   25.37 5.7e-07 ***\n## term          2    228   114.2   85.54 < 2e-16 ***\n## mage          1      9     9.1    6.83  0.0091 ** \n## gained:term   2      1     0.4    0.30  0.7388    \n## Residuals   934   1247     1.3                    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"build.html","id":"r-model-building---sat-scores","chapter":"11 Statistical Model Building","heading":"11.11 R: model building - SAT scores","text":"notes belong end… ’m putting highlight full analysis.Notice \\(C_p\\) F-tests use “full” model MSE. Typically, MSE unbiased predictor \\(\\sigma^2\\) backwards variable selection.BIC usually results fewer parameters model AIC.Using different selection criteria may lead different models (one best model).order variables entered necessarily represent importance. variable entered early can dropped later stage predicted well explanatory variables subsequently added model.Consider multiple regression model:\n\\[\\begin{eqnarray*}\nE[Y] &=& \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 + \\beta_6 X_6\\\\\nY &=& \\mbox{state ave SAT score}\\\\\nX_1 &=& \\% \\mbox{ eligible seniors took exam, } takers\\\\\nX_2 &=& \\mbox{median income families test takers, } income\\\\\nX_3 &=& \\mbox{ave number years formal eduction, } years\\\\\nX_4 &=& \\% \\mbox{ test takers attend public school, } public\\\\\nX_5 &=& \\mbox{total state expenditure public secondary schools (\\$100 /student), } expend \\\\\nX_6 &=& \\mbox{median percentile rank test takers within secondary school class, } rank\n\\end{eqnarray*}\\]","code":"\nsat_data <- read_csv(\"http://pages.pomona.edu/~jsh04747/courses/math158/sat.csv\") %>%\n  mutate(ltakers = log(takers))\n\nsat_n <- nrow(sat_data)  # always be wary of missing rows, though!"},{"path":"build.html","id":"aic-and-bic-in-r","chapter":"11 Statistical Model Building","heading":"AIC and BIC in R","text":"model variablesThe model ltakers","code":"\nlm(sat ~ 1, data = sat_data) %>% tidy()## # A tibble: 1 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)     948.      10.2      92.9 7.89e-56\nlm(sat ~ 1, data = sat_data) %>% glance() %>%\n  mutate(mse = sigma^2, sse = mse*df.residual) %>%\n  select(mse, sse, AIC, BIC, df.residual, nobs)## # A tibble: 1 × 6\n##     mse     sse   AIC   BIC df.residual  nobs\n##   <dbl>   <dbl> <dbl> <dbl>       <int> <int>\n## 1 5112. 245376.  560.  564.          48    49\nlm(sat ~ ltakers, data = sat_data) %>% tidy()## # A tibble: 2 × 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   1112.      12.4       89.8 3.11e-54\n## 2 ltakers        -59.2      4.17     -14.2 1.27e-18\nlm(sat ~ ltakers, data = sat_data) %>% glance() %>%\n  mutate(mse = sigma^2, sse = mse*df.residual) %>%\n  select(mse, sse, AIC, BIC, df.residual, nobs)## # A tibble: 1 × 6\n##     mse    sse   AIC   BIC df.residual  nobs\n##   <dbl>  <dbl> <dbl> <dbl>       <int> <int>\n## 1  987. 46369.  481.  487.          47    49"},{"path":"build.html","id":"best-possible-subsets","chapter":"11 Statistical Model Building","heading":"Best possible subsets","text":"Note outcome best model one variable, best model two variables, etc. Therefore, selection based SSE . job analyst differentiate models different sizes. use \\(R_{adj}^2\\), AIC, BIC, etc. seems like 4 variable model good contender best model.","code":"\nbest_lms<- leaps::regsubsets(sat ~ ltakers + income + years + public + expend + rank, \n        data=sat_data, nvmax = 6)\nbest_lms %>% tidy()## # A tibble: 6 × 11\n##   `(Intercept)` ltakers income years public expend rank  r.squared adj.r.squared\n##   <lgl>         <lgl>   <lgl>  <lgl> <lgl>  <lgl>  <lgl>     <dbl>         <dbl>\n## 1 TRUE          TRUE    FALSE  FALSE FALSE  FALSE  FALSE     0.811         0.807\n## 2 TRUE          TRUE    FALSE  FALSE FALSE  TRUE   FALSE     0.895         0.890\n## 3 TRUE          TRUE    FALSE  TRUE  FALSE  TRUE   FALSE     0.900         0.893\n## 4 TRUE          TRUE    FALSE  TRUE  FALSE  TRUE   TRUE      0.911         0.903\n## 5 TRUE          TRUE    TRUE   TRUE  FALSE  TRUE   TRUE      0.913         0.903\n## 6 TRUE          TRUE    TRUE   TRUE  TRUE   TRUE   TRUE      0.913         0.900\n## # … with 2 more variables: BIC <dbl>, mallows_cp <dbl>"},{"path":"build.html","id":"forward-variable-selection-f-tests","chapter":"11 Statistical Model Building","heading":"Forward Variable Selection: F-tests","text":"Note: Sum Sq refers SSR(new variable \\(|\\) current model) (additional reduction SSE). RSS SSE model contains current variables new variable.","code":"\nadd1(lm(sat ~ 1, data = sat_data), sat ~ ltakers + income + years + \n       public + expend + rank, test = \"F\")## Single term additions\n## \n## Model:\n## sat ~ 1\n##         Df Sum of Sq    RSS AIC F value  Pr(>F)    \n## <none>               245376 419                    \n## ltakers  1    199007  46369 340  201.71 < 2e-16 ***\n## income   1    102026 143350 395   33.45 5.7e-07 ***\n## years    1     26338 219038 416    5.65   0.022 *  \n## public   1      1232 244144 421    0.24   0.629    \n## expend   1       386 244991 421    0.07   0.787    \n## rank     1    190297  55079 348  162.38 < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nadd1(lm(sat ~ ltakers, data = sat_data), sat ~ ltakers + income + years + \n       public + expend + rank, test = \"F\")## Single term additions\n## \n## Model:\n## sat ~ ltakers\n##        Df Sum of Sq   RSS AIC F value  Pr(>F)    \n## <none>              46369 340                    \n## income  1       785 45584 341    0.79  0.3781    \n## years   1      6364 40006 335    7.32  0.0095 ** \n## public  1       449 45920 341    0.45  0.5058    \n## expend  1     20523 25846 313   36.53 2.5e-07 ***\n## rank    1       871 45498 341    0.88  0.3529    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nadd1(lm(sat ~ ltakers + expend, data = sat_data), sat ~ ltakers + income + years + \n       public + expend + rank, test = \"F\")## Single term additions\n## \n## Model:\n## sat ~ ltakers + expend\n##        Df Sum of Sq   RSS AIC F value Pr(>F)\n## <none>              25846 313               \n## income  1        53 25792 315    0.09   0.76\n## years   1      1248 24598 313    2.28   0.14\n## public  1         1 25845 315    0.00   0.96\n## rank    1      1054 24792 313    1.91   0.17"},{"path":"build.html","id":"backward-variable-selection-f-tests","chapter":"11 Statistical Model Building","heading":"Backward Variable Selection: F-tests","text":"add1 (see makes sense add either public income back), neither significant.","code":"\n# all the variables\ndrop1(lm(sat ~ ltakers + income + years + public + expend + rank, \n         data=sat_data), test=\"F\")## Single term deletions\n## \n## Model:\n## sat ~ ltakers + income + years + public + expend + rank\n##         Df Sum of Sq   RSS AIC F value  Pr(>F)    \n## <none>               21397 312                    \n## ltakers  1      2150 23547 315    4.22   0.046 *  \n## income   1       340 21737 311    0.67   0.418    \n## years    1      2532 23928 315    4.97   0.031 *  \n## public   1        20 21417 310    0.04   0.844    \n## expend   1     10964 32361 330   21.52 3.4e-05 ***\n## rank     1      2679 24076 316    5.26   0.027 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# less public\ndrop1(lm(sat ~ ltakers + income + years + expend + rank, \n         data=sat_data), test=\"F\")## Single term deletions\n## \n## Model:\n## sat ~ ltakers + income + years + expend + rank\n##         Df Sum of Sq   RSS AIC F value Pr(>F)    \n## <none>               21417 310                   \n## ltakers  1      2552 23968 313    5.12  0.029 *  \n## income   1       505 21922 309    1.01  0.319    \n## years    1      3011 24428 314    6.05  0.018 *  \n## expend   1     12465 33882 330   25.03  1e-05 ***\n## rank     1      3162 24578 315    6.35  0.016 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# less income\ndrop1(lm(sat ~ ltakers + years + expend + rank, \n         data=sat_data), test=\"F\")## Single term deletions\n## \n## Model:\n## sat ~ ltakers + years + expend + rank\n##         Df Sum of Sq   RSS AIC F value  Pr(>F)    \n## <none>               21922 309                    \n## ltakers  1      5094 27016 317   10.22  0.0026 ** \n## years    1      2870 24792 313    5.76  0.0207 *  \n## expend   1     13620 35542 331   27.34 4.5e-06 ***\n## rank     1      2676 24598 313    5.37  0.0252 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"build.html","id":"forward-stepwise-aic","chapter":"11 Statistical Model Building","heading":"Forward Stepwise: AIC","text":"","code":"\n# lives in the base R stats package, but can be overwritten by other functions\nstats::step(lm(sat ~ 1, data=sat_data), \n     sat ~ ltakers + income + years + public + expend + rank,\n     direction = \"forward\")## Start:  AIC=419\n## sat ~ 1\n## \n##           Df Sum of Sq    RSS AIC\n## + ltakers  1    199007  46369 340\n## + rank     1    190297  55079 348\n## + income   1    102026 143350 395\n## + years    1     26338 219038 416\n## <none>                 245376 419\n## + public   1      1232 244144 421\n## + expend   1       386 244991 421\n## \n## Step:  AIC=340\n## sat ~ ltakers\n## \n##          Df Sum of Sq   RSS AIC\n## + expend  1     20523 25846 313\n## + years   1      6364 40006 335\n## <none>                46369 340\n## + rank    1       871 45498 341\n## + income  1       785 45584 341\n## + public  1       449 45920 341\n## \n## Step:  AIC=313\n## sat ~ ltakers + expend\n## \n##          Df Sum of Sq   RSS AIC\n## + years   1      1248 24598 313\n## + rank    1      1054 24792 313\n## <none>                25846 313\n## + income  1        53 25792 315\n## + public  1         1 25845 315\n## \n## Step:  AIC=313\n## sat ~ ltakers + expend + years\n## \n##          Df Sum of Sq   RSS AIC\n## + rank    1      2676 21922 309\n## <none>                24598 313\n## + public  1       288 24310 314\n## + income  1        19 24578 315\n## \n## Step:  AIC=309\n## sat ~ ltakers + expend + years + rank\n## \n##          Df Sum of Sq   RSS AIC\n## <none>                21922 309\n## + income  1       505 21417 310\n## + public  1       185 21737 311## \n## Call:\n## lm(formula = sat ~ ltakers + expend + years + rank, data = sat_data)\n## \n## Coefficients:\n## (Intercept)      ltakers       expend        years         rank  \n##       399.1        -38.1          4.0         13.1          4.4"},{"path":"build.html","id":"backward-stepwise-bic","chapter":"11 Statistical Model Building","heading":"Backward Stepwise: BIC","text":"get idea complicated models can get, try :","code":"\n# lives in the base R stats package, but can be overwritten by other functions\nstats::step(lm(sat ~ (ltakers + income + years + public + expend + rank), \n        data=sat_data),\n     direction = \"backward\", k=log(sat_n))## Start:  AIC=325\n## sat ~ (ltakers + income + years + public + expend + rank)\n## \n##           Df Sum of Sq   RSS AIC\n## - public   1        20 21417 321\n## - income   1       340 21737 322\n## <none>                 21397 325\n## - ltakers  1      2150 23547 326\n## - years    1      2532 23928 327\n## - rank     1      2679 24076 327\n## - expend   1     10964 32361 342\n## \n## Step:  AIC=321\n## sat ~ ltakers + income + years + expend + rank\n## \n##           Df Sum of Sq   RSS AIC\n## - income   1       505 21922 319\n## <none>                 21417 321\n## - ltakers  1      2552 23968 323\n## - years    1      3011 24428 324\n## - rank     1      3162 24578 324\n## - expend   1     12465 33882 340\n## \n## Step:  AIC=319\n## sat ~ ltakers + years + expend + rank\n## \n##           Df Sum of Sq   RSS AIC\n## <none>                 21922 319\n## - rank     1      2676 24598 320\n## - years    1      2870 24792 321\n## - ltakers  1      5094 27016 325\n## - expend   1     13620 35542 338## \n## Call:\n## lm(formula = sat ~ ltakers + years + expend + rank, data = sat_data)\n## \n## Coefficients:\n## (Intercept)      ltakers        years       expend         rank  \n##       399.1        -38.1         13.1          4.0          4.4step(lm(sat ~ (ltakers + income + years + public + expend + rank)^2),\n            direction = \"backward\")"},{"path":"diag2.html","id":"diag2","chapter":"12 Diagnostic Measures II","heading":"12 Diagnostic Measures II","text":"","code":""},{"path":"diag2.html","id":"main-idea","chapter":"12 Diagnostic Measures II","heading":"Main idea","text":"\nFigure 1.3: point () little influence model, despite large residual. point (b) little influence model consistent model given points. point (c) high residual high leverage impact prediction coefficients model.\nmain idea section understand individual observations (subsets observations) impact model. talk values outlying x-direction outlying y-direction. Depending combination outlyingness, point may may impact model. already pieces (e.g., hat matrix) evaluate influence point model.High residuals (big enough) can impact model. High leverage (big enough) can impact model. big residual big leverage definitely impact coefficients therefore predictions linear model.Consider hat matrix (see Section 8.5):\n\\[\\begin{eqnarray*}\nH &=& X(X^t X)^{-1} X^t\\\\\n\\underline{\\hat{Y}} &=& H\\underline{Y}, \\underline{e} = (-H)\\underline{Y}\\\\\nsd^2(\\underline{e}) &=& \\sigma^2 \\cdot (-H), se^2(\\underline{e}) = MSE \\cdot (-H) \\\\\n\\end{eqnarray*}\\]","code":""},{"path":"diag2.html","id":"outliers-in-the-y-direction","chapter":"12 Diagnostic Measures II","heading":"12.1 Outliers in the Y direction","text":"Points outlying terms \\(Y\\)-values often easy spot via residual plots, model predict values well large residual seen. However, sometimes observation enough influence model, however, residual small even point ``outlying y-direction.”","code":""},{"path":"diag2.html","id":"internally-studentized-residuals","chapter":"12 Diagnostic Measures II","heading":"12.1.1 (Internally) Studentized Residuals","text":"Recall previously defined semi-studentized residuals residual divided MSE (best estimate \\(\\sigma^2\\)). also pointed variance residuals isn’t really \\(\\sigma^2\\) isn’t constant (requirement residuals sum zero).\n\\[\\begin{eqnarray*}\ne_i &=& Y_i - \\hat{Y}_i\\\\\ne_i^* &=& \\frac{Y_i - \\hat{Y}_i}{\\sqrt{MSE}} \\ \\ \\ \\ \\mbox{semi-studentized residuals}\n\\end{eqnarray*}\\]\nhat matrix measure observations fall subspace spanned explanatory variables. hat matrix also provides information predictions (turn determine residuals). turns variance residuals can calculated using hat matrix:\n\\[\\begin{eqnarray*}\nsd^2(\\underline{e}) &=& \\sigma^2 (-H)\\\\\nsd^2(e_i) &=& \\sigma^2 (1-h_{ii})\\\\\nse^2\\{\\underline{e}\\} &=& MSE (-H)\\\\\nse^2(e_i) &=& MSE (1-h_{ii})\\\\\nse(e_i) &=& \\sqrt{MSE(1-h_{ii})}\\\\\nr_i &=& \\frac{e_i}{se(e_i)}  \\ \\ \\ \\mbox{internally studentized residuals}\n\\end{eqnarray*}\\]\nNote studentized residuals constant variance (unlike standard residuals). ? due theory connecting hat-matrix terms variability residuals. \\(h_{ii}\\) large (\\(^{th}\\) point lot leverage), \\(se(e_i)\\) smaller corresponding value points low leverage.","code":""},{"path":"diag2.html","id":"deleted-residuals-externally-studentized-residuals","chapter":"12 Diagnostic Measures II","heading":"12.1.2 Deleted Residuals (externally studentized residuals)","text":"observation far outlying, may influence regression line extent point longer looks outlying. One solution problem fit model without outlying value measure far \\(^{th}\\) response model created without \\(^{th}\\) observation.\nLet \\(\\hat{Y}_{()}\\) represent predicted value \\(^{th}\\) observation created using remaining \\(n-1\\) observations. [Note: think \\(Y_i\\) fixed, \\(Y_{()}\\) varying.]\n\\[\\begin{eqnarray*}\nd_i &=& Y_i - \\hat{Y}_{()} \\ \\ \\ \\ \\mbox{ deleted residuals}\\\\\n&=& \\frac{e_i}{(1-h_{ii})} \\mbox{,   } e_i = Y_i - \\hat{Y}_i \\ \\ \\ \\ \\mbox{ obvious, see text}\\\\\nse^2(d_i) &=& MSE_{()} \\cdot (1 + X_i^t (X_{()}^t X_{()})^{-1} X_i) \\ \\ \\ \\mbox{recall:  } se^2(\\hat{Y}_{h(new)}) = MSE \\cdot (1+ X_h^t (X^t X)^{-1} X_h)\\\\\n&=& \\frac{MSE_{()}}{(1-h_{ii})} \\ \\ \\ \\ \\mbox{ obvious, see text}\\\\\n&&\\\\\nt_i &=& \\frac{d_i}{se(d_i)} = \\frac{e_i}{\\sqrt{MSE_{()}(1-h_{ii})}} \\sim t_{n-1-p}  \\ \\ \\mbox{ studentized deleted residuals}\\\\\n\\end{eqnarray*}\\]Now can legitimately look deviations outside \\(\\pm t_{1 - (\\alpha/2n), n-p-1}\\) bounds. use Bonferroni correction level significance \\(n\\) simultaneous tests.Note SE calculated exact formula used prediction intervals. , predicting \\(^{th}\\) observation \\((n-1)\\) observations., can write \\(t_i\\) function \\(e_i\\) \\(h_{ii}\\). alternative notation makes clear large deleted residuals come combination moderate / large residuals moderate / large leverage.","code":""},{"path":"diag2.html","id":"test-for-outliers","chapter":"12 Diagnostic Measures II","heading":"Test for Outliers","text":"identify outlying \\(Y\\) observations cases whose studentized deleted residuals large absolute value. addition, can conduct formal test means Bonferroni test procedure whether case largest absolute studentized deleted residual outlier. Since know advance case largest absolute value \\(|t_i|\\), consider family tests include \\(n\\) tests, one\ncase.regression model appropriate, case outlying change model, studentized deleted residual follow \\(t-\\)distribution \\(n-p-1\\) degrees freedom. appropriate Bonferroni critical value therefore \\(t_{(1 - \\alpha/2n; n - p - 1)}\\). Note test two-sided since concerned direction residuals absolute values.","code":""},{"path":"diag2.html","id":"outliers-in-the-x-direction","chapter":"12 Diagnostic Measures II","heading":"12.2 Outliers in the X direction","text":"Values outlying respect \\(X\\)-values called leverage points, necessarily obvious multiple linear regression. reason name leverage point moving single point far enough away, can make fitted line whatever want, regardless sample size. Thus, points lot leverage, need handle points care.\ndiagonal elements -called hat matrix give indications leverage points, values outlying leverage points worried .","code":""},{"path":"diag2.html","id":"leverage","chapter":"12 Diagnostic Measures II","heading":"12.2.1 Leverage","text":"\nRecall hat matrix. Note one explanatory variable,\n\\[\\begin{eqnarray*}\nh_i = \\frac{(X_i - \\overline{X})^2}{\\sum_{j=1}^n (X_j - \\overline{X})^2} + \\frac{1}{n}\\\\\n\\end{eqnarray*}\\]\nhigher dimensions, \\(H=X(X^tX)^{-1}X^t\\), \\(X_{n \\times p}\\) matrix explanatory variables. Also, can show \\(h_{ii}\\) following properties: \\[0 \\leq h_{ii} \\leq 1 \\ \\ \\ \\sum_{=1}^n h_{ii} =p \\ \\ \\ \\overline{h} = \\frac{\\sum_{=1}^n h_{ii}}{n} = \\frac{p}{n}\\]\ndiagonal elements hat matrix, \\(h_{ii}\\), called leverage points.Leverage gives (multivariate) sense far \\(^{th}\\) observation bulk data. response variable calculation leverage. Additionally, \\(h_{ii}\\) provide information much \\(^{th}\\) observation impacts fit \\(\\hat{Y}_i\\).Recall \\(\\hat{\\underline{Y}} = H \\underline{Y}\\). , predicted values linear combination observed response. \\(h_{ii}\\) weight observation \\(Y_i\\) determining \\(\\hat{Y}_i\\). larger \\(h_{ii}\\) , important \\(Y_i\\) determining \\(\\hat{Y}_i\\). hat matrix function \\(X\\) , \\(h_{ii}\\) measures role X values determining important \\(Y_i\\) affecting fitted value \\(\\hat{Y}_i\\).larger \\(h_{ii}\\) , smaller variance residual \\(e_i\\). , larger \\(h_{ii}\\), closer \\(\\hat{Y}_i\\) \\(Y_i\\). \\(h_{ii} = 1\\), \\(sd^2(e_i) = 0\\) means fitted value forced equal observed value.","code":""},{"path":"diag2.html","id":"outliers-that-are-influential","chapter":"12 Diagnostic Measures II","heading":"12.3 Outliers that are Influential","text":"consider points influential impact inference. , change either predicted values model. Note influential observation can arise large residual, large leverage, .","code":""},{"path":"diag2.html","id":"dffits-difference-in-fits","chapter":"12 Diagnostic Measures II","heading":"DFFITS (difference in fits)","text":"DFFITS measure influence case \\(\\) fitted value \\(\\hat{Y}_i\\). [Note: \\(\\hat{Y}_i\\) considered varying \\(\\hat{Y}_{()}\\) fixed.] DFFITS measuring \\(\\hat{Y}_i\\) varies (units standard deviation) fit calculated without \\(\\) model.\n\\[\\begin{eqnarray*}\n(DFFITS)_i &=& \\frac{\\hat{Y}_i - \\hat{Y}_{()}}{\\sqrt{MSE_{()} h_{ii}}}\\\\\n&=& t_i \\bigg(\\frac{h_{ii}}{1-h_{ii}}\\bigg)^{1/2}\n\\end{eqnarray*}\\]\nunderstand derivation, recall following variability \\(\\underline{\\hat{Y}}\\).\n\\[\\begin{eqnarray*}\n\\underline{\\hat{Y}} &=& H \\underline{Y}\\\\\nsd^2(\\underline{\\hat{Y}}) &=& H sd^2(Y) H = \\sigma^2 H\\\\\nsd^2(\\hat{Y}_i) &=& \\sigma^2 \\cdot h_{ii}\\\\\nse^2(\\hat{Y}_i) &=& MSE \\cdot h_{ii}\n\\end{eqnarray*}\\]","code":""},{"path":"diag2.html","id":"cooks-distance","chapter":"12 Diagnostic Measures II","heading":"Cook’s Distance","text":"Another measure combines idea outliers types see influence particular point regression. Cook’s Distance measures change regression removing individual point. things change quite bit omission single point, point lot influence model. Define \\(\\hat{Y}_{j()}\\) fitted value \\(j^{th}\\) observation \\(^{th}\\) observation deleted data set. Cook’s Distance measures much \\(\\) changes predictions.\n\\[\\begin{eqnarray*}\nD_i &=&\\frac{\\sum_{j=1}^{n}(\\hat{Y}_j-\\hat{Y}_{j()})^2}{p MSE}\\\\\n&=& \\frac{(\\hat{\\underline{Y}} - \\hat{\\underline{Y}}_{()})^t (\\hat{\\underline{Y}} - \\hat{\\underline{Y}}_{()})}{p MSE}\\\\\n&=& \\frac{e_i^2}{p MSE} \\bigg[ \\frac{h_{ii}}{(1-h_{ii})^2} \\bigg]\n\\end{eqnarray*}\\]\nCook’s Distance shows effect \\(^{th}\\) case fitted values. Note \\(^{th}\\) case can influenced bybig \\(e_i\\) moderate \\(h_{ii}\\)moderate \\(e_i\\) big \\(h_{ii}\\)big \\(e_i\\) big \\(h_{ii}\\)exact way think numbers large complicated, see point much larger others, examine point try understand better. Noting measures squared deviations, useful comparison look 10% 20% cutoff \\(F(p, n-p)\\) distribution:\n\\[\\begin{eqnarray*}\n\\mbox{} &&D_i < F_{0.1, p, n-p} \\mbox{ } F_{0.2, p, n-p} \\rightarrow \\mbox{ influence}\\\\\n\\mbox{} &&D_i > F_{0.5, p, n-p} \\rightarrow \\mbox{ big influence}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"diag2.html","id":"dfbetas-difference-in-betas","chapter":"12 Diagnostic Measures II","heading":"DFBETAS (difference in betas)","text":"DFBETAS measure influence case \\(\\) \\(k^{th}\\) \\(b_k\\) coefficient.\n\\[\\begin{eqnarray*}\n(DFBETAS)_{k()} &=& \\frac{b_k - b_{k()}}{\\sqrt{MSE_{()} c_{kk}}}\\\\\n\\mbox{} c_{kk} &=& (X^t X )^{-1}_{kk}\\\\\n\\mbox{note } sd^2(b_k) &=& \\sigma^2 c_{kk}\n\\end{eqnarray*}\\]\nlarge absolute value \\((DFBETAS)_{k()}\\) indicative large impact \\(^{th}\\) case \\(k^{th}\\) regression coefficient.","code":""},{"path":"diag2.html","id":"variance-inflation-factor-vif","chapter":"12 Diagnostic Measures II","heading":"12.4 Variance Inflation Factor (VIF)","text":"Recall always aware possible multicollinearity can cause following problems:Adding deleting predictor variable changes regression coefficients (terms magnitude well significance).extra sum squares associated explanatory variable varies, depending upon predictor variables already included model.estimated standard deviation regression coefficients become large predictor variables regression model highly correlated .estimated regression coefficients individually may statistically significant [t-test multiple regression model] even though definite relation exists response variable set predictor variables.","code":""},{"path":"diag2.html","id":"informal-diagnostics","chapter":"12 Diagnostic Measures II","heading":"Informal Diagnostics","text":"important predictors: wide confidence intervals, non-significant results individual coefficients [t-tests multiple regression], coefficient opposite sign expectedLarge changes estimated regression coefficients predictor variable added deletedLarge coefficients simple correlation pairs predictor variables","code":""},{"path":"diag2.html","id":"formal-diagnostics","chapter":"12 Diagnostic Measures II","heading":"Formal Diagnostics","text":"Variance Inflation Factor gives quantitative sense multicollinearity within explanatory variables. \\[VIF_k = (1-R_k^2)^{-1}\\] \\(R_k^2\\) coefficient multiple determination \\(X_k\\) regressed \\(p-2\\) explanatory variables.\\(VIF_k = 1\\) \\(R_k^2 = 0\\) (.e., \\(X_k\\) linearly related \\(p-2\\) variables)\\(R_k^2 \\ne 0 \\rightarrow VIF_k > 1\\)\\(R_k^2 = 1 \\rightarrow VIF_k\\) unboundedIt also interesting note standard error estimated coefficients can calculated using VIF.\n\\[\\begin{eqnarray*}\nse^2(b_k) = \\frac{MSE}{(n-1) se^2(X_k)} (VIF_k)\n\\end{eqnarray*}\\]\n, standard error function MSE (total variability around model), \\(se^2(X_k)\\) (variability \\(k^{th}\\) variable, VIF \\(k^{th}\\) variable. Notice \\(k^{th}\\) variable model another variable added correlated \\(k^{th}\\) variable, SE \\(k^{th}\\) coefficient go .","code":""},{"path":"diag2.html","id":"a-strategy-for-dealing-with-problematic-data-points","chapter":"12 Diagnostic Measures II","heading":"12.5 A Strategy for Dealing with Problematic Data Points12","text":"First, check obvious data errors:\nerror just data entry data collection error, correct .\ndata point representative intended study population, delete .\ndata point procedural error invalidates measurement, delete .\nFirst, check obvious data errors:error just data entry data collection error, correct .data point representative intended study population, delete .data point procedural error invalidates measurement, delete .Consider possibility might just misformulated regression model:\nleave important predictors?\nconsider adding interaction terms?\nnon-linearity needs modeled?\nConsider possibility might just misformulated regression model:leave important predictors?consider adding interaction terms?non-linearity needs modeled?non linearity issue, one possibility just reduce scope model. reduce scope model, sure report , readers misuse model.non linearity issue, one possibility just reduce scope model. reduce scope model, sure report , readers misuse model.Decide whether deleting data points warranted:Decide whether deleting data points warranted:delete data points just fit preconceived regression model.must good, objective reason deleting data points.delete data ’ve collected , justify describe reports.sure data point, analyze data twice - without data point - report results analyses.First, foremost, finally - ’s okay use common sense knowledge situation.Note: Added value plots often good idea, read section 10.1. won’t homework exam questions section 10.1Notes diagnostic table handout:first four statistics measures influential value . Leverage measures distance explanatory variables average. Cook’s distances, derivatives, measure much predicted values change point removed model.residual statistics measures well regression line fits value. residual distance point line. standardize residual different ways. studentized residuals contain accurate measure standard error.VIF measures degree collinearity explanatory variables. Collinear variables indicates cautious interpreting coefficients. \\(\\mbox{mean}(VIF) > > 1\\) meant indicate average VIF considerably larger 1.value containing “\\(()\\)” indicates \\(^{th}\\) point removed calculating value. example, \\(MSE_{()}\\) \\(MSE\\) full model containing data except \\(^{th}\\) point.functions R general heading influence.measures. vif function car package.","code":""},{"path":"diag2.html","id":"reflection-questions-9","chapter":"12 Diagnostic Measures II","heading":"12.6  Reflection Questions","text":"influential data point?outlying Y values detected way studentized residuals studentized deleted residuals?leverage, can outlying X values detected using leverage?can potentially influential data points detected way DFFITS Cook’s distance measure?","code":""},{"path":"diag2.html","id":"ethics-considerations-8","chapter":"12 Diagnostic Measures II","heading":"12.7  Ethics Considerations","text":"modeling tets (e.g., nested F-tests) now potential tests discover outliers, seems p-hacking might getting control!p-values first introduced R.. Fisher 1929:.. observation judged significant, rarely produced, absence real cause kind seeking. common practice judge result significant, magnitude produced chance frequently twenty trials. arbitrary, convenient, level significance practical investigator, mean allows deceived every twenty experiments. test significance tells ignore, namely experiments significant results obtained. claim phenomenon experimentally demonstrable knows design experiment rarely fail give significant result. Consequently, isolated significant results know reproduce left suspense pending investigation.2014, George Cobb reflected following debate (Wasserstein Lazar 2016):Q: many colleges grad schools teach p = 0.05?\n: ’s still scientific community journal editors use.Q: many people still use p = 0.05?\n: ’s taught college grad school.led series reflections statements hypothesis tests p-values. Indeed, journal Basic Applied Social Psychology banned null hypothesis significance testing procedures (NHSTP) articles journal (Trafimow Marks 2015).banning NHSTP (null hypothesis significance testing procedures) BASP, implications authors?Question 3. inferential statistical procedures required?Answer Question 3. , state art remains uncertain. However, BASP require strong descriptive statistics, including effect sizes. also encourage presentation frequency distributional data feasible. Finally, encourage use larger sample sizes typical much psychology research, sample size increases, descriptive statistics become increasingly stable sampling error less problem. However, stop short requiring particular sample sizes, possible imagine circumstances typical sample sizes might justifiable.American Statistics Association, contrast, wrote statement positive negative aspects p-values (Wasserstein Lazar 2016). Test using following TRUE/FALSE questions. answers given footnote questions.TRUE FALSE: P-values can indicate incompatible data specified statistical model.13TRUE FALSE: P-values measure probability studied hypothesis true, probability data produced random chance alone.14TRUE FALSE: Scientific conclusions business policy decisions based whether p- value passes specific threshold.15TRUE FALSE: Proper inference requires full reporting transparency.16TRUE FALSE: p-value, statistical significance, measure size effect importance result.17TRUE FALSE: , p-value provide good measure evidence regarding model hypothesis.18","code":""},{"path":"diag2.html","id":"r-leverage-points-residuals","chapter":"12 Diagnostic Measures II","heading":"12.8 R: Leverage Points + Residuals","text":"Recall broom package three important functions:tidy() reports information based explanatory variableglance() reports information based overall modelaugment() reports information based observationLeverage (.hat), DFFITS (dffits()), DFBETAS(dfbetas()), Cook’s distance (.cooksd), residuals (.resid), semi-studentized residuals (.std.resid), deleted studentized residuals (rstudent()) measured per observation.VIF (vif) measured per variable.Consider births data one last time. Every year, US releases public large data set containing information births recorded country. data set interest medical researchers studying relation habits practices expectant mothers birth children. random sample 1,000 cases data set released 2014. Data description .","code":"\noz_lm <- births14 %>%\n  lm(weight ~ weeks + sex + term + gained + premie + \n       mage + whitemom + habit, data = .)"},{"path":"diag2.html","id":"per-observation-measures","chapter":"12 Diagnostic Measures II","heading":"Per observation measures:","text":"leverage residual metrics available directly augment(). values come directly lm() (need connected, via bind_cols(), output, example create residual plot).","code":"\noz_lm %>% augment() %>%\n  select(.resid, .hat, .cooksd, .std.resid) %>%\n  bind_cols(rstudent = oz_lm %>% rstudent())  %>%\n  bind_cols(dffits = oz_lm %>% dffits()) %>%\n  bind_cols(oz_lm %>% dfbetas())## # A tibble: 941 × 16\n##     .resid    .hat   .cooksd .std.resid rstudent  dffits `(Intercept)`    weeks\n##      <dbl>   <dbl>     <dbl>      <dbl>    <dbl>   <dbl>         <dbl>    <dbl>\n##  1 -0.321  0.00775 0.0000792    -0.318   -0.318  -0.0281     -0.00600   0.00690\n##  2  1.38   0.00916 0.00174       1.37     1.37    0.132       0.0245   -0.0290 \n##  3  0.882  0.0115  0.000891      0.876    0.876   0.0944      0.0195   -0.0182 \n##  4 -1.00   0.0110  0.00110      -0.995   -0.995  -0.105      -0.000473 -0.0127 \n##  5  0.502  0.0159  0.000403      0.500    0.500   0.0635     -0.0312    0.0300 \n##  6 -0.774  0.00490 0.000290     -0.767   -0.767  -0.0538     -0.00937   0.00917\n##  7  0.0901 0.0162  0.0000133     0.0898   0.0897  0.0115     -0.00562   0.00534\n##  8 -1.56   0.00955 0.00232      -1.55    -1.55   -0.152       0.0288   -0.0252 \n##  9  1.58   0.00434 0.00107       1.56     1.56    0.103       0.0169   -0.0205 \n## 10  1.75   0.00776 0.00234       1.73     1.73    0.153       0.0265   -0.0177 \n## # … with 931 more rows, and 8 more variables: sexmale <dbl>, termfull <dbl>,\n## #   termlate <dbl>, gained <dbl>, premiepremie <dbl>, mage <dbl>,\n## #   whitemomwhite <dbl>, habitsmoker <dbl>"},{"path":"diag2.html","id":"per-variable-measures","chapter":"12 Diagnostic Measures II","heading":"Per variable measures:","text":"Variance Inflation Factor measured per variable. Note tidy output per coefficient vif() per variable columns won’t line tidy way ’d prefer.","code":"\noz_lm %>% car::vif()##          GVIF Df GVIF^(1/(2*Df))\n## weeks    4.75  1            2.18\n## sex      1.01  1            1.00\n## term     2.95  2            1.31\n## gained   1.01  1            1.01\n## premie   2.59  1            1.61\n## mage     1.01  1            1.01\n## whitemom 1.01  1            1.01\n## habit    1.02  1            1.01"},{"path":"standardized-multiple-regression.html","id":"standardized-multiple-regression","chapter":"13 Standardized Multiple Regression","heading":"13 Standardized Multiple Regression","text":"(skipping standardized multiple regression)Historically, standardized multiple regression model generally used two reasons:control round-errors coefficient estimate calculations (usually problematic multicollinearity).directly compare estimated regression coefficients common units.recently, standardizing coefficients become important methods like Ridge Regression LASSO (see Section 14) constrained optimization techniques differentially affect coefficient estimates.Consider following example:\n\\[ \\hat{Y} = 200 + 20,000 X_1 + 0.2 X_2\\]\nfirst glance, seems like \\(X_1\\) much important factor. ’s units! Suppose units :\n\\[\\begin{eqnarray*}\nY & &\\mbox{dollars}\\\\\nX_1 && \\mbox{thousand dollars}\\\\\nX_2 & &\\mbox{cents}\n\\end{eqnarray*}\\]\neffect mean response $1,000 increase \\(X_1\\) (, one-unit increase) \\(X_2\\) held constant increase $20,000. exactly effect $1,000 increase \\(X_2\\) (.e., 100,000-unit-increase) \\(X_1\\) held constant.Consider following transformations:\n\\[\\begin{eqnarray*}\nY_i^* &=&  \\bigg( \\frac{Y_i - \\overline{Y}}{s_Y} \\bigg)\\\\\nX_{ik}^* &=&  \\bigg( \\frac{X_{ik} - \\overline{X}_k}{s_k} \\bigg) \\ \\ \\ \\ \\ k=1, \\ldots, p-1\\\\\n\\mbox{}&& \\\\\ns_Y &=& \\sqrt{\\frac{\\sum_i (Y_i - \\overline{Y})^2}{n-1}}\\\\\ns_k &=& \\sqrt{\\frac{\\sum_i (X_{ik} - \\overline{X}_k)^2}{n-1}}  \\ \\ \\ \\ \\ k=1, \\ldots, p-1\\\\\n\\end{eqnarray*}\\]Using standardized variables, get standardized regression model:\n\\[\\begin{eqnarray*}\nY_i^* &=& \\beta_i^*X_{i1} ^*+ \\cdots + \\beta_{p-1}^*X_{,p-1}^* + \\epsilon_i^*\\\\\n\\end{eqnarray*}\\]\ndirect algebraic connection standardized parameters \\(\\beta_1^*, \\ldots, \\beta_{p-1}^*\\) original parameters form ordinary multiple regression model \\(\\beta_1, \\ldots, \\beta_{p-1}\\)\\[\\begin{eqnarray*}\n\\beta_k &=& \\bigg(\\frac{s_Y}{s_k} \\bigg) \\beta_k^* \\ \\ \\ \\ \\ k=1, \\ldots, p-1\\\\\n\\beta_0 &=&  \\overline{Y} - \\beta_1\\overline{X}_1 - \\cdots - \\beta_{p-1}\\overline{X}_{p-1}\n\\end{eqnarray*}\\]","code":""},{"path":"standardized-multiple-regression.html","id":"proof-of-relationship-between-standardized-and-unstandardized-coefficients","chapter":"13 Standardized Multiple Regression","heading":"Proof of relationship between standardized and unstandardized coefficients","text":"\\[\\begin{eqnarray*}\nY_i &=& b_0 + b_1 X_{i1} + b_2 X_{i2} + e_i \\\\\nY_i - \\overline{Y} &=& b_0 + b_1 X_{i1} + b_2 X_{i2} + e_i- \\overline{Y} \\\\\n &=& \\overline{Y} - b_1 \\overline{X}_1 - b_2 \\overline{X}_2 + b_1 X_{i1} + b_2 X_{i2} + e_i- \\overline{Y} \\\\\n &=& b_1 (X_{i1} - \\overline{X}) + b_2 (X_{i2} - \\overline{X}_2) + e_i\\\\\n &=& b_1 s_1 (X_{i1} - \\overline{X}) / s_1+ b_2 s_2 (X_{i2} - \\overline{X}_2)/s_2 + e_i\\\\\n &=& b_1 s_1 X^*_{i1}+ b_2 s_2 X^*_{i2}  + e_i\\\\\n (Y_i - \\overline{Y} )/s_y  &=& b_1 (s_1/s_y) X^*_{i1}+ b_2 (s_2 /s_y) X^*_{i2}  + e_i\\\\\nY_i^* &=& b_1^* X_{i1}^* + b_2^* X_{i2}^* + e_i\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"standardized-multiple-regression.html","id":"why-is-there-no-intercept-in-the-standardized-model","chapter":"13 Standardized Multiple Regression","heading":"Why is there no intercept in the standardized model?","text":"Note intercept standardized regression model. variables centered zero, putting average \\(X\\) values (zero) maximum likelihood estimates predict average \\(Y\\) value (zero). Recall solve coefficients, take derivative sum squares set equal zero. process forces OLS model go though point determined average variables (explanatory response).\\[\\begin{eqnarray*}\n\\frac{\\partial \\sum_i (Y_i^* - b_0^* - b_1^*X_i^*)^2}{\\partial b_0^*} &=& 0\\\\\n\\sum_i (Y_i^* - b_0^* - b_1^*X_i^*) &=& 0\\\\\n\\overline{Y}^* &=& b_0^* + b_1^* \\overline{X}^*\\\\\n\\end{eqnarray*}\\]standardized coefficients centered zero, average zero (definition). means model goes average points, goes origin, \\(b_0^* = \\beta_0^* = 0\\).","code":""},{"path":"shrink.html","id":"shrink","chapter":"14 Shrinkage Methods","heading":"14 Shrinkage Methods","text":"Recall two methods ’ve used far find sets variable (coefficients):Choose models using domain knowledge applying computational methods (.e., cross validation) decide model superior. Apply least squares (.e., calculus) find coefficients.Adding variables systematically using statistical criteria (F-tests, adjusted \\(R^2\\), \\(C_p\\), BIC, AIC). Apply least squares (.e., calculus) find coefficients.move new model building algorithm uses \\(p-1\\) explanatory variables constrains coefficient estimates, equivalently, shrinks coefficient estimates toward zero. turns shrinking coefficients toward zero can substantially reduce variance estimates (albeit adding bit bias)., use mathematical optimization shrink (set) coefficients zero (simultaneously solving non-zero coefficients).main reason use shrinkage techniques model lot predictor variables (\\(p\\) big) model lot predictor variables comparison number observations (\\(n \\approx p\\) \\(n < p\\)) — get huge (infinite!) variability associated coefficients!","code":""},{"path":"shrink.html","id":"model-complexity-flexibility","chapter":"14 Shrinkage Methods","heading":"14.1 Model Complexity / Flexibility","text":"words complexity flexibility can thought synonyms. refer () simple model . However, use slightly different. polynomial 20 degrees flexible, might described complex (isn’t hard write ). piecewise cubic model dozens pieces might overly flexible complicated write . maybe examples just given thought complex flexible.models ’ve discussing, flexibility refers number curves “linear” model. curve-y (think: polynomial degree many dozens), model (-)fit observations.many models, can see flexibility scale models class models covered class. non-linear models, flexibility refers whether model fitting high level functions data. Sometimes means -fitting. sometimes lot flexibility can capture true complicated relationships among variables. Note flexibility often expense interpretability.Note Lasso Ridge Regression, models less flexible constrain coefficient estimates. constraints increase bias decrease variability.increases bias: constrained coefficients won’t fit observations well OLS. , trend different data presenting.decrease variability: constrained coefficients attenuate zero means different dataset also coefficients close zero. forcing coefficients small closer one another across different datasets, predictions similar (less variable).\nFigure 1.4: Image credit: Figure 2.7 ISLR\n","code":""},{"path":"shrink.html","id":"on-inverting-matrices","chapter":"14 Shrinkage Methods","heading":"14.2 On Inverting Matrices","text":"\\(X_{n \\times p}\\): data matrix \\(n \\times p\\). think data \\(n\\) points \\(p\\) dimensions. important realize points indexed \\(p\\) dimensions, points also take entire p-dimensional space. [aside: Whether set points takes entire space depend points indexed. example, think (2-dimensional) piece paper floating sky many many points . might index coordinates points paper using three dimensions, however, paper / points actually live 2-dimensional subspace.] reiterate: ideal world, \\(n\\) points live \\(p\\) space considered live smaller dimension \\(p\\). Sometimes, \\(n\\) points indexed live \\(p\\) space, actually take lower dimensional subspace (e.g., two variable columns perfectly correlated).However, times \\(n \\times p\\) matrix lives space smaller \\(p\\). example,two \\(p\\) columns exact linear combinations one another, points actually live \\(p-1\\) space.number points less \\(p\\) (\\(n < p\\)) points live \\(n\\) space. example, way two points take three dimensions!\\(X^t X\\): matrix \\(X^tX\\) linear transformation original \\(X\\) matrix. , \\(X\\) lives \\(p\\) space, can’t linearly transformed higher dimension. transform \\(X\\) higher dimension using functions kind kernel mapping, can’t via linear transformations. say, \\(X\\) rank lower \\(p\\), linear combination also, necessarily, transform data space lower \\(p\\).\\((X^tX)^{-1}\\): inverse matrix also represents mapping. Recall \\(AX = Y\\) \\(X= ^{-1}Y\\). mapping smaller space (smaller \\(p\\)) can’t invert back larger space (.e., back \\(p\\)). \\((X^tX)^{-1}\\) \\(p \\times p\\) matrix, trying invert back \\(p\\) dimensional space. Recall Big Theorem Linear Algebra says \\(p \\times p\\) matrix rank lower \\(p\\), isn’t invertible (also determinant zero, eigenvalues zero, etc.)… point \\(X\\) doesn’t full rank (, dimension less \\(p\\)), problems computing \\((X^tX)^{-1}\\). matrix \\((X^tX)^{-1}\\) vitally important computing least squares coefficients standard errors.","code":""},{"path":"shrink.html","id":"ridge-regression","chapter":"14 Shrinkage Methods","heading":"14.3 Ridge Regression","text":"excellent discussion Ridge Regression given Wieringen (2021).Recall OLS (ordinary least squares) technique minimizes distance observed predicted values response. , found \\(b_0, b_1, \\ldots b_{p-1}\\) minimize:\n\\[SSE = \\sum_{=1}^n \\bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\bigg)^2.\\]Using OLS algorithm modeling, values \\(b_i\\) give exact model model built using standardized variables produce \\(b_i^*\\). However, see, important standardize variables running ridge regression.ease computation, assume variables standardized described (previous section). [Note: ISL describes standardizing dividing standard deviation centering. two different methods produce different models (respect significance, etc.), produce different intercept coefficients. Indeed, scale important aspect consider working shrinkage models.]ridge regression coefficients calculated similar way OLS, optimization equation seeks minimize:\\[\\sum_{=1}^n \\bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\bigg)^2 + \\lambda \\sum_{j=1}^{p-1} b_j^2 = SSE + \\lambda \\sum_{j=1}^{p-1} b_j^2\\]\n\\(\\lambda \\geq 0\\) tuning parameter, determined separately. ridge regression optimization provides trade-two different criteria: variance bias. OLS, ridge regression seeks find coefficients fit data well (minimize SSE!). Additionally, ridge regression shrinks coefficients zero adding penalty smaller values \\(b_j\\) preferred - shrinkage makes estimates less variable. can proved always \\(\\lambda \\ne 0\\) gives smaller E[MSE] OLS. Note: shrinkage apply intercept!Note: Gauss-Markov theorem says OLS solutions best (.e., smallest variance) linear unbiased estimates (BLUE). add bit bias, can better. existence theorem (Theobald 1974) says:\n\\[ \\exists \\ \\  \\lambda \\mbox{ } E[MSE_{RR}] < E[MSE_{OLS}].\\]\nExcellent description theory given Wieringen (2021).","code":""},{"path":"shrink.html","id":"the-ridge-regression-solution","chapter":"14 Shrinkage Methods","heading":"14.3.1 The Ridge Regression Solution","text":"\\[\\begin{eqnarray*}\n\\mbox{OLS: } \\underline{b} &=& (X^t X)^{-1} X^t \\underline{Y}\\\\\n\\mbox{ridge regression: } \\underline{b} &=& (X^t X + \\lambda \\mathtt{})^{-1} X^t \\underline{Y}\\\\\n\\end{eqnarray*}\\]Notes:tuning parameter \\(\\lambda\\) balances effect two different criteria optimization equation. \\(\\lambda=0\\), ridge regression reduced OLS. \\(\\lambda \\rightarrow \\infty\\), coefficient estimates shrink zero.assumed variables centered mean zero ridge regression performed. Therefore, estimated intercept \\[b_0 = \\overline{Y} = \\frac{1}{n} \\sum_i Y_i \\ \\ \\ \\ \\ \\ (= 0 \\mbox{ $Y$ also centered}) \\]Note ridge regression optimization constrained optimization equation, solved Lagrange multipliers. , minimize \\(SSE = \\sum_{=1}^n \\bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\bigg)^2\\) subject \\(\\sum_{j=1}^{p-1} b_j^2 \\leq s\\) value \\(s\\). Note \\(s\\) inversely related \\(\\lambda\\).","code":""},{"path":"shrink.html","id":"ridge-regression-visually","chapter":"14 Shrinkage Methods","heading":"14.3.2 Ridge Regression visually","text":"Ridge regression typically applied situations many many variables. particular, ridge regression help us avoid situations multicollinearity. doesn’t make sense apply situation one two variables. However, demonstrate process visually \\(p=3\\) dimensions difficult visualize higher dimensions.ridge regression coefficient estimates solve following optimization problem:\\[ \\min_\\beta \\Bigg\\{ \\sum_{=1}^n \\Bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\Bigg)^2 \\Bigg\\} \\mbox{  subject  } \\sum_{j=1}^{p-1} b_j^2 \\leq s\\]\nFigure 14.1: red contours represent pairs \\(\\beta\\) coefficients produce constant values SSE data. blue circle represents possible values \\(\\beta\\) given constraint (squared magnitude less cutoff). Image credit: ISLR\n","code":""},{"path":"shrink.html","id":"why-ridge-regression","chapter":"14 Shrinkage Methods","heading":"14.3.3 Why Ridge Regression?","text":"Recall main benefit ridge regression \\(p\\) large (particularly relation \\(n\\)). Remember (multicollinearity) leads instability \\((X^tX)^{-1}\\). leads huge variability coefficient estimates. small change model observations can create wildly different estimates. expected value MSE can quite inflated due variability model. Ridge regression adds small amount bias model, lowers variance substantially creates lower (average) MSE values.Additionally, ridge regression computational advantages subset selection models (subsets requires searching \\(2^{p-1}\\) models).","code":""},{"path":"shrink.html","id":"why-is-ridge-regression-better-than-least-squares","chapter":"14 Shrinkage Methods","heading":"Why is ridge regression better than least squares?","text":"advantage apparent bias-variance trade-. \\(\\lambda\\) increases, flexibility ridge regression fit decreases. leads decrease variance, smaller increase bias. Regular OLS regression fixed high variance, bias. However, lowest test MSE tends occur balance variance bias. Thus, properly tuning \\(\\lambda\\) acquiring less variance cost small amount bias, can find lower potential MSE.Ridge regression works best situations least squares estimates high variance. Ridge regression also much computationally efficient subset method, since possible simultaneously solve values \\(\\lambda\\).","code":""},{"path":"shrink.html","id":"inference-on-ridge-regression-coefficients","chapter":"14 Shrinkage Methods","heading":"14.3.4 Inference on Ridge Regression Coefficients","text":"Note just like OLS coefficients, RR coefficients linear combination \\(Y\\) values based \\(X\\) matrix (now) \\(\\lambda\\). hard, therefore, find variance coefficient vector particular value \\(\\lambda\\). Additionally, theory gives normality (resulting t-statistics) drives normality ridge regression coefficients.\\[var(b^{RR}) = \\sigma^2 W X^t X W \\mbox{ } W = (X^t X + \\lambda \\mathtt{})^{-1}\\]However, distributional properties give theoretical results fixed value \\(\\lambda\\). now discuss estimating \\(\\lambda\\), soon estimate \\(\\lambda\\), becomes dependent data thus random variable. , SE \\(b^{RR}\\) function variability associated data estimating coefficients also variability data estimating \\(\\lambda\\).additional problem RR coefficients known biased, bias easy estimate. Without sense variable centered, SE isn’t particularly meaningful. reasons, functions like lm.ridge() R include tests / p-values approximate SE coefficients (estimate).","code":""},{"path":"shrink.html","id":"how-do-you-choose-lambda","chapter":"14 Shrinkage Methods","heading":"14.4 How do you choose \\(\\lambda\\)?","text":"Note \\(\\lambda\\) function data, therefore random variable estimate (just like estimating coefficients). However, can use diagnostic measures give sense \\(\\lambda\\) values give good variance-bias trade-.Split data test training, plot test MSE function \\(\\lambda\\).Actually cross validate data (remove test samples groups , say 1/10, see \\(\\lambda\\) gives best predictions “new” data) find \\(\\lambda\\) gives smallest MSE cross validated data.Cross Validating Find \\(\\lambda\\)Set \\(\\lambda\\) (e.g., try \\(\\lambda\\) \\(10^{-2}\\) \\(10^{5}\\): lambda.grid = 10^seq(5,-2, length =100))\nRemove 1/10 observations (partition data 10 groups).\nFind RR / Lasso model using remaining 90% observations given value \\(\\lambda\\).\nPredict response value removed points given 90% training values.\nRepeat () - (c) every point predicted test value.\nRemove 1/10 observations (partition data 10 groups).Find RR / Lasso model using remaining 90% observations given value \\(\\lambda\\).Predict response value removed points given 90% training values.Repeat () - (c) every point predicted test value.Using CV predictions, find \\(MSE_\\lambda\\) \\(\\lambda\\) hand.Repeat steps 1 2 across grid \\(\\lambda\\) values.Choose \\(\\lambda\\) value minimizes CV \\(MSE_\\lambda\\).","code":""},{"path":"shrink.html","id":"lasso","chapter":"14 Shrinkage Methods","heading":"14.5 Lasso","text":"Ridge regression least one disadvantage; includes \\(p\\) predictors final model. penalty term set many close zero, never exactly zero. isn’t generally problem prediction accuracy, can make model difficult interpret results. Lasso overcomes disadvantage capable forcing coefficients zero granted \\(\\lambda\\) big enough. Since \\(\\lambda = 0\\) results regular OLS regression, \\(\\lambda\\) approaches \\(\\infty\\) coefficients shrink towards zero.","code":""},{"path":"shrink.html","id":"lasso-coefficients","chapter":"14 Shrinkage Methods","heading":"14.5.1 Lasso Coefficients","text":"lasso (least absolute shrinkage selection operator) coefficients calculated similar constraint ridge regression, calculus much harder now. L-1 norm norm gives sparsity convex (optimization problem can solved). lasso optimization equation seeks minimize:\\[\\sum_{=1}^n \\bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\bigg)^2 + \\lambda \\sum_{j=1}^{p-1} |b_j| = SSE + \\lambda \\sum_{j=1}^{p-1} |b_j|\\]\n\\(\\lambda \\geq 0\\) tuning parameter, determined separately. ridge regression lasso optimization provides trade-bias variance. Lasso seeks find coefficients fit data well (minimize SSE!). Additionally, lasso shrinks coefficients zero adding penalty smaller values \\(b_j\\) preferred. Note: shrinkage apply intercept! minimization quantities ridge regression lasso extremely similar: ridge regression constrains sum squared coefficients; lasso constrains sum absolute coefficients. [ridge regression, use standardized variables modeling.]","code":""},{"path":"shrink.html","id":"lasso-visually","chapter":"14 Shrinkage Methods","heading":"14.5.2 Lasso visually","text":"ridge regression Lasso also typically applied situations many many variables (also avoid multicollinearity). doesn’t make sense apply situation one two variables. However, demonstrate process visually p=3 dimensions difficult visualize higher dimensions. Notice good chance red contours hit turquoise diamond corner (producing coefficients estimated zero). corner effect becomes extreme higher dimensions.lasso coefficient estimates solve following optimization problem:\\[ \\min_\\beta \\Bigg\\{ \\sum_{=1}^n \\Bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\Bigg)^2 \\Bigg\\} \\mbox{  subject  } \\sum_{j=1}^{p-1} |b_j| \\leq s\\]\nFigure 1.5: red contours represent pairs \\(\\beta\\) coefficients produce constant values SSE data. blue diamond represents possible values \\(\\beta\\) given constraint (absolute magnitude less cutoff). Image credit: ISLR\nkey lasso (contrast ridge regression) variable selection shrinking coefficients way zero. say lasso yields sparse models - , subset original variables retained final model.","code":""},{"path":"shrink.html","id":"how-do-you-choose-lambda-1","chapter":"14 Shrinkage Methods","heading":"14.5.3 How do you choose \\(\\lambda\\)?","text":"Note \\(\\lambda\\) function data, therefore random variable estimate (just like estimating coefficients). However, can use diagnostic measures give sense \\(\\lambda\\) values give good variance-bias trade-.Split data test training, plot test MSE function \\(\\lambda\\).Actually cross validate data (remove test samples groups , say 1/10, see \\(\\lambda\\) gives best predictions “new” data) find \\(\\lambda\\) gives smallest MSE cross validated data.","code":""},{"path":"shrink.html","id":"ridge-regression-vs.-lasso","chapter":"14 Shrinkage Methods","heading":"14.6 Ridge Regression vs. Lasso","text":"Quote Introduction Statistical Learning, V2, page 246.two examples illustrate neither ridge regression lasso universally dominate . general, one might expect lasso perform better setting relatively small number predictors substantial coefficients, remaining predictors coefficients small equal zero. Ridge regression perform better response function many predictors, coefficients roughly equal size. However, number predictors related response never known priori real data sets. technique cross-validation can used order determine approach better particular data set.ridge regression, least squares estimates excessively high variance, lasso solution can yield reduction variance expense small increase bias, consequently can generate accurate predictions. Unlike ridge regression, lasso performs variable selection, hence results models easier interpret.\nFigure 14.2: ISLR, pgs 245-246. data Figure 6.8 generated way 45 predictors related response. , none true coefficients beta1,… , beta45 equaled zero. lasso implicitly assumes number coefficients truly equal zero. Consequently, surprising ridge regression outperforms lasso terms prediction error setting. Figure 6.9 illustrates similar situation, except now response function 2 45 predictors. Now lasso tends outperform ridge regression terms bias, variance, MSE.\n\nFigure 14.3: ISLR, pgs 245-246. data Figure 6.8 generated way 45 predictors related response. , none true coefficients beta1,… , beta45 equaled zero. lasso implicitly assumes number coefficients truly equal zero. Consequently, surprising ridge regression outperforms lasso terms prediction error setting. Figure 6.9 illustrates similar situation, except now response function 2 45 predictors. Now lasso tends outperform ridge regression terms bias, variance, MSE.\nTrevor Hastie, Tibshirani, Tibshirani (2020) comment theoretical literature well perform extensive simulations compare best subsets, forward stepwise, Lasso. paper particularly interesting comparison best subsets Lasso.Generally speaking, lasso best subset selection differ terms “aggressiveness” selecting estimating coefficients linear model, lasso less aggressive best subset selection; meanwhile, forward stepwise lands somewhere middle, terms aggressiveness.paper summary given :neither best subset lasso uniformly dominate , best subset generally performing better high signal--noise (SNR) ratio regimes, lasso better low SNR regimes;large proportion settings considered, best subset forward stepwise perform similarly, certain cases high SNR regime, best subset performs better;forward stepwise best subsets tend yield sparser models (tuned validation set), especially high SNR regime;relaxed lasso (actually, simplified version original relaxed estimator defined Meinshausen (Comput. Statist. Data Anal. 52 (2007) 374–393)) overall winner, performing just well lasso low SNR scenarios, nearly well best subset high SNR scenarios.","code":""},{"path":"shrink.html","id":"elastic-net","chapter":"14 Shrinkage Methods","heading":"14.7 Elastic Net","text":"also possible combine ridge regression lasso called elastic net regularization. (R package glmnet allows combined elastic net model.) main idea optimization contains L-1 L-2 penalties. model may produce stable estimates coefficients requires additional tuning parameter estimate. , find coefficients minimize:\\[\\sum_{=1}^n \\bigg( Y_i - b_0 - \\sum_{j=1}^{p-1} b_j X_{ij} \\bigg)^2 + \\lambda \\bigg[(1-\\alpha)(\\frac{1}{2})\\sum_{j=1}^{p-1} b_j^2  + \\alpha \\sum_{j=1}^{p-1} |b_j| \\bigg].\\]","code":""},{"path":"shrink.html","id":"reflection-questions-10","chapter":"14 Shrinkage Methods","heading":"14.8  Reflection Questions","text":"ridge regression coefficient estimates differ OLS estimates? similar?Lasso coefficient estimates differ OLS estimates? similar?difference ridge regression Lasso?ways find good \\(\\lambda\\) ridge regression?1-norm regularization yield “sparse” solutions? (“sparse” mean?)Give two different situations ridge regression Lasso particularly appropriate.","code":""},{"path":"shrink.html","id":"ethics-considerations-9","chapter":"14 Shrinkage Methods","heading":"14.9  Ethics Considerations","text":"","code":""},{"path":"shrink.html","id":"r-ridge-regression","chapter":"14 Shrinkage Methods","heading":"14.10 R: Ridge Regression","text":"","code":""},{"path":"shrink.html","id":"the-data-1","chapter":"14 Shrinkage Methods","heading":"The Data","text":"following dataset TidyTuesday. , explore information Office. analysis taken Julia Silge’s blog. bit data wrangling ’m going hide. Look blog, see source code bookdown file.dataset working imdb_rating response variable. predictor (explanatory) variables : season, episode 28 columns representing number lines particular character.full linear model:","code":"\noffice## # A tibble: 136 × 32\n##    season episode episode_name       andy angela darryl dwight   jim kelly kevin\n##     <dbl>   <dbl> <chr>             <int>  <int>  <int>  <int> <int> <int> <int>\n##  1      1       1 pilot                 0      1      0     29    36     0     1\n##  2      1       2 diversity day         0      4      0     17    25     2     8\n##  3      1       3 health care           0      5      0     62    42     0     6\n##  4      1       5 basketball            0      3     15     25    21     0     1\n##  5      1       6 hot girl              0      3      0     28    55     0     5\n##  6      2       1 dundies               0      1      1     32    32     7     1\n##  7      2       2 sexual harassment     0      2      9     11    16     0     6\n##  8      2       3 office olympics       0      6      0     55    55     0     9\n##  9      2       4 fire                  0     17      0     65    51     4     5\n## 10      2       5 halloween             0     13      0     33    30     3     2\n## # … with 126 more rows, and 22 more variables: michael <int>, oscar <int>,\n## #   pam <int>, phyllis <int>, ryan <int>, toby <int>, erin <int>, jan <int>,\n## #   ken_kwapis <dbl>, greg_daniels <dbl>, b_j_novak <dbl>,\n## #   paul_lieberstein <dbl>, mindy_kaling <dbl>, paul_feig <dbl>,\n## #   gene_stupnitsky <dbl>, lee_eisenberg <dbl>, jennifer_celotta <dbl>,\n## #   randall_einhorn <dbl>, brent_forrester <dbl>, jeffrey_blitz <dbl>,\n## #   justin_spitzer <dbl>, imdb_rating <dbl>\nset.seed(47)\noffice_split <- initial_split(office, strata = season)\noffice_train <- training(office_split)\noffice_test <- testing(office_split)\noffice_lm <- office_train %>%\n  select(-episode_name) %>%\n  lm(imdb_rating ~ ., data = .)\n\noffice_lm %>% tidy()## # A tibble: 31 × 5\n##    term         estimate std.error statistic  p.value\n##    <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)  7.19       0.315     22.8    3.83e-34\n##  2 season      -0.00222    0.0366    -0.0607 9.52e- 1\n##  3 episode      0.0145     0.00730    1.98   5.15e- 2\n##  4 andy         0.00215    0.00424    0.507  6.14e- 1\n##  5 angela       0.00307    0.00865    0.354  7.24e- 1\n##  6 darryl       0.000932   0.00783    0.119  9.06e- 1\n##  7 dwight      -0.00172    0.00380   -0.452  6.53e- 1\n##  8 jim          0.00541    0.00375    1.44   1.54e- 1\n##  9 kelly       -0.0129     0.0101    -1.28   2.05e- 1\n## 10 kevin        0.00279    0.0114     0.244  8.08e- 1\n## # … with 21 more rows"},{"path":"shrink.html","id":"what-if-n-is-really-small","chapter":"14 Shrinkage Methods","heading":"What if \\(n\\) is really small?","text":"good! coefficients estimated! (, 5 points randomly selected, remember \\(p = 31\\).) model breaks \\((X^t X)^{-1}\\) invertible. Notice 5 coefficients estimated SEs estimated.","code":"\nset.seed(47)\noffice_train %>%\n  select(-episode_name) %>%\n  slice_sample(n = 5) %>%\n  lm(imdb_rating ~ ., data = .) %>% \n  tidy()## # A tibble: 31 × 5\n##    term        estimate std.error statistic p.value\n##    <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n##  1 (Intercept)   7.30         NaN       NaN     NaN\n##  2 season        0.228        NaN       NaN     NaN\n##  3 episode       0.0264       NaN       NaN     NaN\n##  4 andy         -0.0197       NaN       NaN     NaN\n##  5 angela        0.0499       NaN       NaN     NaN\n##  6 darryl       NA             NA        NA      NA\n##  7 dwight       NA             NA        NA      NA\n##  8 jim          NA             NA        NA      NA\n##  9 kelly        NA             NA        NA      NA\n## 10 kevin        NA             NA        NA      NA\n## # … with 21 more rows"},{"path":"shrink.html","id":"ridge-regression-model-building","chapter":"14 Shrinkage Methods","heading":"Ridge Regression model building","text":"vignette glmnet https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf useful. glmnet used engine tidymodels process.using generalized linear_reg() function, mixture = 0 specifies ridge regression mixture = 1 specifies Lasso regularization. mixture parameter can number zero one (represents model referred Elastic Net regularization). penalty tuning parameter set later.","code":""},{"path":"shrink.html","id":"recipe","chapter":"14 Shrinkage Methods","heading":"Recipe","text":"First, recipe needs specified. Note episode_name predictor variable, variables need normalized (particularly important scale variables).","code":"\noffice_rec <- recipe(imdb_rating ~ ., data = office_train) %>%\n  update_role(episode_name, new_role = \"ID\") %>%\n  step_zv(all_numeric(), -all_outcomes()) %>%\n  step_normalize(all_numeric(), -all_outcomes())"},{"path":"shrink.html","id":"specify-the-engine-fit","chapter":"14 Shrinkage Methods","heading":"Specify the engine + fit","text":"set penalty 47, ridge regression straightforward (linear algebra) solution. coefficients given ridge regression model (substantially smaller linear regression coefficients, expected).Side note relevant code main ideas ridge regression.constraint part can specified two different ways. (Can also specified \\(\\sum_{j=1}^{p-1}b_j^2 \\leq c\\) \\(c\\), won’t use construction .)Set penalty = P : \\[\\min(SSE + P)\\]Set penalty = P : \\[\\min(SSE + P)\\]Set \\(\\lambda\\) : \\[\\min(SSE + \\lambda\\sum_{j=1}^{p-1}b_{j}^2)\\]Set \\(\\lambda\\) : \\[\\min(SSE + \\lambda\\sum_{j=1}^{p-1}b_{j}^2)\\]P=0 \\(\\lambda\\) also zero (estimates OLS). Additionally, P \\(\\lambda\\) monotonically related. , large penalty corresponds large value \\(\\lambda.\\) said, functions one another.turns glmnet fits model values penalty , can see coefficients value penalty interest. Notice coefficients smaller larger values penalty.can visualize magnitude coefficients regularized toward zero penalty goes . (won’t get relationship \\(\\lambda\\) penalty, penalize magnitude coefficients, penalty can specified per variable.)Prediction done like linear models. predict() used parameters, use penalty = 47 specified :","code":"\nridge_spec <- linear_reg(mixture = 0, penalty = 47) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\")\n\nridge_wf <- workflow() %>%\n  add_recipe(office_rec)\n\nridge_fit <- ridge_wf %>%\n  add_model(ridge_spec) %>%\n  fit(data = office_train)\nridge_fit %>% tidy()## # A tibble: 31 × 3\n##    term         estimate penalty\n##    <chr>           <dbl>   <dbl>\n##  1 (Intercept)  8.36          47\n##  2 season      -0.00110       47\n##  3 episode      0.00107       47\n##  4 andy        -0.000546      47\n##  5 angela       0.00106       47\n##  6 darryl       0.000434      47\n##  7 dwight       0.000952      47\n##  8 jim          0.00150       47\n##  9 kelly        0.000112      47\n## 10 kevin        0.000600      47\n## # … with 21 more rows\nridge_fit %>% tidy(penalty = 0)## # A tibble: 31 × 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)   8.36         0\n##  2 season       -0.0114       0\n##  3 episode       0.107        0\n##  4 andy          0.0351       0\n##  5 angela        0.0235       0\n##  6 darryl        0.0103       0\n##  7 dwight       -0.0200       0\n##  8 jim           0.0895       0\n##  9 kelly        -0.0700       0\n## 10 kevin         0.0129       0\n## # … with 21 more rows\nridge_fit %>%\n  extract_fit_engine() %>%\n  plot(xvar = \"lambda\")\npredict(ridge_fit, new_data = office_train)## # A tibble: 100 × 1\n##    .pred\n##    <dbl>\n##  1  8.36\n##  2  8.36\n##  3  8.36\n##  4  8.36\n##  5  8.36\n##  6  8.36\n##  7  8.36\n##  8  8.36\n##  9  8.36\n## 10  8.36\n## # … with 90 more rows"},{"path":"shrink.html","id":"normalizing-tuning-whole-process","chapter":"14 Shrinkage Methods","heading":"14.10.0.1 Normalizing + Tuning + Whole Process","text":"tune lambda parameter, need start model/engine specification penalty fixed.Next, new workflow() includes new model/engine specification. Notice don’t need set measure = 0 penalty term designated tune().Interestingly, penalty grows, coefficients get close zero. prediction observations (prediction \\(\\overline{Y}\\)). predictions , computation \\(R^2\\) NA.best model can chosen using select_best().","code":"\nridge_spec_tune <- linear_reg(mixture = 0, penalty = tune()) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\")\nset.seed(1234)\noffice_fold <- vfold_cv(office_train, strata = season)\n  \nridge_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\nridge_wf <- workflow() %>%\n  add_recipe(office_rec)\n\nridge_fit <- ridge_wf %>%\n  add_model(ridge_spec_tune) %>%\n  fit(data = office_train)\n\n# this is the line that tunes the model using cross validation\nset.seed(2020)\nridge_cv <- tune_grid(\n  ridge_wf %>% add_model(ridge_spec_tune),\n  resamples = office_fold,\n  grid = ridge_grid\n)\ncollect_metrics(ridge_cv) %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(mean)## # A tibble: 50 × 7\n##    penalty .metric .estimator  mean     n std_err .config              \n##      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1   0.791 rmse    standard   0.465    10  0.0369 Preprocessor1_Model25\n##  2   0.494 rmse    standard   0.466    10  0.0385 Preprocessor1_Model24\n##  3   1.26  rmse    standard   0.467    10  0.0357 Preprocessor1_Model26\n##  4   0.309 rmse    standard   0.470    10  0.0406 Preprocessor1_Model23\n##  5   2.02  rmse    standard   0.472    10  0.0349 Preprocessor1_Model27\n##  6   0.193 rmse    standard   0.476    10  0.0427 Preprocessor1_Model22\n##  7   3.24  rmse    standard   0.477    10  0.0344 Preprocessor1_Model28\n##  8   5.18  rmse    standard   0.483    10  0.0341 Preprocessor1_Model29\n##  9   0.121 rmse    standard   0.484    10  0.0447 Preprocessor1_Model21\n## 10   8.29  rmse    standard   0.488    10  0.0339 Preprocessor1_Model30\n## # … with 40 more rows\nridge_cv %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty, y = mean, color = .metric)) + \n  geom_errorbar(aes(\n    ymin = mean - std_err,\n    ymax = mean + std_err),\n    alpha = 0.5) + \n  geom_line(size = 1.5) + \n  scale_x_log10() \nbest_rr <- select_best(ridge_cv, metric = \"rmse\")\nbest_rr## # A tibble: 1 × 2\n##   penalty .config              \n##     <dbl> <chr>                \n## 1   0.791 Preprocessor1_Model25"},{"path":"shrink.html","id":"the-final-ridge-model","chapter":"14 Shrinkage Methods","heading":"The Final Ridge Model","text":"Using \\(\\lambda\\) value minimum MSE cross validation, output coefficients / model associated best \\(\\lambda\\) ridge regression model. Ridge regression variable selection.","code":"\nfinalize_workflow(ridge_wf %>% add_model(ridge_spec_tune), best_rr) %>%\n  fit(data = office_test) %>% tidy()## # A tibble: 31 × 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)  8.42      0.791\n##  2 season      -0.0327    0.791\n##  3 episode      0.0383    0.791\n##  4 andy         0.00211   0.791\n##  5 angela       0.0233    0.791\n##  6 darryl       0.0264    0.791\n##  7 dwight       0.0523    0.791\n##  8 jim          0.0407    0.791\n##  9 kelly       -0.0347    0.791\n## 10 kevin        0.0371    0.791\n## # … with 21 more rows"},{"path":"shrink.html","id":"r-lasso-regularization","chapter":"14 Shrinkage Methods","heading":"14.11 R: Lasso Regularization","text":"section, ’ll re-run cross validation find value \\(\\lambda\\) minimizes cross validated MSE. tune lambda parameter, need start model/engine specification penalty fixed. Note Lasso, mixture = 1.Next, new workflow() includes new model/engine specification. Notice don’t need set measure = 0 penalty term designated tune().Interestingly, penalty grows, coefficients get close zero. prediction observations (prediction \\(\\overline{Y}\\)). predictions , computation \\(R^2\\) NA.best model can chosen using select_best().","code":"\nlasso_spec_tune <- linear_reg(mixture = 1, penalty = tune()) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"glmnet\")\nlasso_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 50)\n\nlasso_wf <- workflow() %>%\n  add_recipe(office_rec)\n\nlasso_fit <- lasso_wf %>%\n  add_model(lasso_spec_tune) %>%\n  fit(data = office_train)\n\n# this is the line that tunes the model using cross validation\nset.seed(2020)\nlasso_cv <- tune_grid(\n  lasso_wf %>% add_model(lasso_spec_tune),\n  resamples = office_fold,\n  grid = lasso_grid\n)\ncollect_metrics(lasso_cv) %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(desc(.metric))## # A tibble: 50 × 7\n##      penalty .metric .estimator  mean     n std_err .config              \n##        <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1 0.00001   rmse    standard   0.531    10  0.0497 Preprocessor1_Model01\n##  2 0.0000160 rmse    standard   0.531    10  0.0497 Preprocessor1_Model02\n##  3 0.0000256 rmse    standard   0.531    10  0.0497 Preprocessor1_Model03\n##  4 0.0000409 rmse    standard   0.531    10  0.0497 Preprocessor1_Model04\n##  5 0.0000655 rmse    standard   0.531    10  0.0497 Preprocessor1_Model05\n##  6 0.000105  rmse    standard   0.531    10  0.0497 Preprocessor1_Model06\n##  7 0.000168  rmse    standard   0.531    10  0.0497 Preprocessor1_Model07\n##  8 0.000268  rmse    standard   0.530    10  0.0497 Preprocessor1_Model08\n##  9 0.000429  rmse    standard   0.529    10  0.0497 Preprocessor1_Model09\n## 10 0.000687  rmse    standard   0.528    10  0.0496 Preprocessor1_Model10\n## # … with 40 more rows\nlasso_cv %>%\n  collect_metrics() %>%\n  ggplot(aes(x = penalty, y = mean, color = .metric)) + \n  geom_errorbar(aes(\n    ymin = mean - std_err,\n    ymax = mean + std_err),\n    alpha = 0.5) + \n  geom_line(size = 1.5) + \n  scale_x_log10() +\n  ylab(\"RMSE\")\nbest_lasso <- select_best(lasso_cv, metric = \"rmse\")\nbest_lasso## # A tibble: 1 × 2\n##   penalty .config              \n##     <dbl> <chr>                \n## 1  0.0295 Preprocessor1_Model18"},{"path":"shrink.html","id":"the-final-lasso-model","chapter":"14 Shrinkage Methods","heading":"The Final Lasso Model","text":"Using \\(\\lambda\\) value minimum MSE cross validation, output coefficients / model associated best \\(\\lambda\\) ridge regression model. Lasso regularization variable selection. Note large number coefficients set zero.","code":"\nfinalize_workflow(lasso_wf %>% add_model(lasso_spec_tune), best_lasso) %>%\n  fit(data = office_test) %>% tidy()## # A tibble: 31 × 3\n##    term        estimate penalty\n##    <chr>          <dbl>   <dbl>\n##  1 (Intercept)  8.42     0.0295\n##  2 season      -0.112    0.0295\n##  3 episode      0.115    0.0295\n##  4 andy         0        0.0295\n##  5 angela       0.00412  0.0295\n##  6 darryl       0.0195   0.0295\n##  7 dwight       0.0665   0.0295\n##  8 jim          0.0902   0.0295\n##  9 kelly       -0.0518   0.0295\n## 10 kevin        0.125    0.0295\n## # … with 21 more rows"},{"path":"smooth.html","id":"smooth","chapter":"15 Smoothing Methods","heading":"15 Smoothing Methods","text":"different names smoothing functions: smoothers, loess, lowess (locally weighted scatterplot smoothing). slightly different, investigate nuanced differences. Note, however, goal fit model \\(X\\) predicts \\(E[Y | X]\\) necessarily linear \\(X\\). far course, every model fit linear parameters (said differently, expected response linear function explanatory variables, sometimes transformed).\\[E[Y | X] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_{p-1} X_{p-1} = f(X)\\]ridge regression lasso able improve model making simpler, still linear. Linear models can go far, relationships linear!’ve talked transformations, relatedly, polynomial regression. , including powers original variable (\\(X\\), \\(X^2\\), \\(X^3\\), etc.), model can better fit data.","code":""},{"path":"smooth.html","id":"step-functions","chapter":"15 Smoothing Methods","heading":"15.1 Step functions","text":"Step functions cut range variable K distinct regions order produce qualitative variable. effect fitting piecewise constant function.similar idea polynomial regression fit step functions data. , fit flat line different sets values given explanatory variable. Note flat lines don’t requirement monotonically increasing. (Just like polynomial fit allows many different increasing decreasing trends.)Consider following cutpoints partition range \\(X\\): \\(c_1 < c_2 < \\cdots < c_K\\), create \\(K+1\\) new variables:\\[\\begin{eqnarray*}\nC_0(X) &=& (X < c_1)\\\\\nC_1(X) &=& (c_1 \\leq X < c_2)\\\\\nC_2(X) &=& (c_2 \\leq X < c_3)\\\\\n&\\vdots&\\\\\nC_{K-1}(X) &=& (c_{K-1} \\leq X < c_K)\\\\\nC_K(X) &=& (c_K \\leq X)\n\\end{eqnarray*}\\]\\((\\cdot)\\) indicator function returns 1 condition true 0 otherwise. Note \\(C_0(X) + C_1(X) + \\cdots + C_K(X) =1\\), \\(K\\) predictors used model. :\\[E[Y_i | X_i] = \\beta_0 + \\beta_1 C_1(X_i) + \\beta_2 C_2(X_i )+ \\cdots + \\beta_K C_K(X_i) \\]Note one \\(C_k\\) non-zero. \\(\\beta_0\\) can interpreted mean value \\(Y\\) given \\(X < c_1\\). contrast, mean value \\(Y\\) \\(c_j \\leq X < c_{j+1}\\) \\(\\beta_0 + \\beta_j\\). Therefore, \\(\\beta_j\\) interpreted difference expected response \\(X\\) \\(c_j \\leq X < c_{j+1}\\) compared \\(X < c_1\\).\\(K\\) step functions can used create step function model \\(X\\). However, noted using \\(C_1, \\ldots C_K\\) intuitive set \\(K\\) indicator functions. ? Consider following functions related models:\\[\\begin{eqnarray*} \nC_0(X) &=& 1 \\mbox{ } X < 4\\\\\nC_1(X) &=& 1 \\mbox{ } 4 <= X < 10\\\\\nC_2(X) &=& 1 \\mbox{ } 10 <= X < 50\\\\\nC_3(X) &=& 1 \\mbox{ } 50 <= X\n\\end{eqnarray*}\\]Valid models can built 3 basis functions:\\[\\begin{eqnarray*}\n\\mbox{model1:  } E[Y|X] &=& \\beta_0 + \\beta_1 C_1(X) + \\beta_2 C_2(X) + \\beta_3 C_3(X)\\\\\n\\mbox{model2: }  E[Y|X] &=& \\beta_0^* + \\beta_1^* C_0(X) + \\beta_2^* C_1(X) + \\beta_3^* C_2(X) \\\\\n\\end{eqnarray*}\\]\nn.b. \\(*\\) indicates coefficients differ across two models.Given two different models (lead exact prediction models!), note following:\\[\\begin{eqnarray*}\n\\mbox{model1:  } E[Y| X=0] &=& \\beta_0\\\\\n\\mbox{model2:  } E[Y | X=0] &=& \\beta_0^* + \\beta_1^*\\\\\n\\mbox{model2: }  E[Y | X=100] &=& \\beta_0^*\n\\end{eqnarray*}\\]\nsay, intuitive intercept value match X=0. intercept given basis functions equal zero (necessarily X value equals zero).","code":""},{"path":"smooth.html","id":"basis-functions","chapter":"15 Smoothing Methods","heading":"15.1.1 Basis Functions","text":"Polynomial piece-wise-constant regression models special cases basis function approach modeling. idea instead regressing series different \\(X\\) values, can regress different functions \\(X\\). ’ve seen already form transformations \\(X\\). basis function model :\\[E[Y_i | X_i] = \\beta_0 + \\beta_1 bf_1(X_i) + \\beta_2 bf_2(X_i) + \\cdots + \\beta_K bf_K(X_i).\\]Remember assumptions made explanatory variables running OLS model. say functions \\(X\\) cause problem model long variables collinear. Note polynomial regression, basis functions : \\(bf_j(X_i) = X_i^j\\). applying step functions model, basis functions \\(bf_j(X_i) = (c_j \\leq X_i < c_{j+1})\\). basis function model can fit using standard OLS techniques.One big advantages using OLS basis functions get bring along inference tools covered first part course. , estimates standard errors coefficients well ability compute p-values associated nested F-tests. 2*SE bounds estimated curves appropriate errors given predict() function basis function model(s).","code":""},{"path":"smooth.html","id":"regression-splines","chapter":"15 Smoothing Methods","heading":"15.2 Regression splines","text":"Regression splines (also called smoothing splines) flexible polynomials step functions, fact extension two. involve dividing range X K distinct regions. Within region, polynomial function fit data. However, polynomials constrained join smoothly region boundaries, knots. Provided interval divided enough regions, can produce extremely flexible fit.combining ideas polynomial fits step functions, can create model fits locally required horizontal line. particular, cubic functions fit region, model ends something like:\\[\\begin{eqnarray}\nE[Y_i | X_i] =\n\\begin{cases}\n\\beta_{01} + \\beta_{11} X_i + \\beta_{21} X_i^2 + \\beta_{31}X_i^3  \\mbox{   } X_i < c\\\\\n\\beta_{02} + \\beta_{12} X_i + \\beta_{22} X_i^2 + \\beta_{32}X_i^3  \\mbox{   } X_i \\geq c\n\\end{cases}\n\\end{eqnarray}\\]Notes:polynomial can fit (step functions polynomials degree 0!)fit independent normality independence assumptions. However, assumptions important order perform inference.\\(f(X)\\) isn’t linear polynomial X, model can fit locally. Note: every function can written sum polynomials.Keep mind splines computed truncated polynomials can numerically unstable explanatory variables may highly correlated.Note, however, cubic model equation requirement continuity value \\(c\\). solution fit piece-wise polynomial constraint fitted curve must continuous. Additionally, order curves seem smooth} constraint first second derivatives also continuous added. Generally, degree-\\(d\\) spline: fit degree-\\(d\\) polynomials continuity derivatives degree \\(d-1\\) knot.One method fitting continuous cubic models partitioned \\(X\\) variable create following function \\(h(X, \\xi)\\) knot \\(\\xi\\).\\[\\begin{eqnarray*}\nh(X, \\xi) = (X - \\xi)^3_+ =\n\\begin{cases}\n(X-\\xi)^3 & \\mbox{   } X > \\xi\\\\\n0 & \\mbox{  otherwise}\n\\end{cases}\n\\end{eqnarray*}\\]\nfollowing cubic spline \\(K\\) knots modeled \n\\[\\begin{eqnarray}\nE[Y_i | X_i] = \\beta_0 + \\beta_1 X_i + \\beta_2 X_i^2 + \\beta_3 X_i^3 + \\beta_4 h(X_i, \\xi_1) + \\beta_5 h(X_i, \\xi_2) + \\cdots + \\beta_{K+3} h(X_i, \\xi_K). \n\\end{eqnarray}\\]\nNote \\(K+3\\) predictors \\(K+4\\) regression coefficients model. using \\(K+4\\) degrees freedom.little bit work can show spline equation lead continuity first two derivatives discontinuity third derivative (continuous function). Proof ideas:continuity, consider one knot. \\(X\\) approaches \\(\\xi\\) left, follow function without \\(h\\) part function. \\(X\\) approaches \\(\\xi\\) right, get arbitrarily close function without \\(h\\).Consider happens points second knot. , \\(X \\leq \\xi_2\\):\n\\[\\begin{eqnarray*}\n(X - \\xi_1)^3 &=& X^3 - 3X^2 \\xi_1 + 3X \\xi_1^2 - \\xi_1^3\\\\\nE[Y | \\xi_1 < X \\leq \\xi_2] &=& (\\beta_0 - \\beta_4 \\xi_1^3) + (\\beta_1 + \\beta_4 3 \\xi_1^2)X + (\\beta_2 - \\beta_4 3 \\xi_1)X^2 + (\\beta_3 + \\beta_4)X^3\\\\\nE[Y | X \\leq \\xi_1] &=& \\beta_0  + \\beta_1X + \\beta_2X^2 + \\beta_3 X^3\\\\\nlim_{X \\rightarrow \\xi_1^-} E[Y|X] &=& \\beta_0  + \\beta_1 \\xi_1 + \\beta_2\\xi_1^2 + \\beta_3 \\xi_1^3\\\\\nlim_{X \\rightarrow \\xi_1^+} E[Y|X] &=& \\beta_0  - \\beta_4 \\xi_1^3 + \\beta_1 \\xi_1 + \\beta_4 3 \\xi_1^3 + \\beta_2\\xi_1^2 - \\beta_4 3 \\xi_1^3 + \\beta_3 \\xi_1^3 + \\beta_4\\xi_1^3\\\\\n&=&  \\beta_0  + \\beta_1 \\xi_1 + \\beta_2\\xi_1^2 + \\beta_3 \\xi_1^3\\\\\n\\end{eqnarray*}\\]continuity derivatives, recall derivatives respect \\(X\\). taking derivatives \\(E[Y_i]\\) limit \\(X \\rightarrow \\xi\\), continuity first derivatives discontinuity later derivatives can seen.","code":""},{"path":"smooth.html","id":"knots","chapter":"15 Smoothing Methods","heading":"Knots","text":"Knots can placed uniformly places function expected change rapidly. often number knots set software places uniformly. number knots directly related degrees freedom model, setting degrees freedom also sets number knots. Ideally, method cross validation used optimize number knots.","code":""},{"path":"smooth.html","id":"degrees-of-freedom","chapter":"15 Smoothing Methods","heading":"degrees of freedom","text":"Note can fit model based placing knots based number knots (specified degrees freedom places df-degree internal knots). Consider following call model. number knots df-degree, , 7 knots. bs() function, df+1 number coefficients model.","code":""},{"path":"smooth.html","id":"local-regression","chapter":"15 Smoothing Methods","heading":"15.3 Local regression","text":"Local regression similar splines, differs important way. regions allowed overlap, indeed smooth way.Local regression (loess - locally weighted scatterplot smoothing) models fit flexible, non-linear models point \\(x_0\\) using training values close \\(x_0\\). distance training point \\(x_0\\) considered weight given \\(K_{i0}\\). fit weighted least squares regression using weights \\(X\\)-direction. algorithm local regression given (section 7.6, James et al. (2021)):Local Regression \\(X=x_0\\) (\\(p=2\\))Gather fraction \\(s=k/n\\) training points whose \\(X_i\\) closest \\(x_0\\). \\(s\\) called span.Assign weight \\(K_{i0} = K(X_i, x_0)\\) point neighborhood, furthest point \\(x_0\\) weight zero closest point highest weight. \\(k\\) closest points get zero weight.Fit weighted least squares regression \\(Y_i\\) \\(X_i\\) using \\(K_{i0}\\) weights. Find \\(b_0\\) \\(b_1\\) minimize:\n\\[\\sum_{=1}^n K_{i0}(Y_i - b_0 - b_1 X_i)^2.\\]fitted value \\(x_0\\) given \\(\\hat{f}(x_0) = b_0 + b_1x_0\\).Repeat steps 1. - 4. every point; coefficients & SE re-computed time.create smooth regression function, points connected drawing line (surface) predicted value. fraction points non-zero weights given span, \\(s=k/n\\). span plays role similar tuning parameters. can made produce model similar OLS (large span) produce model way -fits data (small span). Cross validation can used find \\(s\\).\nFigure 6.1: Figure 7.9 James et al. (2021) shows weighted local regression.\nLocal Regression can theoretically expanded higher dimensions. Weighted least squares (see section 11.1 Kutner et al. (2004)) given minimizing:\\[\\sum_{=1}^n w_i(Y_i - b_0 - b_1X_{i1} - \\cdots - b_{p-1}X_{,p-1})^2.\\]normal equations solved way given coefficient estimates standard errors (\\(W\\) diagonal matrix weights):\n\\[\\begin{eqnarray*}\n(X^t W X) b_w &=& X^t W Y\\\\\nb_w &=& (X^t W X)^{-1} X^t W Y\\\\\nvar(b_w) &=& \\sigma^2 (X^t W X)^{-1} (X^t W W X) (X^t W X)^{-1}\n\\end{eqnarray*}\\]However, high dimensional settings distance \\(x_0\\) might quite large, points high dimensional neighborhood \\(x_0\\). distance problem leads points zero weight (.e., large distance) points. regression estimates become quite variable hard estimate.","code":""},{"path":"smooth.html","id":"weight-function-for-local-regression","chapter":"15 Smoothing Methods","heading":"15.3.1 Weight function for local regression","text":"standard weight function used local regression called tricubic weight function.\\(s < 1\\), neighborhood includes proportion \\(s\\) points, tricubic weighting\\[K_{i0}  = \\Bigg(1 - \\bigg(\\frac{d(x_i, x_0)}{\\max_{\\S} d(x_i, x_0)} \\bigg)^3 \\Bigg)^3 \\bigg[d(x_i, x_0) < \\max_{\\S} d(x_i, x_0)\\bigg]\\]\n\\(S\\) defines set \\(x_i\\) values \\(k\\) closest points \\(x_0\\).\\(s > 1\\), points used, “maximum distance” () assumed \\(s^{(1/(p-1))}\\) times actual maximum distance \\(p\\) coefficients (\\(p-1\\) explanatory variables). \\(s > 1\\) means weights (\\(K_{i0}\\)) increase larger larger \\(s\\).Distance can defined using distance function, Euclidean default although resistant outliers.Regression done least squares (default) although regression techniques can used (e.g., Tukey’s biweight M-estimation regression… won’t talk class).loess() function R modeling / prediction. ggplot2 default method geom_smooth() plot uses loess.","code":""},{"path":"smooth.html","id":"imputation","chapter":"15 Smoothing Methods","heading":"15.3.2 Imputation","text":"Imputation process replacing missing data summary values (.e., statistics) rest data. biggest reason impute data complete dataset can used analysis. Notice within context models throughout semester, might make sense smooth \\(Y\\) explanatory variables (\\(Y\\) missing observations), also might make sense smooth \\(X_1\\) explanatory variables (\\(X_1\\) missing used linear model).Keep mind algorithm creating smooth prediction \\(x_0\\) depend \\(x_0\\) part model. , prediction possible whether \\(x_0\\) corresponding response value.","code":""},{"path":"smooth.html","id":"normalization","chapter":"15 Smoothing Methods","heading":"15.3.3 Normalization","text":"Microarrays high-throughput analysis techniques require normalization order create apples apples comparisons. Wikipedia, https://en.wikipedia.org/wiki/Microarray_analysis_techniquesComparing two different arrays, two different samples hybridized array generally involves making adjustments systematic errors introduced differences procedures dye intensity effects. Dye normalization two color arrays often achieved local regression. LIMMA provides set tools background correction scaling, well option average -slide duplicate spots.[8] common method evaluating well normalized array , plot MA plot data.\nFigure 6.2: microarray shows differing amounts expression across two conditions (old young yeast). expectation , median, dots (.e., genes) yellow. can seen image, points left side microarray dimmer points right side. imbalance artifact technical limitations technique. Image due Laura Hoopes, Pomona College.\n\nFigure 6.3: [left] M = ratio expression, = product expression (total amount expression). different smooth curves refer different locations microarray chip. normalize, subtract line corresponding dot can thought taking colored lines pulling taut. [right] centering array’s expression values zero (either across location chip ‘print-tip group’ within array ), can apples apples comparison expression across different samples.\n","code":""},{"path":"smooth.html","id":"prediction","chapter":"15 Smoothing Methods","heading":"15.3.4 Prediction","text":"Hurricane Maria devastated Puerto Rico September 2017, much discussion count resulting number deaths. Rolando Acosta Rafael Irizzary use loess models predict excess deaths Post-Hurricane Vital Statistics Expose Fragility Puerto Rico’s Health System.\nFigure 1.5: Increase death rate function date. Note y-axis observed rate minus expected rate (found using trend line given loess broken age group).\n","code":""},{"path":"smooth.html","id":"last-thoughts","chapter":"15 Smoothing Methods","heading":"15.4 Last Thoughts…","text":"","code":""},{"path":"smooth.html","id":"why-smooth","chapter":"15 Smoothing Methods","heading":"15.4.1 Why smooth?","text":"","code":""},{"path":"smooth.html","id":"advantages-of-smoothing","chapter":"15 Smoothing Methods","heading":"Advantages of smoothing","text":"Smoothing methods require known functional relationship \\(X\\) \\(Y\\).\nRegression splines provide functional model\nloess provide functional model\nRegression splines provide functional modelloess provide functional modelThe relationships easy fitand retain many advantages weighted least squares (including confidence estimates predicted values).","code":""},{"path":"smooth.html","id":"disadvantages-of-smoothing","chapter":"15 Smoothing Methods","heading":"Disadvantages of smoothing","text":"Local regression can computationally intensive, methods giving unstable estimates high dimensions (outer range X) due sparsity points.Regression splines arbitrary knots may fit model well.Although interpolation can used get predictions (standard errors), functional form relationship \\(X\\) \\(Y\\) loess. Inference coefficients meaningless.","code":""},{"path":"smooth.html","id":"inference-1","chapter":"15 Smoothing Methods","heading":"15.5 Inference","text":"Keep mind order p-value (probability), must probability model. OLS assumes normal errors, basis functions weighted OLS applied using standard linear model techniques, notion iid normal error structure. Confidence intervals also require probability model order apply standard inferential interpretations.particular, wind temperature observations independent. strong dependency temperature two consecutive days. data much better described autoregressive model goal inference. (Autoregression used, e.g., x-variable time observations correlated. y-variable stock price temperature. haven’t talked types models formal way.) goal descriptive (simply predictive reasons normalization extrapolation), SE values CI bounds needed, smooth curve probably effective even technical conditions hold.","code":""},{"path":"smooth.html","id":"dont-forget","chapter":"15 Smoothing Methods","heading":"Don’t Forget","text":"substitute thinking carefully modeling relationships. Whether linear, non-linear, sparse, locally weighted, optimized. almost never single model one right model. Instead, expertise practice provide strategies help come model describes data well.kernel smoothers, see appendix Sheather (2009) chapter 6 T. Hastie, Tibshirani, Friedman (2001).","code":""},{"path":"smooth.html","id":"reflection-questions-11","chapter":"15 Smoothing Methods","heading":"15.6  Reflection Questions","text":"different types flexible models represented linear model form? (Step functions, polynomials, regression splines, local regression)inferential methods applied models ? necessary technical assumptions make inferential claims?basis function? used similarly / differently step functions, polynomials, regression splines?regression splines, number knots play role? different choice number knots change resulting model? choose fit model fewer knots?local regression, weights play role? different choices made respect weight functions?local regression, span play role? different choices span change resulting model? choose fit model fewer knots?inference claims accessible, else can smoothing techniques used ?","code":""},{"path":"smooth.html","id":"ethics-considerations-10","chapter":"15 Smoothing Methods","heading":"15.7  Ethics Considerations","text":"information presented comes March 2022 blog Peter Ellis free range statistics called Smoothing charts Supreme Court Justice nomination results. ’ll back minute provide context.","code":""},{"path":"smooth.html","id":"politics-blog","chapter":"15 Smoothing Methods","heading":"538 politics blog","text":"538 story describing harder ever confirm supreme court justice.\nFigure 1.6: Image credit: https://fivethirtyeight.com/features/-harder--ever--confirm--supreme-court-justice/\n","code":""},{"path":"smooth.html","id":"modeling","chapter":"15 Smoothing Methods","heading":"Modeling","text":"know semester together, many ways model x-y relationship.\nFigure 1.7: Image credit: https://xkcd.com/2048/\n","code":""},{"path":"smooth.html","id":"twitter","chapter":"15 Smoothing Methods","heading":"Twitter","text":"People twitter get worked things. gist complaints shouldn’t modeling future votes based historical trends.\nFigure 15.1: Image credit: https://twitter.com/AlecStapp/status/1507542987563323393\nFive Thirty Eight said “Supreme Court confirmations increasingly likely resemble party-line votes.” prediction ?\nFigure 6.4: Image credit:https://twitter.com/FiveThirtyEight/status/1488874422337482755\n","code":""},{"path":"smooth.html","id":"averages","chapter":"15 Smoothing Methods","heading":"15.7.1 Averages","text":"Twitter misses point 538 predicting anything. Well, least model doesn’t predict anything. Instead, using loess smooth average time. descriptive model try make inferential claims.Ellis gives fantastic discussion including trying different models providing R code.\nFigure 15.2: Image credit: http://freerangestats.info/blog/2022/03/26/supreme-court-nominations\n","code":""},{"path":"smooth.html","id":"r-smoothing-noaa","chapter":"15 Smoothing Methods","heading":"15.8 R: Smoothing – NOAA","text":"First scrape weather data NOAA. resulting data use wind temperature noon across day year (2014) measured buoy coast Santa Monica.","code":"\nbuoy_url <- \"http://www.ndbc.noaa.gov/view_text_file.php?filename=46025h2014.txt.gz&dir=data/historical/stdmet/\"\nbuoy_data_orig <- read_table(buoy_url, skip=2, col_names=FALSE)\ntemp <- read_table(buoy_url, n_max=1, col_names=FALSE)\ntemp <- unlist(strsplit(unlist(temp), \"\\\\s+\"))\nnames(buoy_data_orig) <- temp\n\nbuoy_data <- buoy_data_orig %>% \n  mutate(WVHT = ifelse(WVHT==99, NA, WVHT)) %>%\n  mutate(DPD = ifelse(DPD==99, NA, DPD)) %>%\n  mutate(APD = ifelse(APD==99, NA, APD)) %>%\n  mutate(MWD = ifelse(MWD==999, NA, MWD)) %>%\n  mutate(PRES = ifelse(PRES==9999, NA, PRES)) %>%\n  mutate(DEWP = ifelse(DEWP==99, NA, DEWP)) %>%\n  select(-VIS, -TIDE) %>% filter(`#YY`==2014)  %>% filter(hh==\"12\") %>%\n  mutate(yearday = yday(mdy(paste(MM,DD,`#YY`, sep=\"-\"))))"},{"path":"smooth.html","id":"polynomial-regression-and-step-functions","chapter":"15 Smoothing Methods","heading":"15.8.1 Polynomial Regression and Step Functions","text":"","code":""},{"path":"smooth.html","id":"cubic-model","chapter":"15 Smoothing Methods","heading":"Cubic Model","text":"","code":"\n# cubic model\nwind_cub <- lm(WTMP ~ poly(yearday,3, raw = TRUE), data=buoy_data)\nwind_cub %>%\n  tidy()## # A tibble: 4 × 5\n##   term                             estimate    std.error statistic   p.value\n##   <chr>                               <dbl>        <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                   16.2        0.196             82.3 6.55e-229\n## 2 poly(yearday, 3, raw = TRUE)1 -0.0544     0.00470          -11.6 2.16e- 26\n## 3 poly(yearday, 3, raw = TRUE)2  0.000655   0.0000299         21.9 3.17e- 67\n## 4 poly(yearday, 3, raw = TRUE)3 -0.00000140 0.0000000539     -26.0 2.14e- 83\n# cubic predictions\nwind_cub %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Cubic Fit\")"},{"path":"smooth.html","id":"step-functions-1","chapter":"15 Smoothing Methods","heading":"Step Functions","text":"","code":"\n# cutting the yearday variable\nbuoy_data %>%\n  summarize(cut(yearday, 4)) %>% table()## .\n## (0.636,92]   (92,183]  (183,274]  (274,365] \n##         90         86         83         90\nlm(WTMP ~ cut(yearday, 4), data=buoy_data) %>%\n  tidy()## # A tibble: 4 × 5\n##   term                     estimate std.error statistic   p.value\n##   <chr>                       <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                 15.4      0.153    101.   1.29e-258\n## 2 cut(yearday, 4)(92,183]      1.79     0.218      8.18 5.32e- 15\n## 3 cut(yearday, 4)(183,274]     5.83     0.220     26.5  4.86e- 85\n## 4 cut(yearday, 4)(274,365]     3.95     0.216     18.3  1.12e- 52\n# step function model\nwind_step <- lm(WTMP ~ cut(yearday, 4), data=buoy_data)\nwind_step %>%\n  tidy()## # A tibble: 4 × 5\n##   term                     estimate std.error statistic   p.value\n##   <chr>                       <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                 15.4      0.153    101.   1.29e-258\n## 2 cut(yearday, 4)(92,183]      1.79     0.218      8.18 5.32e- 15\n## 3 cut(yearday, 4)(183,274]     5.83     0.220     26.5  4.86e- 85\n## 4 cut(yearday, 4)(274,365]     3.95     0.216     18.3  1.12e- 52\n#cubic predictions\nwind_step %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Step Function Fit\")"},{"path":"smooth.html","id":"regression-spline","chapter":"15 Smoothing Methods","heading":"15.8.2 Regression Spline","text":"Note function bs() calculates “B-Spline Basis Polynomial Splines.” can learn ?bs splines package. knots argument gives “internal breakpoints define spline.” degree degree polynomial. , three internal knots.","code":"\nrequire(splines)\nyear_knot1 <- bs(buoy_data$yearday, knots=c(92,183,274), degree=3)\n\nwind_rs1 <- lm(WTMP ~ year_knot1, data=buoy_data)\nwind_rs1 %>% tidy## # A tibble: 7 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)    14.9      0.261     56.9  1.70e-176\n## 2 year_knot11     1.98     0.488      4.06 6.21e-  5\n## 3 year_knot12    -2.47     0.321     -7.68 1.69e- 13\n## 4 year_knot13     6.44     0.391     16.5  2.59e- 45\n## 5 year_knot14     7.91     0.359     22.0  1.37e- 67\n## 6 year_knot15     3.64     0.406      8.97 2.05e- 17\n## 7 year_knot16     1.82     0.363      5.02 8.37e-  7\n# regression spline predictions\nwind_rs1 %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Regression Spline Fit\")"},{"path":"smooth.html","id":"degrees-of-freedom-1","chapter":"15 Smoothing Methods","heading":"degrees of freedom","text":"Note can fit model based placing knots based number knots (specified degrees freedom places df-degree internal knots). Consider following call model. number knots df-degree, , 7 knots. bs() function, df+1 number coefficients model. knots argument supersedes df argument function.","code":"\nyear_knot2 <- bs(buoy_data$yearday, df=10, degree=3)\nwind_rs2 <- lm(WTMP ~ year_knot2, data=buoy_data)\n\n# note that the coefficients are very similar\nwind_rs1 %>% tidy()## # A tibble: 7 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)    14.9      0.261     56.9  1.70e-176\n## 2 year_knot11     1.98     0.488      4.06 6.21e-  5\n## 3 year_knot12    -2.47     0.321     -7.68 1.69e- 13\n## 4 year_knot13     6.44     0.391     16.5  2.59e- 45\n## 5 year_knot14     7.91     0.359     22.0  1.37e- 67\n## 6 year_knot15     3.64     0.406      8.97 2.05e- 17\n## 7 year_knot16     1.82     0.363      5.02 8.37e-  7\nwind_rs2 %>% tidy()## # A tibble: 11 × 5\n##    term         estimate std.error statistic   p.value\n##    <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n##  1 (Intercept)    15.6       0.348    44.9   1.66e-144\n##  2 year_knot21    -0.692     0.642    -1.08  2.82e-  1\n##  3 year_knot22     0.445     0.413     1.08  2.81e-  1\n##  4 year_knot23    -0.865     0.502    -1.72  8.59e-  2\n##  5 year_knot24     0.207     0.412     0.501 6.17e-  1\n##  6 year_knot25     6.04      0.444    13.6   6.14e- 34\n##  7 year_knot26     5.21      0.439    11.9   2.22e- 27\n##  8 year_knot27     7.12      0.450    15.8   1.38e- 42\n##  9 year_knot28     2.98      0.496     6.01  4.73e-  9\n## 10 year_knot29     2.48      0.521     4.77  2.81e-  6\n## 11 year_knot210    0.914     0.489     1.87  6.26e-  2"},{"path":"smooth.html","id":"choosing-df-with-cv","chapter":"15 Smoothing Methods","heading":"Choosing df with CV","text":"code using CV determine optimal number knots (function df) well polynomial degree.fun, ’ve plotted two combinations df degree. degree 1, model piece-wise linear (4 knots, 5 regions). degree 2, model piece-wise quadratic. df = 15 15 non-intercept coefficients. means linear plus quadratic terms (=2) plus 13 knot terms (giving 14 different quadratic models across 14 different regions).","code":"\nset.seed(4747)\nwind_rec <- recipe(WTMP ~ yearday, data = buoy_data) %>%\n  step_bs(yearday, deg_free = tune(), degree = tune())\n\nwind_cv <- vfold_cv(buoy_data, v = 5)\n\nwind_lm <- linear_reg() %>%\n  set_engine(\"lm\")\n\nwind_df <- grid_regular(deg_free(range = c(5, 15)), \n                        degree(range = c(1,5)) , levels = 5)\n\nwind_tuned <- wind_lm %>%\n  tune_grid(wind_rec,\n            resamples = wind_cv,\n            grid = wind_df)\n\ncollect_metrics(wind_tuned)## # A tibble: 50 × 8\n##    deg_free degree .metric .estimator  mean     n std_err .config              \n##       <int>  <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1        5      1 rmse    standard   0.874     5 0.0337  Preprocessor01_Model1\n##  2        5      1 rsq     standard   0.887     5 0.00967 Preprocessor01_Model1\n##  3        7      1 rmse    standard   0.771     5 0.0227  Preprocessor02_Model1\n##  4        7      1 rsq     standard   0.913     5 0.00666 Preprocessor02_Model1\n##  5       10      1 rmse    standard   0.711     5 0.0244  Preprocessor03_Model1\n##  6       10      1 rsq     standard   0.925     5 0.00643 Preprocessor03_Model1\n##  7       12      1 rmse    standard   0.722     5 0.0286  Preprocessor04_Model1\n##  8       12      1 rsq     standard   0.922     5 0.00766 Preprocessor04_Model1\n##  9       15      1 rmse    standard   0.683     5 0.0441  Preprocessor05_Model1\n## 10       15      1 rsq     standard   0.930     5 0.00965 Preprocessor05_Model1\n## # … with 40 more rows\ncollect_metrics(wind_tuned) %>%\n  ggplot(aes(x = deg_free, y = mean, color = as.factor(degree))) + \n  geom_line() + \n  facet_grid(.metric ~ .) + \n  labs(color = \"degree\") + \n  ylab(\"\") + \n  xlab(\"degrees of freedom (# coefficients)\")\n# df = 5, degree = 1\nyear_knot3 <- bs(buoy_data$yearday, df = 5, degree=1)\n\nwind_rs3 <- lm(WTMP ~ year_knot3, data=buoy_data)\nwind_rs3 %>% tidy## # A tibble: 6 × 5\n##   term        estimate std.error statistic   p.value\n##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)   15.6       0.198     79.0  4.28e-222\n## 2 year_knot31   -0.685     0.286     -2.40 1.71e-  2\n## 3 year_knot32    1.81      0.230      7.87 4.62e- 14\n## 4 year_knot33    6.37      0.250     25.5  2.95e- 81\n## 5 year_knot34    5.54      0.243     22.8  1.03e- 70\n## 6 year_knot35    0.959     0.279      3.44 6.61e-  4\n# regression spline predictions\nwind_rs3 %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Regression Spline Fit (df = 5, degree = 1)\")\n# df = 15, degree = 2\nyear_knot4 <- bs(buoy_data$yearday, df = 15, degree=2)\n\nwind_rs4 <- lm(WTMP ~ year_knot4, data=buoy_data)\nwind_rs4 %>% tidy## # A tibble: 16 × 5\n##    term         estimate std.error statistic   p.value\n##    <chr>           <dbl>     <dbl>     <dbl>     <dbl>\n##  1 (Intercept)    15.3       0.328    46.7   3.84e-148\n##  2 year_knot41     0.308     0.551     0.559 5.77e-  1\n##  3 year_knot42    -0.191     0.362    -0.529 5.97e-  1\n##  4 year_knot43     0.314     0.416     0.756 4.50e-  1\n##  5 year_knot44     0.364     0.403     0.904 3.67e-  1\n##  6 year_knot45    -1.08      0.399    -2.70  7.34e-  3\n##  7 year_knot46     3.01      0.397     7.59  3.15e- 13\n##  8 year_knot47     3.87      0.398     9.72  8.20e- 20\n##  9 year_knot48     6.25      0.400    15.6   1.43e- 41\n## 10 year_knot49     4.94      0.412    12.0   8.52e- 28\n## 11 year_knot410    7.58      0.395    19.2   9.34e- 56\n## 12 year_knot411    5.51      0.399    13.8   1.23e- 34\n## 13 year_knot412    5.86      0.401    14.6   9.66e- 38\n## 14 year_knot413    2.89      0.411     7.03  1.14e- 11\n## 15 year_knot414    2.86      0.451     6.35  7.26e- 10\n## 16 year_knot415    1.03      0.463     2.22  2.71e-  2\n# regression spline predictions\nwind_rs4 %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Regression Spline Fit (df = 15, degree = 2)\")"},{"path":"smooth.html","id":"local-regression-loess","chapter":"15 Smoothing Methods","heading":"15.8.3 Local Regression (loess)","text":"","code":"\nwind_lo <- loess(WTMP ~ yearday, span=.3, data=buoy_data)\n\n# loess predictions\nwind_lo %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1, yearday = yearday...2) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point() + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  ggtitle(\"Loess Fit\")"},{"path":"smooth.html","id":"choosing-the-span-with-cv-or-not","chapter":"15 Smoothing Methods","heading":"Choosing the span with CV (or not)","text":"Like regression splines, cross validation can used set span loess. Unfortunately, easy tidymodels way cross validate find span (although logic ).Instead, plots , loess model fit series different span values. Notice, however, NOAA observations, span doesn’t change fit much. large dataset covers changes response variable. , signal quite strong.One interesting aspect loess() support extrapolation, predict smallest largest points dataset left .","code":"\nspan_vals <- seq(0.05, 1, .1)\nwind_full <- data.frame()\n\nfor(i in 1:length(span_vals)){\n  \nwind_lo <- loess(WTMP ~ yearday, \n                     span = span_vals[i],\n                     data = buoy_data)\n\nwind_lo_output <- wind_lo %>% \n  augment(se_fit = TRUE) %>% \n  bind_cols(buoy_data) %>%\n  rename(WTMP = WTMP...1, yearday = yearday...2) %>%\n  mutate(upper = .fitted + 2*.se.fit,\n         lower = .fitted - 2*.se.fit) %>%\n  mutate(span = span_vals[i]) %>%\n  dplyr::select(WTMP, yearday, .fitted, upper, lower, span)\n\n# each time we go through the loop and change the span\n# the new predictions get concatenated onto the full dataset\nwind_full <- wind_full %>% bind_rows(wind_lo_output)\n\n}\n\nwind_full %>%\n  ggplot(aes(x = yearday, y = WTMP)) + \n  geom_point(alpha = .1) + \n  geom_line(aes(y = .fitted), color = \"blue\") + \n  geom_line(aes(y = upper), lty = 3, color = \"blue\") + \n  geom_line(aes(y = lower), lty = 3, color = \"blue\") + \n  facet_wrap(~span) + \n  ggtitle(\"Loess Fit (changing span)\")"},{"path":"smooth.html","id":"imputing-using-local-regression-loess","chapter":"15 Smoothing Methods","heading":"Imputing using Local Regression (loess)","text":"following TechNote19 shows examples loess (local polynomial regression fitting) smoothing various “span” values. online R documentation (?loess) says default span value 0.75, doesn’t give much guidance, visual examples, span value affects smoothing. addition simply smoothing curve, R loess() function can used impute missing data points. example data imputation loess() shown.Let’s take sine curve, add “noise” , see loess “span” parameter affects look smoothed curve.Create sine curve add noise:Plot points noisy sine curve:Apply loess smoothing using default span value 0.75.Compute loess smoothed values points along curve using `augment().Plot loess smoothed curve along points already plotted:Repeat steps 1-5 various span values.Compare “noise” uniform distribution -1 1 () Gaussian noise, mean 0 standard deviation 1.0 ().Let’s use loess() impute data points. Start taking sine curve noise, like computed , leave 15 120 data points using R’s “sample” function. Note gaps (true) sine function superimposed points., use loess() augment() functions get smoothed values defined points:Use loess predict functions also impute values missing points:Compare loess smoothed fit imputed points various span values.Given sine data, span values small 0.10 provide much smoothing can result “jerky” curve. Span values large 2.0 provide perhaps much smoothing, least cases shown . Overall, default value 0.75 worked fairly well “finding” sine curve.","code":"\nset.seed(47)\nperiod <- 120\nx <- 1:120\ny <- sin(2*pi*x/period) + runif(length(x),-1,1)\n\nsine_unif_data <- data.frame(x = x, y = y)\nsine_unif_data %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  ggtitle(\"Sine Curve + Uniform Noise\")\ny_loess <- loess(y ~ x, span=0.2, data = sine_unif_data)\ny_loess %>% augment()## # A tibble: 120 × 4\n##         y     x .fitted  .resid\n##     <dbl> <int>   <dbl>   <dbl>\n##  1  1.01      1   0.558  0.448 \n##  2 -0.148     2   0.550 -0.697 \n##  3  0.679     3   0.543  0.136 \n##  4  0.853     4   0.540  0.312 \n##  5  0.406     5   0.542 -0.136 \n##  6  0.692     6   0.547  0.144 \n##  7  0.136     7   0.554 -0.417 \n##  8  0.345     8   0.560 -0.215 \n##  9  0.541     9   0.571 -0.0303\n## 10  1.35     10   0.589  0.761 \n## # … with 110 more rows\ny_loess %>% augment() %>%\n  mutate(truth = sin(2*pi*x/period)) %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_point() + \n  geom_line(aes(x = x, y = .fitted)) + \n  geom_line(aes(x = x, y = truth), color = \"purple\") +\n  ggtitle(\"Sine Curve + Uniform Noise\")\nloess_unif <- data.frame()\nspanlist <- c(0.10, 0.25, 0.50, 0.75, 1.00, 2.00)\nfor (i in 1:length(spanlist))\n{\n  loess_unif_pred <- loess(y ~ x, span = spanlist[i], \n                           data = sine_unif_data) %>%\n    augment() %>%\n    mutate(span = spanlist[i])\n  \n  loess_unif <- loess_unif %>% bind_rows(loess_unif_pred)\n  \n}\n    loess_unif %>%\n      mutate(truth = sin(2*pi*x/period)) %>%\n      ggplot(aes(x = x, y = y)) + \n      geom_point() + \n      geom_line(aes(x = x, y = .fitted, color = as.factor(span))) + \n      geom_line(aes(x = x, y = truth), color = \"black\") +\n      labs(color = \"span\") + \n      ggtitle(\"Sine Curve + Uniform Noise\")\nset.seed(47)\nperiod <- 120\nx <- 1:120\ny <- sin(2*pi*x/period) + rnorm(length(x), 0, 1)\n\nsine_norm_data <- data.frame(x = x, y = y)\nloess_unif <- data.frame()\nspanlist <- c(0.10, 0.25, 0.50, 0.75, 1.00, 2.00)\nfor (i in 1:length(spanlist))\n{\n  loess_unif_pred <- loess(y ~ x, span = spanlist[i], \n                           data = sine_norm_data) %>%\n    augment() %>%\n    mutate(span = spanlist[i])\n  \n  loess_unif <- loess_unif %>% bind_rows(loess_unif_pred)\n  \n}\n    loess_unif %>%\n      mutate(truth = sin(2*pi*x/period)) %>%\n      ggplot(aes(x = x, y = y)) + \n      geom_point() + \n      geom_line(aes(x = x, y = .fitted, color = as.factor(span))) + \n      geom_line(aes(x = x, y = truth), color = \"black\") +\n      labs(color = \"span\") + \n      ggtitle(\"Sine Curve + Normal Noise\")\nset.seed(47)\nperiod <- 120\nmissing_pts <- sample(1:period, 15)\nmissing_pts##  [1]  27  57 114  15  84   1  26  13  23  70   9  54   6  12 104\n # Create sine curve with noise\nx <- 1:period\ny <- sin(2*pi*x/period) + runif(length(x),-1,1) \nsine_miss_data <- data.frame(x = x, y = y) %>%\n  slice(-missing_pts)\n\n\nsine_miss_data %>%\n  mutate(truth = sin(2*pi*x/period)) %>% \n  bind_rows(data.frame(x = missing_pts, y = NA)) %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_line(aes(y = truth, group = 1), color = \"purple\") + \n  geom_point() + \n  ggtitle(\"Missing data gone from model\")\ny_miss_loess <- loess(y ~ x, span=0.75, data = sine_miss_data)\n\ny_miss_pred <- y_miss_loess %>% \n  augment(newdata = data.frame(x = missing_pts))\nsine_miss_data %>%\n  mutate(truth = sin(2*pi*x/period)) %>% \n  bind_rows(data.frame(x = missing_pts, y = NA)) %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_line(aes(y = truth, group = 1), color = \"purple\") + \n  geom_point() +\n  geom_point(data = y_miss_pred, aes(x = x, y = .fitted),\n             color = \"purple\", size = 4) +\n  ggtitle(\"Missing data imputed\")\ny_miss_loess_75 <- loess(y ~ x, span=0.75, data = sine_miss_data)\ny_miss_pred_75 <- y_miss_loess_75 %>% \n  augment(newdata = data.frame(x = missing_pts))\n\ny_miss_loess_50 <- loess(y ~ x, span=0.5, data = sine_miss_data)\ny_miss_pred_50 <- y_miss_loess_50 %>% \n  augment(newdata = data.frame(x = missing_pts))\n\ny_miss_loess_1 <- loess(y ~ x, span = 1, data = sine_miss_data)\ny_miss_pred_1 <- y_miss_loess_1 %>% \n  augment(newdata = data.frame(x = missing_pts))\n\nsine_miss_data %>%\n  mutate(truth = sin(2*pi*x/period)) %>% \n  bind_rows(data.frame(x = missing_pts, y = NA)) %>%\n  ggplot(aes(x = x, y = y)) + \n  geom_line(aes(y = truth, group = 1), color = \"black\") + \n  geom_point() +\n  geom_point(data = y_miss_pred_75, aes(x = x, y = .fitted),\n             color = \"purple\", size = 4) +\n  geom_point(data = y_miss_pred_50, aes(x = x, y = .fitted),\n             color = \"green\", size = 4) +\n  geom_point(data = y_miss_pred_1, aes(x = x, y = .fitted),\n             color = \"red\", size = 4) +\n  geom_line(data = y_miss_loess_75 %>% augment(), \n            aes(x = x, y = .fitted),\n             color = \"purple\", linetype = 2) +\n  geom_line(data = y_miss_loess_50 %>% augment(), \n            aes(x = x, y = .fitted),\n             color = \"green\", linetype = 2) +\n  geom_line(data = y_miss_loess_1 %>% augment(), \n            aes(x = x, y = .fitted),\n             color = \"red\", linetype = 2) +  \n  labs(title = \"Missing data imputed\",\n       subtitle = \"purple: 0.75; green = 0.5; red = 1.0\")"},{"path":"anova.html","id":"anova","chapter":"16 ANOVA","heading":"16 ANOVA","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
