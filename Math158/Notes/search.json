[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 158 Pomona College: Computational Statistics. notes based extensively Introduction Statistical Learning (James et al. 2021); Applied Linear Statistical Models (Kutner et al. 2004). computing part class taken R Data Science (Wickham Grolemund 2017) Wickham Grolemund well Tidy Modeling R (Kuhn Silge 2021) Kuhn Silge.responsible reading relevant chapters texts. texts good & readable, use . make sure coming class also reading materials associated activities.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims populationWhat content Math 158?\nMath 158 course statistical linear models.goal Math 158 understand modeling linear statistical relationships explanatory / predictor (X) variables response (Y) variables.models grow sophistication semester including multiple linear regression, interaction terms, ridge regression, Lasso, smoothing.Throughout semester, continue talk good modeling practices, ideas extend beyond linear models types inference prediction.think carefully inferential modeling makes sense predictive modeling makes sense. neither type analysis done!math class go quickly, little calculus. , however, learn linear algebra.necessarily use linear models determine causation (need experimental design course discuss issues observational study vs. experiment). E.g., (1) thermostat versus actual temperature, (2) ice cream sales versus boating accidents.take Math 158?\nLinear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.prerequisites Math 158?\nLinear Models requires strong background statistics well algorithmic thinking. formal prerequisite introductory statistics course, AP Statistics, may find working hard first weeks class catch . taken lot mathematics, parts course come easily . However, mathematics degree substitute introductory statistics, taken introductory statistics, majority course work intuitive . must taken prior statistics course pre-requisite Math 158; computer science course helpful.Many derivations much notation course come linear algebra. taken linear algebra enormously helpful class, cover enough notation need linear algebra.worth noting concepts probability theory represent majority mathematical tools used statistics / modeling, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.overlap classes?\nStatistical Linear Models overlaps Econometrics (Econ 167) Applied Econometrics (107). Econometrics focuses probability theory matrix algebra (mathematics) lead derivation linear models. Applied Econometrics focuses tools analysis. Statistical Linear Models focuses use model, assumptions made, conclusions appropriate given results. Additionally, later topics Linear Models typically covered Econometrics.take Math 158?\nprerequisite Linear Models introduction statistics, course moves quickly covers tremendous amount material. ideally suited first year student coming straight AP Statistics. Instead, student focus taking mathematics, CS, interdisciplinary science, statistics courses. students taking Linear Models sophomores juniors.workload Math 158?\none homework assignment per week, two -class midterm exams, two take-home midterm exams, final end semester project. Many students report working 8-10 hours per week outside class.software use? real world applications? mathematics? CS?\nwork done R (using RStudio front end, called integrated development environment, IDE). need either download R RStudio (free) onto computer use Pomona’s server. assignments posted private repositories GitHub. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.assignments turned using R Markdown compiled pdf + pushed GitHub\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Linear Models ubiquitous incredibly powerful. Indeed, often times linear models appropriate (e.g., violating technical conditions) yet end giving almost identical solutions models appropriate. solid understanding linear model framework, however, requires strong foundation theory goes inferential thinking well predictive modeling. review ideas inference introductory statistics.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]","code":""},{"path":"intro.html","id":"simple-linear-regression","chapter":"1 Introduction","heading":"1.2.2 Simple Linear Regression","text":"simplest case, study first, suppose two variables. call one explanatory variable, response variable.Explanatory / Predictor variable: also known independent variable, numeric variable often known advance variable, thought possibly influence value.Response / outcome variable: also known dependent variable, also numeric, thought function predictor variable. n.b., don’t use word “dependent” don’t want send message ’ve measured anything causal model.goal ascertain relationship two. observe sample population interest, \\((x_i,y_i), =1,\\dots,n\\). \\(x_i\\) predictor, \\(y_i\\) response, \\(n\\) sample size.observe sample, actual population , best can hope estimate relationship two variables. resulting estimates give us idea relationship actually population. However, random quantities, depend random sample. , exact. theory hypothesis testing needed\ndetermine much can actually say population quantities, called parameters.Parameter: quantity describes population. Examples population mean, population standard deviation, say relationship two variables polynomial, coefficients function parameters.","code":""},{"path":"intro.html","id":"hypothesis-testing","chapter":"1 Introduction","heading":"1.2.3 Hypothesis Testing","text":"set : population big observe. ’d like know value specific parameters, say instance mean population. can’t however, calculate parameters directly.\nInstead, observe sample random population. Based sample, estimate parameters. However, estimated values exact. need technique uses estimated model say something population model.Null Hypothesis: Denoted \\(H_0\\), null hypothesis usually set believed unless evidence presented otherwise. specific, specifying parameter equal specific value.\n\\(H_0\\) true, theory tells us exactly estimate behaves.Alternative Hypothesis: Denoted \\(H_a\\), alternative hypothesis usually wish show true. general \\(H_0\\), usually form parameter somehow equal value used \\(H_0\\), without specifying exactly think value . result, don’t know estimate behaves, depends value parameter.","code":""},{"path":"intro.html","id":"what-really-is-an-alternative-hypothesis","chapter":"1 Introduction","heading":"What really is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEYIn video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?Since procedure potential wrong, search one makes bad errors infrequently. two types errors can made.Type error: Rejecting \\(H_0\\) \\(H_0\\) actually true. Usually considered worst error possible, thus find procedure makes type error small probability. probability denoted \\(\\alpha\\).Type II error: rejecting \\(H_0\\) \\(H_a\\) actually true. small type II error secondary concern (controlling type error). probability type II error denoted \\(\\beta\\). \\(1-\\beta\\)\nknown power.reality, true reason choose test small value \\(\\alpha\\) value know calculate. \\(\\beta\\) power possible calculate \\(H_a\\) doesn’t tell us value parameter .way hypothesis test carried via p-value, essentially tells us unusual observed data comparison \\(H_0\\). estimates consistent expected \\(H_0\\) true, conclusion \\(H_0\\) false: explanation data strange. definition p-value little tricky.p-value: probability, \\(H_0\\) true, observing data contradictory \\(H_0\\) repeat experiment .p-value .01, means data showed something happens 1 time 100 \\(H_0\\) true. Considering particular set data observed, reasonable conclusion \\(H_0\\) must \ntrue. rule : reject \\(H_0\\) p-value \\(< \\alpha\\). resulting test type error probability \\(\\alpha\\), value get specify. \\(\\alpha\\) often set .05.","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3 Reflection Questions","text":"difference sample population?experimental design issues influence conclusions?type error, type II error, power?p-value (careful, p-value probability \\(H_0\\) true!!!)?regression line ?linear regression always appropriate strategy?properties good fitting line ?line appropriately interpreted?","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.4 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.4.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.4.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"tools-git-github","chapter":"1 Introduction","heading":"Tools: Git & GitHub","text":"must submit assignments via GitHubFollow Jenny Bryan’s advice get set-: http://happygitwithr.com/Class specific instructions https://m158-comp-stats.netlify.app/github.htmlAdmittedly, steep learning curve Git. However, among tools likely use future endeavors, spending little time focusing concepts now may pay big time future. Beyond practicing working http://happygitwithr.com/, may want read little bit Git behind scenes. reference: Learn git concepts, commands good accessible.","code":""},{"path":"intro.html","id":"tools-a-github-merge-conflict-demo","chapter":"1 Introduction","heading":"Tools: a GitHub merge conflict (demo)","text":"GitHub (web) edit README document Commit message describing ., RStudio also edit README document different change.\nCommit changes\nTry push \\(\\rightarrow\\) ’ll get error!\nTry pulling\nResolve merge conflict commit push\nCommit changesTry push \\(\\rightarrow\\) ’ll get error!Try pullingResolve merge conflict commit pushAs work teams run merge conflicts, learning resolve properly important.\nFigure 1.7: https://xkcd.com/1597/\n","code":""},{"path":"intro.html","id":"steps-for-weekly-homework","chapter":"1 Introduction","heading":"Steps for weekly homework","text":"get link new assignment (clicking link create new private repo)Use R (within R Studio)\nNew Project, version control, Git\nClone repo using SSH\nNew Project, version control, GitClone repo using SSHIf exists, rename Rmd file ma158-hw#-lname-fname.RmdDo assignment\ncommit push every problem\ncommit push every problemAll necessary files must folder (e.g., data)","code":""},{"path":"wrang.html","id":"wrang","chapter":"2 Data Wrangling","heading":"2 Data Wrangling","text":"data visualization, data wrangling fundamental part able accurately, reproducibly, efficiently work data. approach taken following chapter based philosophy tidy data takes many precepts database theory. done much work SQL, functionality approach tidy data feel familiar. adept data wrangling, effective data analysis.Information want, data ’ve got. (Kaplan 2015)Embrace ways get help!cheat sheets: https://www.rstudio.com/resources/cheatsheets/tidyverse vignettes: https://www.tidyverse.org/articles/2019/09/tidyr-1-0-0/pivoting: https://tidyr.tidyverse.org/articles/pivot.htmlgoogle need include R tidy tidyverse","code":""},{"path":"wrang.html","id":"datastruc","chapter":"2 Data Wrangling","heading":"2.1 Structure of Data","text":"plotting, analyses, model building, etc., ’s important data structured particular way. Hadley Wickham provides thorough discussion advice cleaning data Wickham (2014).Tidy Data: rows (cases/observational units) columns (variables). key every row case *every} column variable. exceptions.Creating tidy data trivial. work objects (often data tables), functions, arguments (often variables).Active Duty data tidy! cases? data tidy? might data look like tidy form? Suppose case “individual armed forces.” variables use capture information following table?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794Problem: totals different sheetsBetter R: longer format columns - grade, gender, status, service, count (case still total pay grade)Case individual (?): grade, gender, status, service (count row counting)","code":""},{"path":"wrang.html","id":"building-tidy-data","chapter":"2 Data Wrangling","heading":"2.1.1 Building Tidy Data","text":"Within R (really within type computing language, Python, SQL, Java, etc.), need understand build data using patterns language. things consider:object_name = function_name(arguments) way using function create new object.object_name = data_table %>% function_name(arguments) uses chaining syntax extension ideas functions. chaining, value left side %>% becomes first argument function right side.extended chaining. %>% never front line, always connecting one idea continuation idea next line.\n* R, functions take arguments round parentheses (opposed subsetting observations variables data objects happen square parentheses). Additionally, spot left %>% always data table.\n* pipe syntax read , %>%.","code":"object_name = data_table %>%\nfunction_name(arguments) %>% \nfunction_name(arguments)"},{"path":"wrang.html","id":"examples-of-chaining","chapter":"2 Data Wrangling","heading":"2.1.2 Examples of Chaining","text":"pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)","code":""},{"path":"wrang.html","id":"little-bunny-foo-foo","chapter":"2 Data Wrangling","heading":"2.1.2.1 Little Bunny Foo Foo","text":"Hadley Wickham, think tidy data.Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThe nursery rhyme created series steps output step saved object along way.Another approach concatenate functions one output.even worse, one line:Instead, code can written using pipe order function evaluated:babynames year, US Social Security Administration publishes list popular names given babies. 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys.babynames data table babynames package comes Social Security Administration’s listing names givens babies year, number babies sex given name. (names 5 babies published SSA.)","code":"foo_foo <- little_bunny()\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)bop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)bop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))foo_foo %>%\n   hop(through = forest) %>%\n       scoop(up = field_mice) %>%\n           bop(on = head)"},{"path":"wrang.html","id":"data-verbs-on-single-data-frames","chapter":"2 Data Wrangling","heading":"2.1.3 Data Verbs (on single data frames)","text":"Super important resource: RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). use R package dplyr much data wrangling. list verbs helpful wrangling many different types data. See Data Wrangling cheat sheet RStudio additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfsample_n() take random row(s)sample_n() take random row(s)head() grab first rowshead() grab first rowstail() grab last rowstail() grab last rowsfilter() removes unwanted casesfilter() removes unwanted casesarrange() reorders casesarrange() reorders casesselect() removes unwanted variables (rename() )select() removes unwanted variables (rename() )distinct() returns unique values tabledistinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)mutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nsummarize() collapses data frame single row. functions used within summarize() include:min(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]","code":""},{"path":"wrang.html","id":"r-examples-basic-verbs","chapter":"2 Data Wrangling","heading":"2.2 R examples, basic verbs","text":"","code":""},{"path":"wrang.html","id":"datasets","chapter":"2 Data Wrangling","heading":"2.2.1 Datasets","text":"starwars dplyr , although originally SWAPI, Star Wars API, http://swapi.co/.NHANES ?NHANES: NHANES survey data collected US National Center Health Statistics (NCHS) conducted series health nutrition surveys since early 1960’s. Since 1999 approximately 5,000 individuals ages interviewed homes every year complete health examination component survey. health examination conducted mobile examination center (MEC).babynames year, US Social Security Administration publishes list popular names given babies. 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma Olivia leading girls, Noah Liam boys. (names 5 babies published SSA.)","code":""},{"path":"wrang.html","id":"examples-of-chaining-1","chapter":"2 Data Wrangling","heading":"2.2.2 Examples of Chaining","text":"","code":"\nlibrary(babynames)\nbabynames %>% nrow()## [1] 1924665\nbabynames %>% names()## [1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\nbabynames %>% glimpse()## Rows: 1,924,665\n## Columns: 5\n## $ year <dbl> 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n## $ sex  <chr> \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n## $ name <chr> \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n## $ n    <int> 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n## $ prop <dbl> 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\nbabynames %>% head()## # A tibble: 6 × 5\n##    year sex   name          n   prop\n##   <dbl> <chr> <chr>     <int>  <dbl>\n## 1  1880 F     Mary       7065 0.0724\n## 2  1880 F     Anna       2604 0.0267\n## 3  1880 F     Emma       2003 0.0205\n## 4  1880 F     Elizabeth  1939 0.0199\n## 5  1880 F     Minnie     1746 0.0179\n## 6  1880 F     Margaret   1578 0.0162\nbabynames %>% tail()## # A tibble: 6 × 5\n##    year sex   name       n       prop\n##   <dbl> <chr> <chr>  <int>      <dbl>\n## 1  2017 M     Zyhier     5 0.00000255\n## 2  2017 M     Zykai      5 0.00000255\n## 3  2017 M     Zykeem     5 0.00000255\n## 4  2017 M     Zylin      5 0.00000255\n## 5  2017 M     Zylis      5 0.00000255\n## 6  2017 M     Zyrie      5 0.00000255\nbabynames %>% sample_n(size=5)## # A tibble: 5 × 5\n##    year sex   name         n      prop\n##   <dbl> <chr> <chr>    <int>     <dbl>\n## 1  1946 M     Owen       392 0.000238 \n## 2  1953 M     Abelardo    34 0.0000170\n## 3  1996 F     Nicki       50 0.0000261\n## 4  1914 F     Sofia       38 0.0000477\n## 5  1970 M     Lorenza     45 0.0000236\nbabynames %>% mosaic::favstats(n ~ sex, data = .)##   sex min Q1 median Q3   max     mean       sd       n missing\n## 1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n## 2   M   5  7     12 33 94756 223.4940 1932.338  786372       0"},{"path":"wrang.html","id":"data-verbs","chapter":"2 Data Wrangling","heading":"2.2.3 Data Verbs","text":"Taken dplyr tutorial: http://dplyr.tidyverse.org/","code":""},{"path":"wrang.html","id":"starwars","chapter":"2 Data Wrangling","heading":"2.2.3.1 Starwars","text":"","code":"\nlibrary(dplyr)\n\nstarwars %>% dim()## [1] 87 14\nstarwars %>% names()##  [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n##  [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n## [11] \"species\"    \"films\"      \"vehicles\"   \"starships\"\nstarwars %>% head()## # A tibble: 6 × 14\n##   name     height  mass hair_color  skin_color eye_color birth_year sex   gender\n##   <chr>     <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n## 1 Luke Sk…    172    77 blond       fair       blue            19   male  mascu…\n## 2 C-3PO       167    75 <NA>        gold       yellow         112   none  mascu…\n## 3 R2-D2        96    32 <NA>        white, bl… red             33   none  mascu…\n## 4 Darth V…    202   136 none        white      yellow          41.9 male  mascu…\n## 5 Leia Or…    150    49 brown       light      brown           19   fema… femin…\n## 6 Owen La…    178   120 brown, grey light      blue            52   male  mascu…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       8\n## 2 masculine  15 75     80 88.0 1358 106.14694 184.972677 49      17\nstarwars %>% \n  dplyr::filter(species == \"Droid\")## # A tibble: 6 × 14\n##   name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n##   <chr>   <int> <dbl> <chr>      <chr>       <chr>          <dbl> <chr> <chr>   \n## 1 C-3PO     167    75 <NA>       gold        yellow           112 none  masculi…\n## 2 R2-D2      96    32 <NA>       white, blue red               33 none  masculi…\n## 3 R5-D4      97    32 <NA>       white, red  red               NA none  masculi…\n## 4 IG-88     200   140 none       metal       red               15 none  masculi…\n## 5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n## 6 BB8        NA    NA none       none        black             NA none  masculi…\n## # … with 5 more variables: homeworld <chr>, species <chr>, films <list>,\n## #   vehicles <list>, starships <list>\nstarwars %>% \n  dplyr::filter(species != \"Droid\") %>%\n  mosaic::favstats(mass~gender, data = .)##      gender min Q1 median   Q3  max      mean         sd  n missing\n## 1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n## 2 masculine  15 77     80 88.0 1358 109.38222 192.397084 45      16\nstarwars %>% \n  dplyr::select(name, ends_with(\"color\"))## # A tibble: 87 × 4\n##    name               hair_color    skin_color  eye_color\n##    <chr>              <chr>         <chr>       <chr>    \n##  1 Luke Skywalker     blond         fair        blue     \n##  2 C-3PO              <NA>          gold        yellow   \n##  3 R2-D2              <NA>          white, blue red      \n##  4 Darth Vader        none          white       yellow   \n##  5 Leia Organa        brown         light       brown    \n##  6 Owen Lars          brown, grey   light       blue     \n##  7 Beru Whitesun lars brown         light       blue     \n##  8 R5-D4              <NA>          white, red  red      \n##  9 Biggs Darklighter  black         light       brown    \n## 10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n## # … with 77 more rows\nstarwars %>% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %>%\n  dplyr::select(name:mass, bmi)## # A tibble: 87 × 4\n##    name               height  mass   bmi\n##    <chr>               <int> <dbl> <dbl>\n##  1 Luke Skywalker        172    77  26.0\n##  2 C-3PO                 167    75  26.9\n##  3 R2-D2                  96    32  34.7\n##  4 Darth Vader           202   136  33.3\n##  5 Leia Organa           150    49  21.8\n##  6 Owen Lars             178   120  37.9\n##  7 Beru Whitesun lars    165    75  27.5\n##  8 R5-D4                  97    32  34.0\n##  9 Biggs Darklighter     183    84  25.1\n## 10 Obi-Wan Kenobi        182    77  23.2\n## # … with 77 more rows\nstarwars %>% \n  dplyr::arrange(desc(mass))## # A tibble: 87 × 14\n##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender\n##    <chr>    <int> <dbl> <chr>       <chr>      <chr>          <dbl> <chr> <chr> \n##  1 Jabba …    175  1358 <NA>        green-tan… orange         600   herm… mascu…\n##  2 Grievo…    216   159 none        brown, wh… green, y…       NA   male  mascu…\n##  3 IG-88      200   140 none        metal      red             15   none  mascu…\n##  4 Darth …    202   136 none        white      yellow          41.9 male  mascu…\n##  5 Tarfful    234   136 brown       brown      blue            NA   male  mascu…\n##  6 Owen L…    178   120 brown, grey light      blue            52   male  mascu…\n##  7 Bossk      190   113 none        green      red             53   male  mascu…\n##  8 Chewba…    228   112 brown       unknown    blue           200   male  mascu…\n##  9 Jek To…    180   110 brown       fair       blue            NA   male  mascu…\n## 10 Dexter…    198   102 none        brown      yellow          NA   male  mascu…\n## # … with 77 more rows, and 5 more variables: homeworld <chr>, species <chr>,\n## #   films <list>, vehicles <list>, starships <list>\nstarwars %>%\n  dplyr::group_by(species) %>%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %>%\n  dplyr::filter(num > 1)## # A tibble: 9 × 3\n##   species    num  mass\n##   <chr>    <int> <dbl>\n## 1 Droid        6  69.8\n## 2 Gungan       3  74  \n## 3 Human       35  82.8\n## 4 Kaminoan     2  88  \n## 5 Mirialan     2  53.1\n## 6 Twi'lek      2  55  \n## 7 Wookiee      2 124  \n## 8 Zabrak       2  80  \n## 9 <NA>         4  48"},{"path":"wrang.html","id":"nhanes","chapter":"2 Data Wrangling","heading":"2.2.3.2 NHANES","text":"","code":"\nrequire(NHANES)\nnames(NHANES)##  [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n##  [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n##  [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n## [13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n## [17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n## [21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n## [25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n## [29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n## [33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n## [37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n## [41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n## [45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n## [49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n## [53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n## [57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n## [61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n## [65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n## [69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n## [73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"\n# find the sleep variables\nNHANESsleep <- NHANES %>% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"\ndim(NHANESsleep)## [1] 10000    11\n# subset for college students\nNHANESsleep <- NHANESsleep %>% filter(Age %in% c(18:22)) %>% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)##  [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n##  [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n##  [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"\ndim(NHANESsleep)## [1] 655  12\nNHANESsleep %>% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) "},{"path":"wrang.html","id":"summarize-and-group_by","chapter":"2 Data Wrangling","heading":"2.2.4 summarize and group_by","text":"","code":"\n# number of people (cases) in NHANES\nNHANES %>% summarize(n())## # A tibble: 1 × 1\n##   `n()`\n##   <int>\n## 1 10000\n# total weight of all the people in NHANES (silly)\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(sum(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `sum(Weightlb, na.rm = TRUE)`\n##                           <dbl>\n## 1                      1549419.\n# mean weight of all the people in NHANES\nNHANES %>% mutate(Weightlb = Weight*2.2) %>% summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 1 × 1\n##   `mean(Weightlb, na.rm = TRUE)`\n##                            <dbl>\n## 1                           156.\n# repeat the above but for groups\n\n# males versus females\nNHANES %>% group_by(Gender) %>% summarize(n())## # A tibble: 2 × 2\n##   Gender `n()`\n##   <fct>  <int>\n## 1 female  5020\n## 2 male    4980\nNHANES %>% group_by(Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 2 × 2\n##   Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                           <dbl>\n## 1 female                           146.\n## 2 male                             167.\n# smokers and non-smokers\nNHANES %>% group_by(SmokeNow) %>% summarize(n())## # A tibble: 3 × 2\n##   SmokeNow `n()`\n##   <fct>    <int>\n## 1 No        1745\n## 2 Yes       1466\n## 3 <NA>      6789\nNHANES %>% group_by(SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                 186.\n## 2 Yes                                177.\n## 3 <NA>                               144.\n# people with and without diabetes\nNHANES %>% group_by(Diabetes) %>% summarize(n())## # A tibble: 3 × 2\n##   Diabetes `n()`\n##   <fct>    <int>\n## 1 No        9098\n## 2 Yes        760\n## 3 <NA>       142\nNHANES %>% group_by(Diabetes) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 3 × 2\n##   Diabetes `mean(Weightlb, na.rm = TRUE)`\n##   <fct>                             <dbl>\n## 1 No                                155. \n## 2 Yes                               202. \n## 3 <NA>                               21.6\n# break down the smokers versus non-smokers further, by sex\nNHANES %>% group_by(SmokeNow, Gender) %>% summarize(n())## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `n()`\n##   <fct>    <fct>  <int>\n## 1 No       female   764\n## 2 No       male     981\n## 3 Yes      female   638\n## 4 Yes      male     828\n## 5 <NA>     female  3618\n## 6 <NA>     male    3171\nNHANES %>% group_by(SmokeNow, Gender) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 6 × 3\n## # Groups:   SmokeNow [3]\n##   SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                           <dbl>\n## 1 No       female                           167.\n## 2 No       male                             201.\n## 3 Yes      female                           167.\n## 4 Yes      male                             185.\n## 5 <NA>     female                           138.\n## 6 <NA>     male                             151.\n# break down the people with diabetes further, by smoking\nNHANES %>% group_by(Diabetes, SmokeNow) %>% summarize(n())## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `n()`\n##   <fct>    <fct>    <int>\n## 1 No       No        1476\n## 2 No       Yes       1360\n## 3 No       <NA>      6262\n## 4 Yes      No         267\n## 5 Yes      Yes        106\n## 6 Yes      <NA>       387\n## 7 <NA>     No           2\n## 8 <NA>     <NA>       140\nNHANES %>% group_by(Diabetes, SmokeNow) %>% mutate(Weightlb = Weight*2.2) %>% \n  summarize(mean(Weightlb, na.rm=TRUE))## # A tibble: 8 × 3\n## # Groups:   Diabetes [3]\n##   Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n##   <fct>    <fct>                             <dbl>\n## 1 No       No                                183. \n## 2 No       Yes                               175. \n## 3 No       <NA>                              143. \n## 4 Yes      No                                204. \n## 5 Yes      Yes                               204. \n## 6 Yes      <NA>                              199. \n## 7 <NA>     No                                193. \n## 8 <NA>     <NA>                               19.1"},{"path":"wrang.html","id":"babynames","chapter":"2 Data Wrangling","heading":"2.2.5 babynames","text":"","code":"\nbabynames %>% group_by(sex) %>%\n  summarize(total=sum(n))## # A tibble: 2 × 2\n##   sex       total\n##   <chr>     <int>\n## 1 F     172371079\n## 2 M     175749438\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% head()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  1880 F            942\n## 2  1880 M           1058\n## 3  1881 F            938\n## 4  1881 M            997\n## 5  1882 F           1028\n## 6  1882 M           1099\nbabynames %>% group_by(year, sex) %>%\n  summarize(name_count = n_distinct(name)) %>% tail()## # A tibble: 6 × 3\n## # Groups:   year [3]\n##    year sex   name_count\n##   <dbl> <chr>      <int>\n## 1  2015 F          19074\n## 2  2015 M          14024\n## 3  2016 F          18817\n## 4  2016 M          14162\n## 5  2017 F          18309\n## 6  2017 M          14160\nbabysamp <- babynames %>% sample_n(size=50)\nbabysamp %>% select(year) %>% distinct() %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    1    1    1    1    1\nbabysamp %>% distinct() %>% select(year) %>% table()## .\n## 1896 1915 1922 1924 1926 1927 1928 1933 1940 1942 1946 1953 1955 1963 1966 1975 \n##    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n## 1980 1981 1982 1984 1985 1989 1990 1991 1992 1994 1996 1997 1999 2000 2002 2004 \n##    1    2    2    1    1    3    1    1    1    1    1    2    2    1    2    2 \n## 2006 2007 2009 2010 2014 2015 2017 \n##    1    1    3    1    2    1    1\nFrances <- babynames %>%\n  filter(name== \"Frances\") %>%\n  group_by(year, sex) %>%\n  summarize(yrTot = sum(n))\n\nFrances %>% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")"},{"path":"wrang.html","id":"highverb","chapter":"2 Data Wrangling","heading":"2.3 Higher Level Data Verbs","text":"complicated verbs may important sophisticated analyses. See RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.pivot_longer makes many columns 2 columns: pivot_longer(data, cols,  names_to = , value_to = )pivot_wider makes one column multiple columns: pivot_wider(data, names_from = , values_from = )left_join returns rows left table, rows matching keys right table.inner_join returns rows left table matching keys right table (.e., matching rows sets).full_join returns rows tables, join records left matching keys right table.Good practice: always specify argument joining data frames.ever need understand join right join , try find image lay function . found one quite good taken Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti","code":""},{"path":"wrang.html","id":"r-examples-higher-level-verbs","chapter":"2 Data Wrangling","heading":"2.4 R examples, higher level verbs","text":"tidyr 1.0.0 just released! new release means need update tidyr. know latest version following command works console (window ):familiar spread gather, acquaint pivot_longer() pivot_wider(). idea go wide dataframes long dataframes vice versa.","code":"?tidyr::pivot_longer"},{"path":"wrang.html","id":"pivot_longer","chapter":"2 Data Wrangling","heading":"2.4.1 pivot_longer()","text":"pivot military pay grade become longer?https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#\ngid=1811988794Does graph tell us right? done wrong…?","code":"\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\nglimpse(navy_gs)## Rows: 38\n## Columns: 17\n## $ ...1                 <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n## $ `Active Duty Family` <chr> NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n## $ ...3                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n## $ ...4                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n## $ ...5                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n## $ ...6                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n## $ ...7                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n## $ ...8                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n## $ ...9                 <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n## $ ...10                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n## $ ...11                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n## $ ...12                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n## $ ...13                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n## $ ...14                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n## $ ...15                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n## $ ...16                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n## $ ...17                <dbl> NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)## Rows: 30\n## Columns: 16\n## $ pay.grade       <chr> \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n## $ male.sing.wo    <dbl> 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n## $ female.sing.wo  <dbl> 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n## $ tot.sing.wo     <dbl> 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n## $ male.sing.w     <dbl> 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n## $ female.sing.w   <dbl> 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n## $ tot.sing.w      <dbl> 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n## $ male.joint.NA   <dbl> 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n## $ female.joint.NA <dbl> 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n## $ tot.joint.NA    <dbl> 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n## $ male.civ.NA     <dbl> 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n## $ female.civ.NA   <dbl> 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n## $ tot.civ.NA      <dbl> 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n## $ male.tot.NA     <dbl> 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n## $ female.tot.NA   <dbl> 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n## $ tot.tot.NA      <dbl> 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n# get rid of total columns & rows:\n\nnavyWR = navy %>% select(-contains(\"tot\")) %>%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %>%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %>%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %>% head()## # A tibble: 6 × 5\n##   pay.grade sex    marital kids  numPeople\n##   <chr>     <chr>  <chr>   <chr>     <dbl>\n## 1 E-1       male   sing    wo        31229\n## 2 E-1       female sing    wo         5717\n## 3 E-1       male   sing    w           563\n## 4 E-1       female sing    w           122\n## 5 E-1       male   joint   NA          139\n## 6 E-1       female joint   NA          141\nnavyWR %>% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))"},{"path":"wrang.html","id":"pivot_wider","chapter":"2 Data Wrangling","heading":"2.4.2 pivot_wider","text":"","code":"\nlibrary(babynames)\nbabynames %>% dplyr::select(-prop) %>%\n   tidyr::pivot_wider(names_from = sex, values_from = n) ## # A tibble: 1,756,284 × 4\n##     year name          F     M\n##    <dbl> <chr>     <int> <int>\n##  1  1880 Mary       7065    27\n##  2  1880 Anna       2604    12\n##  3  1880 Emma       2003    10\n##  4  1880 Elizabeth  1939     9\n##  5  1880 Minnie     1746     9\n##  6  1880 Margaret   1578    NA\n##  7  1880 Ida        1472     8\n##  8  1880 Alice      1414    NA\n##  9  1880 Bertha     1320    NA\n## 10  1880 Sarah      1288    NA\n## # … with 1,756,274 more rows\nbabynames %>% \n  select(-prop) %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(year), desc(M))## # A tibble: 168,381 × 4\n##     year name         F     M\n##    <dbl> <chr>    <int> <int>\n##  1  2017 Liam        36 18728\n##  2  2017 Noah       170 18326\n##  3  2017 William     18 14904\n##  4  2017 James       77 14232\n##  5  2017 Logan     1103 13974\n##  6  2017 Benjamin     8 13733\n##  7  2017 Mason       58 13502\n##  8  2017 Elijah      26 13268\n##  9  2017 Oliver      15 13141\n## 10  2017 Jacob       16 13106\n## # … with 168,371 more rows\nbabynames %>% \n  pivot_wider(names_from = sex, values_from = n) %>%\n  filter(!is.na(F) & !is.na(M)) %>%\n  arrange(desc(prop))## # A tibble: 12 × 5\n##     year name            prop     F     M\n##    <dbl> <chr>          <dbl> <int> <int>\n##  1  1986 Marquette 0.0000130     24    25\n##  2  1996 Dariel    0.0000115     22    23\n##  3  2014 Laramie   0.0000108     21    22\n##  4  1939 Earnie    0.00000882    10    10\n##  5  1939 Vertis    0.00000882    10    10\n##  6  1921 Vernis    0.00000703     9     8\n##  7  1939 Alvia     0.00000529     6     6\n##  8  1939 Eudell    0.00000529     6     6\n##  9  1939 Ladell    0.00000529     6     6\n## 10  1939 Lory      0.00000529     6     6\n## 11  1939 Maitland  0.00000529     6     6\n## 12  1939 Delaney   0.00000441     5     5"},{"path":"wrang.html","id":"join-use-join-to-merge-two-datasets","chapter":"2 Data Wrangling","heading":"2.4.3 join (use join to merge two datasets)","text":"","code":""},{"path":"wrang.html","id":"first-get-the-data-gapminder","chapter":"2 Data Wrangling","heading":"2.4.3.1 First get the data (GapMinder)","text":"following datasets come GapMinder. first represents country, year, female literacy rate. second represents country, year, GDP (fixed 2000 US$).","code":"\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %>% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %>%\n  filter(!is.na(litRateF))\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %>% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %>%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %>%\n  filter(!is.na(gdp))\nhead(litF)## # A tibble: 6 × 3\n##   country     year  litRateF\n##   <chr>       <chr>    <dbl>\n## 1 Afghanistan 1979      4.99\n## 2 Afghanistan 2011     13   \n## 3 Albania     2001     98.3 \n## 4 Albania     2008     94.7 \n## 5 Albania     2011     95.7 \n## 6 Algeria     1987     35.8\nhead(GDP)## # A tibble: 6 × 3\n##   country year    gdp\n##   <chr>   <chr> <dbl>\n## 1 Albania 1980  1061.\n## 2 Albania 1981  1100.\n## 3 Albania 1982  1111.\n## 4 Albania 1983  1101.\n## 5 Albania 1984  1065.\n## 6 Albania 1985  1060.\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)## [1] 571   4\nsum(is.na(litGDPleft$gdp))## [1] 66\nhead(litGDPleft)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902.\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)## [1] 7988    4\nsum(is.na(litGDPright$gdp))## [1] 0\nhead(litGDPright)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)## [1] 505   4\nsum(is.na(litGDPinner$gdp))## [1] 0\nhead(litGDPinner)## # A tibble: 6 × 4\n##   country year  litRateF   gdp\n##   <chr>   <chr>    <dbl> <dbl>\n## 1 Albania 2001      98.3 1282.\n## 2 Albania 2008      94.7 1804.\n## 3 Albania 2011      95.7 1966.\n## 4 Algeria 1987      35.8 1902.\n## 5 Algeria 2002      60.1 1872.\n## 6 Algeria 2006      63.9 2125.\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)## [1] 8054    4\nsum(is.na(litGDPfull$gdp))## [1] 66\nhead(litGDPfull)## # A tibble: 6 × 4\n##   country     year  litRateF   gdp\n##   <chr>       <chr>    <dbl> <dbl>\n## 1 Afghanistan 1979      4.99   NA \n## 2 Afghanistan 2011     13      NA \n## 3 Albania     2001     98.3  1282.\n## 4 Albania     2008     94.7  1804.\n## 5 Albania     2011     95.7  1966.\n## 6 Algeria     1987     35.8  1902."},{"path":"wrang.html","id":"lubridate","chapter":"2 Data Wrangling","heading":"2.4.4 lubridate","text":"lubridate another R package meant data wrangling (Grolemund Wickham 2011). particular, lubridate makes easy work days, times, dates. base idea start dates ymd (year month day) format transform information whatever want. linked table original paper provides many basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.Example https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html","code":""},{"path":"wrang.html","id":"if-anyone-drove-a-time-machine-they-would-crash","chapter":"2 Data Wrangling","heading":"2.4.4.1 If anyone drove a time machine, they would crash","text":"length months years change often arithmetic can unintuitive. Consider simple operation, January 31st + one month. answer :February 31st (doesn’t exist)March 4th (31 days January 31), orFebruary 28th (assuming leap year)basic property arithmetic + b - b = . solution 1 obeys mathematical property, invalid date. Wickham wants make lubridate consistent possible invoking following rule: adding subtracting month year creates invalid date, lubridate return NA.thought solution 2 3 useful, problem. can still get results clever arithmetic, using special %m+% %m-% operators. %m+% %m-% automatically roll dates back last day month, necessary.","code":""},{"path":"wrang.html","id":"r-examples-lubridate","chapter":"2 Data Wrangling","heading":"2.4.4.2 R examples, lubridate()","text":"","code":""},{"path":"wrang.html","id":"some-basics-in-lubridate","chapter":"2 Data Wrangling","heading":"Some basics in lubridate","text":"","code":"\nrequire(lubridate)\nrightnow <- now()\n\nday(rightnow)## [1] 11\nweek(rightnow)## [1] 2\nmonth(rightnow, label=FALSE)## [1] 1\nmonth(rightnow, label=TRUE)## [1] Jan\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nyear(rightnow)## [1] 2022\nminute(rightnow)## [1] 55\nhour(rightnow)## [1] 15\nyday(rightnow)## [1] 11\nmday(rightnow)## [1] 11\nwday(rightnow, label=FALSE)## [1] 3\nwday(rightnow, label=TRUE)## [1] Tue\n## Levels: Sun < Mon < Tue < Wed < Thu < Fri < Sat"},{"path":"wrang.html","id":"but-how-do-i-create-a-date-object","chapter":"2 Data Wrangling","heading":"But how do I create a date object?","text":"","code":"\njan31 <- ymd(\"2021-01-31\")\njan31 + months(0:11)##  [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n##  [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n## [11] NA           \"2021-12-31\"\nfloor_date(jan31, \"month\") + months(0:11) + days(31)##  [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n##  [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n## [11] \"2021-12-02\" \"2022-01-01\"\njan31 + months(0:11) + days(31)##  [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n##  [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n## [11] NA           \"2022-01-31\"\njan31 %m+% months(0:11)##  [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n##  [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n## [11] \"2021-11-30\" \"2021-12-31\""},{"path":"wrang.html","id":"nyc-flights","chapter":"2 Data Wrangling","heading":"NYC flights","text":"","code":"\nlibrary(nycflights13)\nnames(flights)##  [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n##  [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n##  [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n## [13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n## [17] \"hour\"           \"minute\"         \"time_hour\"\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)## # A tibble: 6 × 22\n##    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n## 1  2013     1     1      517            515         2      830            819\n## 2  2013     1     1      533            529         4      850            830\n## 3  2013     1     1      542            540         2      923            850\n## 4  2013     1     1      544            545        -1     1004           1022\n## 5  2013     1     1      554            600        -6      812            837\n## 6  2013     1     1      554            558        -4      740            728\n## # … with 14 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n## #   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n## #   hour <dbl>, minute <dbl>, time_hour <dttm>, ymdday <date>, weekdy <ord>,\n## #   whichweek <dbl>\nflightsWK <- flights %>% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %>%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %>% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %>%  \n   head()## # A tibble: 6 × 9\n##    year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n##   <int> <int> <int> <date>     <ord>      <dbl>    <int>    <int>    <dbl>\n## 1  2013     1     1 2013-01-01 Tue            1      517      830      227\n## 2  2013     1     1 2013-01-01 Tue            1      533      850      227\n## 3  2013     1     1 2013-01-01 Tue            1      542      923      160\n## 4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n## 5  2013     1     1 2013-01-01 Tue            1      554      812      116\n## 6  2013     1     1 2013-01-01 Tue            1      554      740      150"},{"path":"wrang.html","id":"purrr-for-functional-programming","chapter":"2 Data Wrangling","heading":"2.5 purrr for functional programming","text":"see R package purrr greater detail go, now, let’s get hint works.going focus map family functions just get us started. Lots good purrr functions like pluck() accumulate().Much taken tutorial Rebecca Barter.map functions named output produce. example:map(.x, .f) main mapping function returns listmap(.x, .f) main mapping function returns listmap_df(.x, .f) returns data framemap_df(.x, .f) returns data framemap_dbl(.x, .f) returns numeric (double) vectormap_dbl(.x, .f) returns numeric (double) vectormap_chr(.x, .f) returns character vectormap_chr(.x, .f) returns character vectormap_lgl(.x, .f) returns logical vectormap_lgl(.x, .f) returns logical vectorNote first argument always data object second object always function want iteratively apply element input object.input map function always either vector (like column), list (can non-rectangular), dataframe (like rectangle).list way hold things might different shape:Consider following function:can map() add_ten() function across vector. Note output list (default).use different type input? default behavior still return list!want different type output? use different map() function, map_df(), example.Shorthand lets us get away pre-defining function (useful). Use tilde ~ indicate function:Mostly, tilde used functions already know:","code":"\na_list <- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)## $a_number\n## [1] 5\n## \n## $a_vector\n## [1] \"a\" \"b\" \"c\"\n## \n## $a_dataframe\n##   a b             c\n## 1 1 q       bananas\n## 2 2 b           are\n## 3 3 z so very great\nadd_ten <- function(x) {\n  return(x + 10)\n  }\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)## [[1]]\n## [1] 12\n## \n## [[2]]\n## [1] 15\n## \n## [[3]]\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map(add_ten)## $a\n## [1] 12\n## \n## $b\n## [1] 15\n## \n## $c\n## [1] 20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(add_ten)## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\ndata.frame(a = 2, b = 5, c = 10) %>%\n  map_df(~{.x + 10})## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    12    15    20\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %>%\n  split(.$species) %>%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map_df(tidy)  # map(tidy)## # A tibble: 6 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n## 2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n## 3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n## 4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n## 5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\npenguins %>%\n  group_by(species) %>%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %>%\n  map(tidy)  # map_df(tidy)## [[1]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -2536.     965.       -2.63 0.00948      \n## 2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n## \n## [[2]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic       p.value\n##   <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)        -3037.     997.       -3.05 0.00333      \n## 2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n## \n## [[3]]\n## # A tibble: 2 × 5\n##   term              estimate std.error statistic  p.value\n##   <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n## 2 flipper_length_mm     54.6      5.03     10.9  1.33e-19"},{"path":"wrang.html","id":"reprex","chapter":"2 Data Wrangling","heading":"2.6 reprex()","text":"Help help youIn order create reproducible example …Step 1. Copy code onto clipboardStep 2. Type reprex() ConsoleStep 3. Look Viewer right. Copy Viewer output GitHub, Piazza, email, stackexchange, etc.places learn reprex includeA blog : https://teachdatascience.com/reprex/reprex vignette: https://reprex.tidyverse.org/index.htmlreprex dos donts: https://reprex.tidyverse.org/articles/reprex-dos--donts.htmlJenny Bryan webinar reprex: “Help help . Creating reproducible examples” https://resources.rstudio.com/webinars/help--help--creating-reproducible-examples-jenny-bryanSome advice: https://stackoverflow.com/help/minimal-reproducible-example","code":""},{"path":"wrang.html","id":"reprex-demo","chapter":"2 Data Wrangling","heading":"2.6.0.1 reprex demo","text":"multiple lines code:","code":"reprex(\n  jan31 + months(0:11) + days(31)\n)reprex({\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})reprex({\n  library(lubridate)\n  jan31 <- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})"},{"path":"viz.html","id":"viz","chapter":"3 Visualization","heading":"3 Visualization","text":"Data visualization integral understanding data models. Computational statistics data science sometimes focus models resulting predictions models. doubt structure format data key whether model appropriate good. good data analyst always spend lot time effort exploratory data analysis, much includes making many visualizations data possible.Depending introductory () statistics classes ’ve , instructor may focused less visualizations class. () may even said something like making visualizations incredibly important entire data analysis process. even buy perspective, don’t see good graphics analyses? Andrew Gelman (Gelman 2011) responds stating, “Good statistical graphics hard , much harder running regressions making tables.” goal create graphics visualizations convey statistical information.Nolan (Nolan Perrett 2016) describes three important ways graphics can used convey statistical information. “guiding principles” used way evaluating others’ figures well metric creating visualizations help statistical analysis.Make data stand outThe important idea find anything unusual data. patterns? Outliers? bounds variables? axes scaled? transformations warranted?Facilitate comparisonThe second item allows us consider research questions hand. important variables? emphasize ? variables plotted together? Can super-imposed? color, plotting character, size plot character help bring important relationships? aware plotting issues color blindness. http://colorbrewer2.org/Add informationPlots also add context comparison. Figure legends, axes scales, reference markers (e.g., line \\(y=x\\)) go long way toward helping reader understand message. Captions self-contained (assume user also read text) descriptive; summarize content figure conclusion related message want convey.Randy Pruim asks following question decide whether plot good: plot make comparisons interested …easily? andaccurately?Consider adding alt text allow screen readers parse image. DataViz Society/Nightingale way Amy Cesal article writing good alt text plots/graphs, Writing Alt Text Data Visualization.","code":""},{"path":"viz.html","id":"thoughts","chapter":"3 Visualization","heading":"3.1 Thoughts on Plotting","text":"","code":""},{"path":"viz.html","id":"advice","chapter":"3 Visualization","heading":"3.1.1 Advice","text":"Basic plotting\nAvoid graph elements interfere data\nUse visually prominent symbols\nAvoid -plotting (One way avoid plotting: Jitter values)\nDifferent values data may obscure \nInclude nearly data\nFill data region\nAvoid graph elements interfere dataUse visually prominent symbolsAvoid -plotting (One way avoid plotting: Jitter values)Different values data may obscure otherInclude nearly dataFill data regionEliminate superfluous material\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background images\nExtra tick marks grid lines\nUnnecessary text arrows\nDecimal places beyond measurement error level difference\nChart junk & stuff adds meaning, e.g. butterflies top barplots, background imagesExtra tick marks grid linesUnnecessary text arrowsDecimal places beyond measurement error level differenceFacilitate Comparisons\nPut juxtaposed plots scale\nMake easy distinguish elements superposed plots (e.g. color)\nEmphasizes important difference\nComparison: volume, area, height (careful, volume can seem bigger mean )\nPut juxtaposed plots scaleMake easy distinguish elements superposed plots (e.g. color)Emphasizes important differenceComparison: volume, area, height (careful, volume can seem bigger mean )Choosing Scale (n.b., principles may go counter one another, use judgment.)\nKeep scales x y axes plots facilitate comparison\nZoom focus region contains bulk data\nKeep scale throughout plot (.e. don’t change mid-axis)\nOrigin need scale\nChoose scale improves resolution\nAvoid jiggling baseline\nKeep scales x y axes plots facilitate comparisonZoom focus region contains bulk dataKeep scale throughout plot (.e. don’t change mid-axis)Origin need scaleChoose scale improves resolutionAvoid jiggling baselineHow make plot information rich\nDescribe see caption\nAdd context reference markers (lines points) including text\nAdd legends labels\nUse color plotting symbols add information\nPlot thing different ways/scales\nReduce clutter\nDescribe see captionAdd context reference markers (lines points) including textAdd legends labelsUse color plotting symbols add informationPlot thing different ways/scalesReduce clutterCaptions \ncomprehensive\nSelf-contained\nDescribe graphed\nDraw attention important features\nDescribe conclusions drawn graph\ncomprehensiveSelf-containedDescribe graphedDraw attention important featuresDescribe conclusions drawn graphGood Plot Making Practice\nPut major conclusions graphical form\nProvide reference information\nProof read clarity consistency\nGraphing iterative process\nMultiplicity OK, .e. two plots variable may provide different messages\nMake plots data rich\nPut major conclusions graphical formProvide reference informationProof read clarity consistencyGraphing iterative processMultiplicity OK, .e. two plots variable may provide different messagesMake plots data richCreating statistical graphic iterative process discovery fine tuning. try model process creating visualizations course dedicating class time iterative creation plot. begin either plot screams correction, transform step--step, always thinking goal graph data rich presents clear vision important features data.","code":""},{"path":"viz.html","id":"fonts-matter","chapter":"3 Visualization","heading":"3.1.1.1 Fonts Matter","text":"RStudio::conf 2020, Glamour Graphics, Chase makes important points making good graphics matters. talk might summarized plot : fonts matter.","code":""},{"path":"viz.html","id":"deconstruct","chapter":"3 Visualization","heading":"3.2 Deconstructing a graph","text":"","code":""},{"path":"viz.html","id":"gg","chapter":"3 Visualization","heading":"3.2.1 The Grammar of Graphics (gg)","text":"Yau (2013) Wickham (2014) come taxonomy grammar thinking parts figure just like conceptualize parts body parts sentence.One great way thinking new process: longer necessary talk name graph (e.g., boxplot). Instead now think glyphs (geoms), can put whatever want plot. Note also transition leads passive consumer (need make plot XXX everyone else , just plug data) active participant (want data say? can put information onto graphic?)important questions can ask respect creating figures :want R ? (goal?)R need know?Yau (2013) gives us nine visual cues, Wickham (2014) translates language using ggplot2. (items Baumer, Kaplan, Horton (2021), chapter 2.)Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Visual Cues: aspects figure focus.Position (numerical) relation things?Length (numerical) big (one dimension)?Angle (numerical) wide? parallel something else?Direction (numerical) slope? time series, going ?Shape (categorical) belonging group?Area (numerical) big (two dimensions)? Beware improper scaling!Volume (numerical) big (three dimensions)? Beware improper scaling!Shade (either) extent? severely?Color (either) extent? severely? Beware red/green color blindness.Coordinate System: rectangular, polar, geographic, etc.Coordinate System: rectangular, polar, geographic, etc.Scale: numeric (linear? logarithmic?), categorical (ordered?), timeScale: numeric (linear? logarithmic?), categorical (ordered?), timeContext: comparison (think back ideas Tufte)Context: comparison (think back ideas Tufte)","code":""},{"path":"viz.html","id":"order-matters","chapter":"3 Visualization","heading":"Order Matters","text":"","code":""},{"path":"viz.html","id":"cues-together","chapter":"3 Visualization","heading":"Cues Together","text":"","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-1","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"what-are-the-visual-cues-on-the-plot-2","chapter":"3 Visualization","heading":"What are the visual cues on the plot?","text":"position?length?shape?area/volume?shade/color?coordinate System?scale?","code":""},{"path":"viz.html","id":"the-grammar-of-graphics-in-ggplot2","chapter":"3 Visualization","heading":"3.2.1.1 The grammar of graphics in ggplot2","text":"geom: geometric “shape” used display databar, point, line, ribbon, text, etc.aesthetic: attribute controlling geom displayed respect variablesx position, y position, color, fill, shape, size, etc.scale: adjust information aesthetic map onto plotparticular assignment colors, shapes, sizes, etc.; making axes continuous constrained particular range values.guide: helps user convert visual data back raw data (legends, axes)stat: transformation applied data geom gets itexample: histograms work binned data","code":""},{"path":"viz.html","id":"ggplot2","chapter":"3 Visualization","heading":"3.2.2 ggplot2","text":"ggplot2, aesthetic refers mapping variable information conveys plot. information plotting visualizing information given chapter 2 (Data visualization) Baumer, Kaplan, Horton (2021). Much data presentation represents births 1978 US: date, day year, number births.","code":""},{"path":"viz.html","id":"goals","chapter":"3 Visualization","heading":"Goals","text":"try dogive tour ggplot2give tour ggplot2explain think plots ggplot2 wayexplain think plots ggplot2 wayprepare/encourage learn laterprepare/encourage learn laterWhat can’t one sessionshow every bell whistleshow every bell whistlemake expert using ggplot2make expert using ggplot2","code":""},{"path":"viz.html","id":"getting-help","chapter":"3 Visualization","heading":"Getting help","text":"One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/One best ways get started ggplot google want word ggplot. look images come . often , associated code . also ggplot galleries images, one : https://plot.ly/ggplot2/ggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfLook end presentation. help options .Look end presentation. help options .","code":"\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)##         date births wday year month day_of_year day_of_month day_of_week\n## 1 1978-01-01   7701  Sun 1978     1           1            1           1\n## 2 1978-01-02   7527  Mon 1978     1           2            2           2\n## 3 1978-01-03   8825  Tue 1978     1           3            3           3"},{"path":"viz.html","id":"how-can-we-make-the-plot","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Two Questions:want R ? (goal?)want R ? (goal?)R need know?\ndata source: Births78\naesthetics:\ndate -> x\nbirths -> y\npoints (!)\n\nR need know?data source: Births78data source: Births78aesthetics:\ndate -> x\nbirths -> y\npoints (!)\naesthetics:date -> xbirths -> ypoints (!)Goal: scatterplot = plot points\nggplot() + geom_point()\nGoal: scatterplot = plot pointsggplot() + geom_point()R need know?\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\nR need know?data source: data = Births78data source: data = Births78aesthetics: aes(x = date, y = births)aesthetics: aes(x = date, y = births)","code":""},{"path":"viz.html","id":"how-can-we-make-the-plot-1","chapter":"3 Visualization","heading":"How can we make the plot?","text":"changed?new aesthetic: mapping color day week","code":""},{"path":"viz.html","id":"adding-day-of-week-to-the-data-set","chapter":"3 Visualization","heading":"Adding day of week to the data set","text":"wday() function lubridate package computes day week date.","code":"\nBirths78 <-  \n  Births78 %>% \n  mutate(wday = lubridate::wday(date, label=TRUE))\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-2","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now use lines instead dots","code":"\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-3","chapter":"3 Visualization","heading":"How can we make the plot?","text":"Now two layers, one points one \nlinesThe layers placed one top : points\nlines .layers placed one top : points\nlines .data aes specified ggplot() affect geomsdata aes specified ggplot() affect geoms","code":"\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"alternative-syntax","chapter":"3 Visualization","heading":"Alternative Syntax","text":"","code":"\nBirths78 %>% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"what-does-adding-the-color-argument-do","chapter":"3 Visualization","heading":"What does adding the color argument do?","text":"variable, mapped color aesthetic new variable one value (“navy”). dots get set color, ’s navy.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping","chapter":"3 Visualization","heading":"Setting vs. Mapping","text":"want set color navy dots, outside aesthetic, without dataset variable:Note color = \"navy\" now outside aesthetics list. ’s ggplot2 distinguishes mapping setting.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"how-can-we-make-the-plot-4","chapter":"3 Visualization","heading":"How can we make the plot?","text":"ggplot() establishes default data aesthetics geoms, geom may change defaults.ggplot() establishes default data aesthetics geoms, geom may change defaults.good practice: put ggplot() things affect () layers; rest geom_blah()good practice: put ggplot() things affect () layers; rest geom_blah()","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"setting-vs.-mapping-again","chapter":"3 Visualization","heading":"Setting vs. Mapping (again)","text":"Information gets passed plot via:map variable information inside aes (aesthetic) commandmap variable information inside aes (aesthetic) commandset non-variable information outside aes (aesthetic) commandset non-variable information outside aes (aesthetic) command","code":""},{"path":"viz.html","id":"other-geoms","chapter":"3 Visualization","heading":"Other geoms","text":"help pages tell aesthetics, default stats, etc.","code":"\napropos(\"^geom_\") [1] \"geom_abline\"                  \"geom_area\"                   \n [3] \"geom_ash\"                     \"geom_bar\"                    \n [5] \"geom_barh\"                    \"geom_bin_2d\"                 \n [7] \"geom_bin2d\"                   \"geom_blank\"                  \n [9] \"geom_boxplot\"                 \"geom_boxploth\"               \n[11] \"geom_col\"                     \"geom_colh\"                   \n[13] \"geom_contour\"                 \"geom_contour_filled\"         \n[15] \"geom_count\"                   \"geom_crossbar\"               \n[17] \"geom_crossbarh\"               \"geom_curve\"                  \n[19] \"geom_density\"                 \"geom_density_2d\"             \n[21] \"geom_density_2d_filled\"       \"geom_density_line\"           \n[23] \"geom_density_ridges\"          \"geom_density_ridges_gradient\"\n[25] \"geom_density_ridges2\"         \"geom_density2d\"              \n[27] \"geom_density2d_filled\"        \"geom_dotplot\"                \n[29] \"geom_errorbar\"                \"geom_errorbarh\"              \n[31] \"geom_errorbarh\"               \"geom_freqpoly\"               \n[33] \"geom_function\"                \"geom_hex\"                    \n[35] \"geom_histogram\"               \"geom_histogramh\"             \n[37] \"geom_hline\"                   \"geom_jitter\"                 \n[39] \"geom_label\"                   \"geom_line\"                   \n[41] \"geom_linerange\"               \"geom_linerangeh\"             \n[43] \"geom_lm\"                      \"geom_map\"                    \n[45] \"geom_path\"                    \"geom_point\"                  \n[47] \"geom_pointrange\"              \"geom_pointrangeh\"            \n[49] \"geom_polygon\"                 \"geom_qq\"                     \n[51] \"geom_qq_line\"                 \"geom_quantile\"               \n[53] \"geom_raster\"                  \"geom_rect\"                   \n[55] \"geom_ribbon\"                  \"geom_ridgeline\"              \n[57] \"geom_ridgeline_gradient\"      \"geom_rug\"                    \n[59] \"geom_segment\"                 \"geom_sf\"                     \n[61] \"geom_sf_label\"                \"geom_sf_text\"                \n[63] \"geom_sina\"                    \"geom_smooth\"                 \n[65] \"geom_spline\"                  \"geom_spoke\"                  \n[67] \"geom_step\"                    \"geom_text\"                   \n[69] \"geom_tile\"                    \"geom_violin\"                 \n[71] \"geom_violinh\"                 \"geom_vline\"                  \n[73] \"geom_vridgeline\"             \n?geom_area             # for example"},{"path":"viz.html","id":"lets-try-geom_area","chapter":"3 Visualization","heading":"Let’s try geom_area","text":"Using area produce good plotover plotting hiding much dataextending y-axis 0 may may desirable.","code":"\nBirths78 %>%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")"},{"path":"viz.html","id":"side-note-what-makes-a-plot-good","chapter":"3 Visualization","heading":"Side note: what makes a plot good?","text":"(?) graphics intended help us make comparisonsHow something change time?treatments matter? much?men women respond way?Key plot metric: plot make comparisons interested ineasily, andaccurately?","code":""},{"path":"viz.html","id":"time-for-some-different-data","chapter":"3 Visualization","heading":"Time for some different data","text":"HELPrct: Health Evaluation Linkage Primary care randomized clinical trialSubjects admitted treatment addiction one three substances.","code":"\nhead(HELPrct)##   age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n## 1  37            1    yes   49  3        177      225        0  NA      0\n## 2  37            1    yes   30 22          2       NA        0  NA      0\n## 3  26            1    yes   39  0          3      365       20  NA      0\n## 4  39            1    yes   15  2        189      343        0   1      1\n## 5  32            1    yes   39 12          2       57        0   1      0\n## 6  47            1    yes    6  1         31      365        0  NA      1\n##      sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n## 1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n## 2   male yes homeless 56 62  2     43         NA <NA> 26.670307 36.03694      1\n## 3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n## 4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n## 5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n## 6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n##   racegrp satreat sexrisk substance treat avg_drinks max_drinks\n## 1   black      no       4   cocaine   yes         13         26\n## 2   white      no       7   alcohol   yes         56         62\n## 3   black      no       2    heroin    no          0          0\n## 4   white     yes       4    heroin    no          5          5\n## 5   black      no       6   cocaine    no         10         13\n## 6   black      no       5   cocaine   yes          4          4\n##   hospitalizations\n## 1                3\n## 2               22\n## 3                0\n## 4                2\n## 5               12\n## 6                1"},{"path":"viz.html","id":"who-are-the-people-in-the-study","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"Hmm. ’s y?\nstat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.\nHmm. ’s y?stat_bin() applied data \ngeom_bar() gets thing. Binning creates \ny values.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-1","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-2","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nlibrary(scales)\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"who-are-the-people-in-the-study-3","chapter":"3 Visualization","heading":"Who are the people in the study?","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study","chapter":"3 Visualization","heading":"How old are people in the HELP study?","text":"Notice messagesstat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.stat_bin: Histograms mapping raw data binned data.stat_bin() performs data transformation.binwidth: default binwidth selected, really choose .binwidth: default binwidth selected, really choose .","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`."},{"path":"viz.html","id":"setting-the-binwidth-manually","chapter":"3 Visualization","heading":"Setting the binwidth manually","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-old-are-people-in-the-help-study-other-geoms","chapter":"3 Visualization","heading":"How old are people in the HELP study? – Other geoms","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every geom comes default statfor simple cases, stat stat_identity() nothingwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"selecting-stat-and-geom-manually-1","chapter":"3 Visualization","heading":"Selecting stat and geom manually","text":"Every stat comes default geom, every geom default statwe can specify stats instead geom, preferwe can mix match geoms stats however like","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"more-combinations","chapter":"3 Visualization","heading":"More combinations","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \nHELPrct %>% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"how-much-do-they-drink-i1","chapter":"3 Visualization","heading":"How much do they drink? (i1)","text":"","code":"\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"covariates-adding-in-more-variables","chapter":"3 Visualization","heading":"Covariates: Adding in more variables","text":"Using color linetype:Using color facets","code":"\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\nHELPrct %>% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"boxplots","chapter":"3 Visualization","heading":"Boxplots","text":"Boxplots use stat_quantile() computes five-number summary (roughly five quartiles data) uses define “box” “whiskers.”quantitative variable must y, must additional x variable.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"horizontal-boxplots","chapter":"3 Visualization","heading":"Horizontal boxplots","text":"Horizontal boxplots obtained flipping coordinate system:coord_flip() may used plots well reverse roles\nx y plot.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"axes-scaling-with-boxplots","chapter":"3 Visualization","heading":"Axes scaling with boxplots","text":"can scale continuous axis","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"give-me-some-space","chapter":"3 Visualization","heading":"Give me some space","text":"’ve triggered new feature: dodge (dodging things left/right). can control much set dodge manually.","code":"\nHELPrct %>% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"issues-with-bigger-data","chapter":"3 Visualization","heading":"Issues with bigger data","text":"Although can see generally positive association (expect), plotting may hiding information.","code":"\nrequire(NHANES)\ndim(NHANES)## [1] 10000    76\nNHANES %>%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"using-alpha-opacity","chapter":"3 Visualization","heading":"Using alpha (opacity)","text":"One way deal plotting set opacity low.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"geom_density2d","chapter":"3 Visualization","heading":"geom_density2d","text":"Alternatively (simultaneously) might prefer different geom altogether.","code":"\nNHANES %>% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")"},{"path":"viz.html","id":"multiple-layers","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"multiple-layers-1","chapter":"3 Visualization","heading":"Multiple layers","text":"","code":"\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"things-i-havent-mentioned-much","chapter":"3 Visualization","heading":"Things I haven’t mentioned (much)","text":"coords (coord_flip() good know )coords (coord_flip() good know )themes (customizing appearance)themes (customizing appearance)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)position (position_dodge(), position_jitterdodge(), position_stack(), etc.)transforming axestransforming axes","code":"\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"a-little-bit-of-everything","chapter":"3 Visualization","heading":"A little bit of everything","text":"","code":"\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")"},{"path":"viz.html","id":"want-to-learn-more","chapter":"3 Visualization","heading":"Want to learn more?","text":"docs.ggplot2.org/docs.ggplot2.org/Winston Chang’s: R Graphics CookbookWinston Chang’s: R Graphics Cookbook","code":""},{"path":"viz.html","id":"what-else-can-we-do","chapter":"3 Visualization","heading":"What else can we do?","text":"shinyinteractive graphics / modelinginteractive graphics / modelinghttps://shiny.rstudio.com/https://shiny.rstudio.com/plotlyPlotly R package creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. plotly R library contains ggplotly function , convert ggplot2 figures Plotly object. Furthermore, option manipulating Plotly object style function.https://plot.ly/ggplot2/getting-started/Dynamic documentscombination RMarkdown, ggvis, shiny","code":""},{"path":"slr.html","id":"slr","chapter":"4 Simple Linear Regression","heading":"4 Simple Linear Regression","text":"","code":""},{"path":"slr.html","id":"a-linear-model","chapter":"4 Simple Linear Regression","heading":"4.1 A Linear Model","text":"","code":""},{"path":"slr.html","id":"normal-errors-model","chapter":"4 Simple Linear Regression","heading":"4.2 Normal Errors Model","text":"","code":""},{"path":"slr.html","id":"inference-on-beta_1","chapter":"4 Simple Linear Regression","heading":"4.3 Inference on \\(\\beta_1\\)","text":"","code":""},{"path":"slr.html","id":"estimating-a-response","chapter":"4 Simple Linear Regression","heading":"4.4 Estimating a response","text":"","code":""},{"path":"slr.html","id":"anova-approach-to-regression","chapter":"4 Simple Linear Regression","heading":"4.5 ANOVA approach to regression","text":"","code":""},{"path":"slr.html","id":"descriptive-measures-of-linear-association","chapter":"4 Simple Linear Regression","heading":"4.6 Descriptive Measures of Linear Association","text":"","code":""},{"path":"slr.html","id":"reflection-questions-1","chapter":"4 Simple Linear Regression","heading":"4.7 Reflection Questions","text":"","code":""},{"path":"slr.html","id":"section","chapter":"4 Simple Linear Regression","heading":"4.8 ","text":"","code":""},{"path":"diagnostics.html","id":"diagnostics","chapter":"5 Diagnostic Measures","heading":"5 Diagnostic Measures","text":"","code":""},{"path":"diagnostics.html","id":"model-conditions","chapter":"5 Diagnostic Measures","heading":"5.1 Model Conditions","text":"","code":""},{"path":"diagnostics.html","id":"residuals","chapter":"5 Diagnostic Measures","heading":"5.2 Residuals","text":"","code":""},{"path":"diagnostics.html","id":"transformations","chapter":"5 Diagnostic Measures","heading":"5.3 Transformations","text":"","code":""},{"path":"diagnostics.html","id":"reflection-questions-2","chapter":"5 Diagnostic Measures","heading":"5.4 Reflection Questions","text":"","code":""},{"path":"simult.html","id":"simult","chapter":"6 Simultaneous Inference","heading":"6 Simultaneous Inference","text":"","code":""},{"path":"simult.html","id":"joint-estimation-of-beta_0-and-beta_1","chapter":"6 Simultaneous Inference","heading":"6.1 Joint Estimation of \\(\\beta_0\\) and \\(\\beta_1\\)","text":"","code":""},{"path":"simult.html","id":"confidence-interval-for-mean-response","chapter":"6 Simultaneous Inference","heading":"6.2 Confidence Interval for Mean Response","text":"","code":""},{"path":"simult.html","id":"prediction-interval-for-new-observation","chapter":"6 Simultaneous Inference","heading":"6.3 Prediction Interval for New Observation","text":"","code":""},{"path":"simult.html","id":"reflection-questions-3","chapter":"6 Simultaneous Inference","heading":"6.4 Reflection Questions","text":"simultaneous CIs worry us (error perspective)?difference Bonferroni, Working-Hotelling, Scheff{' e} adjustments?Bonferroni intervals larger two? smaller?say Bonferroni procedure general multiple comparisons procedures?mean one adjustments “conservative?”\\(g\\) large, Working-Hotelling procedure preferred Bonferroni?","code":""},{"path":"simult.html","id":"more","chapter":"6 Simultaneous Inference","heading":"6.5 More","text":"Note sections 4.6 (Inverse Predictions) 4.7 (Choice \\(x\\) values) provide additional information good model building. sections worth reading .","code":""},{"path":"permschp.html","id":"permschp","chapter":"7 Permutation Tests","heading":"7 Permutation Tests","text":"","code":""},{"path":"permschp.html","id":"motivation","chapter":"7 Permutation Tests","heading":"Motivation:","text":"Great video /computational statistical methods can extremely useful. ’s beer mosquitoes! John Rauser Pintrest gives keynote address Strata + Hadoop World Conference October 16, 2014. David Smith, Revolution Analytics blog, October 17, 2014. http://blog.revolutionanalytics.com/2014/10/statistics-doesnt-----hard.htmlA complicated scenario tools applied. point understand gerrymandering. https://www.youtube.com/watch?v=gRCZR_BbjTo&t=125sThe big lesson , IMO, many statistical problems can seem complex, can actually get lot insight recognizing data just one possible instance random process. hypothesis process , can simulate , get intuitive sense surprising data . R excellent tools simulating data, couple hours spent writing code simulate data can often give insights valuable formal data analysis come. (David Smith)Rauser says order follow statistical argument uses simulation, need three things:Ability follow simple logical argument.Random number generation.Iteration","code":""},{"path":"permschp.html","id":"algs","chapter":"7 Permutation Tests","heading":"7.1 Inference Algorithms","text":"","code":""},{"path":"permschp.html","id":"hypothesis-test-algorithm","chapter":"7 Permutation Tests","heading":"7.1.1 Hypothesis Test Algorithm","text":"working nitty gritty details, recall structure hypothesis testing. Consider applet Simulating ANOVA Tables (Chance & Rossman) http://www.rossmanchance.com/applets/AnovaSim.htmlChoose statistic measures effect looking . example, ANOVA F statistic :\\[\\begin{align}\nF &= \\frac{\\text{-group variability}}{\\text{within-group variability}}\\\\\n&= \\frac{\\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{\\cdot})^2/(N-K)}\n\\end{align}\\]Construct sampling distribution statistic effect present population (null sampling distribution). [sampling distributions t statistics F statistics based Central Limit Theorem derived Math 152.]Construct sampling distribution statistic effect present population (null sampling distribution). [sampling distributions t statistics F statistics based Central Limit Theorem derived Math 152.]Locate observed statistic null distribution. value main body distribution easily occur just chance. value tail rarely occur chance evidence something chance operating. [piece going happen permutation tests well analytic tests – point see observed data consistent null distribution.]Locate observed statistic null distribution. value main body distribution easily occur just chance. value tail rarely occur chance evidence something chance operating. [piece going happen permutation tests well analytic tests – point see observed data consistent null distribution.]p-value probability observed data extreme null hypothesis true. [definition analytic, permutation, randomization tests!]\nestimate p-value test significance, estimate sampling distribution test statistic null hypothesis true simulating manner consistent null hypothesis. Alternatively, analytic / mathematical formulas many common statistical hypothesis tests.p-value probability observed data extreme null hypothesis true. [definition analytic, permutation, randomization tests!]\nestimate p-value test significance, estimate sampling distribution test statistic null hypothesis true simulating manner consistent null hypothesis. Alternatively, analytic / mathematical formulas many common statistical hypothesis tests.","code":""},{"path":"permschp.html","id":"permutation-tests-algorithm","chapter":"7 Permutation Tests","heading":"7.1.2 Permutation Tests Algorithm","text":"evaluate p-value permutation test, estimate sampling distribution test statistic null hypothesis true resampling manner consistent null hypothesis (number resamples finite can large!).Choose test statisticChoose test statisticShuffle data (force null hypothesis true)Shuffle data (force null hypothesis true)Create null sampling distribution test statistic (\\(H_0\\))Create null sampling distribution test statistic (\\(H_0\\))Find observed test statistic null sampling distribution compute p-value (observed data extreme). p-value can one two-sided.Find observed test statistic null sampling distribution compute p-value (observed data extreme). p-value can one two-sided.","code":""},{"path":"permschp.html","id":"technical-conditions","chapter":"7 Permutation Tests","heading":"Technical Conditions","text":"Permutation tests fall broad class tests called “non-parametric” tests. label indicates distributional conditions required data (.e., condition data come normal binomial distribution). However, test “non-parametric” meant conditions data, simply distributional parametric conditions data. parameters heart almost parametric tests.permutation tests, basing test population parameters, don’t need make claims (.e., mean particular distribution).Permutation different treatments effect. [Note: exchangeability, population, etc.] null hypothesis true, labels assigning groups interchangeable respect probability distribution.\nNote choice statistic makes test sensitive kinds difference (e.g., difference mean) kinds (e.g., difference variance).\nNote choice statistic makes test sensitive kinds difference (e.g., difference mean) kinds (e.g., difference variance).Parametric example, different populations mean.IMPORTANT KEY IDEA point technical conditions parametric permutation tests create sampling distribution accurately reflects null sampling distribution statistic interest (statistic captures relevant research question information).","code":""},{"path":"permschp.html","id":"perms","chapter":"7 Permutation Tests","heading":"7.2 Permutation tests in practice","text":"test interpreted given different types sampling possibly used collect data?Random Sample concept p-value usually comes idea taking sample population comparing sampling distribution (many many random samples).Random Sample concept p-value usually comes idea taking sample population comparing sampling distribution (many many random samples).Random Experiment context randomized experiment, p-value represents observed data compared “happening chance.”\ninterpretation direct: small chance observed statistic take extreme value, result randomization cases: reject null treatment effect hypothesis. CAUSAL!\nRandom Experiment context randomized experiment, p-value represents observed data compared “happening chance.”interpretation direct: small chance observed statistic take extreme value, result randomization cases: reject null treatment effect hypothesis. CAUSAL!Observational Study context observational studies results less strong, reasonable conclude effect observed sample reflects effect present population.\nsample, consider difference (ratio) ask “difference large rarely occur chance particular sample constructed null setting?”\ndata come random sample, sample (results sample) probably consistent population [.e., can infer results back larger population].\nObservational Study context observational studies results less strong, reasonable conclude effect observed sample reflects effect present population.sample, consider difference (ratio) ask “difference large rarely occur chance particular sample constructed null setting?”data come random sample, sample (results sample) probably consistent population [.e., can infer results back larger population].","code":""},{"path":"permschp.html","id":"other-test-statistics","chapter":"7 Permutation Tests","heading":"7.2.0.1 Other Test Statistics","text":"example class used modification ANOVA F-statistic compare observed data permuted data test statistics. Depending data question, permuted test statistic can take variety forms.Depending data, hypotheses, original data collection structure (e.g., random sampling vs random allocation), choice statistic permutation test vary.","code":""},{"path":"permschp.html","id":"permutation-vs.-randomization-tests","chapter":"7 Permutation Tests","heading":"7.2.1 Permutation vs. Randomization Tests","text":"call randomization tests enumerate possible data permutations. permutation tests, hand, permute data \\(B\\) (\\(< <\\) ) times. [authors call permutation test applied randomized experiment randomization test, use term randomization indicate possible permutations considered.]Main difference: randomization tests consider every possible permutation labels, permutation tests take random sample permutations labels.can applied comparison situation (e.g., one sample t-tests).permute labels \\(H_0\\), example, \\(H_0: F(x) = G(x)\\).can used situations sampling distributions unknown (e.g., differences medians).can used situations sampling distributions based population distributions (e.g., ratio variances).Randomization tests first nonparametric tests conceived (R.. Fisher, 1935).","code":""},{"path":"permschp.html","id":"randomization-p-value","chapter":"7 Permutation Tests","heading":"Randomization p-value","text":"Let \\(t^*\\) observed test statistic. two sample test \\(N\\) total observations \\(n\\) observations group 1, \\({N \\choose n}\\) randomizations, equally likely \\(H_0\\). p-value becomes:\n\\[\\begin{align}\np_R &= P(T \\leq t^* | H_0) = \\frac{\\sum_{=1}^{{N \\choose n}} (t_i \\leq t*)}{{N \\choose n}}\n\\end{align}\\]\nchoose significance level \\(\\alpha = k/{N \\choose n}\\), type error rate :\n\\[\\begin{align}\nP(\\text{type error}) &= P(p_R \\leq \\alpha | H_0)\\\\\n&= P\\bigg(\\sum_{=1}^{{N \\choose n}} (t_i \\leq t*) \\leq k | H_0 \\bigg)\\\\\n&= \\frac{k}{{N \\choose n}}= \\alpha\\\\\n\\text{alternatively }  k&= \\alpha {N \\choose n}\n\\end{align}\\]\npoint say randomization test controls probability Type error minimal conditions subjects randomized treatments (minimal condition, hard practice!!)","code":""},{"path":"permschp.html","id":"permutation-p-value","chapter":"7 Permutation Tests","heading":"Permutation p-value","text":"Now consider permutation test randomly permutes data \\(B\\) times (instead \\({N \\choose n}\\) times). permutation test approximates randomization test. fact, permutation test can analyzed using following binomial random variable:\n\\[\\begin{align}\nX_P &= \\# \\ \\mbox{permutations B give extreme value observed test statistic}\\\\\nX_P &\\sim Bin(p_R, B)\\\\\nSE(X_P) &= \\sqrt{\\frac{p_R (1-p_R)}{B}} \\approx \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\n\\end{align}\\]Consider situation interest small effect, say p-value\\(\\approx 0.01\\). SE less 0.001.\n\\[\\begin{align}\n0.001 &= \\sqrt{ (0.01)\\cdot(0.99) / B}\\\\\nB &= (0.01) \\cdot (0.99) / (0.001)^2\\\\\n&= 9900\n\\end{align}\\]Another way look problem use estimated p-value = \\(\\hat{p}_P = \\frac{X_P}{B}\\) come confidence interval \\(p_R\\).CI \\(p_R \\approx \\hat{p}_P \\pm 1.96 \\sqrt{\\frac{\\hat{p}_P (1-\\hat{p}_P)}{B}}\\)","code":""},{"path":"permschp.html","id":"ci-from-permutation-tests","chapter":"7 Permutation Tests","heading":"7.2.2 CI from Permutation Tests","text":"Use shifts rescaling create CI parameter value using permutation tests. , consider situation data \\(X\\) \\(Y\\) Use one following transformation (depending study):\n\\[\\begin{align}\nW &= Y + \\\\\n\\mbox{} U &= Y / b\n\\end{align}\\]\nrun permutation test interest \\(X\\) vs. \\(W\\) \\(X\\) vs. \\(U\\). series \\(\\) \\(b\\) values can find don’t reject particular level significance (\\(\\alpha\\)) create \\((1-\\alpha)100\\%\\) confidence interval.Usually, however, use bootstrapping confidence intervals permutation tests hypothesis testing.","code":""},{"path":"permschp.html","id":"randomization-example","chapter":"7 Permutation Tests","heading":"7.2.3 Randomization Example","text":"","code":""},{"path":"permschp.html","id":"fishers-exact-test-computationally-efficient-randomization-test","chapter":"7 Permutation Tests","heading":"7.2.3.1 Fisher’s Exact Test – computationally efficient randomization test","text":"N observations classified 2x2 table.observation classified exactly one cell.Row column totals fixed.Given fixed row column totals, can easily calculate interior distribution using hypergeometric. Note single cell filled, cells determined.\\[\\begin{align}\nP(X=x) &= \\frac{{r \\choose x}{{N-r} \\choose{c-x}}}{{N \\choose c}}\\\\\n& \\mbox{col 1, many row 1?}\\\\\nP(X \\leq x) &= \\sum_{=0}^x \\frac{{r \\choose }{{N-r} \\choose {c-}}}{{N \\choose c}}\\\\\n&= \\mbox{p-value}\n\\end{align}\\]common row column totals fixed. (likely just column totals fixed, e.g., men women.) Instead, consider subsets sample space \\(N\\) observations. particular combination row column totals (\\(rc\\)):\\[\\begin{align}\nP(\\mbox{rejecting } H_0 | rc, H_0) &\\leq& \\alpha\\\\\nP(\\mbox{rejecting } H_0 \\ \\ \\forall \\mbox{ subsets } | H_0) &\\leq& \\sum_{rc \\ combos} P(\\mbox{rejecting } H_0 | rc, H_0) P(rc | H_0)\\\\\n&\\leq& \\alpha\n\\end{align}\\](Note: assume $P(rc | H_0) = 1 / # rc $ combos.) test valid \\(\\alpha\\) level, won’t powerful one fixed columns/rows actually meaningful.","code":""},{"path":"permschp.html","id":"r-times-c-tables","chapter":"7 Permutation Tests","heading":"7.2.3.2 \\(r \\times c\\) tables","text":"permute data new waynew test stat\n\\[\\begin{align}\nT = \\sum_{,j} \\frac{(O_{,j} - E_{,j})^2}{E_{,j}}\n\\end{align}\\]2-sided p-value. expect?\n\\[\\begin{align}\nE_{,j} = \\frac{R_i C_j}{N}\n\\end{align}\\]","code":""},{"path":"permschp.html","id":"example-observer","chapter":"7 Permutation Tests","heading":"7.2.3.2.1 Example: Observer","text":"study published Journal Personality Social Psychology (Butler Baumeister, 1998), researchers investigated conjecture observer vested interest decrease subjects’ performance skill-based task. Subjects given time practice playing video game required navigate obstacle course quickly possible. told play game final time observer present. Subjects randomly assigned one two groups:Group told participant observer win $3 participant beat certain threshold.Group B told participant win prize threshold beaten.goal data analysis determine whether effect observer performance. , like \\(\\chi^2\\) test, hypotheses :\\(H_0:\\) association two variables\n\\(H_a:\\) association two variablesThe data 24 subjects given :Card simulation (demonstrate permutation test works)Permutation Test (see Chance Rossman applet automated permutation test, http://www.rossmanchance.com/applets/ChisqShuffle.htm?FET=1)\\[\\begin{align}\nSE(\\mbox{p-value}) = \\sqrt{\\frac{\\hat{p}_r (1-\\hat{p}_r)}{100}} = 0.02\n\\end{align}\\]Randomization Test\n\\[\\begin{align}\nP(X \\leq 3) = \\sum_{=0}^3 \\frac{{11 \\choose }{12 \\choose {12-}}}{{24 \\choose 12}} = 0.0436\n\\end{align}\\]","code":""},{"path":"permschp.html","id":"r-examples","chapter":"7 Permutation Tests","heading":"7.3 R examples","text":"","code":""},{"path":"permschp.html","id":"cloud-seeding-two-sample-test-computationally-very-difficult-to-do-a-randomization-test","chapter":"7 Permutation Tests","heading":"7.3.1 Cloud Seeding (Two sample test – computationally very difficult to do a randomization test)","text":"Cloud seeding data: seeding seeding randomly allocated 52 days seeding appropriate. pilot know whether plane seeding. Rain measured acre-feet.running tests compare means variances obtain following p-values:","code":""},{"path":"permschp.html","id":"r-code","chapter":"7 Permutation Tests","heading":"7.3.1.1 R code","text":"anything, let’s look data. , visualize boxplots histograms. Also, visualize raw scale well log scale. Certainly, log10 scale indicates transformation makes data symmetric.unlogged data:, ’ve formally gone permutation. , resampling coded particularly tidy way, tidy way code loops! Generally, loops fasted way code R, need quickly run code seems like go loop, likely purrr direction want go, https://purrr.tidyverse.org/.","code":"\nclouds <- read_delim(\"figs/cloud-seeding.txt\", \n     \"\\t\", escape_double = FALSE, trim_ws = TRUE) \n\nnames(clouds) <- c(\"unseeded\", \"seeded\")\nclouds <- tidyr::pivot_longer(clouds, cols = 1:2, names_to = \"seeding\", values_to = \"rainfall\") %>%\n  mutate(seeding = as.factor(seeding))\n\nclouds %>%\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot()\nclouds %>%\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding)\nclouds %>%\n  ggplot(aes(x=seeding, y=rainfall)) + geom_boxplot() + scale_y_log10()\nclouds %>%\n  ggplot(aes(x=rainfall)) + geom_histogram(bins = 20) + facet_wrap(~seeding) + scale_x_log10()\nclouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain))## # A tibble: 2 × 3\n##   seeding  meanrain meanlnrain\n##   <fct>       <dbl>      <dbl>\n## 1 seeded       442.       5.13\n## 2 unseeded     165.       3.99\nclouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %>%\n  summarize(diff(meanrain), diff(meanlnrain))## # A tibble: 1 × 2\n##   `diff(meanrain)` `diff(meanlnrain)`\n##              <dbl>              <dbl>\n## 1            -277.              -1.14\nraindiffs <- clouds %>%\n  mutate(lnrain = log(rainfall)) %>%\n  group_by(seeding) %>%\n  summarize(meanrain = mean(rainfall), meanlnrain = mean(lnrain)) %>%\n  summarize(diffrain = diff(meanrain), difflnrain = diff(meanlnrain))\n\nraindiffs## # A tibble: 1 × 2\n##   diffrain difflnrain\n##      <dbl>      <dbl>\n## 1    -277.      -1.14"},{"path":"permschp.html","id":"difference-in-means-after-permuting","chapter":"7 Permutation Tests","heading":"7.3.1.1.1 Difference in means after permuting","text":"","code":"\nreps <- 1000\npermdiffs <- c()\n\nfor(i in 1:reps){\n  onediff <- clouds %>%\n    mutate(permseed = sample(seeding)) %>%\n    group_by(permseed) %>%\n    summarize(meanrain = mean(rainfall)) %>%\n    summarize(diff(meanrain)) %>% pull()\n  \npermdiffs <- c(permdiffs, onediff)\n}\n\npermdiffs %>% data.frame() %>%\n  ggplot(aes(x = permdiffs)) + geom_histogram(bins=30) + geom_vline(xintercept = raindiffs$diffrain, color = \"red\")"},{"path":"permschp.html","id":"ratio-of-variances-after-permuting","chapter":"7 Permutation Tests","heading":"7.3.1.1.2 Ratio of variances after permuting","text":"","code":"\nrainvarratio <- clouds %>%\n    group_by(seeding) %>%\n    summarize(varrain = var(rainfall)) %>%\n    summarize(rainratio = varrain[1] / varrain[2])\n\n\nreps <- 1000\npermvars <- c()\n\nfor(i in 1:reps){\n  oneratio <- clouds %>%\n    mutate(permseed = sample(seeding)) %>%\n    group_by(permseed) %>%\n    summarize(varrain = var(rainfall)) %>%\n    summarize(varrain[1] / varrain[2]) %>% pull()\n  \npermvars <- c(permvars, oneratio)\n}\n\npermvars %>% data.frame() %>%\n  ggplot(aes(x = permvars)) + geom_histogram(bins=30) + geom_vline(xintercept = rainvarratio$rainratio , color = \"red\")"},{"path":"permschp.html","id":"testing-differences-in-means-or-ratios-of-variances","chapter":"7 Permutation Tests","heading":"7.3.1.1.3 Testing differences in means or ratios of variances","text":"evidenced histograms ,permutation test (one-sided) difference means count number permuted differences less equal observed difference means, just 1%.permutation test (one-sided) difference means count number permuted differences less equal observed difference means, just 1%.permutation test (one-sided) ratio variances count number permuted ratios greater equal observed ratio variances, 7%.permutation test (one-sided) ratio variances count number permuted ratios greater equal observed ratio variances, 7%.","code":"\n(sum(raindiffs$diffrain >= permdiffs) + 1) /1000## [1] 0.029\n(sum(rainvarratio$rainratio <= permvars)+1)/1000## [1] 0.083"},{"path":"permschp.html","id":"macnell-teaching-evaluations-stratified-two-sample-t-test","chapter":"7 Permutation Tests","heading":"7.3.2 MacNell Teaching Evaluations (Stratified two-sample t-test)","text":"Boring et al. (2016) reanalyze data MacNell et al. (2014). Students randomized 4 online sections course. two sections, instructors swapped identities. instructor identified female rated lower average? (https://www.math.upenn.edu/~pemantle/active-papers/Evals/stark2016.pdf)\nFigure 7.1: Kraj (2017)\n\nFigure 7.2: Kraj (2017)\n\nFigure 1.6: Mengel, Sauermann, Zölitz (2019)\n\nFigure 7.3: MacNell, Driscoll, Hunt (2015)\n\nFigure 7.4: MacNell, Driscoll, Hunt (2015)\n","code":""},{"path":"permschp.html","id":"r-code-1","chapter":"7 Permutation Tests","heading":"7.3.2.0.1 R code","text":"","code":"\n#macnell <- readr::read_csv(\"https://raw.githubusercontent.com/statlab/permuter/master/data-raw/macnell.csv\")\nlibrary(permuter)\ndata(macnell)\nlibrary(ggridges)\nmacnell %>% \n  mutate(TAID = ifelse(taidgender==1, \"male\", \"female\")) %>%\n  mutate(TAGend = ifelse(tagender==1, \"male\", \"female\")) %>%\nggplot(aes(y=TAGend, x=overall, group = interaction(TAGend, TAID), \n           fill=TAID)) +\n  geom_point(position=position_jitterdodge(jitter.height=0.3, jitter.width = 0, dodge.width = 0.4), aes(color = TAID)) +\n  stat_summary(fun=\"mean\", geom=\"crossbar\", size=.3, width = 1,\n               aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  stat_summary(fun=\"mean\", geom=\"point\", shape = \"X\",\n               size=5, aes(color = TAID),\n               position=position_dodge(width=0.4)) +\n  coord_flip() +\n  xlab(\"\") + ggtitle(\"Overall teaching effectiveness score\")"},{"path":"permschp.html","id":"analysis-goal","chapter":"7 Permutation Tests","heading":"7.3.2.1 Analysis goal","text":"Want know score perceived gender different.\\[H_0:  \\mu_{ID.Female} = \\mu_{ID.Male}\\]\n> Although permutation test, null hypothesis means population distributions , variance aspects distributions across perceived gender.","code":""},{"path":"permschp.html","id":"macnell-data-without-permutation","chapter":"7 Permutation Tests","heading":"7.3.2.2 MacNell Data without permutation","text":"","code":"\nmacnell %>%\n  select(overall, tagender, taidgender) %>% head(15)##    overall tagender taidgender\n## 1        4        0          1\n## 2        4        0          1\n## 3        5        0          1\n## 4        5        0          1\n## 5        5        0          1\n## 6        4        0          1\n## 7        4        0          1\n## 8        5        0          1\n## 9        4        0          1\n## 10       3        0          1\n## 11       5        0          1\n## 12       4        0          1\n## 13       5        1          1\n## 14       5        1          1\n## 15       4        1          1"},{"path":"permschp.html","id":"permuting-macnell-data","chapter":"7 Permutation Tests","heading":"7.3.2.3 Permuting MacNell data","text":"Conceptually, two levels randomization:\\(N_m\\) students randomly assigned male instructor \\(N_f\\) assigned female instructor.\\(N_m\\) students randomly assigned male instructor \\(N_f\\) assigned female instructor.\\(N_j\\) assigned instructor \\(j\\), \\(N_{jm}\\) told instructor male, \\(N_{jf}\\) told instructor female \\(j=m,f\\).\\(N_j\\) assigned instructor \\(j\\), \\(N_{jm}\\) told instructor male, \\(N_{jf}\\) told instructor female \\(j=m,f\\).Stratified two-sample test:instructor, permute perceived gender assignments.Use difference mean ratings female-identified vs male-identified instructors.","code":"\nmacnell %>%\n  group_by(tagender, taidgender) %>%\n  summarize(n())## # A tibble: 4 × 3\n## # Groups:   tagender [2]\n##   tagender taidgender `n()`\n##      <int>      <int> <int>\n## 1        0          0    11\n## 2        0          1    12\n## 3        1          0    13\n## 4        1          1    11macnell %>%   group_by(tagender) %>%  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%  select(overall, tagender, taidgender, permTAID)\n## # A tibble: 47 × 4\n## # Groups:   tagender [2]\n##    overall tagender taidgender permTAID\n##      <dbl>    <int>      <int>    <int>\n##  1       4        0          1        0\n##  2       4        0          1        0\n##  3       5        0          1        1\n##  4       5        0          1        1\n##  5       5        0          1        1\n##  6       4        0          1        1\n##  7       4        0          1        1\n##  8       5        0          1        1\n##  9       4        0          1        1\n## 10       3        0          1        0\n## # … with 37 more rows\nmacnell %>%   group_by(tagender) %>%  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%  ungroup(tagender) %>%  group_by(permTAID) %>%  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%  summarize(diff(pmeans))\n## # A tibble: 1 × 1\n##   `diff(pmeans)`\n##            <dbl>\n## 1          0.188\n\ndiff_means_func <- function(.x){\n  macnell %>% group_by(tagender) %>%\n  mutate(permTAID = sample(taidgender, replace=FALSE)) %>%\n  ungroup(tagender) %>%\n  group_by(permTAID) %>%\n  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%\n  summarize(diff_mean = diff(pmeans))\n  }\n\nmap_df(1:5, diff_means_func)## # A tibble: 5 × 1\n##   diff_mean\n##       <dbl>\n## 1   0.00216\n## 2  -0.463  \n## 3  -0.567  \n## 4  -0.00216\n## 5  -0.00652"},{"path":"permschp.html","id":"observed-vs.-permuted-statistic","chapter":"7 Permutation Tests","heading":"7.3.2.4 Observed vs. Permuted statistic","text":"","code":"\n# observed\nmacnell %>% \n  group_by(taidgender) %>%\n  summarize(pmeans = mean(overall, na.rm=TRUE)) %>%\n  summarize(diff_mean = diff(pmeans))## # A tibble: 1 × 1\n##   diff_mean\n##       <dbl>\n## 1     0.474\n# permuted\nset.seed(47)\nreps = 1000\nperm_diff_means <- map_df(1:reps, diff_means_func)"},{"path":"permschp.html","id":"permutation-sampling-distribution","chapter":"7 Permutation Tests","heading":"7.3.2.5 permutation sampling distribution:","text":".pull-right[]","code":"\n# permutation p-value\nperm_diff_means %>%\n  summarize(p_val = \n      sum(diff_mean > 0.474) / \n      reps)## # A tibble: 1 × 1\n##   p_val\n##   <dbl>\n## 1 0.048"},{"path":"permschp.html","id":"actual-macnell-results","chapter":"7 Permutation Tests","heading":"7.3.2.6 Actual MacNell results","text":"","code":""},{"path":"permschp.html","id":"income-and-health-f-like-test","chapter":"7 Permutation Tests","heading":"7.3.3 Income and Health (F-like test)","text":"Consider NHANES dataset.Income\n(HHIncomeMid - Numerical version HHIncome derived middle income category)\n(HHIncomeMid - Numerical version HHIncome derived middle income category)Health\n(HealthGen - Self-reported rating participant’s health general Reported participants aged 12 years older. One Excellent, Vgood, Good, Fair, Poor.)\n(HealthGen - Self-reported rating participant’s health general Reported participants aged 12 years older. One Excellent, Vgood, Good, Fair, Poor.)","code":""},{"path":"permschp.html","id":"summary-of-the-variables-of-interest","chapter":"7 Permutation Tests","heading":"7.3.3.1 Summary of the variables of interest","text":"","code":"\nNHANES %>% select(HealthGen) %>% table()## .\n## Excellent     Vgood      Good      Fair      Poor \n##       878      2508      2956      1010       187\nNHANES %>% select(HHIncomeMid) %>% summary()##   HHIncomeMid    \n##  Min.   :  2500  \n##  1st Qu.: 30000  \n##  Median : 50000  \n##  Mean   : 57206  \n##  3rd Qu.: 87500  \n##  Max.   :100000  \n##  NA's   :811"},{"path":"permschp.html","id":"mean-income-broken-down-by-health","chapter":"7 Permutation Tests","heading":"7.3.3.2 Mean Income broken down by Health","text":"differences means simply due random chance??differences health, can calculated directly, still don’t know differences due randome chance larger structure.","code":"\nNH.means <- NHANES %>% \n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) %>% \n  group_by(HealthGen) %>% \n  summarize(IncMean = mean(HHIncomeMid, na.rm=TRUE), count=n())\nNH.means## # A tibble: 5 × 3\n##   HealthGen IncMean count\n##   <fct>       <dbl> <int>\n## 1 Excellent  69354.   817\n## 2 Vgood      65011.  2342\n## 3 Good       55662.  2744\n## 4 Fair       44194.   899\n## 5 Poor       37027.   164\nNHANES %>% filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>% \nggplot(aes(x=HealthGen, y=HHIncomeMid)) + \n  geom_boxplot() + \n  geom_jitter(width=0.1, alpha=.2)##           Excellent  Vgood   Good  Fair  Poor\n## Excellent         0   4344  13692 25161 32327\n## Vgood         -4344      0   9348 20817 27983\n## Good         -13692  -9348      0 11469 18635\n## Fair         -25161 -20817 -11469     0  7166\n## Poor         -32327 -27983 -18635 -7166     0"},{"path":"permschp.html","id":"overall-difference","chapter":"7 Permutation Tests","heading":"7.3.3.3 Overall difference","text":"can measure overall differences amount variability means overall mean:\\[F = \\frac{\\text{-group variability}}{\\text{within-group variability}}\\]\n\\[F = \\frac{\\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2/(K-1)}{\\sum_{ij} (X_{ij}-\\overline{X}_{\\cdot})^2/(N-K)}\\]\n\\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2\\]","code":""},{"path":"permschp.html","id":"creating-a-test-statistic","chapter":"7 Permutation Tests","heading":"7.3.3.4 Creating a test statistic","text":"\\[SumSqBtwn = \\sum_i n_i(\\overline{X}_{\\cdot} - \\overline{X})^2\\]","code":"\nNHANES %>% select(HHIncomeMid, HealthGen) %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid))## # A tibble: 6,966 × 2\n##    HHIncomeMid HealthGen\n##          <int> <fct>    \n##  1       30000 Good     \n##  2       30000 Good     \n##  3       30000 Good     \n##  4       40000 Good     \n##  5       87500 Vgood    \n##  6       87500 Vgood    \n##  7       87500 Vgood    \n##  8       30000 Vgood    \n##  9      100000 Vgood    \n## 10       70000 Fair     \n## # … with 6,956 more rows\nGM <- mean(NHANES$HHIncomeMid, na.rm=TRUE)\n\nGM## [1] 57206\nNH.means## # A tibble: 5 × 3\n##   HealthGen IncMean count\n##   <fct>       <dbl> <int>\n## 1 Excellent  69354.   817\n## 2 Vgood      65011.  2342\n## 3 Good       55662.  2744\n## 4 Fair       44194.   899\n## 5 Poor       37027.   164\nNH.means$IncMean - GM## [1]  12148   7805  -1544 -13013 -20179\n(NH.means$IncMean - GM)^2## [1] 1.48e+08 6.09e+07 2.38e+06 1.69e+08 4.07e+08\nNH.means$count## [1]  817 2342 2744  899  164\nNH.means$count * (NH.means$IncMean - GM)^2## [1] 1.21e+11 1.43e+11 6.54e+09 1.52e+11 6.68e+10\nsum(NH.means %>% select(count) %>% pull() * \n      (NH.means %>% select(IncMean) %>% pull() - GM)^2)## [1] 4.89e+11"},{"path":"permschp.html","id":"permuting-the-data","chapter":"7 Permutation Tests","heading":"7.3.3.5 Permuting the data","text":"","code":"\nNHANES %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n  select(HealthGen, HHIncomeMid, IncomePerm) ## # A tibble: 6,966 × 3\n##    HealthGen HHIncomeMid IncomePerm\n##    <fct>           <int>      <int>\n##  1 Good            30000      87500\n##  2 Good            30000      70000\n##  3 Good            30000     100000\n##  4 Good            40000      40000\n##  5 Vgood           87500      87500\n##  6 Vgood           87500     100000\n##  7 Vgood           87500      50000\n##  8 Vgood           30000      50000\n##  9 Vgood          100000      70000\n## 10 Fair            70000      22500\n## # … with 6,956 more rows"},{"path":"permschp.html","id":"permuting-the-data-a-new-test-statistic","chapter":"7 Permutation Tests","heading":"7.3.3.6 Permuting the data & a new test statistic","text":"","code":"\nNHANES %>% \n  filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n  mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n  group_by(HealthGen) %>% \n  summarize(IncMeanP = mean(IncomePerm), count=n()) %>%\n  summarize(teststat = sum(count*(IncMeanP - GM)^2))## # A tibble: 1 × 1\n##       teststat\n##          <dbl>\n## 1 14268151317."},{"path":"permschp.html","id":"lots-of-times","chapter":"7 Permutation Tests","heading":"7.3.3.7 Lots of times…","text":"","code":"\nreps <- 1000\n\nSSB_perm_func <- function(.x){\n  NHANES %>% \n        filter(!is.na(HealthGen)& !is.na(HHIncomeMid)) %>%\n        mutate(IncomePerm = sample(HHIncomeMid, replace=FALSE)) %>%\n        group_by(HealthGen) %>% \n        summarize(IncMeanP = mean(IncomePerm), count=n()) %>%\n        summarize(teststat = sum(count*(IncMeanP - GM)^2)) \n}\n\nSSB_perm_val <- map_dfr(1:reps, SSB_perm_func)\n\nSSB_perm_val## # A tibble: 1,000 × 1\n##        teststat\n##           <dbl>\n##  1 11839838857.\n##  2 14805138617.\n##  3 12238328218.\n##  4 14493898296.\n##  5 19052560418.\n##  6 14099580957.\n##  7 19808304723.\n##  8 14972708855.\n##  9 15543404291.\n## 10 18334398022.\n## # … with 990 more rows"},{"path":"permschp.html","id":"compared-to-the-real-data","chapter":"7 Permutation Tests","heading":"7.3.3.8 Compared to the real data","text":"","code":"\nSSB_obs <- NHANES %>%\n  filter(!is.na(HealthGen) & !is.na(HHIncomeMid)) %>% \n  group_by(HealthGen) %>% \n  summarize(IncMean = mean(HHIncomeMid), count=n()) %>%\n  summarize(obs_teststat = sum(count*(IncMean - GM)^2)) \n\nSSB_obs ## # A tibble: 1 × 1\n##    obs_teststat\n##           <dbl>\n## 1 488767088754.\nsum(SSB_perm_val %>% pull() > SSB_obs %>% pull() ) / reps## [1] 0\nSSB_perm_val %>%\n  ggplot(aes(x = teststat)) +\n  geom_histogram() + \n  geom_vline(data = SSB_obs, aes(xintercept = obs_teststat), color = \"red\") +\n  ylab(\"\") + xlab(\"Permuted SSB Test Statistics\")"},{"path":"boot.html","id":"boot","chapter":"8 Bootstrapping","heading":"8 Bootstrapping","text":"","code":""},{"path":"boot.html","id":"introduction","chapter":"8 Bootstrapping","heading":"8.1 Introduction","text":"permutation tests, going use random samples describe population (assuming simple random sample).Main idea: able estimate variability estimator (difference medians, ordinary least square non-normal errors, etc.).’s strange get \\(\\hat{\\theta}\\) SE(\\(\\hat{\\theta}\\)) data (consider \\(\\hat{p}\\) & \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) \\(\\overline{X}\\) & \\(s/\\sqrt{n}\\)).’ll consider confidence intervals now.Bootstrapping doesn’t help get around small samples.following applets may helpful:logic confidence intervals http://www.rossmanchance.com/applets/ConfSim.htmlBootstrapping actual datasets http://lock5stat.com/statkey/index.html","code":""},{"path":"boot.html","id":"BSnotation","chapter":"8 Bootstrapping","heading":"8.2 Basics & Notation","text":"Let \\(\\theta\\) parameter interest, let \\(\\hat{\\theta}\\) estimate \\(\\theta\\). , ’d take lots samples size \\(n\\) population create sampling distribution \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples \\(F\\):\\[\\begin{align}\n\\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{=1}^B \\hat{\\theta}_i\n\\end{align}\\]\nbest guess \\(\\theta\\). \\(\\hat{\\theta}\\) different \\(\\theta\\), call biased.\n\\[\\begin{align}\nSE(\\hat{\\theta}) &= \\bigg[ \\frac{1}{B-1} \\sum_{=1}^B(\\hat{\\theta}_i - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\\nq_1 &= [0.25 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\\nq_3 &= [0.75 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\\n\\end{align}\\], completely characterize sampling distribution (function \\(\\theta\\)) allow us make inference \\(\\theta\\) \\(\\hat{\\theta}\\).\nFigure 1.2: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig\n","code":""},{"path":"boot.html","id":"the-plug-in-principle","chapter":"8 Bootstrapping","heading":"8.2.1 The Plug-in Principle","text":"Recall\n\\[\\begin{align}\nF(x) &= P(X \\leq x)\\\\\n\\hat{F}(x) &= S(x) = \\frac{\\# \\{X_i \\leq x\\} }{n}\n\\end{align}\\]\n\\(\\hat{F}(x)\\) sufficient statistic \\(F(x)\\). , information \\(F\\) data contained \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) MLE \\(F(x)\\) (probabilities, ’s binomial argument).Note , general, interested parameter, \\(\\theta\\).\n\\[\\begin{align}\n\\theta = t(F) \\ \\ \\ \\ [\\mbox{e.g., } \\mu = \\int x f(x) dx ]\n\\end{align}\\]plug-estimate \\(\\theta\\) :\n\\[\\begin{align}\n\\hat{\\theta} = t(\\hat{F}) \\ \\ \\ \\ [\\mbox{e.g., } \\overline{X} = \\frac{1}{n} \\sum X_i ]\n\\end{align}\\]: estimate parameter, use statistic corresponding quantity sample.\\[\\begin{align}\n\\mbox{Ideal Real World} & \\mbox{Boostrap World}\\\\\nF \\rightarrow x &\\Rightarrow \\hat{F} \\rightarrow x^*\\\\\n\\downarrow &  \\downarrow\\\\\n\\hat{\\theta}  & \\hat{\\theta}^*\n\\end{align}\\]idea boostrapping (fact, bootstrap samples ), depends double arrow. must random sample: , \\(\\hat{F}\\) must good job estimating \\(F\\) order bootstrap concepts meaningful.Note ’ve seen plug--principle :\n\\[\\begin{align}\n\\sqrt{\\frac{p(1-p)}{n}} &\\approx& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\end{align}\\]\n","code":""},{"path":"boot.html","id":"the-bootstrap-idea","chapter":"8 Bootstrapping","heading":"8.2.2 The Bootstrap Idea","text":"can resample sample represent samples actual population! boostrap distribution statistic, based many resamples, represents sampling distribution statistic based many samples. okay?? assuming?\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)","code":""},{"path":"boot.html","id":"bootstrap-procedure","chapter":"8 Bootstrapping","heading":"8.2.3 Bootstrap Procedure","text":"Resample data replacement original sample.Calculate statistic interest resample.Repeat 1. 2. \\(B\\) times.Use bootstrap distribution inference.","code":""},{"path":"boot.html","id":"bootstrap-notation","chapter":"8 Bootstrapping","heading":"8.2.4 Bootstrap Notation","text":"Take many (\\(B\\)) resamples size \\(n\\) sample, \\(\\hat{F}(x)\\) (instead population, \\(F(x)\\) ) create bootstrap distribution \\(\\hat{\\theta}^*\\) (instead sampling distribution \\(\\hat{\\theta}\\)).Let \\(\\hat{\\theta}^*(b)\\) calculated statistic interest \\(b^{th}\\) bootstrap sample. best guess \\(\\theta\\) :\n\\[\\begin{align}\n\\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*(b)\n\\end{align}\\]\n(\\(\\hat{\\theta}^*\\) different \\(\\hat{\\theta}\\), call biased.) estimated value standard error estimate \n\\[\\begin{align}\n\\hat{SE}^* = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*(b) - \\hat{\\theta}^*)^2 \\bigg]^{1/2}\n\\end{align}\\]Just like repeatedly taking samples population, taking resamples sample allows us characterize bootstrap distribution approximates sampling distribution. bootstrap distribution approximates shape, spread, & bias actual sampling distribution.\nFigure 1.3: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig. left image represents mean n=50. center image represents mean n=9. right image represents median n=15.\nStatKey applets demonstrate bootstrapping : http://www.lock5stat.com/StatKey/","code":""},{"path":"boot.html","id":"BSCI","chapter":"8 Bootstrapping","heading":"8.3 Bootstrap Confidence Intervals","text":"","code":""},{"path":"boot.html","id":"normal-standard-ci-with-bootse-typenorm","chapter":"8 Bootstrapping","heading":"8.3.1 Normal (standard) CI with BootSE: type=\"norm\"","text":"Keep mind trying approximate sampling distribution \\(\\hat{\\theta}\\). fact, really able estimate sampling distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). hope :\\[\\begin{align}\n\\hat{F}\\Big(\\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)} \\Big) \\rightarrow F\\Big(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\Big)\n\\end{align}\\]Recall derivation conventional confidence intervals (based assumption sampling distribution test statistic normal close):\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\], ’s endpoints random, 0.95 probability ’ll get random sample produce endpoints capture true parameter.95% CI \\(\\theta\\) : \\[\\hat{\\theta} \\pm z_{(\\alpha/2)} \\hat{SE}^*\\]","code":""},{"path":"boot.html","id":"bootstrap-t-confidence-intervals-typestud","chapter":"8 Bootstrapping","heading":"8.3.2 Bootstrap-t Confidence Intervals: type=\"stud\"","text":"(idea calculating “t-multiplier” used CI. William Gosset went pseudonym “Student” originally figured distribution t-multiplier, following intervals called either “studentized” “t” bootstrap confidence intervals.)Recall derivation conventional confidence intervals:\\[\\begin{align}\nP\\bigg(z_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq z_{(1-\\alpha/2)}\\bigg)&= 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - z_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - z_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &= 1 - \\alpha\\\\\n\\end{align}\\], ’s endpoints random, 0.95 probability ’ll get random sample produce endpoints capture true parameter.use Boot SE within CI formula (interval ). problem interval accurate distribution \\(\\hat{\\theta}\\) reasonably normal. bias skew, CI desired coverage levels (Efron Tibshirani (1993), pg 161 chapter 22).use Boot SE within CI formula (interval ). problem interval accurate distribution \\(\\hat{\\theta}\\) reasonably normal. bias skew, CI desired coverage levels (Efron Tibshirani (1993), pg 161 chapter 22).Now consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{align}\nT^*(b) &= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{align}\\]Now consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{align}\nT^*(b) &= \\frac{\\hat{\\theta}^*(b) - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{align}\\]\\(\\hat{\\theta}^*(b)\\) value \\(\\hat{\\theta}\\) \\(b^{th}\\) bootstrap sample, \\(\\hat{SE}^*(b)\\) estimated standard error \\(\\hat{\\theta}^*(b)\\) \\(b^{th}\\) bootstrap sample. \\(\\alpha^{th}\\) percentile \\(T^*(b)\\) estimated value \\(\\hat{t}^*_\\alpha\\) \\[\\begin{align}\n\\frac{\\# \\{T^*(b) \\leq \\hat{t}^*_{\\alpha/2} \\} }{B} = \\alpha/2\n\\end{align}\\]example, \\(B=1000\\), estimate 5% point \\(50^{th}\\) smallest value \\(T^*(b)\\)s, estimate 95% point \\(950^{th}\\) smallest value \\(T^*(b)\\)s.Finally, boostrap-t confidence interval :\n\\[\\begin{equation} \n(\\hat{\\theta} - \\hat{t}^*_{1-\\alpha/2}\\hat{SE}^*,  \\hat{\\theta} - \\hat{t}^*_{\\alpha/2}\\hat{SE}^*) \\tag{8.1}\n\\end{equation}\\]find bootstrap-t interval, bootstrap twice. algorithm follows:Generate \\(B_1\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*(b)\\).Generate \\(B_1\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*(b)\\).Take \\(B_2\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Take \\(B_2\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Find \\(B_1\\) values \\(T^*(b)\\). Calculate \\(\\hat{t}^*_\\alpha/2\\) \\(\\hat{t}^*_{1-\\alpha/2}\\).Find \\(B_1\\) values \\(T^*(b)\\). Calculate \\(\\hat{t}^*_\\alpha/2\\) \\(\\hat{t}^*_{1-\\alpha/2}\\).Calculate CI equation ((8.1)).Calculate CI equation ((8.1)).\\(B\\cdot \\alpha\\) integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) \\(B+1-k\\).\\(B\\cdot \\alpha\\) integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) \\(B+1-k\\).Bootstrap-t intervals somewhat erratic can influenced outliers. Percentile methods can reliable. [balance best open question depending lot data distribution statistic interest.]Bootstrap-t intervals somewhat erratic can influenced outliers. Percentile methods can reliable. [balance best open question depending lot data distribution statistic interest.]\\(B=100\\) 200 probably enough bootstrap-t CI (500 1000 better). However, \\(B=25\\) may enough estimate SE inner-Bootprocedure. (\\(B=1000\\) needed computing percentiles.)\\(B=100\\) 200 probably enough bootstrap-t CI (500 1000 better). However, \\(B=25\\) may enough estimate SE inner-Bootprocedure. (\\(B=1000\\) needed computing percentiles.)choosing appropriate multiplier:\ncorrect multiplier use, normal multiplier (\\(z\\)) good \\(n\\) samples.\ncorrect multiplier use, t multiplier good samples specified \\(n\\).\ncorrect multiplier use, bootstrap-t multiplier good sample .\nchoosing appropriate multiplier:correct multiplier use, normal multiplier (\\(z\\)) good \\(n\\) samples.correct multiplier use, t multiplier good samples specified \\(n\\).correct multiplier use, bootstrap-t multiplier good sample .resulting intervals typically symmetric (\\(\\hat{t}^*_\\alpha \\ne - \\hat{t}^*_{1-\\alpha}\\)). part improvement \\(z\\) \\(t\\) intervals.resulting intervals typically symmetric (\\(\\hat{t}^*_\\alpha \\ne - \\hat{t}^*_{1-\\alpha}\\)). part improvement \\(z\\) \\(t\\) intervals.Bootstrap-t intervals good location statistics (mean, quantiles, trimmed means) trusted statistics like correlation (necessarily vary based ideas shift).Bootstrap-t intervals good location statistics (mean, quantiles, trimmed means) trusted statistics like correlation (necessarily vary based ideas shift).","code":""},{"path":"boot.html","id":"percentile-confidence-intervals-typeperc","chapter":"8 Bootstrapping","heading":"8.3.3 Percentile Confidence Intervals: type=\"perc\"","text":"interval \\(\\alpha/2\\) \\(1-\\alpha/2\\) quantiles bootstrap distribution statistic \\((1-\\alpha)100\\%\\) bootstrap percentile confidence interval corresponding parameter:\\[\\begin{align}\n[\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}]\n\\end{align}\\]\nneed know percentile interval works… isn’t cool see works???\nwork? isn’t immediately obvious interval capture true parameter, \\(\\theta\\), rate 95%. Consider skewed sampling distribution. observed \\(\\hat{\\theta}\\) comes long tail, obvious short tail side CI get true parameter value correct rate? (Hall (*Bootstrap Edgeworth Expansion}, Springer, 1992, earlier papers) refers Efron’s “backwards” intervals.) , sampling distribution biased, percentiles bootstrap interval won’t capture parameter correct rate.see / percentiles intervals work, first start considering normal sampling distributions function statistic. Let \\(\\phi = g(\\theta), \\hat{\\phi} = g(\\hat{\\theta}), \\hat{\\phi}^* = g(\\hat{\\theta}^*)\\), g monotonic function (assume wlog g increasing). point choose (possible) \\(g(\\cdot)\\) \\[\\begin{equation}\n \\hat{\\phi}^* - \\hat{\\phi} \\sim \\hat{\\phi} - \\phi \\sim N(0, \\sigma^2) \\tag{8.2}.\n \\end{equation}\\]\n, consider logic conventional confidence interval. \\(\\hat{\\phi} - \\phi \\sim N(0, \\sigma^2)\\), interval \\(\\theta\\) derived :\\[\\begin{align}\nP(z_{0.05} \\leq \\frac{\\hat{\\phi} - \\phi}{\\sigma}  ) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} - z_{0.05} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\phi \\leq \\hat{\\phi} + z_{0.95} \\sigma) = 0.95  \\nonumber \\\\\nP(-\\infty \\leq \\theta \\leq g^{-1}(\\hat{\\phi} + z_{0.95} \\sigma)) = 0.95  \\nonumber \\\\\n\\Rightarrow \\mbox{CI } \\theta: \\ \\ \\ (-\\infty, g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})) \\tag{8.3}\n\\end{align}\\]\\(z_{1-\\alpha}\\) \\(100(1-\\alpha)\\) percent point standard normal distribution. Ideally, knew \\(g\\) \\(\\sigma\\), ’d able transformation find \\(g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})\\) (give endpoint confidence interval).Going back ((8.2)) indicates \\(\\hat{\\phi} + \\sigma z_{1-\\alpha} = F^{-1}_{\\hat{\\phi}^*}(1-\\alpha)\\) (\\(\\hat{\\phi} ^* \\sim N(\\hat{\\phi}, \\sigma^2)\\)). , since \\(g\\) monotonically increasing, \\(F^{-1}_{\\hat{\\phi}^*}(1-\\alpha) = g(F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\\)\nSubstituting ((8.3)), gives percentile interval \\(\\theta\\),\\[\\begin{align}\n(-\\infty, F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\n\\end{align}\\](similar argument gives derivation two sided confidence interval. Proof Carpenter Bithell (2000)) order percentile interval appropriate, technical condition normalizing transformation exists. need actually find transformation!transformation respecting property CI transformation respecting , monotone transformation, CI transformed parameter (exactly) transformed CI unstransformed parameter. Let \\(\\phi = m(\\theta)\\).\\[\\begin{align}\n[\\phi_{lo}, \\phi_{}] = [m(\\theta_{lo}), m(\\theta_{})]\n\\end{align}\\]Note idea process creating CI. , create confidence interval using \\(\\phi\\), ’ll get thing created CI using \\(\\theta\\) transformed . straightforward see percentile CI transformation respecting. , monotone transformation statistic parameter, CI transformed appropriately.Let\n\\[\\begin{align}\n\\hat{\\phi} &= 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg)\\\\\nr &=\\frac{e^{2\\phi}+1}{e^{2\\phi}-1}\\\\\n\\end{align}\\]know \\(\\hat{\\phi}\\) approximated normal distribution. , percentile CI \\(\\phi\\) approximate normal theory CI know correct (given \\(\\alpha\\)). CI \\(\\phi\\) can find CI \\(\\rho\\) taking inverse monotonic transformation; rather… can just use r percentile CI start !range preserving property Another advantage percentile interval range preserving. , CI always produces endpoints fall within allowable range parameter.Bias percentile interval , however, perfect. statistic biased estimator parameter, exist transformation distribution centered around correct function parameter. Formally, \n\\[\\begin{align}\n\\hat{\\theta} \\sim N(\\theta + bias, \\hat{SE}^2)\n\\end{align}\\]\ntransformation \\(\\phi = m(\\theta)\\) can fix things . Keep mind standard intervals can fail variety ways, percentile method fixed specific situation sampling distribution non-normal.","code":""},{"path":"boot.html","id":"what-makes-a-ci-procedure-good","chapter":"8 Bootstrapping","heading":"8.3.4 What makes a CI procedure good?","text":"following qualities may may result confidence interval procedure determines choice method researcher.Symmetry (??): interval symmetric, pivotal around value. necessarily good thing. Maybe bad thing force?Resistant: BS-t particularly resistant outliers crazy sampling distributions statistic (can make robust variance stabilizing transformation)Range preserving: CI always contains values fall within allowable range (\\(p, \\rho\\),…)Transformation respecting: monotone transformation, \\(\\phi = m(\\theta)\\), interval \\(\\theta\\) mapped directly \\(m(\\theta)\\). \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) \\((1-\\alpha)100\\)% interval \\(\\theta\\), \\[\\begin{align}\n[\\hat{\\phi}_{(lo)},\\hat{\\phi}_{(hi)}] = [m(\\hat{\\theta}_{(lo)}),m(\\hat{\\theta}_{(hi)})]\n\\end{align}\\]\nexactly interval.Level confidence: central (symmetric) confidence interval, \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) probability \\(\\alpha/2\\) covering \\(\\theta\\) :\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2\\\\\n\\end{align}\\]Note: intervals approximate. judge based accurately cover \\(\\theta\\).\nCI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]\nCI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]\nNote: intervals approximate. judge based accurately cover \\(\\theta\\).CI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]CI first order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{align}\\]CI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]CI second order accurate :\n\\[\\begin{align}\nP(\\theta < \\hat{\\theta}_{(lo)})&=\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{align}\\]BS-t \\(2^{nd}\\) order accurate large general class functions. However, practice, coverage rate doesn’t kick small/med sample sizes unless appropriate transformations make distribution bell-shaped. (Tibshirani 1988)criteria speak coverage rates parameters. note must taken context. Much also depends :\nchoice statistic ; original data distribution; outlying observations; etc.","code":""},{"path":"boot.html","id":"advantages-and-disadvantages","chapter":"8 Bootstrapping","heading":"8.3.4.1 Advantages and Disadvantages","text":"Normal Approximation\nAdvantages similar familiar parametric approach; useful normally distributed \\(\\hat{\\theta}\\); requires least computation (\\(B=50-200\\))\nDisadvantages fails use entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); works \\(\\hat{\\theta}\\) reasonably normal start \nAdvantages similar familiar parametric approach; useful normally distributed \\(\\hat{\\theta}\\); requires least computation (\\(B=50-200\\))Disadvantages fails use entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); works \\(\\hat{\\theta}\\) reasonably normal start withBootstrap-t Confidence Interval\nAdvantages highly accurate CI many cases; handles skewed \\(F(\\hat{\\theta})\\) better percentile method\nDisadvantages invariant transformations; computationally expensive double bootstrap; coverage probabilities best distribution \\(\\hat{\\theta}\\) nice (e.g., normal)\nAdvantages highly accurate CI many cases; handles skewed \\(F(\\hat{\\theta})\\) better percentile methodDisadvantages invariant transformations; computationally expensive double bootstrap; coverage probabilities best distribution \\(\\hat{\\theta}\\) nice (e.g., normal)Percentile\nAdvantages uses entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) asymmetrical; invariant transformations; range respecting; simple execute\nDisadvantages small samples may result low accuracy (dependence tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) unbiased\nAdvantages uses entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) asymmetrical; invariant transformations; range respecting; simple executeDisadvantages small samples may result low accuracy (dependence tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) unbiasedBCa\nAdvantages\npercentile method; allows bias \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can calculated easily \\(\\hat{F}^*(\\hat{\\theta}^*)\\)\nDisadvantages requires limited parametric assumption; computational intervals\nAdvantages\npercentile method; allows bias \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can calculated easily \\(\\hat{F}^*(\\hat{\\theta}^*)\\)Disadvantages requires limited parametric assumption; computational intervals","code":""},{"path":"boot.html","id":"bootstrap-ci-and-hypothesis-testing","chapter":"8 Bootstrapping","heading":"8.3.4.2 Bootstrap CI and Hypothesis Testing","text":"null value parameter contained CI, reject null hypothesis; similarly, reject null value lie inside CI. Using BootCIs, can apply logic, test hypothesis interest (note: can always create one-sided intervals well!). using CIs leaves p-value information. get p-value CI? Consider alternative definition p-value:p-value: smallest level significance reject \\(H_0\\)., want null value (\\(\\theta_0\\)) one endpoints confidence interval level confidence \\(1-2\\alpha_0\\). \\(\\alpha_0\\) one-sided p-value, \\(2\\alpha_0\\) two-sided p-value.percentile intervals,\n\\[\\begin{align}\np-value = \\alpha_0 = \\frac{\\# \\hat{\\theta}^*(b) < \\theta_0}{B}\n\\end{align}\\]\n(without loss generality, assuming set \\(\\hat{\\theta}^*_{lo} = \\theta_0\\)).\nAnother cool bootstrap CI method won’t time cover. responsible remainder bootstrap material notes.\npercentile method, ’ve assumed exists transformation \\(\\theta\\), \\(\\phi(\\theta)\\), \n\\[\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(0,1)\n\\end{align}\\]\ntransformation assumes neither \\(\\theta\\) \\(\\phi\\) biased, assumes variance constant values parameter. , percentage intervals, assume normalizing transformation creates sampling distribution unbiased variance stabilizing. Consider monotone transformation *normalizes} sampling distribution (longer assume unbiased constant variance).now consider case \\(\\theta\\) biased estimator. :\n\\[\\begin{align}\n\\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{c} \\sim N(-z_0,1)\n\\end{align}\\]\n’ve corrected bias, non-constant variance, need adjustment stabilize variance:\\[\\begin{align}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(-z_0 \\sigma_\\phi,\\sigma_\\phi), \\ \\ \\ \\ \\ \\ \\sigma_\\phi = 1 + \\phi\n\\end{align}\\]\n, must exist monotone transformation \\(\\phi\\) \\(\\phi(\\hat{\\theta}) \\sim N\\) \n\\[\\begin{align}\nE(\\phi(\\hat{\\theta})) = \\phi(\\theta) - z_0 [1 + \\phi(\\theta)] & SE(\\phi(\\hat{\\theta})) = 1 + \\phi(\\theta)\n\\end{align}\\]\n(Note: expected value SE ’ve assumed \\(c=1\\). \\(c\\ne1\\), can always choose different transformation, \\(\\phi'\\) \\(c=1\\).) \n\\[\\begin{align}\nP(z_{\\alpha/2} \\leq \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{1 + \\phi(\\theta)} + z_0 \\leq z_{1-\\alpha/2}) = 1 - \\alpha\n\\end{align}\\]\n$(1-)$100% CI \\(\\phi(\\theta)\\) \n\\[\\begin{align}\n\\bigg[ \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{1 + (z_{1-\\alpha/2} - z_0)}, \\frac{\\phi(\\hat{\\theta}) - (z_{\\alpha/2} - z_0)}{1 + (z_{\\alpha/2} - z_0)} \\bigg]\n\\end{align}\\]\nLet’s consider interesting probability question:\n\\[\\begin{align}\nP\\bigg( \\phi(\\hat{\\theta}^*) &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg) = ?\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta}) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ -(1+\\phi(\\hat{\\theta})) (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ - (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} + z_0 &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n= P\\bigg( Z &\\leq \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg) = \\gamma_1\\\\\n\\mbox{} \\gamma_1 &= \\Phi \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n &= \\verb;pnorm; \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\n\\end{align}\\]’ve shown \\(\\gamma_1\\) quantile \\(\\phi(\\hat{\\theta}^*)\\) sampling distribution good estimate lower bound confidence interval \\(\\phi(\\theta)\\). Using argument upper bound, find $(1-)$100% confidence interval \\(\\phi(\\theta)\\) :\\[\\begin{align}\n&[\\phi(\\hat{\\theta}^*)_{\\gamma_1}, \\phi(\\hat{\\theta}^*)_{\\gamma_2}]\\\\\n& \\\\\n\\mbox{} \\gamma_1 &= \\Phi\\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n \\gamma_2 &= \\Phi \\bigg(\\frac{ (z_{1-\\alpha/2} + z_0) }{(1 - (z_{1-\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\end{align}\\]Using transformation respecting property percentile intervals, know $(1-)$100% confidence interval \\(\\theta\\) :\\[\\begin{align}\n&[\\hat{\\theta}^*_{\\gamma_1}, \\hat{\\theta}^*_{\\gamma_2}]\n\\end{align}\\]estimate \\(\\) \\(z_0\\)?bias:\n\\(z_0\\) measure bias. Recall:\\[\\begin{align}\nbias &= E(\\hat{\\theta}) - \\theta\\\\\n\\hat{bias} &= \\hat{\\theta}^* - \\hat{\\theta}\\\\\n\\end{align}\\]remember \\(z_0\\) represents bias \\(\\phi(\\hat{\\theta})\\), \\(\\hat{\\theta}\\) (don’t know \\(\\phi\\)!). , use \\(\\theta\\) see proportion \\(\\theta\\) values low, can map back \\(\\phi\\) space using normal distribution:\\[\\begin{align}\n\\hat{z}_0 &= \\Phi^{-1} \\bigg( \\frac{ \\# \\hat{\\theta}^*(b) < \\hat{\\theta}}{B} \\bigg)\n\\end{align}\\]\n, \\(\\hat{\\theta}^*\\) underestimates \\(\\hat{\\theta}\\), \\(\\hat{\\theta}\\) likely underestimates \\(\\theta\\); \\(z_0 > 0\\). think \\(z_0\\) normal quantile associated proportion Bootreplicates less \\(\\hat{\\theta}\\).skew:\n\\(\\) measure skew.\n\\[\\begin{align}\nbias&= E(\\hat{\\theta} - \\theta)\\\\\nvar &= E(\\hat{\\theta} - \\theta)^2 = \\sigma^2\\\\\nskew &= E(\\hat{\\theta} - \\theta)^3 / \\sigma^3\\\\\n\\end{align}\\]\ncan think skew rate chance standard error normalized scale. skew, estimate \\(=0\\). estimate \\(\\) comes procedure known jackknife.\\[\\begin{align}\n\\hat{} = \\frac{\\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^3}{6 [ \\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^2 ] ^{3/2}}\n\\end{align}\\]","code":""},{"path":"boot.html","id":"r-example-heroin","chapter":"8 Bootstrapping","heading":"8.4 R example: Heroin","text":"Hesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times (days) spent clinic methadone maintenance treatment people addicted heroin. data heroin.txt include amount time subjects stayed facility treatment terminated (column 4). 37% subjects, study ended still clinic (status=0). Thus, survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using median well 25% trimmed mean. treat group 238 patients representative population. (Chance Rossman 2018, Investigation 4.5.3)","code":""},{"path":"boot.html","id":"why-bootstrap","chapter":"8 Bootstrapping","heading":"Why bootstrap?","text":"Motivation: estimate variability statistic (dependent \\(H_0\\) true).","code":""},{"path":"boot.html","id":"reading-in-the-data","chapter":"8 Bootstrapping","heading":"Reading in the data","text":"","code":"\nheroin <- read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nheroin %>%\n  select(-prison)## # A tibble: 238 × 5\n##       id clinic status times  dose\n##    <dbl>  <dbl>  <dbl> <dbl> <dbl>\n##  1     1      1      1   428    50\n##  2     2      1      1   275    55\n##  3     3      1      1   262    55\n##  4     4      1      1   183    30\n##  5     5      1      1   259    65\n##  6     6      1      1   714    55\n##  7     7      1      1   438    65\n##  8     8      1      0   796    60\n##  9     9      1      1   892    50\n## 10    10      1      1   393    65\n## # … with 228 more rows"},{"path":"boot.html","id":"observed-test-statistics","chapter":"8 Bootstrapping","heading":"Observed Test Statistic(s)","text":"","code":"\nheroin %>%\n  summarize(obs_med = median(times), \n            obs_tr_mean = mean(times, trim = 0.25))## # A tibble: 1 × 2\n##   obs_med obs_tr_mean\n##     <dbl>       <dbl>\n## 1    368.        378."},{"path":"boot.html","id":"bootstrapped-data","chapter":"8 Bootstrapping","heading":"Bootstrapped data!","text":"","code":"\nset.seed(4747)\n\nheroin %>% \n  sample_frac(size=1, replace=TRUE) %>%\n  summarize(boot_med = median(times), \n            boot_tr_mean = mean(times, trim = 0.25))## # A tibble: 1 × 2\n##   boot_med boot_tr_mean\n##      <dbl>        <dbl>\n## 1      368         372."},{"path":"boot.html","id":"need-to-bootstrap-a-lot-of-times","chapter":"8 Bootstrapping","heading":"Need to bootstrap a lot of times…","text":"code showing bootstrap using loops (nested create t multipliers needed BS-t intervals). (package boot bootstrap , need write functions use .)set variablesboot stat functionresample functionbootstrapping","code":"\nn_rep1 <- 100\nn_rep2 <- 20\nset.seed(4747)\nboot_stat_func <- function(df){ \n    df %>% \n    mutate(obs_med = median(times),\n           obs_tr_mean = mean(times, trim = 0.25)) %>%\n    sample_frac(size=1, replace=TRUE) %>%\n    summarize(boot_med = median(times), \n              boot_tr_mean = mean(times, trim = 0.25),\n              obs_med = mean(obs_med),\n              obs_tr_mean = mean(obs_tr_mean))}\nboot_1_func <- function(df){\n  df %>% \n    sample_frac(size=1, replace=TRUE)\n}\nmap_df(1:n_rep1, ~boot_stat_func(df = heroin))## # A tibble: 100 × 4\n##    boot_med boot_tr_mean obs_med obs_tr_mean\n##       <dbl>        <dbl>   <dbl>       <dbl>\n##  1     368          372.    368.        378.\n##  2     358          363.    368.        378.\n##  3     431          421.    368.        378.\n##  4     332.         350.    368.        378.\n##  5     310.         331.    368.        378.\n##  6     376          382.    368.        378.\n##  7     366          365.    368.        378.\n##  8     378.         382.    368.        378.\n##  9     394          386.    368.        378.\n## 10     392.         402.    368.        378.\n## # … with 90 more rows"},{"path":"boot.html","id":"what-do-the-data-distributions-look-like","chapter":"8 Bootstrapping","heading":"What do the data distributions look like?","text":"","code":""},{"path":"boot.html","id":"what-do-the-sampling-distributions-look-like","chapter":"8 Bootstrapping","heading":"What do the sampling distributions look like?","text":"distributions median trimmed mean symmetric bell-shaped. However, trimmed mean normal distribution (evidenced points qq plot falling line y=x).","code":""},{"path":"boot.html","id":"what-does-the-boot-output-look-like","chapter":"8 Bootstrapping","heading":"What does the boot output look like?","text":"","code":"\nboot_stats <- map_df(1:n_rep1, ~boot_stat_func(df = heroin))\n\nboot_stats## # A tibble: 100 × 4\n##    boot_med boot_tr_mean obs_med obs_tr_mean\n##       <dbl>        <dbl>   <dbl>       <dbl>\n##  1     362          373.    368.        378.\n##  2     342.         345.    368.        378.\n##  3     388.         393.    368.        378.\n##  4     452          428.    368.        378.\n##  5     400.         400.    368.        378.\n##  6     348          363.    368.        378.\n##  7     399          405.    368.        378.\n##  8     394          398.    368.        378.\n##  9     358          359.    368.        378.\n## 10     299          332.    368.        378.\n## # … with 90 more rows"},{"path":"boot.html","id":"normal-ci-with-boot-se","chapter":"8 Bootstrapping","heading":"95% normal CI with Boot SE","text":"","code":"\nboot_stats %>%\n  summarize(low_med = mean(obs_med) + qnorm(0.025) * sd(boot_med), \n         up_med = mean(obs_med) + qnorm(0.975) * sd(boot_med),\n         low_tr_mean = mean(obs_tr_mean) + qnorm(0.025) * sd(boot_tr_mean), \n         up_tr_mean = mean(obs_tr_mean) + qnorm(0.975) * sd(boot_tr_mean))## # A tibble: 1 × 4\n##   low_med up_med low_tr_mean up_tr_mean\n##     <dbl>  <dbl>       <dbl>      <dbl>\n## 1    301.   434.        332.       425."},{"path":"boot.html","id":"percentile-ci","chapter":"8 Bootstrapping","heading":"95% Percentile CI","text":"","code":"\nboot_stats %>%\n  summarize(perc_CI_med = quantile(boot_med, c(0.025, 0.975)), \n            perc_CI_tr_mean = quantile(boot_tr_mean, c(0.025, 0.975)), \n            q = c(0.025, 0.975))## # A tibble: 2 × 3\n##   perc_CI_med perc_CI_tr_mean     q\n##         <dbl>           <dbl> <dbl>\n## 1        319.            329. 0.025\n## 2        448.            423. 0.975"},{"path":"boot.html","id":"bootstrap-t-ci","chapter":"8 Bootstrapping","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample). necessary bootstrap twice.re-resample function]double bootstrap!","code":"\nboot_2_func <- function(df, reps){\n  resample2 <- 1:reps\n  df %>%\n    summarize(boot_med = median(times), boot_tr_mean = mean(times, trim = 0.25)) %>%\n    cbind(resample2, map_df(resample2, ~df %>% \n             sample_frac(size=1, replace=TRUE) %>%\n             summarize(boot_2_med = median(times), \n                       boot_2_tr_mean = mean(times, trim = 0.25)))) %>%\n    select(resample2, everything())\n}\nboot_2_stats <- data.frame(resample1 = 1:n_rep1) %>%\n  mutate(first_boot = map(1:n_rep1, ~boot_1_func(df = heroin))) %>%\n  mutate(second_boot = map(first_boot, boot_2_func, reps = n_rep2)) "},{"path":"boot.html","id":"summarizing-the-double-bootstrap","chapter":"8 Bootstrapping","heading":"8.4.0.1 Summarizing the double bootstrap","text":"resultssummary resample 1summary resamples","code":"\nboot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) ## # A tibble: 476,000 × 12\n##    resample1    id clinic status times prison  dose resample2 boot_med\n##        <int> <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>     <int>    <dbl>\n##  1         1   137      2      0   563      0    70         1      372\n##  2         1    91      1      0   840      0    80         1      372\n##  3         1   250      1      1   117      0    40         1      372\n##  4         1   168      2      0   788      0    70         1      372\n##  5         1    67      1      1   386      0    60         1      372\n##  6         1     3      1      1   262      0    55         1      372\n##  7         1   104      2      0   713      0    50         1      372\n##  8         1   251      1      1   175      1    60         1      372\n##  9         1    68      1      0   439      0    80         1      372\n## 10         1   118      2      0   532      0    70         1      372\n## # … with 475,990 more rows, and 3 more variables: boot_tr_mean <dbl>,\n## #   boot_2_med <dbl>, boot_2_tr_mean <dbl>\nboot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) %>%\n  select(resample1, resample2, everything() ) %>%\n  filter(resample1 == 1) %>%\n  select(boot_med, boot_tr_mean, boot_2_med, boot_2_tr_mean) %>%\n  skim_without_charts() %>% as_tibble() %>% \n  select(skim_variable, numeric.mean, numeric.sd, numeric.p50)## # A tibble: 4 × 4\n##   skim_variable  numeric.mean numeric.sd numeric.p50\n##   <chr>                 <dbl>      <dbl>       <dbl>\n## 1 boot_med               372         0          372 \n## 2 boot_tr_mean           378.        0          378.\n## 3 boot_2_med             370.       46.6        365.\n## 4 boot_2_tr_mean         372.       24.7        370.\nboot_t_stats <- boot_2_stats %>%\n  unnest(second_boot) %>%\n  unnest(first_boot) %>%\n  group_by(resample1) %>%\n  summarize(boot_se_med = sd(boot_2_med),\n            boot_se_tr_mean = sd(boot_2_tr_mean),\n            boot_med = mean(boot_med),  # doesn't do anything, just copies over\n            boot_tr_mean = mean(boot_tr_mean))  %>% # the variables into the output\n  mutate(boot_t_med = (boot_med - mean(boot_med)) / boot_se_med,\n            boot_t_tr_mean = (boot_tr_mean - mean(boot_tr_mean)) / boot_se_tr_mean)\n\n  \nboot_t_stats## # A tibble: 100 × 7\n##    resample1 boot_se_med boot_se_tr_mean boot_med boot_tr_mean boot_t_med\n##        <int>       <dbl>           <dbl>    <dbl>        <dbl>      <dbl>\n##  1         1        46.6            24.7     372          378.     0.0222\n##  2         2        24.8            25.3     344          370.    -1.09  \n##  3         3        36.9            24.2     372.         380.     0.0145\n##  4         4        20.8            19.7     354          359.    -0.817 \n##  5         5        30.1            22.0     308          331.    -2.09  \n##  6         6        25.5            24.5     318          336.    -2.08  \n##  7         7        24.9            20.3     367          378.    -0.159 \n##  8         8        46.3            25.0     402          393.     0.670 \n##  9         9        32.3            20.8     388.         380.     0.512 \n## 10        10        18.1            15.1     381          384.     0.555 \n## # … with 90 more rows, and 1 more variable: boot_t_tr_mean <dbl>"},{"path":"boot.html","id":"bootstrap-t-ci-1","chapter":"8 Bootstrapping","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample).t-values]multiplierspull numbersBS-t CI","code":"\nboot_t_stats %>%\n  select(boot_t_med, boot_t_tr_mean)## # A tibble: 100 × 2\n##    boot_t_med boot_t_tr_mean\n##         <dbl>          <dbl>\n##  1     0.0222          0.140\n##  2    -1.09           -0.160\n##  3     0.0145          0.257\n##  4    -0.817          -0.767\n##  5    -2.09           -1.98 \n##  6    -2.08           -1.56 \n##  7    -0.159           0.202\n##  8     0.670           0.764\n##  9     0.512           0.280\n## 10     0.555           0.625\n## # … with 90 more rows\nboot_q <- boot_t_stats %>%\n  select(boot_t_med, boot_t_tr_mean) %>%\n  summarize(q_t_med = quantile(boot_t_med, c(0.025, 0.975)), \n            q_t_tr_mean = quantile(boot_t_tr_mean, c(0.025, 0.975)),\n            q = c(0.025, 0.975))\n\nboot_q## # A tibble: 2 × 3\n##   q_t_med q_t_tr_mean     q\n##     <dbl>       <dbl> <dbl>\n## 1   -2.08       -2.01 0.025\n## 2    2.25        2.03 0.975\nboot_q_med <- boot_q %>% select(q_t_med) %>% pull()\nboot_q_med##  2.5% 97.5% \n## -2.08  2.25\nboot_q_tr_mean <- boot_q %>% select(q_t_tr_mean) %>% pull()\nboot_q_tr_mean##  2.5% 97.5% \n## -2.01  2.03\nboot_t_stats %>%\n  summarize(boot_t_CI_med = mean(boot_med) + \n                                  boot_q_med*sd(boot_med),\n            boot_t_CI_tr_mean = mean(boot_tr_mean) + \n                                  boot_q_tr_mean * sd(boot_tr_mean),\n            q = c(0.025, 0.975))## # A tibble: 2 × 3\n##   boot_t_CI_med boot_t_CI_tr_mean     q\n##           <dbl>             <dbl> <dbl>\n## 1          306.              330. 0.025\n## 2          441.              418. 0.975"},{"path":"boot.html","id":"comparison-of-intervals","chapter":"8 Bootstrapping","heading":"8.4.0.2 Comparison of intervals","text":"first three columns correspond CIs true median survival times. second three columns correspond CIs true trimmed mean survival times.","code":""},{"path":"class.html","id":"class","chapter":"9 Classification","heading":"9 Classification","text":"Baumer (2015) provides concise explanation statistics data science work enhance ideas machine learning, one aspect classification:order understand machine learning, one must recognize differences mindset data miner statistician, notably characterized Breiman (2001), distinguished two types models f y, response variable, x, vector explanatory variables. One might consider data model f y \\(\\sim\\) f(x), assess whether f reasonably process generated y x, make inferences f. goal learn real process generated y x.Alternatively, one might construct algorithmic model f, \\(y \\sim f(x)\\), use f predict unobserved values y. can determined f fact good job predicting values y, one might care learn much f. former case, since want learn f, simpler model may preferred. Conversely, latter case, since want predict new values y, may indifferent model complexity (concerns overfitting scalability).Classification supervised learning technique extract general patterns data order build predictor new test validation data set. , model classify new points groups (numerical response values) based model built set data provides known group membership value. consider classifying categories (often one two categories) well predicting numeric variable (e.g., support vector machines linear regression).examples classification techniques include: linear regression, logistic regression, neural networks, classification trees, Random Forests, k-nearest neighbors, support vector machines, näive Bayes, linear discriminant analysis. cover methods bold.Simple Better (Fielding (2007), p. 87)want avoid -fitting model (certainly, bad idea model noise!)Future prediction performance goes many predictors.Simple models provide better insight causality specific associations.Fewer predictors implies fewer variables collect later studies.said, model still represent complexity data! describe trade-“bias-variance” trade-. order fully understand trade-, let’s first cover structure model building also classification method known \\(k\\)-Nearest Neighbors.","code":""},{"path":"class.html","id":"model-building-process","chapter":"9 Classification","heading":"9.1 Model Building Process","text":"classification prediction models basic steps. data preprocessed, model trained, model validated.variables information used train model fully tuned, processed, considered model, won’t matter sophisticated special model . Garbage , garbage .","code":""},{"path":"class.html","id":"cv","chapter":"9 Classification","heading":"9.1.1 Cross Validation","text":"","code":""},{"path":"class.html","id":"bias-variance-trade-off","chapter":"9 Classification","heading":"Bias-variance trade-off","text":"Excellent resourcefor explaining bias-variance trade-: http://scott.fortmann-roe.com/docs/BiasVariance.htmlVariance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Variance refers amount \\(\\hat{f}\\) change estimated using different training set. Generally, closer model fits data, variable (’ll different data set!). model many many explanatory variables often fit data closely.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Bias refers error introduced approximating “truth” model simple. example, often use linear models describe complex relationships, unlikely real life situation actually true linear model. However, true relationship close linear, linear model low bias.Generally, simpler model, lower variance. complicated model, lower bias. class, cross validation used assess model fit. [time permits, Receiver Operating Characteristic (ROC) curves also covered.]\\[\\begin{align}\n\\mbox{prediction error } = \\mbox{ irreducible error } + \\mbox{ bias } + \\mbox{ variance}\n\\end{align}\\]irreducible error irreducible error natural variability comes observations. matter good model , never able predict perfectly.bias bias model represents difference true model model simple. , complicated model (e.g., smaller \\(k\\) \\(k\\)NN), closer points prediction. model gets complicated (e.g., \\(k\\) decreases), bias goes .variance variance represents variability model sample sample. , simple model (big \\(k\\) \\(k\\)NN) change lot sample sample. variance decreases model becomes simple (e.g., \\(k\\) increases).Note bias-variance trade-. want prediction error small, choose model medium respect bias variance. control irreducible error.\nFigure 1.4: Test training error function model complexity. Note error goes monotonically training data. careful overfit!! (Hastie, Tibshirani, Friedman 2001)\nfollowing visualization excellent job communicating trade-bias variance function specific tuning parameter, : minimum node size classification tree. http://www.r2d3.us/visual-intro--machine-learning-part-2/","code":""},{"path":"class.html","id":"implementing-cross-validation","chapter":"9 Classification","heading":"Implementing Cross Validation","text":"\nFigure 9.1: (Flach 2012)\nCross validation typically used two ways.assess model’s accuracy (model assessment).build model (model selection).","code":""},{"path":"class.html","id":"different-ways-to-cv","chapter":"9 Classification","heading":"Different ways to CV","text":"Suppose build classifier given data set. ’d like know well model classifies observations, test samples hand, error rate much lower model’s inherent accuracy rate. Instead, ’d like predict new observations used create model. various ways creating test validation sets data:one training set, one test set [two drawbacks: estimate error highly variable depends points go training set; training data set smaller full data set, error rate biased way overestimates actual error rate modeling technique.]leave one cross validation (LOOCV)remove one observationbuild model using remaining n-1 pointspredict class membership observation removedrepeat removing observation one time\\(V\\)-fold cross validation (\\(V\\)-fold CV)\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]\nLOOCV special case \\(V\\)-fold CV \\(V=n\\)\nadvantage \\(V\\)-fold computational\n\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]\nlike LOOCV except algorithm run \\(V\\) times group (approximately equal size) partition data set.]LOOCV special case \\(V\\)-fold CV \\(V=n\\)advantage \\(V\\)-fold computational\\(V\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher (.e., based \\(n\\) data values). \\(V\\)-fold, prediction \\(n\\) values \\(V\\) models much less correlated. effect average predicted values way less variability data set data set.]","code":""},{"path":"class.html","id":"cv-for-model-assessment-10-fold","chapter":"9 Classification","heading":"CV for Model assessment 10-fold","text":"assume \\(k\\) given \\(k\\)-NNremove 10% databuild model using remaining 90%predict class membership / continuous response 10% observations removedrepeat removing decile one timea good measure model’s ability predict error rate associated predictions data independently predicted","code":""},{"path":"class.html","id":"cv-for-model-selection-10-fold","chapter":"9 Classification","heading":"CV for Model selection 10-fold","text":"set \\(k\\) \\(k\\)-NNbuild model using \\(k\\) value set :\nremove 10% data\nbuild model using remaining 90%\npredict class membership / continuous response 10% observations removed\nrepeat removing decile one time\nremove 10% databuild model using remaining 90%predict class membership / continuous response 10% observations removedrepeat removing decile one timemeasure CV prediction error \\(k\\) value handrepeat steps 1-3 choose \\(k\\) prediction error lowest","code":""},{"path":"class.html","id":"cv-for-model-assessment-and-selection-10-fold","chapter":"9 Classification","heading":"CV for Model assessment and selection 10-fold","text":", one approach use test/training data CV order model assessment selection. Note CV used steps, algorithm slightly complicated.split data training test observationsset \\(k\\) \\(k\\)-NNbuild model using \\(k\\) value set training data:\nremove 10% training data\nbuild model using remaining 90% training data\npredict class membership / continuous response 10% training observations removed\nrepeat removing decile one time training data\nremove 10% training databuild model using remaining 90% training datapredict class membership / continuous response 10% training observations removedrepeat removing decile one time training datameasure CV prediction error \\(k\\) value hand training datarepeat steps 2-4 choose \\(k\\) prediction error lowest training datausing \\(k\\) value given step 5, assess prediction error test data\nFigure 9.2: Nested cross-validation: two cross-validation loops run one inside . (Varoquaux et al. 2017)\n","code":""},{"path":"class.html","id":"tidymodels","chapter":"9 Classification","heading":"9.1.2 tidymodels","text":"tidymodels framework provides series steps allow systematic model building. steps :partition databuild recipeselect modelcreate workflowfit modelvalidate model","code":""},{"path":"class.html","id":"partition-the-data","chapter":"9 Classification","heading":"1. Partition the data","text":"Put testing data pocket (keep secret R!!)\nFigure 9.3: Image credit: Julia Silge\n","code":""},{"path":"class.html","id":"build-a-recipe","chapter":"9 Classification","heading":"2. build a recipe","text":"Start recipe()Define variables involvedDescribe preprocessing step--stepfeature engineering preprocessing:feature engineering process transforming raw data features (variables) better predictors (model hand).Examples include:create new variables (e.g., combine levels -> state region)transform variable (e.g., log, polar coordinates)continuous variables -> discrete (e.g., binning)numerical categorical data -> factors / character strings (one hot encoding)time -> discretized timemissing values -> imputedNA -> levelcontinuous variables -> center & scale (“normalize”)step_ functionsFor information: https://recipes.tidymodels.org/reference/index.html","code":"\nls(pattern = '^step_', env = as.environment('package:recipes'))##  [1] \"step_arrange\"       \"step_bagimpute\"     \"step_bin2factor\"   \n##  [4] \"step_BoxCox\"        \"step_bs\"            \"step_center\"       \n##  [7] \"step_classdist\"     \"step_corr\"          \"step_count\"        \n## [10] \"step_cut\"           \"step_date\"          \"step_depth\"        \n## [13] \"step_discretize\"    \"step_downsample\"    \"step_dummy\"        \n## [16] \"step_factor2string\" \"step_filter\"        \"step_geodist\"      \n## [19] \"step_holiday\"       \"step_hyperbolic\"    \"step_ica\"          \n## [22] \"step_impute_bag\"    \"step_impute_knn\"    \"step_impute_linear\"\n## [25] \"step_impute_lower\"  \"step_impute_mean\"   \"step_impute_median\"\n## [28] \"step_impute_mode\"   \"step_impute_roll\"   \"step_indicate_na\"  \n## [31] \"step_integer\"       \"step_interact\"      \"step_intercept\"    \n## [34] \"step_inverse\"       \"step_invlogit\"      \"step_isomap\"       \n## [37] \"step_knnimpute\"     \"step_kpca\"          \"step_kpca_poly\"    \n## [40] \"step_kpca_rbf\"      \"step_lag\"           \"step_lincomb\"      \n## [43] \"step_log\"           \"step_logit\"         \"step_lowerimpute\"  \n## [46] \"step_meanimpute\"    \"step_medianimpute\"  \"step_modeimpute\"   \n## [49] \"step_mutate\"        \"step_mutate_at\"     \"step_naomit\"       \n## [52] \"step_nnmf\"          \"step_normalize\"     \"step_novel\"        \n## [55] \"step_ns\"            \"step_num2factor\"    \"step_nzv\"          \n## [58] \"step_ordinalscore\"  \"step_other\"         \"step_pca\"          \n## [61] \"step_pls\"           \"step_poly\"          \"step_profile\"      \n## [64] \"step_range\"         \"step_ratio\"         \"step_regex\"        \n## [67] \"step_relevel\"       \"step_relu\"          \"step_rename\"       \n## [70] \"step_rename_at\"     \"step_rm\"            \"step_rollimpute\"   \n## [73] \"step_sample\"        \"step_scale\"         \"step_select\"       \n## [76] \"step_shuffle\"       \"step_slice\"         \"step_spatialsign\"  \n## [79] \"step_sqrt\"          \"step_string2factor\" \"step_unknown\"      \n## [82] \"step_unorder\"       \"step_upsample\"      \"step_window\"       \n## [85] \"step_YeoJohnson\"    \"step_zv\""},{"path":"class.html","id":"select-a-model","chapter":"9 Classification","heading":"3. select a model","text":"specify model:pick modelset mode (regression vs classification, needed)set engineExamples engines classification algorithms cover class:","code":"\nshow_engines(\"nearest_neighbor\")## # A tibble: 2 × 2\n##   engine mode          \n##   <chr>  <chr>         \n## 1 kknn   classification\n## 2 kknn   regression\nshow_engines(\"decision_tree\")## # A tibble: 5 × 2\n##   engine mode          \n##   <chr>  <chr>         \n## 1 rpart  classification\n## 2 rpart  regression    \n## 3 C5.0   classification\n## 4 spark  classification\n## 5 spark  regression\nshow_engines(\"rand_forest\")## # A tibble: 6 × 2\n##   engine       mode          \n##   <chr>        <chr>         \n## 1 ranger       classification\n## 2 ranger       regression    \n## 3 randomForest classification\n## 4 randomForest regression    \n## 5 spark        classification\n## 6 spark        regression\nshow_engines(\"svm_poly\")## # A tibble: 2 × 2\n##   engine  mode          \n##   <chr>   <chr>         \n## 1 kernlab classification\n## 2 kernlab regression\nshow_engines(\"svm_rbf\")## # A tibble: 4 × 2\n##   engine    mode          \n##   <chr>     <chr>         \n## 1 kernlab   classification\n## 2 kernlab   regression    \n## 3 liquidSVM classification\n## 4 liquidSVM regression\nshow_engines(\"linear_reg\")## # A tibble: 5 × 2\n##   engine mode      \n##   <chr>  <chr>     \n## 1 lm     regression\n## 2 glmnet regression\n## 3 stan   regression\n## 4 spark  regression\n## 5 keras  regression"},{"path":"class.html","id":"create-a-workflow","chapter":"9 Classification","heading":"4. Create a workflow","text":"workflow combines model / engine recipe.","code":""},{"path":"class.html","id":"fit-the-model","chapter":"9 Classification","heading":"5. Fit the model","text":"Putting together, fit() give model specifications.","code":""},{"path":"class.html","id":"validate-the-model","chapter":"9 Classification","heading":"6. Validate the model","text":"model parametersSome model parameters tuned data (aren’t).\nlinear model coefficients optimized (tuned)\nk-nn value “k” tuned\nmodel parameters tuned data (aren’t).linear model coefficients optimized (tuned)k-nn value “k” tunedIf model tuned using data, data used assess model.model tuned using data, data used assess model.Cross Validation, iteratively put data pocket.Cross Validation, iteratively put data pocket.example, keep 1/5 data pocket, build model remaining 4/5 data.example, keep 1/5 data pocket, build model remaining 4/5 data.Cross validation tuning parameters. Note cross validation done training data.\nFigure 1.6: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 1.7: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.4: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.5: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.6: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.7: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.8: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.9: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.10: Image credit: Alison Hill\n\\[\\bigg\\Downarrow\\]\nFigure 9.11: Image credit: Alison Hill\n","code":""},{"path":"class.html","id":"reflecting-on-model-building","chapter":"9 Classification","heading":"Reflecting on Model Building","text":"Tidy Modeling R, Kuhn Silge walk example entire model building process. Note stages visited often coming appropriate model.\nFigure 9.12: Image credit: https://www.tmwr.org/\n\nFigure 9.13: Image credit: https://www.tmwr.org/\n\nFigure 9.14: Image credit: https://www.tmwr.org/\n","code":""},{"path":"class.html","id":"r-model-penguins","chapter":"9 Classification","heading":"9.1.3 R model: penguins","text":"\nFigure 9.15: Image credit: Alison Hill\n","code":"\npenguins## # A tibble: 344 × 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # … with 334 more rows, and 2 more variables: sex <fct>, year <int>"},{"path":"class.html","id":"partition-the-data-1","chapter":"9 Classification","heading":"1. Partition the data","text":"","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)"},{"path":"class.html","id":"build-a-recipe-1","chapter":"9 Classification","heading":"2. build a recipe","text":"","code":"\npenguin_recipe <-\n  recipe(body_mass_g ~ species + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm + sex + year,\n         data = penguin_train) %>%\n  step_mutate(year = as.factor(year)) %>%\n  step_unknown(sex, new_level = \"unknown\") %>%\n  step_relevel(sex, ref_level = \"female\") %>%\n  update_role(island, new_role = \"id variable\")\nsummary(penguin_recipe)## # A tibble: 8 × 4\n##   variable          type    role        source  \n##   <chr>             <chr>   <chr>       <chr>   \n## 1 species           nominal predictor   original\n## 2 island            nominal id variable original\n## 3 bill_length_mm    numeric predictor   original\n## 4 bill_depth_mm     numeric predictor   original\n## 5 flipper_length_mm numeric predictor   original\n## 6 sex               nominal predictor   original\n## 7 year              numeric predictor   original\n## 8 body_mass_g       numeric outcome     original"},{"path":"class.html","id":"select-a-model-1","chapter":"9 Classification","heading":"3. select a model","text":"","code":"\npenguin_lm <- linear_reg() %>%\n  set_engine(\"lm\")\npenguin_lm## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"class.html","id":"create-a-workflow-1","chapter":"9 Classification","heading":"4. Create a workflow","text":"","code":"\npenguin_wflow <- workflow() %>%\n  add_model(penguin_lm) %>%\n  add_recipe(penguin_recipe)\npenguin_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 3 Recipe Steps\n## \n## • step_mutate()\n## • step_unknown()\n## • step_relevel()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm"},{"path":"class.html","id":"fit-the-model-1","chapter":"9 Classification","heading":"5. Fit the model","text":"","code":"\npenguin_fit <- penguin_wflow %>%\n  fit(data = penguin_train)\npenguin_fit %>% tidy()## # A tibble: 10 × 5\n##    term              estimate std.error statistic  p.value\n##    <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n##  1 (Intercept)        -2417.     665.      -3.64  3.36e- 4\n##  2 speciesChinstrap    -208.      92.9     -2.24  2.58e- 2\n##  3 speciesGentoo        985.     152.       6.48  5.02e-10\n##  4 bill_length_mm        13.5      8.29     1.63  1.04e- 1\n##  5 bill_depth_mm         80.9     22.1      3.66  3.10e- 4\n##  6 flipper_length_mm     20.8      3.62     5.74  2.81e- 8\n##  7 sexmale              351.      52.6      6.67  1.72e-10\n##  8 sexunknown            47.6    103.       0.460 6.46e- 1\n##  9 year2008             -24.8     47.5     -0.521 6.03e- 1\n## 10 year2009             -61.9     46.0     -1.35  1.80e- 1"},{"path":"class.html","id":"cross-validation","chapter":"9 Classification","heading":"6. Cross validation","text":"(See Section 9.1.1 future R examples full description cross validation.)","code":""},{"path":"class.html","id":"knn","chapter":"9 Classification","heading":"9.2 \\(k\\)-Nearest Neighbors","text":"\\(k\\)-Nearest Neighbor algorithm exactly sounds like .user decides integer value \\(k\\)user decides integer value \\(k\\)user decides distance metric (\\(k\\)-NN algorithms default Euclidean distance)user decides distance metric (\\(k\\)-NN algorithms default Euclidean distance)point classified group majority \\(k\\) closest points training data.point classified group majority \\(k\\) closest points training data.","code":""},{"path":"class.html","id":"k-nn-algorithm","chapter":"9 Classification","heading":"9.2.1 \\(k\\)-NN algorithm","text":"Decide distance metric (e.g., Euclidean distance, 1 - correlation, etc.) find distances point test set point training set. distance measured feature space, , respect explanatory variables (response variable).n.b. machine learning algorithms use “distance” measure, “distance” required mathematical distance metric. Indeed, 1-correlation common distance measure, fails triangle inequality.Consider point test set. Find \\(k\\) closest points training set one test observation.Consider point test set. Find \\(k\\) closest points training set one test observation.Using majority vote, find dominate class \\(k\\) closest points. Predict class label test observation.Using majority vote, find dominate class \\(k\\) closest points. Predict class label test observation.Note: response variable continuous (instead categorical), find average response variable \\(k\\) training point predicted response one test observation.Shortcomings \\(k\\)-NN:one class can dominate large majorityEuclidean distance dominated scaleit can computationally unwieldy (unneeded!!) calculate distances (algorithms search smartly)output doesn’t provide information explanatory variables informative.doesn’t work well large datasets (cost prediction high, model doesn’t always find structure)doesn’t work well high dimensions (curse dimensionality – distance becomes meaningless high dimensions)need lot feature scalingsensitive noise outliersStrengths \\(k\\)-NN:can easily work number categories (outcome variable)can predict quantitative response variablethe bias 1-NN often low (variance high)distance metric can used (algorithm models data appropriately)method straightforward implement / understandthere training period (.e., discrimination function created)model nonparametric (distributional assumptions data)great model imputing missing data","code":""},{"path":"class.html","id":"r-k-nn-penguins","chapter":"9 Classification","heading":"9.2.2 R k-NN: penguins","text":"fit \\(k\\)-Nearest Neighbor algorithm penguins dataset. previously (come), ’ll use entire tidymodels workflow including partitioning data, build recipe, select model, create workflow, fit model, validate model","code":"\nlibrary(GGally) # for plotting\nlibrary(tidymodels)\ndata(penguins)"},{"path":"class.html","id":"penguin-data","chapter":"9 Classification","heading":"penguin data","text":"","code":"\nggpairs(penguins, mapping = ggplot2::aes(color = species), alpha=.4)"},{"path":"class.html","id":"k-nn-to-predict-penguin-species","chapter":"9 Classification","heading":"\\(k\\)-NN to predict penguin species","text":"","code":""},{"path":"class.html","id":"partition-the-data-2","chapter":"9 Classification","heading":"1. Partition the data","text":"","code":""},{"path":"class.html","id":"build-a-recipe-2","chapter":"9 Classification","heading":"2. Build a recipe","text":"","code":"\npenguin_knn_recipe <-\n  recipe(species ~ body_mass_g + island + bill_length_mm + \n           bill_depth_mm + flipper_length_mm,\n         data = penguin_train) %>%\n  update_role(island, new_role = \"id variable\") %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_knn_recipe)## # A tibble: 6 × 4\n##   variable          type    role        source  \n##   <chr>             <chr>   <chr>       <chr>   \n## 1 body_mass_g       numeric predictor   original\n## 2 island            nominal id variable original\n## 3 bill_length_mm    numeric predictor   original\n## 4 bill_depth_mm     numeric predictor   original\n## 5 flipper_length_mm numeric predictor   original\n## 6 species           nominal outcome     original"},{"path":"class.html","id":"select-a-model-2","chapter":"9 Classification","heading":"3. Select a model","text":"(note ’ve used default number neighbors (\\(k=7\\)).)","code":"\npenguin_knn <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\npenguin_knn## K-Nearest Neighbor Model Specification (classification)\n## \n## Computational engine: kknn"},{"path":"class.html","id":"create-a-workflow-2","chapter":"9 Classification","heading":"4. Create a workflow","text":"","code":"\npenguin_knn_wflow <- workflow() %>%\n  add_model(penguin_knn) %>%\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (classification)\n## \n## Computational engine: kknn"},{"path":"class.html","id":"fit-predict","chapter":"9 Classification","heading":"5. Fit (/ predict)","text":"next R code chunk break pieces – , run line one time.","code":"\npenguin_knn_fit <- penguin_knn_wflow %>%\n  fit(data = penguin_train)\npenguin_knn_fit %>% \n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  metrics(truth = species, estimate = .pred_class) %>%\n  filter(.metric == \"accuracy\")## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.988"},{"path":"class.html","id":"what-is-k","chapter":"9 Classification","heading":"What is \\(k\\)?","text":"turns default value \\(k\\) kknn engine 7. 7 best?Cross Validation!!!red observations used fit model, black observations used assess model.\nFigure 9.16: Image credit: Alison Hill\nsaw , cross validation randomly splits training data V distinct blocks roughly equal size.leave first block analysis data fit model.model used predict held-block assessment data.continue process V assessment blocks predictedThe final performance based hold-predictions averaging statistics V blocks.","code":""},{"path":"class.html","id":"b.-a-new-partition-of-the-training-data","chapter":"9 Classification","heading":"1b. A new partition of the training data","text":"","code":"\nset.seed(470)\npenguin_vfold <- vfold_cv(penguin_train,\n                          v = 3, strata = species)"},{"path":"class.html","id":"select-a-model-3","chapter":"9 Classification","heading":"3. Select a model","text":"Now knn model uses tune() indicate actually don’t know many neighbors use.","code":"\npenguin_knn_tune <- nearest_neighbor(neighbors = tune()) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")"},{"path":"class.html","id":"re-create-a-workflow","chapter":"9 Classification","heading":"4. Re-create a workflow","text":"time, use model set number neighbors.","code":"\npenguin_knn_wflow_tune <- workflow() %>%\n  add_model(penguin_knn_tune) %>%\n  add_recipe(penguin_knn_recipe)"},{"path":"class.html","id":"fit-the-model-2","chapter":"9 Classification","heading":"5. Fit the model","text":"model fit three folds created value \\(k\\) k_grid.","code":"\nk_grid <- data.frame(neighbors = seq(1, 15, by = 4))\nk_grid##   neighbors\n## 1         1\n## 2         5\n## 3         9\n## 4        13\npenguin_knn_wflow_tune %>%\n  tune_grid(resamples = penguin_vfold, \n           grid = k_grid) %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\")## # A tibble: 4 × 7\n##   neighbors .metric  .estimator  mean     n   std_err .config             \n##       <dbl> <chr>    <chr>      <dbl> <int>     <dbl> <chr>               \n## 1         1 accuracy multiclass 0.971     2 0.00595   Preprocessor1_Model1\n## 2         5 accuracy multiclass 0.977     2 0.000134  Preprocessor1_Model2\n## 3         9 accuracy multiclass 0.988     2 0.0000668 Preprocessor1_Model3\n## 4        13 accuracy multiclass 0.983     2 0.00568   Preprocessor1_Model4"},{"path":"class.html","id":"validate-the-model-1","chapter":"9 Classification","heading":"6. Validate the model","text":"Using \\(k\\) = 9, model re-trained training data tested test data (estimate overall model accuracy).","code":""},{"path":"class.html","id":"select-a-model-4","chapter":"9 Classification","heading":"3. select a model","text":"","code":"\npenguin_knn_final <- nearest_neighbor(neighbors = 9) %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\npenguin_knn_final## K-Nearest Neighbor Model Specification (classification)\n## \n## Main Arguments:\n##   neighbors = 9\n## \n## Computational engine: kknn"},{"path":"class.html","id":"create-a-workflow-3","chapter":"9 Classification","heading":"4. create a workflow","text":"","code":"\npenguin_knn_wflow_final <- workflow() %>%\n  add_model(penguin_knn_final) %>%\n  add_recipe(penguin_knn_recipe)\n\npenguin_knn_wflow_final## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (classification)\n## \n## Main Arguments:\n##   neighbors = 9\n## \n## Computational engine: kknn"},{"path":"class.html","id":"fit-the-model-3","chapter":"9 Classification","heading":"5. fit the model","text":"","code":"\npenguin_knn_fit_final <- penguin_knn_wflow_final %>%\n  fit(data = penguin_train)"},{"path":"class.html","id":"validate-the-model-2","chapter":"9 Classification","heading":"6. validate the model","text":"Huh. Seems like \\(k=9\\) didn’t well \\(k=7\\) (value tried beginning cross validating).Well, turns , ’s nature variability, randomness, model building.don’t know truth, won’t every find perfect model.","code":"\npenguin_knn_fit_final %>% \n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  metrics(truth = species, estimate = .pred_class) %>%\n  filter(.metric == \"accuracy\")## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.977"},{"path":"class.html","id":"cart","chapter":"9 Classification","heading":"9.3 Decision Trees","text":"Stephanie Yee Tony Chu created following (amazing!) demonstration tree intuition. Step--step, build recursive binary tree order model differences homes SF homes NYC.\nFigure 9.17: http://www.r2d3.us/visual-intro--machine-learning-part-1/ visual introduction machine learning.\nDecision trees used sorts predictive descriptive models. NYT created recursive binary decision tree show patterns identity political affiliation.\nFigure 9.18: https://www.nytimes.com/interactive/2019/08/08/opinion/sunday/party-polarization-quiz.html Quiz: Let Us Predict Whether ’re Democrat Republican NYT, Aug 8, 2019. Note race first dominant node, followed religion.\n","code":""},{"path":"class.html","id":"cart-algorithm","chapter":"9 Classification","heading":"9.3.1 CART algorithm","text":"Basic Classification Regression Trees (CART) Algorithm:Start observations one group.Find variable/split best separates response variable (successive binary partitions based different predictors / explanatory variables).\nEvaluation “homogeneity” within group\nDivide data two groups (“leaves”) split (“node”).\nWithin split, find best variable/split separates outcomes.\nEvaluation “homogeneity” within groupDivide data two groups (“leaves”) split (“node”).Within split, find best variable/split separates outcomes.Continue groups small sufficiently “pure.”Prune tree.Shortcomings CART:Straight CART generally predictive accuracy classification approaches. (improve model - see Random Forests, boosting, bagging)Difficult write / consider CART “model”Without proper pruning, model can easily lead overfittingWith lots predictors, (even greedy) partitioning can become computationally unwieldyOften, prediction performance poorStrengths CART:easy explain; trees easy display graphically (make easy interpret). (mirror typical human decision-making process.)Can handle categorical numerical predictors response variables (indeed, can handle mixed predictors time!).Can handle 2 groups categorical predictionsEasily ignore redundant variables.Perform better linear models non-linear settings. Classification trees non-linear models, immediately use interactions variables.Data transformations may less important (monotone transformations explanatory variables won’t change anything).","code":""},{"path":"class.html","id":"classification-trees","chapter":"9 Classification","heading":"Classification Trees","text":"classification tree used predict categorical response variable (rather quantitative one). end predicted value one commonly occurring class training observations region belongs. goal create regions homogeneous possible respect response variable - categories.measures impurityCalculate classification error rate fraction training observations region belong common class: \\[E_m = 1 - \\max_k(\\hat{p}_{mk})\\]\n\\(\\hat{p}_{mk}\\) represents proportion training observations \\(m\\)th region \\(k\\)th class. However, classification error rate particularly sensitive node purity, two additional measures typically used partition regions., Gini index defined \\[G_m= \\sum_{k=1}^K \\hat{p}_{mk}(1-\\hat{p}_{mk})\\]\nmeasure total variance across \\(K\\) classes. [Recall, variance Bernoulli random variable \\(\\pi\\) = P(success) \\(\\pi(1-\\pi)\\).] Note Gini index takes small value \\(\\hat{p}_{mk}\\) values close zero one. reason, Gini index referred measure node purity - small value indicates node contains predominantly observations single class.Last, cross-entropy defined \\[D_m = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}\\]\nSince \\(0 \\leq \\hat{p}_{mk} \\leq 1\\) follows \\(0 \\leq -\\hat{p}_{mk} \\log\\hat{p}_{mk}\\). One can show cross-entropy take value near zero \\(\\hat{p}_{mk}\\) values near zero near one. Therefore, like Gini index, cross-entropy take small value \\(m\\)th node pure.build tree, typically Gini index cross-entropy used evaluate particular split.prune tree, often classification error used (accuracy final pruned tree goal)Computationally, usually infeasible consider every possible partition observations. Instead looking partitions, perform top approach problem known recursive binary splitting (greedy look current split outcomes splits come).Recursive Binary Splitting Categories (given node)Select predictor \\(X_j\\) cutpoint \\(s\\) splitting predictor space regions \\(\\{X | X_j< s\\}\\) \\(\\{X | X_j \\geq s\\}\\) lead greatest reduction Gini index cross-entropy.\\(j\\) \\(s\\), define pair half-planes \n\\[R_1(j,s) = \\{X | X_j < s\\} \\mbox{ } R_2(j,s) = \\{X | X_j \\geq s\\}\\]\nseek value \\(j\\) \\(s\\) minimize equation:\n\\[\\begin{align}\n& \\sum_{:x_i \\R_1(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + \\sum_{:x_i \\R_2(j,s)} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\mbox{equivalently: } & n_{R_1} \\sum_{k=1}^K \\hat{p}_{{R_1}k}(1-\\hat{p}_{{R_1}k}) + n_{R_2} \\sum_{k=1}^K \\hat{p}_{{R_2}k}(1-\\hat{p}_{{R_2}k})\\\\\n\\end{align}\\]Repeat process, looking best predictor best cutpoint within one previously identified regions (producing three regions, now).Keep repeating process stopping criterion reached - example, region contains 5 observations.","code":""},{"path":"class.html","id":"regression-trees","chapter":"9 Classification","heading":"Regression Trees","text":"goal algorithm regression tree split set possible value data \\(|T|\\) distinct non-overlapping regions, \\(R_1, R_2, \\ldots, R_{|T|}\\). every observation falls region \\(R_m\\), make prediction - mean response values training observations \\(R_m\\). find regions \\(R_1, \\ldots, R_{|T|}\\)?\\(\\Rightarrow\\) Minimize RSS, \\[RSS = \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2\\]\n\\(\\overline{y}_{R_m}\\) mean response training observations within \\(m\\)th region.(Note: chapter (James et al. 2021) refer MSE - mean squared error - addition RSS MSE simply RSS / n, see equation (2.5).), usually infeasible consider every possible partition observations. Instead looking partitions, perform top approach problem known recursive binary splitting (greedy look current split outcomes splits come).Recursive Binary Splitting Numerical Response (given node)Select predictor \\(X_j\\) cutpoint \\(s\\) splitting predictor space regions \\(\\{X | X_j< s\\}\\) \\(\\{X | X_j \\geq s\\}\\) lead greatest reduction RSS.\\(j\\) \\(s\\), define pair half-planes \n\\[R_1(j,s) = \\{X | X_j < s\\} \\mbox{ } R_2(j,s) = \\{X | X_j \\geq s\\}\\]\nsee value \\(j\\) \\(s\\) minimize equation:\n\\[\\sum_{:x_i \\R_1(j,s)} (y_i - \\overline{y}_{R_1})^2 + \\sum_{:x_i \\R_2(j,s)} (y_i - \\overline{y}_{R_2})^2\\]\n\\(\\overline{y}_{R_1}\\) mean response training observations \\(R_1(j,s)\\) \\(\\overline{y}_{R_2}\\) mean response training observations \\(R_2(j,s)\\).Repeat process, looking best predictor best cutpoint within one previously identified regions (producing three regions, now).Keep repeating process stopping criterion reached - example, region contains 5 observations.","code":""},{"path":"class.html","id":"avoiding-overfitting","chapter":"9 Classification","heading":"(Avoiding) Overfitting","text":"Ideally, tree overfit training data. One imagine easy grow tree training data end terminal nodes completely homogeneous (don’t represent test data).See following (amazing!) demonstration intuition model validation / overfitting: http://www.r2d3.us/visual-intro--machine-learning-part-2/One possible algorithm building tree split based reduction RSS (Gini index, etc.) exceeding (presumably high) threshold. However, strategy known short sighted, split later tree may contain large amount information. better strategy grow large tree \\(T_0\\) prune back order obtain subtree. Use cross validation build subtree overfit data.Algorithm: Building Regression TreeUse recursive binary splitting grow large tree training data, stopping terminal node fewer minimum number observations.Apply cost complexity pruning large tree order obtain sequence best subtrees, function \\(\\alpha\\).Use \\(V\\)-fold cross-validation choose \\(\\alpha\\). , divide training observations \\(V\\) folds. \\(v=1, 2, \\ldots, V\\):\nRepeat Steps 1 2 \\(V\\)th fold training data.\nEvaluate mean squared prediction error data left-\\(k\\)th fold, function \\(\\alpha\\).\nvalue \\(\\alpha\\), average prediction error (either misclassification RSS), pick \\(\\alpha\\) minimize average error.\nRepeat Steps 1 2 \\(V\\)th fold training data.Evaluate mean squared prediction error data left-\\(k\\)th fold, function \\(\\alpha\\).\nvalue \\(\\alpha\\), average prediction error (either misclassification RSS), pick \\(\\alpha\\) minimize average error.Return subtree Step 2 corresponds chosen value \\(\\alpha\\).","code":""},{"path":"class.html","id":"cost-complexity-pruning","chapter":"9 Classification","heading":"Cost Complexity Pruning","text":"Also known weakest link pruning, idea consider sequence trees indexed nonnegative tuning parameter \\(\\alpha\\) (instead considering every single subtree). Generally, idea cost larger (complex!) tree. define cost complexity criterion (\\(\\alpha > 0\\)):\n\\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\]\n\\(k(m)\\) class majority observations node \\(m\\) \\(|T|\\) number terminal nodes tree.\\(\\alpha\\) small: \\(\\alpha\\) set small, saying risk worrisome complexity larger trees favored reduce risk.\\(\\alpha\\) large: \\(\\alpha\\) set large, complexity tree worrisome smaller trees favored.way think cost complexity consider \\(\\alpha\\) increasing. \\(\\alpha\\) gets bigger, “best” tree smaller. test error monotonically related size training tree.note \\(\\alpha\\)text (Introduction Statistical Learning) almost everywhere else might look, cost complexity defined previous slides.However, might notice R cost_complexity value typically less 1. can tell, value function minimized R average squared errors missclassification rate.\\[\\begin{align}\n\\mbox{numerical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2 + \\alpha \\cdot |T|\\\\\n\\mbox{categorical: } C_\\alpha(T) &= \\frac{1}{n}\\sum_{m=1}^{|T|} \\sum_{\\R_m} (y_i \\ne k(m)) + \\alpha \\cdot |T|\n\\end{align}\\]","code":""},{"path":"class.html","id":"variations-on-a-theme","chapter":"9 Classification","heading":"Variations on a theme","text":"main ideas consistent throughout CART algorithms. However, exact details implementation can change function function, often times difficult decipher exactly equation used. tree function R, much decision making done deviance defined :\\[\\mbox{numerical: deviance} = \\sum_{m=1}^{|T|}  \\sum_{\\R_m} (y_i - \\overline{y}_{R_m})^2\\]\\[\\mbox{categorical: deviance} = -2\\sum_{m=1}^{|T|} \\sum_{k=1}^K n_{mk} \\log \\hat{p}_{mk}\\]CART algorithm, minimize deviance (types variables). categorical deviance small observations majority group (high proportion). Also, \\(\\lim_{\\epsilon \\rightarrow 0} \\epsilon \\log(\\epsilon) = 0\\). Additionally, methods cross validation can also vary. particular, number variables large, tree algorithm can slow cross validation process - choice \\(\\alpha\\) - needs efficient.","code":""},{"path":"class.html","id":"cv-for-model-building-and-model-assessment","chapter":"9 Classification","heading":"CV for model building and model assessment","text":"Notice CV used model building model assessment. possible (practical, though quite computational!) use practices classification model. algorithm follows.Algorithm: CV \\(V_1\\)-fold CV building \\(V_2\\)-fold CV assessmentPartition data \\(V_1\\) groups.Remove first group, train data remaining \\(V_1-1\\) groups.Use \\(V_2\\)-fold cross-validation (\\(V_1-1\\) groups) choose \\(\\alpha\\). , divide training observations \\(V_2\\) folds find \\(\\alpha\\) minimizes error.Using subtree corresponds chosen value \\(\\alpha\\), predict first \\(V_1\\) hold samples.Repeat steps 2-4 using remaining \\(V_1 - 1\\) groups.","code":""},{"path":"class.html","id":"r-cart-example","chapter":"9 Classification","heading":"9.3.2 R CART Example","text":"Census Bureau divides country “tracts” approximately\nequal population. 1990 Census, California divided 20640 tracts. One data sets (houses http://lib.stat.cmu.edu/datasets/; http://lib.stat.cmu.edu/datasets/houses.zip) records following tract California: Median house price, median house age, total number rooms, total number bedrooms, total number occupants, total number houses, median income (thousands dollars), latitude longitude. appeared Pace Barry (1997), “Sparse Spatial Autoregressions,” Statistics Probability Letters.","code":""},{"path":"class.html","id":"classification-and-regression-trees","chapter":"9 Classification","heading":"Classification and Regression Trees","text":"Classification Trees used predict response class \\(Y\\) input \\(X_1, X_2, \\ldots, X_n\\). continuous response ’s called regression tree, categorical, ’s called classification tree. node tree, check value one input \\(X_i\\) depending (binary) answer continue left right subbranch. reach leaf find prediction (usually simple statistic dataset leaf represents, like common value available classes).Note maxdepth: might expect, maxdepth indicates longest length root tree terminal node. However, rpart (particular, using rpart rpart2 caret), default settings keep tree growing way singular nodes, even high maxdepth.","code":""},{"path":"class.html","id":"regression-trees-1","chapter":"9 Classification","heading":"Regression Trees","text":"technical reasons (e.g., see ), step_log() outcome variable step gives problems predictions end. Therefore, mutate outcome variable within dataset starting model building process.","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\n# recipe\nhouse_cart_recipe <-\n  recipe(logValue ~ Longitude + Latitude ,\n         data = house_train)\n# model\nhouse_cart <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_wflow <- workflow() %>%\n  add_model(house_cart) %>%\n  add_recipe(house_cart_recipe)\n\n# fit\nhouse_cart_fit <- house_cart_wflow %>%\n  fit(data = house_train)"},{"path":"class.html","id":"model-output","chapter":"9 Classification","heading":"Model Output","text":"following scatter plot can made CART built using two numerical predictor variables.","code":"\nhouse_cart_fit## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 15480 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##   1) root 15480 5024.405000 12.08947  \n##     2) Latitude>=38.485 1541  283.738200 11.59436  \n##       4) Latitude>=39.355 506   48.267930 11.31530 *\n##       5) Latitude< 39.355 1035  176.803400 11.73079 *\n##     3) Latitude< 38.485 13939 4321.152000 12.14421  \n##       6) Longitude>=-121.645 10454 3320.946000 12.06198  \n##        12) Latitude>=34.635 2166  491.986400 11.52110  \n##          24) Longitude>=-120.265 1083  166.051200 11.28432 *\n##          25) Longitude< -120.265 1083  204.505800 11.75787 *\n##        13) Latitude< 34.635 8288 2029.685000 12.20333  \n##          26) Longitude>=-118.315 6240 1373.830000 12.09295  \n##            52) Longitude>=-117.575 2130  516.313400 11.87918  \n##             104) Latitude>=33.605 821  123.684300 11.64002 *\n##             105) Latitude< 33.605 1309  316.218800 12.02918  \n##               210) Longitude>=-116.33 97    8.931327 11.17127 *\n##               211) Longitude< -116.33 1212  230.181300 12.09784  \n##                 422) Longitude>=-117.165 796  101.805300 11.94935 *\n##                 423) Longitude< -117.165 416   77.245280 12.38196 *\n##            53) Longitude< -117.575 4110  709.740000 12.20373  \n##             106) Latitude>=33.735 3529  542.838300 12.14908  \n##               212) Latitude< 34.105 2931  379.526800 12.09154  \n##                 424) Longitude< -118.165 1114  147.375800 11.91911 *\n##                 425) Longitude>=-118.165 1817  178.722200 12.19726 *\n##               213) Latitude>=34.105 598  106.051400 12.43109 *\n##             107) Latitude< 33.735 581   92.340630 12.53568 *\n##          27) Longitude< -118.315 2048  348.149000 12.53967  \n##            54) Latitude>=34.165 949  106.791800 12.38022 *\n##            55) Latitude< 34.165 1099  196.395200 12.67735  \n##             110) Longitude>=-118.365 431   85.796770 12.38191 *\n##             111) Longitude< -118.365 668   48.703000 12.86798 *\n##       7) Longitude< -121.645 3485  717.479900 12.39087  \n##        14) Latitude>=37.925 796  133.300900 12.10055 *\n##        15) Latitude< 37.925 2689  497.226200 12.47681 *\n#remotes::install_github(\"grantmcdermott/parttree\")\nlibrary(parttree)\nhouse_train %>%\n  ggplot(aes(y = Longitude, x = Latitude)) + \n  geom_parttree(data = house_cart_fit, alpha = 0.2) +\n  geom_point(aes(color = MedianHouseValue)) "},{"path":"class.html","id":"predicting","chapter":"9 Classification","heading":"Predicting","text":"seen image , 12 region 12 predicted values. plot seems little odd first glance, make sense careful consideration outcome measurement predicted value.","code":"\nhouse_cart_fit %>%\n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1)"},{"path":"class.html","id":"finer-partition","chapter":"9 Classification","heading":"Finer partition","text":":node splits latitude greater 34.675 2182 houses. 513.9564 “deviance” sum squares value node. predicted value average points node: 11.5. terminal node (asterisk).","code":"       12) Latitude>=34.675 2182  513.95640 11.52385  "},{"path":"class.html","id":"more-variables","chapter":"9 Classification","heading":"More variables","text":"Including variables, latitude longitude. Note predictions much better!","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\n# recipe\nhouse_cart_full_recipe <-\n  recipe(logValue ~ . ,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart <- decision_tree() %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_full_wflow <- workflow() %>%\n  add_model(house_cart) %>%\n  add_recipe(house_cart_full_recipe)\n\n# fit\nhouse_cart_full_fit <- house_cart_full_wflow %>%\n  fit(data = house_train)\nhouse_cart_full_fit %>%\n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.01)"},{"path":"class.html","id":"cross-validation-model-building","chapter":"9 Classification","heading":"Cross Validation (model building!)","text":"CV accuracyFinal model + prediction test dataTurns tree “better” complex – ? tree 14 nodes (depth 6) corresponds tree lowest deviance.Predicting final model test data","code":"\nreal.estate <- read.table(\"http://pages.pomona.edu/~jsh04747/courses/math154/CA_housedata.txt\", \n                          header=TRUE) %>%\n  mutate(logValue = log(MedianHouseValue))\n\n# partition\nset.seed(47)\nhouse_split <- initial_split(real.estate)\nhouse_train <- training(house_split)\nhouse_test <- testing(house_split)\n\nset.seed(4321)\nhouse_vfold <- vfold_cv(house_train, v = 10)\n\ncart_grid <- expand.grid(tree_depth = seq(2, 20, by = 2))\n\n# recipe\nhouse_cart_tune_recipe <-\n  recipe(logValue ~ .,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_tune <- decision_tree(tree_depth = tune()) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_tune_wflow <- workflow() %>%\n  add_model(house_cart_tune) %>%\n  add_recipe(house_cart_tune_recipe)\n\n# tuning / fit\nhouse_tuned <- house_cart_tune_wflow %>%\n  tune_grid(resamples = house_vfold, \n           grid = cart_grid) \nhouse_tuned %>% collect_metrics() %>%\n  filter()## # A tibble: 20 × 7\n##    tree_depth .metric .estimator  mean     n std_err .config              \n##         <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n##  1          2 rmse    standard   0.428    10 0.00224 Preprocessor1_Model01\n##  2          2 rsq     standard   0.436    10 0.00665 Preprocessor1_Model01\n##  3          4 rmse    standard   0.383    10 0.00242 Preprocessor1_Model02\n##  4          4 rsq     standard   0.547    10 0.00629 Preprocessor1_Model02\n##  5          6 rmse    standard   0.366    10 0.00239 Preprocessor1_Model03\n##  6          6 rsq     standard   0.588    10 0.00586 Preprocessor1_Model03\n##  7          8 rmse    standard   0.366    10 0.00239 Preprocessor1_Model04\n##  8          8 rsq     standard   0.588    10 0.00586 Preprocessor1_Model04\n##  9         10 rmse    standard   0.366    10 0.00239 Preprocessor1_Model05\n## 10         10 rsq     standard   0.588    10 0.00586 Preprocessor1_Model05\n## 11         12 rmse    standard   0.366    10 0.00239 Preprocessor1_Model06\n## 12         12 rsq     standard   0.588    10 0.00586 Preprocessor1_Model06\n## 13         14 rmse    standard   0.366    10 0.00239 Preprocessor1_Model07\n## 14         14 rsq     standard   0.588    10 0.00586 Preprocessor1_Model07\n## 15         16 rmse    standard   0.366    10 0.00239 Preprocessor1_Model08\n## 16         16 rsq     standard   0.588    10 0.00586 Preprocessor1_Model08\n## 17         18 rmse    standard   0.366    10 0.00239 Preprocessor1_Model09\n## 18         18 rsq     standard   0.588    10 0.00586 Preprocessor1_Model09\n## 19         20 rmse    standard   0.366    10 0.00239 Preprocessor1_Model10\n## 20         20 rsq     standard   0.588    10 0.00586 Preprocessor1_Model10\nhouse_tuned %>%\n  autoplot(metric = \"rmse\")\nhouse_tuned %>% \n  select_best(\"rmse\")## # A tibble: 1 × 2\n##   tree_depth .config              \n##        <dbl> <chr>                \n## 1          6 Preprocessor1_Model03\n# recipe\nhouse_cart_final_recipe <-\n  recipe(logValue ~ .,\n         data = house_train) %>%\n  update_role(MedianHouseValue, new_role = \"id variable\")\n\n# model\nhouse_cart_final <- decision_tree(tree_depth = 6) %>%\n  set_engine(\"rpart\") %>%\n  set_mode(\"regression\")\n\n# workflow\nhouse_cart_final_wflow <- workflow() %>%\n  add_model(house_cart_final) %>%\n  add_recipe(house_cart_final_recipe)\n\n# tuning / fit\nhouse_final <- house_cart_final_wflow %>%\n  fit(data = house_train)\nhouse_final## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: decision_tree()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 0 Recipe Steps\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## n= 15480 \n## \n## node), split, n, deviance, yval\n##       * denotes terminal node\n## \n##  1) root 15480 5024.40500 12.08947  \n##    2) MedianIncome< 3.54635 7696 1992.69800 11.77343  \n##      4) MedianIncome< 2.5165 3632  904.76740 11.57590  \n##        8) Latitude>=34.445 1897  412.81950 11.38488  \n##         16) Longitude>=-120.265 549   63.97662 11.08633 *\n##         17) Longitude< -120.265 1348  279.98120 11.50647 *\n##        9) Latitude< 34.445 1735  347.04430 11.78476  \n##         18) Longitude>=-117.775 645  111.86670 11.52607 *\n##         19) Longitude< -117.775 1090  166.47070 11.93784 *\n##      5) MedianIncome>=2.5165 4064  819.58450 11.94995  \n##       10) Latitude>=37.925 809   91.49688 11.68589 *\n##       11) Latitude< 37.925 3255  657.65510 12.01558  \n##         22) Longitude>=-122.235 2992  563.13610 11.97426  \n##           44) Latitude>=34.455 940  203.99070 11.77685  \n##             88) Longitude>=-120.155 338   31.54079 11.36422 *\n##             89) Longitude< -120.155 602   82.59029 12.00852 *\n##           45) Latitude< 34.455 2052  305.72870 12.06470  \n##             90) Longitude>=-118.285 1476  171.16160 11.95681 *\n##             91) Longitude< -118.285 576   73.36843 12.34115 *\n##         23) Longitude< -122.235 263   31.29310 12.48567 *\n##    3) MedianIncome>=3.54635 7784 1502.97400 12.40194  \n##      6) MedianIncome< 5.59185 5526  876.96730 12.25670  \n##       12) MedianHouseAge< 38.5 4497  651.27750 12.20567  \n##         24) MedianIncome< 4.53095 2616  388.38650 12.11491 *\n##         25) MedianIncome>=4.53095 1881  211.37640 12.33189 *\n##       13) MedianHouseAge>=38.5 1029  162.80030 12.47972 *\n##      7) MedianIncome>=5.59185 2258  224.13060 12.75740  \n##       14) MedianIncome< 7.393 1527  134.00030 12.64684 *\n##       15) MedianIncome>=7.393 731   32.47344 12.98835 *\nhouse_final %>% \n  predict(new_data = house_test) %>%\n  cbind(house_test) %>%\n  ggplot() +\n  geom_point(aes(x = logValue, y = .pred), alpha = 0.1) + \n  xlab(\"log of the Median House Value\") +\n  ylab(\"predicted value of log Median House\")"},{"path":"class.html","id":"bagging","chapter":"9 Classification","heading":"9.4 Bagging","text":"tree based models given CART easy understand implement, suffer high variance. , split training data two parts random fit decision tree halves, results get quite different (might seen homework assignment!). ’d like model produces low variance - one ran different datasets, ’d get (close ) model every time.Bagging = Bootstrap Aggregating. idea sometimes fit multiple models aggregate models together, get smoother model fit give better balance bias fit variance fit. Bagging can applied classifier reduce variability.\nRecall variance sample mean variance data / n. ’ve seen idea averaging outcome gives reduced variability.\n","code":""},{"path":"class.html","id":"bagging-algorithm","chapter":"9 Classification","heading":"9.4.1 Bagging algorithm","text":"Algorithm: Bagging ForestResample (bootstrap) cases (observational units, variables).Build tree new set (bootstrapped) training observations.Average (regression) majority vote (classification).Note every bootstrap sample, approximately 2/3 observations chosen 1/3 chosen.\\[\\begin{align}\nP(\\mbox{observation $$ bootstrap sample}) &= \\bigg(1 - \\frac{1}{n} \\bigg)^n\\\\\n\\lim_{n \\rightarrow \\infty} \\bigg(1 - \\frac{1}{n} \\bigg)^n = \\frac{1}{e} \\approx \\frac{1}{3}\n\\end{align}\\]Shortcomings Bagging:Model even harder “write-” (CART)lots predictors, (even greedy) partitioning can become computationally unwieldy - now computational task even harder! (number trees grown bootstrap sample)Strengths Bagging:Can handle categorical numerical predictors response variables (indeed, can handle mixed predictors time!).Can handle 2 groups categorical predictionsEasily ignore redundant variables.Perform better linear models non-linear settings. Classification trees non-linear models, immediately use interactions variables.Data transformations may less important (monotone transformations explanatory variables won’t change anything).\n\nSimilar bias CART, reduced variance\n\n(can proved).\nSimilar bias CART, reduced variance\nNotes bagging:Bagging alone uses full set predictors determine every tree (observations bootstrapped).Note predict particular observation, start top, walk tree, get prediction. average (majority vote) predictions get one prediction observation hand.Bagging gives smoother decision boundaryBagging can done decision method (just trees).need prune CV trees. reason averaging keeps us overfitting particular observations (think averages contexts: law large numbers). Pruning wouldn’t bad thing terms fit, unnecessary good predictions (add lot complexity algorithm).","code":""},{"path":"class.html","id":"out-of-bag-oob-error-rate","chapter":"9 Classification","heading":"9.4.2 Out Of Bag (OOB) error rate","text":"Additionally, bagging, need cross-validation separate test set get unbiased estimate test set error. estimated internally, run, follows:tree constructed using different bootstrap sample original data. one-third cases left bootstrap sample used construction \\(b^{th}\\) tree.Put case left construction \\(b^{th}\\) tree \\(b^{th}\\) tree get classification. way, test set classification obtained case one-third trees.end run, take \\(j\\) class got votes every time case \\(\\) oob. proportion times \\(j\\) equal true class n averaged cases oob error estimate. proven unbiased many tests.work? Consider following predictions silly toy data set 9 observations. Recall \\(\\sim 1/3\\) observations left bootstrap sample. observations predictions made. table , X given prediction made value.Let OOB prediction \\(^{th}\\) observation \\(\\hat{y}_{(-)}\\)\\[\\begin{align}\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{=1}^n \\textrm{} (y_i \\ne \\hat{y}_{(-)}) \\ \\ \\ \\ \\ \\ \\ \\  \\mbox{classification}\\\\\n\\mbox{OOB}_{\\mbox{error}} &= \\frac{1}{n} \\sum_{=1}^n  (y_i - \\hat{y}_{(-)})^2  \\ \\ \\ \\ \\ \\ \\ \\ \\mbox{regression}\\\\\n\\end{align}\\]","code":""},{"path":"class.html","id":"rf","chapter":"9 Classification","heading":"9.5 Random Forests","text":"Random Forests extension bagging regression trees (note: bagging can done prediction method). , idea infusing extra variability averaging variability, RFs use subset predictor variables every node tree.“Random forests overfit. can run many trees want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","code":""},{"path":"class.html","id":"random-forest-algorithm","chapter":"9 Classification","heading":"9.5.1 Random Forest algorithm","text":"Algorithm: Random ForestBootstrap sample training set.Grow un-pruned tree bootstrap sample.split, select \\(m\\) variables determine best split using \\(m\\) predictors.\nTypically \\(m = \\sqrt{p}\\) \\(\\log_2 p\\), \\(p\\) number features. Random Forests overly sensitive value \\(m\\). [splits chosen trees: according either squared error gini index / cross entropy / classification error.]prune tree. Save tree !Repeat steps 1-2 many many trees.tree grown bootstrap sample, predict OOB samples. tree grown, \\(~1/3\\) training samples won’t bootstrap sample – called bootstrap (OOB) samples. OOB samples can used test data estimate error rate tree.Combine OOB predictions create “--bag” error rate (either majority vote average predictions / class probabilities).trees together represent model used new predictions (either majority vote average).\nFigure 9.19: Building multiple trees combining outputs (predictions). Note image makes choice average tree probabilities instead using majority vote. valid methods creating Random Forest prediction model. http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf\nShortcomings Random Forests:Model even harder “write-” (CART)lots predictors, (even greedy) partitioning can become computationally unwieldy - now computational task even harder! … bagging observations andStrengths Random Forests:refinement bagged trees; quite popular (Random Forests tries improve bagging “de-correlating” trees. tree expectation, average reduce variability.)subset predictors makes Random Forests much faster search predictorscreates diverse set trees can built. Note bootstrapping samples predictor variables, add another level randomness can average decrease variability.Random Forests quite accurategenerally, models overfit data CV needed. However, CV can used fit tuning parameters (\\(m\\), node size, max number nodes, etc.).Notes Random Forests:Bagging alone uses full set predictors determine every tree (observations bootstrapped). Random Forests use subset predictors.Note predict particular observation, start top, walk tree, get prediction. average (majority vote) predictions get one prediction observation hand.Bagging special case Random Forest \\(m=p\\).generally, models overfit data CV needed. However, CV can used fit tuning parameters (\\(m\\), node size, max number nodes, etc.).“Random forests overfit. can run many trees want.” Brieman, http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm","code":""},{"path":"class.html","id":"how-to-choose-parameters","chapter":"9 Classification","heading":"How to choose parameters?","text":"\\(\\#\\) trees\nBuild trees error longer decreases\\(m\\)\nTry recommended defaults, half , twice - pick best (use CV avoid overfitting).","code":""},{"path":"class.html","id":"variable-importance","chapter":"9 Classification","heading":"Variable Importance","text":"learners bad many noisy variables response bound correlate . can measure contribution additional variable model much model accuracy decreased given variable excluded model.importance = decrease node impurity resulting splits variable, averaged trees(“impurity” defined RSS regression trees deviance classification trees).Variable importance measured two different metrics (R help importance):(permutation) accuracy: tree, prediction error --bag portion data recorded (error rate classification, MSE regression).Permute \\(j^{th}\\) variable recalculate prediction error. difference two averaged trees (\\(j^{th}\\) variable) give importance \\(j^{th}\\) variable.purity: decrease (increase, depending plot) node purity: root sum squares (RSS) [deviance/gini classification trees]. , amount total decrease RSS splitting variable, averaged trees.number variables large, forests can run variables, run using important variables first run.","code":""},{"path":"class.html","id":"r-rf-example","chapter":"9 Classification","heading":"9.5.2 R RF Example","text":"(“impurity” defined RSS regression trees deviance classification trees).method= 'ranger' zillion times faster method = 'randomForest' method = 'rf', work.mtry number trees?Get final model:Predict test data:","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins <- penguins %>%\n  drop_na()\n\n# partition\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\n# recipe\npenguin_rf_recipe <-\n  recipe(body_mass_g ~ . ,\n         data = penguin_train) %>%\n  step_unknown(sex, new_level = \"unknown\") %>%\n  step_mutate(year = as.factor(year)) \n\n#model\npenguin_rf <- rand_forest(mtry = tune(),\n                           trees = tune()) %>%\n  set_engine(\"ranger\", importance = \"permutation\") %>%\n  set_mode(\"regression\")\n\n# workflow\npenguin_rf_wflow <- workflow() %>%\n  add_model(penguin_rf) %>%\n  add_recipe(penguin_rf_recipe)\n\n# CV\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\npenguin_grid <- grid_regular(mtry(range = c(2,7)),\n                             trees(range = c(1,500)),\n                             levels = 5)\n\n# tune\npenguin_rf_tune <- \n  penguin_rf_wflow %>%\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\nselect_best(penguin_rf_tune, \"rmse\")## # A tibble: 1 × 3\n##    mtry trees .config              \n##   <int> <int> <chr>                \n## 1     2   375 Preprocessor1_Model16\npenguin_rf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  ggplot() + \n  geom_line(aes(x = trees, y = mean, color = as.factor(mtry)))\npenguin_rf_best <- finalize_model(\n  penguin_rf,\n  select_best(penguin_rf_tune, \"rmse\"))\n\npenguin_rf_best## Random Forest Model Specification (regression)\n## \n## Main Arguments:\n##   mtry = 2\n##   trees = 375\n## \n## Engine-Specific Arguments:\n##   importance = permutation\n## \n## Computational engine: ranger\npenguin_rf_final <-\n  workflow() %>%\n  add_model(penguin_rf_best) %>%\n  add_recipe(penguin_rf_recipe) %>%\n  fit(data = penguin_train)\n\npenguin_rf_final## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: rand_forest()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_unknown()\n## • step_mutate()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Ranger result\n## \n## Call:\n##  ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~2L,      x), num.trees = ~375L, importance = ~\"permutation\", num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1)) \n## \n## Type:                             Regression \n## Number of trees:                  375 \n## Sample size:                      249 \n## Number of independent variables:  7 \n## Mtry:                             2 \n## Target node size:                 5 \n## Variable importance mode:         permutation \n## Splitrule:                        variance \n## OOB prediction error (MSE):       84149.09 \n## R squared (OOB):                  0.8634591\npenguin_rf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  ggplot() +\n  geom_point(aes(x = body_mass_g, y = .pred)) + \n  geom_abline(intercept = 0, slope = 1)"},{"path":"class.html","id":"variable-importance-1","chapter":"9 Classification","heading":"Variable Importance","text":"order get variable importance, need specify importance within model forest.","code":"\nlibrary(vip)\n\npenguin_rf_final %>%\n  extract_fit_parsnip() %>%\n  vip(geom = \"point\")"},{"path":"class.html","id":"model-choices","chapter":"9 Classification","heading":"9.6 Model Choices","text":"soooooo many choices ’ve made along way. following list make realize truth respect given model. Every choice () lead different model.","code":""},{"path":"class.html","id":"support-vector-machines","chapter":"9 Classification","heading":"9.7 Support Vector Machines","text":"Support Vector Machines one algorithm classification. ’ll see, excellent properties, one important aspect note use numeric predictor variables binary response variables (classify two groups).Vladimir Vapnik (b. 1936) created SVMs late 1990s. History: actually work PhD early 60s Soviet Union. Someone Bell Labs asked visit, ended immigrating US. one actually thought SVMs work, eventually (1995 - took 30 years idea implementation) bet dinner classifying handwriting via SVM (using simple kernel) versus neural networks rest history.basic idea SVMs figure way create really complicated decision boundaries. want put straight line widest possible street (draw street gutters 4 points, two positive two negative). decision rule dot product new sample vector \\({\\bf w}\\) perpendicular median “street.”Note: standard formulation SVM requires computer find dot products observations. order , explanatory variables must numeric. order dot products meaningful, data must scale.","code":""},{"path":"class.html","id":"linsvm","chapter":"9 Classification","heading":"9.7.0.1 Linear Separator","text":"Recall ideas kNN trees:today’s decision boundary going based hyperplane separates values “best” way. Certainly, data linearly separable, infinitely many hyperplanes partition data perfectly. SVM, idea find “street” separates positive negative samples give widest margin.\nFigure 9.20: correct project observations can often produce perfect one dimensional (.e., linear) classifier. http://www.rmki.kfki.hu/~banmi/elte/Bishop - Pattern Recognition Machine Learning.pdf\n","code":""},{"path":"class.html","id":"aside-what-is-a-dot-product","chapter":"9 Classification","heading":"Aside: what is a dot product?","text":"Let \\({\\bf x} = (x_1, x_2, \\ldots, x_p)^t\\) \\({\\bf y} = (y_1, y_2, \\ldots, y_p)^t\\) two vectors live \\(R^p\\). dot product defined :\n\\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} = {\\bf x}^t {\\bf y} = \\sum_{=1}^p x_i y_i\n\\end{align}\\]\nFigure 9.21: w known, projection new observation onto w lead linear partition space.\ncan street used get decision rule? known \\({\\bf w}\\) perpendicular street. don’t yet know \\({\\bf w}\\) \\(b\\).“width” street vector perpendicular street (median). don’t know width yet, know know can use perpendicular vector (\\({\\bf w}\\)) figure classify points. Project unknown point (\\({\\bf u}\\)) onto \\({\\bf w}\\) see side street unknown value lands. , projection large enough, classify point positive: \\[{\\bf w} \\cdot {\\bf u} \\geq c?\\][Keep mind \\({\\bf u} \\cdot {\\bf w} = ||{\\bf w}|| \\times\\)(length shadow). , projection length shadow \\({\\bf w}\\) unit vector. aren’t going constrain \\({\\bf w}\\) unit vector (though !). regardless, \\({\\bf u} \\cdot {\\bf w}\\) still gives ability classify proportional length shadow.]Decision rule:\n\\({\\bf w} \\cdot {\\bf u} + b \\geq 0\\) label new sample “positive”\\({\\bf w}\\) created way perpendicular median street. unknown (\\({\\bf u}\\)) vector projected onto \\({\\bf w}\\) see left right side street.don’t know values decision rule! need constraints. Assuming data linearly separable, initial step find \\({\\bf w}\\) \\(b\\), positive samples (\\(x_+\\)) negative samples (\\(x_-\\)) force:\n\\[\\begin{align}\n{\\bf w} \\cdot {\\bf x}_+ + b &\\geq 1 \\tag{9.1}\\\\\n{\\bf w} \\cdot {\\bf x}_- + b &\\leq -1 \\tag{9.2}\n\\end{align}\\]mathematical convenience (don’t 2 equations hanging around), introduce \\(y_i\\) \n\\[\\begin{align}\ny_i &= 1 \\mbox{ positive samples}\\\\\ny_i &= -1 \\mbox{ negative samples}\n\\end{align}\\]simplifies criteria finding \\({\\bf w}\\) \\(b\\) :\n\\[ y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1\\]\n(Multiplying -1 equation ((9.2) switches signs, equation ((9.1)) ((9.2) end types points.), working toward solving \\({\\bf w}\\) \\(b\\), add additional constraint points gutter (margin lines):\\(x_i\\) gutter (definition):\n\\[y_i({\\bf w} \\cdot {\\bf x}_i + b) - 1 = 0\\]Now consider two particular positive negative values live margin (gutter). difference almost width street (want find street wide possible), wrong angle (see street picture ). Remember, goal find street separating pluses minuses wide possible. unit vector, dot \\((x_+ - x_-)\\) get width street!\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||}\n\\end{align}\\]\ndoesn’t us much good yet.\nGoal: Try find wide street possible.\nremember, gutter points constrained: turns \\(x_+ \\cdot {\\bf w} = 1 - b\\) \\(x_- \\cdot {\\bf w} = -1 - b\\). Therefore:\\[\\begin{align}\nwidth = \\frac{(x_+ - x_-) \\cdot {\\bf w}}{|| {\\bf w} ||} = \\frac{(1-b) - (-1-b)}{|| {\\bf w} ||} = \\frac{2}{||w||}\n\\end{align}\\]\nminimize \\((1/2)*||w||^2\\)\n(make mathematically easier). pieces making decision rules optimization problem. , minimize quantity subject constraints given problem.","code":""},{"path":"class.html","id":"lagrange-multipliers","chapter":"9 Classification","heading":"Lagrange multipliers","text":"Recall, Lagrange multipliers, first part optimization, second part constraint. point Lagrange multipliers put together constraint optimization one equation don’t worry constraints longer.\\(L\\) consists two parts. first thing minimize. second set constraints (, summation constraints). constraint multiplier \\(\\alpha_i\\), non-zero \\(\\alpha_i\\) ones connected values gutter.\\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1]\n\\end{align}\\]Find derivatives, set equal zero. Note can differentiate respect vector component wise, ’ll skip notation, one element time.\\[\\begin{align}\n\\frac{\\partial L}{\\partial {\\bf w}} &= {\\bf w} - \\sum \\alpha_i  y_i  {\\bf x}_i = 0 \\rightarrow {\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i \\\\\n\\frac{\\partial L}{\\partial b} &= -\\sum \\alpha_i y_i = 0\\\\\n\\end{align}\\]turns \\({\\bf w}\\) linear sum data vectors, either (turns \\(\\), \\(\\alpha_i=0\\)):\n\\[{\\bf w} = \\sum \\alpha_i  y_i  {\\bf x}_i\\]Use value \\({\\bf w}\\) plug back \\(L\\) minimize\\[\\begin{align}\nL &= \\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum_i \\alpha_i [ y_i ((\\sum_j \\alpha_j y_j {\\bf x}_j) \\cdot{\\bf x}_i + b ) - 1]\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - \\sum \\alpha_i y_i b + \\sum \\alpha_i\\\\\n&= -\\frac{1}{2}(\\sum_i \\alpha_i y_i {\\bf x}_i) \\cdot (\\sum_j \\alpha_j y_j {\\bf x}_j) - 0 + \\sum \\alpha_i\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\n\\end{align}\\]Find minimum expression:\n\\[L = \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j {\\bf x}_i \\cdot  {\\bf x}_j\\]\noptimization depends dot product pairs samples.\ndecision rule also depends dot product new observation original samples. [Note, points margin / gutter can used solve \\(b\\): \\(b =y_i - {\\bf w} \\cdot {\\bf x}_i\\), \\(y_i = 1/y_i\\).]Decision Rule, call positive :\n\\[\\sum \\alpha_i y_i {\\bf x}_i \\cdot {\\bf u} + b \\geq 0\\]Note convex space (can proved), can’t get stuck local maximum.","code":""},{"path":"class.html","id":"notlinsvm","chapter":"9 Classification","heading":"9.7.1 Not Linearly Separable","text":"","code":""},{"path":"class.html","id":"transformations-1","chapter":"9 Classification","heading":"Transformations","text":"\ndata can transformed new space data linearly separable.\ncan transform data different space (linearly separable), can transform data new space thing! , consider function \\(\\phi\\) new space consists vectors \\(\\phi({\\bf x})\\).Consider case circle plane. class boundary segment space considering points within circle belong one class, points outside circle another one. space linearly separable, mapping third dimension make separable. Two great videos: https://www.youtube.com/watch?v=3liCbRZPrZA https://www.youtube.com/watch?v=9NrALgHFwTo .Within transformed space, minimization procedure amount minimizing following:want minimum expression:\n\\[\\begin{align}\nL &= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j \\phi({\\bf x}_i) \\cdot  \\phi({\\bf x}_j)\\\\\n&= \\sum \\alpha_i -\\frac{1}{2} \\sum_i \\sum_j  \\alpha_i \\alpha_j y_i y_j K({\\bf x}_i, {\\bf x}_j)\n\\end{align}\\]Decision Rule, call positive :\n\\[\\begin{align}\n\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b &\\geq& 0\\\\\n\\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b &\\geq& 0\n\\end{align}\\]","code":""},{"path":"class.html","id":"kernel-examples","chapter":"9 Classification","heading":"Kernel Examples:","text":"Kernel 1Consider following transformation, \\(\\phi: R^2 \\rightarrow R^3\\):\n\\[\\begin{align}\n\\phi({\\bf x}) &= (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\\\\\nK({\\bf x}, {\\bf y}) &= \\phi({\\bf x}) \\cdot \\phi({\\bf y}) = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2\\\\\n&= (x_1y_1 + x_2y_2)^2\\\\\nK({\\bf x}, {\\bf y}) &= ({\\bf x} \\cdot {\\bf y})^2\n\\end{align}\\]\nsay, long know dot product original data, can recover dot product transformed space using quadratic kernel.Kernel 2\nWriting polynomial kernel (\\(d=2\\)), can find exact \\(\\phi\\) function. Consider following polynomial kernel \\(d=2\\).\n\\[K({\\bf x}, {\\bf y}) = ({\\bf x} \\cdot {\\bf y} + c)^2\\]\nwriting dot product considering square components separately, get\n\\[\\begin{align}\n({\\bf x} \\cdot {\\bf y} + c)^2 &= (c + \\sum_{=1}^p x_i y_i)^2\\\\\n&= c^2 + \\sum_{=1}^p x_i^2 y_i^2 + \\sum_{=1}^{p-1} \\sum_{j={+1}}^{p} 2x_i y_i x_j y_j + \\sum_{=1}^p 2 cx_i y_i\n\\end{align}\\]\npulling sum apart components \\({\\bf x}\\) \\({\\bf y}\\) vectors separately, find \n\\[\\begin{align}\n\\phi({\\bf x}) = (c, x_1^2, \\ldots, x_p^2, \\sqrt{2}x_1x_2, \\ldots, \\sqrt{2}x_1x_p, \\sqrt{2}x_2x_3, \\ldots, \\sqrt{2}x_{p-1}x_p, \\sqrt{2c}x_1, \\ldots, \\sqrt{2c}x_p)\n\\end{align}\\]Kernel 3\nUsing radial kernel (see ) possible map observations infinite dimensional space yet still consider kernel associated dot product original data. Consider following example \\(x\\) one dimension mapped infinite dimensions.\\[\\begin{align}\n\\phi_{RBF}(x) &= e^{-\\gamma x} \\bigg(1, \\sqrt{\\frac{2\\gamma}{1!}} x, \\sqrt{\\frac{(2\\gamma)^2}{2!}} x^2, \\sqrt{\\frac{(2\\gamma)^3}{3!}} x^3, \\ldots \\bigg)^t\\\\\nK_{RBF} (x,y) &= \\exp( -\\gamma ||x-y||^2)\n\\end{align}\\]\ncross validation used find tuning value \\(\\gamma\\) well penalty parameter \\(C\\).Consider following example http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex8/ex8.html.","code":""},{"path":"class.html","id":"what-if-the-boundary-is-wiggly","chapter":"9 Classification","heading":"What if the boundary is wiggly?","text":"take home message wiggly boundary really best, value \\(\\gamma\\) high represent high model complexity.\nFigure 9.22: Extremely complicated decision boundary\n","code":""},{"path":"class.html","id":"what-if-the-boundary-isnt-wiggly","chapter":"9 Classification","heading":"What if the boundary isn’t wiggly?","text":"boundary low complexity, best value \\(\\gamma\\) probably much lower.\nFigure 9.23: Simple decision boundary\n\nFigure 9.24: Simple decision boundary – reasonable gamma\n\nFigure 9.25: Simple decision boundary – gamma big\n","code":""},{"path":"class.html","id":"kernels","chapter":"9 Classification","heading":"9.7.2 What is a Kernel?","text":"kernel: kernel function function obeys certain mathematical properties. won’t go properties right now, now think kernel function function dot product two vectors, (e.g., measure “similarity” two vectors). \\(K\\) function two vectors \\({\\bf x}\\) \\({\\bf y}\\), kernel function \\(K\\) dot product \\(\\phi()\\) applied vectors. know \\(\\phi()\\) exists \\(K\\) symmetric \\(K_{ij} = K({\\bf x}_i, {\\bf x}_j)\\), matrix \\({\\bf K} = [K_{ij}]\\) positive definite.helpful website kernels: http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html\\[\\begin{align}\nK({\\bf x},{\\bf y}) = \\phi({\\bf x}) \\cdot \\phi({\\bf y})\n\\end{align}\\]","code":""},{"path":"class.html","id":"examples-of-kernels","chapter":"9 Classification","heading":"Examples of kernels:","text":"linear\n\\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\]\nNote, tuning parameter penalty/cost parameter \\(C\\)).linear\n\\[K({\\bf x}, {\\bf y}) = {\\bf x} \\cdot{\\bf y}\\]\nNote, tuning parameter penalty/cost parameter \\(C\\)).polynomial\n\\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma > 0\\]\nNote, \\(\\gamma, r, d\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).polynomial\n\\[K_P({\\bf x}, {\\bf y}) =(\\gamma {\\bf x}\\cdot {\\bf y} + r)^d = \\phi_P({\\bf x}) \\cdot \\phi_P({\\bf y}) \\ \\ \\ \\ \\gamma > 0\\]\nNote, \\(\\gamma, r, d\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).RBF\nradial basis function also called Gaussian kernel similarity Gaussian distribution (aka normal distribution). RBF maps infinite dimensional space, can easily fit training data. Care must taken estimate \\(\\gamma\\).\n\\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} -  {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\]\nNote, \\(\\gamma\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).RBF\nradial basis function also called Gaussian kernel similarity Gaussian distribution (aka normal distribution). RBF maps infinite dimensional space, can easily fit training data. Care must taken estimate \\(\\gamma\\).\n\\[K_{RBF}({\\bf x}, {\\bf y}) = \\exp( - \\gamma ||{\\bf x} -  {\\bf y}||^2) = \\phi_{RBF}({\\bf x}) \\cdot \\phi_{RBF}({\\bf y})\\]\nNote, \\(\\gamma\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)).sigmoid\nsigmoid kernel valid kernel method values \\(\\gamma\\) \\(r\\) [means certain parameter values, \\(\\phi()\\) function may exist].\n\\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\]\nNote, \\(\\gamma, r\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)). One benefit sigmoid kernel equivalence two-layer perceptron neural network.sigmoid\nsigmoid kernel valid kernel method values \\(\\gamma\\) \\(r\\) [means certain parameter values, \\(\\phi()\\) function may exist].\n\\[K_S({\\bf x}, {\\bf y}) = \\tanh(\\gamma {\\bf x}\\cdot {\\bf y} + r) = \\phi_S({\\bf x}) \\cdot \\phi_S({\\bf y})\\]\nNote, \\(\\gamma, r\\) must tuned using cross validation (along penalty/cost parameter \\(C\\)). One benefit sigmoid kernel equivalence two-layer perceptron neural network.","code":""},{"path":"class.html","id":"soft-margins","chapter":"9 Classification","heading":"Soft Margins","text":"data aren’t linearly separable? optimization problem can changed allow points side margin. optimization problem slightly complicated, basically idea:\n\\[y_i({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i  \\ \\ \\ \\ \\ \\ 1 \\leq \\leq n, \\ \\  \\xi_i \\geq 0\\]\nFigure 9.26: Note now problem set points allowed cross boundary. Slack variables (xi_i) allow every point classified correctly slack. Note xi_i=0 point actually calculated correctly.\noptimization problem gets slightly complicated two ways, first, minimization piece includes penalty parameter, \\(C\\) (much misclassification allowed - value \\(C\\) set/tuned optimized), second, constraint now allows points misclassified.Minimize (\\({\\bf w}\\), \\(\\xi_i\\), \\(b\\)):\n\\[\\frac{1}{2} ||{\\bf w}||^2 + C \\sum_{=1}^n \\xi_i\\]\nSubject :\n\\[y_i ({\\bf w} \\cdot {\\bf x}_i + b) \\geq 1 - \\xi_i \\ \\ \\ \\ \\xi_i \\geq 0\\]leads following Lagrangian equation:\n\\[\\begin{align}\nL = \\frac{1}{2}||{\\bf w}||^2 + C \\sum_{=1}^n \\xi_i - \\sum \\alpha_i [ y_i ({\\bf w} \\cdot {\\bf x}_i + b) - 1 + \\xi_i] - \\sum_{=1}^n \\beta_i \\xi_i \\ \\ \\ \\ \\alpha_i, \\beta_i \\geq 0\n\\end{align}\\]\n\\(C\\) now tuning parameter needs set user cross validation.\n","code":""},{"path":"class.html","id":"how-does-c-relate-to-margins","chapter":"9 Classification","heading":"How does \\(C\\) relate to margins?","text":"Notice minimization now many variables (\\(C\\) set/tuned - optimized). allowing misclassification \\(C=0\\), implies \\(\\xi_i\\) can large possible. means algorithm choose widest possible street. widest possible street one hits two extreme data points (“support vectors” now ones edge, ones near separating hyperplane). \\(C\\) small allows constraints (points crossing line) ignored.\\[C=0 \\rightarrow \\mbox{ can lead large training error}\\]\\(C\\) quite large, algorithm try hard classify exactly perfectly. , want \\(\\xi_i\\) close zero possible. projecting high dimensions, can always perfectly classify, large \\(C\\) tend overfit training data give small margin.\n\\[C>>> \\rightarrow \\mbox{ can lead classification rule generalize test data}\\]\nFigure 9.27: first figure, low C value gives large margin. right, high C value gives small margin. classifier better? Well, depends actual data (test, population, etc.) look like! second row large C classifier better; third row, small C classifier better. photo credit: http://stats.stackexchange.com/questions/31066/---influence--c--svms--linear-kernel\n","code":""},{"path":"class.html","id":"support-vector-machine-algorithm","chapter":"9 Classification","heading":"9.7.3 Support Vector Machine algorithm","text":"Algorithm: Support Vector MachineUsing cross validation, find values \\(C, \\gamma, d, r\\), etc. (kernel function!)Using Lagrange multipliers (read: computer), solve \\(\\alpha_i\\) \\(b\\).Classify unknown observation (\\({\\bf u}\\)) “positive” :\n\\[\\sum \\alpha_i y_i \\phi({\\bf x}_i) \\cdot \\phi({\\bf u}) + b  = \\sum \\alpha_i y_i K({\\bf x}_i, {\\bf u}) + b \\geq 0\\]Shortcomings Support Vector Machines:Shortcomings Support Vector Machines:Can classify binary categories (response variable).Can classify binary categories (response variable).predictor variables must numeric.\ngreat differential range allow variables large range dominate predictions. Either linearly scale attribute range [ e.g., (-1, +1) (0,1)] divide standard deviation.\nCategorical variables can used formatted binary factor variables.\nWhatever done training data must also done test data!\npredictor variables must numeric.great differential range allow variables large range dominate predictions. Either linearly scale attribute range [ e.g., (-1, +1) (0,1)] divide standard deviation.Categorical variables can used formatted binary factor variables.Whatever done training data must also done test data!Another problem kernel function .\nprimitive data (e.g., 2d data points), good kernels easy come .\nharder data (e.g., MRI scans), finding sensible kernel function may much harder.\nAnother problem kernel function .primitive data (e.g., 2d data points), good kernels easy come .harder data (e.g., MRI scans), finding sensible kernel function may much harder.really large data, doesn’t perform well large amount required training timeWith really large data, doesn’t perform well large amount required training timeIt also doesn’t perform well data set lot noise .e., target classes overlappingIt also doesn’t perform well data set lot noise .e., target classes overlappingSVM doesn’t directly provide probability estimates, calculated using expensive five-fold cross-validation.SVM doesn’t directly provide probability estimates, calculated using expensive five-fold cross-validation.Strengths Support Vector Machines:Strengths Support Vector Machines:Can always fit linear separating hyper plane high enough dimensional space.Can always fit linear separating hyper plane high enough dimensional space.kernel trick makes possible know transformation functions, \\(\\phi\\).kernel trick makes possible know transformation functions, \\(\\phi\\).optimization convex function, numerical process finding solutions extremely efficient.optimization convex function, numerical process finding solutions extremely efficient.works really well clear margin separationIt works really well clear margin separationIt effective high dimensional spaces.effective high dimensional spaces.effective cases number dimensions greater number samples.effective cases number dimensions greater number samples.uses subset training points decision function (called support vectors), also memory efficient.uses subset training points decision function (called support vectors), also memory efficient.","code":""},{"path":"class.html","id":"classifying-more-than-one-group","chapter":"9 Classification","heading":"9.7.4 Classifying more than one group","text":"two classes, problem needs reduced binary classification problem. Consider groups associated Red, Green, Blue. order figure points get classified Red, two different methods can applied.One vs \ncategory can compared rest groups. create \\(K\\) different classifiers (\\(K=\\) number classes response variable can take ). test value classified according classifier, group assignment given group giving highest value \\({\\bf w}_K \\cdot {\\bf u} + b\\), projection represent classification farthest group center. end, \\(K\\) classifiers.One vs One\nAlternatively, group can compared group (e.g., Red vs. Green, Red vs. Blue, Green vs. Blue). Class membership determine group unknown point often classified. end, \\(K(K-1)/2\\) classifiers.","code":""},{"path":"class.html","id":"r-svm-example","chapter":"9 Classification","heading":"9.7.5 R SVM Example","text":"’ll go back penguin data. first pass, let’s use SVM distinguish male female penguins. removed missing data dataset make predictions easier.","code":"\nlibrary(tidymodels)\nlibrary(palmerpenguins)\n\npenguins <- penguins %>%\n  drop_na()\n\nset.seed(47)\npenguin_split <- initial_split(penguins)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)"},{"path":"class.html","id":"linear-svm-no-tuning","chapter":"9 Classification","heading":"Linear SVM (no tuning)","text":"","code":"\n# recipe\npenguin_svm_recipe <-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)## # A tibble: 5 × 4\n##   variable          type    role      source  \n##   <chr>             <chr>   <chr>     <chr>   \n## 1 bill_length_mm    numeric predictor original\n## 2 bill_depth_mm     numeric predictor original\n## 3 flipper_length_mm numeric predictor original\n## 4 body_mass_g       numeric predictor original\n## 5 sex               nominal outcome   original\n# model\npenguin_svm_lin <- svm_linear() %>%\n  set_engine(\"LiblineaR\") %>%\n  set_mode(\"classification\")\n\npenguin_svm_lin## Linear Support Vector Machine Specification (classification)\n## \n## Computational engine: LiblineaR\n# workflow\npenguin_svm_lin_wflow <- workflow() %>%\n  add_model(penguin_svm_lin) %>%\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_lin_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_linear()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Linear Support Vector Machine Specification (classification)\n## \n## Computational engine: LiblineaR\n# fit\npenguin_svm_lin_fit <- \n  penguin_svm_lin_wflow %>%\n  fit(data = penguin_train)\n\npenguin_svm_lin_fit ## ══ Workflow [trained] ══════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_linear()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## $TypeDetail\n## [1] \"L2-regularized L2-loss support vector classification dual (L2R_L2LOSS_SVC_DUAL)\"\n## \n## $Type\n## [1] 1\n## \n## $W\n##      bill_length_mm bill_depth_mm flipper_length_mm body_mass_g       Bias\n## [1,]       0.248908      1.080195        -0.2256375    1.328448 0.06992734\n## \n## $Bias\n## [1] 1\n## \n## $ClassNames\n## [1] male   female\n## Levels: female male\n## \n## $NbClass\n## [1] 2\n## \n## attr(,\"class\")\n## [1] \"LiblineaR\""},{"path":"class.html","id":"rbf-svm-with-tuning","chapter":"9 Classification","heading":"RBF SVM (with tuning)","text":"","code":"\n# recipe\npenguin_svm_recipe <-\n  recipe(sex ~ bill_length_mm + bill_depth_mm + flipper_length_mm +\n           body_mass_g, data = penguin_train) %>%\n  step_normalize(all_predictors())\n\nsummary(penguin_svm_recipe)## # A tibble: 5 × 4\n##   variable          type    role      source  \n##   <chr>             <chr>   <chr>     <chr>   \n## 1 bill_length_mm    numeric predictor original\n## 2 bill_depth_mm     numeric predictor original\n## 3 flipper_length_mm numeric predictor original\n## 4 body_mass_g       numeric predictor original\n## 5 sex               nominal outcome   original\n# model\npenguin_svm_rbf <- svm_rbf(cost = tune(),\n                           rbf_sigma = tune()) %>%\n  set_engine(\"kernlab\") %>%\n  set_mode(\"classification\")\n\npenguin_svm_rbf## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = tune()\n##   rbf_sigma = tune()\n## \n## Computational engine: kernlab\n# workflow\npenguin_svm_rbf_wflow <- workflow() %>%\n  add_model(penguin_svm_rbf) %>%\n  add_recipe(penguin_svm_recipe)\n\npenguin_svm_rbf_wflow## ══ Workflow ════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: svm_rbf()\n## \n## ── Preprocessor ────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ───────────────────────────────────────────────────────────────────────\n## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = tune()\n##   rbf_sigma = tune()\n## \n## Computational engine: kernlab\n# CV\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train,\n                          v = 4)\n\n# parameters\n# the tuned parameters also have default values you can use\npenguin_grid <- grid_regular(cost(),\n                             rbf_sigma(),\n                             levels = 8)\n\npenguin_grid## # A tibble: 64 × 2\n##         cost     rbf_sigma\n##        <dbl>         <dbl>\n##  1  0.000977 0.0000000001 \n##  2  0.00431  0.0000000001 \n##  3  0.0190   0.0000000001 \n##  4  0.0841   0.0000000001 \n##  5  0.371    0.0000000001 \n##  6  1.64     0.0000000001 \n##  7  7.25     0.0000000001 \n##  8 32        0.0000000001 \n##  9  0.000977 0.00000000268\n## 10  0.00431  0.00000000268\n## # … with 54 more rows\n# tune\n# this takes a few minutes\npenguin_svm_rbf_tune <- \n  penguin_svm_rbf_wflow %>%\n  tune_grid(resamples = penguin_folds,\n            grid = penguin_grid)\n\npenguin_svm_rbf_tune ## # Tuning results\n## # 4-fold cross-validation \n## # A tibble: 4 × 4\n##   splits           id    .metrics           .notes          \n##   <list>           <chr> <list>             <list>          \n## 1 <split [186/63]> Fold1 <tibble [128 × 6]> <tibble [0 × 1]>\n## 2 <split [187/62]> Fold2 <tibble [128 × 6]> <tibble [0 × 1]>\n## 3 <split [187/62]> Fold3 <tibble [128 × 6]> <tibble [0 × 1]>\n## 4 <split [187/62]> Fold4 <tibble [128 × 6]> <tibble [0 × 1]>"},{"path":"class.html","id":"what-is-best","chapter":"9 Classification","heading":"What is best?","text":"","code":"\npenguin_svm_rbf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"accuracy\") %>%\n  ggplot() + \n  geom_line(aes(color = as.factor(cost), y = mean, x = rbf_sigma)) +\n  labs(color = \"Cost\")\npenguin_svm_rbf_tune %>%\n  autoplot()"},{"path":"class.html","id":"rbf-svm-final-model","chapter":"9 Classification","heading":"RBF SVM final model","text":"","code":"\npenguin_svm_rbf_best <- finalize_model(\n  penguin_svm_rbf,\n  select_best(penguin_svm_rbf_tune, \"accuracy\"))\n\npenguin_svm_rbf_best## Radial Basis Function Support Vector Machine Specification (classification)\n## \n## Main Arguments:\n##   cost = 0.371498572284237\n##   rbf_sigma = 1\n## \n## Computational engine: kernlab\npenguin_svm_rbf_final <-\n  workflow() %>%\n  add_model(penguin_svm_rbf_best) %>%\n  add_recipe(penguin_svm_recipe) %>%\n  fit(data = penguin_train)"},{"path":"class.html","id":"test-predictions","chapter":"9 Classification","heading":"Test predictions","text":"","code":"\nlibrary(yardstick)\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  select(sex, .pred_class) %>%\n  table()##         .pred_class\n## sex      female male\n##   female     39    5\n##   male        4   36\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  conf_mat(sex, .pred_class)##           Truth\n## Prediction female male\n##     female     39    4\n##     male        5   36\n# https://yardstick.tidymodels.org/articles/metric-types.html\nclass_metrics <- yardstick::metric_set(accuracy,sens,\n                                       spec, f_meas)\n\npenguin_svm_rbf_final %>%\n  predict(new_data = penguin_test) %>%\n  cbind(penguin_test) %>%\n  class_metrics(truth = sex, estimate = .pred_class)## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.893\n## 2 sens     binary         0.886\n## 3 spec     binary         0.9  \n## 4 f_meas   binary         0.897"},{"path":"unsup.html","id":"unsup","chapter":"10 Unsupervised Methods","heading":"10 Unsupervised Methods","text":"classification models ’ve discussed supervised learning techniques. word supervised refers fact know response variable training observations. Next , ’ll discuss clustering unsupervised technique – none observations given response variable. example, might want cluster hundred melanoma patients based genetic data. looking patterns groups together, don’t preconceived idea patients belong group.also semi-supervised techniques applied data observations labeled . discuss semi-supervised methods class.Clustering creates groups observations via unsupervised methods. cover hierarchical clustering, k-means, k-medoids. cluster two main reasons:Summary: describe data observations’ similarities .Discovery: find new ways groups observations similar.Classification – SUPERVISED! creates predictions (prediction models) unknown future observations via supervised methods. classification group membership (.e., response variable) known training data. covered k-NN, CART, bagging, Random Forests, SVMs.","code":""},{"path":"unsup.html","id":"latent-dirichlet-allocation","chapter":"10 Unsupervised Methods","heading":"10.1 Latent Dirichlet Allocation","text":"LDA views document mixture small (predefined) number topics describe set documents. word (typically common extremely rare words removed modeling) represents occurrence generated one document’s topics (document modeled mixture topics). LDA, model learns composition topic topic mixture document.Wikipedia:natural language processing, latent Dirichlet allocation (LDA) generative statistical model allows sets observations explained unobserved groups explain parts data similar. example, observations words collected documents, posits document mixture small number topics word’s creation attributable one document’s topics.algorithm finding words represent \\(K\\) topics (\\(K\\) chosen advance). [Explained detail http://blog.echen./2011/08/22/introduction--latent-dirichlet-allocation/.]document training data, randomly assign word one \\(K\\) topics.\\(K\\) topics now set words associated (albeit, set words meaningless). improve set words associated topic. word \\(w\\) document \\(d\\):\ntopic \\(t\\), compute two things:\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)\np(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).\n\nReassign \\(w\\) new topic, choose topic \\(t\\) probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according generative model, essentially probability topic \\(t\\) generated word \\(w\\), makes sense resample current word’s topic probability).\nwords, step, ’re assuming topic assignments except current word question correct, updating assignment current word using model documents generated.\ntopic \\(t\\), compute two things:\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)\np(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).\np(topic \\(t\\) \\(|\\) document \\(d\\)) = proportion words document \\(d\\) currently assigned topic \\(t\\)p(word \\(w\\) \\(|\\) topic \\(t\\)) = proportion assignments topic \\(t\\) documents come word \\(w\\).Reassign \\(w\\) new topic, choose topic \\(t\\) probability = p(topic \\(t\\) \\(|\\) document \\(d\\)) * p(word \\(w\\) \\(|\\) topic \\(t\\)) (according generative model, essentially probability topic \\(t\\) generated word \\(w\\), makes sense resample current word’s topic probability).words, step, ’re assuming topic assignments except current word question correct, updating assignment current word using model documents generated.repeating previous steps large number times, list words topic reach steady state. resulting assignments estimate topic mixtures document (counting proportion words assigned topic within document) words associated topic (counting proportion words assigned topic overall).https://ziqixiong.shinyapps.io/TopicModeling/","code":""},{"path":"unsup.html","id":"dissimilarities","chapter":"10 Unsupervised Methods","heading":"10.2 Dissimilarities","text":"Many, though , clustering algorithms based distances objects clustered. Mathematical properties distance function following. Consider two vectors \\({\\bf x}\\) \\({\\bf y}\\) (\\({\\bf x}, {\\bf y} \\\\mathbb{R}^p\\)), distance : \\(d({\\bf x}, {\\bf y})\\).\\(d({\\bf x}, {\\bf y}) \\geq 0\\)\\(d({\\bf x}, {\\bf y}) = d({\\bf y}, {\\bf x})\\)\\(d({\\bf x}, {\\bf y}) = 0\\) iff \\({\\bf x} = {\\bf y}\\)\\(d({\\bf x}, {\\bf y}) \\leq d({\\bf x}, {\\bf z}) + d({\\bf z}, {\\bf y})\\) vectors \\({\\bf z}\\).Triangle InequalityThe key proving triangle inequality distances relies Cauchy-Schwarz inequality.\n\\[\\begin{align}\n{\\bf x} \\cdot {\\bf y} &= || {\\bf x} ||  ||{\\bf y}|| \\cos(\\theta) \\\\\n|{\\bf x} \\cdot {\\bf y}| &\\leq || {\\bf x} ||  ||{\\bf y}|| \n\\end{align}\\]","code":""},{"path":"unsup.html","id":"euclidean-distance","chapter":"10 Unsupervised Methods","heading":"Euclidean Distance","text":"\\[d_E({\\bf x}, {\\bf y}) = \\sqrt{\\sum_{=1}^p (x_i - y_i)^2}\\]Distance properties check .Cauchy-Schwarz:\n\\[\\begin{align}\n\\sum_{=1}^p(x_i - y_i)^2 = \\sum_{=1}^p ( (x_i - z_i) + (z_i - y_i))^2 &\\leq \\Bigg( \\sqrt{\\sum_{=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{=1}^p(z_i - y_i)^2} \\Bigg)^2\\\\\n\\sqrt{\\sum_{=1}^p(x_i - y_i)^2} &\\leq \\sqrt{\\sum_{=1}^p(x_i - z_i)^2} + \\sqrt{\\sum_{=1}^p(z_i - y_i)^2}\\\\\nd_E({\\bf x}, {\\bf y}) &\\leq d_E({\\bf x}, {\\bf z}) + d_E({\\bf z}, {\\bf y})\n\\end{align}\\]Shortcomings:\\(d_E\\) scale invariant.\\(d_E\\) measures magnitude differences, pattern differences.\\(d_E\\) sensitive outliers.Strengths:Directly measures commonly considered “distance.”","code":""},{"path":"unsup.html","id":"pearson-correlation-distance","chapter":"10 Unsupervised Methods","heading":"Pearson Correlation Distance","text":"\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n \\mbox{ } &= 1 - |r_P ({\\bf x}, {\\bf y})|\\\\\n \\mbox{ }   &= 1 - (r_P ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]Notice Euclidean distance Pearson correlation distance similar original observations scaled. Assume sample mean \\({\\bf x}\\) (, \\(\\frac{1}{p} \\sum x_i = \\overline{x} = 0\\)) zero sample standard deviation 1.\\[\\begin{align}\n r_P ({\\bf x}, {\\bf y}) &=  \\frac{\\sum x_i y_i - p \\ \\overline{x} \\ \\overline{y}}{(p-1)s_x s_y}\\\\\n &=  \\frac{1}{(p-1)} \\sum x_i y_i\\\\\n & \\ \\ & \\\\\n d_E({\\bf x}, {\\bf y}) &= \\sqrt{\\sum(x_i - y_i)^2}\\\\\n &=  \\sqrt{ \\sum x_i^2 + \\sum y_i^2 - 2 \\sum x_i y_i}\\\\\n d_E^2 &= 2[(p-1) - \\sum x_i y_i]\\\\\n &= 2(p-1)*[1 - r_P({\\bf x}, {\\bf y})]\n \\end{align}\\]","code":""},{"path":"unsup.html","id":"distance-properties-dont-hold-for-pearson-correlation","chapter":"10 Unsupervised Methods","heading":"Distance properties don’t hold for Pearson correlation","text":"\\({\\bf y}={\\bf x}\\)\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n&= 1 - r_P ({\\bf x}, {\\bf x})\\\\\n&= 1 - 1 = 0\n\\end{align}\\]\\({\\bf y}={\\bf x}\\)\n\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) &= 1 - r_P ({\\bf x}, {\\bf y})\\\\\n&= 1 - r_P ({\\bf x}, {\\bf x})\\\\\n&= 1 - 1 = 0\n\\end{align}\\]\\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\)\n\\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\)\\({\\bf x}=(1,1,0)\\), \\({\\bf y} = (2,1,0)\\), \\({\\bf z} = (1,-1,0)\\)\n\\(r_P({\\bf x}, {\\bf y}) = 0.87\\), \\(r_P({\\bf x}, {\\bf z}) = 0\\), \\(r_P({\\bf y}, {\\bf z}) = 0.5\\)\\(d_P({\\bf x}, {\\bf y}) + d_P({\\bf y}, {\\bf z}) < d_P({\\bf z}, {\\bf x})\\)\n\\(\\rightarrow\\leftarrow\\)Regular Pearson distanceAbsolute Pearson distanceUsing absolute distance doesn’t fix things.Shortcomings:\\(d_P\\) satisfy triangle inequality.\\(d_P\\) sensitive outliers.Strengths:Can measure distance variables different scales (although still sensitive extreme values).","code":"\nx1 <- c(1,2,3)\nx2 <- c(1, 4, 10)\nx3 <- c(9, 2, 2)\n\n# d(1,2)\n1 - cor(x1, x2)## [1] 0.01801949\n# d(1,3)\n1 - cor(x1, x3)## [1] 1.866025\n# d(2,3)\n1 - cor(x2, x3)## [1] 1.755929\n# d(1,3) > d(1,2) + d(2,3)\n1 - cor(x1, x2) + 1 - cor(x2, x3)## [1] 1.773948\n# d(1,2)\n1 - abs(cor(x1, x2))## [1] 0.01801949\n# d(1,3)\n1 - abs(cor(x1, x3))## [1] 0.1339746\n# d(2,3)\n1 - abs(cor(x2, x3))## [1] 0.2440711\n# d(2,3) > d(1,2) + d(1,3)\n1 - abs(cor(x1, x2)) + 1 - abs(cor(x1, x3))## [1] 0.1519941"},{"path":"unsup.html","id":"spearman-correlation-distance","chapter":"10 Unsupervised Methods","heading":"Spearman Correlation Distance","text":"Spearman correlation distance uses Spearman correlation instead Pearson correlation. Spearman correlation simply Pearson correlation applied ranks observations. ranking allows Spearman distance resistant outlying observations.\\[\\begin{align}\nd_S({\\bf x}, {\\bf y}) &= 1 - r_S ({\\bf x}, {\\bf y})\\\\\n \\mbox{ } &= 1 - |r_S ({\\bf x}, {\\bf y})|\\\\\n \\mbox{ }   &= 1 - (r_S ({\\bf x}, {\\bf y}))^2\\\\\n  \\end{align}\\]Shortcomings:\\(d_S\\) also satisfy triangle inequality.\\(d_S\\) loses information shape relationship.Strengths:resistant outlying values","code":""},{"path":"unsup.html","id":"cosine-distance","chapter":"10 Unsupervised Methods","heading":"Cosine Distance","text":"\\[\\begin{align}\nd_C({\\bf x}, {\\bf y}) &=  \\frac{{\\bf x} \\cdot {\\bf y}}{|| {\\bf x} ||  ||{\\bf y}||}\\\\\n&= \\frac{\\sum_{=1}^p x_i y_i}{\\sqrt{\\sum_{=1}^p x_i^2 \\sum_{=1}^p y_i^2}}\\\\\n&= 1 - r_P ({\\bf x}, {\\bf y})  \\ \\ \\ \\ \\mbox{} \\overline{\\bf x} = \\overline{\\bf y} = 0\n\\end{align}\\]Said differently,\\[\\begin{align}\nd_P({\\bf x}, {\\bf y}) = d_C({\\bf x} -  \\overline{\\bf x}, {\\bf y} -  \\overline{\\bf y})\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"haversine-distance","chapter":"10 Unsupervised Methods","heading":"Haversine Distance","text":"Haversine distance great-circle distance (.e., distance two points sphere) used measure distance two locations Earth. Let \\(R\\) radius Earth, (lat1,long1) (lat2, long2) two locations calculate distance.\\[d_{HV} = 2 R \\arcsin \\sqrt{\\sin^2 \\bigg( \\frac{lat2-lat1}{2} \\bigg) + \\cos(lat1) \\cos(lat2) \\sin^2 \\bigg(\\frac{long2 - long1}{2} \\bigg)} \\]Shortcomings:Earth perfect sphereDepending distance used, typically getting one point next done shortest distanceStrengths:Allows calculations, example, two cities.","code":""},{"path":"unsup.html","id":"hamming-distance","chapter":"10 Unsupervised Methods","heading":"Hamming Distance","text":"Hamming distance number coordinates across two vectors whose values differ. vectors binary, Hamming distance equivalent \\(L_1\\) norm difference. (Hamming distance satisfy properties distance metric.) methods, equivalently, calculate proportion coordinates differ.\\[\\begin{align}\nd_H({\\bf x}, {\\bf y}) = \\sum_{=1}^p (x_i \\ne y_i)\n\\end{align}\\]\nFigure 9.3: Hamming distance across two DNA strands 7.\nShortcomings:Can’t measure degree difference categorical variables.Strengths:distance metric.\nGives direct “distance” categorical variables.\nFigure 10.1: function dist R calculates distances given .\n","code":""},{"path":"unsup.html","id":"distance-on-strings","chapter":"10 Unsupervised Methods","heading":"Distance on strings","text":"Consider following infographic compares different methods computing distances strings.\nFigure 1.5: Comparison string distance metrics https://www.kdnuggets.com/2019/01/comparison-text-distance-metrics.html.\n","code":""},{"path":"unsup.html","id":"hier","chapter":"10 Unsupervised Methods","heading":"10.3 Hierarchical Clustering","text":"Hierarchical Clustering set nested clusters organized tree. Note objects belong child cluster also belong parent cluster.Example: Consider following images / data (Laura Hoopes, personal communication; Molecular characterisation soft tissue tumours: gene expression study Nielsen et al., Lancet 2002). first represents microarray sample aging yeast. second set 41 samples soft-tissue tumors (columns) subset 5520 genes (rows) used characterize molecular signatures.Note: ordering variables (samples) affect clustering samples (variables). : can clustering variables / samples either sequentially parallel see trends relationships simultaneously. Clustering observations variables called biclustering.Algorithm: Agglomerative Hierarchical Clustering AlgorithmBegin \\(n\\) observations measure (Euclidean distance) \\({n \\choose 2} = n(n-1)/2\\) pairwise dissimilarities. Treat observation cluster.\\(= n, n - 1, \\ldots , 2\\):Examine pairwise inter-cluster dissimilarities among \\(\\) clusters identify pair clusters least dissimilar (, similar). Fuse two clusters. dissimilarity two clusters indicates height dendrogram fusion placed.Compute new pairwise inter-cluster dissimilarities among \\(- 1\\) remaining clusters.Agglomerative methods start object (e.g., gene) group. Groups merged objects together one group.Divisive methods start objects one group break groups sequentially objects individuals.Single Linkage algorithm defines distance groups closest pair individuals.Complete Linkage algorithm defines distance groups farthest pair individuals.Average Linkage algorithm defines distance groups average distances pairs individuals across groups.Toy Example Single Linkage Agglomerative Hierarchical ClusteringLink B!\n\\[\\begin{align}\nd_{(AB)C} &= \\min(d_{AC}, d_{BC}) = 0.5\\\\\nd_{(AB)D} &= \\min(d_{AD}, d_{BD}) = 0.9\\\\\nd_{(AB)E} &= \\min(d_{AE}, d_{}) = 0.8\\\\\n\\end{align}\\]Link D E!\n\\[\\begin{align}\nd_{(AB)C} &=  0.5\\\\\nd_{(AB)(DE)} &= \\min(d_{AD}, d_{BD}, d_{AE}, d_{}) = 0.8\\\\\nd_{(DE)C} &= \\min(d_{CD}, d_{CE}) = 0.4\\\\\n\\end{align}\\]Link C (DE)!\n\\[\\begin{align}\nd_{(AB)(CDE)} = d_{BC} = 0.5\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"r-hierarchical-example","chapter":"10 Unsupervised Methods","heading":"10.3.1 R hierarchical Example","text":", using penguins dataset, hierarchical clustering run. can look dendrogram, particular alignment categorical variables like species island.Notice using numerical variables. numerical variables scaled (subtract mean divide standard deviation).Full example adapted https://cran.r-project.org/web/packages/dendextend/vignettes/Cluster_Analysis.html.","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_h <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_hclust <- penguins_h %>%\n  dist() %>%\n  hclust(method = \"complete\")\n\npenguin_hclust## \n## Call:\n## hclust(d = ., method = \"complete\")\n## \n## Cluster method   : complete \n## Distance         : euclidean \n## Number of objects: 342\npenguin_dend <- as.dendrogram(penguin_hclust)"},{"path":"unsup.html","id":"plotting","chapter":"10 Unsupervised Methods","heading":"Plotting","text":"basic dendrogram:Zooming cluster:Adding color branches nodes","code":"\nplot(penguin_hclust)\nplot(penguin_dend[[1]])\nlibrary(colorspace) # get nice colors\nspecies_col <- rev(rainbow_hcl(3))[as.numeric(penguins$species)]\nlibrary(dendextend)\n# order the branches as closely as possible to the original order\npenguin_dend <- rotate(penguin_dend, 1:342)\n# color branches based on the clusters\npenguin_dend <- color_branches(penguin_dend, k = 3)\n\n# Manually match the labels, as much as possible, to the real classification of the flowers:\nlabels_colors(penguin_dend) <-\n   rainbow_hcl(3)[sort_levels_values(\n      as.numeric(penguins$species)[order.dendrogram(penguin_dend)]\n   )]\n\n# We shall add the flower type to the labels:\nlabels(penguin_dend) <- paste(as.character(penguins$species)[order.dendrogram(penguin_dend)],\n                           \"(\",labels(penguin_dend),\")\", \n                           sep = \"\")\n# We hang the dendrogram a bit:\npenguin_dend <- hang.dendrogram(penguin_dend,hang_height=0.1)\n\n# reduce the size of the labels:\npenguin_dend <- assign_values_to_leaves_nodePar(penguin_dend, 0.5, \"lab.cex\")\npenguin_dend <- set(penguin_dend, \"labels_cex\", 0.5)\n# And plot:\npar(mar = c(3,3,3,7))\nplot(penguin_dend, \n     main = \"Clustered penguin data set\n     (the labels give the true penguin species)\", \n     horiz =  TRUE,  nodePar = list(cex = .007))\npenguin_species <- rev(levels(penguins$species))\nlegend(\"topleft\", legend = penguin_species, fill = rainbow_hcl(3))"},{"path":"unsup.html","id":"part","chapter":"10 Unsupervised Methods","heading":"10.4 Partitioning Clustering","text":"Partition Clustering division set data objects \\(K\\) non-overlapping subsets (clusters) observation falling exactly one cluster.contrast hierarchical clustering results given (!) number clusters, partitioning methods typically start given \\(k\\) value set distances. goal partition observations \\(k\\) groups objective function optimized. number possible partitions roughly \\(n^k / k!\\) (note: \\(100^{5} / 5! = 83\\) million). [exact number can computed using Sterling numbers.] instead looking partitions, step recursive algorithm.","code":""},{"path":"unsup.html","id":"k-means-clustering","chapter":"10 Unsupervised Methods","heading":"10.4.1 \\(k\\)-means Clustering","text":"fun applet!!https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\\(k\\)-means clustering unsupervised partitioning algorithm designed find partition observations following objective function minimized (find smallest within cluster sum squares):\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K 2 \\sum_{\\C_k} \\sum_{j=1}^p (x_{ij} - \\overline{x}_{kj})^2 \\Bigg\\}\\]described algorithm , reallocating observations can improve minimization criteria algorithm stopping changes observations lower objective function. algorithm leads local optimum, confirmation global minimum occurred. Often \\(k\\)- means algorithm run multiple times different random starts, partition leading lowest objective criteria chosen.Note following algorithm simply one \\(k\\)-means algorithm. algorithms include different way set starting values, different decision recalculate centers, ties, etc.\nFigure 9.10: Introduction Statistical Learning James, Witten, Hastie, Tibshirani.\nAlgorithm: \\(k\\)-Means ClusteringRandomly assign number, 1 \\(k\\), observations. serve initial cluster assignments observations.Iterate cluster assignments stop changing:\n\\(k\\) clusters, compute cluster centroid. \\(k^{th}\\) cluster centroid vector \\(p\\) feature means observations \\(k^{th}\\) cluster.\nAssign observation cluster whose centroid closest (closest defined using Euclidean distance).\n\\(k\\) clusters, compute cluster centroid. \\(k^{th}\\) cluster centroid vector \\(p\\) feature means observations \\(k^{th}\\) cluster.Assign observation cluster whose centroid closest (closest defined using Euclidean distance).Ties? something consistent: example, leave current cluster.\\(k\\)-means algorithm converge / (local) minimize objective function?point closer different center, moving lower objective function.Averages minimize squared differences, taking new average result lower objective function.point equidistant two clusters, point won’t move.algorithm must converge finite number steps finitely many points.","code":""},{"path":"unsup.html","id":"scaling-matters","chapter":"10 Unsupervised Methods","heading":"Scaling matters","text":"Note variables different scales, whichever variable larger magnitude dominate distances (therefore clustering). Unless explicitly want happen (odd), scale variables (subtract mean divide standard deviation) distance calculated Z-scores instead raw data.example , \\(k=2\\) k-means algorithm able see cigar-shaped structure (raw data) distances dominated x1 variable (differentiate clusters).strengthsNo hierarchical structure / points can move one cluster another.Can run range values \\(k\\).shortcomings\\(k\\) predefined run algorithm.\\(k\\)-means based Euclidean distance ().","code":"\nset.seed(47)\nnorm_clust <- data.frame(\n  x1 = rnorm(1000, 0, 15),\n  x2 = c(rnorm(500, 5, 1), rnorm(500, 0, 1)))\n\nnorm_clust %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on raw data\")\nnorm_clust %>%\n  mutate(across(everything(), scale)) %>%\n  kmeans(centers = 2) %>%\n  augment(norm_clust) %>%\n  ggplot() + \n  geom_point(aes(x = x1, y = x2, color = .cluster)) +\n  ggtitle(\"k-means (k=2) on normalized / scaled data\")"},{"path":"unsup.html","id":"r-k-means-example","chapter":"10 Unsupervised Methods","heading":"10.4.2 R k-means Example","text":", using penguins dataset, \\(k\\)-means clustering run. try multiple different values \\(k\\) come partition penguins. can look clusterings scatterplots numerical variables. can also check see clustering aligns categorical variables like species island.Notice using numerical variables. numerical variables scaled (subtract mean divide standard deviation).Full example adapted https://www.tidymodels.org/learn/statistics/k-means/.function augment works observation level:function tidy() works per-cluster level:function glance() works per-model level:","code":"\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\ndata(penguins)\n\npenguins_km <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  mutate(across(bill_length_mm:body_mass_g, scale))\n\nset.seed(47)\npenguin_kclust <- penguins_km %>%\n  kmeans(centers = 3)\n\npenguin_kclust## K-means clustering with 3 clusters of sizes 132, 123, 87\n## \n## Cluster means:\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n## 1     -1.0465260     0.4858415        -0.8899121  -0.7694891\n## 2      0.6562677    -1.0983711         1.1571696   1.0901639\n## 3      0.6600059     0.8157307        -0.2857869  -0.3737654\n## \n## Clustering vector:\n##   [1] 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n##  [38] 1 1 1 1 1 3 1 1 1 1 1 3 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 3 1 1 1 3 1\n##  [75] 3 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 3 1 1 1 3 1 1 1 3 1 3 1 1 1 1 1 1 1 3 1 3\n## [112] 1 3 1 3 1 1 1 1 1 1 1 3 1 1 1 1 1 3 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n## [149] 1 1 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3\n## [297] 1 3 3 3 3 3 3 3 1 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3\n## [334] 3 3 3 3 3 3 3 3 3\n## \n## Within cluster sum of squares by cluster:\n## [1] 122.1477 143.1502 112.9852\n##  (between_SS / total_SS =  72.3 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n## [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\npenguin_kclust %>% augment(penguins_km)## # A tibble: 342 × 5\n##    bill_length_mm[,1] bill_depth_mm[,… flipper_length_… body_mass_g[,1] .cluster\n##                 <dbl>            <dbl>            <dbl>           <dbl> <fct>   \n##  1             -0.883           0.784            -1.42          -0.563  1       \n##  2             -0.810           0.126            -1.06          -0.501  1       \n##  3             -0.663           0.430            -0.421         -1.19   1       \n##  4             -1.32            1.09             -0.563         -0.937  1       \n##  5             -0.847           1.75             -0.776         -0.688  1       \n##  6             -0.920           0.329            -1.42          -0.719  1       \n##  7             -0.865           1.24             -0.421          0.590  1       \n##  8             -1.80            0.480            -0.563         -0.906  1       \n##  9             -0.352           1.54             -0.776          0.0602 3       \n## 10             -1.12           -0.0259           -1.06          -1.12   1       \n## # … with 332 more rows\npenguin_kclust %>% tidy()## # A tibble: 3 × 7\n##   bill_length_mm bill_depth_mm flipper_length_mm body_mass_g  size withinss\n##            <dbl>         <dbl>             <dbl>       <dbl> <int>    <dbl>\n## 1         -1.05          0.486            -0.890      -0.769   132     122.\n## 2          0.656        -1.10              1.16        1.09    123     143.\n## 3          0.660         0.816            -0.286      -0.374    87     113.\n## # … with 1 more variable: cluster <fct>\npenguin_kclust %>% glance()## # A tibble: 1 × 4\n##   totss tot.withinss betweenss  iter\n##   <dbl>        <dbl>     <dbl> <int>\n## 1  1364         378.      986.     2"},{"path":"unsup.html","id":"trying-various-values-of-k","chapter":"10 Unsupervised Methods","heading":"Trying various values of k","text":"values \\(k\\) augmented, tidyed, glanced information can calculated. Note might want find total within sum squares decreases function \\(k\\). (within sum squares always decrease function \\(k?\\) ? ?) seems though \\(k=3\\) \\(k=4\\) probably sufficient (larger values \\(k\\) reduce sums squares substantially).Adding back rest penguin information (species, sex, island, etc.) order determine unsupervised clusters align non-numeric information given dataframe.two variables flipper_length_mm bill_lengh_mm seems two clusters probably sufficient. However, probably make sense look cluster colorings across four variables higher dimensional space (e.g., 3-D projections).Based species, clustering (\\(k=2\\)) seems separate Gentoo, can’t really differentiate Adelie Chinstrap. let \\(k=4\\), get almost perfect partition three species.island, seems like clustering isn’t able (really, !) distinguish penguins three islands.","code":"\nkmax <- 9\npenguin_kclusts <- \n  tibble(k = 1:kmax) %>%\n  mutate(\n    penguin_kclust = map(k, ~kmeans(penguins_km, .x)),\n    tidied = map(penguin_kclust, tidy),\n    glanced = map(penguin_kclust, glance),\n    augmented = map(penguin_kclust, augment, penguins_km)\n  )\n\npenguin_kclusts## # A tibble: 9 × 5\n##       k penguin_kclust tidied           glanced          augmented         \n##   <int> <list>         <list>           <list>           <list>            \n## 1     1 <kmeans>       <tibble [1 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 2     2 <kmeans>       <tibble [2 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 3     3 <kmeans>       <tibble [3 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 4     4 <kmeans>       <tibble [4 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 5     5 <kmeans>       <tibble [5 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 6     6 <kmeans>       <tibble [6 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 7     7 <kmeans>       <tibble [7 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 8     8 <kmeans>       <tibble [8 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\n## 9     9 <kmeans>       <tibble [9 × 7]> <tibble [1 × 4]> <tibble [342 × 5]>\nclusters <- \n  penguin_kclusts %>%\n  unnest(cols = c(tidied))\n\nassignments <- \n  penguin_kclusts %>% \n  unnest(cols = c(augmented))\n\nclusterings <- \n  penguin_kclusts %>%\n  unnest(cols = c(glanced))\nclusterings %>%\n  ggplot(aes(x = k, y = tot.withinss)) + \n  geom_line() + \n  geom_point() + ylab(\"\") +\n  ggtitle(\"Total Within Sum of Squares\")\nassignments <- penguins %>%\n  drop_na(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n  select(species, island, sex, year) %>%\n  slice(rep(1:n(), times = kmax)) %>%\n  cbind(assignments)\nassignments %>%\n ggplot(aes(x = flipper_length_mm, y = bill_length_mm)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\nassignments %>%\n  group_by(k) %>%\n  select(.cluster, species) %>%\n  table() %>%\n  as.data.frame() %>%\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = species, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") + \n  scale_fill_gradient(low = \"white\", high = \"red\") + \n  ggtitle(\"Species vs cluster prediction across different values of k\")\nassignments %>%\n  group_by(k) %>%\n  select(.cluster, island) %>%\n  table() %>%\n  as.data.frame() %>%\n    ggplot() +\n    geom_tile(aes(x = .cluster, y = island, fill = Freq)) + \n  facet_wrap( ~ k) + ylab(\"\") +\n  scale_fill_gradient(low = \"white\", high = \"red\") +\n  ggtitle(\"Island vs cluster prediction across different values of k\")"},{"path":"unsup.html","id":"partitioning-around-medoids","chapter":"10 Unsupervised Methods","heading":"10.4.3 Partitioning Around Medoids","text":"alternative \\(k\\)-means, Kaufman Rousseeuw developed Partitioning around Medoids (Finding Groups Data: introduction cluster analysis, 1990). particular strength PAM allows dissimilarity metric. , dissimilarity based correlations problem, algorithm gets complicated “center” longer defined Euclidean terms.two main steps Build (akin assigning points clusters) Swap (akin redefining cluster centers). objective function algorithm tries minimize average dissimilarity objects closest representative object. (PAM algorithm good (necessarily global optimum) solution minimizing objective function.)\\[\\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{\\C_k}D_i \\Bigg\\} = \\text{arg}\\,\\min\\limits_{C_1, \\ldots, C_k} \\Bigg\\{ \\sum_{k=1}^K \\sum_{\\C_k}d(x_i, m_k) \\Bigg\\}\\]\n\\(D_i\\) represents distance observation \\(\\) closest medoid, \\(m_k\\).strengthsNo hierarchical structure / points can move one cluster another.Can run range values \\(k\\).can use distance measureshortcomings\\(k\\) predefined run algorithm.Algorithm: Partitioning Around Medoids (Elements Statistical Learning (2001), Hastie, Tibshirani, Friedman, pg 469.)Randomly assign number, 1 \\(K\\), observations. serve initial cluster assignments observations.Randomly assign number, 1 \\(K\\), observations. serve initial cluster assignments observations.Iterate cluster assignments stop changing:Iterate cluster assignments stop changing:(Repeat \\(k \\\\{1, 2, ...K\\}\\)) given cluster, \\(C_k\\), find observation cluster minimizing total distance points cluster:\n\\[^*_k = \\text{arg}\\,\\min\\limits_{\\C_k} \\sum_{' \\C_k} d(x_i, x_{'})\\]\n\\(m_k = x_{^*_k}, k=1, 2, \\ldots, K\\) current estimates cluster centers.Given current set cluster centers \\(\\{m_1, m_2, \\ldots, m_K\\}\\), minimize total error assigning observation closest (current) cluster center:\n\\[C_i = \\text{arg}\\,\\min\\limits_{1 \\leq k \\leq K} d(x_i, m_k)\\]","code":""},{"path":"unsup.html","id":"evaluation-metrics","chapter":"10 Unsupervised Methods","heading":"10.5 Evaluation Metrics","text":"Silhouette WidthConsider observation \\(\\\\) cluster \\(Clus1\\). Let\n\\[\\begin{align}\nd(, Clus2) &= \\mbox{average dissimilarity } \\mbox{ objects cluster } Clus2\\\\\n() &=  \\mbox{average dissimilarity } \\mbox{ objects } Clus1.\\\\\nb() &= \\min_{Clus2 \\ne Clus1} d(,Clus2) = \\mbox{distance next closest neighbor cluster}\\\\\n\\mbox{ silhouette width} &= s() = \\frac{b() - ()}{\\max \\{ (), b() \\}}\\\\\n& &\\\\\n\\mbox{average}_{\\Clus1} s() &= \\mbox{average silhouette width cluster $Clus1$}\n\\end{align}\\]\nNote \\(() < b()\\) \\(\\) well classified maximum \\(s() = 1\\). \\(() > b()\\) \\(\\) well classified maximum \\(s() = -1\\).Diameter cluster \\(Clus1\\) (within cluster measure)\n\\[\\begin{align}\n\\mbox{diameter} = \\max_{,j \\Clus1} d(,j)\n\\end{align}\\]Diameter cluster \\(Clus1\\) (within cluster measure)\n\\[\\begin{align}\n\\mbox{diameter} = \\max_{,j \\Clus1} d(,j)\n\\end{align}\\]Separation cluster \\(Clus1\\) (cluster measure)\n\\[\\begin{align}\n\\mbox{separation} = \\min_{\\Clus1, j \\notin Clus1} d(,j)\n\\end{align}\\]Separation cluster \\(Clus1\\) (cluster measure)\n\\[\\begin{align}\n\\mbox{separation} = \\min_{\\Clus1, j \\notin Clus1} d(,j)\n\\end{align}\\]\\(L^*\\): cluster diameter \\(<\\) separation; \\(L\\): cluster \\(\\max_{j \\Clus1} d(,j) < \\min_{k \\notin Clus1} d(,k)\\).\\(L^*\\): cluster diameter \\(<\\) separation; \\(L\\): cluster \\(\\max_{j \\Clus1} d(,j) < \\min_{k \\notin Clus1} d(,k)\\).","code":""},{"path":"unsup.html","id":"pam-example","chapter":"10 Unsupervised Methods","heading":"PAM example","text":"","code":""},{"path":"unsup.html","id":"building-the-clusters","chapter":"10 Unsupervised Methods","heading":"Building the clusters","text":"Start considering random allocation (AC) (BDE)Start considering random allocation (AC) (BDE)second step, calculate within cluster sums distances:second step, calculate within cluster sums distances:: 0.6C: 0.6B: 0.9 + 0.8 = 1.7D: 0.9 + 0.3 = 1.2E: 0.8 + 0.3 = 1.1For cluster 1, doesn’t matter choose C (let’s choose C). cluster 2, choose E (“central” measured closer distance B D).Reallocate points:Cluster1: C points closer C E. B closer C E. Cluster1 (,B,C).Cluster2: E points closer E C. D closer E C. Cluster2 (D,E)Redefine cluster centers:: 0.2 + 0.6 = 0.8B: 0.2 + 0.5 = 0.7C: 0.6 + 0.5 = 1.1D: 0.3E: 0.3Cluster1 now medoid B. Cluster2 (choose randomly) medoid D.Reallocate points:Cluster1: B (,B)Cluster2: D C, E (D, C, E)medoids now B (randomly choose) D. iteration process converged.","code":""},{"path":"unsup.html","id":"evaluating-the-clusters","chapter":"10 Unsupervised Methods","heading":"Evaluating the clusters","text":"(Note: matrix , 4 observations.)Consider data (AB)(CD) clusters, can calculate previous metrics:Silhouette Width\n\\[\\begin{align}\ns(=) = \\frac{b() - ()}{\\max \\{(), b()\\}} = \\frac{0.8 - 0.2}{0.8} = 0.75\\\\\ns(=B) = \\frac{b(B) - (B)}{\\max \\{(B), b(B)\\}} = \\frac{0.7 - 0.2}{0.7} = 0.71\\\\\ns(=C) = \\frac{b(C) - (C)}{\\max \\{(C), b(C)\\}} = \\frac{0.55 - 0.4}{0.55} = .27\\\\\ns(=D) = \\frac{b(D) - (D)}{\\max \\{(D), b(D)\\}} = \\frac{0.95 - 0.4}{0.95} = .57\\\\\n\\mbox{Ave SW} = 0.575\\\\\n\\end{align}\\]Diameter\n\\[\\begin{align}\n\\mbox{diameter}(AB) = 0.2\\\\\n\\mbox{diameter}(CD) = 0.4\\\\\n\\end{align}\\]Separation\n\\[\\begin{align}\n\\mbox{separation}(AB) = \\mbox{separation}(CD) = 0.5\\\\\n\\end{align}\\]","code":""},{"path":"unsup.html","id":"rand-index-adjusted-rand-index","chapter":"10 Unsupervised Methods","heading":"Rand Index / Adjusted Rand Index","text":"based confusion matrix comparing either known truth (labels) comparing two different clusterings (e.g., comparing \\(k\\)-means hierarchical clustering). Let two different clusterings called partition1 partition2.\n* number pairs observations put together partition1 partition2\n* b number pairs observations together partition1 apart partition2\n* c number pairs observations together partition2 apart partition1\n* d number pairs observations apart partitions\\[\\mbox{Rand index} = \\frac{+d}{+b+c+d}\\]cool thing Rand index partitions don’t even number clusters. can absolutely two clusterings (one might known labels, example). Details Adjusted Rand index given http://faculty.washington.edu/kayee/pca/supp.pdf (basic idea center scale Rand index values meaningful).","code":""},{"path":"unsup.html","id":"em-algorithm","chapter":"10 Unsupervised Methods","heading":"10.6 EM algorithm","text":"EM algorithm incredibly useful tool solving complicated maximization procedures, particularly respect maximizing likelihoods (typically parameter estimation). describe procedure context estimating parameters two-component mixture model.Consider Old Faithful geyser Yellowstone National Park, Wyoming, USA following histogram data waiting times eruption:\\[\\begin{align}\nY_1 &\\sim N(\\mu_1, \\sigma_1^2)\\\\\nY_2 &\\sim N(\\mu_2, \\sigma_2^2)\\\\\nY &= (1-\\Delta) Y_1 + \\Delta Y_2\\\\\nP(\\Delta=1) &= \\pi\\\\\n\\end{align}\\]\nsimple two component case, can see representation indicates first generate \\(\\Delta \\\\{0,1\\}\\), , depending result, generate either \\(Y_1\\) \\(Y_2\\). likelihood associated setting :\\[\\begin{align}\ng_Y(y) = (1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)\n\\end{align}\\]\n\\(\\phi_\\theta\\) represents normal distribution vector \\(\\theta=(\\mu, \\sigma)\\) parameters. Typically, statistical theory, find \\(\\theta\\), take derivative log-likelihood find values maximize. , however, likelihood complicated solve \\(\\theta\\) closed form.\\[\\begin{align}\nl(\\theta; {\\bf y}) = \\sum_{=1}^N \\log [(1-\\pi) \\phi_{\\theta_1}(y) + \\pi \\phi_{\\theta_2}(y)].\n\\end{align}\\]know point comes distribution, however, maximization straightforward can use points group one estimate parameters first distribution, points group two estimate parameters second distribution. process assigning points estimating parameters can thought two steps:Expectation: assignment (soft , points weighted) observation group.Maximization: update parameter estimates.Algorithm: EM Algorithm two-component Gaussian mixture. Elements Statistical Learning (2001), Hastie, Tibshirani, Friedman, pg 238.Take initial guesses parameters \\(\\hat{\\mu}_1, \\hat{\\sigma}_1^2, \\hat{\\mu}_2, \\hat{\\sigma}_2^2, \\hat{\\pi}\\).Expectation Step: compute responsibilities:\n\\[ \\hat{\\gamma}_i = \\frac{\\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}{(1-\\hat{\\pi}) \\phi_{\\hat{\\theta}_1} (y_i) + \\hat{\\pi} \\phi_{\\hat{\\theta}_2} (y_i)}, =1, 2, \\ldots, N.\\]Maximization Step: compute weighted means variances:\n\\[\\begin{align}\n\\hat{\\mu}_1 = \\frac{\\sum_{=1}^N (1-\\hat{\\gamma_i})y_i}{\\sum_{=1}^N (1-\\hat{\\gamma_i})} && \\hat{\\sigma}_1^2 = \\frac{\\sum_{=1}^N (1-\\hat{\\gamma_i})(y_i - \\hat{\\mu}_1)^2}{\\sum_{=1}^N (1-\\hat{\\gamma_i})}\\\\\n\\hat{\\mu}_2 = \\frac{\\sum_{=1}^N \\hat{\\gamma_i}y_i}{\\sum_{=1}^N \\hat{\\gamma_i}} && \\hat{\\sigma}_2^2 = \\frac{\\sum_{=1}^N \\hat{\\gamma_i}(y_i - \\hat{\\mu}_2)^2}{\\sum_{=1}^N \\hat{\\gamma_i}}\n\\end{align}\\]\nmixing probability \\(\\hat{\\pi} = \\sum_{=1}^N \\hat{\\gamma}_i / N\\).Iterate Steps 2. 3. convergence.algorithm shows particular allocation points, can maximize given likelihood estimate parameter values (done Maximization Step). However, obvious algorithm first allocation step leads maximization (local global) likelihood. proof EM algorithm converging local maximum likelihood (necessarily converge global max) uses information marginal prior posterior likelihoods parameter values Jensen’s inequality show likelihood decrease iterative steps.Note previous \\(k\\)-means algorithm iterated two steps assigning points clusters estimating cluster centers (thought space scaled Euclidean distance appropriate dimensions). Two differences algorithms covered :\\(k\\)-means uses hard thresholding EM uses soft thresholding\\(k\\)-means uses fixed standard deviation 1, EM allows data/algorithm find standard deviationIndeed, although EM-algorithm slightly different previous \\(k\\)-means algorithm, two methods typically converge result considered different implementations \\(k\\)-means algorithm.See following applet visual representation EM-algorithm converges: http://www.socr.ucla.edu/applets.dir/mixtureem.html.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
