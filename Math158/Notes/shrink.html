<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 14 Shrinkage Methods | Linear Models</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="Recall the two methods we’ve used so far to find sets of variable (and their coefficients): Choose models using domain knowledge and then applying computational methods (i.e., cross validation) to...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 14 Shrinkage Methods | Linear Models">
<meta property="og:type" content="book">
<meta property="og:description" content="Recall the two methods we’ve used so far to find sets of variable (and their coefficients): Choose models using domain knowledge and then applying computational methods (i.e., cross validation) to...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 14 Shrinkage Methods | Linear Models">
<meta name="twitter:description" content="Recall the two methods we’ve used so far to find sets of variable (and their coefficients): Choose models using domain knowledge and then applying computational methods (i.e., cross validation) to...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="wrang.html"><span class="header-section-number">2</span> Data Wrangling</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">3</span> Visualization</a></li>
<li><a class="" href="slr.html"><span class="header-section-number">4</span> Simple Linear Regression</a></li>
<li><a class="" href="infslr.html"><span class="header-section-number">5</span> Inference on SLR Parameters</a></li>
<li><a class="" href="diag1.html"><span class="header-section-number">6</span> Diagnostic Measures I</a></li>
<li><a class="" href="simult.html"><span class="header-section-number">7</span> Simultaneous Inference</a></li>
<li><a class="" href="la.html"><span class="header-section-number">8</span> Regression using Matrices</a></li>
<li><a class="" href="mlr.html"><span class="header-section-number">9</span> Multiple Linear Regression</a></li>
<li><a class="" href="process.html"><span class="header-section-number">10</span> Modeling as a Process6</a></li>
<li><a class="" href="build.html"><span class="header-section-number">11</span> Statistical Model Building</a></li>
<li><a class="" href="diag2.html"><span class="header-section-number">12</span> Diagnostic Measures II</a></li>
<li><a class="" href="standardized-multiple-regression.html"><span class="header-section-number">13</span> Standardized Multiple Regression</a></li>
<li><a class="active" href="shrink.html"><span class="header-section-number">14</span> Shrinkage Methods</a></li>
<li><a class="" href="smooth.html"><span class="header-section-number">15</span> Smoothing Methods</a></li>
<li><a class="" href="anova.html"><span class="header-section-number">16</span> ANOVA</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="shrink" class="section level1" number="14">
<h1>
<span class="header-section-number">14</span> Shrinkage Methods<a class="anchor" aria-label="anchor" href="#shrink"><i class="fas fa-link"></i></a>
</h1>
<p>Recall the two methods we’ve used so far to find sets of variable (and their coefficients):</p>
<ol style="list-style-type: decimal">
<li>Choose models using domain knowledge and then applying <strong>computational</strong> methods (i.e., cross validation) to decide which model is superior. Apply least squares (i.e., calculus) to find the coefficients.<br>
</li>
<li>Adding variables systematically using <strong>statistical</strong> criteria (F-tests, adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span>, BIC, AIC). Apply least squares (i.e., calculus) to find the coefficients.</li>
</ol>
<p>We move to a new model building algorithm that uses all <span class="math inline">\(p-1\)</span> explanatory variables and constrains the coefficient estimates, or equivalently, shrinks the coefficient estimates toward zero. It turns out that shrinking the coefficients toward zero can substantially reduce the variance of the estimates (albeit adding a bit of bias).</p>
<ol start="3" style="list-style-type: decimal">
<li>That is, use <strong>mathematical</strong> optimization to shrink (set) some of the coefficients to zero (while simultaneously solving for the other non-zero coefficients).</li>
</ol>
<p>The main reason to use shrinkage techniques is when the model has a lot of predictor variables (<span class="math inline">\(p\)</span> big) or when the model has a lot of predictor variables in comparison to the number of observations (<span class="math inline">\(n \approx p\)</span> or <span class="math inline">\(n &lt; p\)</span>) — because that is when we get huge (infinite!) variability associated with the coefficients!</p>
<div id="model-complexity-flexibility" class="section level2" number="14.1">
<h2>
<span class="header-section-number">14.1</span> Model Complexity / Flexibility<a class="anchor" aria-label="anchor" href="#model-complexity-flexibility"><i class="fas fa-link"></i></a>
</h2>
<p>The words <strong>complexity</strong> and <strong>flexibility</strong> can be thought of as synonyms. They both refer to how (not) simple a model is. However, their use is slightly different. A polynomial with 20 degrees is very flexible, but might not be described as complex (because it isn’t too hard to write down). A piecewise cubic model with dozens of pieces might not be overly flexible but would be complicated to write down. Or maybe both of the examples just given would be thought of as complex and flexible.</p>
<p>In the models we’ve been discussing, flexibility refers to the number of curves in a “linear” model. The more curve-y (think: polynomial with degree of many dozens), the more the model will (over-)fit the observations.</p>
<p>There are many other models, and you can see the flexibility scale of models from this class and of models not covered in this class. In non-linear models, flexibility refers to whether the model is fitting high level functions of the data. Sometimes that means over-fitting. But sometimes a lot of flexibility can capture true but complicated relationships among the variables. Note that flexibility is often at the expense of interpretability.</p>
<p>Note that for Lasso and Ridge Regression, the models are less flexible because they constrain the coefficient estimates. The constraints increase the bias and decrease the variability.</p>
<ul>
<li>increases in bias: the constrained coefficients won’t fit the observations as well as OLS. That is, the trend is different from what the data are presenting.<br>
</li>
<li>decrease in variability: the constrained coefficients attenuate to zero which means that a different dataset will also have coefficients close to zero. By forcing the coefficients to be small and closer to one another across different datasets, the predictions will then me more similar (and less variable).</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="figs/flexibility.png" alt="A coordinate plane with individual models at particular coordinate points.  Linear models are less flexible but easy to interpret; neural networks are very flexible and hard to interpret." width="786"><p class="caption">
Figure 1.4: Image credit: Figure 2.7 of ISLR
</p>
</div>
</div>
<div id="on-inverting-matrices" class="section level2" number="14.2">
<h2>
<span class="header-section-number">14.2</span> On Inverting Matrices<a class="anchor" aria-label="anchor" href="#on-inverting-matrices"><i class="fas fa-link"></i></a>
</h2>
<p><strong><span class="math inline">\(X_{n \times p}\)</span>:</strong> The data matrix is <span class="math inline">\(n \times p\)</span>. We think of the data as being <span class="math inline">\(n\)</span> points in <span class="math inline">\(p\)</span> dimensions. It is important to realize that not only are the points indexed in <span class="math inline">\(p\)</span> dimensions, but the points also take up the entire p-dimensional space. [As an aside: Whether a set of points takes up an entire space does not depend on how the points are indexed. For example, think about a (2-dimensional) piece of paper floating in the sky with many many points on it. You might index the coordinates of points on the paper using three dimensions, however, the paper / points themselves actually live in a 2-dimensional subspace.] To reiterate: in an <strong>ideal</strong> world, the <span class="math inline">\(n\)</span> points live in <span class="math inline">\(p\)</span> space and cannot be considered to live in a smaller dimension than <span class="math inline">\(p\)</span>. Sometimes, the <span class="math inline">\(n\)</span> points are indexed to live in <span class="math inline">\(p\)</span> space, but actually take up a lower dimensional subspace (e.g., if two of the variable columns are perfectly correlated).</p>
<p><em>However</em>, there are times when an <span class="math inline">\(n \times p\)</span> matrix lives in a space which is smaller than <span class="math inline">\(p\)</span>. For example,</p>
<ol style="list-style-type: decimal">
<li>If two of the <span class="math inline">\(p\)</span> columns are exact linear combinations of one another, then the points will actually live in <span class="math inline">\(p-1\)</span> space.</li>
<li>If the number of points is less than <span class="math inline">\(p\)</span> (<span class="math inline">\(n &lt; p\)</span>) then the points will only live in <span class="math inline">\(n\)</span> space. For example, there is no way for two points take up three dimensions!</li>
</ol>
<p><strong><span class="math inline">\(X^t X\)</span>:</strong> The matrix <span class="math inline">\(X^tX\)</span> is a linear transformation of the original <span class="math inline">\(X\)</span> matrix. That is, if <span class="math inline">\(X\)</span> lives in <span class="math inline">\(p\)</span> space, it can’t be linearly transformed into a higher dimension. We could transform <span class="math inline">\(X\)</span> into a higher dimension by using functions or some kind of kernel mapping, but we can’t do it via linear transformations. That is to say, if <span class="math inline">\(X\)</span> has a rank which is lower than <span class="math inline">\(p\)</span>, any linear combination will also, necessarily, transform the data into a space which is lower than <span class="math inline">\(p\)</span>.</p>
<p><strong><span class="math inline">\((X^tX)^{-1}\)</span>:</strong> The inverse of the matrix also represents a mapping. Recall that if <span class="math inline">\(AX = Y\)</span> then <span class="math inline">\(X= A^{-1}Y\)</span>. But if we are mapping into a smaller space (smaller than <span class="math inline">\(p\)</span>) then we can’t invert back to a larger space (i.e., back to <span class="math inline">\(p\)</span>). And because <span class="math inline">\((X^tX)^{-1}\)</span> is a <span class="math inline">\(p \times p\)</span> matrix, we are trying to invert back to a <span class="math inline">\(p\)</span> dimensional space. Recall the Big Theorem in Linear Algebra that says if a <span class="math inline">\(p \times p\)</span> matrix has rank lower than <span class="math inline">\(p\)</span>, it isn’t invertible (also that it will have determinant zero, will have some eigenvalues that are zero, etc.)</p>
<p><strong>So…</strong> The point is that if <span class="math inline">\(X\)</span> doesn’t have full rank (that is, if it has dimension less than <span class="math inline">\(p\)</span>), there will be problems with computing <span class="math inline">\((X^tX)^{-1}\)</span>. And the matrix <span class="math inline">\((X^tX)^{-1}\)</span> is vitally important in computing both the least squares coefficients and their standard errors.</p>
</div>
<div id="ridge-regression" class="section level2" number="14.3">
<h2>
<span class="header-section-number">14.3</span> Ridge Regression<a class="anchor" aria-label="anchor" href="#ridge-regression"><i class="fas fa-link"></i></a>
</h2>
<p>An excellent discussion of Ridge Regression is given by <span class="citation"><a href="references.html#ref-rr_theory" role="doc-biblioref">Wieringen</a> (<a href="references.html#ref-rr_theory" role="doc-biblioref">2021</a>)</span>.</p>
<p>Recall that the OLS (ordinary least squares) technique minimizes the distance between the observed and predicted values of the response. That is, we found the <span class="math inline">\(b_0, b_1, \ldots b_{p-1}\)</span> that minimize:
<span class="math display">\[SSE = \sum_{i=1}^n \bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \bigg)^2.\]</span></p>
<p>Using the OLS algorithm for modeling, the values of <span class="math inline">\(b_i\)</span> give the exact same model as the model built using the standardized variables which produce <span class="math inline">\(b_i^*\)</span>. However, as you will see, it is important to <strong>standardize all variables before running ridge regression</strong>.</p>
<p>For ease of computation, we will assume from here on that the variables have all been standardized as described above (in a previous section). [Note: ISL describes standardizing by only dividing by the standard deviation and not centering. The two different methods will not produce different models (with respect to significance, etc.), but they will produce different intercept coefficients. Indeed, scale is the important aspect to consider when working with shrinkage models.]</p>
<p>The ridge regression coefficients are calculated in a similar way to OLS, but the optimization equation seeks to minimize:</p>
<p><span class="math display">\[\sum_{i=1}^n \bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \bigg)^2 + \lambda \sum_{j=1}^{p-1} b_j^2 = SSE + \lambda \sum_{j=1}^{p-1} b_j^2\]</span>
where <span class="math inline">\(\lambda \geq 0\)</span> is a <em>tuning parameter</em>, to be determined separately. The ridge regression optimization provides a trade-off between two different criteria: variance and bias. As with OLS, ridge regression seeks to find coefficients that fit the data well (minimize SSE!). Additionally, ridge regression shrinks the coefficients to zero by adding a penalty so that smaller values of <span class="math inline">\(b_j\)</span> are preferred - the shrinkage makes the estimates less variable. It can be proved that there is always a <span class="math inline">\(\lambda \ne 0\)</span> which gives smaller E[MSE] than OLS. Note: the shrinkage does not apply to the intercept!</p>
<p>Note: the Gauss-Markov theorem says that OLS solutions are the best (i.e., smallest variance) linear unbiased estimates (BLUE). But if we add a bit of bias, we can do better. There is an existence theorem <span class="citation">(<a href="references.html#ref-rr_wins" role="doc-biblioref">Theobald 1974</a>)</span> that says:
<span class="math display">\[ \exists \ \  \lambda \mbox{ such that } E[MSE_{RR}] &lt; E[MSE_{OLS}].\]</span>
Excellent description of the theory is given by <span class="citation"><a href="references.html#ref-rr_theory" role="doc-biblioref">Wieringen</a> (<a href="references.html#ref-rr_theory" role="doc-biblioref">2021</a>)</span>.</p>
<div id="the-ridge-regression-solution" class="section level3" number="14.3.1">
<h3>
<span class="header-section-number">14.3.1</span> The Ridge Regression Solution<a class="anchor" aria-label="anchor" href="#the-ridge-regression-solution"><i class="fas fa-link"></i></a>
</h3>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{OLS: } \underline{b} &amp;=&amp; (X^t X)^{-1} X^t \underline{Y}\\
\mbox{ridge regression: } \underline{b} &amp;=&amp; (X^t X + \lambda \mathtt{I})^{-1} X^t \underline{Y}\\
\end{eqnarray*}\]</span></p>
<p><strong>Notes:</strong></p>
<ul>
<li>The tuning parameter <span class="math inline">\(\lambda\)</span> balances the effect of the two different criteria in the optimization equation. When <span class="math inline">\(\lambda=0\)</span>, ridge regression is reduced to OLS. As <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the coefficient estimates will shrink to zero.<br>
</li>
<li>We have assumed that the variables have been centered to have mean zero before ridge regression is performed. Therefore, the estimated intercept will be <span class="math display">\[b_0 = \overline{Y} = \frac{1}{n} \sum_i Y_i \ \ \ \ \ \ (= 0 \mbox{ if $Y$ is also centered}) \]</span><br>
</li>
<li>Note that the ridge regression optimization above is a constrained optimization equation, solved by Lagrange multipliers. That is, we minimize <span class="math inline">\(SSE = \sum_{i=1}^n \bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \bigg)^2\)</span> subject to <span class="math inline">\(\sum_{j=1}^{p-1} b_j^2 \leq s\)</span> for some value of <span class="math inline">\(s\)</span>. Note that <span class="math inline">\(s\)</span> is inversely related to <span class="math inline">\(\lambda\)</span>.</li>
</ul>
</div>
<div id="ridge-regression-visually" class="section level3" number="14.3.2">
<h3>
<span class="header-section-number">14.3.2</span> Ridge Regression visually<a class="anchor" aria-label="anchor" href="#ridge-regression-visually"><i class="fas fa-link"></i></a>
</h3>
<p>Ridge regression is typically applied to situations with many many variables. In particular, ridge regression will help us avoid situations of multicollinearity. It doesn’t make sense to apply it to a situation with only one or two variables. However, we will demonstrate the process visually with <span class="math inline">\(p=3\)</span> dimensions because it is difficult to visualize in higher dimensions.</p>
<p>The ridge regression coefficient estimates solve the following optimization problem:</p>
<p><span class="math display">\[ \min_\beta \Bigg\{ \sum_{i=1}^n \Bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \Bigg)^2 \Bigg\} \mbox{  subject to  } \sum_{j=1}^{p-1} b_j^2 \leq s\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="figs/rrContours.jpg" alt="A coordinate plane with beta 1 on the x-axis and beta 2 on the y-axis.  A blue disk around the origin represents the contraint region.  Red elipses show regions of constant SSE that are hoped to be minimized." width="265"><p class="caption">
Figure 14.1: The red contours represent pairs of <span class="math inline">\(\beta\)</span> coefficients that produce constant values for SSE on the data. The blue circle represents the possible values of <span class="math inline">\(\beta\)</span> given the constraint (that the squared magnitude is less than some cutoff). Image credit: ISLR
</p>
</div>
</div>
<div id="why-ridge-regression" class="section level3" number="14.3.3">
<h3>
<span class="header-section-number">14.3.3</span> Why Ridge Regression?<a class="anchor" aria-label="anchor" href="#why-ridge-regression"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that the main benefit to ridge regression is when <span class="math inline">\(p\)</span> is large (particularly in relation to <span class="math inline">\(n\)</span>). Remember that this (and multicollinearity) leads to instability of <span class="math inline">\((X^tX)^{-1}\)</span>. Which leads to huge variability in the coefficient estimates. A small change in the model or the observations can create wildly different estimates. So the expected value of the MSE can be quite inflated due to the variability of the model. Ridge regression adds a small amount of bias to the model, but it lowers the variance substantially and creates lower (on average) MSE values.</p>
<p>Additionally, ridge regression has computational advantages over the subset selection models (where all subsets requires searching through <span class="math inline">\(2^{p-1}\)</span> models).</p>
<div id="why-is-ridge-regression-better-than-least-squares" class="section level4 unnumbered">
<h4>Why is ridge regression better than least squares?<a class="anchor" aria-label="anchor" href="#why-is-ridge-regression-better-than-least-squares"><i class="fas fa-link"></i></a>
</h4>
<p>The advantage is apparent in the bias-variance trade-off. As <span class="math inline">\(\lambda\)</span> increases, the flexibility of the ridge regression fit decreases. This leads to decrease variance, with a smaller increase in bias. Regular OLS regression is fixed with high variance, but no bias. However, the lowest test MSE tends to occur at a balance between variance and bias. Thus, by properly tuning <span class="math inline">\(\lambda\)</span> and acquiring less variance at the cost of a small amount of bias, we can find a lower potential MSE.</p>
<blockquote>
<p>Ridge regression works best in situations for least squares estimates have high variance. Ridge regression is also much more computationally efficient that any subset method, since it is possible to simultaneously solve for all values of <span class="math inline">\(\lambda\)</span>.</p>
</blockquote>
</div>
</div>
<div id="inference-on-ridge-regression-coefficients" class="section level3" number="14.3.4">
<h3>
<span class="header-section-number">14.3.4</span> Inference on Ridge Regression Coefficients<a class="anchor" aria-label="anchor" href="#inference-on-ridge-regression-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>Note that just like the OLS coefficients, the RR coefficients are a linear combination of the <span class="math inline">\(Y\)</span> values based on the <span class="math inline">\(X\)</span> matrix and (now) <span class="math inline">\(\lambda\)</span>. It is not hard, therefore, to find the variance of the coefficient vector at a particular value of <span class="math inline">\(\lambda\)</span>. Additionally, the same theory that gives normality (and the resulting t-statistics) drives normality for the ridge regression coefficients.</p>
<p><span class="math display">\[var(b^{RR}) = \sigma^2 W X^t X W \mbox{ where } W = (X^t X + \lambda \mathtt{I})^{-1}\]</span></p>
<p>However, all the distributional properties above give theoretical results for a fixed value of <span class="math inline">\(\lambda\)</span>. We now discuss estimating <span class="math inline">\(\lambda\)</span>, but as soon as we estimate <span class="math inline">\(\lambda\)</span>, it becomes dependent on the data and thus a random variable. That is, the SE of <span class="math inline">\(b^{RR}\)</span> is a function of not only the variability associated with the data estimating the coefficients but also the variability of the data estimating <span class="math inline">\(\lambda\)</span>.</p>
<p>An additional problem is that the RR coefficients are known to be biased, and the bias is not easy to estimate. Without a sense of where the variable is centered, the SE isn’t particularly meaningful. For these reasons, functions like <code><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">lm.ridge()</a></code> in R do not include tests / p-values but do approximate the SE of the coefficients (as an estimate).</p>
</div>
</div>
<div id="how-do-you-choose-lambda" class="section level2" number="14.4">
<h2>
<span class="header-section-number">14.4</span> How do you choose <span class="math inline">\(\lambda\)</span>?<a class="anchor" aria-label="anchor" href="#how-do-you-choose-lambda"><i class="fas fa-link"></i></a>
</h2>
<p>Note that <span class="math inline">\(\lambda\)</span> is a function of the data, and therefore a random variable to estimate (just like estimating the coefficients). However, we can use diagnostic measures to give a sense of <span class="math inline">\(\lambda\)</span> values which will give a good variance-bias trade-off.</p>
<ol style="list-style-type: decimal">
<li>Split data into test and training, and plot test MSE as a function of <span class="math inline">\(\lambda\)</span>.<br>
</li>
<li>Actually cross validate the data (remove test samples in groups of, say 1/10, to see which <span class="math inline">\(\lambda\)</span> gives the best predictions on “new” data) and find <span class="math inline">\(\lambda\)</span> which gives the smallest MSE for the cross validated data.</li>
</ol>
<hr>
<p><strong>Cross Validating to Find <span class="math inline">\(\lambda\)</span></strong></p>
<hr>
<ol style="list-style-type: decimal">
<li>Set <span class="math inline">\(\lambda\)</span> (e.g., try <span class="math inline">\(\lambda\)</span> between <span class="math inline">\(10^{-2}\)</span> and <span class="math inline">\(10^{5}\)</span>: <code>lambda.grid = 10^seq(5,-2, length =100)</code>)
<ol style="list-style-type: lower-alpha">
<li>Remove 1/10 of the observations (partition the data into 10 groups).</li>
<li>Find the RR / Lasso model using the remaining 90% of the observations and the given value of <span class="math inline">\(\lambda\)</span>.</li>
<li>Predict the response value for the removed points given the 90% training values.</li>
<li>Repeat (a) - (c) until every point has been predicted as a test value.</li>
</ol>
</li>
<li>Using the CV predictions, find <span class="math inline">\(MSE_\lambda\)</span> for the <span class="math inline">\(\lambda\)</span> at hand.</li>
<li>Repeat steps 1 and 2 across the grid of <span class="math inline">\(\lambda\)</span> values.</li>
<li>Choose the <span class="math inline">\(\lambda\)</span> value that minimizes the CV <span class="math inline">\(MSE_\lambda\)</span>.</li>
</ol>
</div>
<div id="lasso" class="section level2" number="14.5">
<h2>
<span class="header-section-number">14.5</span> Lasso<a class="anchor" aria-label="anchor" href="#lasso"><i class="fas fa-link"></i></a>
</h2>
<p>Ridge regression had at least one disadvantage; it includes all <span class="math inline">\(p\)</span> predictors in the final model. The penalty term will set many of them close to zero, but never exactly to zero. This isn’t generally a problem for prediction accuracy, but it can make the model more difficult to interpret the results. Lasso overcomes this disadvantage and is capable of forcing some of the coefficients to zero granted that <span class="math inline">\(\lambda\)</span> is big enough. Since <span class="math inline">\(\lambda = 0\)</span> results in regular OLS regression, as <span class="math inline">\(\lambda\)</span> approaches <span class="math inline">\(\infty\)</span> the coefficients shrink towards zero.</p>
<div id="lasso-coefficients" class="section level3" number="14.5.1">
<h3>
<span class="header-section-number">14.5.1</span> Lasso Coefficients<a class="anchor" aria-label="anchor" href="#lasso-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>The lasso (least absolute shrinkage and selection operator) coefficients are calculated from a similar constraint to that of ridge regression, but the calculus is much harder now. The L-1 norm is the only norm that gives sparsity and is convex (so that the optimization problem can be solved). The lasso optimization equation seeks to minimize:</p>
<p><span class="math display">\[\sum_{i=1}^n \bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \bigg)^2 + \lambda \sum_{j=1}^{p-1} |b_j| = SSE + \lambda \sum_{j=1}^{p-1} |b_j|\]</span>
where <span class="math inline">\(\lambda \geq 0\)</span> is a <em>tuning parameter</em>, to be determined separately. As with ridge regression lasso optimization provides a trade-off between bias and variance. Lasso seeks to find coefficients that fit the data well (minimize SSE!). Additionally, lasso shrinks the coefficients to zero by adding a penalty so that smaller values of <span class="math inline">\(b_j\)</span> are preferred. Note: the shrinkage does not apply to the intercept! The minimization quantities for ridge regression and lasso are extremely similar: ridge regression constrains the sum of the squared coefficients; lasso constrains the sum of the absolute coefficients. [As with ridge regression, we use standardized variables in modeling.]</p>
</div>
<div id="lasso-visually" class="section level3" number="14.5.2">
<h3>
<span class="header-section-number">14.5.2</span> Lasso visually<a class="anchor" aria-label="anchor" href="#lasso-visually"><i class="fas fa-link"></i></a>
</h3>
<p>As with ridge regression Lasso is also typically applied to situations with many many variables (also to avoid multicollinearity). It doesn’t make sense to apply it to a situation with only one or two variables. However, we will demonstrate the process visually with p=3 dimensions because it is difficult to visualize in higher dimensions. Notice here that there is a very good chance for the red contours to hit the turquoise diamond at a corner (producing some coefficients to be estimated as zero). The corner effect becomes more extreme in higher dimensions.</p>
<p>The lasso coefficient estimates solve the following optimization problem:</p>
<p><span class="math display">\[ \min_\beta \Bigg\{ \sum_{i=1}^n \Bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \Bigg)^2 \Bigg\} \mbox{  subject to  } \sum_{j=1}^{p-1} |b_j| \leq s\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="figs/lassoContours.jpg" alt="A coordinate plane with beta 1 on the x-axis and beta 2 on the y-axis.  A blue diamond around the origin represents the contraint region.  Red elipses show regions of constant SSE that are hoped to be minimized." width="277"><p class="caption">
Figure 1.5: The red contours represent pairs of <span class="math inline">\(\beta\)</span> coefficients that produce constant values for SSE on the data. The blue diamond represents the possible values of <span class="math inline">\(\beta\)</span> given the constraint (that the absolute magnitude is less than some cutoff). Image credit: ISLR
</p>
</div>
<p>The key to lasso (in contrast to ridge regression) is that it does variable selection by shrinking the coefficients all the way to zero. We say that the lasso yields <em>sparse</em> models - that is, only a subset of the original variables will be retained in the final model.</p>
</div>
<div id="how-do-you-choose-lambda-1" class="section level3" number="14.5.3">
<h3>
<span class="header-section-number">14.5.3</span> How do you choose <span class="math inline">\(\lambda\)</span>?<a class="anchor" aria-label="anchor" href="#how-do-you-choose-lambda-1"><i class="fas fa-link"></i></a>
</h3>
<p>Note that <span class="math inline">\(\lambda\)</span> is a function of the data, and therefore a random variable to estimate (just like estimating the coefficients). However, we can use diagnostic measures to give a sense of <span class="math inline">\(\lambda\)</span> values which will give a good variance-bias trade-off.</p>
<ol style="list-style-type: decimal">
<li>Split data into test and training, and plot test MSE as a function of <span class="math inline">\(\lambda\)</span>.<br>
</li>
<li>Actually cross validate the data (remove test samples in groups of, say 1/10, to see which <span class="math inline">\(\lambda\)</span> gives the best predictions on “new” data) and find <span class="math inline">\(\lambda\)</span> which gives the smallest MSE for the cross validated data.</li>
</ol>
</div>
</div>
<div id="ridge-regression-vs.-lasso" class="section level2" number="14.6">
<h2>
<span class="header-section-number">14.6</span> Ridge Regression vs. Lasso<a class="anchor" aria-label="anchor" href="#ridge-regression-vs.-lasso"><i class="fas fa-link"></i></a>
</h2>
<p>Quote from <em>An Introduction to Statistical Learning</em>, V2, page 246.</p>
<blockquote>
<p>These two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.</p>
</blockquote>
<blockquote>
<p>As with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret.</p>
</blockquote>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-11-1"></span>
<img src="figs/fig68RRwins.jpg" alt="From ISLR, pgs 245-246.  The data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response.  That is, none of the true coefficients beta1,... , beta45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE." width="649"><p class="caption">
Figure 14.2: From ISLR, pgs 245-246. The data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response. That is, none of the true coefficients beta1,… , beta45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.
</p>
</div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-11-2"></span>
<img src="figs/fig69lassowins.jpg" alt="From ISLR, pgs 245-246.  The data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response.  That is, none of the true coefficients beta1,... , beta45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE." width="652"><p class="caption">
Figure 14.3: From ISLR, pgs 245-246. The data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response. That is, none of the true coefficients beta1,… , beta45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.
</p>
</div>
</div>
<div id="elastic-net" class="section level2" number="14.7">
<h2>
<span class="header-section-number">14.7</span> Elastic Net<a class="anchor" aria-label="anchor" href="#elastic-net"><i class="fas fa-link"></i></a>
</h2>
<p>It is also possible to combine ridge regression and lasso through what is called <em>elastic net regularization</em>. (The R package <strong>glmnet</strong> allows for the combined elastic net model.) The main idea is that the optimization contains both L-1 and L-2 penalties. The model may produce more stable estimates of the coefficients but requires and additional tuning parameter to estimate. That is, find the coefficients that minimize:</p>
<p><span class="math display">\[\sum_{i=1}^n \bigg( Y_i - b_0 - \sum_{j=1}^{p-1} b_j X_{ij} \bigg)^2 + \lambda \bigg[(1-\alpha)(\frac{1}{2})\sum_{j=1}^{p-1} b_j^2  + \alpha \sum_{j=1}^{p-1} |b_j| \bigg].\]</span></p>
</div>
<div id="reflection-questions-10" class="section level2" number="14.8">
<h2>
<span class="header-section-number">14.8</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-10"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>How do ridge regression coefficient estimates differ from OLS estimates? How are they similar?<br>
</li>
<li>How do Lasso coefficient estimates differ from OLS estimates? How are they similar?<br>
</li>
<li>What is the difference between ridge regression and Lasso?<br>
</li>
<li>What are some of the ways to find a good <span class="math inline">\(\lambda\)</span> for ridge regression?<br>
</li>
<li>Why does the 1-norm regularization yield “sparse” solutions? (What does “sparse” mean?)<br>
</li>
<li>Give two different situations when ridge regression or Lasso are particularly appropriate.</li>
</ol>
</div>
<div id="ethics-considerations-9" class="section level2" number="14.9">
<h2>
<span class="header-section-number">14.9</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-9"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="r-ridge-regression" class="section level2" number="14.10">
<h2>
<span class="header-section-number">14.10</span> R: Ridge Regression<a class="anchor" aria-label="anchor" href="#r-ridge-regression"><i class="fas fa-link"></i></a>
</h2>
<div id="the-data-1" class="section level4 unnumbered">
<h4>The Data<a class="anchor" aria-label="anchor" href="#the-data-1"><i class="fas fa-link"></i></a>
</h4>
<p>The following dataset is from <a href="https://github.com/rfordatascience/tidytuesday">TidyTuesday</a>. Again, we explore the information from <a href="https://www.imdb.com/title/tt0386676/">The Office</a>. The analysis here is taken from <a href="https://juliasilge.com/blog/lasso-the-office/">Julia Silge’s blog</a>. She does a bit of data wrangling that I’m going to hide. Look through her blog, or see the <a href="https://github.com/hardin47/website/tree/gh-pages/Math158">source code</a> for this bookdown file.</p>
<p>The dataset we will be working with has <code>imdb_rating</code> as the response variable. The predictor (explanatory) variables are: <code>season</code>, <code>episode</code> and 28 columns representing the number of lines of a particular character.</p>
<div class="sourceCode" id="cb603"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">office</span></code></pre></div>
<pre><code>## # A tibble: 136 × 32
##    season episode episode_name       andy angela darryl dwight   jim kelly kevin
##     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1      1       1 pilot                 0      1      0     29    36     0     1
##  2      1       2 diversity day         0      4      0     17    25     2     8
##  3      1       3 health care           0      5      0     62    42     0     6
##  4      1       5 basketball            0      3     15     25    21     0     1
##  5      1       6 hot girl              0      3      0     28    55     0     5
##  6      2       1 dundies               0      1      1     32    32     7     1
##  7      2       2 sexual harassment     0      2      9     11    16     0     6
##  8      2       3 office olympics       0      6      0     55    55     0     9
##  9      2       4 fire                  0     17      0     65    51     4     5
## 10      2       5 halloween             0     13      0     33    30     3     2
## # … with 126 more rows, and 22 more variables: michael &lt;int&gt;, oscar &lt;int&gt;,
## #   pam &lt;int&gt;, phyllis &lt;int&gt;, ryan &lt;int&gt;, toby &lt;int&gt;, erin &lt;int&gt;, jan &lt;int&gt;,
## #   ken_kwapis &lt;dbl&gt;, greg_daniels &lt;dbl&gt;, b_j_novak &lt;dbl&gt;,
## #   paul_lieberstein &lt;dbl&gt;, mindy_kaling &lt;dbl&gt;, paul_feig &lt;dbl&gt;,
## #   gene_stupnitsky &lt;dbl&gt;, lee_eisenberg &lt;dbl&gt;, jennifer_celotta &lt;dbl&gt;,
## #   randall_einhorn &lt;dbl&gt;, brent_forrester &lt;dbl&gt;, jeffrey_blitz &lt;dbl&gt;,
## #   justin_spitzer &lt;dbl&gt;, imdb_rating &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb605"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">47</span><span class="op">)</span>
<span class="va">office_split</span> <span class="op">&lt;-</span> <span class="fu">initial_split</span><span class="op">(</span><span class="va">office</span>, strata <span class="op">=</span> <span class="va">season</span><span class="op">)</span>
<span class="va">office_train</span> <span class="op">&lt;-</span> <span class="fu">training</span><span class="op">(</span><span class="va">office_split</span><span class="op">)</span>
<span class="va">office_test</span> <span class="op">&lt;-</span> <span class="fu">testing</span><span class="op">(</span><span class="va">office_split</span><span class="op">)</span></code></pre></div>
<p>The full linear model:</p>
<div class="sourceCode" id="cb606"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">office_lm</span> <span class="op">&lt;-</span> <span class="va">office_train</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">episode_name</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">imdb_rating</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span>

<span class="va">office_lm</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 5
##    term         estimate std.error statistic  p.value
##    &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
##  1 (Intercept)  7.19       0.315     22.8    3.83e-34
##  2 season      -0.00222    0.0366    -0.0607 9.52e- 1
##  3 episode      0.0145     0.00730    1.98   5.15e- 2
##  4 andy         0.00215    0.00424    0.507  6.14e- 1
##  5 angela       0.00307    0.00865    0.354  7.24e- 1
##  6 darryl       0.000932   0.00783    0.119  9.06e- 1
##  7 dwight      -0.00172    0.00380   -0.452  6.53e- 1
##  8 jim          0.00541    0.00375    1.44   1.54e- 1
##  9 kelly       -0.0129     0.0101    -1.28   2.05e- 1
## 10 kevin        0.00279    0.0114     0.244  8.08e- 1
## # … with 21 more rows</code></pre>
</div>
<div id="what-if-n-is-really-small" class="section level4 unnumbered">
<h4>What if <span class="math inline">\(n\)</span> is really small?<a class="anchor" aria-label="anchor" href="#what-if-n-is-really-small"><i class="fas fa-link"></i></a>
</h4>
<p>No good! The coefficients cannot be estimated! (Here, 5 points were randomly selected, remember <span class="math inline">\(p = 31\)</span>.) The model breaks down because <span class="math inline">\((X^t X)^{-1}\)</span> is not invertible. Notice that only 5 coefficients are estimated and no SEs are estimated.</p>
<div class="sourceCode" id="cb608"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">47</span><span class="op">)</span>
<span class="va">office_train</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lm.ridge.html">select</a></span><span class="op">(</span><span class="op">-</span><span class="va">episode_name</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/dplyr/man/slice.html">slice_sample</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">imdb_rating</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 5
##    term        estimate std.error statistic p.value
##    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)   7.30         NaN       NaN     NaN
##  2 season        0.228        NaN       NaN     NaN
##  3 episode       0.0264       NaN       NaN     NaN
##  4 andy         -0.0197       NaN       NaN     NaN
##  5 angela        0.0499       NaN       NaN     NaN
##  6 darryl       NA             NA        NA      NA
##  7 dwight       NA             NA        NA      NA
##  8 jim          NA             NA        NA      NA
##  9 kelly        NA             NA        NA      NA
## 10 kevin        NA             NA        NA      NA
## # … with 21 more rows</code></pre>
</div>
<div id="ridge-regression-model-building" class="section level4 unnumbered">
<h4>Ridge Regression model building<a class="anchor" aria-label="anchor" href="#ridge-regression-model-building"><i class="fas fa-link"></i></a>
</h4>
<p>The vignette for <strong>glmnet</strong> is at <a href="https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf" class="uri">https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.pdf</a> and is very useful. <strong>glmnet</strong> will be used as the engine in the <strong>tidymodels</strong> process.</p>
<p>By using a generalized <code>linear_reg()</code> function, <code>mixture = 0</code> specifies ridge regression and <code>mixture = 1</code> specifies Lasso regularization. The <code>mixture</code> parameter can be any number between zero and one (and then represents a model referred to as Elastic Net regularization). <code>penalty</code> will be a tuning parameter that is set later.</p>
<div id="recipe" class="section level5 unnumbered">
<h5>Recipe<a class="anchor" aria-label="anchor" href="#recipe"><i class="fas fa-link"></i></a>
</h5>
<p>First, a recipe needs to be specified. Note that the <code>episode_name</code> is not a predictor variable, and all the variables need to be normalized (particularly important to scale the variables).</p>
<div class="sourceCode" id="cb610"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">office_rec</span> <span class="op">&lt;-</span> <span class="fu">recipe</span><span class="op">(</span><span class="va">imdb_rating</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">office_train</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">update_role</span><span class="op">(</span><span class="va">episode_name</span>, new_role <span class="op">=</span> <span class="st">"ID"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">step_zv</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span>, <span class="op">-</span><span class="fu">all_outcomes</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">step_normalize</span><span class="op">(</span><span class="fu">all_numeric</span><span class="op">(</span><span class="op">)</span>, <span class="op">-</span><span class="fu">all_outcomes</span><span class="op">(</span><span class="op">)</span><span class="op">)</span></code></pre></div>
</div>
<div id="specify-the-engine-fit" class="section level5 unnumbered">
<h5>Specify the engine + fit<a class="anchor" aria-label="anchor" href="#specify-the-engine-fit"><i class="fas fa-link"></i></a>
</h5>
<p>If we set the penalty to be 47, the ridge regression has a straightforward (linear algebra) solution. The coefficients are given for the ridge regression model (and are substantially smaller than the linear regression coefficients, as expected).</p>
<p><strong>Side note</strong> relevant to the code but not to the main ideas of the ridge regression.</p>
<p>The constraint part can be specified in two different ways. (Can also be specified as <span class="math inline">\(\sum_{j=1}^{p-1}b_j^2 \leq c\)</span> for some <span class="math inline">\(c\)</span>, but we won’t use that construction here.)</p>
<ol style="list-style-type: decimal">
<li><p>Set <code>penalty = P</code> in: <span class="math display">\[\min(SSE + P)\]</span></p></li>
<li><p>Set <span class="math inline">\(\lambda\)</span> in: <span class="math display">\[\min(SSE + \lambda\sum_{j=1}^{p-1}b_{j}^2)\]</span></p></li>
</ol>
<p>If <code>P=0</code> then <span class="math inline">\(\lambda\)</span> will also be zero (and the estimates will be the same as those from OLS). Additionally, <code>P</code> and <span class="math inline">\(\lambda\)</span> will be monotonically related. That is, a large <code>penalty</code> corresponds to a large value of <span class="math inline">\(\lambda.\)</span> That said, they are not functions of one another.</p>
<div class="sourceCode" id="cb611"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_spec</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>mixture <span class="op">=</span> <span class="fl">0</span>, penalty <span class="op">=</span> <span class="fl">47</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span>

<span class="va">ridge_wf</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">office_rec</span><span class="op">)</span>

<span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="va">ridge_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">ridge_spec</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">office_train</span><span class="op">)</span></code></pre></div>
<p>It turns out that <strong>glmnet</strong> fits the model for <em>all</em> values of <code>penalty</code> at once, so we can see the coefficients for any other value of <code>penalty</code> of interest. Notice that the coefficients are smaller for larger values of the <code>penalty</code>.</p>
<div class="sourceCode" id="cb612"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_fit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 3
##    term         estimate penalty
##    &lt;chr&gt;           &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  8.36          47
##  2 season      -0.00110       47
##  3 episode      0.00107       47
##  4 andy        -0.000546      47
##  5 angela       0.00106       47
##  6 darryl       0.000434      47
##  7 dwight       0.000952      47
##  8 jim          0.00150       47
##  9 kelly        0.000112      47
## 10 kevin        0.000600      47
## # … with 21 more rows</code></pre>
<div class="sourceCode" id="cb614"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_fit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span>penalty <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)   8.36         0
##  2 season       -0.0114       0
##  3 episode       0.107        0
##  4 andy          0.0351       0
##  5 angela        0.0235       0
##  6 darryl        0.0103       0
##  7 dwight       -0.0200       0
##  8 jim           0.0895       0
##  9 kelly        -0.0700       0
## 10 kevin         0.0129       0
## # … with 21 more rows</code></pre>
<p>We can visualize how the magnitude of the coefficients are regularized toward zero as the <code>penalty</code> goes up. (We won’t get into the relationship between <span class="math inline">\(\lambda\)</span> and <code>penalty</code>, they both penalize the magnitude of the coefficients, <code>penalty</code> can be specified <em>per</em> variable.)</p>
<div class="sourceCode" id="cb616"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_fit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">extract_fit_engine</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>xvar <span class="op">=</span> <span class="st">"lambda"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="05-shrink_files/figure-html/unnamed-chunk-19-1.png" width="480" style="display: block; margin: auto;"></div>
<p>Prediction is done like other linear models. So if <code><a href="https://rdrr.io/r/stats/predict.html">predict()</a></code> is used with no other parameters, it will use <code>penalty = 47</code> as specified above:</p>
<div class="sourceCode" id="cb617"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">ridge_fit</span>, new_data <span class="op">=</span> <span class="va">office_train</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 100 × 1
##    .pred
##    &lt;dbl&gt;
##  1  8.36
##  2  8.36
##  3  8.36
##  4  8.36
##  5  8.36
##  6  8.36
##  7  8.36
##  8  8.36
##  9  8.36
## 10  8.36
## # … with 90 more rows</code></pre>
</div>
</div>
<div id="normalizing-tuning-whole-process" class="section level4" number="14.10.0.1">
<h4>
<span class="header-section-number">14.10.0.1</span> Normalizing + Tuning + Whole Process<a class="anchor" aria-label="anchor" href="#normalizing-tuning-whole-process"><i class="fas fa-link"></i></a>
</h4>
<p>To tune the lambda parameter, we need to start over with the model/engine specification so that the <code>penalty</code> is not fixed.</p>
<div class="sourceCode" id="cb619"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_spec_tune</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>mixture <span class="op">=</span> <span class="fl">0</span>, penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></code></pre></div>
<p>Next, a new <code>workflow()</code> which includes the new model/engine specification. Notice that we don’t need to set <code>measure = 0</code> here because it was only the <code>penalty</code> term which we designated to <code>tune()</code>.</p>
<div class="sourceCode" id="cb620"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>
<span class="va">office_fold</span> <span class="op">&lt;-</span> <span class="fu">vfold_cv</span><span class="op">(</span><span class="va">office_train</span>, strata <span class="op">=</span> <span class="va">season</span><span class="op">)</span>
  
<span class="va">ridge_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>

<span class="va">ridge_wf</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">office_rec</span><span class="op">)</span>

<span class="va">ridge_fit</span> <span class="op">&lt;-</span> <span class="va">ridge_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">ridge_spec_tune</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">office_train</span><span class="op">)</span>

<span class="co"># this is the line that tunes the model using cross validation</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2020</span><span class="op">)</span>
<span class="va">ridge_cv</span> <span class="op">&lt;-</span> <span class="fu">tune_grid</span><span class="op">(</span>
  <span class="va">ridge_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu">add_model</span><span class="op">(</span><span class="va">ridge_spec_tune</span><span class="op">)</span>,
  resamples <span class="op">=</span> <span class="va">office_fold</span>,
  grid <span class="op">=</span> <span class="va">ridge_grid</span>
<span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb621"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">collect_metrics</span><span class="op">(</span><span class="va">ridge_cv</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">.metric</span> <span class="op">==</span> <span class="st">"rmse"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/dplyr/man/arrange.html">arrange</a></span><span class="op">(</span><span class="va">mean</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 × 7
##    penalty .metric .estimator  mean     n std_err .config              
##      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1   0.791 rmse    standard   0.465    10  0.0369 Preprocessor1_Model25
##  2   0.494 rmse    standard   0.466    10  0.0385 Preprocessor1_Model24
##  3   1.26  rmse    standard   0.467    10  0.0357 Preprocessor1_Model26
##  4   0.309 rmse    standard   0.470    10  0.0406 Preprocessor1_Model23
##  5   2.02  rmse    standard   0.472    10  0.0349 Preprocessor1_Model27
##  6   0.193 rmse    standard   0.476    10  0.0427 Preprocessor1_Model22
##  7   3.24  rmse    standard   0.477    10  0.0344 Preprocessor1_Model28
##  8   5.18  rmse    standard   0.483    10  0.0341 Preprocessor1_Model29
##  9   0.121 rmse    standard   0.484    10  0.0447 Preprocessor1_Model21
## 10   8.29  rmse    standard   0.488    10  0.0339 Preprocessor1_Model30
## # … with 40 more rows</code></pre>
<p>Interestingly, as the penalty grows, the coefficients all get close to zero. So the prediction will be the same for all observations (prediction will be <span class="math inline">\(\overline{Y}\)</span>). When the predictions are all the same, the computation for <span class="math inline">\(R^2\)</span> is NA.</p>
<div class="sourceCode" id="cb623"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">ridge_cv</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">collect_metrics</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">penalty</span>, y <span class="op">=</span> <span class="va">mean</span>, color <span class="op">=</span> <span class="va">.metric</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/aes.html">aes</a></span><span class="op">(</span>
    ymin <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">std_err</span>,
    ymax <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">std_err</span><span class="op">)</span>,
    alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="op">)</span> </code></pre></div>
<div class="inline-figure"><img src="05-shrink_files/figure-html/unnamed-chunk-24-1.png" width="480" style="display: block; margin: auto;"></div>
<p>The best model can be chosen using <code>select_best()</code>.</p>
<div class="sourceCode" id="cb624"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_rr</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">ridge_cv</span>, metric <span class="op">=</span> <span class="st">"rmse"</span><span class="op">)</span>
<span class="va">best_rr</span></code></pre></div>
<pre><code>## # A tibble: 1 × 2
##   penalty .config              
##     &lt;dbl&gt; &lt;chr&gt;                
## 1   0.791 Preprocessor1_Model25</code></pre>
</div>
<div id="the-final-ridge-model" class="section level4 unnumbered">
<h4>The Final Ridge Model<a class="anchor" aria-label="anchor" href="#the-final-ridge-model"><i class="fas fa-link"></i></a>
</h4>
<p>Using the <span class="math inline">\(\lambda\)</span> value for the minimum MSE in the cross validation, we output the coefficients / model associated with the best <span class="math inline">\(\lambda\)</span> for the ridge regression model. <strong>Ridge regression does not do variable selection.</strong></p>
<div class="sourceCode" id="cb626"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">ridge_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu">add_model</span><span class="op">(</span><span class="va">ridge_spec_tune</span><span class="op">)</span>, <span class="va">best_rr</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">office_test</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  8.42      0.791
##  2 season      -0.0327    0.791
##  3 episode      0.0383    0.791
##  4 andy         0.00211   0.791
##  5 angela       0.0233    0.791
##  6 darryl       0.0264    0.791
##  7 dwight       0.0523    0.791
##  8 jim          0.0407    0.791
##  9 kelly       -0.0347    0.791
## 10 kevin        0.0371    0.791
## # … with 21 more rows</code></pre>
</div>
</div>
<div id="r-lasso-regularization" class="section level2" number="14.11">
<h2>
<span class="header-section-number">14.11</span> R: Lasso Regularization<a class="anchor" aria-label="anchor" href="#r-lasso-regularization"><i class="fas fa-link"></i></a>
</h2>
<p>In this section, we’ll re-run the cross validation to find a value of <span class="math inline">\(\lambda\)</span> which minimizes the cross validated MSE. To tune the lambda parameter, we need to start over with the model/engine specification so that the <code>penalty</code> is not fixed. Note that with Lasso, <code>mixture = 1</code>.</p>
<div class="sourceCode" id="cb628"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lasso_spec_tune</span> <span class="op">&lt;-</span> <span class="fu">linear_reg</span><span class="op">(</span>mixture <span class="op">=</span> <span class="fl">1</span>, penalty <span class="op">=</span> <span class="fu">tune</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_mode</span><span class="op">(</span><span class="st">"regression"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">set_engine</span><span class="op">(</span><span class="st">"glmnet"</span><span class="op">)</span></code></pre></div>
<p>Next, a new <code>workflow()</code> which includes the new model/engine specification. Notice that we don’t need to set <code>measure = 0</code> here because it was only the <code>penalty</code> term which we designated to <code>tune()</code>.</p>
<div class="sourceCode" id="cb629"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lasso_grid</span> <span class="op">&lt;-</span> <span class="fu">grid_regular</span><span class="op">(</span><span class="fu">penalty</span><span class="op">(</span>range <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">5</span>, <span class="fl">5</span><span class="op">)</span><span class="op">)</span>, levels <span class="op">=</span> <span class="fl">50</span><span class="op">)</span>

<span class="va">lasso_wf</span> <span class="op">&lt;-</span> <span class="fu">workflow</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_recipe</span><span class="op">(</span><span class="va">office_rec</span><span class="op">)</span>

<span class="va">lasso_fit</span> <span class="op">&lt;-</span> <span class="va">lasso_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">add_model</span><span class="op">(</span><span class="va">lasso_spec_tune</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">office_train</span><span class="op">)</span>

<span class="co"># this is the line that tunes the model using cross validation</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">2020</span><span class="op">)</span>
<span class="va">lasso_cv</span> <span class="op">&lt;-</span> <span class="fu">tune_grid</span><span class="op">(</span>
  <span class="va">lasso_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu">add_model</span><span class="op">(</span><span class="va">lasso_spec_tune</span><span class="op">)</span>,
  resamples <span class="op">=</span> <span class="va">office_fold</span>,
  grid <span class="op">=</span> <span class="va">lasso_grid</span>
<span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb630"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">collect_metrics</span><span class="op">(</span><span class="va">lasso_cv</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">.metric</span> <span class="op">==</span> <span class="st">"rmse"</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/dplyr/man/arrange.html">arrange</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/dplyr/man/desc.html">desc</a></span><span class="op">(</span><span class="va">.metric</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 50 × 7
##      penalty .metric .estimator  mean     n std_err .config              
##        &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                
##  1 0.00001   rmse    standard   0.531    10  0.0497 Preprocessor1_Model01
##  2 0.0000160 rmse    standard   0.531    10  0.0497 Preprocessor1_Model02
##  3 0.0000256 rmse    standard   0.531    10  0.0497 Preprocessor1_Model03
##  4 0.0000409 rmse    standard   0.531    10  0.0497 Preprocessor1_Model04
##  5 0.0000655 rmse    standard   0.531    10  0.0497 Preprocessor1_Model05
##  6 0.000105  rmse    standard   0.531    10  0.0497 Preprocessor1_Model06
##  7 0.000168  rmse    standard   0.531    10  0.0497 Preprocessor1_Model07
##  8 0.000268  rmse    standard   0.530    10  0.0497 Preprocessor1_Model08
##  9 0.000429  rmse    standard   0.529    10  0.0497 Preprocessor1_Model09
## 10 0.000687  rmse    standard   0.528    10  0.0496 Preprocessor1_Model10
## # … with 40 more rows</code></pre>
<p>Interestingly, as the penalty grows, the coefficients all get close to zero. So the prediction will be the same for all observations (prediction will be <span class="math inline">\(\overline{Y}\)</span>). When the predictions are all the same, the computation for <span class="math inline">\(R^2\)</span> is NA.</p>
<div class="sourceCode" id="cb632"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">lasso_cv</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">collect_metrics</span><span class="op">(</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">penalty</span>, y <span class="op">=</span> <span class="va">mean</span>, color <span class="op">=</span> <span class="va">.metric</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_linerange.html">geom_errorbar</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/aes.html">aes</a></span><span class="op">(</span>
    ymin <span class="op">=</span> <span class="va">mean</span> <span class="op">-</span> <span class="va">std_err</span>,
    ymax <span class="op">=</span> <span class="va">mean</span> <span class="op">+</span> <span class="va">std_err</span><span class="op">)</span>,
    alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/scale_continuous.html">scale_x_log10</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/labs.html">ylab</a></span><span class="op">(</span><span class="st">"RMSE"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="05-shrink_files/figure-html/unnamed-chunk-30-1.png" width="480" style="display: block; margin: auto;"></div>
<p>The best model can be chosen using <code>select_best()</code>.</p>
<div class="sourceCode" id="cb633"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">best_lasso</span> <span class="op">&lt;-</span> <span class="fu">select_best</span><span class="op">(</span><span class="va">lasso_cv</span>, metric <span class="op">=</span> <span class="st">"rmse"</span><span class="op">)</span>
<span class="va">best_lasso</span></code></pre></div>
<pre><code>## # A tibble: 1 × 2
##   penalty .config              
##     &lt;dbl&gt; &lt;chr&gt;                
## 1  0.0295 Preprocessor1_Model18</code></pre>
<div id="the-final-lasso-model" class="section level4 unnumbered">
<h4>The Final Lasso Model<a class="anchor" aria-label="anchor" href="#the-final-lasso-model"><i class="fas fa-link"></i></a>
</h4>
<p>Using the <span class="math inline">\(\lambda\)</span> value for the minimum MSE in the cross validation, we output the coefficients / model associated with the best <span class="math inline">\(\lambda\)</span> for the ridge regression model. <strong>Lasso regularization DOES do variable selection.</strong> Note the large number of coefficients that are set to zero.</p>
<div class="sourceCode" id="cb635"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">finalize_workflow</span><span class="op">(</span><span class="va">lasso_wf</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu">add_model</span><span class="op">(</span><span class="va">lasso_spec_tune</span><span class="op">)</span>, <span class="va">best_lasso</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu">fit</span><span class="op">(</span>data <span class="op">=</span> <span class="va">office_test</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 31 × 3
##    term        estimate penalty
##    &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
##  1 (Intercept)  8.42     0.0295
##  2 season      -0.112    0.0295
##  3 episode      0.115    0.0295
##  4 andy         0        0.0295
##  5 angela       0.00412  0.0295
##  6 darryl       0.0195   0.0295
##  7 dwight       0.0665   0.0295
##  8 jim          0.0902   0.0295
##  9 kelly       -0.0518   0.0295
## 10 kevin        0.125    0.0295
## # … with 21 more rows</code></pre>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="standardized-multiple-regression.html"><span class="header-section-number">13</span> Standardized Multiple Regression</a></div>
<div class="next"><a href="smooth.html"><span class="header-section-number">15</span> Smoothing Methods</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#shrink"><span class="header-section-number">14</span> Shrinkage Methods</a></li>
<li><a class="nav-link" href="#model-complexity-flexibility"><span class="header-section-number">14.1</span> Model Complexity / Flexibility</a></li>
<li><a class="nav-link" href="#on-inverting-matrices"><span class="header-section-number">14.2</span> On Inverting Matrices</a></li>
<li>
<a class="nav-link" href="#ridge-regression"><span class="header-section-number">14.3</span> Ridge Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-ridge-regression-solution"><span class="header-section-number">14.3.1</span> The Ridge Regression Solution</a></li>
<li><a class="nav-link" href="#ridge-regression-visually"><span class="header-section-number">14.3.2</span> Ridge Regression visually</a></li>
<li><a class="nav-link" href="#why-ridge-regression"><span class="header-section-number">14.3.3</span> Why Ridge Regression?</a></li>
<li><a class="nav-link" href="#inference-on-ridge-regression-coefficients"><span class="header-section-number">14.3.4</span> Inference on Ridge Regression Coefficients</a></li>
</ul>
</li>
<li><a class="nav-link" href="#how-do-you-choose-lambda"><span class="header-section-number">14.4</span> How do you choose \(\lambda\)?</a></li>
<li>
<a class="nav-link" href="#lasso"><span class="header-section-number">14.5</span> Lasso</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#lasso-coefficients"><span class="header-section-number">14.5.1</span> Lasso Coefficients</a></li>
<li><a class="nav-link" href="#lasso-visually"><span class="header-section-number">14.5.2</span> Lasso visually</a></li>
<li><a class="nav-link" href="#how-do-you-choose-lambda-1"><span class="header-section-number">14.5.3</span> How do you choose \(\lambda\)?</a></li>
</ul>
</li>
<li><a class="nav-link" href="#ridge-regression-vs.-lasso"><span class="header-section-number">14.6</span> Ridge Regression vs. Lasso</a></li>
<li><a class="nav-link" href="#elastic-net"><span class="header-section-number">14.7</span> Elastic Net</a></li>
<li><a class="nav-link" href="#reflection-questions-10"><span class="header-section-number">14.8</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-9"><span class="header-section-number">14.9</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r-ridge-regression"><span class="header-section-number">14.10</span> R: Ridge Regression</a></li>
<li><a class="nav-link" href="#r-lasso-regularization"><span class="header-section-number">14.11</span> R: Lasso Regularization</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/05-shrink.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/05-shrink.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models</strong>" was written by Jo Hardin. It was last built on 2022-05-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
