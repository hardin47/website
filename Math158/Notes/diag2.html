<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 12 Diagnostic Measures II | Linear Models</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="Main idea   Figure 1.3: The point (a) has very little influence on the model, despite having a large residual. The point (b) has very little influence in the model because it is consistent with...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 12 Diagnostic Measures II | Linear Models">
<meta property="og:type" content="book">
<meta property="og:description" content="Main idea   Figure 1.3: The point (a) has very little influence on the model, despite having a large residual. The point (b) has very little influence in the model because it is consistent with...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 12 Diagnostic Measures II | Linear Models">
<meta name="twitter:description" content="Main idea   Figure 1.3: The point (a) has very little influence on the model, despite having a large residual. The point (b) has very little influence in the model because it is consistent with...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="wrang.html"><span class="header-section-number">2</span> Data Wrangling</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">3</span> Visualization</a></li>
<li><a class="" href="slr.html"><span class="header-section-number">4</span> Simple Linear Regression</a></li>
<li><a class="" href="infslr.html"><span class="header-section-number">5</span> Inference on SLR Parameters</a></li>
<li><a class="" href="diag1.html"><span class="header-section-number">6</span> Diagnostic Measures I</a></li>
<li><a class="" href="simult.html"><span class="header-section-number">7</span> Simultaneous Inference</a></li>
<li><a class="" href="la.html"><span class="header-section-number">8</span> Regression using Matrices</a></li>
<li><a class="" href="mlr.html"><span class="header-section-number">9</span> Multiple Linear Regression</a></li>
<li><a class="" href="process.html"><span class="header-section-number">10</span> Modeling as a Process6</a></li>
<li><a class="" href="build.html"><span class="header-section-number">11</span> Statistical Model Building</a></li>
<li><a class="active" href="diag2.html"><span class="header-section-number">12</span> Diagnostic Measures II</a></li>
<li><a class="" href="standardized-multiple-regression.html"><span class="header-section-number">13</span> Standardized Multiple Regression</a></li>
<li><a class="" href="shrink.html"><span class="header-section-number">14</span> Shrinkage Methods</a></li>
<li><a class="" href="smooth.html"><span class="header-section-number">15</span> Smoothing Methods</a></li>
<li><a class="" href="anova.html"><span class="header-section-number">16</span> ANOVA</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="diag2" class="section level1" number="12">
<h1>
<span class="header-section-number">12</span> Diagnostic Measures II<a class="anchor" aria-label="anchor" href="#diag2"><i class="fas fa-link"></i></a>
</h1>
<div id="main-idea" class="section level4 unnumbered">
<h4>Main idea<a class="anchor" aria-label="anchor" href="#main-idea"><i class="fas fa-link"></i></a>
</h4>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="04c-diag2_files/figure-html/unnamed-chunk-3-1.png" alt="The point (a) has very little influence on the model, despite having a large residual.  The point (b) has very little influence in the model because it is consistent with the model given by the other points.  The point (c) has a high residual and high leverage which will impact both the prediction and the coefficients in the model." width="480"><p class="caption">
Figure 1.3: The point (a) has very little influence on the model, despite having a large residual. The point (b) has very little influence in the model because it is consistent with the model given by the other points. The point (c) has a high residual and high leverage which will impact both the prediction and the coefficients in the model.
</p>
</div>
<p>The main idea for this section is to understand how individual observations (or subsets of observations) impact the model. We will talk about values that are outlying in the x-direction or outlying in y-direction. Depending on the combination of outlyingness, the point may or may not impact the model. We already have the pieces (e.g., the hat matrix) to evaluate the influence of the point on the model.</p>
<p>High residuals (if big enough) can impact the model. High leverage (if big enough) can impact the model. But a big residual with big leverage will definitely impact the coefficients and therefore the predictions in a linear model.</p>
<p>Consider the hat matrix (see Section <a href="la.html#fitted">8.5</a>):
<span class="math display">\[\begin{eqnarray*}
H &amp;=&amp; X(X^t X)^{-1} X^t\\
\hat{Y} &amp;=&amp; HY, \underline{e} = (I-H)\underline{Y}\\
\sigma^2\{\underline{e}\} &amp;=&amp; \sigma^2 (I-H), s^2\{\underline{e}\} = MSE(I-H) \\
\end{eqnarray*}\]</span></p>
</div>
<div id="outliers-in-the-y-direction" class="section level2" number="12.1">
<h2>
<span class="header-section-number">12.1</span> Outliers in the Y direction<a class="anchor" aria-label="anchor" href="#outliers-in-the-y-direction"><i class="fas fa-link"></i></a>
</h2>
<p>Points that are outlying in terms of their <span class="math inline">\(Y\)</span>-values are often easy to spot via residual plots, as the model will not predict these values very well and a large residual will be seen. However, sometimes the observation will have enough influence in the model, however, that the residual will be small even if the point is ``outlying in the y-direction.‚Äù</p>
<div id="internally-studentized-residuals" class="section level3" number="12.1.1">
<h3>
<span class="header-section-number">12.1.1</span> (Internally) Studentized Residuals<a class="anchor" aria-label="anchor" href="#internally-studentized-residuals"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that we previously defined semi-studentized residuals as the residual divided by MSE (our best estimate of <span class="math inline">\(\sigma^2\)</span>). But then we also pointed out that the variance of the residuals isn‚Äôt really <span class="math inline">\(\sigma^2\)</span> and isn‚Äôt constant (because of the requirement that the residuals sum to zero).
<span class="math display">\[\begin{eqnarray*}
e_i &amp;=&amp; Y_i - \hat{Y}_i\\
e_i^* &amp;=&amp; \frac{Y_i - \hat{Y}_i}{\sqrt{MSE}} \ \ \ \ \mbox{semi-studentized residuals}
\end{eqnarray*}\]</span>
The hat matrix is a measure of where the observations fall in the subspace spanned by the explanatory variables. The hat matrix also provides the information about the predictions (which in turn determine the residuals). It turns out that the variance of the residuals can be calculated using the hat matrix:
<span class="math display">\[\begin{eqnarray*}
\sigma^2\{\underline{e}\} &amp;=&amp; \sigma^2 (I-H)\\
\sigma^2\{e_i\} &amp;=&amp; \sigma^2 (1-h_{ii})\\
s^2\{\underline{e}\} &amp;=&amp; MSE (I-H)\\
s^2\{e_i\} &amp;=&amp; MSE (1-h_{ii})\\
s\{ e_i \} &amp;=&amp; \sqrt{MSE(1-h_{ii})}\\
r_i &amp;=&amp; \frac{e_i}{s\{e_i\}}  \ \ \ \mbox{internally studentized residuals}
\end{eqnarray*}\]</span>
Note that studentized residuals will have constant variance (unlike the standard residuals).</p>
</div>
<div id="deleted-residuals-externally-studentized-residuals" class="section level3" number="12.1.2">
<h3>
<span class="header-section-number">12.1.2</span> Deleted Residuals (externally studentized residuals)<a class="anchor" aria-label="anchor" href="#deleted-residuals-externally-studentized-residuals"><i class="fas fa-link"></i></a>
</h3>
<p>If the observation is far outlying, it may influence the regression line to such an extent that the point no longer looks outlying. One solution to this problem is to fit the model without the outlying value and then measure how far the <span class="math inline">\(i^{th}\)</span> response is from the model created without the <span class="math inline">\(i^{th}\)</span> observation.<br>
Let <span class="math inline">\(\hat{Y}_{i(i)}\)</span> represent the predicted value for the <span class="math inline">\(i^{th}\)</span> observation created using the remaining <span class="math inline">\(n-1\)</span> observations. [Note: here we think of <span class="math inline">\(Y_i\)</span> to be fixed, and <span class="math inline">\(Y_{i(i)}\)</span> to be varying.]
<span class="math display">\[\begin{eqnarray*}
d_i &amp;=&amp; Y_i - \hat{Y}_{i(i)} \ \ \ \ \mbox{ deleted residuals}\\
&amp;=&amp; \frac{e_i}{(1-h_{ii})} \mbox{,  where  } e_i = Y_i - \hat{Y}_i \ \ \ \ \mbox{ not obvious, see text}\\
s^2\{d_i\} &amp;=&amp; MSE_{(i)} (1 + X_i^t (X_{(i)}^t X_{(i)})^{-1} X_i) \ \ \ \mbox{recall:} s^2\{\hat{Y}_{h(new)} \} = MSE (1+ X_h^t (X^t X)^{-1} X_h)\\
&amp;=&amp; \frac{MSE_{(i)}}{(1-h_{ii})} \ \ \ \ \mbox{ not obvious, see text}\\
&amp;&amp;\\
t_i &amp;=&amp; \frac{d_i}{s\{d_i\}} = \frac{e_i}{\sqrt{MSE_{(i)}(1-h_{ii})}} \sim t_{n-1-p}  \ \ \mbox{ studentized deleted residuals}\\
\end{eqnarray*}\]</span></p>
<ol style="list-style-type: decimal">
<li>Now we can legitimately look for deviations outside of the <span class="math inline">\(\pm t_{1 - (\alpha/2n), n-p-1}\)</span> bounds. We use a Bonferroni correction for the level of significance because we are doing <span class="math inline">\(n\)</span> simultaneous tests.<br>
</li>
<li>Note that the SE is calculated from the exact formula we used for prediction intervals. That is, we are predicting the <span class="math inline">\(i^{th}\)</span> observation from the other <span class="math inline">\((n-1)\)</span> observations.<br>
</li>
<li>As above, we can write <span class="math inline">\(t_i\)</span> as a function of <span class="math inline">\(e_i\)</span> and <span class="math inline">\(h_{ii}\)</span>. The alternative notation makes it clear that large deleted residuals come about from a combination of moderate / large residuals and moderate / large leverage.</li>
</ol>
<div id="test-for-outliers" class="section level4 unnumbered">
<h4>Test for Outliers<a class="anchor" aria-label="anchor" href="#test-for-outliers"><i class="fas fa-link"></i></a>
</h4>
<p>We identify as outlying <span class="math inline">\(Y\)</span> observations those cases whose studentized deleted residuals are large in absolute value. In addition, we can conduct a formal test by means of the Bonferroni test procedure of whether the case with the largest absolute studentized deleted residual is an outlier. Since we do not know in advance which case will have the largest absolute value <span class="math inline">\(|t_i|\)</span>, we consider the family of tests to include <span class="math inline">\(n\)</span> tests, one
for each case.</p>
<p>If the regression model is appropriate, so that no case is outlying because of a change in the model, then each studentized deleted residual will follow the <span class="math inline">\(t-\)</span>distribution with <span class="math inline">\(n-p-1\)</span> degrees of freedom. The appropriate Bonferroni critical value therefore is <span class="math inline">\(t(l - \alpha/2n; n - p - 1)\)</span>. Note that the test is two-sided since we are not concerned with the direction of the residuals but only with their absolute values.</p>
</div>
</div>
</div>
<div id="outliers-in-the-x-direction" class="section level2" number="12.2">
<h2>
<span class="header-section-number">12.2</span> Outliers in the X direction<a class="anchor" aria-label="anchor" href="#outliers-in-the-x-direction"><i class="fas fa-link"></i></a>
</h2>
<p>Values that are outlying with respect to their <span class="math inline">\(X\)</span>-values are called <strong>leverage points</strong>, and are not necessarily obvious in multiple linear regression. The reason for the name leverage point is that by moving a single point far enough away, we can make the fitted line do whatever we want, regardless of the sample size. Thus, such points have a lot of leverage, and we need to handle these points with care.<br>
The diagonal elements of the so-called hat matrix give indications as to leverage points, values that are outlying here are leverage points and should be worried about.</p>
<div id="leverage" class="section level3" number="12.2.1">
<h3>
<span class="header-section-number">12.2.1</span> Leverage<a class="anchor" aria-label="anchor" href="#leverage"><i class="fas fa-link"></i></a>
</h3>
<p>
Recall the hat matrix. Note that if there is only one explanatory variable,
<span class="math display">\[\begin{eqnarray*}
h_i = \frac{(X_i - \overline{X})^2}{\sum_{j=1}^n (X_j - \overline{X})^2} + \frac{1}{n}\\
\end{eqnarray*}\]</span>
In higher dimensions, <span class="math inline">\(H=X(X^tX)^{-1}X^t\)</span>, where <span class="math inline">\(X_{n \times p}\)</span> is the matrix of explanatory variables. Also, we can show that <span class="math inline">\(h_{ii}\)</span> has the following properties: <span class="math display">\[0 \leq h_{ii} \leq 1 \ \ \ \sum_{i=1}^n h_{ii} =p \ \ \ \overline{h} = \frac{\sum_{i=1}^n h_{ii}}{n} = \frac{p}{n}\]</span>
The diagonal elements of the hat matrix, <span class="math inline">\(h_{ii}\)</span>, are called <em>leverage</em> points.</p>
<p><strong>Leverage gives a (multivariate) sense of how far the <span class="math inline">\(i^{th}\)</span> observation is from the bulk of the data.</strong> There is <em>no response variable</em> in the calculation of leverage. Additionally, <span class="math inline">\(h_{ii}\)</span> provide information about how much the <span class="math inline">\(i^{th}\)</span> observation impacts the fit of <span class="math inline">\(\hat{Y}_i\)</span>.</p>
<ol style="list-style-type: decimal">
<li>Recall that <span class="math inline">\(\hat{\underline{Y}} = H \underline{Y}\)</span>. That is, the predicted values are a linear combination of the observed response. <span class="math inline">\(h_{ii}\)</span> is the weight of observation <span class="math inline">\(Y_i\)</span> in determining <span class="math inline">\(\hat{Y}_i\)</span>. The larger <span class="math inline">\(h_{ii}\)</span> is, the more important <span class="math inline">\(Y_i\)</span> is in determining <span class="math inline">\(\hat{Y}_i\)</span>. The hat matrix is a function of <span class="math inline">\(X\)</span> only, so <span class="math inline">\(h_{ii}\)</span> measures the role of the X values in determining how important <span class="math inline">\(Y_i\)</span> is in affecting the fitted value <span class="math inline">\(\hat{Y}_i\)</span>.<br>
</li>
<li>The larger <span class="math inline">\(h_{ii}\)</span> is, the smaller the variance of the residual <span class="math inline">\(e_i\)</span>. That is, the larger <span class="math inline">\(h_{ii}\)</span>, the closer <span class="math inline">\(\hat{Y}_i\)</span> will be to <span class="math inline">\(Y_i\)</span>. If <span class="math inline">\(h_{ii} = 1\)</span>, then <span class="math inline">\(\sigma^2{e_i} = 0\)</span> which means the fitted value is forced to equal the observed value.</li>
</ol>
</div>
</div>
<div id="outliers-that-are-influential" class="section level2" number="12.3">
<h2>
<span class="header-section-number">12.3</span> Outliers that are Influential<a class="anchor" aria-label="anchor" href="#outliers-that-are-influential"><i class="fas fa-link"></i></a>
</h2>
<p>We consider points to be <strong>influential if they impact the inference</strong>. That is, they change either the predicted values or the model. Note that an influential observation can arise from a large residual, large leverage, or both.</p>
<div id="dffits-difference-in-fits" class="section level4 unnumbered">
<h4>DFFITS (difference in fits)<a class="anchor" aria-label="anchor" href="#dffits-difference-in-fits"><i class="fas fa-link"></i></a>
</h4>
<p><strong>DFFITS is a measure of the influence the case <span class="math inline">\(i\)</span> has on the fitted value <span class="math inline">\(\hat{Y}_i\)</span></strong>. [Note: <span class="math inline">\(\hat{Y}_i\)</span> is considered to be varying and <span class="math inline">\(\hat{Y}_{i(i)}\)</span> to be fixed.] That is DFFITS is measuring how <span class="math inline">\(\hat{Y}_i\)</span> varies (in units of standard deviation) from the fit calculated without <span class="math inline">\(i\)</span> in the model.
<span class="math display">\[\begin{eqnarray*}
(DFFITS)_i &amp;=&amp; \frac{\hat{Y}_i - \hat{Y}_{i(i)}}{\sqrt{MSE_{(i)} h_{ii}}}\\
&amp;=&amp; t_i \bigg(\frac{h_{ii}}{1-h_{ii}}\bigg)^{1/2}
\end{eqnarray*}\]</span>
To understand the derivation, recall the following about the variability of <span class="math inline">\(\underline{\hat{Y}}\)</span>.
<span class="math display">\[\begin{eqnarray*}
\underline{\hat{Y}} &amp;=&amp; H \underline{Y}\\
\sigma^2\{\underline{\hat{Y}}\} &amp;=&amp; H \sigma^2 \{Y\} H = \sigma^2 H\\
\sigma^2\{\hat{Y}_i\} &amp;=&amp; \sigma^2 \cdot h_{ii}\\
s^2\{\hat{Y}_i\} &amp;=&amp; MSE \cdot h_{ii}
\end{eqnarray*}\]</span></p>
</div>
<div id="cooks-distance" class="section level4 unnumbered">
<h4>Cook‚Äôs Distance<a class="anchor" aria-label="anchor" href="#cooks-distance"><i class="fas fa-link"></i></a>
</h4>
<p>Another measure combines the idea of outliers of both types to see the influence a particular point is having on your regression. <strong>Cook‚Äôs Distance measures the change in the regression by removing each individual point.</strong> If things change quite a bit by the omission of a single point, then that point was having a lot of influence on your model. Define <span class="math inline">\(\hat{Y}_{j(i)}\)</span> to be the fitted value for the <span class="math inline">\(j^{th}\)</span> observation when the <span class="math inline">\(i^{th}\)</span> observation is deleted from the data set. Cook‚Äôs Distance measures how much <span class="math inline">\(i\)</span> changes <em>all</em> the predictions.
<span class="math display">\[\begin{eqnarray*}
D_i &amp;=&amp;\frac{\sum_{j=1}^{n}(\hat{Y}_j-\hat{Y}_{j(i)})^2}{p MSE}\\
&amp;=&amp; \frac{(\hat{\underline{Y}} - \hat{\underline{Y}}_{(i)})^t (\hat{\underline{Y}} - \hat{\underline{Y}}_{(i)})}{p MSE}\\
&amp;=&amp; \frac{e_i^2}{p MSE} \bigg[ \frac{h_{ii}}{(1-h_{ii})^2} \bigg]
\end{eqnarray*}\]</span>
Cook‚Äôs Distance shows the effect of the <span class="math inline">\(i^{th}\)</span> case on <em>all</em> the fitted values. Note that the <span class="math inline">\(i^{th}\)</span> case can be influenced by</p>
<ol style="list-style-type: decimal">
<li>big <span class="math inline">\(e_i\)</span> and moderate <span class="math inline">\(h_{ii}\)</span><br>
</li>
<li>moderate <span class="math inline">\(e_i\)</span> and big <span class="math inline">\(h_{ii}\)</span><br>
</li>
<li>big <span class="math inline">\(e_i\)</span> and big <span class="math inline">\(h_{ii}\)</span>
</li>
</ol>
<p>The exact way to think about these numbers being large is complicated, but if we see a point that is much larger than others, we should examine that point and try to understand it better. Noting that the above measure squared deviations, a useful comparison is to look at the 10% of 20% cutoff of the <span class="math inline">\(F(p, n-p)\)</span> distribution:
<span class="math display">\[\begin{eqnarray*}
\mbox{if } &amp;&amp;D_i &lt; F_{0.1, p, n-p} \mbox{ or } F_{0.2, p, n-p} \rightarrow \mbox{ no influence}\\
\mbox{if } &amp;&amp;D_i &gt; F_{0.5, p, n-p} \rightarrow \mbox{ big influence}\\
\end{eqnarray*}\]</span></p>
</div>
<div id="dfbetas-difference-in-betas" class="section level4 unnumbered">
<h4>DFBETAS (difference in betas)<a class="anchor" aria-label="anchor" href="#dfbetas-difference-in-betas"><i class="fas fa-link"></i></a>
</h4>
<p><strong>DFBETAS are a measure of the influence of case <span class="math inline">\(i\)</span> on the <span class="math inline">\(k^{th}\)</span> <span class="math inline">\(b_k\)</span> coefficient.</strong>
<span class="math display">\[\begin{eqnarray*}
(DFBETAS)_{k(i)} &amp;=&amp; \frac{b_k - b_{k(i)}}{\sqrt{MSE_{(i)} c_{kk}}}\\
\mbox{where } c_{kk} &amp;=&amp; (X^t X )^{-1}_{kk}\\
\mbox{note } \sigma^2\{ b_k \} &amp;=&amp; \sigma^2 c_{kk}
\end{eqnarray*}\]</span>
A large absolute value of <span class="math inline">\((DFBETAS)_{k(i)}\)</span> is indicative of a large impact of the <span class="math inline">\(i^{th}\)</span> case on the <span class="math inline">\(k^{th}\)</span> regression coefficient.</p>
</div>
</div>
<div id="variance-inflation-factor-vif" class="section level2" number="12.4">
<h2>
<span class="header-section-number">12.4</span> Variance Inflation Factor (VIF)<a class="anchor" aria-label="anchor" href="#variance-inflation-factor-vif"><i class="fas fa-link"></i></a>
</h2>
<p>Recall that we should always be aware of possible multicollinearity which can cause the following problems:</p>
<ol style="list-style-type: decimal">
<li>Adding or deleting a predictor variable changes the regression coefficients (both in terms of magnitude as well as significance).<br>
</li>
<li>The extra sum of squares associated with an explanatory variable varies, depending upon which other predictor variables are already included in the model.<br>
</li>
<li>The estimated standard deviation of the regression coefficients become large when the predictor variables in the regression model are highly correlated with each other.<br>
</li>
<li>The estimated regression coefficients individually may not be statistically significant [t-test in multiple regression model] even though a definite relation exists between the response variable and the set of predictor variables.</li>
</ol>
<div id="informal-diagnostics" class="section level4 unnumbered">
<h4>Informal Diagnostics<a class="anchor" aria-label="anchor" href="#informal-diagnostics"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>For important predictors: wide confidence intervals, non-significant results for individual coefficients [t-tests in multiple regression], coefficient with the opposite sign than would be expected<br>
</li>
<li>Large changes in the estimated regression coefficients when a predictor variable is added or deleted<br>
</li>
<li>Large coefficients of simple correlation between pairs of predictor variables</li>
</ul>
</div>
<div id="formal-diagnostics" class="section level4 unnumbered">
<h4>Formal Diagnostics<a class="anchor" aria-label="anchor" href="#formal-diagnostics"><i class="fas fa-link"></i></a>
</h4>
<p>Variance Inflation Factor gives a quantitative sense of the multicollinearity within the explanatory variables. <span class="math display">\[VIF_k = (1-R_k^2)^{-1}\]</span> <span class="math inline">\(R_k^2\)</span> is the coefficient of multiple determination when <span class="math inline">\(X_k\)</span> is regressed on the other <span class="math inline">\(p-2\)</span> explanatory variables.</p>
<ul>
<li>
<span class="math inline">\(VIF_k = 1\)</span> if <span class="math inline">\(R_k^2 = 0\)</span> (i.e., if <span class="math inline">\(X_k\)</span> is not linearly related to the other <span class="math inline">\(p-2\)</span> variables)<br>
</li>
<li>if <span class="math inline">\(R_k^2 \ne 0 \rightarrow VIF_k &gt; 1\)</span><br>
</li>
<li>if <span class="math inline">\(R_k^2 = 1 \rightarrow VIF_k\)</span> is unbounded</li>
</ul>
<p>It is also interesting to note that the standard error of the estimated coefficients can be calculated using VIF.
<span class="math display">\[\begin{eqnarray*}
s^2\{b_k\} = \frac{MSE}{(n-1) s^2\{X_k\}} (VIF_k)
\end{eqnarray*}\]</span>
That is, the standard error is a function of MSE (the total variability around the model), <span class="math inline">\(s^2\{X_k\}\)</span> (the variability of the <span class="math inline">\(k^{th}\)</span> variable, and the VIF for the <span class="math inline">\(k^{th}\)</span> variable. Notice that if the <span class="math inline">\(k^{th}\)</span> variable is in the model and another variable is added which is <em>correlated</em> to the <span class="math inline">\(k^{th}\)</span> variable, the SE for the <span class="math inline">\(k^{th}\)</span> coefficient will go up.</p>
</div>
<div id="a-strategy-for-dealing-with-problematic-data-points" class="section level3" number="12.4.1">
<h3>
<span class="header-section-number">12.4.1</span> A Strategy for Dealing with Problematic Data Points<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a><a class="anchor" aria-label="anchor" href="#a-strategy-for-dealing-with-problematic-data-points"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<p>First, check for obvious data errors:</p>
<ul>
<li>If the error is just a data entry or data collection error, correct it.</li>
<li>If the data point is not representative of the intended study population, delete it.</li>
<li>If the data point is a procedural error and invalidates the measurement, delete it.</li>
</ul>
</li>
<li>
<p>Consider the possibility that you might have just misformulated your regression model:</p>
<ul>
<li>Did you leave out any important predictors?</li>
<li>Should you consider adding some interaction terms?</li>
<li>Is there any non-linearity that needs to be modeled?</li>
</ul>
</li>
<li><p>If non linearity is an issue, one possibility is to just reduce the scope of your model. If you do reduce the scope of your model, you should be sure to report it, so that readers do not misuse your model.</p></li>
<li><p>Decide whether or not deleting data points is warranted:</p></li>
</ol>
<ul>
<li>Do not delete data points just because they do not fit your preconceived regression model.</li>
<li>You must have a good, objective reason for deleting data points.</li>
<li>If you delete any data after you‚Äôve collected it, justify and describe it in your reports.</li>
<li>If you are not sure what to do about a data point, analyze the data twice - once with and once without the data point - and report the results of both analyses.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li>First, foremost, and finally - it‚Äôs okay to use your common sense and knowledge about the situation.</li>
</ol>
<p>Note: Added value plots are often a good idea, and you should read through section 10.1. But there won‚Äôt be any homework or exam questions on section 10.1</p>
<p><strong>Notes on diagnostic table handout:</strong></p>
<ul>
<li>The first four statistics are measures of how <strong>influential</strong> the value is. Leverage measures the distance of the explanatory variables from the average. Cook‚Äôs distances, and the derivatives, are a measure of how much the predicted values change when the point is removed from the model.</li>
<li>The residual statistics are measures of how well the regression line fits the value. A residual is the distance from the point to the line. We standardize the residual in different ways. The studentized residuals contain the more accurate measure of standard error.</li>
<li>The VIF measures the degree of collinearity between the explanatory variables. Collinear variables indicates that we should be cautious interpreting any coefficients. <span class="math inline">\(\mbox{mean}(VIF) &gt; &gt; 1\)</span> is meant to indicate that the average VIF is considerably larger than 1.</li>
<li>Any value containing a ‚Äú<span class="math inline">\((i)\)</span>‚Äù indicates that the <span class="math inline">\(i^{th}\)</span> point was removed before calculating the value. For example, <span class="math inline">\(MSE_{(i)}\)</span> is the <span class="math inline">\(MSE\)</span> for the full model containing all the data <strong>except</strong> the <span class="math inline">\(i^{th}\)</span> point.</li>
<li>Most of the functions are in R under a general heading of <code>influence.measures</code>. The <code>vif</code> function is in the <strong>car</strong> package.</li>
</ul>
</div>
</div>
<div id="reflection-questions-9" class="section level2" number="12.5">
<h2>
<span class="header-section-number">12.5</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-9"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>What is an influential data point?</li>
<li>How are outlying Y values detected by way of studentized residuals or studentized deleted residuals?</li>
<li>What is leverage, and how can outlying X values be detected using leverage?</li>
<li>How can potentially influential data points be detected by way of DFFITS and Cook‚Äôs distance measure?</li>
</ol>
</div>
<div id="ethics-considerations-8" class="section level2" number="12.6">
<h2>
<span class="header-section-number">12.6</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-8"><i class="fas fa-link"></i></a>
</h2>
</div>
<div id="r" class="section level2" number="12.7">
<h2>
<span class="header-section-number">12.7</span> R:<a class="anchor" aria-label="anchor" href="#r"><i class="fas fa-link"></i></a>
</h2>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="build.html"><span class="header-section-number">11</span> Statistical Model Building</a></div>
<div class="next"><a href="standardized-multiple-regression.html"><span class="header-section-number">13</span> Standardized Multiple Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#diag2"><span class="header-section-number">12</span> Diagnostic Measures II</a></li>
<li>
<a class="nav-link" href="#outliers-in-the-y-direction"><span class="header-section-number">12.1</span> Outliers in the Y direction</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#internally-studentized-residuals"><span class="header-section-number">12.1.1</span> (Internally) Studentized Residuals</a></li>
<li><a class="nav-link" href="#deleted-residuals-externally-studentized-residuals"><span class="header-section-number">12.1.2</span> Deleted Residuals (externally studentized residuals)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#outliers-in-the-x-direction"><span class="header-section-number">12.2</span> Outliers in the X direction</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#leverage"><span class="header-section-number">12.2.1</span> Leverage</a></li></ul>
</li>
<li><a class="nav-link" href="#outliers-that-are-influential"><span class="header-section-number">12.3</span> Outliers that are Influential</a></li>
<li>
<a class="nav-link" href="#variance-inflation-factor-vif"><span class="header-section-number">12.4</span> Variance Inflation Factor (VIF)</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#a-strategy-for-dealing-with-problematic-data-points"><span class="header-section-number">12.4.1</span> A Strategy for Dealing with Problematic Data Points12</a></li></ul>
</li>
<li><a class="nav-link" href="#reflection-questions-9"><span class="header-section-number">12.5</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-8"><span class="header-section-number">12.6</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r"><span class="header-section-number">12.7</span> R:</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/04c-diag2.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/04c-diag2.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models</strong>" was written by Jo Hardin. It was last built on 2022-03-28.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
