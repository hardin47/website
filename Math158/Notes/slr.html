<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Simple Linear Regression | Linear Models</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="4.1 A Linear Model Consider the situation where we have two variables, which we denote by \(x\) and \(Y\); \(x\) is the predictor variable, \(Y\) is the response. We observe \(n\) observations, as...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 4 Simple Linear Regression | Linear Models">
<meta property="og:type" content="book">
<meta property="og:description" content="4.1 A Linear Model Consider the situation where we have two variables, which we denote by \(x\) and \(Y\); \(x\) is the predictor variable, \(Y\) is the response. We observe \(n\) observations, as...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Simple Linear Regression | Linear Models">
<meta name="twitter:description" content="4.1 A Linear Model Consider the situation where we have two variables, which we denote by \(x\) and \(Y\); \(x\) is the predictor variable, \(Y\) is the response. We observe \(n\) observations, as...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Linear Models</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="wrang.html"><span class="header-section-number">2</span> Data Wrangling</a></li>
<li><a class="" href="viz.html"><span class="header-section-number">3</span> Visualization</a></li>
<li><a class="active" href="slr.html"><span class="header-section-number">4</span> Simple Linear Regression</a></li>
<li><a class="" href="infslr.html"><span class="header-section-number">5</span> Inference on SLR Parameters</a></li>
<li><a class="" href="diag1.html"><span class="header-section-number">6</span> Diagnostic Measures I</a></li>
<li><a class="" href="simult.html"><span class="header-section-number">7</span> Simultaneous Inference</a></li>
<li><a class="" href="la.html"><span class="header-section-number">8</span> Regression using Matrices</a></li>
<li><a class="" href="mlr.html"><span class="header-section-number">9</span> Multiple Linear Regression</a></li>
<li><a class="" href="process.html"><span class="header-section-number">10</span> Modeling as a Process6</a></li>
<li><a class="" href="build.html"><span class="header-section-number">11</span> Statistical Model Building</a></li>
<li><a class="" href="diag2.html"><span class="header-section-number">12</span> Diagnostic Measures II</a></li>
<li><a class="" href="standardized-multiple-regression.html"><span class="header-section-number">13</span> Standardized Multiple Regression</a></li>
<li><a class="" href="shrink.html"><span class="header-section-number">14</span> Shrinkage Methods</a></li>
<li><a class="" href="smooth.html"><span class="header-section-number">15</span> Smoothing Methods</a></li>
<li><a class="" href="anova.html"><span class="header-section-number">16</span> ANOVA</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="slr" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Simple Linear Regression<a class="anchor" aria-label="anchor" href="#slr"><i class="fas fa-link"></i></a>
</h1>
<div id="a-linear-model" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> A Linear Model<a class="anchor" aria-label="anchor" href="#a-linear-model"><i class="fas fa-link"></i></a>
</h2>
<p>Consider the situation where we have two variables, which we denote by <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>; <span class="math inline">\(x\)</span> is the predictor variable, <span class="math inline">\(Y\)</span> is the response. We observe <span class="math inline">\(n\)</span> observations, as our sample, denoted by <span class="math inline">\((x_i,y_i)\)</span>. We believe that the two variable are related, namely that <span class="math display">\[Y=f(x)+\epsilon,\]</span> where <span class="math inline">\(\epsilon\)</span> is a random error which accounts for the fact that when we know the values of both <span class="math inline">\(x\)</span> and <span class="math inline">\(f\)</span>, we still won’t know exactly what <span class="math inline">\(Y\)</span> is.</p>
<p>In the two variable case, we assume that <span class="math inline">\(f(x)\)</span> is a linear function of <span class="math inline">\(x\)</span>. That is, we assume the model <span class="math display">\[Y_i=\beta_0+\beta_1 x_i+\epsilon_i.\]</span> Our attempts to estimate the function <span class="math inline">\(f\)</span> has now been reduced to trying to estimate two numbers, the intercept <span class="math inline">\(\beta_0\)</span> and the slope <span class="math inline">\(\beta_1\)</span>: the <em>parameters</em>.</p>
<p>Consider the following 4 models. Note the differences between <strong>statistics</strong> vs. <strong>parameters</strong> and also between <strong>individual observations</strong> vs. <strong>averages</strong>. Convince yourself that you know when to use each model.</p>
<p><span class="math display">\[\begin{eqnarray*}
E[Y_i|x_i] &amp;=&amp; \beta_0 + \beta_1 x_i \\
y_i &amp;=&amp; \beta_0 + \beta_1 x_i + \epsilon_i\\
&amp;&amp; \epsilon_i = y_i -  (\beta_0 + \beta_1 x_i)\\
\hat{y}_i &amp;=&amp; b_0 + b_1 x_i\\
y_i &amp;=&amp; b_0 + b_1 x_i + e_i\\
&amp;&amp; e_i = y_i - \hat{y}_i = y_i -  (b_0 + b_1 x_i)\\
\end{eqnarray*}\]</span></p>
<div id="fitting-the-regression-line-least-squares" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> Fitting the regression line: least squares<a class="anchor" aria-label="anchor" href="#fitting-the-regression-line-least-squares"><i class="fas fa-link"></i></a>
</h3>
<p>How do we fit a regression line? Find <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize the sum of squared distance of the points to the line (called ordinary least squares):</p>
<p>As we discusses previously, to actually calculate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> we would need to observe the entire population. Instead, we estimate these quantities from the sample data that we have. We are essentially trying to find the line that fits the data the best. Which can be thought of as the line that is closest to the points in some sense. Given a particular line and a particular point, how should we think about how far the point is from that line? The way to think about it is in terms of what we want to do with the model. Recall that <span class="math inline">\(x\)</span> is our predictor variable, and <span class="math inline">\(Y\)</span> is the response. In the linear regression context, the set up is usually that <span class="math inline">\(x\)</span> is something that will be known beforehand, and one of our goals will be to predict the response <span class="math inline">\(Y\)</span>. In that sense, the way we should think about a good fitting line is one where the <strong>vertical distance from the points to the line is small</strong>.</p>
<p><strong>Residual:</strong> The vertical distance from a point to our line. The <span class="math inline">\(i^{th}\)</span> residual is defined as follows: <span class="math display">\[e_i=y_i-(b_0+b_1x_i)\]</span> where <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are the intercept and slope of our line under consideration.</p>
<div id="notes" class="section level4 unnumbered">
<h4>Notes:<a class="anchor" aria-label="anchor" href="#notes"><i class="fas fa-link"></i></a>
</h4>
<p>The vertical error measure is only reasonable in the predictor-response relationship. If we were interested in studying the relationship between height and shoe size, the vertical error idea doesn’t really exist (because we don’t think of the variables as explanatory and response - though certainly the model would approximate their linear relationship). A good fit might be based on the perpendicular distance from the point to the line. The reason the vertical error model isn’t always ideal is because we don’t always naturally consider one of the variables to be explanatory and the other to be the response. It is true, however, that we can (and often do) model relationships between variables that don’t have a natural predictor - response relationship. That is, for all of the linear models we will cover, the error (variable) term will be measured in the vertical direction.</p>
<p>Now, to find the “best” fitting line, we search for the line that has the smallest residuals in some sense. In particular, the goal is to try to find the line that minimizes the following quantity: <span class="math display">\[Q=\sum e_i^2 = \sum (y_i-(b_0+b_1x_i))^2.\]</span></p>
<p><strong>SSE:</strong> Sum of squared errors (or residuals), a measure of how well the line fits. SSE is then the value of <span class="math inline">\(Q\)</span> at our optimal values of <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> for a given dataset.</p>
<p>Finding <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> that minimize Q then becomes a calculus problem.
<span class="math display">\[\frac{dQ}{db_0}=-2\sum (y_i-(b_0+b_1x_i)),\qquad \frac{dQ}{db_1}=-2\sum
x_i(y_i-(b_0+b_1x_i))\]</span> Setting these equal to 0 and solving for <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> yields our optimal values, which we denote as <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span></p>
<p><strong>Least Squares Estimates:</strong> <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> (as follows) minimize the sum of squared residuals, and are given by:
<span class="math display">\[b_0=\bar{y}-b_1\bar{x}, \qquad b_1=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum
(x_i-\bar{x})^2}\]</span> (Sometimes <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are referred to as <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1.)\)</span> So now we can write <span class="math display">\[SSE=\sum (y_i-(b_0+b_1x_i))^2\]</span> We can note what we already knew from the above discussion. If we switch the roles of <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span>, our best fitting line will be different. If the relationship was actually symmetric, then switching the roles of <span class="math inline">\(x\)</span> and <span class="math inline">\(Y\)</span> should give a slope of <span class="math inline">\(1/b_1\)</span>, which is not the case.</p>
<p>A question you might ask is why we chose to minimize <span class="math inline">\(\sum e_i^2\)</span> as opposed to perhaps <span class="math inline">\(\sum |e_i|\)</span>, where <span class="math inline">\(|\cdot|\)</span> denotes absolute value. The original motivation was because the math is much easier in the first case, however, it can be shown that in “nice” situations, the statistical properties are better as well.</p>
<p>Consider the very good <a href="http://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm" target="_blank">applet for visualizing the concept of minimizing sums of squares.</a> Note that the applet also allows for the visualization of a line created from minimizing the sum of the absolute errors.</p>
<p>Keep in mind the following notation:</p>
<p><span class="math display">\[\begin{eqnarray*}
E[Y_i] &amp;=&amp; \beta_0 + \beta_1 x_i \mbox{   true mean response}\\
\hat{y}_i &amp;=&amp; b_0 + b_1 x_i \mbox{   estimate of the mean response}\\
 &amp;=&amp; \hat{\beta}_0 + \hat{\beta}_1 x_i\\
e_i &amp;=&amp; y_i - \hat{y}_i\\
\epsilon_i &amp;=&amp; y_i - E[Y_i]\\
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="variance-sigma2" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Variance: <span class="math inline">\(\sigma^2\)</span><a class="anchor" aria-label="anchor" href="#variance-sigma2"><i class="fas fa-link"></i></a>
</h3>
<p>If we only had one variable (e.g., credit card balance), we would estimate the variability of the response variable as <span class="math display">\[\mbox{sample variance} = \frac{SSTO}{n-1} = \frac{1}{(n-1)} \sum_{i=1}^n (y_i - \overline{y})^2,\]</span> where SSTO = sum of squares total.</p>
<p>But here in the regression setting, we are interested in the variance of the error terms (around the line). In particular, the variance of the observed <span class="math inline">\(y_i\)</span> around the line is given by <span class="math inline">\(\sigma^2\)</span>. We estimate <span class="math inline">\(\sigma^2\)</span> using the observed variability around the line - the residual. <span class="math display">\[SSE = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n e_i^2,\]</span> where SSE is sum of squared errors (sometimes called sum of square residuals). Note that we have estimated <em>two</em> parameters, so our degrees of freedom are <span class="math inline">\(df = n-2\)</span>. Our best estimate of <span class="math inline">\(\sigma^2\)</span> is Mean Squared Error (MSE): <span class="math display">\[s^2 = MSE = \frac{SSE}{n-2} = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{y}_i)^2.\]</span> MSE = mean squared error.</p>
</div>
</div>
<div id="normal-errors-model" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Normal Errors Model<a class="anchor" aria-label="anchor" href="#normal-errors-model"><i class="fas fa-link"></i></a>
</h2>
<p>The least squares regression and previous modeling hold for any probability model. However, inference will be easiest given a normal probability model. Given the linear model on the <em>population</em>:
<span class="math display">\[Y_i=\beta_0+\beta_1 x_i+\epsilon_i\]</span>
<span class="math display">\[\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)\]</span></p>
<div id="important-features-of-the-model-with-normal-errors" class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Important Features of the Model with Normal Errors<a class="anchor" aria-label="anchor" href="#important-features-of-the-model-with-normal-errors"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\epsilon_i\)</span> is a <em>random</em> error term (that is, <span class="math inline">\(\epsilon_i\)</span> is a random variable).<br>
</li>
<li>
<span class="math inline">\(Y_i\)</span> is a random variable because of <span class="math inline">\(\epsilon_i\)</span>.<br>
</li>
<li>We assume <span class="math inline">\(E[\epsilon_i]=0\)</span>, therefore <span class="math inline">\(E[Y_i | x_i] = \beta_0 + \beta_1 x_i\)</span>. (E is the expected value which can be thought of as the long run average or the population mean.) That is, the relationship between the explanatory and response variables is <strong>linear</strong>.<br>
</li>
<li>
<span class="math inline">\(\epsilon_i\)</span> is the amount by which <span class="math inline">\(Y_i\)</span> values exceed or fall short of the regression line.<br>
</li>
<li>
<span class="math inline">\(var\{Y_i | x_i\} = var\{\epsilon_i\} = \sigma^2 \rightarrow\)</span> we have <strong>constant variance</strong> around the regression line.<br>
</li>
<li>
<span class="math inline">\(SD\{Y_i, Y_j\} = SD\{\epsilon_i, \epsilon_j \} = 0 \rightarrow\)</span> the <strong>error terms are uncorrelated</strong>.<br>
</li>
<li>
<span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span>, the error terms are <strong>normally distributed</strong>.<br>
</li>
<li><span class="math inline">\(Y_i \sim N(\beta_0 + \beta_1 x_i, \sigma^2)\)</span></li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="figs/normerr.png" alt="An x-y plot showing a regression line with 3D normal curves at each of the predicted regression values on the line." width="95%"><p class="caption">
Figure 1.2: Figs 1.6 from <span class="citation"><a href="references.html#ref-kutner" role="doc-biblioref">Kutner et al.</a> (<a href="references.html#ref-kutner" role="doc-biblioref">2004</a>)</span>.
</p>
</div>
</div>
<div id="technical-conditions" class="section level3" number="4.2.2">
<h3>
<span class="header-section-number">4.2.2</span> Technical Conditions<a class="anchor" aria-label="anchor" href="#technical-conditions"><i class="fas fa-link"></i></a>
</h3>
<p>Though you could use the least squares regression criterion to fit a line through any observed data, the setting here has a series of conditions <strong>which allows us to do inference on our model</strong>. These conditions are crucial to check whenever doing a regression, because if they are not satisfied, nothing you do with your data has any real meaning. That is, your interpretations won’t make sense, and your inference won’t be valid. Here are the conditions:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Condition of Linearity</strong> The relationship is actually linear:<span class="math display">\[Y_i=\beta_0+\beta_1 x_i+\epsilon_i\]</span> It doesn’t make sense to fit a line to data if we don’t believe that the relationship is linear. The <span class="math inline">\(\epsilon\)</span> term is a random variable, thus, for two individuals measured with the same value of <span class="math inline">\(x_i\)</span>, the <span class="math inline">\(Y_i\)</span> will, in general, be different. As <span class="math inline">\(Y_i\)</span> is a
function of <span class="math inline">\(\epsilon_i\)</span>, <span class="math inline">\(Y_i\)</span> is also a random variable.</li>
</ol>
<p>Furthermore, we are assuming that the mean of the <span class="math inline">\(\epsilon_i\)</span> are 0. This tells us that for a fixed value of <span class="math inline">\(x_i\)</span>, the average value of <span class="math inline">\(Y_i\)</span> is given as <span class="math inline">\(\beta_0+\beta_1 x_i\)</span>. We write this as
<span class="math display">\[E[Y_i | x_i] = \mu_{Y_i|x_i}=\beta_0+\beta_1 x_i\]</span> where <span class="math inline">\(\mu\)</span> represents the mean. So the fitted values are the estimates of the mean of the <span class="math inline">\(Y_i\)</span> at the plugged-in value of <span class="math inline">\(x_i\)</span>.</p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Condition of Independence</strong> The individual observations are independent of each other. We are assuming that our data is a random sample from the population of interest. As a contrast to this condition, suppose we are interested in the number of pieces in a jigsaw puzzle and the time it takes to complete it. If all our data come from one person (e.g., multiple puzzles), who happens to be very good at jigsaw puzzles, then our estimate of the line will be much lower than it should be, because this person will finish all the puzzles quickly, i.e. small values for <span class="math inline">\(y_i\)</span>. However, had our data been independent, then we have the chance of also getting someone who is very bad at jigsaw puzzles and things even out to get an unbiased estimate of the line.</p></li>
<li><p><strong>Condition of Constant Variance</strong> The error terms, in addition to having a mean of 0, are assumed to have a variance <span class="math inline">\(\sigma^2\)</span> that does not depend on the value of <span class="math inline">\(x_i\)</span>. This is assumed because we are
looking at each point with equal importance. Suppose that we knew that at a particular value of <span class="math inline">\(x_i\)</span>, the variance of <span class="math inline">\(\epsilon_i\)</span> was 0. Then the observed value of <span class="math inline">\(y_i\)</span> is actually <span class="math inline">\(\mu_y\)</span>, and
thus we should force our line to go through that point, since the true line goes through that point. This is an extreme case, but in the case of non-constant variance, we should regard the values
observed with smaller variation with higher importance, as they will tend to be more accurate. We denote the variance condition by <span class="math display">\[Var(Y_i|x_i)=\sigma^2.\]</span></p></li>
<li><p><strong>Condition of a Normal Distribution</strong> Lastly, we assume that the distribution of the error terms is normal, a common distribution. The reason for the normal condition is theoretic, as the techniques we will be using to say something about the <span class="math inline">\(\beta_i\)</span> based on the <span class="math inline">\(b_i\)</span> and the data assumes the normal distribution, as its easy to work with.</p></li>
</ol>
</div>
<div id="maximum-likelihood" class="section level3" number="4.2.3">
<h3>
<span class="header-section-number">4.2.3</span> Maximum Likelihood<a class="anchor" aria-label="anchor" href="#maximum-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>We won’t cover the details of maximum likelihood. However, it is worth pointing out that maximum likelihood methods are used in many different areas of statistics and are quite powerful. Additionally, in the simple linear regression case with normal errors, the maximum likelihood estimates turn out to be exactly the same as the least squares estimates.</p>
</div>
</div>
<div id="reflection-questions-1" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-1"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>When is the condition of a linear relationship appropriate?<br>
</li>
<li>How should we go about estimating the <span class="math inline">\(\beta_i, i=0,1\)</span>?<br>
</li>
<li>How close are our estimates to the actual population values <span class="math inline">\(\beta_i, i=0,1\)</span>?<br>
</li>
<li>Once we have the estimated function, how do we actually interpret what we have?<br>
</li>
<li>What are the linear model conditions which are important for inference?</li>
</ol>
</div>
<div id="ethics-considerations" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Why does it matter if the technical conditions are violated when reporting the analysis / model?</li>
<li>Do the technical conditions matter for fitting the line? Inference on the line? Neither? Both?</li>
<li>If a strong linear relationship between the predictor and response variables are found, does that mean that the predictor variables <strong>causes</strong> the response?</li>
<li>Why is Simple Linear Regression called “simple?” Is it because the model is easy? (Spoiler: no.)</li>
</ol>
</div>
<div id="r-code-slr" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> R code: SLR<a class="anchor" aria-label="anchor" href="#r-code-slr"><i class="fas fa-link"></i></a>
</h2>
<div id="example-credit-scores" class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> Example: Credit Scores<a class="anchor" aria-label="anchor" href="#example-credit-scores"><i class="fas fa-link"></i></a>
</h3>
<p>Consider a dataset from ISLR on credit scores. Because we don’t know the sampling mechanism used to collect the data, we are unable to generalize the model results to a larger population. However, we can look at the relationship between the variables and build a linear model. Notice that the <code><a href="https://rdrr.io/r/stats/lm.html">lm()</a></code> command is always of the form: <code>lm(response ~ explanatory)</code>.</p>
<p>The <strong>broom</strong> package has three important functions:</p>
<ul>
<li>
<code><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy()</a></code> reports the information which is based on each explanatory variable</li>
<li>
<code><a href="https://rdrr.io/pkg/generics/man/glance.html">glance()</a></code> reports the information which is based on the overall model</li>
<li>
<code><a href="https://rdrr.io/pkg/generics/man/augment.html">augment()</a></code> reports the information which is based on each observation</li>
</ul>
<div class="sourceCode" id="cb237"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://broom.tidymodels.org/">broom</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR</a></span><span class="op">)</span>
<span class="va">Credit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Balance</span> <span class="op">~</span> <span class="va">Limit</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/generics/man/tidy.html">tidy</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept) -293.     26.7         -11.0 1.18e- 24
## 2 Limit          0.172   0.00507      33.9 2.53e-119</code></pre>
<div class="sourceCode" id="cb239"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Credit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Balance</span> <span class="op">~</span> <span class="va">Limit</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/generics/man/glance.html">glance</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 1 × 12
##   r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.743         0.742  234.     1148. 2.53e-119     1 -2748. 5502. 5514.
## # … with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
<div class="sourceCode" id="cb241"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Credit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">Balance</span> <span class="op">~</span> <span class="va">Limit</span>, data <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/generics/man/augment.html">augment</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 400 × 8
##    Balance Limit .fitted  .resid    .hat .sigma     .cooksd .std.resid
##      &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1     333  3606    326.    6.87 0.00310   234. 0.00000135      0.0294
##  2     903  6645    848.   55.3  0.00422   234. 0.000119        0.237 
##  3     580  7075    922. -342.   0.00507   233. 0.00548        -1.47  
##  4     964  9504   1338. -374.   0.0132    233. 0.0174         -1.61  
##  5     331  4897    548. -217.   0.00251   234. 0.00109        -0.929 
##  6    1151  8047   1088.   62.6  0.00766   234. 0.000280        0.269 
##  7     203  3388    289.  -85.7  0.00335   234. 0.000227       -0.368 
##  8     872  7114    928.  -56.2  0.00516   234. 0.000151       -0.241 
##  9     279  3300    274.    5.39 0.00347   234. 0.000000929     0.0231
## 10    1350  6819    878.  472.   0.00454   233. 0.00937         2.03  
## # … with 390 more rows</code></pre>
<div class="sourceCode" id="cb243"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">Credit</span> <span class="op"><a href="https://rdrr.io/pkg/magrittr/man/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">Limit</span>, y <span class="op">=</span> <span class="va">Balance</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="va">lm</span>, se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/labs.html">xlab</a></span><span class="op">(</span><span class="st">"Credit limit (in $)"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://rdrr.io/pkg/ggplot2/man/labs.html">ylab</a></span><span class="op">(</span><span class="st">"Credit card balance (in $)"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-slr_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;"></div>
<p>We can assume that there is a <em>population</em> model underlying the relationship between <code>Limit</code> and <strong>average</strong> <code>Balance</code>. For example, it is possible that the true (unknown) population model is: <span class="math display">\[ E(Balance) = -300 + 0.2 \cdot Limit.\]</span> Note that we consider <code>Balance</code> to be random and <code>Limit</code> to be fixed. Also, note that in order to do inference, we should see the error terms normally distributed around the regression line with a constant variance for all values of x.</p>
<p>Consider someone who has a $5,000 <code>Limit</code> and had a credit card balance of $1,000. The <em>population error</em> term for that person will be: <span class="math display">\[\epsilon_{5000 bal} = 1000 - [-300 + 0.2 \cdot 5000] =  \$700.\]</span></p>
<p>Consider someone who has a $2,000 <code>Limit</code> and had a credit card balance of $50. The <em>population error</em> term for that person will be: <span class="math display">\[\epsilon_{2000 bal} = 50 - [-300 + 0.2 \cdot 1000] =  -\$50.\]</span></p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="viz.html"><span class="header-section-number">3</span> Visualization</a></div>
<div class="next"><a href="infslr.html"><span class="header-section-number">5</span> Inference on SLR Parameters</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#slr"><span class="header-section-number">4</span> Simple Linear Regression</a></li>
<li>
<a class="nav-link" href="#a-linear-model"><span class="header-section-number">4.1</span> A Linear Model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fitting-the-regression-line-least-squares"><span class="header-section-number">4.1.1</span> Fitting the regression line: least squares</a></li>
<li><a class="nav-link" href="#variance-sigma2"><span class="header-section-number">4.1.2</span> Variance: \(\sigma^2\)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#normal-errors-model"><span class="header-section-number">4.2</span> Normal Errors Model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#important-features-of-the-model-with-normal-errors"><span class="header-section-number">4.2.1</span> Important Features of the Model with Normal Errors</a></li>
<li><a class="nav-link" href="#technical-conditions"><span class="header-section-number">4.2.2</span> Technical Conditions</a></li>
<li><a class="nav-link" href="#maximum-likelihood"><span class="header-section-number">4.2.3</span> Maximum Likelihood</a></li>
</ul>
</li>
<li><a class="nav-link" href="#reflection-questions-1"><span class="header-section-number">4.3</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations"><span class="header-section-number">4.4</span>  Ethics Considerations</a></li>
<li>
<a class="nav-link" href="#r-code-slr"><span class="header-section-number">4.5</span> R code: SLR</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#example-credit-scores"><span class="header-section-number">4.5.1</span> Example: Credit Scores</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/02-slr.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/02-slr.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Linear Models</strong>" was written by Jo Hardin. It was last built on 2022-05-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
