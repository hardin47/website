# Correlation & Regression as Models {#regdesc}

```{r global_options, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, fig.height=3.5,
                      fig.width=5, 
                      fig.align = "center", cache=TRUE)
library(tidyverse)
library(infer)
library(mosaic)
```

<!--
## 4/16/20 Agenda {#Apr16}
1. Definition of correlation (r)
2. Interpretation of correlation (r)  
3. (Probably not: Inference on $\rho$)
-->

The next topic will focus on modeling using two quantitative variables.  That is, both the explanatory and the response variables are measured on a numeric scale.   

To get started, consider a handful of crop types taken from [Our World in Data](https://ourworldindata.org/crop-yields) as part of [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2020/2020-09-01).  Each point in each plot represents a different country.  The x and y variables represent the proportion of total yield in the last 50 years which is due to that crop type. 


```{r echo = FALSE}
library(naniar)
crops_country <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv') %>%
  rename(wheat = `Wheat (tonnes per hectare)`,
         rice = `Rice (tonnes per hectare)`,
         maize = `Maize (tonnes per hectare)`,
         soybeans = `Soybeans (tonnes per hectare)`,
         potatoes = `Potatoes (tonnes per hectare)`,
         beans = `Beans (tonnes per hectare)`,
         peas = `Peas (tonnes per hectare)`,
         cassava = `Cassava (tonnes per hectare)`,
         barley = `Barley (tonnes per hectare)`,
         cocoa = `Cocoa beans (tonnes per hectare)`,
         bananas = `Bananas (tonnes per hectare)`) %>% 
    filter(!is.na(Code)) %>%  # remove continents, etc.
  group_by(Code) %>%  # to sum and percent by country
  summarize(swheat = sum(wheat, na.rm = TRUE),
            srice = sum(rice, na.rm = TRUE),
            smaize = sum(maize, na.rm = TRUE),
            ssoybeans = sum(soybeans, na.rm = TRUE),
            spotatoes = sum(potatoes, na.rm = TRUE),
            sbeans = sum(beans, na.rm = TRUE),
            speas = sum(peas, na.rm = TRUE),
            scassava = sum(cassava, na.rm = TRUE),
            sbarley = sum(barley, na.rm = TRUE),
            scocoa = sum(cocoa, na.rm = TRUE),
            sbananas = sum(bananas, na.rm = TRUE),
            pwheat = 100*swheat / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            price = 100*srice / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pmaize = 100*smaize / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            psoybeans = 100*ssoybeans / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            ppotatoes = 100*spotatoes / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pbeans = 100*sbeans / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            ppeas = 100*speas / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pcassava = 100*scassava / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pbarley = 100*sbarley / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pcocoa = 100*scocoa / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            pbananas = 100*sbananas / (swheat + srice + smaize + ssoybeans + spotatoes + sbeans + speas + scassava + sbarley + scocoa + sbananas),
            ) %>%
  replace_with_na_all(condition = ~.x == 0)%>%
  mutate(nmiss = rowSums(is.na(.))/2) %>%
  filter(nmiss <= 5)  # filter out countries that don't have data for most of the crops.
```

```{r plots6, fig.cap="The % of total yield for different crops (across the last 50 years).  Each point represents a country.", warning=FALSE, fig.width=7, fig.height = 10, echo = FALSE}
sb <- ggplot(crops_country) +
  geom_point(aes(x = psoybeans, y = pbananas)) +
  xlim(c(0,6)) + 
  xlab("% soybeans") +
  ylab("% bananas")

sc <- ggplot(crops_country) +
  geom_point(aes(x = psoybeans, y = pcassava)) +
  xlim(c(0,6)) + 
  xlab("% soybeans") +
  ylab("% cassava")

mc <- ggplot(crops_country) +
  geom_point(aes(x = pmaize, y = pcassava)) +
  xlim(c(0,15)) + 
  xlab("% maize") +
  ylab("% cassava")

peb <- ggplot(crops_country) +
  geom_point(aes(x = ppotatoes, y = pbananas)) +
  xlim(c(0,60)) + 
  xlab("% potatoes") +
  ylab("% bananas")

cb <- ggplot(crops_country) +
  geom_point(aes(x = pcocoa, y = pbananas)) +
  xlab("% cocoa") +
  ylab("% bananas")

wb <- ggplot(crops_country) +
  geom_point(aes(x = pwheat, y = pbarley)) +
  xlab("% wheat") +
  ylab("% barley")

pob <- ggplot(crops_country) +
  geom_point(aes(x = ppeas, y = pbarley)) +
  xlab("% peas") +
  ylab("% barley")

ggpubr::ggarrange( peb, sc, mc, cb, pob, wb, 
                   labels = c("A", "B", "C", "D", "E", "F"),
                   ncol = 2, nrow = 3)

```


Order the six scatterplots from strongest negative to strongest positive linear relationship.  Some questions to ask yourself:

* What would the correlation be if there was a perfect positive relationship?
* What would the correlation be if there was a perfect negative relationship?
* What would the correlation be if there was no relationship?

## Correlation {#cor}

Correlation measures the **linear** association between two numerical variables.  [Note, that when describing how two categorical (or one numerical & one categorical) variables vary together, they are said to be *associated* instead of *correlated*.]  

> The *correlation coefficient* measures the strength and direction of the linear association between two numerical variables.

### Estimating Correlation

The value of the correlation is defined as:

\begin{eqnarray*}
r &=& \frac{ \sum_i (x_i  - \overline{x})(y_i - \overline{y})}{\sqrt{\sum_i(x_i - \overline{x})^2} \sqrt{ \sum_i(y_i - \overline{y})^2}}\\
r &=& \frac{1}{n-1} \sum_{i=1}^n \bigg(\frac{x_i - \overline{x}}{s_x} \bigg) \bigg(\frac{y_i - \overline{y}}{s_y} \bigg)
\end{eqnarray*}


```{r plots6ave, fig.cap="The % of total yield for different crops (across the last 50 years).  Each point represents a country.  Now lines at the average x and average y values have been superimposed onto the plots.", echo = FALSE, warning=FALSE, fig.width=7}

pwheatmn <- mean(crops_country$pwheat, na.rm = TRUE)
pbarleymn <- mean(crops_country$pbarley, na.rm = TRUE)
ppotatoesmn <- mean(crops_country$ppotatoes, na.rm = TRUE)
pbananasmn <- mean(crops_country$pbananas, na.rm = TRUE)

textdf1 <- data.frame(
  x = c(15, 1, 1, 15),
  y = c(12.5, 12.5, .5, .5),
  text = c("I", "II", "III", "IV"))

textdf2 <- data.frame(
  x = c(60, 0, 0, 60),
  y = c(70, 70, 0, 0),
  text = c("I", "II", "III", "IV"))

wb2 <- ggplot(crops_country, aes(x = pwheat, y = pbarley)) +
  geom_point(color = "red") +
  geom_vline(xintercept = pwheatmn) +
  geom_hline(yintercept = pbarleymn) +
  xlab("% wheat") +
  ylab("% barley") +
  geom_text(data = textdf1, aes(x = x, y = y, label = text), size = 8)
  

peb2 <- ggplot(crops_country) +
  geom_point(aes(x = ppotatoes, y = pbananas), color = "red") +
  geom_vline(xintercept = ppotatoesmn) +
  geom_hline(yintercept = pbananasmn) +
  xlim(c(0,60)) + 
  xlab("% potatoes") +
  ylab("% bananas") +
  geom_text(data = textdf2, aes(x = x, y = y, label = text), size = 8)


ggpubr::ggarrange(  wb2, peb2,
                   ncol = 2, nrow = 1)
```



For each red dot (on each plot), consider the distance the observation is from the $\overline{X}$ line and the $\overline{Y}$ line.  Is the observation (red dot) above both?  below both?  above one and below the other?  

How does the particular red dot (observation) contribute to the correlation?  In a positive way (to make $r$ bigger)?  In a negative way (to make $r$ smaller)?

Some ideas worth thinking about:

* quadratic plots can have zero correlation yet a perfect functional relationship
* $-1 \leq r \leq 1$
* correlation does not imply causation (ice cream & boating accidents!)
* for inference with $\rho$ as well as $\beta_1$, the data should come from a bivariate normal distribution.  That is, histograms of $X$ and $Y$ should both be normal, and the scatterplot should be a cloud.
* correlation will go down when only a narrow range of X values is represented (see denominator of r).
* measurement error biases the estimate of a correlation coefficient toward zero.

We can calculate the correlation value for each of the crop plots and order them from strongest negative to strongest positive linear relationship: $A \rightarrow D \rightarrow B \rightarrow C \rightarrow E \rightarrow F$

```{r echo = FALSE}
library(kableExtra)
temptbl <- tribble(
 ~variable,  ~col1, ~col2, ~corval,
 "A", "potatoes", "bananas", round(cor(crops_country$ppotatoes, crops_country$pbananas, use = "pairwise.complete.obs"), digits = 2),
 "B", "soybeans", "cassava", round(cor(crops_country$psoybeans, crops_country$pcassava, use = "pairwise.complete.obs"), digits = 2),
 "C", "maize", "cassava", round(cor(crops_country$pmaize, crops_country$pcassava, use = "pairwise.complete.obs"), digits = 2),
 "D", "cocoa", "bananas", round(cor(crops_country$pcocoa, crops_country$pbananas, use = "pairwise.complete.obs"), digits = 2),
 "E", "peas", "barley", round(cor(crops_country$ppeas, crops_country$pbarley, use = "pairwise.complete.obs"), digits = 2),
 "F", "wheat", "barley", round(cor(crops_country$pwheat, crops_country$pbarley, use = "pairwise.complete.obs"), digits = 2),
)

temptbl %>%
 kable(caption = "Correlation of percentage of total yield across different crops.",
  col.names = c("Graph", "x-variable", "y-variable", "correlation")) %>%
 kable_styling() 
```


### Coefficient of Determination: $R^2$

The coefficient of determination ($R^2$) is the square of the correlation (given above).  However, it also has an additional interpretation that will be useful for us.  It can measure how much of the original variability in Y is given by the regression line.  Both SSE and least-squares will be defined below when we fit a line to the scatter plot of observations.

SSE is "sum of squared errors" (think about how $s^2$ is defined).  So, $SSE(\overline{y})$ is the amount the response variable varies on its own.  $SSE(\mbox{least-squares})$ is the amount the response variable varies around the regression line (see Section \@ref(slr)).

\begin{eqnarray*}
R^2 &=& \frac{SSE(\overline{y}) - SSE(\mbox{least-squares})}{SSE(\overline{y})} \\
 &=& \frac{Var(y_i) - Var(e_i)}{Var(y_i)} \\
 &=& 1 - \frac{Var(e_i)}{Var(y_i)}\\
\end{eqnarray*}

The value $e_i$ is discussed in detail below, but it is the distance from the observed response variable to the prediction on the line:  \begin{align}e_i=y_i-\hat{y}_i\end{align}

$R^2$ can be used even in models with many explanatory variables.  As such, the way to think about $R^2$ is in terms of how much of the variability in the response variable was removed (when we learned the values of the explanatory variables).  $R^2$ **is the proportion reduction in the variability of the response variable which is explained by the explanatory variable.**



<!--
## 4/21/20 Agenda {#Apr21}
1. Least Squares estimation of the line
2. Distribution of the least squares line from sample to sample

## 4/23/20 Agenda {#Apr23}
1. Inferential technical conditions
2. Residual Plots
3. Transformations
4. Prediction Intervals
-->

## Simple Linear Regression {#slr}

*Regression* is a method that predicts the value of one numerical variable from that of another.  That is, as an extension to describing the degree of linearity of the relationship (correlation), the goal is now to create the best linear model -- often for prediction.  Note that many of the characteristics explored with correlation are applicable for regression.  However, correlation treats $X$ and $Y$ as interchangeable, whereas regression treats $X$ as fixed and known and $Y$ as random and unknown.  As we have previously, we call $X$ the explanatory variable, and $Y$ the response variable.  Again, we do not assume that there is any causal mechanism between $X$ and $Y$ even if they have a strong linear (or otherwise) relationship.


#### Predicted Values {-}

The predicted values of Y from a regression line estimate the *mean value* of $Y$ for all individuals that have a given value of $X$.  Notice the Roman letters (English letters) representing statistics:

\begin{eqnarray*}
\hat{y} &=& b_0 + b_1 x\\
\hat{y}_i &=& b_0 + b_1 x_i\\
y_i &=& b_0 + b_1 x_i + e_i\\
e_i &=& y_i - \hat{y}_i = y_i -  (b_0 + b_1 x_i)\\
\end{eqnarray*}

Notice, that we are predicting the **mean** value of the response variable at a given value of the explanatory variable!  

###  Least Squares estimation of the regression line {#ls}


To find the values of the regression statistics, the sum of squared errors is minimized.

**SSE:** Sum of squared errors (or residuals) is a measure of how closely the line fits to the points.  SSE is the value of the squared deviations calculated at the "best" possible values of $\beta_0$ and $\beta_1$ for a given dataset.

\begin{eqnarray*}
SSE = \sum_i (y_i - \hat{y}_i)^2 = \sum_i (y_i - (b_0 + b_1 x_i) )^2
\end{eqnarray*}
is minimized by the values:
\begin{eqnarray*}
b_0 = \overline{y} - b_1 \overline{x} & \ \ \ \ \ \ \ & b_1 = r \frac{s_y}{s_x}
\end{eqnarray*}

To find the "best" fitting line, we searched for the line that has the smallest residuals (SSE) in some sense.  In particular, the goal is to try to find the line that minimizes the following quantity: $$Q=\sum e_i^2 = \sum (y_i-(b_0+b_1x_i))^2.$$


Finding $b_0$ and $b_1$ that minimize Q is a calculus problem.
$$\frac{dQ}{db_0}=-2\sum (y_i-(b_0+b_1x_i)),\qquad \frac{dQ}{db_1}=-2\sum
x_i(y_i-(b_0+b_1x_i))$$ 
Setting both derivatives equal to 0 and solving for $b_0$ and $b_1$ yields the optimal values, denoted as $b_0$ and $b_1$

One aspect of the optimization problem that is worth pointing out has to do with the role of the two variables of interest $X$ and $Y$.   If we switch the roles of $X$ and $Y$, the best fitting line will be different.  If the relationship was invariant to which variable we choose as a response, then switching the roles of $X$ and $Y$ should give a slope of $1/b_1$, which is not the case.  [Note that the role of $X$ and $Y$ **is** invariant when calculating the correlation but not when calculating the least squares regression line.]

#### Residuals {-}

Residuals measure the scatter of points above and below the least squares regression line.  We use the residuals in many of the calculations and interpretations of the model.  Indeed, the goal of the linear regression is to find a model that has small residuals.  That is, ideally, the known variable $X$ will tell us all there is to know about the unknown variable $Y$.

\begin{eqnarray*}
e_i &=& (y_i - \hat{y}_i)\\
MSE&=& \frac{\sum_i (y_i - \hat{y}_i)^2}{n-2} = \frac{\sum_i (e_i)^2}{n-2} = s^2\\
SSE &=& \sum_i (y_i - \hat{y}_i)^2 = \sum_i (e_i)^2\\
R^2 &=& 1 - \frac{Var(e_i)}{Var(y_i)}
\end{eqnarray*}



## R code for regression 


### Example: Cat Jumping^[@iscam, Inv 5.6 & 5.13] (Correlation & SLR) {#ex:cat}



Consider the cat data given in Investigations 5.6 and 5.13.  The idea is to understand cat jumping velocity as a function of body characteristics. Note that the correlation $r=-0.496$  between bodymass and velocity.



```{r}
cats <- read_table2("http://www.rossmanchance.com/iscam2/data/CatJumping.txt")

ggplot(cats, aes(x=bodymass, y = velocity)) +
  geom_point() +
  geom_smooth(method = "lm", se=FALSE)
```


####  Correlation {-}
```{r}
cats %>%
  select(bodymass, velocity) %>%
  cor()
```





#### Simple Linear Regression {-}

```{r}
library(broom)

cats %>%
  lm(velocity ~ bodymass, data = .) %>%
  tidy()
```



#### Residual Plot {-}

And to work with the residuals, use `augment()`.

```{r}
cats %>%
lm(velocity ~ bodymass, data = .) %>% augment()

cats %>%
lm(velocity ~ bodymass, data = .) %>% augment() %>%
  ggplot(aes(x = .fitted, y = .resid)) + 
  geom_point() +
  geom_hline(yintercept = 0)
```




### Example: Housing Prices^[@iscam, Inv 5.14]  (SLR & MLR & Prediction) {#ex:houses}

```{r message=FALSE, warning=FALSE}
library(GGally)
house = read.table("http://www.rossmanchance.com/iscam2/data/housing.txt", 
                   header=TRUE, sep="\t")
names(house)
```

#### Descriptive Statistics {-}

A good first step is to investigate how all the variables relate to one another.  The `ggpairs` function come from the R package `GGally`.

```{r} 
ggpairs(house, columns = c(1,2,4,5))
```


## Reflection Questions

### Correlation & Simple Linear Regression: Chapter 3

* What does correlation measure?
* What values of correlation ($r$) would indicate large positive correlation?  What about a large negative correlation?
* Sketch a scatterplot that has a strong relationship between the two variables but a correlation of virtually zero (hint: your plot will not be linear).
* How is $R^2$ interpreted?  Why is that?  Make the argument which uses the ideas of "percent of variability in the response variable"? 
* In a linear model what is an error / residual term and how it is calculated?
* How do we find the values of $b_0$ and $b_1$ for estimating the least squares line?
* Is least squares the only possible way to fit a line to a cloud of points?  How else might we do it?  Why do we tend to use least squares?
* Why is it dangerous to extrapolate?
* Is linear regression always appropriate when comparing two continuous variables?


