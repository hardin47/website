[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 58(B) Pomona College: Introduction (Bio)Statistics. notes based extensively “Introduction Modern Statistics” Çetinkaya-Rundel Hardin (Çetinkaya-Rundel Hardin 2021). also using many examples (applets) “Investigating Statistics, Concepts, Applications, Methods” Chance Rossman (Chance Rossman 2018).responsible reading relevant chapters text. text good & readable, use . Also, much deeper understanding material spend time working applets http://www.rossmanchance.com/iscam3/files.html. make sure coming class also reading materials associated activities.```","code":""},{"path":"rfunc.html","id":"rfunc","chapter":"1 R functions","heading":"1 R functions","text":"help us navigate / remember use , following sections consolidate R functions used class assignments.","code":""},{"path":"rfunc.html","id":"applets","chapter":"1 R functions","heading":"1.1 Applets","text":"main source -class applets come Chance Rossman (2018) can found: http://www.rossmanchance.com/applets/index2021.htmlThe main source -class applets come Chance Rossman (2018) can found: http://www.rossmanchance.com/applets/index2021.htmlWe also use RR/applet Ken Kleinman : https://kenkleinman.shinyapps.io/odds-tool/also use RR/applet Ken Kleinman : https://kenkleinman.shinyapps.io/odds-tool/","code":""},{"path":"rfunc.html","id":"data-structure","chapter":"1 R functions","heading":"1.2 Data Structure","text":"Always, important understand format data. example, many rows (observational units)? many columns (variables)? variables numbers categories? many ways see data, highly recommended regularly check back remind data structure.glimpse() prints data variable types (makes columns rows)names() prints column (variable) namesstr() like glimpse() provides little information structure dataframehead() prints first rows dataframe (tail() prints last rows)click “environment” tab, click name dataframe see data console","code":""},{"path":"rfunc.html","id":"wrangling","chapter":"1 R functions","heading":"1.3 Wrangling","text":"Data wrangling used working change data one format another. regularly used pipe function (%>%) layer commands. Data wrangling even bigger part data analysis pipeline start work continuous variables (e.g., height).pipe syntax (%>%) takes data frame (data table) sends argument function. mapping goes first available argument function. example:x %>% f(y) f(x, y)y %>% f(x, ., z) f(x,y,z)great source help data wrangling cheatsheet : https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfA great source help data wrangling cheatsheet : https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdfData verbs take data tables input give data tables output (’s can use chaining syntax!). functions R package dplyr, used much data wrangling. list verbs helpful wrangling many different types data.\nsample_n() take random row(s)\nhead() grab first rows\ntail() grab last rows\nfilter() removes unwanted cases\narrange() reorders cases\nselect() removes unwanted variables (rename())\ndistinct() returns unique values table\nmutate() transforms variable (transmute() like mutate, returns new variables)\ngroup_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).\nsummarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values variable (column) called x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\n\nData verbs take data tables input give data tables output (’s can use chaining syntax!). functions R package dplyr, used much data wrangling. list verbs helpful wrangling many different types data.sample_n() take random row(s)head() grab first rowstail() grab last rowsfilter() removes unwanted casesarrange() reorders casesselect() removes unwanted variables (rename())distinct() returns unique values tablemutate() transforms variable (transmute() like mutate, returns new variables)group_by() tells R SUCCESSIVE functions keep mind groups items. group_by() makes sense verbs later (like summarize()).summarize() collapses data frame single row. functions used within summarize() include:\nmin(), max(), mean(), sum(), sd(), median(), IQR()\nn(): number observations current group\nn_distinct(x): count number unique values variable (column) called x\nfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]\nmin(), max(), mean(), sum(), sd(), median(), IQR()n(): number observations current groupn_distinct(x): count number unique values variable (column) called xfirst_value(x), last_value(x) nth_value(x, n): work similarly x[1], x[length(x)], x[n]happen using function exists dplyr package different package, ’ll want tell computer find appropriate function. example, dplyr::filter().happen using function exists dplyr package different package, ’ll want tell computer find appropriate function. example, dplyr::filter().","code":""},{"path":"rfunc.html","id":"plotting","chapter":"1 R functions","heading":"1.4 Plotting","text":"R package ggplot2 used visualizations. Remember layers plot put together + symbol (instead %>% command).great source help data visualization cheatsheet : https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfA great source help data visualization cheatsheet : https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdfEach plot starts ggplot(data) adds layers. minimal additional layer geom_XXX() layer describes geometry plot.plot starts ggplot(data) adds layers. minimal additional layer geom_XXX() layer describes geometry plot.things notice:\nlayering graph pieces, use +. (layering data wrangling, use %>%.)\ngeom_XXX() put XXX-type--plot onto graph.\naes() function takes data columns puts onto graph. aes() used data columns always need working data variables.\nfull set types plots given : https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf (many places online).\nthings notice:layering graph pieces, use +. (layering data wrangling, use %>%.)geom_XXX() put XXX-type--plot onto graph.aes() function takes data columns puts onto graph. aes() used data columns always need working data variables.full set types plots given : https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf (many places online).happen using function exists ggplot2 package different package, ’ll want tell computer find appropriate function. example, ggplot2::xlim().happen using function exists ggplot2 package different package, ’ll want tell computer find appropriate function. example, ggplot2::xlim().","code":""},{"path":"rfunc.html","id":"statistical-inference","chapter":"1 R functions","heading":"1.5 Statistical Inference","text":"main simulation tools used creating null distributions come R package infer.\nmany examples available infer vignette page: https://infer-dev.netlify.com/index.html\n\nTypically, following steps followed:\n\ncalculate test statistic\n\nteststat <- data %>%\n   specify(variable information) %>%\n   calculate(form statistic)\nTypically, following steps followed:\ncalculate test statistic\n\ncreate null values statistic\n\nvisualize null sampling distribution (statistic)\n\nvisualize null sampling distribution observed statistic overlaid\n\ncalculate p-value\nhappen using function exists infer package also different package, ’ll want tell computer find appropriate function. example, infer::specify().","code":"teststat <- data %>%\n   specify(variable information) %>%\n   calculate(the form of the statistic)nullstats <- data %>%\n   specify(variable information) %>%\n   hypothesize(give information about the type of null hypothesis) %>%\n   generate(repeat the process, provide info about the process) %>%\n   calculate(the form of the statistic)nullstats %>%\n   visualize()nullstats %>%\n   visualize() +\n   shade_p_value(specify where the observed statistics is)nullstats %>%\n   get_p_value(specify the observed statistic and the direction of the test)"},{"path":"rfunc.html","id":"probability-models","chapter":"1 R functions","heading":"1.6 Probability models","text":"Generally, ’ve used mosaic package calculates probabilities adds graphical representation calculated values can checked intuition. functions ’ve used include:xpnorm() normal probabilityxpnorm() normal probabilityxqnorm() normal quantile (also called: cutoff, z*)xqnorm() normal quantile (also called: cutoff, z*)xpbinom() binomial probabilityxpbinom() binomial probabilityIf happen using function exists mosaic package different package, ’ll want tell computer use appropriate function. example, mosaic::xpnorm().happen using function exists mosaic package different package, ’ll want tell computer use appropriate function. example, mosaic::xpnorm().","code":""},{"path":"intro.html","id":"intro","chapter":"2 Introduction","heading":"2 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"2 Introduction","heading":"2.1 Course Logistics","text":"Statistics?\nGenerally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 2.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims population","code":""},{"path":"intro.html","id":"vocabulary","chapter":"2 Introduction","heading":"Vocabulary","text":"observational unit represented row dataframe. object measure things.variable represented column dataframe. characteristic measured observational unit.statistic numerical measurement get sample, function data. [Also sometimes called estimate.]parameter numerical measurement population. never know true value parameter.content Math 58(B)?\nclass introduction statistical ideas using R. cover majority statistical methods used standard analyses (e.g., t-tests, chi-squared analysis, confidence intervals, binomial tests, etc.). main inferential techniques covered using theoretical approximations (e.g., central limit theorem) well computational methods (e.g., permutation tests bootstrapping). Focus understanding methods interpreting results.difference Math 58 Math 58B?two classes remarkably similar content structure. Indeed, similarities classes differences. main differences handful topics different across two classes.Learning Outcomes Math 58(B)?Given study, identify population, sample, parameter, statistic, observational unit, variable.Describe differences , benefits , conclusions can drawn observational studies versus experiments.Given dataset research query, create appropriate figure R.Given dataset research query, compute appropriate statistics R.Describe difference distribution sample data sampling distribution particular statistic.particular research question, identify whether task requires descriptive analysis / model, graphic, confidence interval, hypothesis test,Apply empirical rule approximation confidence intervals hypothesis testing settings means proportions.able describe words p-value .Write appropriate null alternative hypotheses, choose correct analysis technique.Run hypothesis test / confidence interval analysis R.Identify appropriate summarize relationship two variables using least squares line.Describe optimization procedure leads least squares fit (although necessarily calculations).Provide settings causal claim warranted, strong correlation possibly due spurious relationships.Use regression line make predictions distinguish prediction interval independent response compared confidence interval slope parameter.descriptive analysis, visualization, confidence interval, hypothesis test, words communicate conclusion analysis original context data.Use R Markdown run reproducible analyses include aspects data analysis.take Math 58(B)?\nEvery educated citizen basic understanding statistics. Ok, ok, biases, ’m person thinks ! (https://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education) terms academic interests, take introductory statistics like take upper division statistics planning analyze data field outside statistics (e.g., biology, EA, psychology, etc.). Upper division statistics courses require introductory statistics, easy just “learn” statistics summer. highly recommend taking introductory statistics course.already taken AP Statistics, may may want repeat material. strong course excellent teacher scored well exam, probably need repeat material. uncertain many concepts, may want re-take course jumping upper division statistics courses. use R extensively, probably didn’t use R AP Statistics classes. upper division statistics classes expect able jump R head first, introductory statistics gives gentle introduction R.prerequisites Math 58(B)?formal prerequisite semester calculus, almost calculus entire semester. However, student Math 58(B) quantitatively inclined ready see many new mathematical, algorithmic, computational ideas quickly throughout semester.overlap classes?\nconsiderable overlap Math 58 Math 58B; take Math 58 Math 58B. differences two sections lie examples well handful topics different across two courses. also quite bit overlap introductory statistics courses (e.g., Econ 57, Pysch 158, Politics 90, AP Statistics). introductory statistics courses cover quite bit probability without getting deeply inferential ideas. focus statistics instead probability emphasis understanding intuition mathematical derivations inform analysis tool. also focus computer can help us gain deeper understanding analyses .take Math 58 Math 58B?\nIntroductory (Bio)Statistics taken early undergraduate schedule possible. taking Math 58(B) open possibilities taking upper division statistics classes. Additionally, background covered Math 58(B) provide deeper understanding concepts covering science social science courses.workload Math 58(B)?\nMath 58(B) meets twice week 75 min lecture week hour lab. Every week one homework one lab assignment. two midterm exams, -class take-home section. final exam also -class take-home parts. class known extremely difficult time consuming; however, require stay material, assignments, come class meetings (participation part grade).software use? real world applications? mathematics? CS?\nwork done R using RStudio front end. need either download R RStudio (free) onto computer use Pomona’s server. class mix many real world applications case studies, higher level math, programming, communication skills. final project requires analysis dataset choosing.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.http://swirlstats.com/ one way walk learning basics R.assignments turned using R Markdown compiled pdf.\nFigure 2.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 2.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"ex:helper","chapter":"2 Introduction","heading":"2.2 Example: Friend or Foe","text":"example comes Investigation 1.1: Friend Foe? Chance Rossman (2018). idea use simulation determine likely data nothing interesting going .study reported November 2007 issue Nature, researchers investigated whether infants take account individual’s actions towards others evaluating individual appealing aversive, perhaps laying foundation social interaction (Hamlin, Wynn, Bloom 2007). words, children aren’t even yet talking still form impressions someone’s friendliness based actions? one component study, 10-month-old infants shown “climber” character (piece wood “googly” eyes glued onto ) make hill two tries. infants shown two scenarios climber’s next try, one climber pushed top hill another character (“helper” toy) one climber pushed back hill another character (“hinderer” toy). infant alternately shown\ntwo scenarios several times. child presented pieces wood (helper hinderer characters) asked pick one play . Videos demonstrating component study can found http://campuspress.yale.edu/infantlab/media/.One important design consideration keep mind order equalize potential influencing factors shape, color, position, researchers varied colors shapes wooden characters even side toys presented infants. researchers found 14 16 infants chose helper hinderer.","code":""},{"path":"intro.html","id":"always-ask","chapter":"2 Introduction","heading":"Always Ask","text":"observational units?\ninfants\ninfantsWhat variable? type variable?\nchoice helper hindered: categorical\nchoice helper hindered: categoricalWhat statistic?\n\\(\\hat{p}\\) = proportion infants chose helper = 14/16 = 0.875\n\\(\\hat{p}\\) = proportion infants chose helper = 14/16 = 0.875What parameter?\np = proportion infants might choose helper (measurable!)\np = proportion infants might choose helper (measurable!)p-value probability data extreme nothing interesting going .","code":""},{"path":"intro.html","id":"computation","chapter":"2 Introduction","heading":"Computation","text":"","code":"\nlibrary(infer)\n\n# to control the randomness\nset.seed(47)\n\n# first create a data frame with the Infant data\nInfants <- read.delim(\"http://www.rossmanchance.com/iscam3/data/InfantData.txt\")\n\nInfants %>% head()##     choice\n## 1   helper\n## 2 hinderer\n## 3   helper\n## 4   helper\n## 5   helper\n## 6   helper\n# then find the proportion who help\n(p_obs <- Infants %>%\n    specify(response = choice, success = \"helper\") %>%\n    calculate(stat = \"prop\") )## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1 0.875\n# now apply the infer framework to get the null proportion\nnull_help <- Infants %>%\n  specify(response = choice, success = \"helper\") %>%\n  hypothesize(null = \"point\", p = .5) %>%\n  generate(reps = 1000, type = \"simulate\") %>%\n  calculate(stat = \"prop\")\n\n# then visualize the null sampling distribution & p-value\nvisualize(null_help, bins = 13) +\n  shade_p_value(obs_stat = p_obs, direction = \"two_sided\")\n# calculate the actual p-value\nnull_help %>%\n  get_p_value(obs_stat = p_obs, direction = \"two_sided\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1   0.002"},{"path":"intro.html","id":"logic-for-what-we-believe","chapter":"2 Introduction","heading":"Logic for what we believe","text":"look back study, can tell researchers varied color, shape, side, etc. make sure nothing systematic infants chose block (e.g., watch Blue’s Clues might love color blue, wouldn’t always want helper shape blue).excellent design survey rules outside influence reason many infants chose helper shape.ruled random chance mechanism larger number infants chose helper shape. (reject null hypothesis.)ruled random chance mechanism larger number infants chose helper shape. (reject null hypothesis.)conclude babies inclined helpful. , likely choose helper hindered. [Note: don’t evidence choose helper. , might predisposed. might modeling parents. might notice need lot help, etc.]conclude babies inclined helpful. , likely choose helper hindered. [Note: don’t evidence choose helper. , might predisposed. might modeling parents. might notice need lot help, etc.]","code":""},{"path":"intro.html","id":"experim","chapter":"2 Introduction","heading":"2.3 Types of Studies","text":"two basic types studies encountered observational experimental.experiment, researchers assign treatments cases. , researchers decide gets level treatment (also known explanatory variable). treatment assigned randomly, experiment known randomized experiment.experiment, researchers assign treatments cases. , researchers decide gets level treatment (also known explanatory variable). treatment assigned randomly, experiment known randomized experiment.observational study, researchers observe explanatory response variable without interfering data arise.observational study, researchers observe explanatory response variable without interfering data arise.three important types variables studies, include variables part model one category variables, confounding:Explanatory variable one potential explanation changes (e.g., smoking level).Response variable measured outcome interest (e.g., lung cancer).Confounding variable variable (typically measured!) associated explanatory response variables.","code":""},{"path":"intro.html","id":"example-hand-writing-sat-scores","chapter":"2 Introduction","heading":"2.3.1 Example: Hand Writing & SAT Scores1","text":"article handwriting appeared October 11, 2006 issue Washington Post. article mentioned among students took essay portion SAT exam 2005-06, wrote cursive style scored significantly higher essay, average, students used printed block letters. Researchers wanted know whether simply writing cursive way increase scores.Identify observational units, variables, types variables, parameter interest, statistic measured. type study ?Identify observational units, variables, types variables, parameter interest, statistic measured. type study ?Q1 writing cursive cause higher scores? potential confounding variables?Q1 writing cursive cause higher scores? potential confounding variables?article also mentioned different study one essay given graders. graders shown cursive version essay graders shown version printed block letters. average score assigned essay cursive style significantly higher average score assigned essay printed block letters.change? observational units, variables, types variables, parameter interest, statistic measured. type study ?change? observational units, variables, types variables, parameter interest, statistic measured. type study ?Q2 can conclusion include causal statement now? ? changed?Q2 can conclusion include causal statement now? ? changed?","code":""},{"path":"intro.html","id":"example-have-a-nice-trip","chapter":"2 Introduction","heading":"2.3.2 Example: Have a Nice Trip2","text":"area research biomechanics gerontology concerns falls fall-related injuries, especially elderly people. Recent studies focused individuals respond large postural disturbances (e.g., tripping, induced slips). One question whether subjects can instructed improve recovery perturbations. Suppose researchers want compare two recovery strategies, lowering (making next step shorter, normal step time) elevating (using longer normal step length normal step time). Subjects first trained one two recovery strategies, asked apply feel tripping. researchers induce subject trip walking (harnessed safety), using concealed mechanical obstacle.Suppose following 24 subjects agreed participate study. males female recruited females tend better balance (lower center gravity).Females: Alisha, Alice, Betty, Martha, Audrey, Mary, Barbie, AnnaMales: Matt, Peter, Shawn, Brad, Michael, Kyle, Russ, Patrick, Bob, Kevin, Mitch, Marvin,\nPaul, Pedro, Roger, SamThe applet http://www.rossmanchance.com/applets/Subjects.html helpful visualizing confounding variables removed treatment randomly assigned.Q1 want allow women trained “lowering” technique men trained “elevating” technique?Q1 want allow women trained “lowering” technique men trained “elevating” technique?Q2 randomize treatment? affect gender balance? Height distribution? Gene distribution? Factor “X?”Q2 randomize treatment? affect gender balance? Height distribution? Gene distribution? Factor “X?”Q3 gender balance across two treatments required study? treatment randomly allocated observational units? , change Q2?Q3 gender balance across two treatments required study? treatment randomly allocated observational units? , change Q2?","code":""},{"path":"intro.html","id":"study-conclusions","chapter":"2 Introduction","heading":"2.3.3 Study conclusions","text":"ideas surrounding study design typically connect question causality: possible infer causality end study? However, words use (“random allocation”) sound lot like words used describing sampling (“random sample” “random selection”).Random sampling random allocation DIFFERENT concepts! importantly, conclusions made two different study characteristics different.Random selection Random sample - unit population equally likely chosen sample.Random allocation - observational unit equally likely assigned treatments (explanatory variable).\nFigure 2.4: Random Sample vs Randomized Experiment, taken Ramsey Schafer (2012)\n\nFigure 2.5: Random Sample vs Randomized Experiment, taken https://askgoodquestions.blog/\nideal world, every study participants randomly sampled population randomly allocated treatments. However, limitations ethical research makes simultaneously random processes difficult. ? Consider following:clinical trial, makes sense randomly allocate subjects. , however, randomly select people population take part clinical trial. ?clinical trial, makes sense randomly allocate subjects. , however, randomly select people population take part clinical trial. ?political poll, seems reasonable participants called (necessarily people respond!) random sample population. make sense, however, randomly allocate people different treatments. ?political poll, seems reasonable participants called (necessarily people respond!) random sample population. make sense, however, randomly allocate people different treatments. ?","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"2 Introduction","heading":"2.4 Reflection Questions","text":"","code":""},{"path":"intro.html","id":"types-of-studies-ims-chapter-1","chapter":"2 Introduction","heading":"2.4.1 Types of studies: IMS Chapter 1","text":"difference observational study experiment?aren’t studies done experiments?confounding variable?looked Figure 2.4 Figure 2.5? understand two figures? explain main message friend? [Random sampling vs. Random allocation]statistical meaning word cause different usage sentence: ball hit head caused get headache.meanings words: randomized, double-blind (single-blind), control, placebo, comparative. ideas important interpreting study results?","code":""},{"path":"regdesc.html","id":"regdesc","chapter":"3 Correlation & Regression as Models","heading":"3 Correlation & Regression as Models","text":"next topic focus modeling using two quantitative variables. , explanatory response variables measured numeric scale.get started, consider handful crop types taken World Data part Tidy Tuesday. point plot represents different country. x y variables represent proportion total yield last 50 years due crop type.\nFigure 3.1: % total yield different crops (across last 50 years). point represents country.\nOrder six scatterplots strongest negative strongest positive linear relationship. questions ask :correlation perfect positive relationship?correlation perfect negative relationship?correlation relationship?","code":""},{"path":"regdesc.html","id":"cor","chapter":"3 Correlation & Regression as Models","heading":"3.1 Correlation","text":"Correlation measures linear association two numerical variables. [Note, describing two categorical (one numerical & one categorical) variables vary together, said associated instead correlated.]correlation coefficient measures strength direction linear association two numerical variables.","code":""},{"path":"regdesc.html","id":"estimating-correlation","chapter":"3 Correlation & Regression as Models","heading":"3.1.1 Estimating Correlation","text":"value correlation defined :\\[\\begin{eqnarray*}\nr &=& \\frac{ \\sum_i (x_i  - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_i(x_i - \\overline{x})^2} \\sqrt{ \\sum_i(y_i - \\overline{y})^2}}\\\\\nr &=& \\frac{1}{n-1} \\sum_{=1}^n \\bigg(\\frac{x_i - \\overline{x}}{s_x} \\bigg) \\bigg(\\frac{y_i - \\overline{y}}{s_y} \\bigg)\n\\end{eqnarray*}\\]\nFigure 3.2: % total yield different crops (across last 50 years). point represents country. Now lines average x average y values superimposed onto plots.\nred dot (plot), consider distance observation \\(\\overline{X}\\) line \\(\\overline{Y}\\) line. observation (red dot) ? ? one ?particular red dot (observation) contribute correlation? positive way (make \\(r\\) bigger)? negative way (make \\(r\\) smaller)?ideas worth thinking :quadratic plots can zero correlation yet perfect functional relationship\\(-1 \\leq r \\leq 1\\)correlation imply causation (ice cream & boating accidents!)inference \\(\\rho\\) well \\(\\beta_1\\), data come bivariate normal distribution. , histograms \\(X\\) \\(Y\\) normal, scatterplot cloud.correlation go narrow range X values represented (see denominator r).measurement error biases estimate correlation coefficient toward zero.can calculate correlation value crop plots order strongest negative strongest positive linear relationship: \\(\\rightarrow D \\rightarrow B \\rightarrow C \\rightarrow E \\rightarrow F\\)\nTable 3.1: Correlation percentage total yield across different crops.\n","code":""},{"path":"regdesc.html","id":"coefficient-of-determination-r2","chapter":"3 Correlation & Regression as Models","heading":"3.1.2 Coefficient of Determination: \\(R^2\\)","text":"coefficient determination (\\(R^2\\)) square correlation (given ). However, also additional interpretation useful us. can measure much original variability Y given regression line. SSE least-squares defined fit line scatter plot observations.SSE “sum squared errors” (think \\(s^2\\) defined). , \\(SSE(\\overline{y})\\) amount response variable varies . \\(SSE(\\mbox{least-squares})\\) amount response variable varies around regression line (see Section 3.2).\\[\\begin{eqnarray*}\nR^2 &=& \\frac{SSE(\\overline{y}) - SSE(\\mbox{least-squares})}{SSE(\\overline{y})} \\\\\n &=& \\frac{Var(y_i) - Var(e_i)}{Var(y_i)} \\\\\n &=& 1 - \\frac{Var(e_i)}{Var(y_i)}\\\\\n\\end{eqnarray*}\\]value \\(e_i\\) discussed detail , distance observed response variable prediction line: \\[\\begin{align}e_i=y_i-\\hat{y}_i\\end{align}\\]\\(R^2\\) can used even models many explanatory variables. , way think \\(R^2\\) terms much variability response variable removed (learned values explanatory variables). \\(R^2\\) proportion reduction variability response variable explained explanatory variable.","code":""},{"path":"regdesc.html","id":"slr","chapter":"3 Correlation & Regression as Models","heading":"3.2 Simple Linear Regression","text":"Regression method predicts value one numerical variable another. , extension describing degree linearity relationship (correlation), goal now create best linear model – often prediction. Note many characteristics explored correlation applicable regression. However, correlation treats \\(X\\) \\(Y\\) interchangeable, whereas regression treats \\(X\\) fixed known \\(Y\\) random unknown. previously, call \\(X\\) explanatory variable, \\(Y\\) response variable. , assume causal mechanism \\(X\\) \\(Y\\) even strong linear (otherwise) relationship.","code":""},{"path":"regdesc.html","id":"predicted-values","chapter":"3 Correlation & Regression as Models","heading":"Predicted Values","text":"predicted values Y regression line estimate mean value \\(Y\\) individuals given value \\(X\\). Notice Roman letters (English letters) representing statistics:\\[\\begin{eqnarray*}\n\\hat{y} &=& b_0 + b_1 x\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\ne_i &=& y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]Notice, predicting mean value response variable given value explanatory variable!","code":""},{"path":"regdesc.html","id":"ls","chapter":"3 Correlation & Regression as Models","heading":"3.2.1 Least Squares estimation of the regression line","text":"find values regression statistics, sum squared errors minimized.SSE: Sum squared errors (residuals) measure closely line fits points. SSE value squared deviations calculated “best” possible values \\(\\beta_0\\) \\(\\beta_1\\) given dataset.\\[\\begin{eqnarray*}\nSSE = \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (y_i - (b_0 + b_1 x_i) )^2\n\\end{eqnarray*}\\]\nminimized values:\n\\[\\begin{eqnarray*}\nb_0 = \\overline{y} - b_1 \\overline{x} & \\ \\ \\ \\ \\ \\ \\ & b_1 = r \\frac{s_y}{s_x}\n\\end{eqnarray*}\\]find “best” fitting line, searched line smallest residuals (SSE) sense. particular, goal try find line minimizes following quantity: \\[Q=\\sum e_i^2 = \\sum (y_i-(b_0+b_1x_i))^2.\\]Finding \\(b_0\\) \\(b_1\\) minimize Q calculus problem.\n\\[\\frac{dQ}{db_0}=-2\\sum (y_i-(b_0+b_1x_i)),\\qquad \\frac{dQ}{db_1}=-2\\sum\nx_i(y_i-(b_0+b_1x_i))\\]\nSetting derivatives equal 0 solving \\(b_0\\) \\(b_1\\) yields optimal values, denoted \\(b_0\\) \\(b_1\\)One aspect optimization problem worth pointing role two variables interest \\(X\\) \\(Y\\). switch roles \\(X\\) \\(Y\\), best fitting line different. relationship invariant variable choose response, switching roles \\(X\\) \\(Y\\) give slope \\(1/b_1\\), case. [Note role \\(X\\) \\(Y\\) invariant calculating correlation calculating least squares regression line.]","code":""},{"path":"regdesc.html","id":"residuals","chapter":"3 Correlation & Regression as Models","heading":"Residuals","text":"Residuals measure scatter points least squares regression line. use residuals many calculations interpretations model. Indeed, goal linear regression find model small residuals. , ideally, known variable \\(X\\) tell us know unknown variable \\(Y\\).\\[\\begin{eqnarray*}\ne_i &=& (y_i - \\hat{y}_i)\\\\\nMSE&=& \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{n-2} = \\frac{\\sum_i (e_i)^2}{n-2} = s^2\\\\\nSSE &=& \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (e_i)^2\\\\\nR^2 &=& 1 - \\frac{Var(e_i)}{Var(y_i)}\n\\end{eqnarray*}\\]","code":""},{"path":"regdesc.html","id":"r-code-for-regression","chapter":"3 Correlation & Regression as Models","heading":"3.3 R code for regression","text":"","code":""},{"path":"regdesc.html","id":"ex:cat","chapter":"3 Correlation & Regression as Models","heading":"3.3.1 Example: Cat Jumping3 (Correlation & SLR)","text":"Consider cat data given Investigations 5.6 5.13. idea understand cat jumping velocity function body characteristics. Note correlation \\(r=-0.496\\) bodymass velocity.","code":"\ncats <- read_table2(\"http://www.rossmanchance.com/iscam2/data/CatJumping.txt\")\n\nggplot(cats, aes(x=bodymass, y = velocity)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se=FALSE)"},{"path":"regdesc.html","id":"correlation","chapter":"3 Correlation & Regression as Models","heading":"Correlation","text":"","code":"\ncats %>%\n  select(bodymass, velocity) %>%\n  cor()##            bodymass   velocity\n## bodymass  1.0000000 -0.4964022\n## velocity -0.4964022  1.0000000"},{"path":"regdesc.html","id":"simple-linear-regression","chapter":"3 Correlation & Regression as Models","heading":"Simple Linear Regression","text":"","code":"\nlibrary(broom)\n\ncats %>%\n  lm(velocity ~ bodymass, data = .) %>%\n  tidy()## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 394.      23.4         16.8  1.35e-11\n## 2 bodymass     -0.0122   0.00533     -2.29 3.61e- 2"},{"path":"regdesc.html","id":"residual-plot","chapter":"3 Correlation & Regression as Models","heading":"Residual Plot","text":"work residuals, use augment().","code":"\ncats %>%\nlm(velocity ~ bodymass, data = .) %>% augment()## # A tibble: 18 x 8\n##    velocity bodymass .fitted .resid   .hat .sigma   .cooksd .std.resid\n##       <dbl>    <dbl>   <dbl>  <dbl>  <dbl>  <dbl>     <dbl>      <dbl>\n##  1     334.     3640    350. -15.6  0.0656   30.3 0.0104       -0.545 \n##  2     387.     2670    362.  25.4  0.131    29.7 0.0640        0.920 \n##  3     411.     5600    326.  84.6  0.119    19.8 0.629         3.05  \n##  4     319.     4130    344. -25.5  0.0557   29.8 0.0232       -0.887 \n##  5     369.     3020    358.  11.1  0.101    30.4 0.00867       0.394 \n##  6     359.     2660    362.  -3.23 0.132    30.6 0.00105      -0.117 \n##  7     345.     3240    355. -10.4  0.0853   30.4 0.00624      -0.366 \n##  8     325.     5140    332.  -7.19 0.0844   30.5 0.00297      -0.254 \n##  9     301.     3690    349. -48.1  0.0639   27.7 0.0962       -1.68  \n## 10     332.     3620    350. -18.5  0.0664   30.2 0.0149       -0.648 \n## 11     313.     5310    330. -17.1  0.0957   30.2 0.0196       -0.608 \n## 12     317.     5560    327.  -9.86 0.116    30.4 0.00823      -0.354 \n## 13     376.     3970    346.  29.5  0.0572   29.5 0.0321        1.03  \n## 14     372.     3770    348.  23.9  0.0615   29.9 0.0228        0.834 \n## 15     314.     5100    332. -18.0  0.0820   30.2 0.0179       -0.634 \n## 16     368.     2950    358.   9.01 0.106    30.5 0.00614       0.322 \n## 17     286.     7930    298. -11.5  0.508    30.3 0.157        -0.552 \n## 18     352.     3550    351.   1.32 0.0692   30.6 0.0000797     0.0463\ncats %>%\nlm(velocity ~ bodymass, data = .) %>% augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0)"},{"path":"regdesc.html","id":"ex:houses","chapter":"3 Correlation & Regression as Models","heading":"3.3.2 Example: Housing Prices4 (SLR & MLR & Prediction)","text":"","code":"\nlibrary(GGally)\nhouse = read.table(\"http://www.rossmanchance.com/iscam2/data/housing.txt\", \n                   header=TRUE, sep=\"\\t\")\nnames(house)## [1] \"sqft\"     \"price\"    \"City\"     \"bedrooms\" \"baths\""},{"path":"regdesc.html","id":"descriptive-statistics","chapter":"3 Correlation & Regression as Models","heading":"Descriptive Statistics","text":"good first step investigate variables relate one another. ggpairs function come R package GGally.","code":"\nggpairs(house, columns = c(1,2,4,5))"},{"path":"regdesc.html","id":"reflection-questions-1","chapter":"3 Correlation & Regression as Models","heading":"3.4 Reflection Questions","text":"","code":""},{"path":"regdesc.html","id":"correlation-simple-linear-regression-chapter-3","chapter":"3 Correlation & Regression as Models","heading":"3.4.1 Correlation & Simple Linear Regression: Chapter 3","text":"Describe linear model multiple variables.Describe error / residual term calculated multiple variables.(three-ish) statistics interest chapter? parameters interest?correlation measure?find values \\(b_0\\) \\(b_1\\) estimating least squares line?dangerous extrapolate?interpret \\(R^2\\)? ?linear regression always appropriate comparing two continuous variables?","code":""},{"path":"foundations-for-inference.html","id":"foundations-for-inference","chapter":"4 Foundations for Inference","heading":"4 Foundations for Inference","text":"","code":""},{"path":"foundations-for-inference.html","id":"randtest","chapter":"4 Foundations for Inference","heading":"4.1 Randomization Test","text":"Hypothesis testing arguably fundamental process decision making statistics.\nparticular hypothesis test given called Randomization Test. Many hypothesis tests (slightly different structure pieces) seen throughout course.Definition 4.1  Randomization TestA hypothesis test based simulated random assignment explanatory variable. simulations done condition association explanatory variable response variable.section describe structure randomization test within framework example gender discrimination.Example 4.1  Gender DiscriminationWe consider study investigating gender discrimination 1970s, set context personnel decisions within bank.5 research question hope answer , “females discriminated promotion decisions made male managers?”participants study 48 male bank supervisors attending management institute University North Carolina 1972. asked assume role personnel director bank given personnel file judge whether person promoted branch manager position. files given participants identical, except half indicated candidate male half indicated candidate female. files randomly assigned subjects.supervisor recorded gender associated assigned file promotion decision. Using results study summarized Table 2.1, like evaluate females unfairly discriminated promotion decisions. study, smaller proportion females promoted males (0.583 versus 0.875), unclear whether difference provides convincing evidence females unfairly discriminated . (Çetinkaya-Rundel Hardin (2021))","code":""},{"path":"foundations-for-inference.html","id":"always-ask-1","chapter":"4 Foundations for Inference","heading":"Always Ask","text":"observational units?\nsupervisor\nsupervisorWhat variables? type variables?\nwhether resume male female (explanatory, categorical)\n\ndecision promote promote (response, categorical)\n\nwhether resume male female (explanatory, categorical)\nwhether resume male female (explanatory, categorical)decision promote promote (response, categorical)\ndecision promote promote (response, categorical)statistic?\n\\(\\hat{p}_m - \\hat{p}_f\\) = 21/24 - 14/24 = 0.292 (difference proportion men promoted proportion women promoted)\n\\(\\hat{p}_m - \\hat{p}_f\\) = 21/24 - 14/24 = 0.292 (difference proportion men promoted proportion women promoted)parameter?\n\\(p_m - p_f\\) = true difference probability man promoted minus probability woman promoted.\n\\(p_m - p_f\\) = true difference probability man promoted minus probability woman promoted.","code":""},{"path":"foundations-for-inference.html","id":"hypotheses","chapter":"4 Foundations for Inference","heading":"Hypotheses","text":"H0: Null hypothesis. variables gender decision independent. relationship, therefore observed difference proportion males females promoted due chance.HA: Alternative hypothesis. variables gender decision independent. observed difference proportion males females promoted due chance.","code":""},{"path":"foundations-for-inference.html","id":"computation-1","chapter":"4 Foundations for Inference","heading":"Computation","text":"","code":"\nlibrary(infer)\n\n# to control the randomness\nset.seed(47)\n\n# first create a data frame with the discrimination data\ndiscrim <- data.frame(gender = c(rep(\"male\", 24), rep(\"female\", 24)),\n                      decision = c(rep(\"promote\", 21), rep(\"not\", 3), \n                                   rep(\"promote\", 14), rep(\"not\", 10)))\n\ndiscrim %>% head()##   gender decision\n## 1   male  promote\n## 2   male  promote\n## 3   male  promote\n## 4   male  promote\n## 5   male  promote\n## 6   male  promote\n# then find the difference in proportion who are promoted\n(diff_obs <- discrim %>%\n    specify(decision ~ gender, success = \"promote\") %>%\n    calculate(stat = \"diff in props\", order = c(\"male\", \"female\")) )## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1 0.292\n# now apply the infer framework to get the null differences in proportions\nnull_discrim <- discrim %>%\n  specify(decision ~ gender, success = \"promote\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 10000, type = \"permute\") %>%\n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n\n# then visualize the null sampling distribution & p-value\nvisualize(null_discrim, bins = 10) +\n  shade_p_value(obs_stat = diff_obs, direction = \"greater\")\n# calculate the actual p-value\nnull_discrim %>%\n  get_p_value(obs_stat = diff_obs, direction = \"greater\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1   0.026"},{"path":"foundations-for-inference.html","id":"logic-for-what-we-believe-1","chapter":"4 Foundations for Inference","heading":"Logic for what we believe","text":"know study experiment, systematic differences group received “male” applications “female” applications.know study experiment, systematic differences group received “male” applications “female” applications.’ve ruled random chance reason huge difference proportions. (reject null hypothesis.) lived null reality, ’d see data like 2.5% time.’ve ruled random chance reason huge difference proportions. (reject null hypothesis.) lived null reality, ’d see data like 2.5% time.conclude gender decision independent. , knowing gender changes probability promotion.conclude gender decision independent. , knowing gender changes probability promotion.","code":""},{"path":"foundations-for-inference.html","id":"structure-of-hypothesis-testing","chapter":"4 Foundations for Inference","heading":"4.2 Structure of Hypothesis testing","text":"","code":""},{"path":"foundations-for-inference.html","id":"hypotheses-1","chapter":"4 Foundations for Inference","heading":"4.2.1 Hypotheses","text":"Hypothesis Testing compares data expectation specific null hypothesis. data unusual, assuming null hypothesis true, null hypothesis rejected.Hypothesis Testing compares data expectation specific null hypothesis. data unusual, assuming null hypothesis true, null hypothesis rejected.Null Hypothesis, \\(H_0\\), specific statement population made purposes argument. good null hypothesis statement interesting reject.Null Hypothesis, \\(H_0\\), specific statement population made purposes argument. good null hypothesis statement interesting reject.Alternative Hypothesis, \\(H_A\\), specific statement population researcher’s interest demonstrate. Typically, alternative hypothesis contains values population included null hypothesis.Alternative Hypothesis, \\(H_A\\), specific statement population researcher’s interest demonstrate. Typically, alternative hypothesis contains values population included null hypothesis.two-sided (two-tailed) test, alternative hypothesis includes values sides value specified null hypothesis.two-sided (two-tailed) test, alternative hypothesis includes values sides value specified null hypothesis.one-sided (one-tailed) test, alternative hypothesis includes parameter values one side value specified null hypothesis. \\(H_0\\) rejected data depart direction stated \\(H_A\\).one-sided (one-tailed) test, alternative hypothesis includes parameter values one side value specified null hypothesis. \\(H_0\\) rejected data depart direction stated \\(H_A\\).","code":""},{"path":"foundations-for-inference.html","id":"other-pieces-of-the-process","chapter":"4 Foundations for Inference","heading":"4.2.2 Other pieces of the process","text":"statistic numerical measurement get sample, function data. [Also sometimes called estimate.]statistic numerical measurement get sample, function data. [Also sometimes called estimate.]parameter numerical measurement population. never know true value parameter.parameter numerical measurement population. never know true value parameter.test statistic quantity calculated data used evaluate compatible data result expected null hypothesis.test statistic quantity calculated data used evaluate compatible data result expected null hypothesis.null distribution sampling distribution outcomes test statistic assumption null hypothesis true.null distribution sampling distribution outcomes test statistic assumption null hypothesis true.p-value probability obtaining data (data showing great greater difference null hypothesis) null hypothesis true. p-value number calculated dataset.p-value probability obtaining data (data showing great greater difference null hypothesis) null hypothesis true. p-value number calculated dataset.","code":""},{"path":"foundations-for-inference.html","id":"examples-of-hypotheses","chapter":"4 Foundations for Inference","heading":"Examples of Hypotheses","text":"Identify whether following statements appropriate null hypothesis alternative hypothesis test:number hours preschool children spend watching TV affects behave children day care. AlternativeThe number hours preschool children spend watching TV affects behave children day care. AlternativeMost genetic mutations deleterious. AlternativeMost genetic mutations deleterious. AlternativeA diet fast foods effect liver function. NullA diet fast foods effect liver function. NullCigarette smoking influences risk suicide. AlternativeCigarette smoking influences risk suicide. AlternativeGrowth rates forest trees unaffected increases carbon dioxide levels atmosphere. NullGrowth rates forest trees unaffected increases carbon dioxide levels atmosphere. NullThe number hours grade-school children spend homework predicts future success standardized tests. AlternativeThe number hours grade-school children spend homework predicts future success standardized tests. AlternativeKing cheetahs average run speed standard spotted cheetahs. NullKing cheetahs average run speed standard spotted cheetahs. NullThe risk facial clefts equal babies born mothers take folic acid supplements compared mothers . NullThe risk facial clefts equal babies born mothers take folic acid supplements compared mothers . NullThe mean length African elephant tusks changed last 100 years. AlternativeThe mean length African elephant tusks changed last 100 years. AlternativeCaffeine intake pregnancy affects mean birth weight. AlternativeCaffeine intake pregnancy affects mean birth weight. Alternative","code":""},{"path":"foundations-for-inference.html","id":"what-is-an-alternative-hypothesis","chapter":"4 Foundations for Inference","heading":"What is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEYIn video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?","code":""},{"path":"foundations-for-inference.html","id":"all-together-structure-of-a-hypothesis-test","chapter":"4 Foundations for Inference","heading":"4.2.3 All together: structure of a hypothesis test","text":"decide research question (determine test)collect data, specify variables intereststate null (alternative) hypothesis values (often statements parameters)\nnull claim science want reject\nalternative claim science want demonstrate\nnull claim science want rejectthe alternative claim science want demonstrategenerate (null) sampling distribution describe variability statistic calculated along wayvisualize distribution statistics null modelget_p_value measure consistency observed statistic possible values statistic null modelmake conclusion using words describe research settingExample 4.2  Randomization test Gerrymandering:Note idea creating null distribution can apply wide range possible settings. key swap observations around null hypothesis “randomizing null hypothesis” helps get researcher conclusion.youtube video describing permuting (.e., randomizing) different voting boundaries come null distribution districts. problem (stated) possible describe using mathematical functions, can derive solution using computational approaches. [https://www.youtube.com/watch?v=gRCZR_BbjTo]Example 4.3  Email Paper & Pencil?6\npeople likely lie e-mail pencil paper? study reported meeting Academy Management involved 48 graduate students studying business particular university participated bargaining game (Naquin, Kurtzberg, & Belkin, 2008). response variable interest whether person misrepresented (lied ) size pot negotiating another player. participants randomly assigned use e-mail communication, whereas others used paper pencil. turned 24 26 used e-mail guilty lying pot size, compared 14 22 used paper pencil.usual, state following: observational units, variables, population interest, statistic measure, parameter.Let \\(p\\) = probability cheating (anyone larger population, say graduate students studying business US)\\(H_0: p_e = p_{pp}\\)\\(H_A: p_e \\ne p_{pp}\\)statistic interest difference sample proportions: \\(\\hat{p}_e - \\hat{p}_{pp} = 0.287\\)Using two-way tables applet Chance Rossman (2018), see two-sided p-value (approximately) 0.04.number observations half big proportions? (12 13 used e-mail guilty lying pot size, compared 7 11 used paper pencil.) two-sided p-value (approximately) 0.13.number observations twice big proportions? (48 52 used e-mail guilty lying pot size, compared 28 44 used paper pencil. two-sided p-value virtually zero.original data shows borderline evidence reject null hypothesis half data convincing (’d start science). double data extremely convincing. say variables truly independent, difference 28.7% lot large samples much small samples.(conclusion , let’s assume decision reject \\(H_0\\) happens p-value less 0.05.)Conclusion: (original data) p-value less 0.05, can reject null hypothesis. conclude probability cheating larger use email compared use pencil & paper. treatment randomly allocated, believe email vs pencil & paper led increase cheating. Unlikely can generalize people, probably reasonable generalize students business institution, maybe even institutions.","code":""},{"path":"foundations-for-inference.html","id":"CI","chapter":"4 Foundations for Inference","heading":"4.3 Confidence Intervals","text":"Randomization tests best suited modeling experiments treatment (explanatory variable) randomly assigned observational units attempt answer simple yes/question.example, consider following research questions can well assessed randomization test:vaccine make less likely person get malaria compared getting vaccine?drinking caffeine versus drinking caffeine affect quickly person can tap finger?Can predict candidate win upcoming election?section, however, instead interested estimating (testing) unknown value population parameter. example,much less likely get malaria get vaccine?much faster (slower) can person tap finger, average, drink caffeine first?proportion vote go candidate ?now, explore situation focus single proportion, introduce new simulation method, bootstrapping.","code":""},{"path":"foundations-for-inference.html","id":"boot","chapter":"4 Foundations for Inference","heading":"4.3.1 Bootstrapping","text":"Bootstrapping best suited modeling studies data generated random sampling population.\ngoal bootstrapping get understanding variability statistic (sample sample).\nvariability statistic can combined point estimate statistic order make claims population parameter.cases (indeed, even case described one proportion!) mathematical model can used describe variability statistic interest. encounter mathematical models (including distributions normal distribution, t-distribution chi squared distribution) later sections. However, now, approximate distribution statistic using repeated sampling.distribution statistic (called Sampling Distribution) contains information (, example, graph like histogram mathematical model) possible values statistic often values likely appear.Recall (Example 2.2 Kissing example HW) hypothesized value true proportion (e.g., \\(p=0.5\\) babies helping; \\(p=0.8\\) kissing), can understand / graph possible values \\(\\hat{p}\\) repeated samples.\n, true proportion people kiss right \\(p=0.8\\), select randomly bag 80% red marbles (20% white marbles) can understand variability associated sample proportion couples kiss right sample 124 couples.Example 4.4  Let’s say want find proportion videos YouTube take place outside. don’t idea true value , ’d like estimate population value. take random sample 128 videos (lots websites take random samples YouTube videos, tbh, don’t know “random” actually ), find 37 take place outside.originally hypothesized 47% YouTube videos happen outside, variability sample proportion can described following histogram. say, student class individually took random sample 128 YouTube videos (, condition p = 0.47), sample proportions vary . simulation computer, think repeatedly selecting 128 marbles bag 47% red.remember trying model situation hypothesized value parameter mind. , need use information sample estimate unknown characteristics population.Due theory beyond scope book, turns resamples taken sample, vary around \\(\\hat{p}\\) way individual samples vary around \\(p\\). Let’s break little bit.saw figure , \\(p=0.47\\), sample proportions vary 0.37 0.57, centered 0.47. , sample proportions (\\(\\hat{p}\\) values) vary around \\(p=0.47\\) plus minus 0.1. Also, note shape distribution reasonably bell-shaped symmetric.happens use data pseudo-population? actual dataset, 37 128 videos took place outside. \\(\\hat{p} = 0.29\\). bag marbles now 29% red marbles (71% white marbles), new bootstrapped sample proportions (\\(\\hat{p}_{BS}\\)) vary? Note 29% red marbles bag, resample bootstrapped proportion now varies around 0.29! range possible values still roughly plus minus 0.1, shape distribution bell-shaped symmetric.Definition 4.2  Resample. resample select observations observed sample one time, measure , replace back population, repeat new “resample” exactly size original sample.Note physical object connects resampling bag marbles. example, kissing setting, bag marbles 37 red marbles 91 white marbles. replacing marble selected color recorded, effectively creating infinitely large bag marbles 29% red.bootstrap process typically referred resampling replacement.Two good applets understanding bootstrapping sampling distributions :\n* StatKey (Statistics: Unlocking Power Data) http://www.lock5stat.com/StatKey/bootstrap_1_cat/bootstrap_1_cat.html\n* ISCAM http://www.rossmanchance.com/applets/2021/oneprop/OneProp.htm","code":""},{"path":"foundations-for-inference.html","id":"bootCI","chapter":"4 Foundations for Inference","heading":"4.3.2 Bootstrapping Confidence Intervals","text":"point estimate (also called statistic) gives single value best guess parameter interest.\nAlthough best guess, rarely perfect, expect error (.e., variability) estimated value.\nconfidence interval provides range plausible values parameter.\nconfidence level long-run percent intervals capture true parameter.Reminder :\n* goal find interval plausible values parameter, true proportion \\(p\\).\n* working one dataset, , one sample proportion \\(\\hat{p}\\).\n* can resample original dataset (resample called bootstrapping) find shape sample proportions vary.\n* Due theory beyond scope book, turns resamples taken sample, vary around \\(\\hat{p}\\) way individual samples vary around \\(p\\).two feet , two feet .Definition 4.3  95% Bootstrap Percentile Confidence Interval \\(p\\)bootstrapped proportions, find bootstrap value 2.5% 97.5% refer “lower” “upper” respectively.interval given : (lower, upper) 95% confidence interval \\(p\\).might suspect, goal create intervals capture true parameter lower rate (say, 90%) endpoints interval taken 5% 95% bootstrapped proportion values.\ngoal create intervals capture true parameter higher rate (say, 99%) endpoints interval taken 0.5% 99.5% bootstrapped proportion values.\nlarger confidence level capture true parameter higher rate (good thing!), comes cost also creating interval much wider 90% interval.\ninterval wide helpful trying understand population hand (don’t attempt create “100% intervals” given (0,1), useless learning anything new).","code":""},{"path":"foundations-for-inference.html","id":"a-note-on-sample-size","chapter":"4 Foundations for Inference","heading":"4.3.2.1 A note on sample size","text":"working applets, may notice variability proportions decreases substantially larger sample sizes. However rate confidence procedure captures parameter completely separated value sample sizes.\nconfidence procedure capture set rate regardless sample size.\nlarger sample size create narrow intervals (small sample size), capture true parameter often.","code":""},{"path":"foundations-for-inference.html","id":"normal-model","chapter":"4 Foundations for Inference","heading":"4.4 Normal Model","text":"","code":""},{"path":"foundations-for-inference.html","id":"CLT","chapter":"4 Foundations for Inference","heading":"4.4.1 Central Limit Therm","text":"","code":""},{"path":"foundations-for-inference.html","id":"example-reeses-pieces","chapter":"4 Foundations for Inference","heading":"Example: Reese’s Pieces7","text":"many examples, Reese’s Pieces example comes Chance Rossman (2018). example focuses samples orange Reese’s Pieces vary sample sample. Today aren’t particularly interested specific research question, instead trying understand details model describes \\(\\hat{p}\\) varies sample sample. [Spoiler: distribution going look like bell! mathematical model describes variability called normal distribution.]Notes applet: http://www.rossmanchance.com/applets/OneProp/OneProp.htm?candy=1How sampling distribution change function \\(p\\) \\(n\\)?normal distribution placed top empirical (computational) distribution, fit well?sampling distribution probability distribution possible values statistic possible samples size population. Note: increasing sample size reduces spread sampling distribution statistic (.e., increases precision).","code":""},{"path":"foundations-for-inference.html","id":"normal-probability-curve","chapter":"4 Foundations for Inference","heading":"Normal Probability Curve","text":"symmetricbell-shapedcentered \\(\\mu\\)\\(\\sigma\\) shows point inflectiondraw picture every time start normal problem!Central Limit TheoremThe Central Limit Theorem says sampling distribution average bell shaped distribution \\(n\\) big enough.sampling distribution \\(\\hat{p} = X/n\\) can thought taking lots random samples population, calculating \\(\\hat{p}\\), creating histogram. can easily calculate ’d expect sampling distribution know \\(p\\), true population proportion.\\(\\hat{p}\\) actually average, sampling distribution \\(\\hat{p}\\) can described normal distribution (long \\(n\\) big enough).\\[\\begin{eqnarray*}\n\\hat{p} &=& \\frac{X}{n}\\\\\nSD(\\hat{p}) = \\sigma_{\\hat{p}} &=& \\sqrt{\\frac{p (1-p)}{n}}\\\\\nSE(\\hat{p}) &=& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\hat{p} &\\sim& N\\bigg(p, \\sqrt{\\frac{p(1-p)}{n}} \\bigg) \\ \\ \\ \\ \\ \\mbox{ (sample size large enough)}\\\\\n\\end{eqnarray*}\\]Notice slight difference \\(SD\\) (uses \\(p\\)) \\(SE\\) (uses \\(\\hat{p}\\)). won’t make big deal difference (indeed, book calls equations \\(SD\\)). expect 95% \\(\\hat{p}\\) values within 2 standard deviations mean. , 95% \\(\\hat{p}\\) :\n\\[\\begin{eqnarray*}\np \\pm 2 \\sqrt{\\frac{p(1-p)}{n}}\n\\end{eqnarray*}\\]\nput differently, referring randomly selected \\(\\hat{p}\\),\n\\[\\begin{eqnarray*}\nP\\bigg( - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} - p \\leq 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\\nP\\bigg(\\hat{p} - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq  p \\leq \\hat{p} + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\n\\end{eqnarray*}\\]’d love create interval \\(p\\) using \\(\\hat{p} \\pm 2 \\sqrt{\\frac{p(1-p)}{n}}\\), don’t know \\(p\\)! One option use \\(SE(\\hat{p})\\) estimate variability.","code":""},{"path":"foundations-for-inference.html","id":"the-empirical-rule","chapter":"4 Foundations for Inference","heading":"The Empirical Rule","text":"bell-shaped, symmetric distribution,","code":""},{"path":"foundations-for-inference.html","id":"norm","chapter":"4 Foundations for Inference","heading":"4.4.2 Normal Probabilities & Z scores","text":"","code":""},{"path":"foundations-for-inference.html","id":"z-score","chapter":"4 Foundations for Inference","heading":"Z score","text":"Z score observation number standard deviations falls mean. compute Z score observation x follows distribution mean \\(\\mu\\) standard deviation \\(\\sigma\\) using\\[ Z = \\frac{x - \\mu}{\\sigma}\\]","code":""},{"path":"foundations-for-inference.html","id":"normal-probabilities","chapter":"4 Foundations for Inference","heading":"Normal probabilities","text":"return Reese’s Pieces example investigate probability particular number orange candies, using normal approximation.Remember: \\[SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\]respective Z score : \\[ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\]probability sample 25 candies, get less 40% orange (provided machine colors 50% candies orange). Answer: 0.1587What probability sample 25 candies, get less 40% orange (provided machine colors 50% candies orange). Answer: 0.1587What probability sample 250 candies, get less 40% orange (provided machine colors 50% candies orange). Answer: 0.0007888What probability sample 250 candies, get less 40% orange (provided machine colors 50% candies orange). Answer: 0.0007888What probability sample 25 candies, get 40% 55% orange (provided machine colors 50% candies orange). Answer: 0.5328What probability sample 25 candies, get 40% 55% orange (provided machine colors 50% candies orange). Answer: 0.5328Note normal probabilities can estimated variable distribution well approximated bell shape given normal curve. calculate Z scores probabilities non-proportion setting ask whether values possibly normal. (think?)","code":"\nlibrary(mosaic)\n# (a)\n(0.4 - 0.5) / sqrt(0.5*0.5/25)## [1] -1\nxpnorm(-1, 0, 1)## [1] 0.1586553\n# (b)\n(0.4 - 0.5) / sqrt(0.5*0.5/250)## [1] -3.162278\nxpnorm(-3.16, 0, 1)## [1] 0.0007888457\n# (c)\n(0.55 - 0.5) / sqrt(0.5*0.5/25)## [1] 0.5\nxpnorm(c(-1, 0.5), 0, 1)## [1] 0.1586553 0.6914625"},{"path":"foundations-for-inference.html","id":"example-athletic-comparison","chapter":"4 Foundations for Inference","heading":"4.4.2.1 Example: Athletic comparison8","text":"example allows comparison two athletes based speed strength. following information provided sample individuals measured:Speed measured time required run distance 40 yards, smaller times indicating desirable (faster) speeds. data, times run 40 yards mean 4.60 seconds standard devotion 0.15 seconds, minimum 4.40 seconds.Speed measured time required run distance 40 yards, smaller times indicating desirable (faster) speeds. data, times run 40 yards mean 4.60 seconds standard devotion 0.15 seconds, minimum 4.40 seconds.Strength measured amount weight lifted, weight indicating desirable (greater) strength data, amount weight lifted mean 310 pounds standard deviation 25 pounds.Strength measured amount weight lifted, weight indicating desirable (greater) strength data, amount weight lifted mean 310 pounds standard deviation 25 pounds.Calculate interpret Z score player can lift weight 370 pounds.\\[Z = \\frac{370-310}{25} = 2.4\\]z-score tells us player can lift 370 pounds lifting 2.4 SDs average. Saying weight 2.4 SDs away average leave important information direction.Consider two players, B (data given ). player selected team one player can selected?first glance, can see faster, B stronger. Understanding player performs (strength speed) relative rest players first step answering question. calculate four Z scores, one player task:\\[\\begin{align*}\nZ_{Aspeed} =  \\frac{4.42 - 4.6}{0.15} = -1.2\\\\\nZ_{Astrength} = \\frac{370-310}{25} = 2.4\\\\\nZ_{Bspeed} =  \\frac{4.57 - 4.6}{0.15} = -0.2\\\\\nZ_{Bstrength} = \\frac{375-310}{25} = 2.6\\\\\n\\end{align*}\\]calculating Z scores, found Player B slightly stronger Player , Player considerably faster Player B. question advised us consider criteria equally valuable, Player better choice.Using full information speed data, think distribution 40 yard running times approximately normal?! minimum close mean normal distribution provide reasonable model. “close” mean ? Let’s see many standard deviations minimum mean:\\[ Z = \\frac{4.4 - 4.6}{0.15} = -1.33 \\]Z score tells us minimum speed -1.33 standard deviations mean. According normal distribution (see plot ), expect 9% observations lower 4.4 seconds, normal distribution seem great fit observations.","code":"\nxpnorm(-1.333, 0, 1, plot = TRUE)## [1] 0.0912659"},{"path":"foundations-for-inference.html","id":"normCI","chapter":"4 Foundations for Inference","heading":"4.4.3 Normal Theory Confidence Intervals","text":"","code":""},{"path":"foundations-for-inference.html","id":"conditions-for-when-the-sampling-distribution-of-hatp-is-nearly-normal-the-central-limit-theorem","chapter":"4 Foundations for Inference","heading":"Conditions for when the sampling distribution of \\(\\hat{p}\\) is nearly normal (The Central Limit Theorem!!)","text":"sampling distribution \\(\\hat{p}\\), taken sample size \\(n\\) population true proportion \\(p\\), nearly normal :sample observations independentwe expected see least 10 successes 10 failures samples. Said differently, \\(np \\geq 10\\) \\(n(1-p) \\geq 10\\). sometimes called success-failure condition.conditions met, sampling distribution \\(\\hat{p}\\) nearly normal mean \\(p\\) standard error:\\[SE_{\\hat{p}} = SE (\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\]","code":""},{"path":"foundations-for-inference.html","id":"how-far-is-hatp-from-p","chapter":"4 Foundations for Inference","heading":"How far is \\(\\hat{p}\\) from \\(p\\) ???","text":"Great news, \\(SE(\\hat{p})\\) measures distance can expect \\(\\hat{p}\\) \\(p\\)!!! Indeed, Z score tells us distance \\(\\hat{p}\\) \\(p\\) units standard error.normal distribution provides percentages often Z scores fall certain ranges.empirical rule, expect 95% \\(\\hat{p}\\) values within 2 standard deviations mean. , 95% \\(\\hat{p}\\) :\n\\[\\begin{eqnarray*}\np \\pm 2 \\sqrt{\\frac{p(1-p)}{n}}\n\\end{eqnarray*}\\]\nput differently, referring randomly selected \\(\\hat{p}\\),\n\\[\\begin{eqnarray*}\nP\\bigg( p - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} \\leq p + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\\nP\\bigg( - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} - p \\leq 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\\nP\\bigg(\\hat{p} - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq  p \\leq \\hat{p} + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\n\\end{eqnarray*}\\]Putting together, create confidence interval \\(p\\) says 95% samples create confidence intervals capture true (unknown \\(p\\)):\\[95\\% \\mbox{ CI }p:  \\hat{p} \\pm 1.96 \\sqrt{\\frac{p(1-p)}{n}}\\]different percentage needed, change multiplier appropriately:","code":""},{"path":"foundations-for-inference.html","id":"confidence-interval-formula","chapter":"4 Foundations for Inference","heading":"Confidence Interval Formula","text":"\\[\\mbox{ CI }p:  \\hat{p} \\pm Z^* \\sqrt{\\frac{p(1-p)}{n}}\\]\\(Z^*\\)? defined using normal distribution centered zero standard deviation one.example, 99% confidence interval desired, find \\(Z^*\\) value captures 99% observations \\(-Z^*\\) \\(Z^*\\).\\[99\\% \\mbox{ CI }p:  \\hat{p} \\pm 2.58 \\sqrt{\\frac{p(1-p)}{n}}\\]","code":"\nxpnorm(c(-2.58, 2.58), 0, 1, plot = TRUE)## [1] 0.004940016 0.995059984"},{"path":"foundations-for-inference.html","id":"what-does-the-percentage-level-mean","chapter":"4 Foundations for Inference","heading":"What does the percentage level mean?","text":"confidence level long-run percent intervals capture true parameter.","code":""},{"path":"foundations-for-inference.html","id":"example-changes-in-extreme-poverty","chapter":"4 Foundations for Inference","heading":"4.4.4 Example: changes in extreme poverty","text":"","code":""},{"path":"foundations-for-inference.html","id":"in-class-activity-set-up","chapter":"4 Foundations for Inference","heading":"In-class activity set-up","text":"Recall -class activity:may familiar Hans Rosling founded website https://www.gapminder.org/ dedicated life promoting awareness global health issues, see Ted talks : https://www.ted.com/playlists/474/the_best_hans_rosling_talks_yo. One question liked ask :percentage world’s population live extreme poverty doubled, halved, remained past twenty years?go , answer question. extreme poverty doubled, halved, remained ? think?correct answer percentage halved, 5% sample 1005 U.S. adults 2017 got right. Rosling liked say chimpanzees better people: three options, expect 33.33% chimpanzees answer correctly.fact students randomly guessing, many standard deviations away “random guess” value 0.05? [Hint: use proportions percentages calculations.]use computer (except calculator, feel free use calculator use computer / R calculator). Note: need know many people asked, look .Solution\\[SD(\\hat{p}) = \\sqrt{p(1-p)/n} = \\sqrt{(1/3)(2/3)/1005} = 0.0149\\]far 0.05 (1/3) units standard deviation? ’s just Z score! Yikes, 5% value 19 STANDARD DEVIATIONS RANDOM GUESSING!!!say humans much worse random guessing answering question poverty? (hypothesis test , just reflection distance observed data random guess answer.)SolutionNot humans wrong, wrong extremely high rate. , wrong way can’t possibly guessing. must something question makes many people get wrong (maybe seeing media narrative describes continued problems extreme poverty?)find percent samples produced small \\(\\hat{p}\\) people indeed random guessing. Unsurprisingly, proportion samples exceedingly small:","code":"\nsqrt((1/3)*(2/3)/1005)## [1] 0.01486999\nZ_p = (0.05 - (1/3)) / sqrt((1/3)*(2/3)/1005)\nZ_p## [1] -19.05404\nxpnorm(-19.05, 0, 1, plot=TRUE)## [1] 3.28511e-81"},{"path":"foundations-for-inference.html","id":"confidence-interval-for-true-population-proportion","chapter":"4 Foundations for Inference","heading":"4.4.4.1 Confidence Interval for true population proportion","text":"Given extreme poverty set-, question turns one hypothesis test one confidence interval. Note making one change question, curious proportion people think rate doubled.\\[\\begin{eqnarray*}\np &=& \\mbox{true proportion people incorrectly believe % }\\\\\n&=& \\mbox{ world's population live extreme poverty doubled}\\\\\n\\hat{p} &=& \\mbox{sample proportion people incorrectly believe % }\\\\\n&=& \\mbox{ world's population live extreme poverty doubled}\n\\end{eqnarray*}\\]turns sample 1005 adult Americans, 593 people thought rate doubled.9\\[\\hat{p} = \\frac{593}{1005} = 0.59\\]95% confidence interval true proportion adult Americans think rate doubled (0.56, 0.62). 95% confident true proportion adult Americans think extreme poverty rate doubled 0.56 0.62.\\[ \\hat{p} \\pm 1.96 * \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]Question: Survey researchers typically select one random sample population, produce confidence interval based sample.\nknow whether resulting confidence interval successful capturing unknown value population parameter?Answer: don’t know! never know interval actually captures parameter . just know lifetime scientists, capture rate set.Question: can’t know sure whether confidence interval contains value population parameter, grounds can confident ?Answer: well, agree process created CI.","code":"\n593/1005 - 1.96 * sqrt((593/1005)*(412/1005) / 1005)## [1] 0.5596421\n593/1005 + 1.96 * sqrt((593/1005)*(412/1005) / 1005)## [1] 0.6204574"},{"path":"foundations-for-inference.html","id":"modCI","chapter":"4 Foundations for Inference","heading":"4.4.5 Modifying CIs","text":"","code":""},{"path":"foundations-for-inference.html","id":"changing-n","chapter":"4 Foundations for Inference","heading":"Changing \\(n\\)","text":"can see CI formula, increasing \\(n\\) effect decreasing width CI.\\[ \\hat{p} \\pm 1.96 * \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]","code":""},{"path":"foundations-for-inference.html","id":"changing-p","chapter":"4 Foundations for Inference","heading":"Changing \\(p\\)","text":"different value \\(p\\) means sampling distribution different center (different SE), coverage rate change, SE probably won’t change much.","code":""},{"path":"foundations-for-inference.html","id":"changing-the-confidence-level","chapter":"4 Foundations for Inference","heading":"Changing the confidence level","text":"choice \\(Z^*\\) determines (, say, lifetime scientist) percent research confidence intervals capture true parameter interest. Note larger \\(Z^*\\) value, likely sample produce CI captures true parameter.Note \\(Z^* = 1.645\\) produces CIs capture 90% rate. \\(Z^* = 2.58\\) produces CIs capture 99% rate.Question: don’t always use 99.99% CIs?Answer: intervals typically wide provide real information actual population parameter.","code":"\nxpnorm(1.645, 0, 1)## [1] 0.9500151\nxpnorm(2.58, 0, 1)## [1] 0.99506"},{"path":"foundations-for-inference.html","id":"samp","chapter":"4 Foundations for Inference","heading":"4.5 Sampling","text":"","code":""},{"path":"foundations-for-inference.html","id":"example-aliens-on-earth","chapter":"4 Foundations for Inference","heading":"4.5.1 Example: aliens on Earth10","text":"Assume alien landed Earth wants understand gender diversity humans. Fortunately, alien took good statistics course home planet, knows take sample human beings produce confidence interval proportion. Unfortunately, alien happens upon 2019 US Senate sample human beings. US Senate 25 senators self-identify female gender (ever!) among 100 members 2019.Calculate alien’s 95% confidence interval. (uh… confidence interval ?)calculation becomes .25 \\(\\pm\\) .085, interval (.165 \\(\\rightarrow\\) .335).Interpret interval.alien 95% confident proportion humans earth self identify female .165 .335.consistent experience living planet?, actual proportion humans self identify female much larger interval, closer 0.5.went wrong?alien select random sample humans. fact, alien’s sampling method biased toward -representing self-identifying females.saw applet, 5% 95% confidence intervals fail capture actual value population parameter. explanation went wrong ?! explanation 5% intervals failing relevant selected random samples . lack random sampling problem .reasonable alien conclude, 95% confidence, 16.5% 33.5% US senators year 2019 self-identify female?. know (sure, 100% confidence) exactly 25% U.S. senators 2019 self identify female. ’s entire population interest, ’s reason calculate confidence interval.Confidence intervals appropriate data collected biased sampling method. confidence interval calculated sample can provide dubious misleading information.Confidence intervals appropriate access entire population interest. unusual happy circumstance, simply describe population.","code":""},{"path":"foundations-for-inference.html","id":"example-gettysburg-address","chapter":"4 Foundations for Inference","heading":"4.5.2 Example: Gettysburg Address11","text":"authorship literary works often topic debate. works attributed Shakespeare actually written Bacon Marlowe? anonymously published Federalist Papers written Hamilton, Madison, Jay? authors writings contained Bible? fields “literary computing” “forensic stylometry” examine ways numerically analyzing authors’ works, looking variables sentence length rates occurrence specific words.passage course Abraham Lincoln’s Gettysburg Address, given November 19, 1863 battlefield near Gettysburg, PA. characterizing passage, ideally examine every word. However, often much convenient even efficient examine subset words.Step 1: sample 10 representative words.representative Gettysburg Address ways? length? [Note, parameter representing true average word length 4.29 letters.]class, found different samples (.e., different student’s selection 10 words) produced different sample means. generally, sample means varied well true population mean 4.29 letters.Step 2: sample 10 random words., class different sample produced different sample means. now sample means varied around center 4.29 letters.Step 3: sample 20 random words.-class samples centered around 4.29 letters, less variable (sample sample) 20 words selected 10 words selected.","code":""},{"path":"foundations-for-inference.html","id":"key-sampling-terms","chapter":"4 Foundations for Inference","heading":"4.5.3 Key sampling terms","text":"convenience sample individuals easily accessible likely included sample. instance, political survey done stopping people walking Bronx, represent New York City. often difficult discern sub-population convenience sample represents.simple random sample equivalent using raffle select cases. means case population equal chance included implied connection cases sample.sampling distribution distribution possible values statistic possible samples size population.increasing sample size reduces spread sampling distribution statistic (.e., increases precision).sampling distribution statistic depend population size! (assume “big enough” sample isn’t basically set population.)characteristics resulting samples systematically different population, call sampling mechanism biased. distribution sample statistics, repeated samples population, centered value population parameter, distribution statistic said unbiased.","code":""},{"path":"foundations-for-inference.html","id":"errors","chapter":"4 Foundations for Inference","heading":"4.6 Errors & Power","text":"significance level, \\(\\alpha\\), probability used criterion rejecting null hypothesis. p-value test less equal \\(\\alpha\\), null hypothesis rejected. p-value greater \\(\\alpha\\), null hypothesis rejected. significance level number calculated experiment run based dataset. (Often significance level set journal granting agency.)significance level, \\(\\alpha\\), probability used criterion rejecting null hypothesis. p-value test less equal \\(\\alpha\\), null hypothesis rejected. p-value greater \\(\\alpha\\), null hypothesis rejected. significance level number calculated experiment run based dataset. (Often significance level set journal granting agency.)rejection region values statistic need able reject \\(H_0\\).rejection region values statistic need able reject \\(H_0\\).type error rejecting true null hypothesis. significance level \\(\\alpha\\) sets probability committing type error.type error rejecting true null hypothesis. significance level \\(\\alpha\\) sets probability committing type error.type II error failing reject false null hypothesis.type II error failing reject false null hypothesis.power test probability random sample lead rejection false null hypothesis.power test probability random sample lead rejection false null hypothesis.","code":""},{"path":"foundations-for-inference.html","id":"example-baseball-player","chapter":"4 Foundations for Inference","heading":"4.6.1 Example: baseball player12","text":"following example taken Chance Rossman (2018), used explain many important nuanced ideas related structure hypothesis testing. provide basic idea , encouraged go applet convince idea true understand idea true. http://www.rossmanchance.com/applets/power.htmlSet-: Assume manager professional baseball team. One players (many years) 0.250 hitter. means every time goes bat 1 4 chance hitting ball (baseball aficionados may want talk baseball errors point, won’t mentioning baseball errors today’s example).player tells working extremely hard -season improved become 0.333 hitter. , now believes every time goes bat 1 3 chance hitting ball.may aware profession baseball players good make lot money. increase hitting ball 1 4 tries 1 3 tries worth many millions dollars. course, player trying convince now worth many additional millions dollars paid accordingly.? Well, need convince , indeed, improved.get started, ’ll just ask one usual questions: parameter interest?\\(p\\) = baseball players current probability hitting ball","code":""},{"path":"foundations-for-inference.html","id":"errors-lessons-learned","chapter":"4 Foundations for Inference","heading":"4.6.2 Errors: lessons learned","text":"encouraged go applet convince understand ideas true. http://www.rossmanchance.com/applets/power.htmlWhat Type Type II errors/ Type error means manager became convinced player better 0.250 hitter reality just still 0.250 hitter. Type II error means player improved well enough 20 -bats convince manager.worried type error? Player like minimize probability Type II error – manager missing improvement. manager like minimize probability Type error – incorrectly thinking player improvedWhat factors impact power? ?Increasing sample size increases power. sample size increases, distribution sample proportion gets narrow (SE decreases). SE decrease means null alternative curves overlap less. always ability take observations, although might extremely expensive time consuming measure data.Increasing sample size increases power. sample size increases, distribution sample proportion gets narrow (SE decreases). SE decrease means null alternative curves overlap less. always ability take observations, although might extremely expensive time consuming measure data.Increasing significance level \\(\\alpha\\) increase power. Ideally, probabilities types errors small, unfortunately inversely related: one error probability decreases, increases (unless factors change also). ’s typically done practice set maximum allowable probability Type error advance setting level significance \\(\\alpha\\), common value 0.05, followed 0.10 0.01, determine sample size necessary probability Type II error specific value.Increasing significance level \\(\\alpha\\) increase power. Ideally, probabilities types errors small, unfortunately inversely related: one error probability decreases, increases (unless factors change also). ’s typically done practice set maximum allowable probability Type error advance setting level significance \\(\\alpha\\), common value 0.05, followed 0.10 0.01, determine sample size necessary probability Type II error specific value.Increasing distance null alternative increase power. Unfortunately, little control alternative value. science determine alternative (case, baseball player’s ability determined alternative value). better science (.e., non-null) , better chances convincing audience (.e., publishing) results interesting. (Consider : much easier convince someone 8th graders taller, average, kindergartners convince someone 1st graders taller, average kindergartners.)Increasing distance null alternative increase power. Unfortunately, little control alternative value. science determine alternative (case, baseball player’s ability determined alternative value). better science (.e., non-null) , better chances convincing audience (.e., publishing) results interesting. (Consider : much easier convince someone 8th graders taller, average, kindergartners convince someone 1st graders taller, average kindergartners.)Type error rate double consider two sides?Consider situation null hypothesis really true. wait make alternative hypothesis ’ve seen data. choose rejection region 5% tail region one side. reject observed statistic tail (reminder: example null hypothesis really true!). Well, instead making Type error 5% time, process described actually makes “rejection” 10% time!CI \\(p\\) overlap particular number, consistent rejecting null HT value \\(p\\)?95% CI overlap \\(p\\) (example, p=0.47), \\(p\\) \\(\\hat{p}\\) 1.96 SEs away . \\(p\\) \\(\\hat{p}\\) 1.96 SEs away , Z score associate \\(\\hat{p}\\) larger (absolute value) 1.96 (definition Z score!). Z score larger (absolute value) 1.96, two-sided p-value less 0.05.","code":""},{"path":"foundations-for-inference.html","id":"reflection-questions-2","chapter":"4 Foundations for Inference","heading":"4.7 Reflection Questions","text":"","code":""},{"path":"foundations-for-inference.html","id":"hypothesis-testing-ims-chapter-5","chapter":"4 Foundations for Inference","heading":"4.7.1 hypothesis testing: IMS Chapter 5","text":"difference statistic parameter?typical study, one statistic one statistic? know value statistic?typical study, one parameter one parameter? know value parameter?Explain means statistic distribution.p-value?difference one- two-sided hypothesis?difference null hypothesis alternative hypothesis?","code":""},{"path":"foundations-for-inference.html","id":"normal-model-ims-section-5.3","chapter":"4 Foundations for Inference","heading":"4.7.2 normal model: IMS Section 5.3","text":"mean something normal distribution?can use normal curve calculate percentages probabilities?mean \\(\\hat{p}\\) distribution? Can explain words?central limit theorem tell us distribution \\(\\hat{p}\\)?technical conditions important order central limit theorem apply?Z score measure?","code":""},{"path":"foundations-for-inference.html","id":"confidence-intervals-ims-section-5.2","chapter":"4 Foundations for Inference","heading":"4.7.3 confidence intervals: IMS Section 5.2","text":"confidence interval?Part CI interpretation includes phrase “95% confident.” Explain 95% means.can find appropriate \\(Z^*\\) value?difference Z score \\(Z^*\\)?computing confidence interval (.e., don’t preconceived idea \\(p\\)), standard deviation \\(\\hat{p}\\) estimated?using normal distribution create confidence interval \\(p\\), critical value , say, 94.7% interval calculated?","code":""},{"path":"foundations-for-inference.html","id":"sampling-ims-section-1.3","chapter":"4 Foundations for Inference","heading":"4.7.4 sampling: IMS Section 1.3","text":"good take random samples?simple random sample?don’t researchers always take random samples?benefit(s) large sample provide study?difference practical significance statistical significance?","code":""},{"path":"foundations-for-inference.html","id":"errors-power-ims-section-6.2","chapter":"4 Foundations for Inference","heading":"4.7.5 errors & power: IMS Section 6.2","text":"never okay accept \\(H_0\\)?difference Type Type II error?worse: Type error Type II error?power? power calculated? power depend ?","code":""},{"path":"inference-for-categorical-data.html","id":"inference-for-categorical-data","chapter":"5 Inference for categorical data","heading":"5 Inference for categorical data","text":"","code":""},{"path":"inference-for-categorical-data.html","id":"inference-for-a-single-proportion","chapter":"5 Inference for categorical data","heading":"5.1 Inference for a single proportion","text":"Previously, used normal approximation describe distribution different values \\(\\hat{p}\\) random samples taken. learned central limit theorem describes distribution (see box section 3.1.1 page 124):take random, independent samples\\(np \\geq 10\\) \\(n(1-p) \\geq 10\\)\\[\\hat{p} \\sim N(p, \\sqrt{p(1-p)/n}).\\]\\[\\mbox{Z score} = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}}\\] bigger \\(Z^*\\) value particular value \\(\\alpha\\), know can reject \\(p\\) (Null Hypothesis value) true population parameter.interval estimate desired, \\(p\\) hypothesized, confidence interval created using:\\[\\hat{p} \\pm Z^* \\cdot \\sqrt{\\hat{p}(1-\\hat{p})}/n.\\]IMPORTANT: recall, interval method capturing parameter.","code":""},{"path":"inference-for-categorical-data.html","id":"binomial-distribution","chapter":"5 Inference for categorical data","heading":"5.2 Binomial distribution","text":"Binomial distribution describes exact probabilities associated binary outcomes. typically time cover Binomial distribution Introduction Biostatistics.Çetinkaya-Rundel Hardin (2021) discuss binomial distribution. Chance Rossman (2018), however, provide quite bit detail binomial concepts chapter 1.","code":""},{"path":"inference-for-categorical-data.html","id":"example-pop-quiz","chapter":"5 Inference for categorical data","heading":"5.2.1 Example: pop quiz","text":"5 problems quiz; everyone number papers 1. 5. problems multiple choice answers , B, C, D. Go ahead. ’ll grade papers everyone done.Solution: 1.B, 2.C, 3.B, 4.C, 5.AThe binomial distribution provides probability distribution number “successes” fixed number independent trials, probability success trial.\nOutcome trial can stated success / failure.\nnumber trials (\\(n\\)) fixed.\nSeparate trials independent.\nprobability success (\\(p\\)) every trial.\nbinomial distribution provides probability distribution number “successes” fixed number independent trials, probability success trial.Outcome trial can stated success / failure.number trials (\\(n\\)) fixed.Separate trials independent.probability success (\\(p\\)) every trial.\\[\\begin{eqnarray*}\nP(X=k) &=& {n \\choose k} p^k (1-p)^{n-k}\\\\\n{n \\choose k} &=& \\frac{ n!}{(n-k)! k!}\n\\end{eqnarray*}\\]example… \\(n=5\\). many ways get 2 successes?\n\\[\\begin{eqnarray*}\n{5 \\choose 2} &=& \\frac{ 5!}{2! 3!} = \\frac{ 5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1}{(3 \\cdot 2 \\cdot 1)(2 \\cdot 1)}\n\\end{eqnarray*}\\]numerator represents number possibilities 5 questions. don’t distinguish successes, don’t want double count . Similarly failures.class: different groups work probability 0, 1, 2, … 5 correct answers.\\[\\begin{eqnarray*}\nP(X=0) = {5 \\choose 0} (0.25)^0(0.75)^5  = 0.2373 && P(X=3) = {5 \\choose 3} (0.25)^3(0.75)^2  = 0.0879\\\\\nP(X=1) = {5 \\choose 1} (0.25)^1(0.75)^4  = 0.3955 && P(X=4) = {5 \\choose 4} (0.25)^4(0.75)^1  = 0.0146\\\\\nP(X=2) = {5 \\choose 2} (0.25)^2(0.75)^3  = 0.2637 && P(X=5) = {5 \\choose 5} (0.25)^5(0.75)^0  = 0.0010\\\\\n\\end{eqnarray*}\\]","code":"\nlibrary(mosaic)\nxpbinom(2, size = 5, prob = 0.25)  # P(X <= 2) vs. P(X > 2)## [1] 0.8964844\nxpbinom(3, size = 5, prob = 0.25)  # P(X <= 3) vs. P(X > 3)## [1] 0.984375"},{"path":"inference-for-categorical-data.html","id":"binomial-hypothesis-testing","chapter":"5 Inference for categorical data","heading":"5.2.2 Binomial Hypothesis Testing","text":"Consider example beginning semester babies choosing helper toy (instead hinderer), section 2.2. Recall 14 16 babies chose helper toy.binomial distribution apply setting? Let’s check:two choices? Yes, helper hinderer.fixed \\(n\\)? Yes, 16 babies.\\(p\\) ? Presumably. inherent \\(p\\) represents probability baby choose helper toy. choosing babies population \\(p\\).independent? hope ! babies don’t know tell experiment.really inclination babies choose helper toy, many babies researchers needed choose helper order get published?Let’s choose \\(\\alpha = 0.01\\). means \\(p=0.5\\), make Type error less 1% time. calculations , see rejection region \\(\\{ X \\geq 14 \\}\\). , researchers reject null hypothesis \\(\\alpha = 0.01\\) significance level, needed see 14, 15, 16 babies choose helper (16).\\[\\begin{eqnarray*}\nP(X \\geq 12) &=& {16 \\choose 12} (0.5)^{12}(0.5)^{4} + 0.0106 = 0.0384\\\\\nP(X \\geq 13) &=& {16 \\choose 13} (0.5)^{13}(0.5)^{3} + 0.00209 = 0.0106\\\\\nP(X \\geq 14) &=& {16 \\choose 14} (0.5)^{14}(0.5)^{2} + 0.000259 = 0.00209\\\\\nP(X \\geq 15) &=& {16 \\choose 15} (0.5)^{15}(0.5)^{1} + 0.0000153 = 0.000259\\\\\nP(X = 16) &=& {16 \\choose 16} (0.5)^{16}(0.5)^{0} = 0.0000153\\\\\n\\end{eqnarray*}\\]","code":"\nxpbinom(12, 16, 0.5)## [1] 0.9893646\nxpbinom(13, 16, 0.5)## [1] 0.9979095"},{"path":"inference-for-categorical-data.html","id":"binomial-power","chapter":"5 Inference for categorical data","heading":"5.2.3 Binomial Power","text":"Let’s say researchers inkling babies liked helpers. thought probably 70% babies preferred helpers. researchers needed decide 16 babies enough research. , measure 16 babies, convincing evidence babies actually prefer helper? Said differently, 16 babies, power test?\\[\\begin{eqnarray*}\n\\mbox{power} &=& P(X \\geq 14 | p = 0.7)\\\\\n&=& P(X=14 | p=0.7) + P(X = 15 | p=0.7)  + P(X = 16 | p=0.7)\\\\\n&=& {16 \\choose 14} (0.7)^{14}(0.3)^{2} + {16 \\choose 15} (0.7)^{15}(0.3)^{1} + {16 \\choose 16} (0.7)^{16}(0.3)^{0}\\\\\n&=& 0.099\n\\end{eqnarray*}\\]Yikes! babies actually prefer helper 90% time?\\[\\mbox{power} = P(X \\geq 14 | p = 0.9) = 0.789\\]","code":"\n1 - xpbinom(13, 16, 0.7)## [1] 0.09935968\n1 - xpbinom(13, 16, 0.9)## [1] 0.7892493"},{"path":"inference-for-categorical-data.html","id":"binomial-confidence-intervals-for-p","chapter":"5 Inference for categorical data","heading":"5.2.4 Binomial Confidence Intervals for \\(p\\)","text":"binomial distribution allow “plus minus” creation range plausible values confidence interval. Instead, hypothesis testing used directly come plausible values parameter \\(p\\). method outlines much tedious z - CI , produce exact interval \\(p\\) appropriate coverage level.Consider confidence interval created following way:Step 1: Collect data, calculate \\(\\hat{p}\\) particular dataset.Step 2: Test series values \\(p'\\) using observed \\(\\hat{p}\\) dataset hand.Step 3: List values \\(p'\\) rejected. Sort find smallest biggest value: (\\(p_{small}, p_{big}\\)).Ask whether true parameter (let’s call \\(p\\)) interval.type error made \\(p\\) tested, \\(p\\) interval.\\(p\\) rejected, interval.often type error made? 5% time. Therefore (\\(p_{small}, p_{big}\\)) 95% CI true population parameter \\(p\\).","code":""},{"path":"inference-for-categorical-data.html","id":"relative-risk","chapter":"5 Inference for categorical data","heading":"5.3 Relative Risk","text":"Previously (e.g., Gender discrimination example, ??) working proportion success two separate groups, proportion success subtracted (see also lab 4). Next week, differences proportions revisited, see section 5.5. First , new statistic interest relative risk, followed odds ratios.particular, interest ratio probabilities. [Note: decision measure ratio instead difference comes trying model particular research question hand. nothing inherently better ratios versus differences. , however, often easier think small probability changes done ratio instead difference.]\\[\\mbox{Relative Risk (RR)} = \\frac{\\mbox{proportion successes group 1}}{\\mbox{proportion successes group 2}}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"inference-on-relative-risk","chapter":"5 Inference for categorical data","heading":"5.3.1 Inference on Relative Risk","text":"Due theory won’t cover, fairly good mathematical approximation describes natural log relative risk varies sample sample:\\[\\ln(\\hat{RR})  \\stackrel{\\mbox{approx}}{\\sim}   N\\Bigg(\\ln(RR), \\sqrt{\\frac{1}{} - \\frac{1}{+C} + \\frac{1}{B} - \\frac{1}{B+D}}\\Bigg)\\]Statistic: \\[\\hat{p}_1 / \\hat{p}_2 = \\frac{/(+C) }{B/ (B+D)}\\]Null Hypothesis: \\[H_0: p_1/p_2 = 1\\]CI: CI true relative risk population, \\(p_1/p_2\\)\\[\\mbox{exponentiate} \\Bigg[ \\ln(\\hat{p}_1/\\hat{p}_2) \\pm z^*\\sqrt{ \\frac{1}{} - \\frac{1}{+C} + \\frac{1}{B} - \\frac{1}{B+D}}\\Bigg]\\]remember relative risk:percent change defined :\n\\[\\begin{eqnarray*}\n(RR - 1)*100\\% = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\hat{p}_2}*100\\% = \\mbox{percent change 2 1}\n\\end{eqnarray*}\\]percent change defined :\n\\[\\begin{eqnarray*}\n(RR - 1)*100\\% = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\hat{p}_2}*100\\% = \\mbox{percent change 2 1}\n\\end{eqnarray*}\\]CI \\(p_1/p_2\\) typically considered significant 1 interval. usually null hypothesis \\(H_0: p_1 = p_2\\) equivalently, \\(H_0: p_1/p_2 = 1\\).CI \\(p_1/p_2\\) typically considered significant 1 interval. usually null hypothesis \\(H_0: p_1 = p_2\\) equivalently, \\(H_0: p_1/p_2 = 1\\).","code":""},{"path":"inference-for-categorical-data.html","id":"using-infer-for-inference-on-rr","chapter":"5 Inference for categorical data","heading":"5.3.2 Using infer for inference on RR","text":"difference proportions, infer syntax can used simulate sampling distribution sample relative risk null hypothesis population proportions identical.NOTE order provide syntax comparable correct RR , smoking specified response variable, lungs specified explanatory variable.","code":"\nlibrary(infer)\nWynderGraham <- data.frame(lungs = c(rep(\"cancer\", 605), rep(\"healthy\", 780)),\n                            smoking = c(rep(\"light\", 22), rep(\"heavy\", 583),\n                                        rep(\"light\", 204), rep(\"heavy\", 576)))\n\n(obs_RR <- WynderGraham %>%\n  specify(smoking ~ lungs, success = \"heavy\") %>%\n  calculate(stat = \"ratio of props\", order = c(\"cancer\", \"healthy\")))## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  1.30\nnull_RR <- WynderGraham %>%\n  specify(smoking ~ lungs, success = \"heavy\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  infer::calculate(stat = \"ratio of props\", order= c(\"cancer\", \"healthy\"))\n\nnull_RR %>%\n  visualize() +\n  shade_p_value(obs_stat = obs_RR, direction = \"right\")"},{"path":"inference-for-categorical-data.html","id":"odds-ratios","chapter":"5 Inference for categorical data","heading":"5.4 Odds Ratios","text":"Experience shows introductory statistics students seen odds odds ratios prior mathematical scientific study. makes odds ratios new idea, fundamentally hard idea. say, perfectly acceptable find relative risk intuitive idea can easily discuss odds ratios strange idea hard interpret. discouraged! Odds ratios fundamentally harder understand relative risk, simply new idea.Çetinkaya-Rundel Hardin (2021) discuss relative risk odds ratios. Chance Rossman (2018), however, provide quite bit detail concepts Investigations 3.9, 3.10, 3.11.\\[\\mbox{risk} = \\frac{\\mbox{number successes}}{\\mbox{total number}}\\]\\[\\mbox{odds} = \\frac{\\mbox{number successes}}{\\mbox{number failures}}\\]\\[\\mbox{Odds Ratio ()} = \\frac{\\mbox{odds success group 1}}{\\mbox{odds success group 2}}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"example-smoking-and-lung-cancer","chapter":"5 Inference for categorical data","heading":"5.4.1 Example: Smoking and Lung Cancer13","text":"World War II, evidence began mounting link cigarette smoking pulmonary carcinoma (lung cancer). 1950s, three now classic articles published topic. One studies conducted United States Wynder Graham.14 found records large number patients specific type lung cancer hospitals California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, Utah. study, researchers focused 605 male patients form lung cancer. Another 780 male hospital patients similar age economic distributions without type lung cancer interviewed St. Louis, Boston, Cleveland, Hines, IL. Subjects (family members) interviewed assess smoking habits, occupation, education, etc. table classifies non-smoker light smoker, least moderate smoker.following two-way table replicates counts 605 male patients form cancer “control-group” 780 males.Given results study, think can generalize sample population? Explain make clear know difference sample population.order focus research question, combine data two groups: light smoking less 10 cigarettes per day, heavy smoking 10 cigarettes per day. 2x2 observed data now:Causation? (experiment possible confounding variables?)Case-control study (605 lung cancer, 780 without… baseline rate?)response variable explanatory variable? happens role two variables switched?lung cancer considered success light smoking baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{583/1159}{22/226} = 5.17\\\\\n&=& \\frac{583/576}{22/204} = 9.39\\\\\n\\end{eqnarray*}\\]risk lung cancer 5.17 times higher heavy smoke don’t smoke.odds lung cancer 9.39 times higher heavy smoke don’t smoke.heavy smoking considered success healthy baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{583/605}{576/780} = 1.31\\\\\n&=& \\frac{583/22}{576/204} = 9.39\\\\\n\\end{eqnarray*}\\]risk heavy smoking 1.31 times higher lung cancer don’t lung cancer.odds heavy smoking 9.39 times higher lung cancer don’t lung cancer.Observational study (worked place?)Observational study (worked place?)Cross sectional (one point time)Cross sectional (one point time)Healthy worker effect (stayed home sick?)Healthy worker effect (stayed home sick?)Explanatory variable one potential explanation changes (smoking level).Explanatory variable one potential explanation changes (smoking level).Response variable measured outcome interest (lung cancer).Response variable measured outcome interest (lung cancer).Case-control study: identify observational units response variableCase-control study: identify observational units response variableCohort study: identify observational units explanatory variableCohort study: identify observational units explanatory variableThe risk light smoker person lung cancer can estimated, possible way estimate risk lung cancer light smoker. Consider population 1,000,000 people:\\[\\begin{eqnarray*}\nP(\\mbox{light} | \\mbox{lung cancer}) &=& \\frac{49,000}{50,000} = 0.98\\\\\nP(\\mbox{lung cancer} | \\mbox{light}) &=& \\frac{49,000}{100,000} = 0.49\\\\\n\\end{eqnarray*}\\]explanatory variable?explanatory variable?response variable?response variable?relative risk?relative risk?odds ratio?odds ratio?Group \nGroup B\nexpl = smoking status\nexpl = lung cancer\nresp = lung cancer\nresp = smoking status\nlung cancer considered success smoking baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{49/100}{1/900} = 441\\\\\n&=& \\frac{49/51}{1/899} = 863.75\\\\\n\\end{eqnarray*}\\]lung cancer considered success smoking baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{49/100}{1/900} = 441\\\\\n&=& \\frac{49/51}{1/899} = 863.75\\\\\n\\end{eqnarray*}\\]light smoking considered success healthy baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{49/50}{51/950} = 18.25\\\\\n&=& \\frac{49/1}{51/899} = 863.75\\\\\n\\end{eqnarray*}\\]light smoking considered success healthy baseline:\n\\[\\begin{eqnarray*}\nRR &=& \\frac{49/50}{51/950} = 18.25\\\\\n&=& \\frac{49/1}{51/899} = 863.75\\\\\n\\end{eqnarray*}\\]matter variable choose explanatory versus response! Though, general, baseline odds baseline risk (can’t know case-control study) still number can provide lot information study.IMPORTANT: Relative risk used case-control studies odds ratios can used!","code":""},{"path":"inference-for-categorical-data.html","id":"inference-on-odds-ratios","chapter":"5 Inference for categorical data","heading":"5.4.2 Inference on Odds Ratios","text":"Due theory won’t cover, fairly good mathematical approximation describes natural log odds ratio varies sample sample:\\[\\ln(\\hat{}) \\stackrel{\\mbox{approx}}{\\sim}  N\\Bigg(\\ln(), \\sqrt{\\frac{1}{} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\Bigg)\\]Statistic: \\[\\hat{} = \\frac{D}{B C}\\]Null Hypothesis: \\[H_0: = 1\\]CI: CI true odds ratio population, \\(\\)\\[\\mbox{exponentiate} \\Bigg[ \\ln{\\hat{}} \\pm z^* \\sqrt{ \\frac{1}{} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\Bigg]\\]","code":""},{"path":"inference-for-categorical-data.html","id":"or-is-more-extreme-than-rr","chapter":"5 Inference for categorical data","heading":"5.4.2.1 OR is more extreme than RR","text":"Without loss generality, assume true \\(RR > 1\\), implying \\(p_1 / p_2 > 1\\) \\(p_1 > p_2\\).Note following sequence consequences:\\[\\begin{eqnarray*}\nRR = \\frac{p_1}{p_2} &>& 1\\\\\n\\frac{1 - p_1}{1 - p_2} &<& 1\\\\\n\\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& 1\\\\\n\\frac{p_1}{p_2} \\cdot \\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& \\frac{p_1}{p_2}\\\\\n&>& RR\n\\end{eqnarray*}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"confidence-interval-for-or-same-idea-as-with-rr","chapter":"5 Inference for categorical data","heading":"5.4.3 Confidence Interval for OR (same idea as with RR)","text":"\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{})) &\\approx& \\sqrt{ \\frac{1}{} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\n\\end{eqnarray*}\\], \\((1-\\alpha)100\\%\\) CI \\(\\ln()\\) :\n\\[\\begin{eqnarray*}\n\\ln(\\hat{}) \\pm z_{1-\\alpha/2} SE(\\ln(\\hat{}))\n\\end{eqnarray*}\\]gives \\((1-\\alpha)100\\%\\) CI \\(\\):\n\\[\\begin{eqnarray*}\n(e^{\\ln() - z_{1-\\alpha/2} SE(\\ln())}, e^{\\ln() + z_{1-\\alpha/2} SE(\\ln())})\n\\end{eqnarray*}\\]\\(\\frac{583/576}{22/204} = 9.39\\)\nBack example… = 9.39.\n\\[\\begin{eqnarray*}\nSE(\\ln(\\hat{})) &=& \\sqrt{\\frac{1}{583} + \\frac{1}{576} + \\frac{1}{22} + \\frac{1}{204}}\\\\\n&=& 0.232\\\\\n90\\% \\mbox{ CI } \\ln() && \\ln(9.39) \\pm 1.645 \\cdot 0.232\\\\\n&& 2.24 \\pm 1.645 \\cdot 0.232\\\\\n&& (1.858, 2.62)\\\\\n90\\% \\mbox{ CI } && (e^{1.858}, e^{2.62})\\\\\n&& (6.41, 13.75)\\\\\n\\end{eqnarray*}\\]90% confident true \\(\\ln()\\) 1.858 2.62. 90% confident true \\(\\) 6.41 13.75. , true odds getting lung cancer smoke heavily somewhere 6.41 13.75 times higher don’t, 90% confidence.Note 1: use theory allows us understand sampling distribution \\(\\ln(\\hat{}).\\) use process creating CIs transform back \\(\\).Note 2: good general guidelines checking whether sample sizes large enough normal approximation. authorities agree one can get away smaller sample sizes differences two proportions. sample sizes pass rough check discussed \\(\\chi^2\\), large enough support inferences based approximate normality log estimated odds ratio, . (Ramsey Schafer 2012, 541)one author, normal approximation hold, need expected counts cell least 5. (Pagano Gauvreau 2000, 355)Note 3: cells zero, many people add 0.5 cell’s observed value.Note 4: always extreme RR (one reason careful…)Note 5: \\(RR \\approx \\) RR small (denominator similar denominator RR).","code":"\n(SE_lnOR = sqrt( 1/583 + 1/576 + 1/22 + 1/204))## [1] 0.2319653\nxqnorm(0.95, 0, 1, plot=FALSE)## [1] 1.644854\nlog(9.39) - 1.645*0.232## [1] 1.858005\nlog(9.39) + 1.645*0.232## [1] 2.621285\nexp(log(9.39) - 1.645*0.232)## [1] 6.410936\nexp(log(9.39) + 1.645*0.232)## [1] 13.75339"},{"path":"inference-for-categorical-data.html","id":"using-infer-for-inference-on-or","chapter":"5 Inference for categorical data","heading":"5.4.4 Using infer for inference on OR","text":"difference proportions, infer syntax can used simulate sampling distribution sample odds ratio null hypothesis population proportions identical.NOTE order provide syntax comparable correct RR , smoking specified response variable, lungs specified explanatory variable.","code":"\nlibrary(infer)\nWynderGraham <- data.frame(lungs = c(rep(\"cancer\", 605), rep(\"healthy\", 780)),\n                            smoking = c(rep(\"light\", 22), rep(\"heavy\", 583),\n                                        rep(\"light\", 204), rep(\"heavy\", 576)))\n\n(obs_OR <- WynderGraham %>%\n  specify(smoking ~ lungs, success = \"heavy\") %>%\n  calculate(stat = \"odds ratio\", order = c(\"cancer\", \"healthy\")))## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  9.39\nnull_OR <- WynderGraham %>%\n  specify(smoking ~ lungs, success = \"heavy\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"odds ratio\", order= c(\"cancer\", \"healthy\"))\n\nnull_OR %>%\n  visualize() +\n  shade_p_value(obs_stat = obs_OR, direction = \"right\")"},{"path":"inference-for-categorical-data.html","id":"ex:cov","chapter":"5 Inference for categorical data","heading":"5.4.5 Example: MERS-CoV","text":"following study case-control study, impossible estimate proportion cases population. However, notice authors don’t try . flip explanatory response variables case status predicting clinical variables. setting, authors able present relative risk estimates, still chose provide odds ratios (possibly odds ratios somewhat standard medical literature).Middle East Respiratory Syndrome Coronavirus: Case-Control Study Hospitalized Patients15Background. paucity data regarding differentiating characteristics patients laboratory-confirmed negative Middle East respiratory syndrome coronavirus (MERS-CoV).Methods. hospital-based case-control study comparing MERS-CoV–positive patients (cases) MERS-CoV–negative controls.Results. total 17 case patients 82 controls mean age 60.7 years 57 years, respectively (P = .553), included. statistical differences observed relation sex, presence fever cough, presence single multilobar infiltrate chest radiography. case patients likely overweight control group (mean body mass index, 32 vs 27.8; P = .035), diabetes mellitus (87% vs 47%; odds ratio [], 7.24; P = .015), end-stage renal disease (33% vs 7%; , 7; P = .012). time admission, tachypnea (27% vs 60%; , 0.24; P = .031) respiratory distress (15% vs 51%; , 0.15; P = .012) less frequent among case patients. MERS-CoV patients likely normal white blood cell count control group (82% vs 52%; , 4.33; P = .029). Admission chest radiography interstitial infiltrates frequent case patients controls (67% vs 20%; , 8.13; P = .001). Case patients likely admitted intensive care unit (53% vs 20%; , 4.65; P = .025) high mortality rate (76% vs 15%; , 18.96; P < .001).Conclusions. clinical predictors enhance ability predict patients pneumonia MERS-CoV. However, prospective analysis matched case-control studies may shed light predictors infection.Consider results diabetes. 17 cases, 13 diabetes; 82 controls, 35 diabetes. data can summarized follows:","code":"\nMERSCoV <- data.frame(coronov = c(rep(\"case\", 17), rep(\"control\", 82)),\n                      diab = c(rep(\"hasdiab\", 13), rep(\"nodiab\", 4), \n                               rep(\"hasdiab\", 35), rep(\"nodiab\", 47)))\ntable(MERSCoV)##          diab\n## coronov   hasdiab nodiab\n##   case         13      4\n##   control      35     47"},{"path":"inference-for-categorical-data.html","id":"ci-for-95-or","chapter":"5 Inference for categorical data","heading":"CI for 95% OR","text":"calculations , can find CI true diabetes MRES-CoV without.95% confident true odds diabetes 1.31 times 14.5 times higher CoV without. Note results calculated match results paper.Working backwards percentages, 13 87% cases, 15 cases. 35 47% controls, 74 controls. Using revised numbers, odds ratio \\(\\hat{}\\) = (13/2)/(35/39) = 7.24, CI (1.53, 34.37).\nFigure 5.1: Al-Tawfig et al. Middle East Respiratory Syndrome Coronavirus: Case-Control Study Hospitalized Patients\n","code":"\n(ORhat = (13/4)/(35/47))## [1] 4.364286\n(SE_lnOR = sqrt( 1/13 + 1/4 + 1/35 + 1/47))## [1] 0.6138168\nxqnorm(0.975, 0, 1, plot=FALSE)## [1] 1.959964\nlog(ORhat) - 1.96 * SE_lnOR## [1] 0.2703735\nlog(ORhat) + 1.96 * SE_lnOR## [1] 2.676536\nexp(log(ORhat) - 1.96 * SE_lnOR)## [1] 1.310454\nexp(log(ORhat) + 1.96 * SE_lnOR)## [1] 14.53465\n(ORhat = (13/2)/(35/39))## [1] 7.242857\n(SE_lnOR = sqrt( 1/13 + 1/2 + 1/35 + 1/39))## [1] 0.7944404\nexp(log(ORhat) - 1.96 * SE_lnOR)## [1] 1.526401\nexp(log(ORhat) + 1.96 * SE_lnOR)## [1] 34.36776"},{"path":"inference-for-categorical-data.html","id":"diffprop","chapter":"5 Inference for categorical data","heading":"5.5 Difference of two proportions","text":"","code":""},{"path":"inference-for-categorical-data.html","id":"clt-for-difference-in-two-proportions","chapter":"5 Inference for categorical data","heading":"5.5.1 CLT for difference in two proportions","text":", apply mathematical model (.e., normal distribution) derived central limit theorem investigate properties statistic interest. , statistic interest difference two sample proportions: \\(\\hat{p}_1 - \\hat{p}_2\\). CLT describes \\(\\hat{p}_1 - \\hat{p}_2\\) varies many random samples taken population.single sample proportion, normal distribution good fit certain technical conditions:Independence data independent within two groups. Generally satisfied data come two independent random samples data come randomized experiment. However, may times independence condition seems reasonable even precisely met.Independence data independent within two groups. Generally satisfied data come two independent random samples data come randomized experiment. However, may times independence condition seems reasonable even precisely met.Success-failure condition (.e., large enough sample sized). need least 10 successes 10 failures (expected) group. authors suggest 5 group sufficient.Success-failure condition (.e., large enough sample sized). need least 10 successes 10 failures (expected) group. authors suggest 5 group sufficient.","code":""},{"path":"inference-for-categorical-data.html","id":"the-central-limit-theorem-for-hatp_1---hatp_2","chapter":"5 Inference for categorical data","heading":"The Central Limit Theorem for \\(\\hat{p}_1 - \\hat{p}_2\\):","text":"central limit theorem describes \\(\\hat{p}_1 - \\hat{p}_2\\) varies many random samples taken population.\\[\\hat{p}_1 - \\hat{p}_2 \\sim N\\Bigg(p_1 - p_2, \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\Bigg)\\]","code":""},{"path":"inference-for-categorical-data.html","id":"ht-difference-in-proportions","chapter":"5 Inference for categorical data","heading":"5.5.2 HT: difference in proportions","text":"Note equation describing central limit theorem formula variability \\(\\hat{p}_1 - \\hat{p}_2\\). ,\\[SE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\]However, testing particular hypothesis, research question (usually) provide values \\(p_1\\) \\(p_2\\) use formula SE. Instead, research question usually one independence, , knowing level explanatory (group) variable tells nothing probability response variable. Indeed, typically null hypothesis written :\\[H_0: p_1 = p_2\\]alternative hypothesis incorporating direction research claim.order calculate p-value, sampling distribution \\(\\hat{p}_1 - \\hat{p}_2\\) \\(H_0\\) needed. CLT start understanding distribution \\(\\hat{p}_1 - \\hat{p}_2\\), additional step incorporates null hypothesis \\(p_1 = p_2\\) implemented SE.\n\\(H_0: p_1 = p_2\\) true, best guess true value either \\(p_1\\) \\(p_2\\) :\\[\\hat{p}_{pooled} = \\frac{\\mbox{number successed}}{\\mbox{number observations}} = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"two-proportion-z-test","chapter":"5 Inference for categorical data","heading":"Two proportion z-test","text":"perform hypothesis test using normal distribution (.e., central limit theorem) use z-score test statistic xpnorm find p-value.\\[\\mbox{Z score} = \\frac{(\\hat{p}_1 - \\hat{p}_2) - 0}{\\sqrt{\\frac{\\hat{p}_{pooled}(1-\\hat{p}_{pooled})}{n_1} + \\frac{\\hat{p}_{pooled}(1-\\hat{p}_{pooled})}{n_2}}}\\]\\[\\mbox{p-value} = \\mbox{probability Z score extreme using N(0,1) probability}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"ci-difference-in-proportions","chapter":"5 Inference for categorical data","heading":"5.5.3 CI: difference in proportions","text":"creating confidence interval true parameter interest, underlying research assumption values \\(p_1\\) \\(p_2\\). best can calculate SE use sample values.population parameter: \\(p_1 - p_2\\): true difference success proportion (probability) groups 1 2.CI \\(p_1 - p_2\\):\\[(\\hat{p}_1 - \\hat{p}_2) \\pm Z^* \\sqrt{\\frac{\\hat{p}_1 (1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2 (1-\\hat{p}_2)}{n_2}}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"example-government-shutdown","chapter":"5 Inference for categorical data","heading":"5.5.4 Example: Government Shutdown16","text":"United States federal government shutdown 2018-2019 occurred December 22, 2018 January 25, 2019, span 35 days. Survey USA poll 608 randomly sampled Americans time period reported 48% (77 160 people) make less $40,000 per year 55% (247 448 people) make $40,000 per year said government shutdown affected personally.Notice observational units selected entire population: using response explanatory variable. (type study called cross-classification study.) beauty selected entire population good sense proportions group well proportion people shutdown affected .Notice observational units selected entire population: using response explanatory variable. (type study called cross-classification study.) beauty selected entire population good sense proportions group well proportion people shutdown affected .Test research claim proportion people affected shutdown different comparing make $40,000 less $40,000 per year.Test research claim proportion people affected shutdown different comparing make $40,000 less $40,000 per year.p-value test 0.128 indicating evidence difference proportion people affected shutdown across two income groups. NOTE claim “difference!” claim “evidence difference.” Try explain (classmate) difference two claims.\\[\\mbox{p-value} = 2* P( Z \\leq -1.522) = 0.128\\]95% confidence interval (\\(p_{<40K}- p_{ \\geq40K}\\)) ), p proportion said government shutdown affected personally, (-0.16, 0.02).Determine following statements true false, explain reasoning identify statement false.17At 5% significance level, data provide convincing evidence real difference proportion affected personally Americans make less $40,000 annually Americans make $40,000 annually.5% significance level, data provide convincing evidence real difference proportion affected personally Americans make less $40,000 annually Americans make $40,000 annually.95% confident 16% 2% fewer Americans make less $40,000 per year personally affected government shutdown compared make $40,000 per year.95% confident 16% 2% fewer Americans make less $40,000 per year personally affected government shutdown compared make $40,000 per year.90% confidence interval (\\(p_{<40K}- p_{ \\geq40K}\\)) wider (-0.16, 0.02) interval.90% confidence interval (\\(p_{<40K}- p_{ \\geq40K}\\)) wider (-0.16, 0.02) interval.95% confidence interval (\\(p_{ \\geq40K} - p_{<40K}\\)) (-0.02, 0.16).95% confidence interval (\\(p_{ \\geq40K} - p_{<40K}\\)) (-0.02, 0.16).","code":"\n(p_pool <- (247+77)/ 614)## [1] 0.5276873\n(z_score <- (0.48 - 0.55) / sqrt(p_pool*(1-p_pool) *(1/160 + 1/448)))## [1] -1.522447\n2*xpnorm(z_score,0,1)## [1] 0.1278972\n(z_star95 <- xqnorm(0.975, 0, 1))## [1] 1.959964\n(0.48 - 0.55) - z_star95*sqrt(0.48*(1-0.48)/160 + 0.55*(1-0.55)/448)## [1] -0.1600828\n(0.48 - 0.55) + z_star95*sqrt(0.48*(1-0.48)/160 + 0.55*(1-0.55)/448)## [1] 0.0200828"},{"path":"inference-for-categorical-data.html","id":"chisq1","chapter":"5 Inference for categorical data","heading":"5.6 Goodness-of-fit: One categorical variable (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels","text":"Consider \\(E_k\\) number expected \\(k^{th}\\) category.testing null hypothesis pre-specified set proportions (probabilities) across \\(K\\) categories, test statistics :\\[X^2 = \\sum_{k=1}^K \\frac{(O_k - E_k)^2}{E_k} \\sim \\chi^2_{K-1}\\]null sampling distribution well-described chi-square distribution \\(K-1\\) degrees freedom … :case contributes count table independent cases table.particular scenario (.e. cell count) least 5 expected cases. (sample size criterion)conditions don’t hold, test statistic won’t predicted distribution, p-value calculations meaningless.","code":""},{"path":"inference-for-categorical-data.html","id":"example-household-ages","chapter":"5 Inference for categorical data","heading":"5.6.1 Example: Household Ages18","text":"Suppose class picnic, people everyone’s household showed . ages representative ages Americans? Probably . , random sample! unrepresentative ages?2010 Census estimates19 percent people following age categories.","code":""},{"path":"inference-for-categorical-data.html","id":"is-the-age-distribution-of-the-people-from-households-in-our-class-typical-of-that-of-all-residents-of-the-us","chapter":"5 Inference for categorical data","heading":"Is the age distribution of the people from households in our class typical of that of all residents of the US?","text":"Let’s collect data. Note never expect last two columns exact values, even class perfect random sample. (?)Somehow need measure closely observed data match expected values. chi-square statistic (\\(\\chi^2\\)):\\[\\chi^2 = \\sum_{k=1}^K \\frac{(O_k - E_k)^2}{E_k}\\]Let’s use data collected class calculate observed \\(\\chi^2\\) test statistic. big enough indicate individuals class’s households don’t follow 2010 Census proportions? know? need null hypothesis!\\(H_0: p_1 = 0.2403, p_2 = 0.3653, p_3 = 0.3943\\)\\(H_A: \\mbox{ } H_0\\)null hypothesis specified 2010 Census. alternative hypothesis deviation claim.observed test statistic :\\[\\begin{eqnarray*}\nX^2 &=& \\frac{(5 - 12.015)^2}{12.015} + \\frac{(22 - 18.265)^2}{18.27} + \\frac{(23-19.715)^2}{19.715}\\\\\n&=& 5.41\n\\end{eqnarray*}\\]know value observed test statistic “large enough” ? need distribution test statistic assuming null hypothesis true. Let’s generate \\[\\begin{eqnarray*}\nX^2 &=& \\frac{(13 - 12.015)^2}{12.015} + \\frac{(18 - 18.265)^2}{18.27} + \\frac{(19-19.715)^2}{19.715}\\\\\n&=& 0.1105\n\\end{eqnarray*}\\]class, used random numbers (pieces paper) generate null sampling distribution \\(X^2\\). turns , also mathematical model describes variability \\(X^2\\): chi-square distribution \\(K-1\\) degrees freedom. p-value says can’t reject \\(H_0\\), don’t know household ages come distribution census percentages. (clear: conclusion know nothing. don’t evidence reject \\(H_0\\). also doesn’t mean know \\(H_0\\) true. Unfortunately, can’t conclude anything.)","code":"\n1 - xpchisq(5.41, 2)## [1] 0.06687032"},{"path":"inference-for-categorical-data.html","id":"example-flax-seed","chapter":"5 Inference for categorical data","heading":"5.6.2 Example: Flax Seed","text":"Researchers studied mutant type flax seed hoped produce oil use margarine shortening. amount palmitic acid flax seed important factor research; related factor whether seed brown variegated. seeds classified six combinations palmitic acid color. According hypothesized genetic model, six combinations occur 3:6:3:1:2:1 ratio.\\[\\begin{eqnarray*}\nH_0: && p_1 = 3/16, p_2=6/16, p_3 = 3/16, p_4 = 1/16, p_5=2/16, p_6 = 1/16\\\\\nH_A: && \\mbox{ distribution } H_0\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\chi^2 &=& \\frac{(15-13.5)^2}{13.5} + \\frac{(26-27)^2}{27} + \\frac{(15-13.5)^2}{13.5} + \\frac{(0-4.5)^2}{4.5} + \\frac{(8-9)^2}{9} + \\frac{(8-4.5)^2}{4.5}\\\\\n&=& 7.71\\\\\n\\mbox{p-value} &=& P(\\chi^2_5 \\geq 7.71)\\\\\n&=& 0.173\\\\\n\\end{eqnarray*}\\]","code":"\n1 - xpchisq(7.71, 5)## [1] 0.172959"},{"path":"inference-for-categorical-data.html","id":"how-could-we-simulate-power","chapter":"5 Inference for categorical data","heading":"How could we simulate power?","text":"Consider flax seed example, household ages example, use random digits.Come alternative hypothesis specified probabilities type seed.Allocate digits appropriately given alternative model.Randomly generate 72 random digits (00 99) collect observed data based alternative model.Calculate test statistic randomly generated observed data (compared expected counts \\(H_0\\)), indicate whether 11.07 (see \\(\\chi^2_5\\) cutoff).Repeat 3 & 4 many many times. power estimated proportion times reject null hypothesis alternative true.","code":"\nxqchisq(.95, 5)## [1] 11.0705"},{"path":"inference-for-categorical-data.html","id":"chisq2","chapter":"5 Inference for categorical data","heading":"5.7 Independence: Two categorical variables (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels each","text":"working binary variables, research questions two variables. main question now whether association two categorical variables interest.\\(H_0\\):two variables independent\\(H_A\\): two variables independentHow know test statistic big number ? Well, turns test statistic approximate \\(\\chi^2\\) distribution degrees freedom = \\((r- 1)\\cdot (c-1)\\) \\(H_0\\) true. long :random sample population.expect least 1 observation every cell (\\(E_i \\geq 1 \\forall \\))expect least 5 observations 80% cells (\\(E_i \\geq 5\\) 80% \\(\\))\\[X^2 = \\sum_{\\mbox{cells}} \\frac{(Obs - Exp)^2}{Exp} \\sim \\chi^2_{(r-1)(c-1)}\\]Consider following (silly?) example data CA vs. notCA soda preference:number people group category, wanted absolutely association two variables soda preference location:distribution Coke Pepsi preference CA vs CA, many Californians prefer Coke? 60!\\[\\mbox{# CA prefer Coke} = 90 \\cdot \\frac{80}{120} = 60\\]\nrest table can filled similar manner:Coke & Pepsi example motivates idea many observations expect see cell association variables. Note expected number almost always decimal value.\\[\\mbox{Exp} = \\frac{(\\mbox{row total})(\\mbox{col total})}{\\mbox{table total}}\\]","code":""},{"path":"inference-for-categorical-data.html","id":"example-nightlights","chapter":"5 Inference for categorical data","heading":"5.7.1 Example: Nightlights20","text":"Myopia, near-sightedness, typically develops childhood years. Recent studies explored whether association development myopia use night-lights infants. Quinn, Shin, Maguire, Stone (1999) examined type light children aged 2-16 exposed . January June 1998, parents 479 children seen outpatients university pediatric ophthalmology clinic completed questionnaire (children already developed serious eye conditions excluded). One questions asked “lighting condition /child sleep night?” age 2 years. following two-way table classifies children’s eye condition whether slept kind light (e.g., night light full room light) darkness.data given following R code:\\(H_0\\): association lighting condition eye condition\\(H_A\\): association lighting condition eye conditionWhat observational units?observational units?explanatory response variables?explanatory response variables?Let’s say conclude association (reject \\(H_0\\)). Can also conclude lighting causes particular eye conditions?Let’s say conclude association (reject \\(H_0\\)). Can also conclude lighting causes particular eye conditions?Try come many confounding variables possible.Try come many confounding variables possible.chi-square test can applied table counts. test statistic 56.513 small p-value. Note observed expected tables can pulled chisq.test() output.conclusion Inv 5.3 Chance Rossman (2018) excellent:segmented bar graph reveals children sample incidence near-sightedness increases level lighting increases. random sample two categorical variables, can perform chi-square test association. expected counts large (smallest 14.25 > 5), can apply chi-square test data. p-value chi-square test essentially zero, says association eye condition lighting population, ’s virtually impossible chance alone produce table conditional distributions differ much actual study. Thus, sample data provide overwhelming evidence indeed association eye condition \nlighting population children like study. closer analysis table chi-square calculation reveals many fewer children near-sightedness expected “darkness” group many children near-sightedness expected “room light” group. remember, draw cause--effect conclusion lighting eye condition observational study. Several confounding variables explain observed association. example, perhaps near-sighted children tend near-sighted parents prefer leave light vision difficulties, also passing genetic predisposition children. also careful generalizing sample larger population children making voluntary visits eye doctor selected random larger population.","code":"\nlights <- data.frame(eyesight = c(rep(\"far\", 40), rep(\"neither\", 114), rep(\"near\", 18),\n                           rep(\"far\", 39), rep(\"neither\", 115), rep(\"near\", 78),\n                           rep(\"far\", 12), rep(\"neither\", 22), rep(\"near\", 41)),\n                     lighting = c(rep(\"dark\", 172), rep(\"nightlight\", 232), rep(\"roomlight\", 75)))\n\ntable(lights)##          lighting\n## eyesight  dark nightlight roomlight\n##   far       40         39        12\n##   near      18         78        41\n##   neither  114        115        22\nlights %>%\n  ggplot() +\n  geom_bar(aes(x = lighting, fill = eyesight), position = \"fill\")\n(chi.lights <- lights %>%\n  select(eyesight, lighting) %>%\n  table() %>%\n  chisq.test())## \n##  Pearson's Chi-squared test\n## \n## data:  .\n## X-squared = 56.513, df = 4, p-value = 1.565e-11\nchi.lights$observed##          lighting\n## eyesight  dark nightlight roomlight\n##   far       40         39        12\n##   near      18         78        41\n##   neither  114        115        22\nchi.lights$expected##          lighting\n## eyesight      dark nightlight roomlight\n##   far     32.67641   44.07516  14.24843\n##   near    49.19415   66.35491  21.45094\n##   neither 90.12944  121.56994  39.30063"},{"path":"inference-for-categorical-data.html","id":"reflection-questions-3","chapter":"5 Inference for categorical data","heading":"5.8 Reflection Questions","text":"","code":""},{"path":"inference-for-categorical-data.html","id":"no-ims-relative-risk-odds-ratios","chapter":"5 Inference for categorical data","heading":"5.8.1 (no IMS) Relative Risk & Odds Ratios","text":"differences cross-classification, cohort, case-control studies?appropriate calculate differences ratios proportions? isn’t appropriate?odds calculated? calculated?can’t calculate statistics based proportions? ``fix\" work?statistic interest? parameter interest?look natural log RR natural log finding confidence intervals respective parameters?calculate SE \\(\\ln(\\hat{RR})\\) \\(\\ln(\\hat{})\\)?CI \\(\\ln(RR)\\) \\(\\ln()\\), ? process work?","code":""},{"path":"inference-for-categorical-data.html","id":"binary-variables-ims-section-6.2","chapter":"5 Inference for categorical data","heading":"5.8.2 2 binary variables: IMS Section 6.2","text":"statistic interest? parameter interest?inference change now binary (response) data taken two populations?inference stay now binary (response) data taken two populations?Central Limit Theorem say two sample proportions?appropriate apply hypothesis test data? appropriate apply confidence interval data?calculate SE(\\(\\hat{p}_1 - \\hat{p}_2\\))?technical conditions must hold Central Limit Theorem apply?","code":""},{"path":"inference-for-categorical-data.html","id":"categorical-variables-ims-section-6.3","chapter":"5 Inference for categorical data","heading":"5.8.3 2 categorical variables: IMS Section 6.3","text":"describe data seen \\(r \\times c\\) tables?Describe simulation mechanism creates sampling distribution assumption null hypothesis true (like cards first week class using gender discrimination example).test statistic (infer simulation chi-square test mathematical model!!)? need complicated test statistic didn’t need one \\(2 \\times 2\\) tables?compute expected count? intuition behind computation?one benefit two sample z-test proportions ? , one thing can \\(2\\times 2\\) table instead \\(r \\times c\\) table?Describe directionality test statistic. , values \\(X^2\\) make reject \\(H_0\\)?technical assumptions chi-square test? need technical assumptions?null alternative hypotheses?","code":""},{"path":"inference-for-categorical-data.html","id":"no-ims-binomial-probabilities-not-covered","chapter":"5 Inference for categorical data","heading":"5.8.4 (no IMS) Binomial probabilities (not covered)","text":"can binomial distribution used calculate probabilities?technical conditions binomial distribution?normal distribution different binomial distribution? (one answer normal describes continuous variable binomial describes discrete variable. mean? another distinction?)technical conditions allowing normal distribution approximate binomial distribution?one reason choose use normal distribution?one reason choose use binomial distribution?","code":""},{"path":"interlude-in-spring-2020.html","id":"interlude-in-spring-2020","chapter":"6 Interlude in Spring 2020","heading":"6 Interlude in Spring 2020","text":"","code":""},{"path":"interlude-in-spring-2020.html","id":"census","chapter":"6 Interlude in Spring 2020","heading":"6.1 Census","text":"recently filled census form. full list questions asked:addressname (residents address); including many people live addressthe ownership status residence livemy sex (binary options provided, asked residents)birthday (asked residents)Hispanic, Latino, Spanish origin (asked residents)race (ethnicity options, asked residents)relationship status residents . flexibility given marital status, family status, etc. flexibility sex (e.g., choose whether spouse married , required choose whether sex opposite sex )","code":""},{"path":"interlude-in-spring-2020.html","id":"what-about-college-students","chapter":"6 Interlude in Spring 2020","heading":"6.1.1 What about College students?","text":"Census provides great information website (particularly respect changes due COVID-19).https://2020census.gov/en/--count.htmlhttps://2020census.gov/en/news-events/press-releases/modifying-2020-operations.htmlCollege students live away home counted - -campus residence live sleep time, even home April 1, 2020. live housing designed college students (dorms apartments “--bed” leases), counted part Group Quarters Operation. live campus housing designed college students (private house apartment), count address.recent 2020 Census Group Quarters Advance Contact operation contacted college/university student housing administrators get input enumeration methods allow students participate 2020 Census. majority, 47 percent, chosen eResponse methodology 7 percent chose paper listings… 35 percent, however, chose drop-/pick-allows students self-respond using Individual Census Questionnaire (ICQ). contacting schools ask whether like change preference light emerging situation.general, students colleges universities temporarily closed due COVID-19 virus still counted part process. Even home census day, April 1, counted according residence criteria states counted live sleep time.","code":""},{"path":"interlude-in-spring-2020.html","id":"covid19","chapter":"6 Interlude in Spring 2020","heading":"6.2 COVID-19","text":"Thursday’s optional class meeting COVID-19 connects ideas introductory statistics.good starting place solid information COVID-19:\nhttps://www.sciencemuseumgroup.org.uk/coronavirus-science---know--dont-know---virus/","code":""},{"path":"interlude-in-spring-2020.html","id":"dashboard-for-predicting","chapter":"6 Interlude in Spring 2020","heading":"6.2.1 Dashboard for Predicting","text":"Statistician Create COVID-19 Dashboard Predict Infection: https://magazine.amstat.org/blog/2020/06/01/dashboard--predict-infection/Statistician Create COVID-19 Dashboard Predict Infection: https://magazine.amstat.org/blog/2020/06/01/dashboard--predict-infection/Dashboard : https://covid19.stat.iastate.edu/Dashboard : https://covid19.stat.iastate.edu/","code":""},{"path":"interlude-in-spring-2020.html","id":"visualizing-the-data","chapter":"6 Interlude in Spring 2020","heading":"6.2.2 Visualizing the data","text":"Information Beautiful put together series visualizations considereing pandemic many different angles: https://informationisbeautiful.net/visualizations/covid-19-coronavirus-infographic-datapack/find dashboards among best following pandemic. Good display, constantly updated, reliable data.Coronavirus US: Latest Map Case Count, NY Times: https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html?action=click&module=Top%20Stories&pgtype=HomepageCoronavirus US: Latest Map Case Count, NY Times: https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html?action=click&module=Top%20Stories&pgtype=HomepageCoronavirus COVID-19 Global Cases Center Systems Science Engineering (CSSE) Johns Hopkins University (JHU): https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6Coronavirus COVID-19 Global Cases Center Systems Science Engineering (CSSE) Johns Hopkins University (JHU): https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6COVID-19 Global Pandemic Real-time Report: https://ncov.dxy.cn/ncovh5/view/en_pneumonia?link=&share=&source=COVID-19 Global Pandemic Real-time Report: https://ncov.dxy.cn/ncovh5/view/en_pneumonia?link=&share=&source=COVIDcast: Real-time COVID-19 Indicators (including doctor visits, symptoms, Google search trends): https://covidcast.cmu.edu/COVIDcast: Real-time COVID-19 Indicators (including doctor visits, symptoms, Google search trends): https://covidcast.cmu.edu/said, also worth thinking visualize data responsibly.Visualizing COVID-19 data (responsibly): https://medium.com/nightingale/ten-considerations---create-another-chart--covid-19-27d3bd691be8","code":""},{"path":"interlude-in-spring-2020.html","id":"current-medical-studies-on-treatment-of-covid-19","chapter":"6 Interlude in Spring 2020","heading":"6.2.3 Current medical studies on treatment of COVID-19","text":"French study investigating azithromycin – chloroquine COVID-19.https://www.sciencedirect.com/science/article/pii/S0924857920300996French Confirmed COVID-19 patients included single arm protocol early March March 16th, receive 600mg hydroxychloroquine daily viral load nasopharyngeal swabs tested daily hospital setting. Depending clinical presentation, azithromycin added treatment. Untreated patients another center cases refusing protocol included negative controls. Presence absence virus Day6-post inclusion considered end point.Assuming 50% efficacy hydroxychloroquine reducing viral load day 7, 85% power, type error rate 5% 10% loss follow-, calculated total 48 COVID-19 patients (.e., 24 cases hydroxychloroquine group 24 control group) required analysis (Fleiss CC). Statistical differences evaluated Pearson’s chi-square Fisher’s exact tests categorical variables, appropriate. Means quantitative data compared using Student’s t-test.\nFigure 2.1: Gautret et al. Hydroxychloroquine azithromycin treatment COVID-19: results open-label non-randomized clinical trial\nlate March, launches global megatrial four promising coronavirus treatments (antibiotics). study, include many thousands patients dozens countries, designed simple possible even hospitals overwhelmed onslaught COVID-19 patients can participate.https://www.sciencemag.org/news/2020/03/-launches-global-megatrial-four--promising-coronavirus-treatments#Enrolling subjects SOLIDARITY easy. person confirmed case COVID-19 deemed eligible, physician can enter patient’s data website, including underlying condition change course disease, diabetes HIV infection. participant sign informed consent form scanned sent electronically. physician states drugs available hospital, website randomize patient one drugs available local standard care COVID-19.“, measurements documentation required,” says Ana Maria Henao Restrepo, medical officer ’s Department Immunization Vaccines Biologicals. Physicians record day patient left hospital died, duration hospital stay, whether patient required oxygen ventilation, says. “’s .”design double-blind, gold standard medical research, placebo effects patients knowing received candidate drug. says balance scientific rigor speed.Teaching resources introducing COVID-19 class Laura Le, Kari Lock Morgan, Lucy D’Agostino McGowan: https://coronavirus-teaching-resources.netlify.app/","code":""},{"path":"interlude-in-spring-2020.html","id":"studies-related-to-covid-19","chapter":"6 Interlude in Spring 2020","heading":"6.2.4 Studies related to COVID-19","text":"Medical studies discussing side effects azithromycin – chloroquine treatment. https://threadreaderapp.com/thread/1242119303811514369.htmlMedical studies discussing side effects azithromycin – chloroquine treatment. https://threadreaderapp.com/thread/1242119303811514369.htmlStudies cloth masks vs medical masks healthcare workers. https://bmjopen.bmj.com/content/5/4/e006577Studies cloth masks vs medical masks healthcare workers. https://bmjopen.bmj.com/content/5/4/e006577Different blood types COVID-19Different blood types COVID-19https://www.medrxiv.org/content/10.1101/2020.03.11.20031096v1 (yet peer-reviewed)authors write “significantly higher risk” blood group , one reader pointed comments section, mean risk greatly higher; means p-value small. Notice CI odds ratio 1, comes close 1 centered 1.20 , say, 2.5.\n“Meta-analyses pooled data showed blood group significantly higher risk COVID-19 (odds ratio-, 1.20; 95% confidence interval-CI [1.02,1.43], P = 0.02) compared non-blood groups.”language might lead reader think “blood group much higher risk COVID-19” markedly untrue! , significance completely different concept compared “lot.”","code":""},{"path":"interlude-in-spring-2020.html","id":"being-careful-with-your-analysis","chapter":"6 Interlude in Spring 2020","heading":"6.2.5 Being careful with your analysis","text":"Late summer 2020, continues reliable data US. Understand COVID-19 Numbers ProPublica team walks hard sense disease point:\nCase counts won’t give full picture.\nDon’t want wrong? Wait beat.\nthings can’t know certain.\nTake deep breath try look big picture.\nFind trusted sources.\nnumbers show us today.\nBottom line: don’t pandemic control.\nCase counts won’t give full picture.Don’t want wrong? Wait beat.things can’t know certain.Take deep breath try look big picture.Find trusted sources.numbers show us today.Bottom line: don’t pandemic control.evidence racial disparities COVID-19 disease due biology. evidence points differences health care access socio-economic factors.\n“Racial Health Disparities Covid-19 — Caution Context” Chowkwanyun Reed, New England Journal Medicine, https://www.nejm.org/doi/full/10.1056/NEJMp2012910\n“Racial Health Disparities Covid-19 — Caution Context” Chowkwanyun Reed, New England Journal Medicine, https://www.nejm.org/doi/full/10.1056/NEJMp2012910CDC report: “Severe Outcomes Among Patients Coronavirus Disease 2019 (COVID-19) — United States, February 12–March 16, 2020” https://www.cdc.gov/mmwr/volumes/69/wr/mm6912e2.htmPay attention counts per group. 20-44 24 years. 65-74 9 years.\nFigure 2.2: CDC report: Severe Outcomes Among Patients Coronavirus Disease 2019 (COVID-19) — United States, February 12–March 16, 2020\ntalk 2% fatality rate, fatality incredibly difficult measure early disease.\nnumber reported confirmed cases COVID-19 continues slow , 2% fatality rate people quoting may appear rise two main factors: -reporting number cases delay symptoms first appearing death. possible errors cancel end correct wrong reasons!\nAlso, “fatality rate” incredibly misleading number varies much based age. Averaging ages give different numbers based age distribution country hand.\nmaybe just important & harder measure: fatality rate pandemic? pandemic cause deaths directly due coronavirus also due : cardiac arrest (emergency condition) without adequate space ERs; lack food / heat people unable work; lack access medical supplies / dialysis / pharmaceuticals; etc.\ntalk 2% fatality rate, fatality incredibly difficult measure early disease.number reported confirmed cases COVID-19 continues slow , 2% fatality rate people quoting may appear rise two main factors: -reporting number cases delay symptoms first appearing death. possible errors cancel end correct wrong reasons!Also, “fatality rate” incredibly misleading number varies much based age. Averaging ages give different numbers based age distribution country hand.maybe just important & harder measure: fatality rate pandemic? pandemic cause deaths directly due coronavirus also due : cardiac arrest (emergency condition) without adequate space ERs; lack food / heat people unable work; lack access medical supplies / dialysis / pharmaceuticals; etc.trade-putting cap disease resisting tracking personal data?\nCan smart thermometers help track coronavirus? (March 18, 2020) https://www.nytimes.com/2020/03/18/health/coronavirus-fever-thermometers.html\nFollow-: Restrictions Slowing Coronavirus Infections, New Data Suggests (March 30, 2020) https://www.nytimes.com/2020/03/30/health/coronavirus-restrictions-fevers.html?searchResultPosition=1\nSocial distancing scoreboard based movement mobile phones: https://www.unacast.com/covid19/social-distancing-scoreboard (info work: https://www.unacast.com/post/-unacast-social-distancing-scoreboard)\ntrade-putting cap disease resisting tracking personal data?Can smart thermometers help track coronavirus? (March 18, 2020) https://www.nytimes.com/2020/03/18/health/coronavirus-fever-thermometers.htmlCan smart thermometers help track coronavirus? (March 18, 2020) https://www.nytimes.com/2020/03/18/health/coronavirus-fever-thermometers.htmlFollow-: Restrictions Slowing Coronavirus Infections, New Data Suggests (March 30, 2020) https://www.nytimes.com/2020/03/30/health/coronavirus-restrictions-fevers.html?searchResultPosition=1Follow-: Restrictions Slowing Coronavirus Infections, New Data Suggests (March 30, 2020) https://www.nytimes.com/2020/03/30/health/coronavirus-restrictions-fevers.html?searchResultPosition=1Social distancing scoreboard based movement mobile phones: https://www.unacast.com/covid19/social-distancing-scoreboard (info work: https://www.unacast.com/post/-unacast-social-distancing-scoreboard)Social distancing scoreboard based movement mobile phones: https://www.unacast.com/covid19/social-distancing-scoreboard (info work: https://www.unacast.com/post/-unacast-social-distancing-scoreboard)","code":""},{"path":"inference-for-numerical-data.html","id":"inference-for-numerical-data","chapter":"7 Inference for numerical data","heading":"7 Inference for numerical data","text":"","code":""},{"path":"inference-for-numerical-data.html","id":"important-measures-related-to-quantitative-numeric-variables","chapter":"7 Inference for numerical data","heading":"7.1 Important measures related to quantitative (numeric) variables","text":"","code":""},{"path":"inference-for-numerical-data.html","id":"quantitative-descriptives","chapter":"7 Inference for numerical data","heading":"7.1.1 Quantitative Descriptives","text":"measures can look get first sense whether two groups different (let alone substantially different enough us conclude difference related population). might look called Five Number Summary.Five Number Summary: Min, Q1, Median, Q3, Max\nQ1 = median values median\nQ3 = median values median\nIQR = Interquartile range (measure spread/variability) = Q3 - Q1\n1.5 x IQR rule possible outliers: observation falls 1.5IQR outside Q1 Q3, flag observation possible outlier.\nQ1 = median values medianQ3 = median values medianIQR = Interquartile range (measure spread/variability) = Q3 - Q11.5 x IQR rule possible outliers: observation falls 1.5IQR outside Q1 Q3, flag observation possible outlier.Boxplot\nBox spans Q1 Q3\nLine box marks median (M)\nPerpendicular line extends box smallest largest observations within 1.5IQR Q1 Q3.\nDots observations outside 1.5IQR\nBox spans Q1 Q3Line box marks median (M)Perpendicular line extends box smallest largest observations within 1.5IQR Q1 Q3.Dots observations outside 1.5IQRSummaries often used variable bell-shaped distribution\\[\\begin{eqnarray*}\n\\mbox{sample mean} &=& \\overline{X} = \\frac{1}{n} \\sum_{=1}^n X_i\\\\\n\\mbox{sample standard deviation} &=& s = \\sqrt{\\frac{1}{n-1} \\sum_{=1}^n (X_i - \\overline{X})^2}\\\\\n\\mbox{sample variance} &=& s^2 = \\frac{1}{n-1} \\sum_{=1}^n (X_i - \\overline{X})^2\n\\end{eqnarray*}\\]Loosely, standard deviation size typical deviation mean data set. Note divide \\(n-1\\) instead \\(n\\) true deviation defined average observations true mean \\(\\mu\\), , fact, always closer \\(\\overline{X}\\) \\(\\mu\\).","code":""},{"path":"inference-for-numerical-data.html","id":"mean1dist","chapter":"7 Inference for numerical data","heading":"7.1.2 Sampling distribution of a sample mean","text":", Central Limit Theorem tells us averages normally distributed sample size large enough. , means:\n\\[\\overline{X} \\sim N(\\mu, \\sigma/\\sqrt{n})\\]\n\\(SD(\\overline{X}) = \\sigma/\\sqrt{n}\\) \\(SE(\\overline{X}) = s/\\sqrt{n}\\). \\(\\mu\\) center population observations sample data taken. \\(\\sigma\\) variability population observations sample data taken., won’t spend much time worried difference \\(SD(\\overline{X})\\) \\(SE(\\overline{X})\\). Generally, ’ll know / use \\(SE(\\overline{X}) = s/\\sqrt{n}\\). Typically, quantitative variables, “large enough” least 30 observations.Spend time clicking different datasets ICAM applet: http://www.rossmanchance.com/applets/OneSample.html?showBoth=1You notice:population (sample data) skewed, sampling distribution sample mean normal (bell-shaped) sample size large.larger sample size, less variable sampling distribution.sample size change distribution dataset (middle graph). middle graph always representation population graph (left side); although small sample sizes, middle graph somewhat sparse.actual data analysis, see middle graph. see population graph (left side) sampling distribution (right side).","code":""},{"path":"inference-for-numerical-data.html","id":"mean1inf","chapter":"7 Inference for numerical data","heading":"7.2 Inference for a single mean, \\(\\mu\\)","text":"","code":""},{"path":"inference-for-numerical-data.html","id":"mathematical-model-for-distribution-of-the-sample-mean","chapter":"7 Inference for numerical data","heading":"7.2.1 Mathematical model for distribution of the sample mean","text":"coming mathematical model appropriate section, important notice almost never know true variability data (.e., \\(\\sigma\\)). Instead, almost always estimate \\(\\sigma\\) using \\(s\\), sample standard deviation. turns estimate variability used denominator, sampling distribution becomes variability (longer tails). Recall tails distribution interested, don’t want get wrong!!\\(\\sigma\\) somehow known: \\[\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\]typical situation \\(\\sigma\\) estimated using \\(s\\): \\[\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{df = n-1}\\]","code":""},{"path":"inference-for-numerical-data.html","id":"hypothesis-testing-isrs-4.1","chapter":"7 Inference for numerical data","heading":"7.2.1.1 Hypothesis Testing (ISRS 4.1)","text":"\\(H_0: \\mu = \\mu_0\\) true, know : \\[\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{df = n-1}\\], can use \\(t_{df = n-1}\\) distribution find p-value test. Note, R use function xpt mosaic package.","code":""},{"path":"inference-for-numerical-data.html","id":"confidence-intervals-isrs-4.1.4","chapter":"7 Inference for numerical data","heading":"7.2.1.2 Confidence Intervals (ISRS 4.1.4)","text":"setting null hypothesis statement interval estimate needed, interval created exact way done proportions using: \\[\\overline{X} \\pm t_{n-1}^* \\cdot SE(\\overline{X})\\]thing : \\[\\overline{X} \\pm t_{n-1}^* \\cdot s/ \\sqrt{n}\\]","code":""},{"path":"inference-for-numerical-data.html","id":"predint","chapter":"7 Inference for numerical data","heading":"7.2.1.3 Prediction Intervals (ISCAM 2.6, not in ISRS)","text":"prediction interval different confidence interval!!! Remember confidence interval range values try capture parameter. prediction interval meant capture 95% future observations (see example healthy body temperatures). Note order capture variability observations, combine variability center interval (\\(s/\\sqrt{n}\\)) variability observations (\\(s\\)).\\((1-\\alpha)100%\\) prediction interval \\((1-\\alpha)\\) probability capturing new observation population.\\[\\overline{X} \\pm t_{n-1}^* \\cdot s \\sqrt{1 + \\frac{1}{n}}\\]","code":""},{"path":"inference-for-numerical-data.html","id":"example-healthy-body-temperature","chapter":"7 Inference for numerical data","heading":"7.2.2 Example: healthy body temperature21","text":"study hand meant determine whether average healthy body temperature actually 98.6 F.22Body temperatures (oral temperatures using digital thermometer) recorded healthy men women, aged 18-40 years, volunteers Shigella vaccine trials University Maryland Center Vaccine Development, Baltimore. adults, mean body temperature found 98.249 F standard deviation 0.733 F.23In order work analysis imperative understand data collected part research.","code":""},{"path":"inference-for-numerical-data.html","id":"hypothesis-test-on-true-average-healthy-body-temperature","chapter":"7 Inference for numerical data","heading":"7.2.2.1 Hypothesis test on true average healthy body temperature","text":"first research question want ask : surprising select group 13 participants average healthy body temperature 98.249 F ?questions set perfectly hypothesis test!\\(H_0: \\mu = 98.6\\)\\(H_A: \\mu \\ne 98.6\\)use t-distribution investigate claim.\\[t-score = \\frac{98.249 - 98.6}{0.733/\\sqrt{130}} = -5.46\\]likely standardized version test statistic happen null hypothesis true? Well, \\(H_0\\) true, t-statistics t-distribution. can use t-distribution find p-value (recall p-value probability data extreme \\(H_0\\) true.)test statistic -5.46, even two-sided p-value (area doubled) way less 0.001.","code":"\n2 * mosaic::xpt(-5.46, df = 129, ncp = 0)## [1] 2.354246e-07"},{"path":"inference-for-numerical-data.html","id":"confidence-interval-for-true-average-healthy-body-temperature","chapter":"7 Inference for numerical data","heading":"7.2.2.2 Confidence interval for true average healthy body temperature","text":"Possibly interesting confidence interval tell us range plausible values healthy body temperatures.confidence interval given following formula: \\[\\overline{X} \\pm t_{n-1}^* \\cdot s/ \\sqrt{n}\\]calculated (98.121, 98.376). , 95% confident true average healthy body temperature somewhere 98.121 F 98.376 F. Note 98.6 F interval!!! Wow.","code":"\nmosaic::xqt(.975, df = 129)## [1] 1.978524\n98.249 - 1.9785 * 0.733 / sqrt(130)## [1] 98.12181\n98.249 + 1.9785 * 0.733 / sqrt(130)## [1] 98.37619"},{"path":"inference-for-numerical-data.html","id":"prediction-interval-for-individual-healthy-body-temperatures","chapter":"7 Inference for numerical data","heading":"7.2.2.3 Prediction interval for individual healthy body temperatures24","text":"Note fundamental difference goal confidence interval goal prediction interval calculated section. confidence interval interval plausible values population parameter. prediction interval future individual observations.\\((1-\\alpha)100%\\) prediction interval \\((1-\\alpha)\\) probability capturing new observation population., 95% prediction interval healthy body temperatures can calculated using:\\[\\overline{X} \\pm t_{n-1}^* \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n}}\\]\\[98.249 \\pm t_{129}^* \\cdot 0.733 \\cdot \\sqrt{1 + \\frac{1}{130}}\\]gives 95% prediction interval (96.79 F, 99.70 F). 0.95 probability reach population, person selected healthy body temperature 96.79 F 99.70 F. Said differently, 95% individuals population healthy body temperature 96.79 F 99.70 F (much wider range values confidence interval!)","code":"\nmosaic::xqt(.975, df = 129)## [1] 1.978524\n98.249 - 1.9785*0.733*sqrt(1 + 1/130)## [1] 96.79319\n98.249 + 1.9785*0.733*sqrt(1 + 1/130)## [1] 99.70481"},{"path":"inference-for-numerical-data.html","id":"mean2inf","chapter":"7 Inference for numerical data","heading":"7.3 Comparing two independent means","text":"turns setting random samples taken (e.g., NBA salaries) setting random allocation done (e.g., sleep deprivation), t-distribution describes distribution test statistic quite well. Note variability associated difference means uses variability samples (individual sample sizes!).\\[\\begin{eqnarray*}\n\\mbox{parameter} &=& \\mu_1 - \\mu_2\\\\\n\\mbox{statistic} &=& \\overline{X}_1 - \\overline{X}_2\\\\\nSE_{\\overline{X}_1 - \\overline{X}_2} &=& ?????\n\\end{eqnarray*}\\]general, math done variance (just squared standard deviations).\n\\[\\begin{eqnarray*}\nvar(- B) &=& var() + var(B)\\\\\nvar(\\overline{X}_1 - \\overline{X}_2) &=& var(\\overline{X}_1) + var(\\overline{X}_2)\\\\\n&=& \\sigma^2_1 / n_1 + \\sigma^2_2 / n_2\\\\\nSE(\\overline{X}_1 - \\overline{X}_2) &=& \\sqrt{s^2_1 / n_1 + s^2_2 / n_2}\\\\\n\\end{eqnarray*}\\]methods can used samples different sizes variability two samples quite different (\\(s_1 \\ne s_2\\)). use procedures, exact degrees freedom straightforward calculate:\\[\\begin{eqnarray*}\ndf &=& \\frac{ \\bigg(\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2} \\bigg)^2}{ \\bigg[ \\frac{(s^2_1/n_1)^2}{n_1 - 1} + \\frac{(s^2_2/n_2)^2}{n_2 - 1} \\bigg] }\\ \\ \\ \\ \\ \\ \\ \\mbox{Yikes!!!!}\\\\\ndf &\\approx& \\min \\{ n_1 - 1, n_2 -1 \\} \\\\\n\\end{eqnarray*}\\]SE appropriately defined, hypothesis test confidence interval follow methods earlier semester.\\(H_0: \\mu_1 - \\mu_2 = 0\\)\\(H_A: \\mu_1 - \\mu_2 \\ne 0\\)\\(H_0\\) true:\\[\\begin{eqnarray*}\nt &=& \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)_0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}\\\\\n&=& \\frac{(\\overline{X}_1 - \\overline{X}_2) - 0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}\\\\\n&\\sim& t_{\\min \\{ n_1 - 1, n_2 -1 \\} }\n\\end{eqnarray*}\\]means \\(t_{\\min \\{ n_1 - 1, n_2 -1 \\} }\\)-distribution can used find p-value associated t-score:\n\\[\\mbox{t-score} = \\frac{(\\overline{X}_1 - \\overline{X}_2) - 0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}.\\]Additionally, \\((1-\\alpha)100\\)% confidence interval \\((\\mu_1 - \\mu_2)\\) can found computing:\\[(\\overline{X}_1 - \\overline{X}_2) \\pm t_{\\min \\{ n_1 - 1, n_2 -1 \\} }^* \\cdot \\sqrt{s_1^2 / n_1 + s_2^2 / n_2}.\\]","code":""},{"path":"inference-for-numerical-data.html","id":"r-code-for-inference-on-1-or-2-means.","chapter":"7 Inference for numerical data","heading":"7.4 R code for inference on 1 or 2 means.","text":", R used primarily calculator way find appropriate values t-distribution (using mosaic::xpt mosaic::xqt \\(\\rightarrow\\) note along first argument (either probability place x-axis) important add degrees freedom df possibly argument ncp=0 centers graph zero).Consider teacher salary data available OpenIntro textbook.data set contains teacher salaries 2009-2010 71 teachers employed St. Louis Public School Michigan, well several covariates.\nPosted opendata.socrata.com Jeff Kowalski. Original source: http://stlouis.edzone.net","code":"\nteachers <- read_delim(\"https://www.openintro.org/data/tab-delimited/teacher.txt\", \n                       delim= \"\\t\")"},{"path":"inference-for-numerical-data.html","id":"t.test","chapter":"7 Inference for numerical data","heading":"7.4.1 t.test()","text":"function typically used t-tests function t.test(). Note t.test() function requires complete dataset, just summary statistics. However, t.test() can used variety tests ’ve seen (ones haven’t seen!): one sample t-test, two independent samples t-test (without equal variances), paired t-test.","code":""},{"path":"inference-for-numerical-data.html","id":"one-sample-t-test","chapter":"7 Inference for numerical data","heading":"7.4.1.1 One sample t-test","text":"example, might interested testing whether average salary (teachers St Louis) $47,000 year. p-value extremely small. reject \\(H_0\\). , can claim true average base salary $47,000. (Note, calculate CI, use alternative = \"two.sided\".)\\(H_0: \\mu = 47,000\\)\\(H_A: \\mu > 47,000\\)","code":"\nt.test(teachers$base, mu = 47000, alternative = \"greater\")## \n##  One Sample t-test\n## \n## data:  teachers$base\n## t = 7.9466, df = 70, p-value = 1.146e-11\n## alternative hypothesis: true mean is greater than 47000\n## 95 percent confidence interval:\n##  54440.82      Inf\n## sample estimates:\n## mean of x \n##  56415.96"},{"path":"inference-for-numerical-data.html","id":"two-independent-samples-t-test","chapter":"7 Inference for numerical data","heading":"7.4.1.2 Two independent samples t-test","text":", maybe interest knowing whether base salary teachers BA degree less MA degree, average. Note, \\(\\mu\\) denotes average salary population group denoted subscript. p-value 0.442, reject null hypothesis. (Note, calculate CI, use alternative = \"two.sided\".)\\(H_0: \\mu_{BA} = \\mu_{MA}\\)\\(H_A: \\mu_{BA} < \\mu_{MA}\\)","code":"\nt.test(base ~ degree, alternative = \"less\", data = teachers)## \n##  Welch Two Sample t-test\n## \n## data:  base by degree\n## t = -0.14639, df = 65.238, p-value = 0.442\n## alternative hypothesis: true difference in means is less than 0\n## 95 percent confidence interval:\n##      -Inf 3664.912\n## sample estimates:\n## mean in group BA mean in group MA \n##         56257.10         56609.56"},{"path":"inference-for-numerical-data.html","id":"bootstrapping-in-r","chapter":"7 Inference for numerical data","heading":"7.4.2 Bootstrapping in R","text":"Notice simulation syntax almost identical covered working proportions. Also, notice computational approach gives almost identical answers mathematical model (t-distribution) .","code":""},{"path":"inference-for-numerical-data.html","id":"one-sample-bootstrapping-on-mean","chapter":"7 Inference for numerical data","heading":"7.4.2.1 One sample bootstrapping on mean","text":"","code":"\nlibrary(infer)\nset.seed(47)\n# calculate the observed test statistic\n# note that we could use `stat = \"median\"` or `stat = \"t\"`\n( x_bar_base <- teachers %>%\n  specify(response = base) %>%\n  calculate(stat = \"mean\") )## # A tibble: 1 x 1\n##     stat\n##    <dbl>\n## 1 56416.\n# create the null sampling distribution\nnull_dist <- teachers %>%\n  specify(response = base) %>%\n  hypothesize(null = \"point\", mu = 47000) %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"mean\")\n\n# visualize the null sampling distribution\nvisualize(null_dist) +\n  shade_p_value(obs_stat = x_bar_base, direction = \"greater\")\n# calculate the p-value\nnull_dist %>%\n  get_p_value(obs_stat = x_bar_base, direction = \"greater\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1       0"},{"path":"inference-for-numerical-data.html","id":"two-independent-samples-comparing-two-means","chapter":"7 Inference for numerical data","heading":"7.4.2.2 Two independent samples comparing two means","text":"p-value now 0.464 (close 0.442 given smooth t-distribution curve).","code":"\nset.seed(47)\n# calculate the observed test statistic\n( diff_x_bar_base <- teachers %>%\n  specify(base ~ degree) %>%\n  calculate(stat = \"diff in means\", order = c(\"MA\", \"BA\")) )## # A tibble: 1 x 1\n##    stat\n##   <dbl>\n## 1  352.\n# create the null sampling distribution\nnull_dist <- teachers %>%\n  specify(base ~ degree) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type =\"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"MA\", \"BA\")) \n\n# visualize the null sampling distribution\nvisualize(null_dist) +\n  shade_p_value(obs_stat = diff_x_bar_base, direction = \"greater\")\n# calculate the p-value\nnull_dist %>%\n  get_p_value(obs_stat = diff_x_bar_base, direction = \"greater\")## # A tibble: 1 x 1\n##   p_value\n##     <dbl>\n## 1   0.454"},{"path":"inference-for-numerical-data.html","id":"nba-salaries-example-from-iscam-inv-4.2","chapter":"7 Inference for numerical data","heading":"7.4.3 NBA Salaries example from ISCAM Inv 4.2","text":"R code ISCAM book. ’ve written series steps slightly different way results.","code":""},{"path":"inference-for-numerical-data.html","id":"eda","chapter":"7 Inference for numerical data","heading":"7.4.3.1 EDA","text":"thinking inference, let’s look population (2017-18). information today population includes salaries NBA players 2017. can see particular year, average salary Western conference slightly higher.","code":"\nNBAsalary <- read_delim(\"http://www.rossmanchance.com/iscam3/data/NBASalaries2017.txt\", delim = \"\\t\", escape_double = FALSE, trim_ws = TRUE)\n\n\nggplot(NBAsalary) +\n  geom_boxplot(aes(x=conference, y = salary))\nggplot(NBAsalary) +\n  geom_histogram(aes(fill = conference, x = salary))\nggplot(NBAsalary) +\n  geom_histogram(aes(x = salary)) + \n  facet_wrap(~ conference)\nNBAsalary %>%\n  group_by(conference) %>%\n  summarize(mu = mean(salary), sigma = sd(salary), N = n(), min(salary), max(salary), median(salary))## # A tibble: 2 x 7\n##   conference    mu sigma     N `min(salary)` `max(salary)` `median(salary)`\n##   <chr>      <dbl> <dbl> <int>         <dbl>         <dbl>            <dbl>\n## 1 eastern     6.61  7.01   227        0.0577          33.3             3.81\n## 2 western     7.42  7.76   221        0.0320          34.4             4"},{"path":"inference-for-numerical-data.html","id":"sampling-distribution-for-one-mean","chapter":"7 Inference for numerical data","heading":"7.4.3.2 Sampling distribution for one mean","text":"considering sample means vary, let’s visualize samples conference.One way think difference means varies first visualize variability distribution single mean (.e., one conference). Let’s look variability Eastern conference well variability Western conference.Note population analysis (full set observations), see \\(\\sigma \\approx 7\\). histograms standard deviation close \\(\\sigma / \\sqrt{20} = 1.5\\). ?two histograms centered place? ?","code":"\nNBAsalary %>%\n  filter(conference == \"eastern\") %>%\n  sample_n(size = 20, replace = FALSE) %>%\n  ggplot() + \n  geom_histogram(aes(x = salary)) + xlab(\"eastern salary\")\n\nNBAsalary %>%\n  filter(conference == \"western\") %>%\n  sample_n(size = 20, replace = FALSE) %>%\n  ggplot() + \n  geom_histogram(aes(x = salary)) + xlab(\"western salary\")\nNBAsalary %>%\n  filter(conference == \"eastern\") %>%\n  rep_sample_n(size = 20, replace = FALSE, reps = 500) %>%\n  summarize(mean_sal = mean(salary)) %>%\n  ggplot() +\n  geom_histogram(aes(x=mean_sal))\nNBAsalary %>%\n  filter(conference == \"western\") %>%\n  rep_sample_n(size = 20, replace = FALSE, reps = 500) %>%\n  summarize(mean_sal = mean(salary)) %>%\n  ggplot() +\n  geom_histogram(aes(x=mean_sal))"},{"path":"inference-for-numerical-data.html","id":"sampling-distribution-for-two-means","chapter":"7 Inference for numerical data","heading":"7.4.3.3 Sampling distribution for two means","text":"Note: code selects 20 random salaries Eastern NBA conference 20 random salaries Western NBA conference. Using two different samples, t-statistic selected. whole process repeated 1000 times.","code":"\nset.seed(4747)\n\nt_salaries <- data.frame(meandiff = double(), tstat = double())\nfor(i in 1:1000){\n  \n  one_t<- NBAsalary %>%\n    group_by(conference) %>%\n    sample_n(size = 20, replace = FALSE) %>% \n    summarize(mn = mean(salary), sd = sd(salary), n = n()) %>%\n    pivot_wider(names_from = conference, values_from = 2:4) %>%\n    summarize(meandiff = (mn_eastern - mn_western), \n              tstat = (mn_eastern - mn_western) / \n                sqrt(sd_eastern^2 / n_eastern + sd_western^2 / n_western)) \n\n  t_salaries[i,] <- one_t\n}\n\n\nt_salaries %>%\n  ggplot() +\n  geom_histogram(aes(x=meandiff)) +\n  geom_vline(xintercept = 0)\nt_salaries %>%\n  ggplot() +\n  geom_histogram(aes(x=tstat)) +\n  geom_vline(xintercept = 0)"},{"path":"inference-for-numerical-data.html","id":"reflection-questions-4","chapter":"7 Inference for numerical data","heading":"7.5 Reflection Questions","text":"reflection questions cover marked asterisk *.","code":""},{"path":"inference-for-numerical-data.html","id":"quantitative-variable-ims-section-7.1","chapter":"7 Inference for numerical data","heading":"7.5.1 1 quantitative variable: IMS Section 7.1","text":"changed studies (data structure) Chapters 2 & 3?statistic interest now? parameter interest?difference distribution data distribution statistic? theoretical difference well computational difference.limiting sampling distribution statistic? (Note, answer big samples, Central Limit Theorem works limit… .e., sample size big.)interest statistics sample mean, tool can use finding alternative statistic’s sampling distribution?Explain intuition behind bootstrapping.\nExplain intuition behind bootstrapping.Explain SE statistic calculated using bootstrapping.\nExplain SE statistic calculated using bootstrapping.difference normal distribution t distribution?use z use t?use confidence interval use hypothesis test?different information boxplot give versus histogram?","code":""},{"path":"inference-for-numerical-data.html","id":"means-1-quantitative-variable-1-binary-variable-ims-section-7.2","chapter":"7 Inference for numerical data","heading":"7.5.2 2 means (1 quantitative variable, 1 binary variable): IMS Section 7.2","text":"changed studies (data structure) section 4.1?statistic interest now? parameter interest?sampling distribution statistic interest?t-distribution become relevant?degrees freedom general? actual degrees freedom test section 4.3?null mechanism different across three analysis methods section 3.2: randomization test, two-sample t-test, random sampling test (n.b. also called parametric bootstrap)?\nnull mechanism different across three analysis methods section 3.2: randomization test, two-sample t-test, random sampling test (n.b. also called parametric bootstrap)?create CI? interpret CI?data normal? strategies can try ?","code":""},{"path":"inference-for-numerical-data.html","id":"paired-sample-difference-in-means-ims-section-7.3","chapter":"7 Inference for numerical data","heading":"7.5.2.1 Paired sample, difference in means: IMS Section 7.3","text":"changed studies (data structure) section 4.2 compared 4.1 4.3?statistic interest now? parameter interest?sampling distribution statistic interest?benefit pairing analysis?happens paired study analyzed independent two sample study? (happens p-value? happens CI?)easiest way think / analyze paired data?","code":""},{"path":"inference-for-numerical-data.html","id":"not-covering-in-spring-2021-anova-ims-section-7.5","chapter":"7 Inference for numerical data","heading":"7.5.2.2 * (not covering in Spring 2021) ANOVA: IMS Section 7.5","text":"tests called ANalysis VAriance (ANOVA)?Describe variability numerator variability denominator. measure?null alternative hypotheses ANOVA?features data affect power test? power mean ?technical conditions? need equal variances ?","code":""},{"path":"reginf.html","id":"reginf","chapter":"8 Inference on Correlation & Regression","heading":"8 Inference on Correlation & Regression","text":"last topic semester focus inference setting two quantitative variables. , explanatory response variables measured numeric scale, model using correlation least square regression, ask whether lines come population relationship (.e., null hypothesis).","code":""},{"path":"reginf.html","id":"inference-for-correlation","chapter":"8 Inference on Correlation & Regression","heading":"8.0.1 Inference for correlation","text":"Correlation measures association two numerical variables. [Note, describing two categorical (one numerical & one categorical) variables vary together, said associated instead correlated.]correlation coefficient measures strength direction linear association two numerical variables.Note: won’t actually cover inference correlation class, notes inference correlation included can see process similar statistics seen course point.Parameter: \\(\\rho\\)\nStatistic: \\(r\\)\nSE\\(_r: \\sqrt{\\frac{1-r^2}{n-2}}\\), \\(r\\) normally distributed \\(\\rho\\) = 0! Otherwise, distribution \\(r\\) sample sample skewed (think scenario \\(\\rho = 0.9\\)).","code":""},{"path":"reginf.html","id":"hypothesis-testing","chapter":"8 Inference on Correlation & Regression","heading":"8.0.1.1 Hypothesis Testing","text":"\\[\\begin{eqnarray*}\nH_0:&& \\rho = 0\\\\\nH_A:&& \\rho \\ne 0\\\\\nt^* &=& \\frac{r}{SE_r} = \\frac{r}{\\sqrt{(1-r^2)/(n-2)}}\\\\\nt^* &\\sim& t_{n-2}  \\mbox{ } H_0 \\mbox{ true}\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"confidence-interval","chapter":"8 Inference on Correlation & Regression","heading":"8.0.1.2 Confidence Interval","text":"\\(\\rho \\ne 0\\), SE might okay, sampling distribution \\(r\\) normal (thus \\(t\\) use SE).Let:\\[\\begin{eqnarray*}\nz &=& 0.5 \\ln \\bigg( \\frac{1+r}{1-r} \\bigg)\\\\\n\\xi &=& 0.5 \\ln \\bigg( \\frac{1+\\rho}{1-\\rho} \\bigg)\\\\\nvar(z) &=& \\sqrt{\\frac{1}{n-3}}\\\\\n95\\% \\mbox{ CI } \\xi : &&\\\\\nz &\\pm& 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\\n\\mbox{95% confident } && \\\\\n&&z - 1.96 \\cdot  \\sqrt{\\frac{1}{n-3}} \\leq \\xi \\leq z + 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\\n&& \\leq \\xi \\leq b\\\\\n&& \\leq 0.5 \\ln \\bigg(\\frac{1+\\rho}{1-\\rho} \\bigg) \\leq b\\\\\n&& \\frac{e^{2a} - 1}{e^{2a} + 1} \\leq \\rho \\leq \\frac{e^{2b} - 1}{e^{2b} + 1}\n\\end{eqnarray*}\\]See Cat Jumping25 example section 8.3.1.HT:\\[\\begin{eqnarray*}\nH_0:&& \\rho = 0\\\\\nH_a:&& \\rho \\ne 0\\\\\nt^* &=& \\frac{r}{\\sqrt{(1-r^2)/(n-2)}} = \\frac{-0.496}{\\sqrt{(1-0.496^2) / (18-2)}}= -2.2849\\\\\np-value &=& 2 \\cdot P(t_{18-2} \\leq -2.2849) = 2\\cdot(pt(-2.2849,16)) = 0.036 \\mbox{  (borderline significant)}\n\\end{eqnarray*}\\]CI:\\[\\begin{eqnarray*}\n95\\% \\mbox{CI } \\xi :&& \\\\\nz \\pm 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}&& \\\\\n\\mbox{} 95\\% \\mbox{ confident }&&\\\\\n 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg) - 1.96 \\cdot  \\sqrt{\\frac{1}{n-3}} &\\leq \\xi \\leq& 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg) + 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\\n 0.5 \\ln\\bigg(\\frac{1 - 0.496}{1+0.496}\\bigg) - 1.96 \\cdot  \\sqrt{\\frac{1}{18-3}} &\\leq \\xi \\leq &0.5 \\ln\\bigg(\\frac{1-0.496}{1+0.496}\\bigg) + 1.96 \\cdot \\sqrt{\\frac{1}{18-3}}\\\\\n -1.05 &\\leq \\xi \\leq &-0.04\\\\\n \\frac{e^{2\\cdot -1.05} - 1}{e^{2\\cdot -1.05} + 1} &\\leq \\rho \\leq& \\frac{e^{2\\cdot -0.04} - 1}{e^{2\\cdot -0.04} + 1}\\\\\n&& (-0.781, -0.04)\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"simple-linear-regression-1","chapter":"8 Inference on Correlation & Regression","heading":"8.1 Simple Linear Regression","text":"Regression method predicts value one numerical variable another. , extension describing degree linearity relationship (correlation), goal now create best linear model – often prediction. Note many characteristics explored correlation applicable regression. However, correlation treats \\(X\\) \\(Y\\) interchangeable, whereas regression treats \\(X\\) fixed known \\(Y\\) random unknown. previously, call \\(X\\) explanatory variable, \\(Y\\) response variable. , assume causal mechanism \\(X\\) \\(Y\\) even strong linear (otherwise) relationship.","code":""},{"path":"reginf.html","id":"population-model","chapter":"8 Inference on Correlation & Regression","heading":"Population Model","text":"Notice Greek letters representing parameters:\\[\\begin{eqnarray*}\nE[y|x] &=& \\beta_0 + \\beta_1 x \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n\\epsilon_i &=& y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"predicted-values-1","chapter":"8 Inference on Correlation & Regression","heading":"Predicted Values","text":"predicted values Y regression line estimate mean value \\(Y\\) individuals given value \\(X\\). Notice Roman letters (English letters) representing statistics:\\[\\begin{eqnarray*}\n\\hat{y} &=& b_0 + b_1 x\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\ne_i &=& y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]Notice, predicting mean value response variable given value explanatory variable!","code":""},{"path":"reginf.html","id":"infbeta1","chapter":"8 Inference on Correlation & Regression","heading":"8.1.1 Inference on the slope, \\(\\beta_1\\)","text":"section 3.2.1, slope calculated using least squares.","code":""},{"path":"reginf.html","id":"se-of-the-slope","chapter":"8 Inference on Correlation & Regression","heading":"SE of the slope","text":"\\[\\begin{eqnarray*}\nSE(b_1) &=& \\sqrt{\\frac{MSE}{\\sum_i (x_i - \\overline{x})^2}} = \\sqrt{\\frac{MSE}{(n-1)s_x^2}} = = \\sqrt{\\frac{\\frac{\\sum_i (y_i - \\hat{y}_i)^2}{n-2}}{(n-1)s_x^2}}\\\\\n\\end{eqnarray*}\\]Just like statistic, value \\(b_1\\) can calculated every sample. manner \\(b_1\\) varies sample sample becomes sampling distribution sample slope. SE slope standard deviation associated sampling distribution slope. resulting inference theory similar saw mean. CLT describes \\(b_1\\) normal distribution, estimating \\(SE(b_1)\\) induces extra variability leads t-score test statistic t-distribution df = \\(n-2\\).\\[\\begin{eqnarray*}\n\\mbox{t-score} = \\frac{b_1 - \\beta_1}{SE(b_1)} \\sim t_{n-2}\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"ci-for-the-slope","chapter":"8 Inference on Correlation & Regression","heading":"CI for the slope","text":"\\[\\begin{eqnarray*}\nb_1- t^*_{\\alpha/2, n-2} SE(b_1) \\leq \\beta_1 \\leq b_1 + t^*_{\\alpha/2, n-2} SE(b_1)\n\\end{eqnarray*}\\]See Housing Prices26 example section 8.3.2.\\[\\begin{eqnarray*}\nt_{\\alpha/2,n-2} &=& qt(.975, 18-2) = 2.1199\\\\\nSE(b_1) &=& 26.4\\\\\nb_1 &=& 202\\\\\nb_1 &\\pm& t_{\\alpha/2, n-2} SE(b_1): 202 \\pm 2.1199 \\cdot 26.4\\\\\n&& (146.03, 257.97)\\\\\n\\end{eqnarray*}\\], 95% confident true average change price associated additional square foot house $146.03 $257.97.","code":""},{"path":"reginf.html","id":"ht-for-the-slope","chapter":"8 Inference on Correlation & Regression","heading":"HT for the slope","text":"mentioned previously,\n\\[\\begin{eqnarray*}\n\\mbox{t-score} = \\frac{b_1 - \\beta_1}{SE(b_1)} \\sim t_{n-2}\n\\end{eqnarray*}\\]Typically, interested testing whether slope zero. null hypothesis \\(H_0: \\beta_1 = 0\\) addresses whether non-zero linear relationship \\(X\\) \\(Y\\):\\[\\begin{eqnarray*}\nH_0:&& \\mbox{ slope regression line zero}, \\beta_1=0\\\\\nH_A:&& \\mbox{ slope regression line zero}, \\beta_1 \\ne 0\\\\\n\\end{eqnarray*}\\]previous tests, alternative can one- two-sided (depending research question)., back housing data… (consider two sided test)\n\\[\\begin{eqnarray*}\nt^* &=& 7.67\\\\\np-value &=& 2 \\cdot P(t_{16} \\geq 7.67) = 2 \\cdot (1-pt(7.67, 16)) \\approx 0\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"regression-technical-conditions","chapter":"8 Inference on Correlation & Regression","heading":"Regression Technical Conditions","text":"L: value X, population possible Y-values whose mean lies “true” regression line (linearity): value X, Y-measurements represent random sample population possible Y-values (independence) [Consider example lack independence. researcher trying determine whether number pieces puzzle linearly associated time complete puzzle. first choose 2 people let 10 puzzles . let 20 independent people puzzles. first experiment create slope particular two people sampled (may may close parameter). second one create slope close 20 people sampled. Note effective variability first model based n=2, think based n=20 (don’t notice lack independence). second slope based n=20, correct associated variability.]N: value X, distribution possible Y-values normal (normality)E: variance Y-values values X (equal variance)","code":""},{"path":"reginf.html","id":"residual-plots","chapter":"8 Inference on Correlation & Regression","heading":"Residual Plots","text":"Within residual plot, looking types things want scatter plot. [See residual plots provided section 8.3.1.]roughly symmetric cloud points horizontal line zero, higher density points close line far line,little noticeable curvature move left right along X-axis, andapproximately equal variance points line values X.","code":""},{"path":"reginf.html","id":"MLR","chapter":"8 Inference on Correlation & Regression","heading":"8.2 Multiple Linear Regression","text":"simple linear regression, consider \\(n\\) observations. response variable \\(^{th}\\) individual denoted \\(Y_i\\), . variation remaining \\(Y_i\\) isn’t explained predictors also remain , denoted \\(\\epsilon_i\\) called random error. Since now one explanatory variable, need add additional subscript \\(X\\), denoting value \\(k^{th}\\) predictor variable \\(^{th}\\) individual \\(X_{ik}\\). Thus model now\\[\\begin{eqnarray*}\nY_i&=&\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}+ \\cdots + \\beta_{p-1}X_{,p-1} + \\epsilon_i\\\\\nE[Y]&=&\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+ \\cdots + \\beta_{p-1}X_{p-1}\\\\\nY_i&=&b_0+b_1X_{i1}+b_2X_{i2}+ \\cdots + b_{p-1}X_{,p-1} + e_i\\\\\n\\widehat{Y}&=&b_0+b_1X_{1}+b_2X_{2}+ \\cdots + b_{p-1}X_{p-1}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"reginf.html","id":"fitting-the-model","chapter":"8 Inference on Correlation & Regression","heading":"Fitting the Model","text":"estimate coefficients, use principle , least squares. , minimize\n\\[\\sum_{=1}^n(Y_i-(b_0+b_1X_{i1}+b_2X_{i2} + \\cdots + b_{p-1}X_{,p-1}))^2\\]\ninterested finding least squares estimates (\\(b_i\\)) parameters model \\(\\beta_i\\).","code":""},{"path":"reginf.html","id":"MLRmod","chapter":"8 Inference on Correlation & Regression","heading":"8.2.1 Model selection","text":"many aspects linear model consider (offer entire course! Math 158: Linear models), focus interpreting testing single coefficients model.Consider following model (based R housing example ). predicting price home (\\(\\ln\\) units).\\[\\widehat{Y} = 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot \\mbox{# bedrooms}\\]Now, let’s compare two houses. two houses exact square feet (note, doesn’t matter square feet !!). House 1 (H1) 4 bedrooms, house 2 (H2) 3 bedrooms.coefficient associated number bedrooms indicates change price (\\(\\ln\\) units) keeping variables constant. , comparing prediction average price home two homes square feet one unit difference bedrooms, price home predicted -0.0603 \\(\\ln\\) units less home bedrooms. (seem make sense? square feet, bedrooms chop house make less desirable maybe?)\\[\\begin{eqnarray*}\n\\widehat{Y}_{H1} &=& 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot 4\\\\\n\\widehat{Y}_{H2} &=& 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot 3\\\\\n\\widehat{Y}_{H1} - \\widehat{Y}_{H2} &=& (-0.0603 \\cdot 4) - (-0.0603 \\cdot 3) =  -0.0603\\\\\n\\end{eqnarray*}\\]","code":"\nhouse = read.table(\"http://www.rossmanchance.com/iscam2/data/housing.txt\", \n                   header=TRUE, sep=\"\\t\")\nlm(log(price) ~ sqft + bedrooms, data=house) %>% tidy()## # A tibble: 3 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 12.2      0.174         70.1  1.39e-73\n## 2 sqft         0.000468 0.0000660      7.09 4.73e-10\n## 3 bedrooms    -0.0603   0.0572        -1.05 2.95e- 1"},{"path":"reginf.html","id":"inference-about-regression-parameters","chapter":"8 Inference on Correlation & Regression","heading":"Inference about Regression Parameters","text":"However, coefficient bedrooms isn’t significant (, associated p-value larger reasonable level significance, like \\(\\alpha = 0.05\\)). wait, p-value even calculated?least squares coefficient estimate SE create test statistic t distribution null hypothesis true (note now estimating \\(p\\) parameters, degrees freedom \\(n-p\\)).\\[\\begin{eqnarray*}\n\\frac{b_k - \\beta_k}{s\\{b_k\\}} \\sim t_{(n-p)}\n\\end{eqnarray*}\\]\n\\((1-\\alpha)100\\%\\) CI \\(\\beta_k\\) given \\[b_k \\pm t_{(1-\\alpha/2, n-p)} s\\{b_k\\}\\]t-test done separately \\(\\beta\\) coefficient. test addresses effect removing variable hand. testing interpretation regression coefficients done variables model.coefficient bedrooms significant given sqft model. Note don’t sqft, bedrooms acts surrogate sqft important (significant!). case, however, sqft better predictor bedrooms.","code":"\nlm(log(price) ~  bedrooms, data=house) %>% tidy()## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   12.3      0.219      56.3  1.03e-66\n## 2 bedrooms       0.178    0.0587      3.03 3.31e- 3"},{"path":"reginf.html","id":"which-variables-to-choose","chapter":"8 Inference on Correlation & Regression","heading":"Which variables to choose?","text":", numerous ways build model. important principles keep mind:ideally variables model significant (remove variables high p-values)LINE model conditions hold (may need transform response explanatory variables)variables consistent data context (sure analysis done experts field)","code":""},{"path":"reginf.html","id":"checking-model-assumptions","chapter":"8 Inference on Correlation & Regression","heading":"8.2.2 Checking model assumptions","text":", LINE model conditions checked using residual plots. Note housing example, residual plot log transformation response variable improved.","code":"\nlibrary(tidyverse)\nlibrary(broom)\nhouse = read.table(\"http://www.rossmanchance.com/iscam2/data/housing.txt\", \n                   header=TRUE, sep=\"\\t\") %>%\n  mutate(ln_price = log(price))\n\nlm(price ~  sqft, data=house)  %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for price as a function of sqft\")\nlm(ln_price ~  sqft, data=house)  %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid))+ \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for ln price as a function of sqft\")"},{"path":"reginf.html","id":"r-code-for-regression-1","chapter":"8 Inference on Correlation & Regression","heading":"8.3 R code for regression","text":"","code":""},{"path":"reginf.html","id":"ex:cat2","chapter":"8 Inference on Correlation & Regression","heading":"8.3.1 Example: Cat Jumping27 (Correlation & SLR)","text":"Consider cat data given Investigations 5.6 5.13. idea understand cat jumping velocity function body characteristics. Note correlation \\(r=-0.496\\) bodymass velocity.","code":"\ncats <- read_table2(\"http://www.rossmanchance.com/iscam2/data/CatJumping.txt\")\n\nggplot(cats, aes(x=bodymass, y = velocity)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se=FALSE)"},{"path":"reginf.html","id":"correlation-1","chapter":"8 Inference on Correlation & Regression","heading":"Correlation","text":"","code":"\ncats %>%\n  select(bodymass, velocity) %>%\n  cor()##            bodymass   velocity\n## bodymass  1.0000000 -0.4964022\n## velocity -0.4964022  1.0000000"},{"path":"reginf.html","id":"simple-linear-regression-2","chapter":"8 Inference on Correlation & Regression","heading":"Simple Linear Regression","text":"alternative way work output specific pieces (instead entire summary output).","code":"\nsummary(lm(velocity ~ bodymass, data = cats))## \n## Call:\n## lm(formula = velocity ~ bodymass, data = cats)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -48.069 -16.729  -8.524  10.546  84.625 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 394.473238  23.441939  16.828 1.35e-11 ***\n## bodymass     -0.012196   0.005332  -2.287   0.0361 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 29.6 on 16 degrees of freedom\n## Multiple R-squared:  0.2464, Adjusted R-squared:  0.1993 \n## F-statistic: 5.232 on 1 and 16 DF,  p-value: 0.03613\nlibrary(broom)\n\nlm(velocity ~ bodymass, data = cats) %>% tidy(conf.int = TRUE)## # A tibble: 2 x 7\n##   term        estimate std.error statistic  p.value conf.low  conf.high\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>      <dbl>\n## 1 (Intercept) 394.      23.4         16.8  1.35e-11 345.     444.      \n## 2 bodymass     -0.0122   0.00533     -2.29 3.61e- 2  -0.0235  -0.000893"},{"path":"reginf.html","id":"residual-plot-1","chapter":"8 Inference on Correlation & Regression","heading":"Residual Plot","text":"work residuals, use augment().","code":"\nlm(velocity ~ bodymass, data = cats) %>% augment()## # A tibble: 18 x 8\n##    velocity bodymass .fitted .resid .std.resid   .hat .sigma   .cooksd\n##       <dbl>    <dbl>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>     <dbl>\n##  1     334.     3640    350. -15.6     -0.545  0.0656   30.3 0.0104   \n##  2     387.     2670    362.  25.4      0.920  0.131    29.7 0.0640   \n##  3     411.     5600    326.  84.6      3.05   0.119    19.8 0.629    \n##  4     319.     4130    344. -25.5     -0.887  0.0557   29.8 0.0232   \n##  5     369.     3020    358.  11.1      0.394  0.101    30.4 0.00867  \n##  6     359.     2660    362.  -3.23    -0.117  0.132    30.6 0.00105  \n##  7     345.     3240    355. -10.4     -0.366  0.0853   30.4 0.00624  \n##  8     325.     5140    332.  -7.19    -0.254  0.0844   30.5 0.00297  \n##  9     301.     3690    349. -48.1     -1.68   0.0639   27.7 0.0962   \n## 10     332.     3620    350. -18.5     -0.648  0.0664   30.2 0.0149   \n## 11     313.     5310    330. -17.1     -0.608  0.0957   30.2 0.0196   \n## 12     317.     5560    327.  -9.86    -0.354  0.116    30.4 0.00823  \n## 13     376.     3970    346.  29.5      1.03   0.0572   29.5 0.0321   \n## 14     372.     3770    348.  23.9      0.834  0.0615   29.9 0.0228   \n## 15     314.     5100    332. -18.0     -0.634  0.0820   30.2 0.0179   \n## 16     368.     2950    358.   9.01     0.322  0.106    30.5 0.00614  \n## 17     286.     7930    298. -11.5     -0.552  0.508    30.3 0.157    \n## 18     352.     3550    351.   1.32     0.0463 0.0692   30.6 0.0000797\nlm(velocity ~ bodymass, data = cats) %>% augment() %>%\n  ggplot(aes(x = .fitted, y = .resid)) + \n  geom_point() +\n  geom_hline(yintercept = 0)"},{"path":"reginf.html","id":"confidence-prediction-intervals","chapter":"8 Inference on Correlation & Regression","heading":"Confidence & Prediction Intervals","text":"","code":"\nm_cats <- lm(velocity ~ bodymass, data = cats)\n\npredict(m_cats, newdata=data.frame(bodymass=4700), interval=\"confidence\")##        fit      lwr      upr\n## 1 337.1514 321.3081 352.9947\npredict(m_cats, newdata=data.frame(bodymass=4700), interval=\"prediction\")##        fit      lwr     upr\n## 1 337.1514 272.4378 401.865"},{"path":"reginf.html","id":"plotting-1","chapter":"8 Inference on Correlation & Regression","heading":"Plotting!","text":"","code":"\ncatConf <- m_cats %>% augment() %>%\n  cbind(predict(m_cats, interval=\"confidence\") )  # cbind binds the columns together\n\ncatConf %>% head()##   velocity bodymass  .fitted     .resid .std.resid       .hat   .sigma\n## 1    334.5     3640 350.0793 -15.579293 -0.5445423 0.06563257 30.28374\n## 2    387.3     2670 361.9095  25.390452  0.9203785 0.13125270 29.74812\n## 3    410.8     5600 326.1749  84.625139  3.0468951 0.11941906 19.80527\n## 4    318.6     4130 344.1032 -25.503185 -0.8867122 0.05570221 29.80778\n## 5    368.7     3020 357.6409  11.059101  0.3939761 0.10053270 30.41969\n## 6    358.8     2660 362.0315  -3.231509 -0.1172061 0.13224725 30.55520\n##       .cooksd      fit      lwr      upr\n## 1 0.010414418 350.0793 334.0049 366.1536\n## 2 0.063990821 361.9095 339.1781 384.6410\n## 3 0.629490813 326.1749 304.4923 347.8574\n## 4 0.023189898 344.1032 329.2947 358.9116\n## 5 0.008674247 357.6409 337.7467 377.5351\n## 6 0.001046793 362.0315 339.2141 384.8490\nggplot(catConf, aes(x=bodymass)) + \n  geom_point(aes(y=velocity)) + \n  geom_line(aes(y=.fitted)) +\n  geom_ribbon(aes(ymin=lwr, ymax=upr), fill=\"blue\", alpha=0.2)\ncatPred <- m_cats %>% augment() %>%\n  cbind(predict(m_cats, interval=\"prediction\") )  # cbind binds the columns together\n\ncatPred %>% head()##   velocity bodymass  .fitted     .resid .std.resid       .hat   .sigma\n## 1    334.5     3640 350.0793 -15.579293 -0.5445423 0.06563257 30.28374\n## 2    387.3     2670 361.9095  25.390452  0.9203785 0.13125270 29.74812\n## 3    410.8     5600 326.1749  84.625139  3.0468951 0.11941906 19.80527\n## 4    318.6     4130 344.1032 -25.503185 -0.8867122 0.05570221 29.80778\n## 5    368.7     3020 357.6409  11.059101  0.3939761 0.10053270 30.41969\n## 6    358.8     2660 362.0315  -3.231509 -0.1172061 0.13224725 30.55520\n##       .cooksd      fit      lwr      upr\n## 1 0.010414418 350.0793 285.3088 414.8498\n## 2 0.063990821 361.9095 295.1746 428.6445\n## 3 0.629490813 326.1749 259.7898 392.5599\n## 4 0.023189898 344.1032 279.6352 408.5712\n## 5 0.008674247 357.6409 291.8183 423.4635\n## 6 0.001046793 362.0315 295.2672 428.7958\nggplot(catPred, aes(x=bodymass)) + \n  geom_point(aes(y=velocity)) + \n  geom_line(aes(y=.fitted)) +\n  geom_ribbon(aes(ymin=lwr, ymax=upr), fill=\"blue\", alpha=0.2)"},{"path":"reginf.html","id":"ex:houses2","chapter":"8 Inference on Correlation & Regression","heading":"8.3.2 Example: Housing Prices28 (SLR & MLR & Prediction)","text":"","code":"\nlibrary(GGally)\nhouse = read.table(\"http://www.rossmanchance.com/iscam2/data/housing.txt\", \n                   header=TRUE, sep=\"\\t\") %>%\n  mutate(ln_price = log(price))\nnames(house)## [1] \"sqft\"     \"price\"    \"City\"     \"bedrooms\" \"baths\"    \"ln_price\""},{"path":"reginf.html","id":"descriptive-statistics-1","chapter":"8 Inference on Correlation & Regression","heading":"Descriptive Statistics","text":"good first step investigate variables relate one another. ggpairs function come R package GGally.","code":"\nggpairs(house, columns = c(1,2,4,5))"},{"path":"reginf.html","id":"simple-linear-regression-3","chapter":"8 Inference on Correlation & Regression","heading":"Simple Linear Regression","text":"p-values explanatory variables (sqft bedrooms) significant. Sqft seems significant, indeed, first model higher \\(R^2\\) - , higher proportion variability price explained sqft (42.07%) number bedrooms (8.08%).However, important us ask whether either relationships actually fit technical conditions linear regression model. can see pairs plots relationships look Linear, ’ll assume variables collected Independently, Normality Equality error structure can check using residual plots.plots, seems like residuals higher variability positive residuals. Additionally, seems variability residuals increases larger fitted observations.natural log transformation fix problems.Though residual plot ever look perfect, residual plots seem fit technical conditions model better untransformed data.","code":"\nmod.sqft <- lm(price~sqft, data = house)\nmod.sqft %>% tidy()## # A tibble: 2 x 5\n##   term        estimate std.error statistic  p.value\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   65930.   60994.       1.08 2.83e- 1\n## 2 sqft            202.      26.4      7.67 3.35e-11\nmod.bed <- lm(price ~ bedrooms, data=house)\nmod.bed %>% tidy()## # A tibble: 2 x 5\n##   term        estimate std.error statistic p.value\n##   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n## 1 (Intercept)  220612.   107208.      2.06 0.0428 \n## 2 bedrooms      76865.    28802.      2.67 0.00919\nmod.sqft %>% glance()## # A tibble: 1 x 12\n##   r.squared adj.r.squared  sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl>  <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.421         0.414 2.22e5      58.8 3.35e-11     1 -1139. 2283. 2290.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nmod.bed %>% glance()## # A tibble: 1 x 12\n##   r.squared adj.r.squared  sigma statistic p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl>  <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1    0.0808        0.0695 2.80e5      7.12 0.00919     1 -1158. 2321. 2329.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nmod.sqft %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid))+ \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for price as a function of sqft\")\nmod.bed %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid))+ \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for price as a function of bedrooms\")\nmod.lnsqft <- lm(ln_price ~ sqft, data = house)\nmod.lnbed <- lm(ln_price ~ bedrooms, data=house)\nmod.lnsqft %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid))+ \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for ln(price) as a function of sqft\")\nmod.lnbed %>% augment () %>%\n  ggplot(aes(x = .fitted, y = .resid))+ \n  geom_point() + \n  geom_hline(yintercept=0) +\n  ggtitle(\"Residual plot for ln(price) as a function of bedrooms\")"},{"path":"reginf.html","id":"multiple-linear-regression","chapter":"8 Inference on Correlation & Regression","heading":"Multiple Linear Regression","text":"price variable large skew (ln() transformation helped residuals), following models use ln(price) response variable. happens try predict price (actually ln(price), ) using sqft bedrooms?Note: natural log function R log().Sqft & bedroomsAlthough \\(R^2\\) value went (44.84% variability log price explained sqft bedrooms), p-value bedrooms isn’t significant. p-value can interpreted hypothesis test slope coefficient given variables model.0.353 = P(slope -.06827 extreme sqft model relationship bedrooms price)output says sqft model, don’t actually need know anything number bedrooms (even though bedrooms significant predictor ).Sqft & bathroomsSeems like really don’t need bathrooms! information sqft sufficient predicting price, information bathrooms doesn’t help much .Final modelThe final model run log(price) using sqft. Note coefficients \\(R^2\\) values change slightly (original analysis) response variable logged.","code":"\nlm(log(price) ~ sqft + bedrooms, data=house) %>% tidy()## # A tibble: 3 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 12.2      0.174         70.1  1.39e-73\n## 2 sqft         0.000468 0.0000660      7.09 4.73e-10\n## 3 bedrooms    -0.0603   0.0572        -1.05 2.95e- 1\nlm(log(price) ~ sqft + bedrooms, data=house) %>% glance()## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.448         0.435 0.450      32.5 4.62e-11     2  -50.0  108.  118.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nlm(log(price) ~ sqft + baths, data=house) %>% tidy()## # A tibble: 3 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 12.1      0.151        80.0   4.07e-78\n## 2 sqft         0.000450 0.0000705     6.39  1.02e- 8\n## 3 baths       -0.0377   0.0746       -0.505 6.15e- 1\nlm(log(price) ~ sqft + baths, data=house) %>% glance()## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.443         0.429 0.452      31.8 7.07e-11     2  -50.4  109.  118.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nlm(log(price) ~ sqft + bedrooms + baths, data=house) %>% tidy()## # A tibble: 4 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept) 12.2      0.175        69.5   1.38e-72\n## 2 sqft         0.000463 0.0000719     6.45  8.32e- 9\n## 3 bedrooms    -0.0683   0.0730       -0.935 3.53e- 1\n## 4 baths        0.0168   0.0947        0.177 8.60e- 1\nlm(log(price) ~ sqft + bedrooms + baths, data=house) %>% glance()## # A tibble: 1 x 12\n##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n##       <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n## 1     0.449         0.428 0.453      21.4 2.98e-10     3  -49.9  110.  122.\n## # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nsummary(mod.lnsqft)## \n## Call:\n## lm(formula = log(price) ~ sqft, data = house)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.08988 -0.29591 -0.05899  0.28717  1.20206 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 1.204e+01  1.236e-01   97.36  < 2e-16 ***\n## sqft        4.274e-04  5.349e-05    7.99 7.87e-12 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4502 on 81 degrees of freedom\n## Multiple R-squared:  0.4407, Adjusted R-squared:  0.4338 \n## F-statistic: 63.83 on 1 and 81 DF,  p-value: 7.874e-12"},{"path":"reginf.html","id":"prediction","chapter":"8 Inference on Correlation & Regression","heading":"Prediction","text":"prediction intervals single sample, can now create intervals either average (confidence interval) individual (prediction interval).","code":""},{"path":"reginf.html","id":"confidence-interval-1","chapter":"8 Inference on Correlation & Regression","heading":"Confidence interval:","text":"95% confident true average log price 2000 sqft home 12.79 ln$ 12.99 ln$. (predicted value natural-log-dollars … hard interpret , back-transforming can little tricky beyond scope semester).","code":"\npredict(mod.lnsqft, newdata=data.frame(sqft=2000), interval=\"confidence\")##        fit      lwr      upr\n## 1 12.89125 12.79211 12.99038"},{"path":"reginf.html","id":"prediction-interval","chapter":"8 Inference on Correlation & Regression","heading":"Prediction interval:","text":"95% homes 2000 sqft 11.99 ln$ 13.79 ln$. Now back-transforming easy (averages), 95% homes 2000 sqft $161,126 $977,301 (just \\(e^{11.98994}\\) \\(e^{13.79255}\\)).","code":"\npredict(mod.lnsqft, newdata=data.frame(sqft=2000), interval=\"prediction\")##        fit      lwr      upr\n## 1 12.89125 11.98994 13.79255"},{"path":"reginf.html","id":"plotting-confidence-bounds","chapter":"8 Inference on Correlation & Regression","heading":"Plotting confidence bounds:","text":"confidence interval around line gives bounds parameter represented line. 95% confident true population line lies within bounds. Note interval wider endpoints (variability higher ends).prediction interval around line bounds individual points. , 95% observations captured inside interval. confidence interval, prediction interval also wider ends, harder see prediction intervals confidence intervals","code":"\nhouseConf <- mod.lnsqft %>% augment() %>%\n  cbind(predict(mod.lnsqft, interval=\"confidence\") )  # cbind binds the columns together\n\nhouseConf %>% head()##   log(price) sqft  .fitted       .hat    .sigma      .cooksd  .std.resid\n## 1   12.73376 3392 13.48618 0.03494894 0.4448768 5.240194e-02 -1.70116646\n## 2   13.71004 4100 13.78878 0.06748451 0.4529518 1.186788e-03 -0.18110383\n## 3   13.01398 3200 13.40412 0.02856505 0.4508768 1.136429e-02 -0.87917543\n## 4   12.38839 1436 12.65019 0.01862011 0.4520790 3.268479e-03 -0.58696923\n## 5   12.84133 1944 12.86731 0.01247712 0.4530341 2.130978e-05 -0.05807924\n## 6   12.61120 1500 12.67755 0.01744506 0.4529817 1.961713e-04 -0.14865344\n##        fit      lwr      upr\n## 1 13.48618 13.31871 13.65366\n## 2 13.78878 13.55606 14.02150\n## 3 13.40412 13.25272 13.55553\n## 4 12.65019 12.52795 12.77244\n## 5 12.86731 12.76725 12.96738\n## 6 12.67755 12.55923 12.79587\nggplot(houseConf, aes(x=sqft)) + \n  geom_point(aes(y=`log(price)`)) + \n  geom_line(aes(y=.fitted)) +\n  geom_ribbon(aes(ymin=lwr, ymax=upr), fill=\"blue\", alpha=0.2)\nhousePred <- mod.lnsqft %>% augment() %>%\n  cbind(predict(mod.lnsqft, interval=\"prediction\") )  # cbind binds the columns together\n\nhousePred %>% head()##   log(price) sqft  .fitted       .hat    .sigma      .cooksd  .std.resid\n## 1   12.73376 3392 13.48618 0.03494894 0.4448768 5.240194e-02 -1.70116646\n## 2   13.71004 4100 13.78878 0.06748451 0.4529518 1.186788e-03 -0.18110383\n## 3   13.01398 3200 13.40412 0.02856505 0.4508768 1.136429e-02 -0.87917543\n## 4   12.38839 1436 12.65019 0.01862011 0.4520790 3.268479e-03 -0.58696923\n## 5   12.84133 1944 12.86731 0.01247712 0.4530341 2.130978e-05 -0.05807924\n## 6   12.61120 1500 12.67755 0.01744506 0.4529817 1.961713e-04 -0.14865344\n##        fit      lwr      upr\n## 1 13.48618 12.57483 14.39754\n## 2 13.78878 12.86321 14.71435\n## 3 13.40412 12.49558 14.31266\n## 4 12.65019 11.74606 13.55433\n## 5 12.86731 11.96591 13.76872\n## 6 12.67755 11.77393 13.58116\nggplot(housePred, aes(x=sqft)) + \n  geom_point(aes(y=`log(price)`)) + \n  geom_line(aes(y=.fitted)) +\n  geom_ribbon(aes(ymin=lwr, ymax=upr), fill=\"blue\", alpha=0.2)"},{"path":"reginf.html","id":"predicting-with-more-than-one-explanatory-variable","chapter":"8 Inference on Correlation & Regression","heading":"Predicting with more than one explanatory variable:","text":"predict function still works give estimates average value predicted individual values, plot now much harder draw three explanatory variables, need 4-d plot visualize model predictions., hard back-transform prediction average (end thinking median), can back-transform interval individual prices. 95% homes 2000sqft, 3 bedrooms, 2 baths cost $164,312 $1,011,356.","code":"\nsqftbedbathlm = lm(log(price)~sqft + bedrooms + baths, data=house)\n\npredict(sqftbedbathlm, \n        newdata=data.frame(sqft=2000, bedrooms=3, baths=2), \n        interval=\"confidence\", level=.95)##        fit      lwr      upr\n## 1 12.91816 12.80085 13.03548\npredict(sqftbedbathlm, \n        newdata=data.frame(sqft=2000, bedrooms=3, baths=2), \n        interval=\"prediction\", level=.95)##        fit      lwr     upr\n## 1 12.91816 12.00953 13.8268"},{"path":"reginf.html","id":"ex:1819flu","chapter":"8 Inference on Correlation & Regression","heading":"8.3.3 Example: 1918-19 Flu and Excess Deaths29","text":"coming know, measuring impact COVID-19 difficult. recent NYTimes article compares total deaths 2020 compared 2015-2019 8 different regions.\nFigure 8.1: US Death Toll 2020, NY Times, April 28, 2020, https://www.nytimes.com/interactive/2020/04/28/us/coronavirus-death-toll-total.html\nstill early model COVID-19 well, information 1918-19 Flu pandemic similar many ways current COVID-19 pandemic. 2007, Markel et al. published research Journal American Medical Association detailing results different social distancing practices across US, “Nonpharmaceutical Interventions Implemented US Cities 1918-1919 Influenza Pandemic” [JAMA, Aug 8, 2007, Vol 298, 6].\nFigure 8.2: Nonpharmaceutical Interventions Implemented US Cities 1918-1919 Influenza Pandemic, https://jamanetwork.com/journals/jama/fullarticle/208354\nconclusions worth restating:Conclusions findings demonstrate strong association early, sustained, layered application nonpharmaceutical interventions mitigating consequences 1918-1919 influenza pandemic United States. planning future severe influenza pandemics, nonpharmaceutical interventions considered inclusion companion measures developing effective vaccines medications prophylaxis treatment.entire paper fascinating great job describing different interventions related outcomes, focus regression analysis done model excess death rate. data used come directly page 647 manuscript.\nFigure 8.3: Note figure 1b seems y-axis mis-labelled (magnitude first mortality peak). Table 4 uses test linear regression (Wilcoxon rank sum test – two sample test means done ranked data rather raw data) compare outcome variables broken two groups: median response time median response time.\n","code":"\nflu_1819 <- read_csv(\"1918_1919flu.csv\", \n                     col_types = cols(`Date of peak Excess death rate` = col_date(format = \"%m/%d/%y\")))\n\nnames(flu_1819) <- c(\"place\", \"responseTime\", \"daysNonpharm\", \"datePeak\", \n                     \"timePeak\", \"magPeak\", \"excessDeaths\")\n\nhead(flu_1819)## # A tibble: 6 x 7\n##   place       responseTime daysNonpharm datePeak   timePeak magPeak excessDeaths\n##   <chr>              <dbl>        <dbl> <date>        <dbl>   <dbl>        <dbl>\n## 1 Albany, NY             3           47 2018-10-24       15   162.          553.\n## 2 Baltimore,…           10           43 2018-10-18        9   182.          559.\n## 3 Birmingham…            9           48 2018-10-22       13    70.9         592.\n## 4 Boston, MA            13           50 2018-10-03        8   160.          710 \n## 5 Buffalo, NY           12           49 2018-10-22       12   141.          530.\n## 6 Cambridge,…           14           49 2018-10-03        8   126.          541"},{"path":"reginf.html","id":"correlation-between-variables","chapter":"8 Inference on Correlation & Regression","heading":"Correlation between variables","text":"consistent manuscript, Spearman correlation used instead Pearson. Spearman Pearson correlation applied ranks observations (instead raw values observations). Calculating Spearman impact lessening influence outlying observations.correlations calculated match values manuscript, repeat analysis without St Paul, MN Grand Rapids, MI see impact analysis. remove two cities without justifiable reason (something makes fundamentally different cities, worth including model); however, worth re-calculations just investigate impact individual observations. seems impact (e.g., excessDeaths responseTime) possibly impact moderate.","code":"\nflu_1819 %>%\n  select(excessDeaths, magPeak, responseTime, daysNonpharm, timePeak) %>%\n  cor(method=\"spearman\")##              excessDeaths    magPeak responseTime daysNonpharm   timePeak\n## excessDeaths    1.0000000  0.7639686    0.3658238   -0.3925996 -0.1619664\n## magPeak         0.7639686  1.0000000    0.3090137   -0.5739853 -0.2541940\n## responseTime    0.3658238  0.3090137    1.0000000   -0.6808529 -0.7346764\n## daysNonpharm   -0.3925996 -0.5739853   -0.6808529    1.0000000  0.6135956\n## timePeak       -0.1619664 -0.2541940   -0.7346764    0.6135956  1.0000000\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place !=  \"Grand Rapids, MI\") %>%\n  select(excessDeaths, magPeak, responseTime, daysNonpharm, timePeak) %>%\n  cor(method=\"spearman\")##              excessDeaths    magPeak responseTime daysNonpharm   timePeak\n## excessDeaths    1.0000000  0.7402439    0.5093635   -0.4663501 -0.2989616\n## magPeak         0.7402439  1.0000000    0.4603040   -0.6752625 -0.4170257\n## responseTime    0.5093635  0.4603040    1.0000000   -0.6638306 -0.6969834\n## daysNonpharm   -0.4663501 -0.6752625   -0.6638306    1.0000000  0.6007545\n## timePeak       -0.2989616 -0.4170257   -0.6969834    0.6007545  1.0000000"},{"path":"reginf.html","id":"model-building","chapter":"8 Inference on Correlation & Regression","heading":"Model building","text":", St Paul Grand Rapids removed model. , reporting analysis (JAMA authors correctly), two cities removed without justifiable reason (something makes fundamentally different cities, worth including model); however, worth re-calculations just investigate impact individual observations.Note predicting excessDeaths magPeak significant variable daysNonpharm. variable adds significantly model","code":"\n# excessDeaths as the response variable:\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(excessDeaths ~ responseTime + daysNonpharm, data = .) %>%\n  tidy()## # A tibble: 3 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   552.       67.0        8.24 5.50e-10\n## 2 responseTime    5.57      3.90       1.43 1.62e- 1\n## 3 daysNonpharm   -0.828     0.503     -1.65 1.08e- 1\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(excessDeaths ~  daysNonpharm, data = .) %>%\n  tidy()## # A tibble: 2 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    628.      41.7       15.0  7.92e-18\n## 2 daysNonpharm    -1.25     0.412     -3.03 4.28e- 3\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(excessDeaths ~  responseTime, data = .) %>%\n  tidy()## # A tibble: 2 x 5\n##   term         estimate std.error statistic  p.value\n##   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)    452.       28.8      15.7  1.90e-18\n## 2 responseTime     9.35      3.23      2.90 6.14e- 3\n# magPeak as the response variable:\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(magPeak ~ responseTime + daysNonpharm, data = .) %>%\n  tidy()## # A tibble: 3 x 5\n##   term         estimate std.error statistic       p.value\n##   <chr>           <dbl>     <dbl>     <dbl>         <dbl>\n## 1 (Intercept)  167.        21.0      7.95   0.00000000132\n## 2 responseTime  -0.0555     1.22    -0.0455 0.964        \n## 3 daysNonpharm  -0.715      0.157   -4.55   0.0000541\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(log(magPeak) ~ responseTime + daysNonpharm, data = .) %>%\n  tidy()## # A tibble: 3 x 5\n##   term          estimate std.error statistic  p.value\n##   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   5.20       0.211     24.6    5.69e-25\n## 2 responseTime -0.000741   0.0123    -0.0603 9.52e- 1\n## 3 daysNonpharm -0.00769    0.00158   -4.86   2.05e- 5"},{"path":"reginf.html","id":"checking-residuals","chapter":"8 Inference on Correlation & Regression","heading":"Checking Residuals","text":"Turns residuals models aren’t great (maybe authors used ranked based methods like Spearman correlation Wilcoxon rank sum test??? – tests different technical conditions, based normal theory!)","code":"\n# excessDeaths as the response variable:\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(excessDeaths ~  daysNonpharm, data = .) %>%\n  augment() %>%\n  ggplot() +\n  geom_point(aes(x=.fitted, y = .resid)) + \n  geom_hline(yintercept = 0) +\n  ggtitle(\"St Paul & Grand Rapids removed, excessDeath vs days Nonparm\")\nflu_1819 %>%\n  lm(excessDeaths ~  daysNonpharm, data = .) %>%\n  augment() %>%\n  ggplot() +\n  geom_point(aes(x=.fitted, y = .resid)) + \n  geom_hline(yintercept = 0) +\n  ggtitle(\"St Paul & Grand Rapids NOT removed, excessDeath vs days Nonparm\")\n# magPeak as the response variable:\nflu_1819 %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(magPeak ~ responseTime + daysNonpharm, data = .) %>%\n  augment() %>%\n  ggplot() +\n  geom_point(aes(x=.fitted, y = .resid)) + \n  geom_hline(yintercept = 0) +\n  ggtitle(\"St Paul & Grand Rapids removed, magPeak vs days responseTime\")\nflu_1819 %>%\n  lm(magPeak ~ responseTime + daysNonpharm, data = .) %>%\n  augment() %>%\n  ggplot() +\n  geom_point(aes(x=.fitted, y = .resid)) + \n  geom_hline(yintercept = 0) +\n  ggtitle(\"St Paul & Grand Rapids NOT removed, magPeak vs days responseTime\")\nflu_1819 %>%\n  mutate(ln_magPeak = log(magPeak)) %>%\n  filter(place != \"St Paul, MN\" & place != \"Grand Rapids, MI\") %>%\n  lm(ln_magPeak ~ responseTime + daysNonpharm, data = .) %>%\n  augment() %>%\n  ggplot() +\n  geom_point(aes(x=.fitted, y = .resid)) + \n  geom_hline(yintercept = 0) +\n  ggtitle(\"St Paul & Grand Rapids removed, log(magPeak) vs days responseTime\")"},{"path":"reginf.html","id":"what-else-do-we-know-about-covid-19-right-now","chapter":"8 Inference on Correlation & Regression","heading":"What else do we know about COVID-19 right now?","text":"Clinical trials starting show assessing know anti-viral pharmaceutical interventions patients COVID-19.","code":""},{"path":"reginf.html","id":"study-1","chapter":"8 Inference on Correlation & Regression","heading":"Study 1:","text":"\nFigure 8.4: Remdesivir adults severe COVID-19: randomised, double-blind, placebo-controlled, multicentre trial, https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31022-9/fulltext\nFindings Feb 6, 2020, March 12, 2020, 237 patients enrolled randomly assigned treatment group (158 remdesivir 79 placebo); one patient placebo group withdrew randomisation included ITT population. Remdesivir use associated difference time clinical improvement (hazard ratio 1·23 [95% CI 0·87–1·75]). Although statistically significant, patients receiving remdesivir numerically faster time clinical improvement receiving placebo among patients symptom duration 10 days less (hazard ratio 1·52 [0·95–2·43]). Adverse events reported 102 (66%) 155 remdesivir recipients versus 50 (64%) 78 placebo recipients. Remdesivir stopped early adverse events 18 (12%) patients versus four (5%) patients stopped placebo early.animal studies shown remdesivir inhibits SARS-CoV-2 replicationstudy randomized, double-blind, placebo-controlled, multi-centeradults laboratory-confirmed SARS-CoV-2 infection, low oxygen, pneumoniarandomly assigned 2:1 ratio remdesivir vs placebonot statistically significant !patients remdesivir numerically faster time clinical improvementtrial stopped enrolling expected number patients outbreak COVID-19 brought control","code":""},{"path":"reginf.html","id":"study-2","chapter":"8 Inference on Correlation & Regression","heading":"Study 2:","text":"Gilead, large pharmaceutical company, issued press release describing results clinical trial involving remdesivir. mostly seeking understand long treatment continue adverse effects drug.\nFigure 8.5: Gilead press release (peer reviewed publication yet), https://www.gilead.com/news--press/press-room/press-releases/2020/4/gilead-announces-results--phase-3-trial--investigational-antiviral-remdesivir--patients--severe-covid-19https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31022-9/fulltext\nstudy, time clinical improvement 50 percent patients 10 days 5-day treatment group 11 days 10-day treatment group. half patients treatment groups discharged hospital Day 14 (5-day: 60.0%, n=120/200 vs.10-day: 52.3% n=103/197; p=0.14). Day 14, 64.5 percent (n=129/200) patients 5-day treatment group 53.8 percent (n=106/197) patients 10-day treatment group achieved clinical recovery.two patient groups 5-day 10-day treatment courses (placebo), randomizedpatients ventilators enrolledThe NY Times covered Gilead announement. Indeed, outcome trials important political ramifications well.\nFigure 8.6: Gilead remdesivir trial covered NYT, https://www.nytimes.com/2020/04/29/health/gilead-remdesivir-coronavirus.html\n","code":""},{"path":"reginf.html","id":"reflection-questions-5","chapter":"8 Inference on Correlation & Regression","heading":"8.4 Reflection Questions","text":"","code":""},{"path":"reginf.html","id":"correlation-simple-linear-regression-ims-sections-8.1-8.2","chapter":"8 Inference on Correlation & Regression","heading":"8.4.1 Correlation & Simple Linear Regression: IMS Sections 8.1 & 8.2","text":"Describe linear model multiple variables.Describe error / residual term calculated multiple variables.(three-ish) statistics interest chapter? parameters interest?correlation measure?find values \\(b_0\\) \\(b_1\\) estimating least squares line?dangerous extrapolate?interpret \\(R^2\\)? ?mean say \\(b_1\\) sampling distribution? never talk sampling distribution \\(\\beta_1\\)?need LINE technical conditions inference parts analysis estimation parts analysis?linear regression always appropriate comparing two continuous variables?LINE technical conditions? conditions assessed?three factors influence \\(SE(b_1)\\)? (Note: something influences \\(SE(b_1)\\), means inference also effected. huge \\(SE(b_1)\\), hard tell slope significant t value small.)mean randomization test slope? , explain process randomization test . (See shuffle options Analyzing Two Quantitative Variables applet.)someone transform either variables?difference confidence interval prediction interval? bigger? make sense? centers intervals differ? (don’t. ?)","code":""},{"path":"reginf.html","id":"multiple-linear-regression-ims-chapter-4-section-8.4","chapter":"8 Inference on Correlation & Regression","heading":"8.4.2 Multiple Linear Regression: IMS Chapter 4 & Section 8.4","text":"Describe linear model multiple variables.Describe error / residual term calculated multiple variables.model change multiple variables included?p-values interpreted now multiple variables?\\(R^2\\) interpreted? difference \\(R^2\\) \\(R^2_{adj}\\)?variables chosen final model?model conditions assessed?","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
