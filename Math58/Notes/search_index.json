[
["index.html", "Introduction to (Bio)Statistics Class Information", " Introduction to (Bio)Statistics Jo Hardin 2020-06-05 Class Information Class notes for Math 58(B) at Pomona College: Introduction to (Bio)Statistics. The notes are based extensively on “Introductory Statistics with Randomization and Simulation” by Diez, Barr, and etinkaya-Rundel (Diez, Barr, and Çetinkaya-Rundel 2014). We will also be using many examples (and applets) from “Investigating Statistics, Concepts, Applications, and Methods” by Chance and Rossman (Chance and Rossman 2018). You are responsible for reading the relevant chapters in the text. The text is very good &amp; readable, so you should use it. Also, you will have a much deeper understanding of the material if you spend time working through the applets at http://www.rossmanchance.com/iscam3/files.html. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chap Notes Section 1/21/20 Intro to Data / R 2 [intro], 2.1 [Jan21] 1/23/20 Foundations for Inference ISRS 2.1-2.3 3.1 [Jan23], 3.2 [Examp: gender] 1/28/20 Normality ISRS 2.4-2.5 3.4 [Jan28], 3.5.1 [CLT] 1/30/20 Normality ISRS 2.6-2.7 3.6 [Jan30], 3.6.1 [Normal Dist] 2/4/20 Confidence Intervals ISRS 2.8 3.7 [Feb4], 3.8 [CI] 2/6/20 Confidence Intervals ISRS 2.8 3.9 [Feb6], 3.8.3 [modifying CI] 2/11/20 Sampling ISRS 1.3-1.4 3.10 [Feb11], 3.11 sampling 2/13/20 Errors &amp; Power ISRS 3.1 &amp; 2.3 3.12 [Feb13], 3.13 [Errors&amp;Power] 2/18/20 58: Binomial distrib, 58B: RR &amp; OR ISCAM Chp 1 4.2 [Feb18 M58], 4.5 [Feb18 M58B] 2/20/20 58: Binomial Power, 58B: CIs for RR &amp; OR ISCAM 3.9-3.11 4.3 [Feb20 M58], 4.6 [Feb20 M58B] 2/25/20 Two binary variables ISRS 3.2 4.9 [Feb25] 4.10 [Diff 2 Prop] 2/27/20 Experiments ISRS 1.4 &amp; 1.5 4.11 [Feb27] 4.12 [Experiments 3/3/20 chi-sq one variable ISRS 3.3 4.13 [Mar3] 4.14 [ChiSq 1 Var] 3/5/20 chi-sq two variables ISRS 3.4 4.15 [Mar5] 4.16 [ChiSq 2 Vars] 3/10/20 review for Exam 1 3/12/20 Exam 1 3/17/20 Spring Break 1 3/19/20 Spring Break 1 3/24/20 Spring Break 2 Census 4.20 Census 3/26/20 Spring Break 2 COVID-19 4.22 COVID-19 3/31/20 Sampling Dist of \\(\\overline{X}\\) ISRS 4.1 &amp; Inv 2.4 5.1 [Mar31] 5.2.2 [Dist 1 mean] 4/2/20 Inference on \\(\\mu\\) ISRS 4.1 &amp; Inv 2.5 5.3 [Apr2] 5.5 [Inf 1 mean] 4/7/20 Prediction Intervals Inv 2.6 5.4 [Apr7] 5.5.1.3 [Pred Int] 4/9/20 Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) ISRS 4.3 &amp; Inv 4.2 5.6 [Apr9] 5.8 [Inf 2 means] 4/14/20 Inference on \\(\\mu_1 - \\mu_2\\) ISRS 4.3 &amp; Inv 4.5 - 4.6 5.7 [Apr14] 5.8 [Inf 2 means] 4/16/20 Correlation (r) ISRS 5.1 &amp; Inv 5.6 - 5.7 6.1 [Apr16] 6.2 [Cor] 4/21/20 Least Squares line ISRS 5.2 &amp; Inv 5.10 - 5.11 6.3 [Apr21] 6.5.1 [LS line] 4/23/20 Inference on \\(\\beta_1\\) ISRS 5.3 - 5.4 &amp; Inv 5.13 - 5.14 6.4 [Apr23] 6.5.2 [Inf \\(\\beta_1\\)] 4/28/20 Multiple Linear Regression ISRS 6.1 - 6.2 6.6 [Apr28] 6.8 [MLR] 4/30/20 Model Selection ISRS 6.3 - 6.4 6.7 [Apr30] 6.8.1 [MLR model] 5/5/20 review 6.9.3 [1918-19 Flu] References "],
["rfunc.html", "Chapter 1 R functions 1.1 Applets 1.2 Data Structure 1.3 Wrangling 1.4 Plotting 1.5 Statistical Inference 1.6 Probability models", " Chapter 1 R functions To help us navigate / remember when to use what, the following sections consolidate some of the R functions used in class and on assignments. 1.1 Applets The main source of in-class applets has come from Chance and Rossman (2018) and can be found: http://www.rossmanchance.com/iscam3/files.html In Math 58B we also use the RR/OR applet by Ken Kleinman at: https://kenkleinman.shinyapps.io/odds-tool/ 1.2 Data Structure Always, it is important to understand the format of the data. For example, how many rows (observational units)? How many columns (variables)? Are the variables numbers or categories? There are many ways to see the data, and it is highly recommended that you regularly check back to remind yourself of the data structure. glimpse() prints the data with variable types (but makes the columns into rows) names() prints the column (variable) names str() is like glimpse() but provides a little more information about the structure of the dataframe head() prints the first few rows of the dataframe (tail() prints the last few rows) click on the “environment” tab, then click on the name of the dataframe to see the data in the console 1.3 Wrangling Data wrangling is used when working to change data in one format to another. We have regularly used the pipe function (%&gt;%) to layer commands. Data wrangling will be an even bigger part of the data analysis pipeline when we start to work with continuous variables (e.g., height). The pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example: x %&gt;% f(y) is the same as f(x, y) y %&gt;% f(x, ., z) is the same as f(x,y,z) A great source of help is the data wrangling cheatsheet here: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Data verbs take data tables as input and give data tables as output (that’s how we can use the chaining syntax!). The functions below are from the R package dplyr, and they will be used to do much of the data wrangling. Below is a list of verbs which will be helpful in wrangling many different types of data. sample_n() take a random row(s) head() grab the first few rows tail() grab the last few rows filter() removes unwanted cases arrange() reorders the cases select() removes unwanted variables (and rename()) distinct() returns the unique values in a table mutate() transforms the variable (and transmute() like mutate, returns only new variables) group_by() tells R that SUCCESSIVE functions keep in mind that there are groups of items. So group_by() only makes sense with verbs later on (like summarize()). summarize() collapses a data frame to a single row. Some functions that are used within summarize() include: min(), max(), mean(), sum(), sd(), median(), and IQR() n(): number of observations in the current group n_distinct(x): count the number of unique values in the variable (column) called x first_value(x), last_value(x) and nth_value(x, n): work similarly to x[1], x[length(x)], and x[n] If you happen to be using a function that exists in dplyr and in a different package, you’ll want to tell the computer to use the appropriate function. For example, dplyr::filter(). 1.4 Plotting The R package ggplot2 will be used for all visualizations. Remember that the layers of a plot are put together with the + symbol (instead of the %&gt;% command). A great source of help is the data visualization cheatsheet here: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Each plot starts with ggplot(data) and then adds layers. The minimal additional layer is a geom_XXX() layer which describes the geometry of the plot. Some things to notice: when layering graph pieces, use +. (When layering data wrangling, use %&gt;%.) geom_XXX will put the XXX-type-of-plot onto the graph. aes is the function which takes the data columns and puts them onto the graph. aes is used only with data columns and you always need it if you are working with data variables. A full set of types of plots is given here: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf (and in many other places online). If you happen to be using a function that exists in ggplot2 and in a different package, you’ll want to tell the computer to use the appropriate function. For example, ggplot2::xlim(). 1.5 Statistical Inference The main simulation tools we have used for creating null distributions come from the R package infer. There are many examples available on the infer vignette page: https://infer-dev.netlify.com/index.html Typically, the following steps are followed: calculate the test statistic teststat &lt;- data %&gt;% specify(variable information) %&gt;% calculate(the form of the statistic) create the null values of the statistic nullstats &lt;- data %&gt;% specify(variable information) %&gt;% hypothesize(give information about the type of null hypothesis) %&gt;% generate(repeat the process, provide info about the process) %&gt;% calculate(the form of the statistic) visualize the null sampling distribution (of the statistic) nullstats %&gt;% visualize() visualize the null sampling distribution with the observed statistic overlaid nullstats %&gt;% visualize() + shade_p_value(specify where the observed statistics is) calculate the p-value nullstats %&gt;% get_p_value(specify the observed statistic and the direction of the test) If you happen to be using a function that exists in infer and in a different package, you’ll want to tell the computer to use the appropriate function. For example, infer::specify(). 1.6 Probability models Generally, we’ve used the mosaic package which calculates probabilities and adds a graphical representation so that the calculated values can be checked against your intuition. Some of the functions we’ve used include: xpnorm normal probability xqnorm normal quantile (also called: cutoff, z*) xpbinom binomial probability If you happen to be using a function that exists in mosaic and in a different package, you’ll want to tell the computer to use the appropriate function. For example, mosaic::xpnorm(). References "],
["intro.html", "Chapter 2 Introduction 2.1 1/21/20 Agenda 2.2 Course Logistics 2.3 Example: Friend or Foe", " Chapter 2 Introduction 2.1 1/21/20 Agenda Syllabus &amp; Course Outline Example: Friend or Foe 2.2 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Figure 2.1: Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. [Also sometimes called an estimate.] A parameter is a numerical measurement of the population. We never know the true value of the parameter. What is the content of Math 58(B)? This class will be an introduction to statistical ideas using R. We will cover the majority of statistical methods which are used in standard analyses (e.g., t-tests, chi-squared analysis, confidence intervals, binomial tests, etc.). The main inferential techniques will be covered using both theoretical approximations (e.g., the central limit theorem) as well as computational methods (e.g., permutation tests and bootstrapping). Focus will be on understanding he methods and interpreting results. Our goal in this course is to learn how to better evaluate quantitative information with regards to data. We’ll be sure to keep in mind: What is the difference between Math 58 and Math 58B? The two classes are remarkably similar in content and structure. Indeed, there are more similarities to the classes than there are differences. The main differences have to do with a handful of topics which are different across the two classes. topic for Math 58 topic for Math 58B early in the semester Binomial probabilities relative risk &amp; odds ratios later in the semester introduction to multiple regression introduction to logistic regression Who should take Math 58(B)? Every educated citizen should have a basic understanding of statistics. Ok, ok, I have my own biases, but I’m not the only person who thinks this! (https://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education) In terms of your academic interests, you should take introductory statistics if you would like to take upper division statistics or if you are planning to analyze data in a field outside of statistics (e.g., biology, EA, psychology, etc.). Upper division statistics courses require introductory statistics, and it is not easy to just “learn” statistics on your own over the summer. I highly recommend taking an introductory statistics course. If you have already taken AP Statistics, you may or may not want to repeat the material. If you had a strong course with an excellent teacher and scored well on the exam, you probably do not need to repeat the material. If you are uncertain about many of the concepts, then you may want to re-take the course before jumping into upper division statistics courses. We will use R extensively, and you probably didn’t use R in your AP Statistics classes. Most upper division statistics classes will expect you to be able to jump into R head first, and introductory statistics gives a more gentle introduction to R. What are the prerequisites for Math 58(B)? The formal prerequisite is a semester of calculus, but we do almost no calculus in the entire semester. However, a student in Math 58(B) should be quantitatively inclined and ready to see many new mathematical, algorithmic, and computational ideas quickly throughout the semester. Is there overlap with other classes? There is considerable overlap between Math 58 and Math 58B; you should not take both Math 58 and Math 58B. The differences between the two sections lie in the examples as well as a handful of topics that are different across the two courses. There is also quite a bit of overlap with other introductory statistics courses (e.g., Econ 57, Pysch 158, Politics 90, AP Statistics). Some introductory statistics courses cover quite a bit of probability without getting deeply into inferential ideas. We will focus on statistics instead of probability with an emphasis on understanding the intuition and mathematical derivations that inform the analysis tool. We will also focus on how the computer can help us gain a deeper understanding of the analyses we are doing. When should I take Math 58 or Math 58B? Introductory (Bio)Statistics should be taken as early in your undergraduate schedule as possible. By taking Math 58(B) you will open up the possibilities for taking upper division statistics classes. Additionally, the background covered in Math 58(B) will provide you with a deeper understanding of the concepts you are covering in your science and social science courses. What is the workload for Math 58(B)? Math 58(B) meets twice a week for 75 min for lecture and once a week for an hour for lab. Every week there will be one homework and one lab assignment. There are two midterm exams, each with an in-class and take-home section. The final exam will also have in-class and take-home parts. The class is not known to be extremely difficult or time consuming; however, it does require that you stay up with the material, do all of the assignments, and come to all class meetings (participation is a part of your grade). What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Figure 2.2: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 2.3: Jessica Ward, PhD student at Newcastle University 2.3 Example: Friend or Foe This example comes from Investigation 1.1: Friend or Foe? Chance and Rossman (2018). The idea is to use simulation to determine how likely our data would be if nothing interesting was going on. In a study reported in the November 2007 issue of Nature, researchers investigated whether infants take into account an individual’s actions towards others in evaluating that individual as appealing or aversive, perhaps laying for the foundation for social interaction (Hamlin, Wynn, and Bloom, 2007). In other words, do children who aren’t even yet talking still form impressions as to someone’s friendliness based on their actions? In one component of the study, 10-month-old infants were shown a “climber” character (a piece of wood with “googly” eyes glued onto it) that could not make it up a hill in two tries. Then the infants were shown two scenarios for the climber’s next try, one where the climber was pushed to the top of the hill by another character (the “helper” toy) and one where the climber was pushed back down the hill by another character (the “hinderer” toy). The infant was alternately shown these two scenarios several times. Then the child was presented with both pieces of wood (the helper and the hinderer characters) and asked to pick one to play with. Videos demonstrating this component of the study can be found at http://campuspress.yale.edu/infantlab/media/. One important design consideration to keep in mind is that in order to equalize potential influencing factors such as shape, color, and position, the researchers varied the colors and shapes of the wooden characters and even on which side the toys were presented to the infants. The researchers found that 14 of the 16 infants chose the helper over the hinderer. Always Ask What are the observational units? infants What is the variable? What type of variable? choice of helper or hindered: categorical What is the statistic? \\(\\hat{p}\\) = proportion of infants who chose helper = 14/16 = 0.875 What is the parameter? p = proportion of all infants who might choose helper (not measurable!) p-value is the probability of our data or more extreme if nothing interesting is going on. completely arbitrary cutoff \\(\\rightarrow\\) generally accepted conclusion p-value \\(&gt;\\) 0.10 \\(\\rightarrow\\) no evidence against the null model 0.05 \\(&lt;\\) p-value \\(&lt;\\) 0.10 \\(\\rightarrow\\) moderate evidence against the null model 0.01 \\(&lt;\\) p-value \\(&lt;\\) 0.05 \\(\\rightarrow\\) strong evidence against the null model p-value \\(&lt;\\) 0.01 \\(\\rightarrow\\) very strong evidence against the null model Computation library(infer) # to control the randomness set.seed(47) # first create a data frame with the Infant data Infants &lt;- read.delim(&quot;http://www.rossmanchance.com/iscam3/data/InfantData.txt&quot;) Infants %&gt;% head() ## choice ## 1 helper ## 2 hinderer ## 3 helper ## 4 helper ## 5 helper ## 6 helper # then find the proportion who help (p_obs &lt;- Infants %&gt;% specify(response = choice, success = &quot;helper&quot;) %&gt;% calculate(stat = &quot;prop&quot;) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.875 # now apply the infer framework to get the null proportion null_help &lt;- Infants %&gt;% specify(response = choice, success = &quot;helper&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = .5) %&gt;% generate(reps = 1000, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) # then visualize the null sampling distribution &amp; p-value visualize(null_help, bins = 13) + shade_p_value(obs_stat = p_obs, direction = &quot;two_sided&quot;) # calculate the actual p-value null_help %&gt;% get_p_value(obs_stat = p_obs, direction = &quot;two_sided&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.002 Logic for what we believe If we look back to the study, we can tell that the researchers varied color, shape, side, etc. to make sure there was nothing systematic about how the infants chose the block (e.g., if they all watch Blue’s Clues they might love the color blue, so we wouldn’t always want the helper shape to be blue). The excellent design survey rules out outside influence as the reason so many of the infants chose the helper shape. We ruled out random chance as the mechanism for the larger number of infants who chose the helper shape. (We reject the null hypothesis.) We conclude that babies are inclined to be helpful. That is, they are more likely to choose the helper than the hindered. [Note: we don’t have any evidence for why they choose the helper. That is, they might be predisposed. They might be modeling their parents. They might notice that they need a lot of help, etc.] References "],
["foundations-for-inference.html", "Chapter 3 Foundations for Inference 3.1 1/23/20 Agenda 3.2 Example: Gender Discrimination 3.3 Structure of Hypothesis testing 3.4 1/28/20 Agenda 3.5 Normal Model 3.6 1/30/20 Agenda 3.7 2/4/20 Agenda 3.8 Confidence Intervals 3.9 2/6/20 Agenda 3.10 2/11/20 Agenda 3.11 Sampling 3.12 2/13/20 Agenda 3.13 Errors &amp; Power 3.14 Reflection Questions", " Chapter 3 Foundations for Inference 3.1 1/23/20 Agenda Example: gender discrimination infer again Hypothesis testing structure 3.2 Example: Gender Discrimination We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.1 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?” The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects. For each supervisor we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in Table 2.1, we would like to evaluate if females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against. (Diez, Barr, and Çetinkaya-Rundel (2014), pg 61) decision promoted not promoted total male 21 3 24 female 14 10 24 total 35 13 48 Always Ask What are the observational units? supervisor What are the variables? What type of variables? whether the resume was male or female (categorical) decision to promote or not promote (categorical) What is the statistic? \\(\\hat{p}_m - \\hat{p}_f\\) = 21/24 - 14/24 = 0.292 (the difference between the proportion of men who were promoted and the proportion of women who were promoted) What is the parameter? \\(p_m - p_f\\) = the true difference in the probability of a man being promoted minus the probability of a woman being promoted. Hypotheses H0: Null hypothesis. The variables gender and decision are independent. They have no relationship, and therefore any observed difference between the proportion of males and females who were promoted is due to chance. HA: Alternative hypothesis. The variables gender and decision are not independent. Any observed difference between the proportion of males and females who were promoted is not due to chance. Computation library(infer) # to control the randomness set.seed(47) # first create a data frame with the discrimination data discrim &lt;- data.frame(gender = c(rep(&quot;male&quot;, 24), rep(&quot;female&quot;, 24)), decision = c(rep(&quot;promote&quot;, 21), rep(&quot;not&quot;, 3), rep(&quot;promote&quot;, 14), rep(&quot;not&quot;, 10))) discrim %&gt;% head() ## gender decision ## 1 male promote ## 2 male promote ## 3 male promote ## 4 male promote ## 5 male promote ## 6 male promote # then find the difference in proportion who are promoted (diff_obs &lt;- discrim %&gt;% specify(decision ~ gender, success = &quot;promote&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.292 # now apply the infer framework to get the null differences in proportions null_discrim &lt;- discrim %&gt;% specify(decision ~ gender, success = &quot;promote&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) # then visualize the null sampling distribution &amp; p-value visualize(null_discrim, bins = 10) + shade_p_value(obs_stat = diff_obs, direction = &quot;greater&quot;) # calculate the actual p-value null_discrim %&gt;% get_p_value(obs_stat = diff_obs, direction = &quot;greater&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.026 Logic for what we believe We know that the study was an experiment, so there should be no systematic differences between the group who received “male” applications and “female” applications. We’ve ruled out random chance as the reason for the huge difference in proportions. (We reject the null hypothesis.) if we lived in the null reality, we’d only see data like these about 2.5% of the time. We conclude that gender and decision are not independent. That is, knowing the gender changes the probability of promotion. 3.3 Structure of Hypothesis testing 3.3.1 Hypotheses Hypothesis Testing compares data to the expectation of a specific null hypothesis. If the data are unusual, assuming that the null hypothesis is true, then the null hypothesis is rejected. The Null Hypothesis, \\(H_0\\), is a specific statement about a population made for the purposes of argument. A good null hypothesis is a statement that would be interesting to reject. The Alternative Hypothesis, \\(H_A\\), is a specific statement about a population that is in the researcher’s interest to demonstrate. Typically, the alternative hypothesis contains all the values of the population that are not included in the null hypothesis. In a two-sided (or two-tailed) test, the alternative hypothesis includes values on both sides of the value specified by the null hypothesis. In a one-sided (or one-tailed) test, the alternative hypothesis includes parameter values on only one side of the value specified by the null hypothesis. \\(H_0\\) is rejected only if the data depart from it in the direction stated by \\(H_A\\). 3.3.2 Other pieces of the process A statistic is a numerical measurement we get from the sample, a function of the data. [Also sometimes called an estimate.] A parameter is a numerical measurement of the population. We never know the true value of the parameter. The test statistic is a quantity calculated from the data that is used to evaluate how compatible the data are with the result expected under the null hypothesis. The null distribution is the sampling distribution of outcomes for a test statistic under the assumption that the null hypothesis is true. The p-value is the probability of obtaining the data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. The p-value is a number calculated from the dataset. Examples of Hypotheses Identify whether each of the following statements is more appropriate as the null hypothesis or as the alternative hypothesis in a test: The number of hours preschool children spend watching TV affects how they behave with other children when at day care. Alternative Most genetic mutations are deleterious. Alternative A diet of fast foods has no effect on liver function. Null Cigarette smoking influences risk of suicide. Alternative Growth rates of forest trees are unaffected by increases in carbon dioxide levels in the atmosphere. Null The number of hours that grade-school children spend doing homework predicts their future success on standardized tests. Alternative King cheetahs on average run the same speed as standard spotted cheetahs. Null The risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not. Null The mean length of African elephant tusks has changed over the last 100 years. Alternative Caffeine intake during pregnancy affects mean birth weight. Alternative What is an Alternative Hypothesis? Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEY In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on. What is the point? Why did we see the video? How does it relate the to the material from class? What is the relationship to sampling distributions? 3.3.3 All together: structure of a hypothesis test decide on a research question (which will determine the test) collect data, specify the variables of interest state the null (and alternative) hypothesis values (often statements about parameters) the null claim is the science we want to reject the alternative claim is the science we want to prove generate a (null) sampling distribution to describe the variability of the statistic that was calculated along the way visualize the distribution of the statistics under the null model get_p_value to measure the consistency of the observed statistic and the possible values of the statistic under the null model make a conclusion using words that describe the research setting 3.4 1/28/20 Agenda Central Limit Theorem Mathematical approximation for the distribution of one sample proportion 3.5 Normal Model 3.5.1 Central Limit Therm Example: Reese’s Pieces2 As with many of the examples, the Reese’s Pieces example comes from Chance and Rossman (2018). The example focuses on how the samples of orange Reese’s Pieces vary from sample to sample. Today we aren’t particularly interested in a specific research question, instead we are trying to understand the details of the model which describes how \\(\\hat{p}\\) varies from sample to sample. [Spoiler: the distribution is going to look like a bell! and the mathematical model which describes the variability is called the normal distribution.] Notes from the applet: http://www.rossmanchance.com/applets/OneProp/OneProp.htm?candy=1 How does the sampling distribution change as a function of \\(p\\) and \\(n\\)? When a normal distribution is placed on top of the empirical (computational) distribution, does it fit well? A sampling distribution is the probability distribution of all possible values of the statistic in all possible samples of the same size from the same population. Note: increasing the sample size reduces the spread of the sampling distribution of a statistic (i.e., increases the precision). Normal Probability Curve symmetric bell-shaped centered at \\(\\mu\\) \\(\\sigma\\) shows the point of inflection draw a picture every time you start a normal problem! The Central Limit Theorem The Central Limit Theorem says that the sampling distribution of an average will have a bell shaped distribution if \\(n\\) is big enough. The sampling distribution of \\(\\hat{p} = X/n\\) can be thought of as taking lots of random samples from a population, calculating \\(\\hat{p}\\), and creating a histogram. We can easily calculate what we’d expect from that sampling distribution if we know \\(p\\), the true population proportion. Because \\(\\hat{p}\\) is actually an average, the sampling distribution of \\(\\hat{p}\\) can be described by a normal distribution (as long as \\(n\\) is big enough). \\[\\begin{eqnarray*} \\hat{p} &amp;=&amp; \\frac{X}{n}\\\\ SD(\\hat{p}) = \\sigma_{\\hat{p}} &amp;=&amp; \\sqrt{\\frac{p (1-p)}{n}}\\\\ SE(\\hat{p}) &amp;=&amp; \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\ \\hat{p} &amp;\\sim&amp; N\\bigg(p, \\sqrt{\\frac{p(1-p)}{n}} \\bigg) \\ \\ \\ \\ \\ \\mbox{ (if the sample size is large enough)}\\\\ \\end{eqnarray*}\\] Notice the slight difference between \\(SD\\) (uses \\(p\\)) and \\(SE\\) (uses \\(\\hat{p}\\)). We won’t make a big deal of the difference here (and indeed, your book calls both equations \\(SD\\)). We would expect 95% of our \\(\\hat{p}\\) values to be within 2 standard deviations of the mean. That is, 95% of \\(\\hat{p}\\) are: \\[\\begin{eqnarray*} p \\pm 2 \\sqrt{\\frac{p(1-p)}{n}} \\end{eqnarray*}\\] Or put differently, when referring to a randomly selected \\(\\hat{p}\\), \\[\\begin{eqnarray*} P\\bigg( - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} - p \\leq 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\ P\\bigg(\\hat{p} - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq p \\leq \\hat{p} + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95 \\end{eqnarray*}\\] We’d love to create our interval for \\(p\\) using \\(\\hat{p} \\pm 2 \\sqrt{\\frac{p(1-p)}{n}}\\), but we don’t know \\(p\\)! One option is to use \\(SE(\\hat{p})\\) in the estimate of the variability. The Empirical Rule In a bell-shaped, symmetric distribution, % of data in what interval \\(\\approx 68\\%\\) of the observations fall within 1 st dev of the mean \\(\\approx 95\\%\\) of the observations fall within 2 st dev of the mean \\(\\approx 99.7\\%\\) of the observations fall within 3 st dev of the mean 3.6 1/30/20 Agenda Normal distribution (no q-q plots) Calculating normal probabilities 3.6.1 Normal Probabilities &amp; Z scores Z score A Z score of an observation is the number of standard deviations it falls above or below the mean. We compute the Z score for an observation x that follows a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) using \\[ Z = \\frac{x - \\mu}{\\sigma}\\] Normal probabilities We return to the Reese’s Pieces example to investigate the probability of a particular number of orange candies, using the normal approximation. Remember: \\[SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\] And the respective Z score is: \\[ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\] What is the probability that in a sample of 25 candies, you would get less than 40% orange (provided that the machine colors 50% of the candies orange). Answer: 0.1587 What is the probability that in a sample of 250 candies, you would get less than 40% orange (provided that the machine colors 50% of the candies orange). Answer: 0.0007888 What is the probability that in a sample of 25 candies, you would get between 40% and 55% orange (provided that the machine colors 50% of the candies orange). Answer: 0.5328 library(mosaic) # (a) (0.4 - 0.5) / sqrt(0.5*0.5/25) ## [1] -1 xpnorm(-1, 0, 1) ## [1] 0.1586553 # (b) (0.4 - 0.5) / sqrt(0.5*0.5/250) ## [1] -3.162278 xpnorm(-3.16, 0, 1) ## [1] 0.0007888457 # (c) (0.55 - 0.5) / sqrt(0.5*0.5/25) ## [1] 0.5 xpnorm(c(-1, 0.5), 0, 1) ## [1] 0.1586553 0.6914625 Note that normal probabilities can be estimated for any variable that has a distribution which is well approximated by the bell shape given by a normal curve. Below we calculate Z scores and probabilities for a non-proportion setting and then ask whether the values could possibly be normal. (What do you think?) 3.6.1.1 Example: Athletic comparison3 The example below allows for a comparison between two athletes based on speed and strength. The following information is provided about the sample of individuals who were measured: Speed is measured by the time required to run a distance of 40 yards, with smaller times indicating more desirable (faster) speeds. From the data, the times to run 40 yards have a mean of 4.60 seconds and a standard devotion of 0.15 seconds, with a minimum of 4.40 seconds. Strength is measured by the amount of weight lifted, with more weight indicating more desirable (greater) strength From the data, the amount of weight lifted has a mean of 310 pounds and a standard deviation of 25 pounds. mean std dev minimum Time to run 40 yards 4.60 sec 0.15 sec 4.40 sec Amount lifted 310 lbs 25 lbs NA Calculate and interpret the Z score for a player who can lift weight of 370 pounds. \\[Z = \\frac{370-310}{25} = 2.4\\] This z-score tells us that a player who can lift 370 pounds is lifting 2.4 SDs more than average. Saying that this weight is 2.4 SDs away from the average would leave out important information about direction. Consider two players, A and B (with data given as below). Which player should be selected for the team if only one player can be selected? Player A Player B Time to run 40 yards 4.42 sec 4.57 sec Amount lifted 370 lbs 375 lbs At a first glance, we can see that A is faster, and B is stronger. Understanding how each player performs (in strength and speed) relative to the rest of the players is the first step in answering the question. We will calculate four Z scores, one for each player and each task: \\[\\begin{align*} Z_{Aspeed} = \\frac{4.42 - 4.6}{0.15} = -1.2\\\\ Z_{Astrength} = \\frac{370-310}{25} = 2.4\\\\ Z_{Bspeed} = \\frac{4.57 - 4.6}{0.15} = -0.2\\\\ Z_{Bstrength} = \\frac{375-310}{25} = 2.6\\\\ \\end{align*}\\] After calculating Z scores, it is found that Player B is only slightly stronger than Player A, but Player A is considerably faster than Player B. Because the question advised us to consider both criteria as equally valuable, Player A is the better choice. Using the full information about the speed data, do you think that the distribution of 40 yard running times is approximately normal? NO! The minimum is too close to the mean for the normal distribution to provide a reasonable model. What does “too close” mean here? Let’s see how many standard deviations the minimum is below the mean: \\[ Z = \\frac{4.4 - 4.6}{0.15} = -1.33 \\] The Z score tells us that the minimum speed is only -1.33 standard deviations below the mean. According to the normal distribution (see the plot below), we would expect about 9% of the observations to be lower than 4.4 seconds, so the normal distribution does not seem to be a great fit to these observations. xpnorm(-1.333, 0, 1, plot = TRUE) ## [1] 0.0912659 3.7 2/4/20 Agenda Theoretical basis for confidence intervals \\(Z^*\\) (different from Z score!) Example: extreme poverty 3.8 Confidence Intervals 3.8.1 Theoretical set-up Conditions for when the sampling distribution of \\(\\hat{p}\\) is nearly normal (The Central Limit Theorem!!) The sampling distribution for \\(\\hat{p}\\), taken from a sample of size \\(n\\) from a population with a true proportion \\(p\\), is nearly normal when: the sample observations are independent we expected to see at least 10 successes and 10 failures in our samples. Said differently, \\(np \\geq 10\\) and \\(n(1-p) \\geq 10\\). This is sometimes called the success-failure condition. If the conditions are met, then the sampling distribution of \\(\\hat{p}\\) is nearly normal with mean \\(p\\) and standard error: \\[SE_{\\hat{p}} = SE (\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\] How far is \\(\\hat{p}\\) from \\(p\\) ??? Great news, the \\(SE(\\hat{p})\\) measures the distance we can expect between \\(\\hat{p}\\) from \\(p\\)!!! Indeed, a Z score tells us the distance between \\(\\hat{p}\\) from \\(p\\) in units of standard error. The normal distribution provides percentages for how often Z scores should fall in certain ranges. From the empirical rule, we would expect 95% of our \\(\\hat{p}\\) values to be within 2 standard deviations of the mean. That is, 95% of \\(\\hat{p}\\) are: \\[\\begin{eqnarray*} p \\pm 2 \\sqrt{\\frac{p(1-p)}{n}} \\end{eqnarray*}\\] Or put differently, when referring to a randomly selected \\(\\hat{p}\\), \\[\\begin{eqnarray*} P\\bigg( p - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} \\leq p + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\ P\\bigg( - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} - p \\leq 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\ P\\bigg(\\hat{p} - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq p \\leq \\hat{p} + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95 \\end{eqnarray*}\\] Putting it all together, we create a confidence interval for \\(p\\) which says that 95% of all samples will create confidence intervals that capture the true (unknown \\(p\\)): \\[95\\% \\mbox{ CI for }p: \\hat{p} \\pm 1.96 \\sqrt{\\frac{p(1-p)}{n}}\\] And if a different percentage is needed, change the multiplier appropriately: Confidence Interval Formula \\[\\mbox{ CI for }p: \\hat{p} \\pm Z^* \\sqrt{\\frac{p(1-p)}{n}}\\] What is \\(Z^*\\)? It is defined using the normal distribution which is centered at zero with a standard deviation of one. For example, if a 99% confidence interval is desired, find the \\(Z^*\\) value that captures 99% of the observations between \\(-Z^*\\) and \\(Z^*\\). \\[99\\% \\mbox{ CI for }p: \\hat{p} \\pm 2.58 \\sqrt{\\frac{p(1-p)}{n}}\\] xpnorm(c(-2.58, 2.58), 0, 1, plot = TRUE) ## [1] 0.004940016 0.995059984 What does the percentage level mean? A confidence level is the long-run percent of intervals that capture the true parameter. 3.8.2 Example: changes in extreme poverty In-class activity set-up Recall from the in-class activity: Some of you may be familiar with Hans Rosling who founded the website https://www.gapminder.org/ and dedicated his life to promoting awareness of global health issues, see his Ted talks here: https://www.ted.com/playlists/474/the_best_hans_rosling_talks_yo. One question he liked to ask is: Has the percentage of the world’s population who live in extreme poverty doubled, halved, or remained about the same over the past twenty years? Before you go on, answer the question. Has the extreme poverty doubled, halved, or remained about the same? What do you think? The correct answer is that this percentage has halved, but only 5% of a sample of 1005 U.S. adults in 2017 got this right. Rosling liked to say that chimpanzees would do better than people: With only three options, we would expect 33.33% of chimpanzees to answer correctly. If in fact the students are randomly guessing, how many standard deviations away from the “random guess” value is 0.05? [Hint: use proportions and not percentages in your calculations.] note: we covered this in class on Tuesday, so it’s in the notes, but the formula doesn’t show up in your text until the box on page 124 in section 3.1.1. Do not use the computer here (except as a calculator, and feel free to use a calculator or use the computer / R as a calculator). Note: you need to know how many people were asked, look above. Solution \\[SD(\\hat{p}) = \\sqrt{p(1-p)/n} = \\sqrt{(1/3)(2/3)/1005} = 0.0149\\] sqrt((1/3)*(2/3)/1005) ## [1] 0.01486999 How far is 0.05 from (1/3) in units of standard deviation? That’s just a Z score! Yikes, the 5% value is MORE THAN 19 STANDARD DEVIATIONS BELOW RANDOM GUESSING!!! Z_p = (0.05 - (1/3)) / sqrt((1/3)*(2/3)/1005) Z_p ## [1] -19.05404 What does this say about humans doing so much worse than random guessing when answering the question about poverty? (No hypothesis test here, just a reflection on the distance between the observed data and the random guess answer.) Solution Not only are humans wrong, but they are wrong at an extremely high rate. That is, they are wrong in such a way that they can’t possibly be guessing. There must be something about the question that makes so many people get it wrong (maybe that they are all seeing the same media narrative which describes continued problems with extreme poverty?) We could find the percent of samples that would have produced such a small \\(\\hat{p}\\) if people were indeed random guessing. Unsurprisingly, the proportion of such samples is exceedingly small: xpnorm(-19.05, 0, 1, plot=TRUE) ## [1] 3.28511e-81 3.8.2.1 Confidence Interval for true population proportion Given the extreme poverty set-up above, the question turns from one of a hypothesis test to one of a confidence interval. Note that we are making one more change to the question, we are curious about the proportion of people who think that the rate has doubled. \\[\\begin{eqnarray*} p &amp;=&amp; \\mbox{true proportion of people who incorrectly believe that the % of the}\\\\ &amp;=&amp; \\mbox{ world’s population who live in extreme poverty has doubled}\\\\ \\hat{p} &amp;=&amp; \\mbox{sample proportion of people who incorrectly believe that the % of the}\\\\ &amp;=&amp; \\mbox{ world’s population who live in extreme poverty has doubled} \\end{eqnarray*}\\] It turns out that in the sample of 1005 adult Americans, 593 people thought that the rate had doubled.4 \\[\\hat{p} = \\frac{593}{1005} = 0.59\\] A 95% confidence interval for the true proportion of adult Americans who think the rate has doubled is (0.56, 0.62). We are 95% confident that the true proportion of adult Americans who think the extreme poverty rate has doubled is between 0.56 and 0.62. \\[ \\hat{p} \\pm 1.96 * \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] 593/1005 - 1.96 * sqrt((593/1005)*(412/1005) / 1005) ## [1] 0.5596421 593/1005 + 1.96 * sqrt((593/1005)*(412/1005) / 1005) ## [1] 0.6204574 Question: Survey researchers typically select only one random sample from a population, and then they produce a confidence interval based on that sample.How do we know whether the resulting confidence interval is successful in capturing the unknown value of the population parameter? Answer: we don’t know! We never know if the interval actually captures the parameter or not. We just know that over our lifetime as scientists, we will capture at the rate we set. Question: If we can’t know for sure whether the confidence interval contains the value of the population parameter, on what grounds can we be confident about this? Answer: well, we agree about the process that created the CI. 3.8.3 Modifying CIs Changing \\(n\\) As we can see from the CI formula, increasing \\(n\\) has the effect of decreasing the width of the CI. \\[ \\hat{p} \\pm 1.96 * \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] Changing \\(p\\) A different value of \\(p\\) means that the sampling distribution will have a different center (and a different SE), but the coverage rate will not change, and the SE probably won’t change very much. Changing the confidence level The choice of \\(Z^*\\) determines (over, say, your lifetime as a scientist) the percent of your research confidence intervals that will capture the true parameter of interest. Note that the larger the \\(Z^*\\) value, the more likely it is that a sample will produce a CI which captures the true parameter. Note that \\(Z^* = 1.645\\) produces CIs that capture at a 90% rate. \\(Z^* = 2.58\\) produces CIs that capture at a 99% rate. xpnorm(1.645, 0, 1) ## [1] 0.9500151 xpnorm(2.58, 0, 1) ## [1] 0.99506 Question: why don’t we always use 99.99% CIs? Answer: because the intervals would typically be too wide to provide any real information about the actual population parameter. 3.9 2/6/20 Agenda Putting together all the pieces of the CI Effects of sample size, \\(p\\), and confidence level on CI What is the confidence level? 3.10 2/11/20 Agenda Biased sampling Simple Random Sampling 3.11 Sampling 3.11.1 Example: aliens on Earth5 Assume that an alien has landed on Earth and wants to understand the gender diversity of humans. Fortunately, the alien took a good statistics course on its home planet, so it knows to take a sample of human beings and produce a confidence interval for this proportion. Unfortunately, the alien happens upon the 2019 US Senate as its sample of human beings. The US Senate has 25 senators who self-identify as having a female gender (its most ever!) among its 100 members in 2019. Calculate the alien’s 95% confidence interval. (uh… confidence interval for what?) This calculation becomes .25 \\(\\pm\\) .085, which is the interval (.165 \\(\\rightarrow\\) .335). Interpret the interval. The alien would be 95% confident that the proportion of all humans on earth who self identify as female is between .165 and .335. Is this consistent with your experience living on this planet? No, the actual proportion of humans who self identify as female is much larger than this interval, closer to 0.5. What went wrong? The alien did not select a random sample of humans. In fact, the alien’s sampling method was very biased toward under-representing self-identifying females. As we saw with the applet, about 5% of all 95% confidence intervals fail to capture the actual value of the population parameter. Is that the explanation for what went wrong here? No! The explanation about 5% of all intervals failing is only relevant when you have selected random samples over and over again. The lack of random sampling is the problem here. Would it be reasonable for the alien to conclude, with 95% confidence, that between 16.5% and 33.5% of US senators in the year 2019 self-identify as female? No. We know (for sure, with 100% confidence) that exactly 25% of U.S. senators in 2019 self identify as female. If that’s the entire population of interest, there’s no reason to calculate a confidence interval. Confidence intervals are not appropriate when the data were collected with a biased sampling method. A confidence interval calculated from such a sample can provide very dubious and misleading information. Confidence intervals are not appropriate when you have access to the entire population of interest. In this unusual and happy circumstance, you should simply describe the population. 3.11.2 Example: Gettysburg Address6 The authorship of literary works is often a topic for debate. Were some of the works attributed to Shakespeare actually written by Bacon or Marlowe? Which of the anonymously published Federalist Papers were written by Hamilton, which by Madison, which by Jay? Who were the authors of the writings contained in the Bible? The fields of “literary computing” and “forensic stylometry” examine ways of numerically analyzing authors’ works, looking at variables such as sentence length and rates of occurrence of specific words. The above passage is of course Abraham Lincoln’s Gettysburg Address, given November 19, 1863 on the battlefield near Gettysburg, PA. In characterizing this passage, we would ideally examine every word. However, often it is much more convenient and even more efficient to only examine a subset of words. Step 1: sample 10 representative words. Are they representative of the Gettysburg Address in all ways? What about in length? [Note, the parameter representing the the true average word length is 4.29 letters.] In class, we found that different samples (i.e., different student’s selection of 10 words) produced different sample means. But that generally, those sample means varied well above the true population mean of 4.29 letters. Step 2: sample 10 random words. Again, in class different sample produced different sample means. But now the sample means varied around the center of 4.29 letters. Step 3: sample 20 random words. The in-class samples are again centered around 4.29 letters, but they are less variable (from sample to sample) when 20 words are selected than when 10 words were selected. 3.11.3 Key sampling terms convenience sample where individuals who are easily accessible are more likely to be included in the sample. For instance, if a political survey is done by stopping people walking in the Bronx, it will not represent all of New York City. It is often difficult to discern what sub-population a convenience sample represents. simple random sample equivalent to using a raffle to select cases. This means that each case in the population has an equal chance of being included and there is no implied connection between the cases in the sample. A sampling distribution is the distribution of all possible values of the statistic in all possible samples of the same size from the same population. increasing the sample size reduces the spread of the sampling distribution of a statistic (i.e., increases the precision). the sampling distribution of a statistic does not depend on the population size! (we assume it is “big enough” so that the sample isn’t basically the same set as the population.) when characteristics of the resulting samples are systematically different from the population, we call the sampling mechanism biased. If the distribution of the sample statistics, under repeated samples from the same population, is centered at the value of the population parameter, the distribution of the statistic is said to be unbiased. 3.12 2/13/20 Agenda Type I &amp; Type II errors Power CI and HT together 3.13 Errors &amp; Power The significance level, \\(\\alpha\\), is a probability used as a criterion for rejecting the null hypothesis. If the p-value for a test is less than or equal to \\(\\alpha\\), then the null hypothesis is rejected. If the p-value is greater than \\(\\alpha\\), then the null hypothesis is not rejected. The significance level is a number calculated before the experiment is run and not based on the dataset. (Often the significance level is set by the journal or granting agency.) The rejection region is the values of the statistic we would need to be able to reject \\(H_0\\). A type I error is rejecting a true null hypothesis. The significance level \\(\\alpha\\) sets the probability of committing a type I error. A type II error is failing to reject a false null hypothesis. The power of a test is the probability that a random sample will lead to rejection of a false null hypothesis. Truth \\(H_0\\) true \\(H_A\\) true Test Reject \\(H_0\\) type I error 😄 Fail to reject \\(H_0\\) 😄 type II error 3.13.1 Example: baseball player7 The following example is taken from Chance and Rossman (2018), and is used to explain many of the most important and nuanced ideas related to the structure of hypothesis testing. I will provide the basic idea here, but you are encouraged to go to the applet on your own to convince yourself that the idea is true and that you understand why the idea is true. http://www.rossmanchance.com/applets/power.html Set-up: Assume that you are a manager of a professional baseball team. One of your players has (for many years) been a 0.250 hitter. That means every time he goes up to bat he has a 1 in 4 chance of hitting the ball (baseball aficionados may want to talk about baseball errors at this point, but we won’t be mentioning baseball errors in today’s example). Your player tells you that he has been working extremely hard over the off-season and has improved to become a 0.333 hitter. That is, he now believes that every time he goes up to bat he has a 1 in 3 chance of hitting the ball. You may be aware that profession baseball players who are good make a lot of money. And an increase from hitting the ball 1 in 4 tries to 1 in 3 tries is worth many millions of dollars. Of course, your player is trying to convince you that he is now worth many additional millions of dollars and should be paid accordingly. What should you do? Well, you need him to convince you that he has, indeed, improved. Before we get started, we’ll just ask one of our usual questions: what is the parameter of interest? \\(p\\) = baseball players current probability of hitting the ball 3.13.2 Errors: lessons learned You are encouraged to go to the applet on your own to convince yourself that you understand why the ideas below are true. http://www.rossmanchance.com/applets/power.html What are the Type I and Type II errors/ A Type I error means the manager became convinced the player is better than a 0.250 hitter but in reality he is just still a 0.250 hitter. A Type II error means the player has improved but does not do well enough in his 20 at-bats to convince the manager. Who is worried about which type of error? Player would like to minimize the probability of a Type II error – of the manager missing his improvement. The manager would like to minimize the probability of a Type I error – incorrectly thinking the player has improved What factors impact power? And how? Increasing sample size increases power. As the sample size increases, the distribution of the sample proportion gets more narrow (the SE decreases). The SE decrease means that the null and alternative curves overlap less. You will always have the ability to take more observations, although it might be extremely expensive or time consuming to measure more data. Increasing the significance level \\(\\alpha\\) will increase the power. Ideally, the probabilities of both types of errors would be small, but unfortunately they are inversely related: as one error probability decreases, the other increases (unless other factors change also). What’s typically done in practice is to set the maximum allowable probability of Type I error in advance by setting the level of significance \\(\\alpha\\), the most common value is 0.05, followed by 0.10 and 0.01, and then determine the sample size necessary for the probability of a Type II error to be below a specific value. Increasing the distance between the null and alternative will increase the power. Unfortunately, you have very little control over the alternative value. Your science will determine the alternative (in this case, the baseball player’s ability determined his alternative value). The better your science (i.e., the more non-null) it is, the better your chances are of convincing your audience (i.e., publishing) that your results are interesting. (Consider this: it is much easier to convince someone that 8th graders are taller, on average, than kindergartners than it is to convince someone that 1st graders are taller, on average than kindergartners.) Why does the Type I error rate double if we consider two sides? Consider the situation where the null hypothesis really is true. And you wait to make your alternative hypothesis until after you’ve seen the data. You choose your rejection region to be the 5% tail region on one side. You reject if the observed statistic is in that tail (reminder: in this example the null hypothesis is really true!). Well, instead of making a Type I error 5% of the time, the process described above actually makes a “rejection” 10% of the time! If a CI for \\(p\\) does not overlap a particular number, why is it consistent with rejecting a null HT for that value of \\(p\\)? If a 95% CI does not overlap \\(p\\) (for example, p=0.47), then \\(p\\) and \\(\\hat{p}\\) are more than 1.96 SEs away from each other. If \\(p\\) and \\(\\hat{p}\\) are more than 1.96 SEs away from each other, then the Z score associate with \\(\\hat{p}\\) is larger (in absolute value) than 1.96 (by definition of the Z score!). If the Z score is larger (in absolute value) than 1.96, then the two-sided p-value will be less than 0.05. 3.14 Reflection Questions 3.14.1 hypothesis testing: Chapter 2, Sections 1-4 What is the difference between a statistic and a parameter? In a typical study, do you have one statistic or more than one statistic? And do you know the value of the statistic? In a typical study, do you have one parameter or more than one parameter? And do you know the value of the parameter? Explain what it means for a statistic to have a distribution. What is a p-value? What is the difference between a one- and two-sided hypothesis? What is the difference between a null hypothesis and an alternative hypothesis? 3.14.2 normal model: Chapter 2, Sections 5-7 What does it mean for something to have a normal distribution? How can you use the normal curve to calculate percentages or probabilities? What does it mean for \\(\\hat{p}\\) to have a distribution? Can you explain in words? What does the central limit theorem tell us about the distribution of \\(\\hat{p}\\)? What technical conditions are important in order for the central limit theorem to apply? What does a Z score measure? 3.14.3 confidence intervals: Chapter 2, Section 8 What is a confidence interval? Part of the CI interpretation includes a phrase “95% confident.” Explain what 95% means. How can you find the appropriate \\(Z^*\\) value? What is the difference between a Z score and \\(Z^*\\)? When computing a confidence interval (i.e., when we don’t have a preconceived idea for \\(p\\)), how is the standard deviation of \\(\\hat{p}\\) estimated? When using the normal distribution to create a confidence interval for \\(p\\), how is the critical value for, say, a 94.7% interval calculated? 3.14.4 sampling: Chapter 1, Sections 3-4 Why is it good to take random samples? What is a simple random sample? Why don’t researchers always take random samples? What benefit(s) does a large sample provide to the study? What is the difference between practical significance and statistical significance? 3.14.5 errors &amp; power: Chapter 2, Section 3 Why is it never okay to accept \\(H_0\\)? What is the difference between a Type I and Type II error? Which is worse: a Type I error or a Type II error? What is power? How is power calculated? What does power depend on? References "],
["inference-for-categorical-data.html", "Chapter 4 Inference for categorical data 4.1 Inference for a single proportion 4.2 2/18/20 Math 58 Agenda 4.3 2/20/20 Math 58 Agenda 4.4 Binomial distribution (Math 58 only) 4.5 2/18/20 Math 58B Agenda 4.6 2/20/20 Math 58B Agenda 4.7 Relative Risk (Math 58B only) 4.8 Odds Ratios (Math 58B only) 4.9 2/25/20 Agenda 4.10 Difference of two proportions 4.11 2/27/20 Agenda 4.12 Types of Studies 4.13 3/3/20 Agenda 4.14 Goodness-of-fit: One categorical variable (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels 4.15 3/5/20 Agenda 4.16 Independence: Two categorical variables (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels each 4.17 3/10/20 &amp; 3/12/20 Agenda 4.18 3/17/20 &amp; 3/19/20 Agenda 4.19 3/24/20 Agenda 4.20 Census 4.21 3/26/20 Agenda 4.22 COVID-19 4.23 Reflection Questions", " Chapter 4 Inference for categorical data 4.1 Inference for a single proportion Previously, we used the normal approximation to describe the distribution of different values for \\(\\hat{p}\\) when random samples are taken. We learned that the central limit theorem describes the distribution such that if (see box in section 3.1.1 on page 124): we take random, independent samples \\(np \\geq 10\\) and \\(n(1-p) \\geq 10\\) then \\[\\hat{p} \\sim N(p, \\sqrt{p(1-p)/n}).\\] If the \\[\\mbox{Z score} = \\frac{\\hat{p} - p}{\\sqrt{p(1-p)/n}}\\] is bigger than the \\(Z^*\\) value at a particular value of \\(\\alpha\\), then we know we can reject \\(p\\) (the Null Hypothesis value) as the true population parameter. If an interval estimate is desired, and no \\(p\\) is hypothesized, then a confidence interval is created using: \\[\\hat{p} \\pm Z^* \\cdot \\sqrt{\\hat{p}(1-\\hat{p})}/n.\\] IMPORTANT: recall, the above interval is a method for capturing the parameter. 4.2 2/18/20 Math 58 Agenda Math 58 Only Binomial distribution 4.3 2/20/20 Math 58 Agenda Math 58 Only Binomial hypothesis testing Power Confidence Intervals 4.4 Binomial distribution (Math 58 only) Math 58 (not Math 58B) will cover the binomial distribution which describes the exact probabilities associated with binary outcomes. Diez, Barr, and Çetinkaya-Rundel (2014) do not discuss the binomial distribution. Chance and Rossman (2018), however, provide quite a bit of detail about the binomial concepts in chapter 1. 4.4.1 Example: pop quiz There are 5 problems on this quiz; everyone number their papers 1. to 5. Each of the problems is multiple choice with answers A, B, C, or D. Go ahead. We’ll grade the papers when everyone is done. Solution: 1.B, 2.C, 3.B, 4.C, 5.A The binomial distribution provides the probability distribution for the number of “successes” in a fixed number of independent trials, when the probability of success is the same in each trial. Outcome of each trial can be stated as a success / failure. The number of trials (\\(n\\)) is fixed. Separate trials are independent. The probability of success (\\(p\\)) is the same in every trial. \\[\\begin{eqnarray*} P(X=k) &amp;=&amp; {n \\choose k} p^k (1-p)^{n-k}\\\\ {n \\choose k} &amp;=&amp; \\frac{ n!}{(n-k)! k!} \\end{eqnarray*}\\] In our example… \\(n=5\\). How many ways are there to get 2 successes? \\[\\begin{eqnarray*} {5 \\choose 2} &amp;=&amp; \\frac{ 5!}{2! 3!} = \\frac{ 5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1}{(3 \\cdot 2 \\cdot 1)(2 \\cdot 1)} \\end{eqnarray*}\\] The numerator represents the number of possibilities for each of the 5 questions. But we don’t distinguish between successes, so we don’t want to double count those. Similarly for failures. SSSFF SSFFS SSFSF SFFSS SFSFS SFSSF FFSSS FSFSS FSSFS FSSSF In class: different groups work out the probability of 0, 1, 2, … 5 correct answers. \\[\\begin{eqnarray*} P(X=0) = {5 \\choose 0} (0.25)^0(0.75)^5 = 0.2373 &amp;&amp; P(X=3) = {5 \\choose 3} (0.25)^3(0.75)^2 = 0.0879\\\\ P(X=1) = {5 \\choose 1} (0.25)^1(0.75)^4 = 0.3955 &amp;&amp; P(X=4) = {5 \\choose 4} (0.25)^4(0.75)^1 = 0.0146\\\\ P(X=2) = {5 \\choose 2} (0.25)^2(0.75)^3 = 0.2637 &amp;&amp; P(X=5) = {5 \\choose 5} (0.25)^5(0.75)^0 = 0.0010\\\\ \\end{eqnarray*}\\] library(mosaic) xpbinom(2, size = 5, prob = 0.25) # P(X &lt;= 2) vs. P(X &gt; 2) ## [1] 0.8964844 xpbinom(3, size = 5, prob = 0.25) # P(X &lt;= 3) vs. P(X &gt; 3) ## [1] 0.984375 4.4.2 Binomial Hypothesis Testing Consider the example from the beginning of the semester on babies choosing the helper toy (instead of the hinderer), section 2.3. Recall that 14 of the 16 babies chose the helper toy. Does the binomial distribution apply to this setting? Let’s check: two choices? Yes, helper or hinderer. fixed \\(n\\)? Yes, there were 16 babies. \\(p\\) same? Presumably. There is some inherent \\(p\\) which represents the probability that a baby would choose a helper toy. And we are choosing babies from a population with that \\(p\\). independent? I hope so! These babies don’t know each other or tell each other about the experiment. If there really had been no inclination of the babies to choose the helper toy, how many babies would the researchers have needed to choose the helper in order to get published? Let’s choose \\(\\alpha = 0.01\\). That means that if \\(p=0.5\\), then we should make a Type I error less than 1% of the time. From the calculations below, we see that the rejection region is \\(\\{ X \\geq 14 \\}\\). That is, for the researchers to reject the null hypothesis at the \\(\\alpha = 0.01\\) significance level, they would have needed to see 14, 15, or 16 babies choose the helper (out of 16). \\[\\begin{eqnarray*} P(X \\geq 12) &amp;=&amp; {16 \\choose 12} (0.5)^{12}(0.5)^{4} + 0.0106 = 0.0384\\\\ P(X \\geq 13) &amp;=&amp; {16 \\choose 13} (0.5)^{13}(0.5)^{3} + 0.00209 = 0.0106\\\\ P(X \\geq 14) &amp;=&amp; {16 \\choose 14} (0.5)^{14}(0.5)^{2} + 0.000259 = 0.00209\\\\ P(X \\geq 15) &amp;=&amp; {16 \\choose 15} (0.5)^{15}(0.5)^{1} + 0.0000153 = 0.000259\\\\ P(X = 16) &amp;=&amp; {16 \\choose 16} (0.5)^{16}(0.5)^{0} = 0.0000153\\\\ \\end{eqnarray*}\\] xpbinom(12, 16, 0.5) ## [1] 0.9893646 xpbinom(13, 16, 0.5) ## [1] 0.9979095 4.4.3 Binomial Power Let’s say that the researchers had an inkling that babies liked helpers. But they thought that probably only about 70% of babies preferred helpers. The researchers then needed to decide if 16 babies was enough for them to do their research. That is, if they only measure 16 babies, will they have convincing evidence that babies actually prefer the helper? Said differently, with 16 babies, what is the power of the test? \\[\\begin{eqnarray*} \\mbox{power} &amp;=&amp; P(X \\geq 14 | p = 0.7)\\\\ &amp;=&amp; P(X=14 | p=0.7) + P(X = 15 | p=0.7) + P(X = 16 | p=0.7)\\\\ &amp;=&amp; {16 \\choose 14} (0.7)^{14}(0.3)^{2} + {16 \\choose 15} (0.7)^{15}(0.3)^{1} + {16 \\choose 16} (0.7)^{16}(0.3)^{0}\\\\ &amp;=&amp; 0.099 \\end{eqnarray*}\\] Yikes! What if babies actually prefer the helper 90% of the time? \\[\\mbox{power} = P(X \\geq 14 | p = 0.9) = 0.789\\] 1 - xpbinom(13, 16, 0.7) ## [1] 0.09935968 1 - xpbinom(13, 16, 0.9) ## [1] 0.7892493 4.4.4 Binomial Confidence Intervals for \\(p\\) The binomial distribution does not allow for the “plus or minus” creation of a range of plausible values for the confidence interval. Instead, hypothesis testing is used directly to come up with plausible values for the parameter \\(p\\). The method outlines below is much more tedious than the z - CI , but it does produce an exact interval for \\(p\\) with the appropriate coverage level. Consider a confidence interval created in the following way: Step 1: Collect data, calculate \\(\\hat{p}\\) for that particular dataset. Step 2: Test a series of values for \\(p&#39;\\) using the observed \\(\\hat{p}\\) from the dataset at hand. Step 3: List all the values for \\(p&#39;\\) that were not rejected. Sort them and find the smallest and biggest value: (\\(p_{small}, p_{big}\\)). Ask yourself whether the true parameter (let’s call it \\(p\\)) is in the interval. If a type I error was made when \\(p\\) was tested, then \\(p\\) is not in the interval. If \\(p\\) was not rejected, then it is in the interval. How often will a type I error be made? 5% of the time. Therefore (\\(p_{small}, p_{big}\\)) is a 95% CI for the true population parameter \\(p\\). 4.5 2/18/20 Math 58B Agenda Math 58B Only Relative Risk Odds Ratios Case-control studies 4.6 2/20/20 Math 58B Agenda Math 58B Only CI for relative risk CI for odds ratios 4.7 Relative Risk (Math 58B only) Math 58B (not Math 58) will cover relative risk, the ratio of two success proportions. Previously (e.g., Gender discrimination example, 3.2) when working with the proportion of success in two separate groups, the proportion of success was subtracted (see also lab 4). Next week, differences in proportions will be revisited, see section 4.10. First up, the new statistic of interest will be relative risk, followed by odds ratios. In particular, interest is in the ratio of probabilities. [Note: the decision to measure a ratio instead of a difference comes with trying to model the particular research question at hand. There is nothing inherently better about ratios versus differences. It is, however, often easier to think about how a small probability changes if it is done as a ratio instead of a difference.] \\[\\mbox{Relative Risk (RR)} = \\frac{\\mbox{proportion of successes in group 1}}{\\mbox{proportion of successes in group 2}}\\] Definition 4.1 Relative Risk The relative risk (RR) is the ratio of risks for each group. We say, “The risk of success is RR times higher for those in group 1 compared to those in group 2.” 4.7.1 Inference on Relative Risk Due to some theory we won’t cover, there is a fairly good mathematical approximation which describes how the natural log of the relative risk varies from sample to sample: \\[\\ln(\\hat{RR}) \\stackrel{\\mbox{approx}}{\\sim} N\\Bigg(\\ln(RR), \\sqrt{\\frac{1}{A} - \\frac{1}{A+C} + \\frac{1}{B} - \\frac{1}{B+D}}\\Bigg)\\] explanatory 1 explanatory 2 response 1 A B response 2 C D Statistic: \\[\\hat{p}_1 / \\hat{p}_2 = \\frac{A/(A+C) }{B/ (B+D)}\\] Null Hypothesis: \\[H_0: p_1/p_2 = 1\\] CI: The CI is for the true relative risk in the population, \\(p_1/p_2\\) \\[\\mbox{exponentiate} \\Bigg[ \\ln(\\hat{p}_1/\\hat{p}_2) \\pm z^*\\sqrt{ \\frac{1}{A} - \\frac{1}{A+C} + \\frac{1}{B} - \\frac{1}{B+D}}\\Bigg]\\] To remember with relative risk: The percent change is defined as: \\[\\begin{eqnarray*} (RR - 1)*100\\% = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\hat{p}_2}*100\\% = \\mbox{percent change from 2 to 1} \\end{eqnarray*}\\] The CI for \\(p_1/p_2\\) is typically considered significant if 1 is not in the interval. That is because usually the null hypothesis is \\(H_0: p_1 = p_2\\) or equivalently, \\(H_0: p_1/p_2 = 1\\). 4.7.2 Using infer for inference on RR As with the difference in proportions, the infer syntax can be used to simulate a sampling distribution of the sample relative risk under the null hypothesis that the population proportions are identical. NOTE in order to provide syntax that was comparable and correct for the RR and the OR, smoking has been specified as the response variable, and lungs has been specified as the explanatory variable. library(infer) WynderGraham &lt;- data.frame(lungs = c(rep(&quot;cancer&quot;, 605), rep(&quot;healthy&quot;, 780)), smoking = c(rep(&quot;light&quot;, 22), rep(&quot;heavy&quot;, 583), rep(&quot;light&quot;, 204), rep(&quot;heavy&quot;, 576))) (obs_RR &lt;- WynderGraham %&gt;% specify(smoking ~ lungs, success = &quot;heavy&quot;) %&gt;% calculate(stat = &quot;ratio of props&quot;, order = c(&quot;cancer&quot;, &quot;healthy&quot;))) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 1.30 null_RR &lt;- WynderGraham %&gt;% specify(smoking ~ lungs, success = &quot;heavy&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% infer::calculate(stat = &quot;ratio of props&quot;, order= c(&quot;cancer&quot;, &quot;healthy&quot;)) null_RR %&gt;% visualize() + shade_p_value(obs_stat = obs_RR, direction = &quot;right&quot;) 4.8 Odds Ratios (Math 58B only) Experience shows that very few introductory statistics students have seen odds or odds ratios in their prior mathematical or scientific study. That makes odds ratios a new idea, but not a fundamentally hard idea. Which is to say, it is perfectly acceptable to find relative risk a very intuitive idea that you can easily discuss and odds ratios a very strange idea which is hard to interpret. Do not be discouraged! Odds ratios are not fundamentally harder to understand than relative risk, they are simply a new idea. Math 58B (not Math 58) will cover odds ratios, the ratio of two success odds. Diez, Barr, and Çetinkaya-Rundel (2014) do not discuss relative risk and odds ratios. Chance and Rossman (2018), however, provide quite a bit of detail about the concepts in Investigations 3.9, 3.10, 3.11. \\[\\mbox{risk} = \\frac{\\mbox{number of successes}}{\\mbox{total number}}\\] \\[\\mbox{odds} = \\frac{\\mbox{number of successes}}{\\mbox{number of failures}}\\] \\[\\mbox{Odds Ratio (OR)} = \\frac{\\mbox{odds of success in group 1}}{\\mbox{odds of success in group 2}}\\] Definition 4.2 Odds Ratio A related concept to risk is odds. It is often used in horse racing, where “success” is typically defined as losing. So, if the odds are 3 to 1 we would expect to lose 3/4 of the time. The odds ratio (OR) is the ratio of odds for each group. We say, “The odds of success is OR times higher for those in group 1 compared to those group 2.” 4.8.1 Example: Smoking and Lung Cancer8 After World War II, evidence began mounting that there was a link between cigarette smoking and pulmonary carcinoma (lung cancer). In the 1950s, three now classic articles were published on the topic. One of these studies was conducted in the United States by Wynder and Graham.9 They found records from a large number of patients with a specific type of lung cancer in hospitals in California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, and Utah. Of those in the study, the researchers focused on 605 male patients with this form of lung cancer. Another 780 male hospital patients with similar age and economic distributions without this type of lung cancer were interviewed in St. Louis, Boston, Cleveland, and Hines, IL. Subjects (or family members) were interviewed to assess their smoking habits, occupation, education, etc. The table below classifies them as non-smoker or light smoker, or at least a moderate smoker. The following two-way table replicates the counts for the 605 male patients with the same form of cancer and for the “control-group” of 780 males. none light mod heavy heavy excessive chain \\(&lt;\\) 1/day 1-9/day 10-15/day 16-20/day 21-34/day 35\\(+\\)/day patients 8 14 61 213 187 122 controls 114 90 148 278 90 60 Given the results of the study, do you think we can generalize from the sample to the population? Explain and make it clear that you know the difference between a sample and a population. In order to focus the research question, combine the data into two groups: light smoking is less than 10 cigarettes per day, heavy smoking is 10 or more cigarettes per day. The 2x2 observed data is now: light smoking heavy smoking cancer 22 583 605 healthy 204 576 780 226 1159 1385 Causation? (Is it an experiment or are there possible confounding variables?) Case-control study (605 with lung cancer, 780 without… baseline rate?) What is the response variable and what is the explanatory variable? What happens if the role of the two variables is switched? Group A Group B expl = smoking status expl = lung cancer resp = lung cancer resp = smoking status If lung cancer is considered a success and light smoking is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{583/1159}{22/226} = 5.17\\\\ OR &amp;=&amp; \\frac{583/576}{22/204} = 9.39\\\\ \\end{eqnarray*}\\] The risk of lung cancer is 5.17 times higher for those who heavy smoke than for those who don’t smoke. The odds of lung cancer is 9.39 times higher for those who heavy smoke than for those who don’t smoke. If heavy smoking is considered a success and healthy is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{583/605}{576/780} = 1.31\\\\ OR &amp;=&amp; \\frac{583/22}{576/204} = 9.39\\\\ \\end{eqnarray*}\\] The risk of heavy smoking is 1.31 times higher for those who have lung cancer than for those who don’t have lung cancer. The odds of heavy smoking is 9.39 times higher for those who have lung cancer than for those who don’t have lung cancer. Observational study (who worked in each place?) Cross sectional (only one point in time) Healthy worker effect (who stayed home sick?) Explanatory variable is one that is a potential explanation for any changes (here smoking level). Response variable is the measured outcome of interest (here lung cancer). Case-control study: identify observational units by the response variable Cohort study: identify observational units by the explanatory variable The risk of being a light smoker if the person has lung cancer can be estimated, but there is no possible way to estimate the risk of lung cancer if you are a light smoker. Consider a population of 1,000,000 people: no smoking light smoking cancer 1,000 49,000 50,000 healthy 899,000 51,000 950,000 900,000 100,000 1,000,000 \\[\\begin{eqnarray*} P(\\mbox{light} | \\mbox{lung cancer}) &amp;=&amp; \\frac{49,000}{50,000} = 0.98\\\\ P(\\mbox{lung cancer} | \\mbox{light}) &amp;=&amp; \\frac{49,000}{100,000} = 0.49\\\\ \\end{eqnarray*}\\] What is the explanatory variable? What is the response variable? relative risk? odds ratio? Group A Group B expl = smoking status expl = lung cancer resp = lung cancer resp = smoking status If lung cancer is considered a success and no smoking is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{49/100}{1/900} = 441\\\\ OR &amp;=&amp; \\frac{49/51}{1/899} = 863.75\\\\ \\end{eqnarray*}\\] If light smoking is considered a success and healthy is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{49/50}{51/950} = 18.25\\\\ OR &amp;=&amp; \\frac{49/1}{51/899} = 863.75\\\\ \\end{eqnarray*}\\] OR is the same no matter which variable you choose as explanatory versus response! Though, in general, baseline odds or baseline risk (which we can’t know with a case-control study) is still a number that can provide a lot of information about the study. IMPORTANT: Relative risk cannot be used with case-control studies but odds ratios can be used! 4.8.2 Inference on Odds Ratios Due to some theory we won’t cover, there is a fairly good mathematical approximation which describes how the natural log of the odds ratio varies from sample to sample: \\[\\ln(\\hat{OR}) \\stackrel{\\mbox{approx}}{\\sim} N\\Bigg(\\ln(OR), \\sqrt{\\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\Bigg)\\] explanatory 1 explanatory 2 response 1 A B response 2 C D Statistic: \\[\\hat{OR} = \\frac{A D}{B C}\\] Null Hypothesis: \\[H_0: OR = 1\\] CI: The CI is for the true odds ratio in the population, \\(OR\\) \\[\\mbox{exponentiate} \\Bigg[ \\ln{\\hat{OR}} \\pm z^* \\sqrt{ \\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}\\Bigg]\\] 4.8.2.1 OR is more extreme than RR Without loss of generality, assume the true \\(RR &gt; 1\\), implying \\(p_1 / p_2 &gt; 1\\) and \\(p_1 &gt; p_2\\). Note the following sequence of consequences: \\[\\begin{eqnarray*} RR = \\frac{p_1}{p_2} &amp;&gt;&amp; 1\\\\ \\frac{1 - p_1}{1 - p_2} &amp;&lt;&amp; 1\\\\ \\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &amp;&gt;&amp; 1\\\\ \\frac{p_1}{p_2} \\cdot \\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &amp;&gt;&amp; \\frac{p_1}{p_2}\\\\ OR &amp;&gt;&amp; RR \\end{eqnarray*}\\] 4.8.3 Confidence Interval for OR (same idea as with RR) \\[\\begin{eqnarray*} SE(\\ln (\\hat{OR})) &amp;\\approx&amp; \\sqrt{ \\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}} \\end{eqnarray*}\\] So, a \\((1-\\alpha)100\\%\\) CI for the \\(\\ln(OR)\\) is: \\[\\begin{eqnarray*} \\ln(\\hat{OR}) \\pm z_{1-\\alpha/2} SE(\\ln(\\hat{OR})) \\end{eqnarray*}\\] Which gives a \\((1-\\alpha)100\\%\\) CI for the \\(OR\\): \\[\\begin{eqnarray*} (e^{\\ln(OR) - z_{1-\\alpha/2} SE(\\ln(OR))}, e^{\\ln(OR) + z_{1-\\alpha/2} SE(\\ln(OR))}) \\end{eqnarray*}\\] \\(\\frac{583/576}{22/204} = 9.39\\) Back to the example… OR = 9.39. \\[\\begin{eqnarray*} SE(\\ln(\\hat{OR})) &amp;=&amp; \\sqrt{\\frac{1}{583} + \\frac{1}{576} + \\frac{1}{22} + \\frac{1}{204}}\\\\ &amp;=&amp; 0.232\\\\ 90\\% \\mbox{ CI for } \\ln(OR) &amp;&amp; \\ln(9.39) \\pm 1.645 \\cdot 0.232\\\\ &amp;&amp; 2.24 \\pm 1.645 \\cdot 0.232\\\\ &amp;&amp; (1.858, 2.62)\\\\ 90\\% \\mbox{ CI for } OR &amp;&amp; (e^{1.858}, e^{2.62})\\\\ &amp;&amp; (6.41, 13.75)\\\\ \\end{eqnarray*}\\] (SE_lnOR = sqrt( 1/583 + 1/576 + 1/22 + 1/204)) ## [1] 0.2319653 xqnorm(0.95, 0, 1, plot=FALSE) ## [1] 1.644854 log(9.39) - 1.645*0.232 ## [1] 1.858005 log(9.39) + 1.645*0.232 ## [1] 2.621285 exp(log(9.39) - 1.645*0.232) ## [1] 6.410936 exp(log(9.39) + 1.645*0.232) ## [1] 13.75339 We are 90% confident that the true \\(\\ln(OR)\\) is between 1.858 and 2.62. We are 90% confident that the true \\(OR\\) is between 6.41 and 13.75. That is, the true odds of getting lung cancer if you smoke heavily are somewhere between 6.41 and 13.75 times higher than if you don’t, with 90% confidence. Note 1: we use the theory which allows us to understand the sampling distribution for the \\(\\ln(\\hat{OR}).\\) We use the process for creating CIs to transform back to \\(OR\\). Note 2: There are not good general guidelines for checking whether the sample sizes are large enough for the normal approximation. Most authorities agree that one can get away with smaller sample sizes here than for the differences of two proportions. If the sample sizes pass the rough check discussed for \\(\\chi^2\\), they should be large enough to support inferences based on the approximate normality of the log of the estimated odds ratio, too. (Ramsey and Schafer 2012, 541) From one author, for the normal approximation to hold, we need the expected counts in each cell to be at least 5. (Pagano and Gauvreau 2000, 355) Note 3: If any of the cells are zero, many people will add 0.5 to that cell’s observed value. Note 4: The OR will always be more extreme than the RR (one more reason to be careful…) Note 5: \\(RR \\approx OR\\) if RR is very small (the denominator of the OR will be very similar to the denominator of the RR). 4.8.4 Using infer for inference on OR As with the difference in proportions, the infer syntax can be used to simulate a sampling distribution of the sample odds ratio under the null hypothesis that the population proportions are identical. NOTE in order to provide syntax that was comparable and correct for the RR and the OR, smoking has been specified as the response variable, and lungs has been specified as the explanatory variable. library(infer) WynderGraham &lt;- data.frame(lungs = c(rep(&quot;cancer&quot;, 605), rep(&quot;healthy&quot;, 780)), smoking = c(rep(&quot;light&quot;, 22), rep(&quot;heavy&quot;, 583), rep(&quot;light&quot;, 204), rep(&quot;heavy&quot;, 576))) (obs_OR &lt;- WynderGraham %&gt;% specify(smoking ~ lungs, success = &quot;heavy&quot;) %&gt;% calculate(stat = &quot;odds ratio&quot;, order = c(&quot;cancer&quot;, &quot;healthy&quot;))) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 9.39 null_OR &lt;- WynderGraham %&gt;% specify(smoking ~ lungs, success = &quot;heavy&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;odds ratio&quot;, order= c(&quot;cancer&quot;, &quot;healthy&quot;)) null_OR %&gt;% visualize() + shade_p_value(obs_stat = obs_OR, direction = &quot;right&quot;) 4.8.5 Example: MERS-CoV The following study is a case-control study, so it is impossible to estimate the proportion of cases in the population. However, you will notice that the authors don’t try to do that. They flip the explanatory and response variables so that the case status is predicting all of the other clinical variables. In such a setting, the authors would have been able to present relative risk estimates, but they still chose to provide odds ratios (possibly because odds ratios are somewhat standard in the medical literature). Middle East Respiratory Syndrome Coronavirus: A Case-Control Study of Hospitalized Patients10 Background. There is a paucity of data regarding the differentiating characteristics of patients with laboratory-confirmed and those negative for Middle East respiratory syndrome coronavirus (MERS-CoV). Methods. This is a hospital-based case-control study comparing MERS-CoV–positive patients (cases) with MERS-CoV–negative controls. Results. A total of 17 case patients and 82 controls with a mean age of 60.7 years and 57 years, respectively (P = .553), were included. No statistical differences were observed in relation to sex, the presence of a fever or cough, and the presence of a single or multilobar infiltrate on chest radiography. The case patients were more likely to be overweight than the control group (mean body mass index, 32 vs 27.8; P = .035), to have diabetes mellitus (87% vs 47%; odds ratio [OR], 7.24; P = .015), and to have end-stage renal disease (33% vs 7%; OR, 7; P = .012). At the time of admission, tachypnea (27% vs 60%; OR, 0.24; P = .031) and respiratory distress (15% vs 51%; OR, 0.15; P = .012) were less frequent among case patients. MERS-CoV patients were more likely to have a normal white blood cell count than the control group (82% vs 52%; OR, 4.33; P = .029). Admission chest radiography with interstitial infiltrates was more frequent in case patients than in controls (67% vs 20%; OR, 8.13; P = .001). Case patients were more likely to be admitted to the intensive care unit (53% vs 20%; OR, 4.65; P = .025) and to have a high mortality rate (76% vs 15%; OR, 18.96; P &lt; .001). Conclusions. Few clinical predictors could enhance the ability to predict which patients with pneumonia would have MERS-CoV. However, further prospective analysis and matched case-control studies may shed light on other predictors of infection. Consider the results above on diabetes. Of 17 cases, 13 had diabetes; of 82 controls, 35 had diabetes. So the data can be summarized as follows: MERSCoV &lt;- data.frame(coronov = c(rep(&quot;case&quot;, 17), rep(&quot;control&quot;, 82)), diab = c(rep(&quot;hasdiab&quot;, 13), rep(&quot;nodiab&quot;, 4), rep(&quot;hasdiab&quot;, 35), rep(&quot;nodiab&quot;, 47))) table(MERSCoV) ## diab ## coronov hasdiab nodiab ## case 13 4 ## control 35 47 CI for 95% OR As with the calculations above, we can find a CI for the true OR of diabetes for those with MRES-CoV and those without. We are 95% confident that the true odds of diabetes are between 1.31 times and 14.5 times higher for those with CoV than those without. Note that the results calculated here do not match with the results in the paper. (ORhat = (13/4)/(35/47)) ## [1] 4.364286 (SE_lnOR = sqrt( 1/13 + 1/4 + 1/35 + 1/47)) ## [1] 0.6138168 xqnorm(0.975, 0, 1, plot=FALSE) ## [1] 1.959964 log(ORhat) - 1.96 * SE_lnOR ## [1] 0.2703735 log(ORhat) + 1.96 * SE_lnOR ## [1] 2.676536 exp(log(ORhat) - 1.96 * SE_lnOR) ## [1] 1.310454 exp(log(ORhat) + 1.96 * SE_lnOR) ## [1] 14.53465 Working backwards from their percentages, if 13 is 87% of their cases, then there are 15 cases. If 35 is 47% of their controls, then there are 74 controls. Using the revised numbers, the odds ratio would by \\(\\hat{OR}\\) = (13/2)/(35/39) = 7.24, with a CI of (1.53, 34.37). (ORhat = (13/2)/(35/39)) ## [1] 7.242857 (SE_lnOR = sqrt( 1/13 + 1/2 + 1/35 + 1/39)) ## [1] 0.7944404 exp(log(ORhat) - 1.96 * SE_lnOR) ## [1] 1.526401 exp(log(ORhat) + 1.96 * SE_lnOR) ## [1] 34.36776 Figure 4.1: Al-Tawfig et al. Middle East Respiratory Syndrome Coronavirus: A Case-Control Study of Hospitalized Patients 4.9 2/25/20 Agenda Difference in Proportion HT Difference in Proportion CI 4.10 Difference of two proportions 4.10.1 CLT for difference in two proportions As before, we apply the mathematical model (i.e., normal distribution) derived from the central limit theorem to investigate the properties of the statistic of interest. Here, the statistic of interest is the difference in two sample proportions: \\(\\hat{p}_1 - \\hat{p}_2\\). The CLT describes how \\(\\hat{p}_1 - \\hat{p}_2\\) varies as many random samples are taken from the population. As with the single sample proportion, the normal distribution is a good fit only under certain technical conditions: Independence The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment. However, there may be times when the independence condition seems reasonable even if it is not precisely met. Success-failure condition (i.e., large enough sample sized). We need at least 10 successes and 10 failures (expected) in each group. Some authors suggest that 5 of each in each group is sufficient. The Central Limit Theorem for \\(\\hat{p}_1 - \\hat{p}_2\\): The central limit theorem describes how \\(\\hat{p}_1 - \\hat{p}_2\\) varies as many random samples are taken from the population. \\[\\hat{p}_1 - \\hat{p}_2 \\sim N\\Bigg(p_1 - p_2, \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\Bigg)\\] 4.10.2 HT: difference in proportions Note that the equation above describing the central limit theorem has a formula for the variability of \\(\\hat{p}_1 - \\hat{p}_2\\). That is, \\[SE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\] However, when testing a particular hypothesis, the research question does not (usually) provide values of \\(p_1\\) and \\(p_2\\) to use in the formula for the SE. Instead, the research question is usually one of independence, that is, that knowing the level of the explanatory (group) variable tells you nothing about the probability of the response variable. Indeed, typically the null hypothesis is written as: \\[H_0: p_1 = p_2\\] with the alternative hypothesis incorporating the direction of the research claim. In order to calculate a p-value, the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) under \\(H_0\\) is needed. The CLT is a start to understanding the distribution of \\(\\hat{p}_1 - \\hat{p}_2\\), but the additional step which incorporates the null hypothesis of \\(p_1 = p_2\\) is implemented through the SE. If \\(H_0: p_1 = p_2\\) is true, then our best guess for the true value of either \\(p_1\\) or \\(p_2\\) is: \\[\\hat{p}_{pooled} = \\frac{\\mbox{number of successed}}{\\mbox{number of observations}} = \\frac{\\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2}\\] Two proportion z-test To perform a hypothesis test using the normal distribution (i.e., the central limit theorem) we use a z-score as the test statistic and then xpnorm to find the p-value. \\[\\mbox{Z score} = \\frac{(\\hat{p}_1 - \\hat{p}_2) - 0}{\\sqrt{\\frac{\\hat{p}_{pooled}(1-\\hat{p}_{pooled})}{n_1} + \\frac{\\hat{p}_{pooled}(1-\\hat{p}_{pooled})}{n_2}}}\\] \\[\\mbox{p-value} = \\mbox{probability of Z score or more extreme using N(0,1) probability}\\] 4.10.3 CI: difference in proportions When creating a confidence interval for the true parameter of interest, there is no underlying research assumption about the values of \\(p_1\\) and \\(p_2\\). The best we can do to calculate the SE is to use the sample values. population parameter: \\(p_1 - p_2\\): the true difference in success proportion (or probability) between groups 1 and 2. CI for \\(p_1 - p_2\\): \\[(\\hat{p}_1 - \\hat{p}_2) \\pm Z^* \\sqrt{\\frac{\\hat{p}_1 (1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2 (1-\\hat{p}_2)}{n_2}}\\] 4.10.4 Example: Government Shutdown11 The United States federal government shutdown of 2018-2019 occurred from December 22, 2018 until January 25, 2019, a span of 35 days. A Survey USA poll of 608 randomly sampled Americans during this time period reported that 48% (77 of 160 people) of those who make less than $40,000 per year and 55% (247 of 448 people) of those who make $40,000 or more per year said the government shutdown has not at all affected them personally. Notice that the observational units have been selected from the entire population: not by using the response or explanatory variable. (This type of study is called a cross-classification study.) The beauty of having been selected from the entire population is that we have a good sense of both the proportions of each group as well as the proportion of people for whom the shutdown has affected them. Test the research claim that the proportion of people who are affected by the shutdown is different in comparing those who make more than $40,000 and less than $40,000 per year. The p-value for the test is 0.128 indicating that there is no evidence of a difference in the proportion of people affected by the shutdown across the two income groups. NOTE we cannot claim “no difference”!! We claim “there is no evidence of a difference.” Try to explain to yourself (or your classmate) the difference in those two claims. \\[\\mbox{p-value} = 2* P( Z \\leq -1.522) = 0.128\\] (p_pool &lt;- (247+77)/ 614) ## [1] 0.5276873 (z_score &lt;- (0.48 - 0.55) / sqrt(p_pool*(1-p_pool) *(1/160 + 1/448))) ## [1] -1.522447 2*xpnorm(z_score,0,1) ## [1] 0.1278972 A 95% confidence interval for (\\(p_{&lt;40K}- p_{ \\geq40K}\\)) ), where p is the proportion of those who said the government shutdown has not at all affected them personally, is (-0.16, 0.02). (z_star95 &lt;- xqnorm(0.975, 0, 1)) ## [1] 1.959964 (0.48 - 0.55) - z_star95*sqrt(0.48*(1-0.48)/160 + 0.55*(1-0.55)/448) ## [1] -0.1600828 (0.48 - 0.55) + z_star95*sqrt(0.48*(1-0.48)/160 + 0.55*(1-0.55)/448) ## [1] 0.0200828 Determine if the following statements are true or false, and explain your reasoning if you identify the statement as false.12 At the 5% significance level, the data provide convincing evidence of a real difference in the proportion who are not affected personally between Americans who make less than $40,000 annually and Americans who make $40,000 or more annually. We are 95% confident that 16% more to 2% fewer Americans who make less than $40,000 per year are not at all personally affected by the government shutdown compared to those who make $40,000 or more per year. A 90% confidence interval for (\\(p_{&lt;40K}- p_{ \\geq40K}\\)) would be wider than the (-0.16, 0.02) interval. A 95% confidence interval for(\\(p_{ \\geq40K} - p_{&lt;40K}\\)) is (-0.02, 0.16). 4.11 2/27/20 Agenda Observational Studies Experiments Causation 4.12 Types of Studies The two basic types of studies encountered are observational and experimental. In an experiment, researchers assign treatments to cases. That is, the researchers decide who gets which level of the treatment (also known as explanatory variable). When the treatment is assigned randomly, the experiment is known as a randomized experiment. In an observational study, the researchers observe both the explanatory and the response variable without interfering in how the data arise. Remembering the types of variables in most studies, we add one more category of variables: a confounding variable: Explanatory variable is one that is a potential explanation for any changes (here smoking level). Response variable is the measured outcome of interest (here lung cancer). Confounding variable is a variable (typically not measured!) that is associated with both the explanatory and response variables. 4.12.1 Example: Hand Writing &amp; SAT Scores13 An article about handwriting appeared in the October 11, 2006 issue of the Washington Post. The article mentioned that among students who took the essay portion of the SAT exam in 2005-06, those who wrote in cursive style scored significantly higher on the essay, on average, than students who used printed block letters. Researchers wanted to know whether simply writing in cursive would be a way to increase scores. Identify the observational units, the variables, the types of variables, the parameter of interest, and the statistic which was measured. What type of study was it? Q1 does writing in cursive cause higher scores? What are some potential confounding variables? The article also mentioned a different study in which the same one essay was given to all graders. But some graders were shown a cursive version of the essay and the other graders were shown a version with printed block letters. The average score assigned to the essay with the cursive style was significantly higher than the average score assigned to the essay with the printed block letters. Do any of these change? the observational units, the variables, the types of variables, the parameter of interest, and the statistic which was measured. What type of study was it? Q2 can the conclusion include a causal statement now? Why? What changed? 4.12.2 Example: Have a Nice Trip14 An area of research in biomechanics and gerontology concerns falls and fall-related injuries, especially for elderly people. Recent studies have focused on how individuals respond to large postural disturbances (e.g., tripping, induced slips). One question is whether subjects can be instructed to improve their recovery from such perturbations. Suppose researchers want to compare two such recovery strategies, lowering (making the next step shorter, but in normal step time) and elevating (using a longer or normal step length with normal step time). Subjects will have first been trained on one of these two recovery strategies, and they will be asked to apply it after they feel themselves tripping. The researchers will then induce the subject to trip while walking (but harnessed for safety), using a concealed mechanical obstacle. Suppose the following 24 subjects have agreed to participate in such a study. Both males and female were recruited because females tend to have better balance (lower center of gravity). Females: Alisha, Alice, Betty, Martha, Audrey, Mary, Barbie, Anna Males: Matt, Peter, Shawn, Brad, Michael, Kyle, Russ, Patrick, Bob, Kevin, Mitch, Marvin, Paul, Pedro, Roger, Sam The applet at http://www.rossmanchance.com/applets/Subjects.html is helpful for visualizing why confounding variables are removed when the treatment is randomly assigned. Q1 Why would we not want to allow all the women to be trained in the “lowering” technique and all the men trained in the “elevating” technique? Q2 Why do we randomize the treatment? How does it affect gender balance? Height distribution? Gene distribution? Factor “X”? Q3 What if gender balance across the two treatments is required for the study? How is the treatment randomly allocated to the observational units? That is, what would change from Q2? 4.12.3 Study conclusions The ideas surrounding study design typically connect to the question of causality: is it possible or not to infer causality at the end of the study? However, the words we use (“random allocation”) sound a lot like the words we used when describing sampling (“random sample” or “random selection”). Random sampling and random allocation are VERY DIFFERENT concepts! And most importantly, the conclusions made from the two different study characteristics are different. Random selection or Random sample - each unit in the population is equally likely to be chosen for the sample. Random allocation - each observational unit is equally likely to be assigned to any of the treatments (explanatory variable). Figure 4.2: Random Sample vs Randomized Experiment, taken from Ramsey and Schafer (2012) Figure 4.3: Random Sample vs Randomized Experiment, taken from https://askgoodquestions.blog/ In an ideal world, every study would have participants who were randomly sampled from the population and randomly allocated to the treatments. However, the limitations of ethical research makes simultaneously doing both random processes difficult. Why is that? Consider the following: In a clinical trial, it makes sense to randomly allocate the subjects. You cannot, however, randomly select people from the population to take part in the clinical trial. Why not? In a political poll, it seems reasonable that the participants who are called (not necessarily the people who respond!) are a random sample from the population. It does not make sense, however, to randomly allocate those people to different treatments. Why not? 4.13 3/3/20 Agenda More than two proportions Chi-square goodness-of-fit test 4.14 Goodness-of-fit: One categorical variable (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels Consider \\(E_k\\) which is the number expected in the \\(k^{th}\\) category. When testing a null hypothesis of a pre-specified set of proportions (or probabilities) across \\(K\\) categories, the test statistics is: \\[X^2 = \\sum_{k=1}^K \\frac{(O_k - E_k)^2}{E_k} \\sim \\chi^2_{K-1}\\] which has a null sampling distribution which is well-described by a chi-square distribution with \\(K-1\\) degrees of freedom … if: Each case that contributes a count to the table is independent of all the other cases in the table. Each particular scenario (i.e. cell count) has at least 5 expected cases. (sample size criterion) If the conditions don’t hold, then the test statistic won’t have the predicted distribution, so any p-value calculations will be meaningless. 4.14.1 Example: Household Ages15 Suppose we had a class picnic, and all the people in everyone’s household showed up. Would their ages be representative of the ages of all Americans? Probably not. After all, this is not a random sample! But how unrepresentative are the ages? The 2010 Census estimates16 the percent of people in the following age categories. Age 2010 Census Percent &lt;18 24.03% 18-44 36.53% 45+ 39.43% Is the age distribution of the people from households in our class typical of that of all residents of the US? Let’s collect some data. Note that we would never expect the last two columns to have the exact same values, even if the class was a perfect random sample. (Why not?) Age 2010 Census Percent Number Observed in Class Expected Number in Class &lt;18 24.03% 5 12.015 18-44 36.54% 22 18.27 45+ 39.43% 23 19.715 Somehow we need to measure how closely the observed data match the expected values. We have the chi-square statistic (\\(\\chi^2\\)): \\[\\chi^2 = \\sum_{k=1}^K \\frac{(O_k - E_k)^2}{E_k}\\] Let’s use the data collected from class to calculate an observed \\(\\chi^2\\) test statistic. Is it big enough to indicate that individuals from our class’s households don’t follow the 2010 Census proportions? How would we know? We need a null hypothesis! \\(H_0: p_1 = 0.2403, p_2 = 0.3653, p_3 = 0.3943\\) \\(H_A: \\mbox{ not } H_0\\) The null hypothesis is as specified by the 2010 Census. The alternative hypothesis is a deviation from that claim. The observed test statistic is: \\[\\begin{eqnarray*} X^2 &amp;=&amp; \\frac{(5 - 12.015)^2}{12.015} + \\frac{(22 - 18.265)^2}{18.27} + \\frac{(23-19.715)^2}{19.715}\\\\ &amp;=&amp; 5.41 \\end{eqnarray*}\\] But how would we know if the value of the observed test statistic is “large enough” ? We need the distribution of the test statistic assuming the null hypothesis is true. Let’s generate it Age Random Digits Number Observed in Random Sample Expected Number in Random Sample &lt;18 0 - 25 13 \\(50 \\cdot 0.2403 = 12.015\\) 18-44 26 - 60 18 \\(50 \\cdot 0.3654 = 18.27\\) 45+ 61 - 99 19 \\(50 \\cdot 0.3943 = 19.715\\) \\[\\begin{eqnarray*} X^2 &amp;=&amp; \\frac{(13 - 12.015)^2}{12.015} + \\frac{(18 - 18.265)^2}{18.27} + \\frac{(19-19.715)^2}{19.715}\\\\ &amp;=&amp; 0.1105 \\end{eqnarray*}\\] In class, we used random numbers (on pieces of paper) to generate the null sampling distribution of \\(X^2\\). It turns out, there is also a mathematical model which describes the variability of \\(X^2\\): the chi-square distribution with \\(K-1\\) degrees of freedom. The p-value below says that we can’t reject \\(H_0\\), we don’t know that our household ages come from a distribution other than the census percentages. (To be clear: the conclusion is that we know nothing. We don’t have evidence to reject \\(H_0\\). But that also doesn’t mean we know \\(H_0\\) is true. Unfortunately, we can’t conclude anything.) 1 - xpchisq(5.41, 2) ## [1] 0.06687032 4.14.2 Example: Flax Seed Researchers studied a mutant type of flax seed that they hoped would produce oil for use in margarine and shortening. The amount of palmitic acid in the flax seed was an important factor in this research; a related factor was whether the seed was brown or was variegated. The seeds were classified into six combinations or palmitic acid and color. According to a hypothesized genetic model, the six combinations should occur in a 3:6:3:1:2:1 ratio. Color Acid Level Observed Expected Brown Low 15 13.5 Brown Intermediate 26 27 Brown High 15 13.5 Variegated Low 0 4.5 Variegated Intermediate 8 9 Variegated High 8 4.5 Total 72 72 \\[\\begin{eqnarray*} H_0: &amp;&amp; p_1 = 3/16, p_2=6/16, p_3 = 3/16, p_4 = 1/16, p_5=2/16, p_6 = 1/16\\\\ H_A: &amp;&amp; \\mbox{ not the distribution in } H_0 \\end{eqnarray*}\\] \\[\\begin{eqnarray*} \\chi^2 &amp;=&amp; \\frac{(15-13.5)^2}{13.5} + \\frac{(26-27)^2}{27} + \\frac{(15-13.5)^2}{13.5} + \\frac{(0-4.5)^2}{4.5} + \\frac{(8-9)^2}{9} + \\frac{(8-4.5)^2}{4.5}\\\\ &amp;=&amp; 7.71\\\\ \\mbox{p-value} &amp;=&amp; P(\\chi^2_5 \\geq 7.71)\\\\ &amp;=&amp; 0.173\\\\ \\end{eqnarray*}\\] 1 - xpchisq(7.71, 5) ## [1] 0.172959 How could we simulate power? Consider the flax seed example, As with the household ages example, use random digits. Come up with an alternative hypothesis that specified the probabilities of each type of seed. Allocate digits appropriately given the alternative model. Randomly generate 72 random digits (from 00 to 99) and collect observed data based on the alternative model. Calculate the test statistic from the randomly generated observed data (as compared to the expected counts under \\(H_0\\)), and indicate whether it is above 11.07 (see below for the \\(\\chi^2_5\\) cutoff). Repeat 3 &amp; 4 many many times. The power will be estimated by the proportion of times you reject the null hypothesis when the alternative is true. xqchisq(.95, 5) ## [1] 11.0705 4.15 3/5/20 Agenda More than two levels (two variables) Chi-square test of independence 4.16 Independence: Two categorical variables (\\(\\chi^2\\) test) \\(\\geq\\) 2 levels each As when we were working with binary variables, most research questions have to do with two variables. Our main question now will be whether there is an association between two categorical variables of interest. \\(H_0\\):the two variables are independent \\(H_A\\): the two variables are not independent How do we know if our test statistic is a big number or not? Well, it turns out that the test statistic will have an approximate \\(\\chi^2\\) distribution with degrees of freedom = \\((r- 1)\\cdot (c-1)\\) when \\(H_0\\) is true. As long as: We have a random sample from the population. We expect at least 1 observation in every cell (\\(E_i \\geq 1 \\forall i\\)) We expect at least 5 observations in 80% of the cells (\\(E_i \\geq 5\\) for 80% of \\(i\\)) \\[X^2 = \\sum_{\\mbox{all cells}} \\frac{(Obs - Exp)^2}{Exp} \\sim \\chi^2_{(r-1)(c-1)}\\] Consider the following (silly?) example data on CA vs. notCA and soda preference: CA no CA total Coke 72 8 80 Pepsi 18 22 40 total 90 30 120 What if we had those same number of people in each group and category, but we wanted absolutely no association between the two variables of soda preference and location: CA no CA total Coke 80 Pepsi 40 total 90 30 120 If the distribution of Coke and Pepsi preference were the same in CA vs not CA, how many Californians would prefer Coke? 60! \\[\\mbox{# CA who prefer Coke} = 90 \\cdot \\frac{80}{120} = 60\\] The rest of the table can be filled out in a similar manner: CA no CA total Coke 60 20 80 Pepsi 30 10 40 total 90 30 120 The Coke &amp; Pepsi example motivates the idea of how many observations we expect to see in each cell if there is no association between the variables. Note that the expected number is almost always a decimal value. \\[\\mbox{Exp} = \\frac{(\\mbox{row total})(\\mbox{col total})}{\\mbox{table total}}\\] 4.16.1 Example: Nightlights17 Myopia, or near-sightedness, typically develops during the childhood years. Recent studies have explored whether there is an association between development of myopia and the use of night-lights with infants. Quinn, Shin, Maguire, and Stone (1999) examined the type of light children aged 2-16 were exposed to. Between January and June 1998, the parents of 479 children who were seen as outpatients in a university pediatric ophthalmology clinic completed a questionnaire (children who had already developed serious eye conditions were excluded). One of the questions asked was “Under which lighting condition did/does your child sleep at night?” before the age of 2 years. The following two-way table classifies the children’s eye condition and whether or not they slept with some kind of light (e.g., a night light or full room light) or in darkness. The data are given by the following R code: lights &lt;- data.frame(eyesight = c(rep(&quot;far&quot;, 40), rep(&quot;neither&quot;, 114), rep(&quot;near&quot;, 18), rep(&quot;far&quot;, 39), rep(&quot;neither&quot;, 115), rep(&quot;near&quot;, 78), rep(&quot;far&quot;, 12), rep(&quot;neither&quot;, 22), rep(&quot;near&quot;, 41)), lighting = c(rep(&quot;dark&quot;, 172), rep(&quot;nightlight&quot;, 232), rep(&quot;roomlight&quot;, 75))) table(lights) ## lighting ## eyesight dark nightlight roomlight ## far 40 39 12 ## near 18 78 41 ## neither 114 115 22 lights %&gt;% ggplot() + geom_bar(aes(x = lighting, fill = eyesight), position = &quot;fill&quot;) \\(H_0\\): There is no association between lighting condition and eye condition \\(H_A\\): There is an association between lighting condition and eye condition What are the observational units? What are the explanatory and response variables? Let’s say that we conclude there is an association (we reject \\(H_0\\)). Can we also conclude that lighting causes particular eye conditions? Try to come up with as many confounding variables as possible. The chi-square test can be applied to the table of counts. The test statistic is 56.513 with a very small p-value. Note that the observed and expected tables can be pulled out of the chisq.test() output. (chi.lights &lt;- lights %&gt;% select(eyesight, lighting) %&gt;% table() %&gt;% chisq.test()) ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 56.513, df = 4, p-value = 1.565e-11 chi.lights$observed ## lighting ## eyesight dark nightlight roomlight ## far 40 39 12 ## near 18 78 41 ## neither 114 115 22 chi.lights$expected ## lighting ## eyesight dark nightlight roomlight ## far 32.67641 44.07516 14.24843 ## near 49.19415 66.35491 21.45094 ## neither 90.12944 121.56994 39.30063 The conclusion from Inv 5.3 in Chance and Rossman (2018) is excellent: The segmented bar graph reveals that for the children in this sample the incidence of near-sightedness increases as the level of lighting increases. When we have a random sample with two categorical variables, we can perform a chi-square test of association. Because the expected counts are large (smallest is 14.25 &gt; 5), we can apply the chi-square test to these data. The p-value of this chi-square test is essentially zero, which says that if there were no association between eye condition and lighting in the population, then it’s virtually impossible for chance alone to produce a table in which the conditional distributions would differ by as much as they did in the actual study. Thus, the sample data provide overwhelming evidence that there is indeed an association between eye condition and lighting in the population of children like those in this study. A closer analysis of the table and the chi-square calculation reveals that there are many fewer children with near-sightedness than would be expected in the “darkness” group and many more children with near-sightedness than would be expected in the “room light” group. But remember, we cannot draw a cause-and-effect conclusion between lighting and eye condition because this is an observational study. Several confounding variables could explain the observed association. For example, perhaps near-sighted children tend to have near-sighted parents who prefer to leave a light on because of their own vision difficulties, while also passing this genetic predisposition on to their children. We also have to be careful in generalizing from this sample to a larger population because the children were making voluntary visits to an eye doctor and were not selected at random from a larger population. 4.17 3/10/20 &amp; 3/12/20 Agenda Review for exam Exam 1 4.18 3/17/20 &amp; 3/19/20 Agenda Spring Break 1 ! 4.19 3/24/20 Agenda Census 4.20 Census I recently filled out my census form. Here is the full list of questions that I was asked: address name (of all residents at my address); including how many people live at the address the ownership status of the residence where I live my sex (only binary options provided, asked for all residents) birthday (asked for all residents) Hispanic, Latino, or Spanish origin (asked for all residents) race (with ethnicity options, asked for all residents) relationship status of each of the residents to me. flexibility given for marital status, family status, etc. but no flexibility for sex (e.g., I could choose whether my spouse and I were married or not, but I was required to choose whether they were the same sex as me or the opposite sex as me) 4.20.1 What about College students? The Census provides great information at their website (particularly with respect to the changes due to COVID-19). https://2020census.gov/en/who-to-count.html https://2020census.gov/en/news-events/press-releases/modifying-2020-operations.html College students who live away from home should be counted at the on- or off-campus residence where they live and sleep most of the time, even if they are at home on April 1, 2020. If they live in housing designed for college students (such as dorms and apartments with “by-the-bed” leases), they will be counted as part of the Group Quarters Operation. If they live off campus in housing that is not designed for college students (such as a private house or apartment), they should count themselves at that address. During our recent 2020 Census Group Quarters Advance Contact operation we contacted college/university student housing administrators to get their input on the enumeration methods that will allow students to participate in the 2020 Census. The majority, about 47 percent, have chosen the eResponse methodology and about 7 percent chose paper listings… About 35 percent, however, chose drop-off/pick-up which allows students to self-respond using an Individual Census Questionnaire (or ICQ). We are contacting those schools to ask whether they would like to change that preference in light of the emerging situation. In general, students in colleges and universities temporarily closed due to the COVID-19 virus will still be counted as part of this process. Even if they are home on census day, April 1, they should be counted according to the residence criteria which states they should be counted where they live and sleep most of the time. 4.21 3/26/20 Agenda COVID-19 4.22 COVID-19 Thursday’s optional class meeting is about COVID-19 and how it connects to the ideas in introductory statistics. T A good starting place for solid information about COVID-19: https://www.sciencemuseumgroup.org.uk/coronavirus-science-what-we-know-and-dont-know-about-the-virus/ 4.22.0.1 Dashboard for Predicting Statistician Create COVID-19 Dashboard to Predict Infection: https://magazine.amstat.org/blog/2020/06/01/dashboard-to-predict-infection/ Dashboard is here: https://covid19.stat.iastate.edu/ Visualizing the data: I find these two dashboards to be among the best out there. Good display, constantly being updated, reliable data. Coronavirus COVID-19 Global Cases by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University (JHU): https://www.arcgis.com/apps/opsdashboard/index.html#/bda7594740fd40299423467b48e9ecf6 COVID-19 Global Pandemic Real-time Report: https://ncov.dxy.cn/ncovh5/view/en_pneumonia?link=&amp;share=&amp;source= That said, it is also worth thinking about how to visualize the data and to do so responsibly. Visualizing COVID-19 data (responsibly): https://medium.com/nightingale/ten-considerations-before-you-create-another-chart-about-covid-19-27d3bd691be8 Current medical studies on treatment of COVID-19: French study investigating azithromycin – chloroquine on COVID-19. https://www.sciencedirect.com/science/article/pii/S0924857920300996 French Confirmed COVID-19 patients were included in a single arm protocol from early March to March 16th, to receive 600mg of hydroxychloroquine daily and their viral load in nasopharyngeal swabs was tested daily in a hospital setting. Depending on their clinical presentation, azithromycin was added to the treatment. Untreated patients from another center and cases refusing the protocol were included as negative controls. Presence and absence of virus at Day6-post inclusion was considered the end point. Assuming a 50% efficacy of hydroxychloroquine in reducing the viral load at day 7, a 85% power, a type I error rate of 5% and 10% loss to follow-up, we calculated that a total of 48 COVID-19 patients (i.e., 24 cases in the hydroxychloroquine group and 24 in the control group) would be required for the analysis (Fleiss with CC). Statistical differences were evaluated by Pearson’s chi-square or Fisher’s exact tests as categorical variables, as appropriate. Means of quantitative data were compared using Student’s t-test. Figure 4.4: Gautret et al. Hydroxychloroquine and azithromycin as a treatment of COVID-19: results of an open-label non-randomized clinical trial In late March, WHO launches global megatrial of the four most promising coronavirus treatments (not any antibiotics). The study, which could include many thousands of patients in dozens of countries, has been designed to be as simple as possible so that even hospitals overwhelmed by an onslaught of COVID-19 patients can participate. https://www.sciencemag.org/news/2020/03/who-launches-global-megatrial-four-most-promising-coronavirus-treatments# Enrolling subjects in SOLIDARITY will be easy. When a person with a confirmed case of COVID-19 is deemed eligible, the physician can enter the patient’s data into a WHO website, including any underlying condition that could change the course of the disease, such as diabetes or HIV infection. The participant has to sign an informed consent form that is scanned and sent to WHO electronically. After the physician states which drugs are available at his or her hospital, the website will randomize the patient to one of the drugs available or to the local standard care for COVID-19. “After that, no more measurements or documentation are required,” says Ana Maria Henao Restrepo, a medical officer at WHO’s Department of Immunization Vaccines and Biologicals. Physicians will record the day the patient left the hospital or died, the duration of the hospital stay, and whether the patient required oxygen or ventilation, she says. “That’s all.” The design is not double-blind, the gold standard in medical research, so there could be placebo effects from patients knowing they received a candidate drug. But WHO says it had to balance scientific rigor against speed. Studies related to COVID-19: Medical studies discussing side effects of azithromycin – chloroquine treatment. https://threadreaderapp.com/thread/1242119303811514369.html Studies on cloth masks vs medical masks for healthcare workers. https://bmjopen.bmj.com/content/5/4/e006577 Different blood types and COVID-19 https://www.medrxiv.org/content/10.1101/2020.03.11.20031096v1 (not yet peer-reviewed) The authors write of a “significantly higher risk” for blood group A but, as one reader pointed out in the comments section, this does not mean that the risk is greatly higher; it means that a p-value was small. Notice that the CI for the odds ratio is above 1, but it comes close to 1 and is centered a 1.20 and not, say, 2.5. “Meta-analyses on the pooled data showed that blood group A had a significantly higher risk for COVID-19 (odds ratio-OR, 1.20; 95% confidence interval-CI [1.02,1.43], P = 0.02) compared with non-A blood groups.” The language might lead the reader to think “blood group A has a much higher risk for COVID-19” which is markedly untrue! That is, significance is a completely different concept as compared to “a lot”. Being careful with your analysis: CDC report: “Severe Outcomes Among Patients with Coronavirus Disease 2019 (COVID-19) — United States, February 12–March 16, 2020” https://www.cdc.gov/mmwr/volumes/69/wr/mm6912e2.htm Pay attention to counts per group. 20-44 is 24 years. 65-74 is 9 years. Figure 4.5: CDC report: Severe Outcomes Among Patients with Coronavirus Disease 2019 (COVID-19) — United States, February 12–March 16, 2020 There has been some talk about a 2% fatality rate, but fatality is incredibly difficult to measure so early in the disease. If the number of reported confirmed cases of COVID-19 continues to slow down, the 2% fatality rate people have been quoting may appear to rise because of two main factors: under-reporting of the number of cases and the delay from symptoms first appearing to death. It is possible that the errors will cancel each other out and end up being correct for the wrong reasons! Also, the “fatality rate” is an incredibly misleading number because it varies so much based on age. Averaging over all ages will give different numbers based on the age distribution of the country at hand. And maybe just as important &amp; harder to measure: what is the fatality rate for the pandemic? The pandemic will cause deaths directly due to the coronavirus and also due to: cardiac arrest (or other emergency condition) without adequate space in ERs; lack of food / heat for people who are unable to work; lack of access to medical supplies / dialysis / pharmaceuticals; etc. What is the trade-off between putting a cap on the disease and resisting tracking our personal data? Can smart thermometers help track the coronavirus? (March 18, 2020) https://www.nytimes.com/2020/03/18/health/coronavirus-fever-thermometers.html Follow-up: Restrictions are Slowing Coronavirus Infections, New Data Suggests (March 30, 2020) https://www.nytimes.com/2020/03/30/health/coronavirus-restrictions-fevers.html?searchResultPosition=1 Social distancing scoreboard based on movement of mobile phones: https://www.unacast.com/covid19/social-distancing-scoreboard (info about their work: https://www.unacast.com/post/the-unacast-social-distancing-scoreboard) 4.23 Reflection Questions 4.23.1 (no ISRS) Binomial probabilities (Math 58 only) How can the binomial distribution be used to calculate probabilities? What are the technical conditions of the binomial distribution? How is the normal distribution different from the binomial distribution? (one answer is that the normal describes a continuous variable and the binomial describes a discrete variable. what does that mean? what is another distinction?) What are the technical conditions allowing the normal distribution to approximate the binomial distribution? What is one reason to choose to use the normal distribution? What is one reason to choose to use the binomial distribution? 4.23.2 (no ISRS) Relative Risk &amp; Odds Ratios (Math 58B only) What is the differences between cross-classification, cohort, and case-control studies? When is it not appropriate to calculate differences or ratios of proportions? Why isn’t it appropriate? How are odds calculated? How is OR calculated? What do we do when we we can’t calculate statistics based on proportions? Why does this ``fix&quot; work? What is the statistic of interest? What is the parameter of interest? Why do we look at the natural log of the RR and the natural log of the OR when finding confidence intervals for the respective parameters? How do you calculate the SE for the \\(\\ln(\\hat{RR})\\) and \\(\\ln(\\hat{OR})\\)? Once you have the CI for \\(\\ln(RR)\\) or for \\(\\ln(OR)\\), what do you do? Why does that process work? 4.23.3 2 binary variables: Chapter 3, Section 2 What is the statistic of interest? What is the parameter of interest? How does the inference change now that there is binary (response) data taken from two populations? How does the inference stay the same now that there is binary (response) data taken from two populations? What does the Central Limit Theorem say about two sample proportions? When is it appropriate to apply a hypothesis test to the data? And when is it appropriate to apply a confidence interval to the data? How do we calculate SE(\\(\\hat{p}_1 - \\hat{p}_2\\))? What technical conditions must hold for the Central Limit Theorem to apply? 4.23.4 Types of studies: Chapter 1, Sections 4-5 What is the difference between an observational study and an experiment? Why aren’t all studies done as experiments? What is a confounding variable? Have you looked at Figure 4.2 and Figure 4.3? Do you understand the two figures? Could you explain what their main message is to a friend? [Random sampling vs. Random allocation] How is the statistical meaning of the word cause different from the usage in the sentence: The ball that hit me in the head caused me to get a headache. What are the meanings of the words: randomized, double-blind (single-blind), control, placebo, significant, and comparative. Why are these ideas important to interpreting study results? 4.23.5 2 categorical variables: Chapter 3, Section 3 How would you describe the data seen in \\(r \\times c\\) tables? Describe the simulation mechanism that creates a sampling distribution under the assumption that the null hypothesis is true (like the cards in the first week of class using the gender discrimination example). What is the test statistic (for both the infer simulation and the chi-square test with the mathematical model!!)? Why do we need a complicated test statistic here and we didn’t need one with \\(2 \\times 2\\) tables? How do you compute the expected count? What is the intuition behind the computation? What is one benefit that the two sample z-test of proportions has? That is, what is one thing we can do if we have a \\(2\\times 2\\) table instead of an \\(r \\times c\\) table? Describe the directionality of the test statistic. That is, what values of \\(X^2\\) make you reject \\(H_0\\)? What are the technical assumptions for the chi-square test? Why do you need the technical assumptions? What are the null and alternative hypotheses? References "],
["inference-for-numerical-data.html", "Chapter 5 Inference for numerical data 5.1 3/31/20 Agenda 5.2 Important measures related to quantitative (numeric) variables 5.3 4/2/20 Agenda 5.4 4/7/20 Agenda 5.5 Inference for a single mean, \\(\\mu\\) 5.6 4/9/20 Agenda 5.7 4/14/20 Agenda 5.8 Comparing two independent means 5.9 R code for inference on 1 or 2 means. 5.10 Reflection Questions", " Chapter 5 Inference for numerical data 5.1 3/31/20 Agenda New statistics: mean, standard deviation, standard error of the mean Sampling distribution of the sample mean 5.2 Important measures related to quantitative (numeric) variables 5.2.1 Quantitative Descriptives What measures can we look at to get a first sense of whether two groups are different (let alone substantially different enough for us to conclude a difference in a related population). We might look at what is called the Five Number Summary. Five Number Summary: Min, Q1, Median, Q3, Max Q1 = median of the values below the median Q3 = median of the values above the median IQR = Interquartile range (measure of spread/variability) = Q3 - Q1 1.5 x IQR rule for possible outliers: If an observation falls more than 1.5IQR outside of Q1 or Q3, flag the observation as a possible outlier. Boxplot Box spans Q1 to Q3 Line in box marks median (M) Perpendicular line extends from box to smallest and largest observations within 1.5IQR of Q1 and Q3. Dots for observations outside of 1.5IQR Summaries often used when variable has a bell-shaped distribution \\[\\begin{eqnarray*} \\mbox{sample mean} &amp;=&amp; \\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\\\ \\mbox{sample standard deviation} &amp;=&amp; s = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2}\\\\ \\mbox{sample variance} &amp;=&amp; s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\end{eqnarray*}\\] Loosely, the standard deviation is the size of the typical deviation from the mean of the data set. Note that we divide by \\(n-1\\) instead of by \\(n\\) because the true deviation is defined as the average of the observations from the true mean \\(\\mu\\), and, in fact, they will always be closer to \\(\\overline{X}\\) than to \\(\\mu\\). 5.2.2 Sampling distribution of a sample mean As before, the Central Limit Theorem tells us that averages are normally distributed if the sample size is large enough. Here, that means: \\[\\overline{X} \\sim N(\\mu, \\sigma/\\sqrt{n})\\] where \\(SD(\\overline{X}) = \\sigma/\\sqrt{n}\\) and \\(SE(\\overline{X}) = s/\\sqrt{n}\\). \\(\\mu\\) is the center of the population of observations from which the sample data were taken. \\(\\sigma\\) is the variability of the population of observations from which the sample data were taken. As before, we won’t spend much time worried about the difference between \\(SD(\\overline{X})\\) and \\(SE(\\overline{X})\\). Generally, we’ll only know / use \\(SE(\\overline{X}) = s/\\sqrt{n}\\). Typically, with quantitative variables, “large enough” is at least 30 or so observations. Spend some time clicking through different datasets in the ICAM applet: http://www.rossmanchance.com/applets/OneSample.html?showBoth=1 You should notice: If the population (or sample of data) is skewed, the sampling distribution of the sample mean is normal (bell-shaped) when the sample size is large. The larger the sample size, the less variable the sampling distribution. The sample size does not change the distribution of the dataset (the middle graph). The middle graph will always be a representation of the population graph (left side); although with small sample sizes, the middle graph is somewhat sparse. In an actual data analysis, we only see the middle graph. We do not see the population graph (left side) or the sampling distribution (right side). 5.3 4/2/20 Agenda The t-distribution Standardized t-score Hypothesis Testing &amp; Confidence Intervals for one mean 5.4 4/7/20 Agenda Review of confidence interval for one mean Prediction Interval for a future observation 5.5 Inference for a single mean, \\(\\mu\\) 5.5.1 Mathematical model for distribution of the sample mean Before coming up with the mathematical model appropriate for this section, it is important to notice that we almost never know the true variability of the data (i.e., \\(\\sigma\\)). Instead, we almost always have to estimate \\(\\sigma\\) using \\(s\\), the sample standard deviation. It turns out that when the estimate of the variability is used in the denominator, the sampling distribution becomes more variability (longer tails). Recall that it is the tails of the distribution in which we are the most interested, so we don’t want to get those wrong!! If \\(\\sigma\\) is somehow known: \\[\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\] But in the more typical situation where \\(\\sigma\\) is estimated using \\(s\\): \\[\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{df = n-1}\\] 5.5.1.1 Hypothesis Testing (ISRS 4.1) If \\(H_0: \\mu = \\mu_0\\) is true, then we know that: \\[\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{df = n-1}\\] That is, we can use the \\(t_{df = n-1}\\) distribution to find the p-value for the test. Note, in R we we use the function xpt in the mosaic package. 5.5.1.2 Confidence Intervals (ISRS 4.1.4) In the setting where there is no null hypothesis statement and an interval estimate is needed, the interval is created in the exact same way as was done with proportions using: \\[\\overline{X} \\pm t_{n-1}^* \\cdot SE(\\overline{X})\\] Which is the same thing as: \\[\\overline{X} \\pm t_{n-1}^* \\cdot s/ \\sqrt{n}\\] 5.5.1.3 Prediction Intervals (ISCAM 2.6, not in ISRS) A prediction interval is different from a confidence interval!!! Remember that a confidence interval is a range of values that try to capture a parameter. A prediction interval is meant to capture 95% of future observations (see below for the example on healthy body temperatures). Note that in order to capture the variability in the observations, we combine the variability of the center of the interval (\\(s/\\sqrt{n}\\)) with the variability of the observations themselves (\\(s\\)). A \\((1-\\alpha)100%\\) prediction interval has a \\((1-\\alpha)\\) probability of capturing a new observation from the population. \\[\\overline{X} \\pm t_{n-1}^* \\cdot s \\sqrt{1 + \\frac{1}{n}}\\] 5.5.2 Example: healthy body temperature18 The study at hand is meant to determine whether the average healthy body temperature is actually 98.6 F.19 Body temperatures (oral temperatures using a digital thermometer) were recorded for healthy men and women, aged 18-40 years, who were volunteers in Shigella vaccine trials at the University of Maryland Center for Vaccine Development, Baltimore. For these adults, the mean body temperature was found to be 98.249 F with a standard deviation of 0.733 F.20 In order to work through the analysis it is imperative that we understand the data that was collected as part of the research. center variability of data variability of sample means sample size \\(\\overline{X} = 98.249\\) F \\(s = 0.733\\) F \\(SE(\\overline{X}) = \\frac{s}{\\sqrt{n}} = \\frac{0.733}{\\sqrt{130}} = 0.0643\\) \\(n=130\\) \\(\\mu\\) = true ave healthy body temp (unknown!) \\(\\sigma\\) = true sd of healthy body temps (unknown!) \\(SD(\\overline{X}) = \\frac{\\sigma}{\\sqrt{n}}\\) = unknown! 5.5.2.1 Hypothesis test on true average healthy body temperature The first research question we want to ask is: how surprising would it be to select a group of 13 participants who have an average healthy body temperature of 98.249 F ? The questions is set up perfectly for a hypothesis test! \\(H_0: \\mu = 98.6\\) \\(H_A: \\mu \\ne 98.6\\) We use the t-distribution to investigate the claim. \\[t-score = \\frac{98.249 - 98.6}{0.733/\\sqrt{130}} = -5.46\\] How likely is the standardized version of our test statistic to happen if the null hypothesis is true? Well, if \\(H_0\\) is true, then the t-statistics should have a t-distribution. So we can use the t-distribution to find the p-value (recall that the p-value is the probability of the data or more extreme if \\(H_0\\) is true.) The test statistic is -5.46, and even a two-sided p-value (the area doubled) is way less than 0.001. 2 * mosaic::xpt(-5.46, df = 129, ncp = 0) ## [1] 2.354246e-07 5.5.2.2 Confidence interval for true average healthy body temperature Possibly more interesting is the confidence interval which would tell us a range of plausible values for healthy body temperatures. The confidence interval is given by the following formula: \\[\\overline{X} \\pm t_{n-1}^* \\cdot s/ \\sqrt{n}\\] and is calculated to be (98.121, 98.376). That is, we are 95% confident that the true average healthy body temperature is somewhere between 98.121 F and 98.376 F. Note that 98.6 F is not in the interval!!! Wow. mosaic::xqt(.975, df = 129) ## [1] 1.978524 98.249 - 1.9785 * 0.733 / sqrt(130) ## [1] 98.12181 98.249 + 1.9785 * 0.733 / sqrt(130) ## [1] 98.37619 5.5.2.3 Prediction interval for individual healthy body temperatures21 Note the fundamental difference between the goal of the confidence interval above and the goal of the prediction interval calculated in this section. A confidence interval is an interval of plausible values for a population parameter. A prediction interval is for a future individual observations. A \\((1-\\alpha)100%\\) prediction interval has a \\((1-\\alpha)\\) probability of capturing a new observation from the population. Here, a 95% prediction interval for healthy body temperatures can be calculated using: \\[\\overline{X} \\pm t_{n-1}^* \\cdot s \\cdot \\sqrt{1 + \\frac{1}{n}}\\] \\[98.249 \\pm t_{129}^* \\cdot 0.733 \\cdot \\sqrt{1 + \\frac{1}{130}}\\] Which gives a 95% prediction interval of (96.79 F, 99.70 F). There is a 0.95 probability that if I reach into the population, the person selected will have a healthy body temperature between 96.79 F and 99.70 F. Said differently, 95% of the individuals in the population will have a healthy body temperature between 96.79 F and 99.70 F (a much wider range of values than the confidence interval!) mosaic::xqt(.975, df = 129) ## [1] 1.978524 98.249 - 1.9785*0.733*sqrt(1 + 1/130) ## [1] 96.79319 98.249 + 1.9785*0.733*sqrt(1 + 1/130) ## [1] 99.70481 5.6 4/9/20 Agenda under the random sample model: Sampling distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) Hypothesis testing (and CI) of \\(\\mu_1 - \\mu_2\\) 5.7 4/14/20 Agenda under the random allocation model: Sampling distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) Confidence interval (and HT) for \\(\\mu_1 - \\mu_2\\) 5.8 Comparing two independent means It turns out that in both the setting where random samples are taken (e.g., NBA salaries) and the setting where random allocation is done (e.g., sleep deprivation), the t-distribution describes the distribution of the test statistic quite well. Note that the variability associated with the difference in means uses the variability of both the samples (and their individual sample sizes!). \\[\\begin{eqnarray*} \\mbox{parameter} &amp;=&amp; \\mu_1 - \\mu_2\\\\ \\mbox{statistic} &amp;=&amp; \\overline{X}_1 - \\overline{X}_2\\\\ SE_{\\overline{X}_1 - \\overline{X}_2} &amp;=&amp; ????? \\end{eqnarray*}\\] In general, the math is done on the variance (which is just the squared standard deviations). \\[\\begin{eqnarray*} var(A - B) &amp;=&amp; var(A) + var(B)\\\\ var(\\overline{X}_1 - \\overline{X}_2) &amp;=&amp; var(\\overline{X}_1) + var(\\overline{X}_2)\\\\ &amp;=&amp; \\sigma^2_1 / n_1 + \\sigma^2_2 / n_2\\\\ SE(\\overline{X}_1 - \\overline{X}_2) &amp;=&amp; \\sqrt{s^2_1 / n_1 + s^2_2 / n_2}\\\\ \\end{eqnarray*}\\] The above methods can be used when the samples are of different sizes and when the variability in the two samples is quite different (\\(s_1 \\ne s_2\\)). If we use the above procedures, the exact degrees of freedom are not straightforward to calculate: \\[\\begin{eqnarray*} df &amp;=&amp; \\frac{ \\bigg(\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2} \\bigg)^2}{ \\bigg[ \\frac{(s^2_1/n_1)^2}{n_1 - 1} + \\frac{(s^2_2/n_2)^2}{n_2 - 1} \\bigg] }\\ \\ \\ \\ \\ \\ \\ \\mbox{Yikes!!!!}\\\\ df &amp;\\approx&amp; \\min \\{ n_1 - 1, n_2 -1 \\} \\\\ \\end{eqnarray*}\\] With the SE appropriately defined, the hypothesis test and confidence interval follow the methods from earlier in the semester. \\(H_0: \\mu_1 - \\mu_2 = 0\\) \\(H_A: \\mu_1 - \\mu_2 \\ne 0\\) When \\(H_0\\) is true: \\[\\begin{eqnarray*} t &amp;=&amp; \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)_0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}\\\\ &amp;=&amp; \\frac{(\\overline{X}_1 - \\overline{X}_2) - 0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}\\\\ &amp;\\sim&amp; t_{\\min \\{ n_1 - 1, n_2 -1 \\} } \\end{eqnarray*}\\] Which means that the \\(t_{\\min \\{ n_1 - 1, n_2 -1 \\} }\\)-distribution can be used to find a p-value associated with the t-score: \\[\\mbox{t-score} = \\frac{(\\overline{X}_1 - \\overline{X}_2) - 0}{\\sqrt{s_1^2 / n_1 + s_2^2 / n_2}}.\\] Additionally, a \\((1-\\alpha)100\\)% confidence interval for \\((\\mu_1 - \\mu_2)\\) can be found by computing: \\[(\\overline{X}_1 - \\overline{X}_2) \\pm t_{\\min \\{ n_1 - 1, n_2 -1 \\} }^* \\cdot \\sqrt{s_1^2 / n_1 + s_2^2 / n_2}.\\] 5.9 R code for inference on 1 or 2 means. Above, R is used primarily as a calculator and a way to find the appropriate values from the t-distribution (using mosaic::xpt and mosaic::xqt \\(\\rightarrow\\) note that along with the first argument (either a probability or a place on the x-axis) it is important to add the degrees of freedom df and possibly the argument ncp=0 which centers the graph at zero). Consider the teacher salary data available in the OpenIntro textbook. This data set contains teacher salaries from 2009-2010 for 71 teachers employed by the St. Louis Public School in Michigan, as well as several covariates. Posted on opendata.socrata.com by Jeff Kowalski. Original source: http://stlouis.edzone.net teachers &lt;- read_delim(&quot;https://www.openintro.org/data/tab-delimited/teacher.txt&quot;, delim= &quot;\\t&quot;) 5.9.1 t.test The function which is typically used to do t-tests is the function t.test. Note that the t.test function requires a complete dataset, not just the summary statistics. However, the t.test can be used to do any of the variety of tests we’ve seen (and the ones we haven’t seen!): one sample t-test, two independent samples t-test (with or without equal variances), paired t-test. 5.9.1.1 One sample t-test For example, we might be interested in testing whether the average salary (of all teachers in St Louis) is above $47,000 a year. The p-value is extremely small. We reject \\(H_0\\). That is, we can claim that the true average base salary is above $47,000. (Note, to calculate a CI, use alternative = &quot;two.sided&quot;.) \\(H_0: \\mu = 47,000\\) \\(H_A: \\mu &gt; 47,000\\) t.test(teachers$base, mu = 47000, alternative = &quot;greater&quot;) ## ## One Sample t-test ## ## data: teachers$base ## t = 7.9466, df = 70, p-value = 1.146e-11 ## alternative hypothesis: true mean is greater than 47000 ## 95 percent confidence interval: ## 54440.82 Inf ## sample estimates: ## mean of x ## 56415.96 5.9.1.2 Two independent samples t-test Or, maybe interest is in knowing whether the base salary for teachers with a BA degree is less than those with an MA degree, on average. Note, \\(\\mu\\) denotes the average salary in the population group denoted by the subscript. The p-value is 0.442, so we would not reject the null hypothesis. (Note, to calculate a CI, use alternative = &quot;two.sided&quot;.) \\(H_0: \\mu_{BA} = \\mu_{MA}\\) \\(H_A: \\mu_{BA} &lt; \\mu_{MA}\\) t.test(base ~ degree, alternative = &quot;less&quot;, data = teachers) ## ## Welch Two Sample t-test ## ## data: base by degree ## t = -0.14639, df = 65.238, p-value = 0.442 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf 3664.912 ## sample estimates: ## mean in group BA mean in group MA ## 56257.10 56609.56 5.9.2 infer We aren’t going to cover the bootstrapping or randomization tests for the quantitative variables. But notice that the infer syntax is almost identical to that which we covered when we were working with proportions. Also, notice that the computational approach gives almost identical answers to the mathematical model (t-distribution) from above. 5.9.2.1 One sample bootstrapping on mean set.seed(47) # calculate the observed test statistic # note that we could use `stat = &quot;median&quot;` or `stat = &quot;t&quot;` ( x_bar_base &lt;- teachers %&gt;% specify(response = base) %&gt;% calculate(stat = &quot;mean&quot;) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 56416. # create the null sampling distribution null_dist &lt;- teachers %&gt;% specify(response = base) %&gt;% hypothesize(null = &quot;point&quot;, mu = 47000) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;mean&quot;) # visualize the null sampling distribution visualize(null_dist) + shade_p_value(obs_stat = x_bar_base, direction = &quot;greater&quot;) # calculate the p-value null_dist %&gt;% get_p_value(obs_stat = x_bar_base, direction = &quot;greater&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0 5.9.2.2 Two independent samples comparing two means p-value is now 0.464 (very close to 0.442 given by the smooth t-distribution curve). set.seed(47) # calculate the observed test statistic ( diff_x_bar_base &lt;- teachers %&gt;% specify(base ~ degree) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;MA&quot;, &quot;BA&quot;)) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 352. # create the null sampling distribution null_dist &lt;- teachers %&gt;% specify(base ~ degree) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type =&quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;MA&quot;, &quot;BA&quot;)) # visualize the null sampling distribution visualize(null_dist) + shade_p_value(obs_stat = diff_x_bar_base, direction = &quot;greater&quot;) # calculate the p-value null_dist %&gt;% get_p_value(obs_stat = diff_x_bar_base, direction = &quot;greater&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.454 5.9.3 NBA Salaries example from ISCAM Inv 4.2 There is R code in the ISCAM book. I’ve written the series of steps in a slightly different way with the same results. 5.9.3.1 EDA Before thinking about inference, let’s look at the population (for 2017-18). The information for today is the population because it includes the salaries of all NBA players in 2017. We can see that in this particular year, the average salary in the Western conference is slightly higher. NBAsalary &lt;- read_delim(&quot;http://www.rossmanchance.com/iscam3/data/NBASalaries2017.txt&quot;, delim = &quot;\\t&quot;, escape_double = FALSE, trim_ws = TRUE) ggplot(NBAsalary) + geom_boxplot(aes(x=conference, y = salary)) ggplot(NBAsalary) + geom_histogram(aes(fill = conference, x = salary)) ggplot(NBAsalary) + geom_histogram(aes(x = salary)) + facet_wrap(~ conference) NBAsalary %&gt;% group_by(conference) %&gt;% summarize(mu = mean(salary), sigma = sd(salary), N = n(), min(salary), max(salary), median(salary)) ## # A tibble: 2 x 7 ## conference mu sigma N `min(salary)` `max(salary)` `median(salary)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 eastern 6.61 7.01 227 0.0577 33.3 3.81 ## 2 western 7.42 7.76 221 0.0320 34.4 4 5.9.3.2 Sampling distribution for one mean Before considering how the sample means vary, let’s visualize samples from each conference. NBAsalary %&gt;% filter(conference == &quot;eastern&quot;) %&gt;% sample_n(size = 20, replace = FALSE) %&gt;% ggplot() + geom_histogram(aes(x = salary)) + xlab(&quot;eastern salary&quot;) NBAsalary %&gt;% filter(conference == &quot;western&quot;) %&gt;% sample_n(size = 20, replace = FALSE) %&gt;% ggplot() + geom_histogram(aes(x = salary)) + xlab(&quot;western salary&quot;) One way to think about how the difference in means varies is to first visualize the variability in the distribution for a single mean (i.e., from one conference). Let’s look at the variability in the Eastern conference as well as the variability in the Western conference. Note that from the population analysis above (full set of observations), we see that \\(\\sigma \\approx 7\\). So the histograms below should have a standard deviation of close to \\(\\sigma / \\sqrt{20} = 1.5\\). Do they? Are the two histograms centered at the same place? Should they be? NBAsalary %&gt;% filter(conference == &quot;eastern&quot;) %&gt;% rep_sample_n(size = 20, replace = FALSE, reps = 500) %&gt;% summarize(mean_sal = mean(salary)) %&gt;% ggplot() + geom_histogram(aes(x=mean_sal)) NBAsalary %&gt;% filter(conference == &quot;western&quot;) %&gt;% rep_sample_n(size = 20, replace = FALSE, reps = 500) %&gt;% summarize(mean_sal = mean(salary)) %&gt;% ggplot() + geom_histogram(aes(x=mean_sal)) 5.9.3.3 Sampling distribution for two means Note: the code selects 20 random salaries from the Eastern NBA conference and 20 random salaries from the Western NBA conference. Using those two different samples, a t-statistic is selected. The whole process is repeated 1000 times. set.seed(4747) t_salaries &lt;- data.frame(meandiff = double(), tstat = double()) for(i in 1:1000){ one_t&lt;- NBAsalary %&gt;% group_by(conference) %&gt;% sample_n(size = 20, replace = FALSE) %&gt;% summarize(mn = mean(salary), sd = sd(salary), n = n()) %&gt;% pivot_wider(names_from = conference, values_from = 2:4) %&gt;% summarize(meandiff = (mn_eastern - mn_western), tstat = (mn_eastern - mn_western) / sqrt(sd_eastern^2 / n_eastern + sd_western^2 / n_western)) t_salaries[i,] &lt;- one_t } t_salaries %&gt;% ggplot() + geom_histogram(aes(x=meandiff)) + geom_vline(xintercept = 0) t_salaries %&gt;% ggplot() + geom_histogram(aes(x=tstat)) + geom_vline(xintercept = 0) 5.10 Reflection Questions In the interest of pairing down topics for Spring 2020, the following topics will not be covered: all of the computational tests associated with two means (e.g., bootstrapping, randomization test, etc.). This includes the following sections in the books that we will not cover: ISCAM Inv 2.9, 4.4, 4.9, 4.11 &amp; ISRS 4.5. we will not discussed paired samples (e.g., “before and after”, “left and right car tires”, etc.). This includes the following sections in the books that we will not cover: ISCAM Inv 4.8, 4.9, 4.10, 4.11 &amp; ISRS 4.2. we will not cover ANVOA. This includes the following sections in the books that we will not cover: ISCAM Inv 5.4, 5.5 &amp; ISRS 4.4. The reflection questions below that we will not cover are marked with an asterisk *. 5.10.1 1 quantitative variable: Chapter 4, Section 1 What changed about the studies (data structure) from Chapters 2 &amp; 3? What is the statistic of interest now? What is the parameter of interest? What is the difference between the distribution of the data and the distribution of the statistic? There is a theoretical difference as well as a computational difference. What is the limiting sampling distribution of the statistic? (Note, the answer here is for big samples, that is the Central Limit Theorem works only where there is a limit… i.e., the sample size is big.) If interest is in a statistics other than the sample mean, what is a tool we can use for finding the alternative statistic’s sampling distribution? Explain the intuition behind bootstrapping. Explain how the SE for the statistic is calculated using bootstrapping. What is the difference between a normal distribution and a t distribution? When do we use a z and when do we use a t? When would you use a confidence interval and when would you use a hypothesis test? What different information does a boxplot give versus a histogram? 5.10.2 2 means (1 quantitative variable, 1 binary variable): Chapter 4, Section 3 What changed about the studies (data structure) from section 4.1? What is the statistic of interest now? What is the parameter of interest? What is the sampling distribution for the statistic of interest? How is the t-distribution become relevant? What are degrees of freedom in general? What are the actual degrees of freedom for the test in section 4.3? How is the null mechanism different across the three analysis methods in section 3.2: randomization test, two-sample t-test, random sampling test (n.b. this is also called the parametric bootstrap)? How do you create a CI? How do you interpret the CI? What if your data are NOT normal? What strategies can you try out? 5.10.2.1 * (not covering in Spring 2020) Paired sample, difference in means: Chapter 4, Section 2 What changed about the studies (data structure) in section 4.2 as compared with 4.1 or 4.3? What is the statistic of interest now? What is the parameter of interest? What is the sampling distribution for the statistic of interest? What benefit does pairing have on the analysis? What happens if a paired study is analyzed as if it were an independent two sample study? (What happens to the p-value? What happens to the CI?) What is the easiest way to think of / analyze paired data? 5.10.2.2 * (not covering in Spring 2020) ANOVA: Chapter 4, Section 4 Why are these tests called ANalysis Of VAriance (ANOVA)? Describe the variability in the numerator and the variability in the denominator. What does each measure? What are the null and alternative hypotheses for ANOVA? What features of the data affect the power of the test? What does power mean here? What are the technical conditions? Why do we need equal variances here? Inv 2.5, Chance &amp; Rossman, ISCAM↩ Conventional wisdom says that the reason 98.6 has hung around is because it translates to 35 C. Indeed, it it agreed that, to the nearest integer, the average healthy human body temperature is 37 C. But there is also some consensus that it is slightly lower than 37 C (if we are willing to use more significant digits). The idea is that we have hung on to 98.6 because the decimal feels like a precise measurement. In reality, it is just the conversion from 37 C to F.↩ Mackowiak, Wasserman, &amp; Levine, Journal of the American Medical Association, 1992↩ Inv 2.6, Chance &amp; Rossman, ISCAM↩ "],
["correlation-regression.html", "Chapter 6 Correlation &amp; Regression 6.1 4/16/20 Agenda 6.2 Correlation 6.3 4/21/20 Agenda 6.4 4/23/20 Agenda 6.5 Simple Linear Regression 6.6 4/28/20 Agenda 6.7 4/30/20 Agenda 6.8 Multiple Linear Regression 6.9 R code for regression 6.10 Reflection Questions", " Chapter 6 Correlation &amp; Regression 6.1 4/16/20 Agenda Definition of correlation (r) Interpretation of correlation (r) (Probably not: Inference on \\(\\rho\\)) The last topic of the semester will focus on modeling and inference using two quantitative variables. That is, both the explanatory and the response variables are measured on a numeric scale. To get started, consider a handful of variables taken on the top 80 PGA golfers in 2004. The example comes from Investigation 5.7 in Chance and Rossman (2018) Figure 2.1: Investigation 5.7, Drive for show, putt for dough, Chance and Rossman (2018) Rank the seven scatterplots from strongest negative to strongest positive. Some questions to ask yourself: What would the correlation be if there was a perfect positive relationship? What would the correlation be if there was a perfect negative relationship? What would the correlation be if there was no relationship? 6.2 Correlation Correlation measures the association between two numerical variables. [Note, that when describing how two categorical (or one numerical &amp; one categorical) variables vary together, they are said to be associated instead of correlated.] The correlation coefficient measures the strength and direction of the linear association between two numerical variables. 6.2.1 Estimating Correlation The value of the correlation is defined as: \\[\\begin{eqnarray*} r &amp;=&amp; \\frac{ \\sum_i (x_i - \\overline{x})(y_i - \\overline{y})}{\\sqrt{\\sum_i(x_i - \\overline{x})^2} \\sqrt{ \\sum_i(y_i - \\overline{y})^2}}\\\\ r &amp;=&amp; \\frac{1}{n-1} \\sum_{i=1}^n \\bigg(\\frac{x_i - \\overline{x}}{s_x} \\bigg) \\bigg(\\frac{y_i - \\overline{y}}{s_y} \\bigg) \\end{eqnarray*}\\] Figure 2.2: Scatterplots with average X and Y values superimposed. Investigation 5.7, Drive for show, putt for dough, Chance and Rossman (2018) For each red dot (on each plot), consider the distance the observation is from the \\(\\overline{X}\\) line and the \\(\\overline{Y}\\) line. Is the observation (red dot) above both? below both? above one and below the other? How does the particular red dot (observation) contribute to the correlation? In a positive way (to make \\(r\\) bigger)? In a negative way (to make \\(r\\) smaller)? Some ideas worth thinking about: quadratic plots can have zero correlation yet a perfect functional relationship \\(-1 \\leq r \\leq 1\\) correlation does not imply causation (ice cream &amp; boating accidents!) for inference with \\(\\rho\\) as well as \\(\\beta_1\\), the data should come from a bivariate normal distribution. That is, histograms of \\(X\\) and \\(Y\\) should both be normal, and the scatterplot should be a cloud. correlation will go down when only a narrow range of X values is represented (see denominator of r). measurement error biases the estimate of a correlation coefficient toward zero. 6.2.2 Coefficient of Determination – \\(R^2\\) The coefficient of determination (\\(R^2\\)) is the square of the correlation (given above). However, it also has an additional interpretation that will be useful for us. It can measure how much of the original variability in Y is given by the regression line. Both SSE and least-squares will be defined below when we fit a line to the scatter plot of observations. SSE is “sum of squared errors” (think about how \\(s^2\\) is defined). So, \\(SSE(\\overline{y})\\) is the amount the response variable varies on its own. \\(SSE(\\mbox{least-squares})\\) is the amount the response variable varies around the line. \\[\\begin{eqnarray*} R^2 &amp;=&amp; \\frac{SSE(\\overline{y}) - SSE(\\mbox{least-squares})}{SSE(\\overline{y})} \\\\ &amp;=&amp; \\frac{Var(y_i) - Var(e_i)}{Var(y_i)} \\\\ &amp;=&amp; 1 - \\frac{Var(e_i)}{Var(y_i)}\\\\ \\end{eqnarray*}\\] [The value \\(e_i\\) is discussed in detail below, but it is the distance from the observed response variable to the prediction on the line: $$e_i = y_i - _i.]$$ \\(R^2\\) can be used even in models with many explanatory variables. As such, the way to think about \\(R^2\\) is in terms of how much of the variability in the response variable was removed (when we learned the values of the explanatory variables). \\(R^2\\) is the proportion reduction in the variability of the response variable which is explained by the explanatory variable. 6.2.3 Inference for correlation Note: we won’t actually cover inference for correlation in class, but the notes on inference for correlation are included so that you can see that the process is very similar to all of the other statistics seen in the course to this point. Parameter: \\(\\rho\\) Statistic: \\(r\\) SE\\(_r: \\sqrt{\\frac{1-r^2}{n-2}}\\) BUT, \\(r\\) is only normally distributed when \\(\\rho\\) = 0! Otherwise, the distribution of \\(r\\) from sample to sample is skewed (think about the scenario when \\(\\rho = 0.9\\)). 6.2.3.1 Hypothesis Testing \\[\\begin{eqnarray*} H_0:&amp;&amp; \\rho = 0\\\\ H_A:&amp;&amp; \\rho \\ne 0\\\\ t^* &amp;=&amp; \\frac{r}{SE_r} = \\frac{r}{\\sqrt{(1-r^2)/(n-2)}}\\\\ t^* &amp;\\sim&amp; t_{n-2} \\mbox{ when } H_0 \\mbox{ is true} \\end{eqnarray*}\\] 6.2.3.2 Confidence Interval If \\(\\rho \\ne 0\\), then the SE might be okay, but the sampling distribution of \\(r\\) will not be normal (and thus will not be a \\(t\\) when we use the SE). Let: \\[\\begin{eqnarray*} z &amp;=&amp; 0.5 \\ln \\bigg( \\frac{1+r}{1-r} \\bigg)\\\\ \\xi &amp;=&amp; 0.5 \\ln \\bigg( \\frac{1+\\rho}{1-\\rho} \\bigg)\\\\ var(z) &amp;=&amp; \\sqrt{\\frac{1}{n-3}}\\\\ 95\\% \\mbox{ CI for } \\xi : &amp;&amp;\\\\ z &amp;\\pm&amp; 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\ \\mbox{we&#39;re 95% confident that } &amp;&amp; \\\\ &amp;&amp;z - 1.96 \\cdot \\sqrt{\\frac{1}{n-3}} \\leq \\xi \\leq z + 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\ &amp;&amp; a \\leq \\xi \\leq b\\\\ &amp;&amp; a \\leq 0.5 \\ln \\bigg(\\frac{1+\\rho}{1-\\rho} \\bigg) \\leq b\\\\ &amp;&amp; \\frac{e^{2a} - 1}{e^{2a} + 1} \\leq \\rho \\leq \\frac{e^{2b} - 1}{e^{2b} + 1} \\end{eqnarray*}\\] See the Cat Jumping22 example below in section 6.9.1. HT: \\[\\begin{eqnarray*} H_0:&amp;&amp; \\rho = 0\\\\ H_a:&amp;&amp; \\rho \\ne 0\\\\ t^* &amp;=&amp; \\frac{r}{\\sqrt{(1-r^2)/(n-2)}} = \\frac{-0.496}{\\sqrt{(1-0.496^2) / (18-2)}}= -2.2849\\\\ p-value &amp;=&amp; 2 \\cdot P(t_{18-2} \\leq -2.2849) = 2\\cdot(pt(-2.2849,16)) = 0.036 \\mbox{ (borderline significant)} \\end{eqnarray*}\\] CI: \\[\\begin{eqnarray*} 95\\% \\mbox{CI for } \\xi :&amp;&amp; \\\\ z \\pm 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}&amp;&amp; \\\\ \\mbox{we&#39;re } 95\\% \\mbox{ confident that}&amp;&amp;\\\\ 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg) - 1.96 \\cdot \\sqrt{\\frac{1}{n-3}} &amp;\\leq \\xi \\leq&amp; 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg) + 1.96 \\cdot \\sqrt{\\frac{1}{n-3}}\\\\ 0.5 \\ln\\bigg(\\frac{1 - 0.496}{1+0.496}\\bigg) - 1.96 \\cdot \\sqrt{\\frac{1}{18-3}} &amp;\\leq \\xi \\leq &amp;0.5 \\ln\\bigg(\\frac{1-0.496}{1+0.496}\\bigg) + 1.96 \\cdot \\sqrt{\\frac{1}{18-3}}\\\\ -1.05 &amp;\\leq \\xi \\leq &amp;-0.04\\\\ \\frac{e^{2\\cdot -1.05} - 1}{e^{2\\cdot -1.05} + 1} &amp;\\leq \\rho \\leq&amp; \\frac{e^{2\\cdot -0.04} - 1}{e^{2\\cdot -0.04} + 1}\\\\ &amp;&amp; (-0.781, -0.04) \\end{eqnarray*}\\] 6.3 4/21/20 Agenda Least Squares estimation of the line Distribution of the least squares line from sample to sample 6.4 4/23/20 Agenda Inferential technical conditions Residual Plots Transformations Prediction Intervals 6.5 Simple Linear Regression Regression is a method that predicts the value of one numerical variable from that of another. That is, as an extension to describing the degree of linearity of the relationship (correlation), the goal is now to create the best linear model – often for prediction. Note that many of the characteristics explored with correlation are applicable for regression. However, correlation treats \\(X\\) and \\(Y\\) as interchangeable, whereas regression treats \\(X\\) as fixed and known and \\(Y\\) as random and unknown. As we have previously, we call \\(X\\) the explanatory variable, and \\(Y\\) the response variable. Again, we do not assume that there is any causal mechanism between \\(X\\) and \\(Y\\) even if they have a strong linear (or otherwise) relationship. Population Model Notice the Greek letters representing parameters: \\[\\begin{eqnarray*} E[y|x] &amp;=&amp; \\beta_0 + \\beta_1 x \\\\ y_i &amp;=&amp; \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\ \\epsilon_i &amp;=&amp; y_i - (\\beta_0 + \\beta_1 x_i)\\\\ \\end{eqnarray*}\\] Predicted Values The predicted values of Y from a regression line estimate the mean value of \\(Y\\) for all individuals that have a given value of \\(X\\). Notice the Roman letters (English letters) representing statistics: \\[\\begin{eqnarray*} \\hat{y} &amp;=&amp; b_0 + b_1 x\\\\ \\hat{y}_i &amp;=&amp; b_0 + b_1 x_i\\\\ y_i &amp;=&amp; b_0 + b_1 x_i + e_i\\\\ e_i &amp;=&amp; y_i - \\hat{y}_i = y_i - (b_0 + b_1 x_i)\\\\ \\end{eqnarray*}\\] Notice, that we are predicting the mean value of the response variable at a given value of the explanatory variable! 6.5.1 Least Squares estimation of the regression line To find the values of the regression statistics, the sum of squared errors is minimized. SSE: Sum of squared errors (or residuals) is a measure of how closely the line fits to the points. SSE is the value of the squared deviations calculated at the “best” possible values of \\(\\beta_0\\) and \\(\\beta_1\\) for a given dataset. \\[\\begin{eqnarray*} SSE = \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (y_i - (b_0 + b_1 x_i) )^2 \\end{eqnarray*}\\] is minimized by the values: \\[\\begin{eqnarray*} b_0 = \\overline{y} - b_1 \\overline{x} &amp; \\ \\ \\ \\ \\ \\ \\ &amp; b_1 = r \\frac{s_y}{s_x} \\end{eqnarray*}\\] To find the “best” fitting line, we searched for the line that has the smallest residuals (SSE) in some sense. In particular, the goal is to try to find the line that minimizes the following quantity: \\[Q=\\sum e_i^2 = \\sum (y_i-(b_0+b_1x_i))^2.\\] Finding \\(b_0\\) and \\(b_1\\) that minimize Q is a calculus problem. \\[\\frac{dQ}{db_0}=-2\\sum (y_i-(b_0+b_1x_i)),\\qquad \\frac{dQ}{db_1}=-2\\sum x_i(y_i-(b_0+b_1x_i))\\] Setting both derivatives equal to 0 and solving for \\(b_0\\) and \\(b_1\\) yields the optimal values, denoted as \\(b_0\\) and \\(b_1\\) One aspect of the optimization problem that is worth pointing out has to do with the role of the two variables of interest \\(X\\) and \\(Y\\). If we switch the roles of \\(X\\) and \\(Y\\), the best fitting line will be different. If the relationship was invariant to which variable we choose as a response, then switching the roles of \\(X\\) and \\(Y\\) should give a slope of \\(1/b_1\\), which is not the case. [Note that the role of \\(X\\) and \\(Y\\) is invariant when calculating the correlation but not when calculating the least squares regression line.] Residuals Residuals measure the scatter of points above and below the least squares regression line. We use the residuals in many of the calculations and interpretations of the model. Indeed, the goal of the linear regression is to find a model that has small residuals. That is, ideally, the known variable \\(X\\) will tell us all there is to know about the unknown variable \\(Y\\). \\[\\begin{eqnarray*} e_i &amp;=&amp; (y_i - \\hat{y}_i)\\\\ MSE&amp;=&amp; \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{n-2} = \\frac{\\sum_i (e_i)^2}{n-2} = s^2\\\\ SSE &amp;=&amp; \\sum_i (y_i - \\hat{y}_i)^2 = \\sum_i (e_i)^2\\\\ R^2 &amp;=&amp; 1 - \\frac{Var(e_i)}{Var(y_i)} \\end{eqnarray*}\\] 6.5.2 Inference on the slope, \\(\\beta_1\\) SE of the slope \\[\\begin{eqnarray*} SE(b_1) &amp;=&amp; \\sqrt{\\frac{MSE}{\\sum_i (x_i - \\overline{x})^2}} = \\sqrt{\\frac{MSE}{(n-1)s_x^2}} = = \\sqrt{\\frac{\\frac{\\sum_i (y_i - \\hat{y}_i)^2}{n-2}}{(n-1)s_x^2}}\\\\ \\end{eqnarray*}\\] Just like any other statistic, the value of \\(b_1\\) can be calculated for every sample. The manner in which \\(b_1\\) varies from sample to sample becomes the sampling distribution of the sample slope. The SE of the slope will be the standard deviation associated with the sampling distribution of the slope. The resulting inference theory is very similar to that which we saw with the mean. The CLT describes \\(b_1\\) to have a normal distribution, and estimating the \\(SE(b_1)\\) induces extra variability which leads to a t-score test statistic with a t-distribution with df = \\(n-2\\). \\[\\begin{eqnarray*} \\mbox{t-score} = \\frac{b_1 - \\beta_1}{SE(b_1)} \\sim t_{n-2} \\end{eqnarray*}\\] CI for the slope \\[\\begin{eqnarray*} b_1- t^*_{\\alpha/2, n-2} SE(b_1) \\leq \\beta_1 \\leq b_1 + t^*_{\\alpha/2, n-2} SE(b_1) \\end{eqnarray*}\\] See the Housing Prices23 example below in section 6.9.2. \\[\\begin{eqnarray*} t_{\\alpha/2,n-2} &amp;=&amp; qt(.975, 18-2) = 2.1199\\\\ SE(b_1) &amp;=&amp; 26.4\\\\ b_1 &amp;=&amp; 202\\\\ b_1 &amp;\\pm&amp; t_{\\alpha/2, n-2} SE(b_1): 202 \\pm 2.1199 \\cdot 26.4\\\\ &amp;&amp; (146.03, 257.97)\\\\ \\end{eqnarray*}\\] That is, we are 95% confident that the true average change in price associated with an additional square foot of house is between $146.03 and $257.97. HT for the slope As mentioned previously, \\[\\begin{eqnarray*} \\mbox{t-score} = \\frac{b_1 - \\beta_1}{SE(b_1)} \\sim t_{n-2} \\end{eqnarray*}\\] Typically, interested is in testing whether or not the slope is zero. The null hypothesis of \\(H_0: \\beta_1 = 0\\) addresses whether there is a non-zero linear relationship between \\(X\\) and \\(Y\\): \\[\\begin{eqnarray*} H_0:&amp;&amp; \\mbox{ the slope of the regression line is zero}, \\beta_1=0\\\\ H_A:&amp;&amp; \\mbox{ the slope of the regression line is not zero}, \\beta_1 \\ne 0\\\\ \\end{eqnarray*}\\] As with previous tests, the alternative can be one- or two-sided (depending on the research question). Again, back to the housing data… (consider a two sided test) \\[\\begin{eqnarray*} t^* &amp;=&amp; 7.67\\\\ p-value &amp;=&amp; 2 \\cdot P(t_{16} \\geq 7.67) = 2 \\cdot (1-pt(7.67, 16)) \\approx 0 \\end{eqnarray*}\\] Regression Technical Conditions L: At each value of X, there is a population of possible Y-values whose mean lies on the “true” regression line (linearity) I: At each value of X, the Y-measurements represent a random sample from the population of possible Y-values (independence) [Consider this example of lack of independence. The researcher is trying to determine whether the number of pieces in a puzzle is linearly associated with the time to complete the puzzle. At first we choose 2 people and let them do 10 puzzles each. Then we let 20 independent people do the puzzles. The first experiment will create a slope which is particular to the two people sampled (it may or may not be close to the parameter). The second one will create a slope close to the 20 people sampled. Note that the effective variability of the first model is based on n=2, but we think it is based on n=20 (if we don’t notice the lack of independence). The second slope is based on n=20, and so it will have the correct associated variability.] N: At each value of X, the distribution of possible Y-values is normal (normality) E: The variance of Y-values is the same at all values of X (equal variance) Residual Plots Within a residual plot, you should be looking for the same types of things you want in a scatter plot. [See the residual plots provided in section 6.9.1.] a roughly symmetric cloud of points above and below the horizontal line at zero, with a higher density of points close to the line than far from the line, little noticeable curvature as we move from left to right along the X-axis, and approximately equal variance of points above and below the line at all values of X. 6.6 4/28/20 Agenda Multiple Linear Regression 6.7 4/30/20 Agenda Choosing model Residual Plots Prediction Intervals (harder to plot!) 6.8 Multiple Linear Regression As with simple linear regression, consider \\(n\\) observations. The response variable for the \\(i^{th}\\) individual is denoted by \\(Y_i\\), as before. The variation remaining in \\(Y_i\\) that isn’t explained by our predictors will also remain the same, denoted by \\(\\epsilon_i\\) and called the random error. Since we now have more than one explanatory variable, we will need to add an additional subscript on \\(X\\), denoting the value of the \\(k^{th}\\) predictor variable for the \\(i^{th}\\) individual by \\(X_{ik}\\). Thus our model is now \\[\\begin{eqnarray*} Y_i&amp;=&amp;\\beta_0+\\beta_1X_{i1}+\\beta_2X_{i2}+ \\cdots + \\beta_{p-1}X_{i,p-1} + \\epsilon_i\\\\ E[Y]&amp;=&amp;\\beta_0+\\beta_1X_{1}+\\beta_2X_{2}+ \\cdots + \\beta_{p-1}X_{p-1}\\\\ Y_i&amp;=&amp;b_0+b_1X_{i1}+b_2X_{i2}+ \\cdots + b_{p-1}X_{i,p-1} + e_i\\\\ \\widehat{Y}&amp;=&amp;b_0+b_1X_{1}+b_2X_{2}+ \\cdots + b_{p-1}X_{p-1}\\\\ \\end{eqnarray*}\\] Fitting the Model To estimate the coefficients, use the same principle as before, that of least squares. That is, minimize \\[\\sum_{i=1}^n(Y_i-(b_0+b_1X_{i1}+b_2X_{i2} + \\cdots + b_{p-1}X_{i,p-1}))^2\\] We are interested in finding the least squares estimates (\\(b_i\\)) of the parameters of the model \\(\\beta_i\\). 6.8.1 Model selection While there are many aspects of a linear model to consider (we offer an entire course! Math 158: Linear models), here we will focus on interpreting and testing single coefficients in the model. Consider the following model (based on R housing example below). We are predicting the price of a home (in \\(\\ln\\) units). \\[\\widehat{Y} = 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot \\mbox{# bedrooms}\\] Now, let’s compare two houses. The two houses have the exact same square feet (note, it doesn’t matter what the square feet is!!). House 1 (H1) has 4 bedrooms, house 2 (H2) has 3 bedrooms. The coefficient associated with the number of bedrooms indicates the change in price (in \\(\\ln\\) units) keeping all other variables constant. That is, if comparing the prediction of the average price of a home for two homes that have the same square feet but a one unit difference in bedrooms, the price of the home is predicted to be -0.0603 \\(\\ln\\) units less for the home with more bedrooms. (Does that seem to make sense? Same square feet, more bedrooms chop up the house and make it less desirable maybe?) \\[\\begin{eqnarray*} \\widehat{Y}_{H1} &amp;=&amp; 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot 4\\\\ \\widehat{Y}_{H2} &amp;=&amp; 12.2 + 0.000468 \\cdot \\mbox{sqft} - 0.0603 \\cdot 3\\\\ \\widehat{Y}_{H1} - \\widehat{Y}_{H2} &amp;=&amp; (-0.0603 \\cdot 4) - (-0.0603 \\cdot 3) = -0.0603\\\\ \\end{eqnarray*}\\] house = read.table(&quot;http://www.rossmanchance.com/iscam2/data/housing.txt&quot;, header=TRUE, sep=&quot;\\t&quot;) lm(log(price) ~ sqft + bedrooms, data=house) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.2 0.174 70.1 1.39e-73 ## 2 sqft 0.000468 0.0000660 7.09 4.73e-10 ## 3 bedrooms -0.0603 0.0572 -1.05 2.95e- 1 Inference about Regression Parameters However, the coefficient on bedrooms isn’t significant (that is, the associated p-value is larger than any reasonable level of significance, like \\(\\alpha = 0.05\\)). But wait, how is the p-value even calculated? The least squares coefficient estimate and the SE create a test statistic that will have a t distribution when the null hypothesis is true (note that we are now estimating \\(p\\) parameters, so our degrees of freedom is \\(n-p\\)). \\[\\begin{eqnarray*} \\frac{b_k - \\beta_k}{s\\{b_k\\}} \\sim t_{(n-p)} \\end{eqnarray*}\\] A \\((1-\\alpha)100\\%\\) CI for \\(\\beta_k\\) is given by\\[b_k \\pm t_{(1-\\alpha/2, n-p)} s\\{b_k\\}\\] The t-test is done separately for EACH \\(\\beta\\) coefficient. The test addresses the effect of removing only the variable at hand. Both testing and interpretation of the regression coefficients are done with all other variables in the model. The coefficient on bedrooms is not significant given sqft is in the model. Note that if we don’t have sqft, then bedrooms acts as a surrogate for sqft and it is important (and significant!). In this case, however, sqft is a better predictor than bedrooms. lm(log(price) ~ bedrooms, data=house) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.3 0.219 56.3 1.03e-66 ## 2 bedrooms 0.178 0.0587 3.03 3.31e- 3 Which variables to choose? Again, there are numerous ways to build a model. Some important principles to keep in mind: ideally the variables in the model are significant (remove variables with high p-values) the LINE model conditions should hold (you may need to transform the response or explanatory variables) the variables should be consistent with the data context (be sure that your analysis is done with experts in the field) 6.8.2 Checking model assumptions As before, the LINE model conditions are checked by using residual plots. Note that in the housing example, the residual plot after log transformation on the response variable is improved. lm(price ~ sqft, data=house) %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for price as a function of sqft&quot;) lm(log(price) ~ sqft, data=house) %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for ln price as a function of sqft&quot;) 6.9 R code for regression 6.9.1 Example: Cat Jumping24 (Correlation &amp; SLR) Consider the cat data given in Investigations 5.6 and 5.13. The idea is to understand cat jumping velocity as a function of body characteristics. Note that the correlation \\(r=-0.496\\) between bodymass and velocity. cats &lt;- read_table2(&quot;http://www.rossmanchance.com/iscam2/data/CatJumping.txt&quot;) ggplot(cats, aes(x=bodymass, y = velocity)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se=FALSE) Correlation cats %&gt;% select(bodymass, velocity) %&gt;% cor() ## bodymass velocity ## bodymass 1.0000000 -0.4964022 ## velocity -0.4964022 1.0000000 Simple Linear Regression summary(lm(velocity ~ bodymass, data = cats)) ## ## Call: ## lm(formula = velocity ~ bodymass, data = cats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.069 -16.729 -8.524 10.546 84.625 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 394.473238 23.441939 16.828 1.35e-11 *** ## bodymass -0.012196 0.005332 -2.287 0.0361 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.6 on 16 degrees of freedom ## Multiple R-squared: 0.2464, Adjusted R-squared: 0.1993 ## F-statistic: 5.232 on 1 and 16 DF, p-value: 0.03613 An alternative way to work with the output is in specific pieces (instead of the entire summary output). library(broom) lm(velocity ~ bodymass, data = cats) %&gt;% tidy(conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 394. 23.4 16.8 1.35e-11 345. 444. ## 2 bodymass -0.0122 0.00533 -2.29 3.61e- 2 -0.0235 -0.000893 Residual Plot And to work with the residuals, use augment(). lm(velocity ~ bodymass, data = cats) %&gt;% augment() ## # A tibble: 18 x 9 ## velocity bodymass .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 334. 3640 350. 7.58 -15.6 0.0656 30.3 1.04e-2 ## 2 387. 2670 362. 10.7 25.4 0.131 29.7 6.40e-2 ## 3 411. 5600 326. 10.2 84.6 0.119 19.8 6.29e-1 ## 4 319. 4130 344. 6.99 -25.5 0.0557 29.8 2.32e-2 ## 5 369. 3020 358. 9.38 11.1 0.101 30.4 8.67e-3 ## 6 359. 2660 362. 10.8 -3.23 0.132 30.6 1.05e-3 ## 7 345. 3240 355. 8.64 -10.4 0.0853 30.4 6.24e-3 ## 8 325. 5140 332. 8.60 -7.19 0.0844 30.5 2.97e-3 ## 9 301. 3690 349. 7.48 -48.1 0.0639 27.7 9.62e-2 ## 10 332. 3620 350. 7.62 -18.5 0.0664 30.2 1.49e-2 ## 11 313. 5310 330. 9.16 -17.1 0.0957 30.2 1.96e-2 ## 12 317. 5560 327. 10.1 -9.86 0.116 30.4 8.23e-3 ## 13 376. 3970 346. 7.08 29.5 0.0572 29.5 3.21e-2 ## 14 372. 3770 348. 7.34 23.9 0.0615 29.9 2.28e-2 ## 15 314. 5100 332. 8.48 -18.0 0.0820 30.2 1.79e-2 ## 16 368. 2950 358. 9.64 9.01 0.106 30.5 6.14e-3 ## 17 286. 7930 298. 21.1 -11.5 0.508 30.3 1.57e-1 ## 18 352. 3550 351. 7.78 1.32 0.0692 30.6 7.97e-5 ## # … with 1 more variable: .std.resid &lt;dbl&gt; lm(velocity ~ bodymass, data = cats) %&gt;% augment() %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept = 0) Confidence &amp; Prediction Intervals m_cats &lt;- lm(velocity ~ bodymass, data = cats) predict(m_cats, newdata=data.frame(bodymass=4700), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 337.1514 321.3081 352.9947 predict(m_cats, newdata=data.frame(bodymass=4700), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 337.1514 272.4378 401.865 Plotting! catConf &lt;- m_cats %&gt;% augment() %&gt;% cbind(predict(m_cats, interval=&quot;confidence&quot;) ) # cbind binds the columns together catConf %&gt;% head() ## velocity bodymass .fitted .se.fit .resid .hat .sigma ## 1 334.5 3640 350.0793 7.582579 -15.579293 0.06563257 30.28374 ## 2 387.3 2670 361.9095 10.722878 25.390452 0.13125270 29.74812 ## 3 410.8 5600 326.1749 10.228079 84.625139 0.11941906 19.80527 ## 4 318.6 4130 344.1032 6.985435 -25.503185 0.05570221 29.80778 ## 5 368.7 3020 357.6409 9.384496 11.059101 0.10053270 30.41969 ## 6 358.8 2660 362.0315 10.763427 -3.231509 0.13224725 30.55520 ## .cooksd .std.resid fit lwr upr ## 1 0.010414418 -0.5445423 350.0793 334.0049 366.1536 ## 2 0.063990821 0.9203785 361.9095 339.1781 384.6410 ## 3 0.629490813 3.0468951 326.1749 304.4923 347.8574 ## 4 0.023189898 -0.8867122 344.1032 329.2947 358.9116 ## 5 0.008674247 0.3939761 357.6409 337.7467 377.5351 ## 6 0.001046793 -0.1172061 362.0315 339.2141 384.8490 ggplot(catConf, aes(x=bodymass)) + geom_point(aes(y=velocity)) + geom_line(aes(y=.fitted)) + geom_ribbon(aes(ymin=lwr, ymax=upr), fill=&quot;blue&quot;, alpha=0.2) catPred &lt;- m_cats %&gt;% augment() %&gt;% cbind(predict(m_cats, interval=&quot;prediction&quot;) ) # cbind binds the columns together catPred %&gt;% head() ## velocity bodymass .fitted .se.fit .resid .hat .sigma ## 1 334.5 3640 350.0793 7.582579 -15.579293 0.06563257 30.28374 ## 2 387.3 2670 361.9095 10.722878 25.390452 0.13125270 29.74812 ## 3 410.8 5600 326.1749 10.228079 84.625139 0.11941906 19.80527 ## 4 318.6 4130 344.1032 6.985435 -25.503185 0.05570221 29.80778 ## 5 368.7 3020 357.6409 9.384496 11.059101 0.10053270 30.41969 ## 6 358.8 2660 362.0315 10.763427 -3.231509 0.13224725 30.55520 ## .cooksd .std.resid fit lwr upr ## 1 0.010414418 -0.5445423 350.0793 285.3088 414.8498 ## 2 0.063990821 0.9203785 361.9095 295.1746 428.6445 ## 3 0.629490813 3.0468951 326.1749 259.7898 392.5599 ## 4 0.023189898 -0.8867122 344.1032 279.6352 408.5712 ## 5 0.008674247 0.3939761 357.6409 291.8183 423.4635 ## 6 0.001046793 -0.1172061 362.0315 295.2672 428.7958 ggplot(catPred, aes(x=bodymass)) + geom_point(aes(y=velocity)) + geom_line(aes(y=.fitted)) + geom_ribbon(aes(ymin=lwr, ymax=upr), fill=&quot;blue&quot;, alpha=0.2) 6.9.2 Example: Housing Prices25 (SLR &amp; MLR &amp; Prediction) library(GGally) house = read.table(&quot;http://www.rossmanchance.com/iscam2/data/housing.txt&quot;, header=TRUE, sep=&quot;\\t&quot;) names(house) ## [1] &quot;sqft&quot; &quot;price&quot; &quot;City&quot; &quot;bedrooms&quot; &quot;baths&quot; Descriptive Statistics A good first step is to investigate how all the variables relate to one another. The ggpairs function come from the R package GGally. ggpairs(house, columns = c(1,2,4,5)) Simple Linear Regression mod.sqft &lt;- lm(price~sqft, data = house) mod.sqft %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 65930. 60994. 1.08 2.83e- 1 ## 2 sqft 202. 26.4 7.67 3.35e-11 mod.bed &lt;- lm(price ~ bedrooms, data=house) mod.bed %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 220612. 107208. 2.06 0.0428 ## 2 bedrooms 76865. 28802. 2.67 0.00919 The p-values for both explanatory variables (sqft and bedrooms) are significant. Sqft seems more significant, and indeed, the first model has a higher \\(R^2\\) - that is, a higher proportion of the variability in price is explained by sqft (42.07%) than by number of bedrooms (8.08%). mod.sqft %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.421 0.414 2.22e5 58.8 3.35e-11 2 -1139. 2283. ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; mod.bed %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0808 0.0695 2.80e5 7.12 0.00919 2 -1158. 2321. 2329. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; However, it is important for us to ask whether either of the relationships actually fit the technical conditions of the linear regression model. We can see from the pairs plots that the relationships look Linear, we’ll assume the variables were collected Independently, but the Normality and the Equality of the error structure we can check using residual plots. mod.sqft %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for price as a function of sqft&quot;) mod.bed %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for price as a function of bedrooms&quot;) For both of the plots, it seems like the residuals have higher variability for positive residuals. Additionally, it seems that the variability of the residuals increases for larger fitted observations. A natural log transformation should fix both of these problems. mod.lnsqft &lt;- lm(log(price)~sqft, data = house) mod.lnbed &lt;- lm(log(price) ~ bedrooms, data=house) mod.lnsqft %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for ln(price) as a function of sqft&quot;) mod.lnbed %&gt;% augment () %&gt;% ggplot(aes(x = .fitted, y = .resid))+ geom_point() + geom_hline(yintercept=0) + ggtitle(&quot;Residual plot for ln(price) as a function of bedrooms&quot;) Though no residual plot will ever look perfect, these residual plots seem to fit the technical conditions of the model better than the untransformed data. Multiple Linear Regression Because the price variable had a large skew (and the ln() transformation helped the residuals), the following models will all use ln(price) as the response variable. What happens when we try to predict price (actually ln(price), here) using BOTH sqft and bedrooms? Note: the natural log function in R is log(). lm(log(price) ~ sqft + bedrooms, data=house) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.2 0.174 70.1 1.39e-73 ## 2 sqft 0.000468 0.0000660 7.09 4.73e-10 ## 3 bedrooms -0.0603 0.0572 -1.05 2.95e- 1 lm(log(price) ~ sqft + bedrooms, data=house) %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.448 0.435 0.450 32.5 4.62e-11 3 -50.0 108. 118. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; lm(log(price) ~ sqft + baths, data=house) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.1 0.151 80.0 4.07e-78 ## 2 sqft 0.000450 0.0000705 6.39 1.02e- 8 ## 3 baths -0.0377 0.0746 -0.505 6.15e- 1 lm(log(price) ~ sqft + baths, data=house) %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.443 0.429 0.452 31.8 7.07e-11 3 -50.4 109. 118. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; lm(log(price) ~ sqft + bedrooms + baths, data=house) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.2 0.175 69.5 1.38e-72 ## 2 sqft 0.000463 0.0000719 6.45 8.32e- 9 ## 3 bedrooms -0.0683 0.0730 -0.935 3.53e- 1 ## 4 baths 0.0168 0.0947 0.177 8.60e- 1 lm(log(price) ~ sqft + bedrooms + baths, data=house) %&gt;% glance() ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.449 0.428 0.453 21.4 2.98e-10 4 -49.9 110. 122. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; Sqft &amp; bedrooms Although the \\(R^2\\) value went up (44.84% of variability in log price is explained by sqft and bedrooms), the p-value on bedrooms isn’t significant. The p-value here can be interpreted as a hypothesis test on the slope coefficient given the other variables in the model. 0.353 = P(a slope of -.06827 or more extreme if sqft is in the model and there is no relationship between bedrooms and price) Our output says that once we have sqft in the model, we don’t actually need to know anything about the number of bedrooms (even though bedrooms was a significant predictor on its own). Sqft &amp; bathrooms Seems like we really don’t need bathrooms! The information about sqft is sufficient for predicting the price, and information about bathrooms doesn’t help much at all. Final model The final model will be run on log(price) using only sqft. Note that the coefficients and the \\(R^2\\) values change slightly (from the original analysis) because the response variable is logged. summary(mod.lnsqft) ## ## Call: ## lm(formula = log(price) ~ sqft, data = house) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.08988 -0.29591 -0.05899 0.28717 1.20206 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.204e+01 1.236e-01 97.36 &lt; 2e-16 *** ## sqft 4.274e-04 5.349e-05 7.99 7.87e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4502 on 81 degrees of freedom ## Multiple R-squared: 0.4407, Adjusted R-squared: 0.4338 ## F-statistic: 63.83 on 1 and 81 DF, p-value: 7.874e-12 Prediction As with the prediction intervals we had when we had a single sample, we can now create intervals for either an average (a confidence interval) of an individual (a prediction interval). Confidence interval: predict(mod.lnsqft, newdata=data.frame(sqft=2000), interval=&quot;confidence&quot;) ## fit lwr upr ## 1 12.89125 12.79211 12.99038 I am 95% confident that the true average log price for a 2000 sqft home is between 12.79 ln$ and 12.99 ln$. (The predicted value is in natural-log-dollars … which is hard to interpret , but back-transforming can be a little tricky and beyond the scope of this semester). Prediction interval: predict(mod.lnsqft, newdata=data.frame(sqft=2000), interval=&quot;prediction&quot;) ## fit lwr upr ## 1 12.89125 11.98994 13.79255 I am 95% of homes with 2000 sqft are between 11.99 ln$ and 13.79 ln$. Now back-transforming is easy (because there are no averages), so 95% of homes with 2000 sqft are between $161,126 and $977,301 (which are just \\(e^{11.98994}\\) and \\(e^{13.79255}\\)). Plotting confidence bounds: A confidence interval around the line gives bounds on the parameter represented by the line. So we are 95% confident that the true population line lies within the bounds. Note that the interval is wider at the endpoints (because the variability is higher at the ends). houseConf &lt;- mod.lnsqft %&gt;% augment() %&gt;% cbind(predict(mod.lnsqft, interval=&quot;confidence&quot;) ) # cbind binds the columns together houseConf %&gt;% head() ## log.price. sqft .fitted .se.fit .resid .hat .sigma ## 1 12.73376 3392 13.48618 0.08417041 -0.75242702 0.03494894 0.4448768 ## 2 13.71004 4100 13.78878 0.11696192 -0.07874048 0.06748451 0.4529518 ## 3 13.01398 3200 13.40412 0.07609566 -0.39014391 0.02856505 0.4508768 ## 4 12.38839 1436 12.65019 0.06143750 -0.26180404 0.01862011 0.4520790 ## 5 12.84133 1944 12.86731 0.05029208 -0.02598585 0.01247712 0.4530341 ## 6 12.61120 1500 12.67755 0.05946735 -0.06634311 0.01744506 0.4529817 ## .cooksd .std.resid fit lwr upr ## 1 5.240194e-02 -1.70116646 13.48618 13.31871 13.65366 ## 2 1.186788e-03 -0.18110383 13.78878 13.55606 14.02150 ## 3 1.136429e-02 -0.87917543 13.40412 13.25272 13.55553 ## 4 3.268479e-03 -0.58696923 12.65019 12.52795 12.77244 ## 5 2.130978e-05 -0.05807924 12.86731 12.76725 12.96738 ## 6 1.961713e-04 -0.14865344 12.67755 12.55923 12.79587 ggplot(houseConf, aes(x=sqft)) + geom_point(aes(y=log.price.)) + geom_line(aes(y=.fitted)) + geom_ribbon(aes(ymin=lwr, ymax=upr), fill=&quot;blue&quot;, alpha=0.2) A prediction interval around the line bounds the individual points. That is, 95% of the observations are captured inside the interval. As with the confidence interval, the prediction interval is also wider at the ends, but it is harder to see in prediction intervals than confidence intervals housePred &lt;- mod.lnsqft %&gt;% augment() %&gt;% cbind(predict(mod.lnsqft, interval=&quot;prediction&quot;) ) # cbind binds the columns together housePred %&gt;% head() ## log.price. sqft .fitted .se.fit .resid .hat .sigma ## 1 12.73376 3392 13.48618 0.08417041 -0.75242702 0.03494894 0.4448768 ## 2 13.71004 4100 13.78878 0.11696192 -0.07874048 0.06748451 0.4529518 ## 3 13.01398 3200 13.40412 0.07609566 -0.39014391 0.02856505 0.4508768 ## 4 12.38839 1436 12.65019 0.06143750 -0.26180404 0.01862011 0.4520790 ## 5 12.84133 1944 12.86731 0.05029208 -0.02598585 0.01247712 0.4530341 ## 6 12.61120 1500 12.67755 0.05946735 -0.06634311 0.01744506 0.4529817 ## .cooksd .std.resid fit lwr upr ## 1 5.240194e-02 -1.70116646 13.48618 12.57483 14.39754 ## 2 1.186788e-03 -0.18110383 13.78878 12.86321 14.71435 ## 3 1.136429e-02 -0.87917543 13.40412 12.49558 14.31266 ## 4 3.268479e-03 -0.58696923 12.65019 11.74606 13.55433 ## 5 2.130978e-05 -0.05807924 12.86731 11.96591 13.76872 ## 6 1.961713e-04 -0.14865344 12.67755 11.77393 13.58116 ggplot(housePred, aes(x=sqft)) + geom_point(aes(y=log.price.)) + geom_line(aes(y=.fitted)) + geom_ribbon(aes(ymin=lwr, ymax=upr), fill=&quot;blue&quot;, alpha=0.2) Predicting with more than one explanatory variable: The predict function still works to give estimates of the average value and the predicted individual values, but the plot is now much harder to draw because with three explanatory variables, we would need a 4-d plot to visualize the model and the predictions. sqftbedbathlm = lm(log(price)~sqft + bedrooms + baths, data=house) predict(sqftbedbathlm, newdata=data.frame(sqft=2000, bedrooms=3, baths=2), interval=&quot;confidence&quot;, level=.95) ## fit lwr upr ## 1 12.91816 12.80085 13.03548 predict(sqftbedbathlm, newdata=data.frame(sqft=2000, bedrooms=3, baths=2), interval=&quot;prediction&quot;, level=.95) ## fit lwr upr ## 1 12.91816 12.00953 13.8268 Again, it is hard to back-transform the prediction for the average (we end up thinking about it as a median), but we can back-transform the interval of individual prices. 95% of homes with 2000sqft, 3 bedrooms, and 2 baths cost between $164,312 and $1,011,356. 6.9.3 Example: 1918-19 Flu and Excess Deaths26 As we are coming to know, measuring the impact of COVID-19 is difficult. A recent NYTimes article compares total deaths in 2020 as compared with 2015-2019 for 8 different regions. Figure 6.1: US Death Toll 2020, NY Times, April 28, 2020, https://www.nytimes.com/interactive/2020/04/28/us/coronavirus-death-toll-total.html While it is still too early to model COVID-19 well, we do have information from the 1918-19 Flu pandemic that was similar in many ways to the current COVID-19 pandemic. In 2007, Markel et al. published research in the Journal of the American Medical Association detailing the results from different social distancing practices across the US, “Nonpharmaceutical Interventions Implemented by US Cities During the 1918-1919 Influenza Pandemic” [JAMA, Aug 8, 2007, Vol 298, No 6]. Figure 6.2: Nonpharmaceutical Interventions Implemented by US Cities During the 1918-1919 Influenza Pandemic, https://jamanetwork.com/journals/jama/fullarticle/208354 Their conclusions are worth restating: Conclusions These findings demonstrate a strong association between early, sustained, and layered application of nonpharmaceutical interventions and mitigating the consequences of the 1918-1919 influenza pandemic in the United States. In planning for future severe influenza pandemics, nonpharmaceutical interventions should be considered for inclusion as companion measures to developing effective vaccines and medications for prophylaxis and treatment. While the entire paper is fascinating and does a great job describing different interventions and related outcomes, we will focus on the regression analysis done to model the excess death rate. The data used below come directly from page 647 of the manuscript. Figure 6.3: Note that figure 1b seems to have the y-axis mis-labelled (it should be magnitude of first mortality peak). Table 4 uses a test other than linear regression (Wilcoxon rank sum test – a two sample test of means done on ranked data rather than raw data) to compare the outcome variables broken into two groups: below the median response time and above the median response time. flu_1819 &lt;- read_csv(&quot;1918_1919flu.csv&quot;, col_types = cols(`Date of peak Excess death rate` = col_date(format = &quot;%m/%d/%y&quot;))) names(flu_1819) &lt;- c(&quot;place&quot;, &quot;responseTime&quot;, &quot;daysNonpharm&quot;, &quot;datePeak&quot;, &quot;timePeak&quot;, &quot;magPeak&quot;, &quot;excessDeaths&quot;) head(flu_1819) ## # A tibble: 6 x 7 ## place responseTime daysNonpharm datePeak timePeak magPeak excessDeaths ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alban… 3 47 2018-10-24 15 162. 553. ## 2 Balti… 10 43 2018-10-18 9 182. 559. ## 3 Birmi… 9 48 2018-10-22 13 70.9 592. ## 4 Bosto… 13 50 2018-10-03 8 160. 710 ## 5 Buffa… 12 49 2018-10-22 12 141. 530. ## 6 Cambr… 14 49 2018-10-03 8 126. 541 Correlation between variables To be consistent with the manuscript, Spearman correlation is used instead of Pearson. Spearman is the Pearson correlation applied to the ranks of the observations (instead of the raw values of the observations). Calculating Spearman has the impact of lessening the influence of outlying observations. The correlations calculated below match the values in the manuscript, but we repeat the analysis without St Paul, MN and Grand Rapids, MI to see their impact on the analysis. I would not remove the two cities without a justifiable reason (something that makes them fundamentally different from the other cities, worth not including in the model); however, it is worth re-calculations just to investigate the impact of individual observations. Here it seems that there is some impact (e.g., excessDeaths and responseTime) but possibly the impact is only moderate. flu_1819 %&gt;% select(excessDeaths, magPeak, responseTime, daysNonpharm, timePeak) %&gt;% cor(method=&quot;spearman&quot;) ## excessDeaths magPeak responseTime daysNonpharm timePeak ## excessDeaths 1.0000000 0.7639686 0.3658238 -0.3925996 -0.1619664 ## magPeak 0.7639686 1.0000000 0.3090137 -0.5739853 -0.2541940 ## responseTime 0.3658238 0.3090137 1.0000000 -0.6808529 -0.7346764 ## daysNonpharm -0.3925996 -0.5739853 -0.6808529 1.0000000 0.6135956 ## timePeak -0.1619664 -0.2541940 -0.7346764 0.6135956 1.0000000 flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% select(excessDeaths, magPeak, responseTime, daysNonpharm, timePeak) %&gt;% cor(method=&quot;spearman&quot;) ## excessDeaths magPeak responseTime daysNonpharm timePeak ## excessDeaths 1.0000000 0.7402439 0.5093635 -0.4663501 -0.2989616 ## magPeak 0.7402439 1.0000000 0.4603040 -0.6752625 -0.4170257 ## responseTime 0.5093635 0.4603040 1.0000000 -0.6638306 -0.6969834 ## daysNonpharm -0.4663501 -0.6752625 -0.6638306 1.0000000 0.6007545 ## timePeak -0.2989616 -0.4170257 -0.6969834 0.6007545 1.0000000 Model building Below, both St Paul and Grand Rapids have been removed from the model. Again, in reporting the analysis (as the JAMA authors did correctly), the two cities would not be removed without a justifiable reason (something that makes them fundamentally different from the other cities, worth not including in the model); however, it is worth re-calculations just to investigate the impact of individual observations. Note that in predicting both excessDeaths and magPeak the most significant variable is daysNonpharm. No other variable adds significantly to the model # excessDeaths as the response variable: flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(excessDeaths ~ responseTime + daysNonpharm, data = .) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 552. 67.0 8.24 5.50e-10 ## 2 responseTime 5.57 3.90 1.43 1.62e- 1 ## 3 daysNonpharm -0.828 0.503 -1.65 1.08e- 1 flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(excessDeaths ~ daysNonpharm, data = .) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 628. 41.7 15.0 7.92e-18 ## 2 daysNonpharm -1.25 0.412 -3.03 4.28e- 3 flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(excessDeaths ~ responseTime, data = .) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 452. 28.8 15.7 1.90e-18 ## 2 responseTime 9.35 3.23 2.90 6.14e- 3 # magPeak as the response variable: flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(magPeak ~ responseTime + daysNonpharm, data = .) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 167. 21.0 7.95 0.00000000132 ## 2 responseTime -0.0555 1.22 -0.0455 0.964 ## 3 daysNonpharm -0.715 0.157 -4.55 0.0000541 flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(log(magPeak) ~ responseTime + daysNonpharm, data = .) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 5.20 0.211 24.6 5.69e-25 ## 2 responseTime -0.000741 0.0123 -0.0603 9.52e- 1 ## 3 daysNonpharm -0.00769 0.00158 -4.86 2.05e- 5 Checking Residuals Turns out the residuals for these models aren’t great (maybe that is why the authors used ranked based methods like Spearman correlation and Wilcoxon rank sum test??? – those tests have different technical conditions, not based in normal theory!) # excessDeaths as the response variable: flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(excessDeaths ~ daysNonpharm, data = .) %&gt;% augment() %&gt;% ggplot() + geom_point(aes(x=.fitted, y = .resid)) + geom_hline(yintercept = 0) + ggtitle(&quot;St Paul &amp; Grand Rapids removed, excessDeath vs days Nonparm&quot;) flu_1819 %&gt;% lm(excessDeaths ~ daysNonpharm, data = .) %&gt;% augment() %&gt;% ggplot() + geom_point(aes(x=.fitted, y = .resid)) + geom_hline(yintercept = 0) + ggtitle(&quot;St Paul &amp; Grand Rapids NOT removed, excessDeath vs days Nonparm&quot;) # magPeak as the response variable: flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(magPeak ~ responseTime + daysNonpharm, data = .) %&gt;% augment() %&gt;% ggplot() + geom_point(aes(x=.fitted, y = .resid)) + geom_hline(yintercept = 0) + ggtitle(&quot;St Paul &amp; Grand Rapids removed, magPeak vs days responseTime&quot;) flu_1819 %&gt;% lm(magPeak ~ responseTime + daysNonpharm, data = .) %&gt;% augment() %&gt;% ggplot() + geom_point(aes(x=.fitted, y = .resid)) + geom_hline(yintercept = 0) + ggtitle(&quot;St Paul &amp; Grand Rapids NOT removed, magPeak vs days responseTime&quot;) flu_1819 %&gt;% filter(place != &quot;St Paul, MN&quot; &amp; place != &quot;Grand Rapids, MI&quot;) %&gt;% lm(log(magPeak) ~ responseTime + daysNonpharm, data = .) %&gt;% augment() %&gt;% ggplot() + geom_point(aes(x=.fitted, y = .resid)) + geom_hline(yintercept = 0) + ggtitle(&quot;St Paul &amp; Grand Rapids removed, log(magPeak) vs days responseTime&quot;) What else do we know about COVID-19 right now? Clinical trials are starting to show up assessing know anti-viral pharmaceutical interventions on patients with COVID-19. Study 1: Figure 6.4: Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial, https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31022-9/fulltext Findings Between Feb 6, 2020, and March 12, 2020, 237 patients were enrolled and randomly assigned to a treatment group (158 to remdesivir and 79 to placebo); one patient in the placebo group who withdrew after randomisation was not included in the ITT population. Remdesivir use was not associated with a difference in time to clinical improvement (hazard ratio 1·23 [95% CI 0·87–1·75]). Although not statistically significant, patients receiving remdesivir had a numerically faster time to clinical improvement than those receiving placebo among patients with symptom duration of 10 days or less (hazard ratio 1·52 [0·95–2·43]). Adverse events were reported in 102 (66%) of 155 remdesivir recipients versus 50 (64%) of 78 placebo recipients. Remdesivir was stopped early because of adverse events in 18 (12%) patients versus four (5%) patients who stopped placebo early. animal studies have shown remdesivir inhibits SARS-CoV-2 replication study was randomized, double-blind, placebo-controlled, multi-center adults with laboratory-confirmed SARS-CoV-2 infection, low oxygen, pneumonia randomly assigned in a 2:1 ratio to remdesivir vs placebo not statistically significant ! patients on remdesivir had a numerically faster time to clinical improvement trial was stopped before enrolling the expected number of patients because the outbreak of COVID-19 was brought under control Study 2: Gilead, a large pharmaceutical company, issued a press release describing results from a clinical trial involving remdesivir. They were mostly seeking to understand how long the treatment should continue and what are the adverse effects of the drug. Figure 6.5: Gilead press release (no peer reviewed publication yet), https://www.gilead.com/news-and-press/press-room/press-releases/2020/4/gilead-announces-results-from-phase-3-trial-of-investigational-antiviral-remdesivir-in-patients-with-severe-covid-19https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(20)31022-9/fulltext In this study, the time to clinical improvement for 50 percent of patients was 10 days in the 5-day treatment group and 11 days in the 10-day treatment group. More than half of patients in both treatment groups were discharged from the hospital by Day 14 (5-day: 60.0%, n=120/200 vs.10-day: 52.3% n=103/197; p=0.14). At Day 14, 64.5 percent (n=129/200) of patients in the 5-day treatment group and 53.8 percent (n=106/197) of patients in the 10-day treatment group achieved clinical recovery. two patient groups are 5-day and 10-day treatment courses (no placebo), randomized patients on ventilators were not enrolled The NY Times covered the Gilead announement. Indeed, the outcome of the trials have important political ramifications as well. Figure 6.6: Gilead remdesivir trial covered in the NYT, https://www.nytimes.com/2020/04/29/health/gilead-remdesivir-coronavirus.html 6.10 Reflection Questions 6.10.1 Correlation &amp; Simple Linear Regression: Chapter 5, Section 1-4 Describe the linear model with multiple variables. Describe the error / residual term and how it is calculated with multiple variables. What are the (three-ish) statistics of interest in this chapter? What are the parameters of interest? What does correlation measure? How do we find the values of \\(b_0\\) and \\(b_1\\) for estimating the least squares line? Why is it dangerous to extrapolate? How do we interpret \\(R^2\\)? Why is that? What does it mean to say that \\(b_1\\) has a sampling distribution? Why is it that we would never talk about the sampling distribution of \\(\\beta_1\\)? Why do we need the LINE technical conditions for the inference parts of the analysis but not for the estimation parts of the analysis? Is linear regression always appropriate when comparing two continuous variables? What are the LINE technical conditions? How are the conditions assessed? What are the three factors that influence the \\(SE(b_1)\\)? (Note: when something influences \\(SE(b_1)\\), that means the inference is also effected. If you have a huge \\(SE(b_1)\\), it will be hard to tell if the slope is significant because the t value will be small.) What does it mean to do a randomization test for the slope? That is, explain the process of doing a randomization test here. (See shuffle options in the Analyzing Two Quantitative Variables applet.) Why would someone transform either of the variables? What is the difference between a confidence interval and a prediction interval? Which is bigger? Why does that make sense? How do the centers of the intervals differ? (They don’t. Why not?) 6.10.2 Multiple Linear Regression: Chapter 6, Section 1-3 Describe the linear model with multiple variables. Describe the error / residual term and how it is calculated with multiple variables. How does the model change when multiple variables are included? How are p-values interpreted now that there are multiple variables? How is \\(R^2\\) interpreted? What is the difference between \\(R^2\\) and \\(R^2_{adj}\\)? How are variables chosen for the final model? How are the model conditions assessed? References "],
["references.html", "References", " References "]
]
