[
["index.html", "Introduction to (Bio)Statistics Class Information", " Introduction to (Bio)Statistics Jo Hardin 2020-01-29 Class Information Class notes for Math 58(B) at Pomona College: Introduction to (Bio)Statistics. The notes are based extensively on “Introductory Statistics with Randomization and Simulation” by Diez, Barr, and etinkaya-Rundel (Diez, Barr, and Çetinkaya-Rundel 2014). We will also be using many examples (and applets) from “Investigating Statistics, Concepts, Applications, and Methods” by Chance and Rossman (Chance and Rossman 2018). You are responsible for reading the relevant chapters in the text. The text is very good &amp; readable, so you should use it. Also, you will have a much deeper understanding of the material if you spend time working through the applets at http://www.rossmanchance.com/iscam3/files.html. You should make sure you are coming to class and also reading the materials associated with the activities. Day Topic Book Chap Notes Section 1/21/20 Intro to Data / R 1 [intro], 1.1 [Jan21] 1/23/20 Foundations for Inference ISRS 2.1-2.3 2.1 [Jan23], 2.2 [Examp: gender] 1/28/20 Normality ISRS 2.4-2.5 2.4 [Jan28], 2.5.1 [CLT] 1/30/20 Normality ISRS 2.6-2.7 2.6 [Jan30], 2.6.1 [Normal Dist] References "],
["intro.html", "Chapter 1 Introduction 1.1 1/21/20 Agenda 1.2 Course Logistics 1.3 Example: Friend or Foe", " Chapter 1 Introduction 1.1 1/21/20 Agenda Syllabus &amp; Course Outline Example: Friend or Foe 1.2 Course Logistics What is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics. Figure 1.1: Probability vs. Statistics descriptive statistics describe the sample at hand with no intent on making generalizations. inferential statistics use a sample to make claims about a population Vocabulary A statistic is a numerical measurement we get from the sample, a function of the data. [Also sometimes called an estimate.] A parameter is a numerical measurement of the population. We never know the true value of the parameter. What is the content of Math 58(B)? This class will be an introduction to statistical ideas using R. We will cover the majority of statistical methods which are used in standard analyses (e.g., t-tests, chi-squared analysis, confidence intervals, binomial tests, etc.). The main inferential techniques will be covered using both theoretical approximations (e.g., the central limit theorem) as well as computational methods (e.g., permutation tests and bootstrapping). Focus will be on understanding he methods and interpreting results. Our goal in this course is to learn how to better evaluate quantitative information with regards to data. We’ll be sure to keep in mind: What is the difference between Math 58 and Math 58B? The two classes are remarkably similar in content and structure. Indeed, there are more similarities to the classes than there are differences. The main differences have to do with a handful of topics which are different across the two classes. topic for Math 58 topic for Math 58B early in the semester Binomial probabilities relative risk &amp; odds ratios later in the semester introduction to multiple regression introduction to logistic regression Who should take Math 58(B)? Every educated citizen should have a basic understanding of statistics. Ok, ok, I have my own biases, but I’m not the only person who thinks this! (https://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education) In terms of your academic interests, you should take introductory statistics if you would like to take upper division statistics or if you are planning to analyze data in a field outside of statistics (e.g., biology, EA, psychology, etc.). Upper division statistics courses require introductory statistics, and it is not easy to just “learn” statistics on your own over the summer. I highly recommend taking an introductory statistics course. If you have already taken AP Statistics, you may or may not want to repeat the material. If you had a strong course with an excellent teacher and scored well on the exam, you probably do not need to repeat the material. If you are uncertain about many of the concepts, then you may want to re-take the course before jumping into upper division statistics courses. We will use R extensively, and you probably didn’t use R in your AP Statistics classes. Most upper division statistics classes will expect you to be able to jump into R head first, and introductory statistics gives a more gentle introduction to R. What are the prerequisites for Math 58(B)? The formal prerequisite is a semester of calculus, but we do almost no calculus in the entire semester. However, a student in Math 58(B) should be quantitatively inclined and ready to see many new mathematical, algorithmic, and computational ideas quickly throughout the semester. Is there overlap with other classes? There is considerable overlap between Math 58 and Math 58B; you should not take both Math 58 and Math 58B. The differences between the two sections lie in the examples as well as a handful of topics that are different across the two courses. There is also quite a bit of overlap with other introductory statistics courses (e.g., Econ 57, Pysch 158, Politics 90, AP Statistics). Some introductory statistics courses cover quite a bit of probability without getting deeply into inferential ideas. We will focus on statistics instead of probability with an emphasis on understanding the intuition and mathematical derivations that inform the analysis tool. We will also focus on how the computer can help us gain a deeper understanding of the analyses we are doing. When should I take Math 58 or Math 58B? Introductory (Bio)Statistics should be taken as early in your undergraduate schedule as possible. By taking Math 58(B) you will open up the possibilities for taking upper division statistics classes. Additionally, the background covered in Math 58(B) will provide you with a deeper understanding of the concepts you are covering in your science and social science courses. What is the workload for Math 58(B)? Math 58(B) meets twice a week for 75 min for lecture and once a week for an hour for lab. Every week there will be one homework and one lab assignment. There are two midterm exams, each with an in-class and take-home section. The final exam will also have in-class and take-home parts. The class is not known to be extremely difficult or time consuming; however, it does require that you stay up with the material, do all of the assignments, and come to all class meetings (participation is a part of your grade). What software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R using RStudio as a front end. You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing. You may use R on the Pomona server: https://rstudio.campus.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.) If you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/. http://swirlstats.com/ is one way to walk through learning the basics of R. All assignments should be turned in using R Markdown compiled to pdf. Figure 1.2: Taken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim Figure 1.3: Jessica Ward, PhD student at Newcastle University 1.3 Example: Friend or Foe This example comes from Investigation 1.1: Friend or Foe? Chance and Rossman (2018). The idea is to use simulation to determine how likely our data would be if nothing interesting was going on. In a study reported in the November 2007 issue of Nature, researchers investigated whether infants take into account an individual’s actions towards others in evaluating that individual as appealing or aversive, perhaps laying for the foundation for social interaction (Hamlin, Wynn, and Bloom, 2007). In other words, do children who aren’t even yet talking still form impressions as to someone’s friendliness based on their actions? In one component of the study, 10-month-old infants were shown a “climber” character (a piece of wood with “googly” eyes glued onto it) that could not make it up a hill in two tries. Then the infants were shown two scenarios for the climber’s next try, one where the climber was pushed to the top of the hill by another character (the “helper” toy) and one where the climber was pushed back down the hill by another character (the “hinderer” toy). The infant was alternately shown these two scenarios several times. Then the child was presented with both pieces of wood (the helper and the hinderer characters) and asked to pick one to play with. Videos demonstrating this component of the study can be found at http://campuspress.yale.edu/infantlab/media/. One important design consideration to keep in mind is that in order to equalize potential influencing factors such as shape, color, and position, the researchers varied the colors and shapes of the wooden characters and even on which side the toys were presented to the infants. The researchers found that 14 of the 16 infants chose the helper over the hinderer. Always Ask What are the observational units? infants What is the variable? What type of variable? choice of helper or hindered: categorical What is the statistic? \\(\\hat{p}\\) = proportion of infants who chose helper = 14/16 = 0.875 What is the parameter? p = proportion of all infants who might choose helper (not measurable!) p-value is the probability of our data or more extreme if nothing interesting is going on. completely arbitrary cutoff \\(\\rightarrow\\) generally accepted conclusion p-value \\(&gt;\\) 0.10 \\(\\rightarrow\\) no evidence against the null model 0.05 \\(&lt;\\) p-value \\(&lt;\\) 0.10 \\(\\rightarrow\\) moderate evidence against the null model 0.01 \\(&lt;\\) p-value \\(&lt;\\) 0.05 \\(\\rightarrow\\) strong evidence against the null model p-value \\(&lt;\\) 0.01 \\(\\rightarrow\\) very strong evidence against the null model Computation library(infer) # to control the randomness set.seed(47) # first create a data frame with the Infant data Infants &lt;- read.delim(&quot;http://www.rossmanchance.com/iscam3/data/InfantData.txt&quot;) Infants %&gt;% head() ## choice ## 1 helper ## 2 hinderer ## 3 helper ## 4 helper ## 5 helper ## 6 helper # then find the proportion who help (p_obs &lt;- Infants %&gt;% specify(response = choice, success = &quot;helper&quot;) %&gt;% calculate(stat = &quot;prop&quot;) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.875 # now apply the infer framework to get the null proportion null_help &lt;- Infants %&gt;% specify(response = choice, success = &quot;helper&quot;) %&gt;% hypothesize(null = &quot;point&quot;, p = .5) %&gt;% generate(reps = 1000, type = &quot;simulate&quot;) %&gt;% calculate(stat = &quot;prop&quot;) # then visualize the null sampling distribution &amp; p-value visualize(null_help, bins = 13) + shade_p_value(obs_stat = p_obs, direction = &quot;two_sided&quot;) # calculate the actual p-value null_help %&gt;% get_p_value(obs_stat = p_obs, direction = &quot;two_sided&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.002 Logic for what we believe If we look back to the study, we can tell that the researchers varied color, shape, side, etc. to make sure there was nothing systematic about how the infants chose the block (e.g., if they all watch Blue’s Clues they might love the color blue, so we wouldn’t always want the helper shape to be blue). The excellent design survey rules out outside influence as the reason so many of the infants chose the helper shape. We ruled out random chance as the mechanism for the larger number of infants who chose the helper shape. (We reject the null hypothesis.) We conclude that babies are inclined to be helpful. That is, they are more likely to choose the helper than the hindered. [Note: we don’t have any evidence for why they choose the helper. That is, they might be predisposed. They might be modeling their parents. They might notice that they need a lot of help, etc.] References "],
["foundations-for-inference.html", "Chapter 2 Foundations for Inference 2.1 1/23/20 Agenda 2.2 Example: Gender Discrimination 2.3 Structure of Hypothesis testing 2.4 1/28/20 Agenda 2.5 Normal Model 2.6 1/30/20 Agenda 2.7 Confidence Intervals", " Chapter 2 Foundations for Inference 2.1 1/23/20 Agenda Example: gender discrimination infer again Hypothesis testing structure 2.2 Example: Gender Discrimination We consider a study investigating gender discrimination in the 1970s, which is set in the context of personnel decisions within a bank.1 The research question we hope to answer is, “Are females discriminated against in promotion decisions made by male managers?” The participants in this study were 48 male bank supervisors attending a management institute at the University of North Carolina in 1972. They were asked to assume the role of the personnel director of a bank and were given a personnel file to judge whether the person should be promoted to a branch manager position. The files given to the participants were identical, except that half of them indicated the candidate was male and the other half indicated the candidate was female. These files were randomly assigned to the subjects. For each supervisor we recorded the gender associated with the assigned file and the promotion decision. Using the results of the study summarized in Table 2.1, we would like to evaluate if females are unfairly discriminated against in promotion decisions. In this study, a smaller proportion of females are promoted than males (0.583 versus 0.875), but it is unclear whether the difference provides convincing evidence that females are unfairly discriminated against. (Diez, Barr, and Çetinkaya-Rundel (2014), pg 61) decision promoted not promoted total male 21 3 24 female 14 10 24 total 35 13 48 Always Ask What are the observational units? supervisor What are the variables? What type of variables? whether the resume was male or female (categorical) decision to promote or not promote (categorical) What is the statistic? \\(\\hat{p}_m - \\hat{p}_f\\) = 21/24 - 14/24 = 0.292 (the difference between the proportion of men who were promoted and the proportion of women who were promoted) What is the parameter? \\(p_m - p_f\\) = the true difference in the probability of a man being promoted minus the probability of a woman being promoted. Hypotheses H0: Null hypothesis. The variables gender and decision are independent. They have no relationship, and therefore any observed difference between the proportion of males and females who were promoted is due to chance. HA: Alternative hypothesis. The variables gender and decision are not independent. Any observed difference between the proportion of males and females who were promoted is not due to chance. Computation library(infer) # to control the randomness set.seed(47) # first create a data frame with the discrimination data discrim &lt;- data.frame(gender = c(rep(&quot;male&quot;, 24), rep(&quot;female&quot;, 24)), decision = c(rep(&quot;promote&quot;, 21), rep(&quot;not&quot;, 3), rep(&quot;promote&quot;, 14), rep(&quot;not&quot;, 10))) discrim %&gt;% head() ## gender decision ## 1 male promote ## 2 male promote ## 3 male promote ## 4 male promote ## 5 male promote ## 6 male promote # then find the difference in proportion who are promoted (diff_obs &lt;- discrim %&gt;% specify(decision ~ gender, success = &quot;promote&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) ) ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.292 # now apply the infer framework to get the null differences in proportions null_discrim &lt;- discrim %&gt;% specify(decision ~ gender, success = &quot;promote&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) # then visualize the null sampling distribution &amp; p-value visualize(null_discrim, bins = 10) + shade_p_value(obs_stat = diff_obs, direction = &quot;greater&quot;) # calculate the actual p-value null_discrim %&gt;% get_p_value(obs_stat = diff_obs, direction = &quot;greater&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0.026 Logic for what we believe We know that the study was an experiment, so there should be no systematic differences between the group who received “male” applications and “female” applications. We’ve ruled out random chance as the reason for the huge difference in proportions. (We reject the null hypothesis.) if we lived in the null reality, we’d only see data like these about 2.5% of the time. We conclude that gender and decision are not independent. That is, knowing the gender changes the probability of promotion. 2.3 Structure of Hypothesis testing 2.3.1 Hypotheses Hypothesis Testing compares data to the expectation of a specific null hypothesis. If the data are unusual, assuming that the null hypothesis is true, then the null hypothesis is rejected. The Null Hypothesis, \\(H_0\\), is a specific statement about a population made for the purposes of argument. A good null hypothesis is a statement that would be interesting to reject. The Alternative Hypothesis, \\(H_A\\), is a specific statement about a population that is in the researcher’s interest to demonstrate. Typically, the alternative hypothesis contains all the values of the population that are not included in the null hypothesis. In a two-sided (or two-tailed) test, the alternative hypothesis includes values on both sides of the value specified by the null hypothesis. In a one-sided (or one-tailed) test, the alternative hypothesis includes parameter values on only one side of the value specified by the null hypothesis. \\(H_0\\) is rejected only if the data depart from it in the direction stated by \\(H_A\\). 2.3.2 Other pieces of the process A statistic is a numerical measurement we get from the sample, a function of the data. [Also sometimes called an estimate.] A parameter is a numerical measurement of the population. We never know the true value of the parameter. The test statistic is a quantity calculated from the data that is used to evaluate how compatible the data are with the result expected under the null hypothesis. The null distribution is the sampling distribution of outcomes for a test statistic under the assumption that the null hypothesis is true. The p-value is the probability of obtaining the data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. The p-value is a number calculated from the dataset. Examples of Hypotheses Identify whether each of the following statements is more appropriate as the null hypothesis or as the alternative hypothesis in a test: The number of hours preschool children spend watching TV affects how they behave with other children when at day care. Alternative Most genetic mutations are deleterious. Alternative A diet of fast foods has no effect on liver function. Null Cigarette smoking influences risk of suicide. Alternative Growth rates of forest trees are unaffected by increases in carbon dioxide levels in the atmosphere. Null The number of hours that grade-school children spend doing homework predicts their future success on standardized tests. Alternative King cheetahs on average run the same speed as standard spotted cheetahs. Null The risk of facial clefts is equal for babies born to mothers who take folic acid supplements compared with those from mothers who do not. Null The mean length of African elephant tusks has changed over the last 100 years. Alternative Caffeine intake during pregnancy affects mean birth weight. Alternative What is an Alternative Hypothesis? Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEY In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on. What is the point? Why did we see the video? How does it relate the to the material from class? What is the relationship to sampling distributions? 2.3.3 All together: structure of a hypothesis test collect data, specify the variables of interest provide the null (and alternative) hypothesis values (often statements about parameters) the null claim is the science we want to reject the alternative claim is the science we want to prove generate a (null) sampling distribution to describe the variability of the statistic that was calculated along the way visualize the distribution of the statistics under the null model get_p_value to measure the consistency of the observed statistic and the possible values of the statistic under the null model make a conclusion using words that describe the research setting 2.4 1/28/20 Agenda Central Limit Theorem Mathematical approximation for one proportion 2.5 Normal Model 2.5.1 Central Limit Therm Example: Reese’s Pieces As with many of the examples, the Reese’s Pieces example comes from Chance and Rossman (2018). The example focuses on how the samples of orange Reese’s Pieces vary from sample to sample. Today we aren’t particularly interested in a specific research question, instead we are trying to understand the details of the model which describes how \\(\\hat{p}\\) varies from sample to sample. [Spoiler: the distribution is going to look like a bell! and the mathematical model which describes the variability is called the normal distribution.] Notes from the applet: http://www.rossmanchance.com/applets/OneProp/OneProp.htm?candy=1 How does the sampling distribution change as a function of \\(p\\) and \\(n\\)? When a normal distribution is placed on top of the empirical (computational) distribution, does it fit well? A sampling distribution is the probability distribution of all possible values of the statistic in all possible samples of the same size from the same population. Note: increasing the sample size reduces the spread of the sampling distribution of a statistic (i.e., increases the precision). Normal Probability Curve symmetric bell-shaped centered at \\(\\mu\\) \\(\\sigma\\) shows the point of inflection draw a picture every time you start a normal problem! The Central Limit Theorem The Central Limit Theorem says that the sampling distribution of an average will have a bell shaped distribution if \\(n\\) is big enough. The sampling distribution of \\(\\hat{p} = X/n\\) can be thought of as taking lots of random samples from a population, calculating \\(\\hat{p}\\), and creating a histogram. We can easily calculate what we’d expect from that sampling distribution if we know \\(p\\), the true population proportion. In fact, we saw two different sampling distributions (under two different hypotheses) in the Baseball example. Because \\(\\hat{p}\\) is actually an average (we talked about that with the birth data), the sampling distribution of \\(\\hat{p}\\) can be described by a normal distribution (as long as \\(n\\) is big enough). \\[\\begin{eqnarray*} \\hat{p} &amp;=&amp; \\frac{X}{n}\\\\ SD(\\hat{p}) = \\sigma_{\\hat{p}} &amp;=&amp; \\sqrt{\\frac{p (1-p)}{n}}\\\\ SE(\\hat{p}) &amp;=&amp; \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\ \\hat{p} &amp;\\sim&amp; N\\bigg(p, \\frac{p(1-p)}{n} \\bigg) \\ \\ \\ \\ \\ \\mbox{ (if the sample size is large enough)}\\\\ \\end{eqnarray*}\\] We would expect 95% of our \\(\\hat{p}\\) values to be within 2 standard deviations of the mean. That is, 95% of \\(\\hat{p}\\) are: \\[\\begin{eqnarray*} p \\pm 2 \\sqrt{\\frac{p(1-p)}{n}} \\end{eqnarray*}\\] Or put differently, when referring to a randomly selected \\(\\hat{p}\\), \\[\\begin{eqnarray*} P\\bigg( - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq \\hat{p} - p \\leq 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95\\\\ P\\bigg(\\hat{p} - 2 \\sqrt{\\frac{p(1-p)}{n}} \\leq p \\leq \\hat{p} + 2 \\sqrt{\\frac{p(1-p)}{n}}\\bigg) = 0.95 \\end{eqnarray*}\\] We’d love to create our interval for \\(p\\) using \\(\\hat{p} \\pm 2 \\sqrt{\\frac{p(1-p)}{n}}\\), but we don’t know \\(p\\)! One option is to use \\(SE(\\hat{p})\\) in the estimate of the variability. The Empirical Rule In a bell-shaped, symmetric distribution, % of data in what interval \\(\\approx 68\\%\\) of the observations fall within 1 st dev of the mean \\(\\approx 95\\%\\) of the observations fall within 2 st dev of the mean \\(\\approx 99.7\\%\\) of the observations fall within 3 st dev of the mean 2.6 1/30/20 Agenda Normal distribution (no q-q plots) Calculating normal probabilities 2.6.1 Normal Probabilities &amp; Z scores Z score A Z score of an observation is the number of standard deviations it falls above or below the mean. We compute the Z score for an observation x that follows a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) using \\[ Z = \\frac{x - \\mu}{\\sigma}\\] Normal probabilities We return to the Reese’s Pieces example to investigate the probability of a particular number of orange candies, using the normal approximation. Remember: \\[SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}\\] And the respective Z score is: \\[ Z = \\frac{\\hat{p} - p}{\\sqrt{\\frac{p(1-p)}{n}}}\\] What is the probability that in a sample of 25 candies, you would get less than 40% orange (provided that the machine colors 50% of the candies orange). Answer: 0.1587 What is the probability that in a sample of 250 candies, you would get less than 40% orange (provided that the machine colors 50% of the candies orange). Answer: 0.0007888 What is the probability that in a sample of 25 candies, you would get between 40% and 55% orange (provided that the machine colors 50% of the candies orange). Answer: 0.5328 library(mosaic) # (a) (0.4 - 0.5) / sqrt(0.5*0.5/25) ## [1] -1 xpnorm(-1, 0, 1) ## [1] 0.1586553 # (b) (0.4 - 0.5) / sqrt(0.5*0.5/250) ## [1] -3.162278 xpnorm(-3.16, 0, 1) ## [1] 0.0007888457 # (c) (0.55 - 0.5) / sqrt(0.5*0.5/25) ## [1] 0.5 xpnorm(c(-1, 0.5), 0, 1) ## [1] 0.1586553 0.6914625 Note that normal probabilities can be estimated for any variable that has a distribution which is well approximated by the bell shape given by a normal curve. Below we calculate Z scores and probabilities for a non-proportion setting and then ask whether the values could possibly be normal. (What do you think?) 2.6.1.1 Example: Athletic comparison2 The example below allows for a comparison between two athletes based on speed and strength. The following information is provided about the sample of individuals who were measured: Speed is measured by the time required to run a distance of 40 yards, with smaller times indicating more desirable (faster) speeds. From the data, the times to run 40 yards have a mean of 4.60 seconds and a standard devotion of 0.15 seconds, with a minimum of 4.40 seconds. Strength is measured by the amount of weight lifted, with more weight indicating more desirable (greater) strength From the data, the amount of weight lifted has a mean of 310 pounds and a standard deviation of 25 pounds. mean std dev minimum Time to run 40 yards 4.60 sec 0.15 sec 4.40 sec Amount lifted 310 lbs 25 lbs NA Calculate and interpret the Z score for a player who can lift weight of 370 pounds. \\[Z = \\frac{370-310}{25} = 2.4\\] This z-score tells us that a player who can lift 370 pounds is lifting 2.4 SDs more than average. Saying that this weight is 2.4 SDs away from the average would leave out important information about direction. Consider two players, A and B (with data given as below). Which player should be selected for the team if only one player can be selected? Player A Player B Time to run 40 yards 4.42 sec 4.57 sec Amount lifted 370 lbs 375 lbs At a first glance, we can see that A is faster, and B is stronger. Understanding how each player performs (in strength and speed) relative to the rest of the players is the first step in answering the question. We will calculate four Z scores, one for each player and each task: \\[\\begin{align*} Z_{Aspeed} = \\frac{4.42 - 4.6}{0.15} = -1.2\\\\ Z_{Astrength} = \\frac{370-310}{25} = 2.4\\\\ Z_{Bspeed} = \\frac{4.57 - 4.6}{0.15} = -0.2\\\\ Z_{Bstrength} = \\frac{375-310}{25} = 2.6\\\\ \\end{align*}\\] After calculating Z scores, it is found that Player B is only slightly stronger than Player A, but Player A is considerably faster than Player B. Because the question advised us to consider both criteria as equally valuable, Player A is the better choice. Using the full information about the speed data, do you think that the distribution of 40 yard running times is approximately normal? NO! The minimum is too close to the mean for the normal distribution to provide a reasonable model. What does “too close” mean here? Let’s see how many standard deviations the minimum is below the mean: \\[ Z = \\frac{4.4 - 4.6}{0.15} = -1.33 \\] The Z score tells us that the minimum speed is only -1.33 standard deviations below the mean. According to the normal distribution (see the plot below), we would expect about 9% of the observations to be lower than 4.4 seconds, so the normal distribution does not seem to be a great fit to these observations. xpnorm(-1.333, 0, 1, plot = TRUE) ## [1] 0.0912659 2.7 Confidence Intervals References "],
["references.html", "References", " References "]
]
