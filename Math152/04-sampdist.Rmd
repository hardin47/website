# Sampling Distributions of Estimators {#sampdist}

A **statistic** is a function of some observed random variables.  The **sampling distribution** of a statistic tells us which values a statistic assumes and how likely those values are.  Because a statistic is a function of the random variables $X_1, X_2, \ldots, X_n$, if we know the distribution of $X$, in principle, we should be able to derive the distribution of our statistic.

::: {.example}
If the data are normal, the sampling distribution of their mean is also normal.  While the result may look like the Central Limit Theorem, there is no limiting behavior here.  That is, the sampling distribution of $\overline{X}$ is normal regardless of the sample size.  The proof come directly from the result that linear combinations of normal random variables are also normal.

\begin{eqnarray*}
X_i &\sim& N(\mu, \sigma^2)\\
\overline{X} &\sim& N(\mu, \sigma^2 / n)
\end{eqnarray*}
:::

## The Chi-Square Distribution

The chi-square distribution is a probability distribution with the following characteristics.

\begin{eqnarray*}
X &\sim& \chi^2_n\\
f_X(x) &=& \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2 -1} e^{-x/2} \ \ \ \ \ \ x > 0\\
E[X] &=& n\\
Var[X] &=& 2n\\
\psi_X(t) &=& \Bigg( \frac{1}{1-2t} \Bigg)^{n/2} \ \ \ \ \ \ t < 1/2\\
( &=&  E[e^{tX}] )\\
\end{eqnarray*}

Recall Moment Generating Functions $(\psi_X(t))$, @degroot page 205.

::: {.theorem}
@degroot 7.2.1

Let $X_1, X_2, \ldots X_k \sim \chi^2_{n_i}, \ \ i=1, \ldots, k$, independently.  Then, $X_1 + X_2 + \cdots + X_k = \sum_{i=1}^k X_i \sim \chi^2_{n_1+n_2 +\cdots+n_k}$.

That is, if the data are independent chi-square random variables, the sampling distribution of their sum is also chi-square.
:::


::: {.proof}

\begin{eqnarray*}
Y&=& \sum_{i=1}^k X_i\\
\psi_Y (t) &=& E[e^{Yt} ]\\
&=& E[e^{t \sum_{i=1}^k X_i}]\\
&=& \prod_{i=1}^k E[e^{tX_i}]\\
&=& \prod_{i=1}^k \psi_{X_i}(t)\\
&=& \prod_{i=1}^k \bigg( \frac{1}{1-2t} \bigg)^{ n_i /2}\\
&=& \Bigg( \frac{1}{1-2t} \Bigg)^{\sum n_i /2}
\end{eqnarray*}
See theorem 4.4.3, pg 207.
:::


::: {.theorem}
@degroot 7.2.1 1/2

If $Z \sim N(0,1), Y=Z^2,$ then $Y \sim \chi^2_1$.

Note that the result here is to provide the distribution of a transformation of a random variable.  There is a single data value $(Z)$, and the result provides the distribution of another single value, $(Y).$  The value $Y$ would typically not be referred to as a statistic because it is not a summary of observations.

That said, just below, $Z$ itself will be a statistic (instead of a single value) and then both $Z$ and $Y$ will have sampling distributions!
:::

::: {.proof}
Let $\Phi$ and $\phi$ be the cdf and pdf of Z.\\
Let F and f be the cdf and pdf of Y.\\
\begin{eqnarray*}
F(y) &=& P(Y \leq y) = P(Z^2 \leq y)\\
&=& P(-y^{1/2} \leq Z \leq y^{1/2})\\
&=& \Phi(y^{1/2}) -  \Phi(- y^{1/2})  \ \ \ \ \ y > 0\\
\end{eqnarray*}
\smallskip
\begin{eqnarray*}
f(y) &=& \frac{\partial F(y)}{\partial y} = \phi(y^{1/2}) \cdot \frac{1}{2} y^{-1/2} - \phi(-y^{1/2}) \cdot \frac{1}{2} -y^{-1/2}\\
&=& \frac{1}{2} y^{-1/2} ( \phi(y^{1/2}) + \phi(-y^{1/2})) \ \ \ \ \ \ y > 0\\
\mbox{we know} && \phi(y^{1/2}) = \phi(-y^{1/2}) = \frac{1}{\sqrt{2 \pi}} e^{-y/2}\\
%\therefore
f(y) &=& y^{-1/2} \frac{1}{\sqrt{2 \pi}} e^{-y/2} \ \ \ \ \ \ y > 0 \\
&=& \frac{1}{2^{1/2}\pi^{1/2}} y^{1/2 - 1} e^{-y/2} \ \ \ \ \ \ y >0\\
Y &\sim& \chi^2_1\\
\end{eqnarray*}
note: $\Gamma(1/2) = \sqrt{\pi}$.
:::



By combining Theorems 7.2.1 and 7.2.1 1/2, we get:

::: {.theorem}
@degroot 7.2.2

If $X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(0,1)$,
\begin{eqnarray*}
\sum_{i=1}^k X_i^2 \sim \chi^2_k
\end{eqnarray*}
\noindent
Note: if $X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(\mu, \sigma^2)$,
\begin{eqnarray*}
\frac{X_i - \mu}{\sigma} &\sim& N(0,1)\\
\sum_{i=1}^k \frac{(X_i - \mu)^2}{\sigma^2} &\sim& \chi^2_k\\
\end{eqnarray*}

If the data are $iid$ normal, the sum of their squared values has a sampling distribution which is chi-square.
:::


## Independence of the Mean and Variance of a Normal Random Variable


::: {.theorem}
@degroot 8.3.1

Let $X_1, X_2, \ldots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$.

1. $\overline{X}$ and $\frac{1}{n} \sum(X_i - \overline{X})^2$ are independent.  (This is **only** true for normal random variables.  You should read through the proof in your book.) 
2. $\overline{X} \sim N(\mu, \sigma^2/n)$.  (Not the CLT, why not?) 
3. $\frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}$.  (Main idea is that we only have $n-1$ independent things.)
:::



## The t-distribution

Let $Z \sim N(0,1)$ and $Y \sim \chi^2_n$.  If $Z$ and $Y$ are independent, then:

::: {.definition}
\begin{eqnarray*}
X = \frac{Z}{\sqrt{Y/n}} \sim t_n \mbox{  by definition}
\end{eqnarray*}
:::

\begin{eqnarray*}
f(x) &=& \frac{\Gamma(\frac{n+1}{2})}{(n \pi)^{1/2} \Gamma(\frac{n}{2})} (1 + \frac{x^2}{n})^{-(n+1)/2} \ \ \ \ n > 2\\
E[X] &=&0\\
Var(X) &=& \frac{n}{n-2}
\end{eqnarray*}

Remember:
\begin{eqnarray*}
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1) \mbox{ independently of } \frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}
\end{eqnarray*}

\begin{eqnarray*}
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{\sigma^2}/(n-1)}} &=& \frac{\overline{X} - \mu}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{n-1}/n}}\\
&=& \frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1} !
\end{eqnarray*}

As stated above, the t-distribution is defined as the distribution which is given when a standard normal is divided by the square root of a chi-square random variable divided by its degrees of freedom.  And while that may seem obtuse at first glance, it comes in extremely handy when standardizing a sample mean by using the standard error (instead of the standard deviation) of the mean.


::: {.example}
According to some investors, foreign stocks have the potential for high yield, but the variability in their dividends may be greater than what is typical for American companies.   Let's say we take a random sample of 10 foreign stocks; assume also that we know the population distribution from which American stocks come (i.e., we have the American parameters).  If **we believe that foreign stock prices are distributed similarly  (normal with the same mean and variance)** to American stock prices, how likely is it that a sample of 10 foreign stocks would produce a standard deviation which is 50% bigger than American stocks?

\begin{eqnarray*}
P(\hat{\sigma} / \sigma > 1.5 ) &=& ?\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &\sim& \chi^2_{n-1} \ \ \ \ \mbox{(normality assumption)}\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &=& n\frac{\sum (X_i - \overline{X})^2/n}{\sigma^2}\\
&=& \frac{n \hat{\sigma^2}}{\sigma^2}\\
P(\hat{\sigma} / \sigma > 1.5 ) &=& P(\hat{\sigma}^2 / \sigma^2 > 1.5^2 ) \\
&=& P(n \hat{\sigma}^2 / \sigma^2 > n 1.5^2 )\\
&=& 1 - \chi^2_{n-1} (n 1.5^2)\\
&=& 1 - \chi^2_{n-1} (22.5)\\
&=& 1 - pchisq(22.5,9) = 0.00742 \ \ \ \mbox{ in R}
\end{eqnarray*}
:::

::: {.example}
Suppose we take a random sample of foreign stocks (both $\mu$ and $\sigma^2$ unknown).  Find the value of $k$ such that the sample mean is no more than $k$ sample standard deviations $(s)$ above the mean $\mu$ with probability 0.90.

\noindent
Data:  $n=10$, $\hat{\mu}  = \overline{x}$, $s^2 = \frac{\sum(x_i - \overline{x})^2}{n-1}$, $s = \sigma'$.

\begin{eqnarray*}
P(\overline{X} < \mu + k s) &=& 0.9\\
P\Bigg(\frac{\overline{X} - \mu}{s} < k \Bigg) = P\Bigg(\frac{\overline{X} - \mu}{s/\sqrt{n}} < k \sqrt{n}\Bigg) &=& 0.9\\
\frac{\overline{X} - \mu}{s / \sqrt{n}} &\sim& t_9\\
\sqrt{n} k &=& 1.383\\
k &=& \frac{1.383}{\sqrt{10}} = 0.437\\
\mbox{note, in R: } qt(0.9,9) &=& 1.383
\end{eqnarray*}

How would this problem have been different if we had known $\sigma$?  Or even if we had wanted the answer to the question in terms of number of population standard deviations?
:::


## <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions

1. What does it mean for a statistic to have a sampling distribution? 
2. 

## <i class="fas fa-balance-scale"></i> Ethics Considerations

1. 


## R code: Tanks Example

How can a random sample of integers between 1 and $N$ (with $N$ unknown to the researcher) be used to estimate $N$?  This problem is known as the German tank problem and is derived directly from a situation where the Allies used maximum likelihood estimation to determine how many tanks the Axes had produced.  See \url{https://en.wikipedia.org/wiki/German_tank_problem}.

1. The tanks are numbered from 1 to $N$.  Working with your group, randomly select five tanks, without replacement, from the bowl.  The tanks are numbered:

\vspace{4cm}

\item
Think about how you would use your data to estimate $N$.  (Come up with at least 3 estimators.)  Come to a consensus within the group as to how this should be done.  One person from your group will report out after the warm-up is over.  Ideally, the person to report out will be someone who has not yet spoken in class this semester.  Step-up if you haven't yet spoken.  Step back if you speak regularly.

Our estimates of $N$ are:

\vspace{3cm}

Our rules or formulas for the estimators of $N$ based on a sample of n (in your case 5) integers are:


\pagebreak

\normalsize
\noindent
Assuming the random variables are distributed according to a discrete uniform.   (Tbh, this model is with replacement, but the answers you get aren't much different than without replacement if $n < < N$.)
\begin{eqnarray*}
X_i \sim P(X=x | N) = \frac{1}{N} \ \ \ \ \ x = 1,2,\ldots, N \ \ \ \ i=1,2,\ldots, n
\end{eqnarray*}

\item
What is the method of moments estimator of N?

\vspace{7cm}

\item
What is the maximum likelihood estimator of N?
\vspace{4cm}


\end{enumerate}


\pagebreak


\subsection*{Theoretical Mean Squared Error}

\noindent
Most of our estimators are made up of four basic functions of the data: the mean, the median, the min, and the max.  Fortunately, we know something about the moments of these functions:
\begin{center}
\large
\begin{tabular}{l|c|c}
g($\underline{X}$) & E[ g($\underline{X}$) ] & Var( g($\underline{X}$) )\\
\hline
& &\\
\vspace{-.3cm}
$\overline{X}$ & $\frac{N+1}{2}$ & \ \ \ $\frac{(N+1)(N-1)}{12 n}$ \ \ \ \\
& &\\
\vspace{-.3cm}
median($\underline{X}$) = M \ \ \ & $\frac{N+1}{2}$ & $\frac{(N-1)^2}{4 n}$\\
& &\\
\vspace{-.3cm}
min($\underline{X}$) & \ \ $\frac{(N-1)}{n} + 1$ \ \  & $\bigg(\frac{N-1}{n}\bigg)^2$\\
& &\\
\vspace{-.3cm}
max($\underline{X}$) & $N - \frac{(N-1)}{n}$ & $\bigg(\frac{N-1}{n}\bigg)^2$\\
\end{tabular}
\end{center}

\vspace{1cm}

\normalsize
\noindent
Using this information, we can calculate the MSE for 4 of the estimators that we have derived.  (Remember that MSE = Variance + Bias$^2$.)

\begin{eqnarray}
\mbox{MSE } ( 2 \cdot \overline{X} - 1) &=& \frac{4 (N+1) (N-1)}{12n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&=& \frac{4 (N+1) (N-1)}{12n} \\
\nonumber \\
\mbox{MSE } ( 2 \cdot M - 1) &=& \frac{4 (N-1)^2}{4n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&=& \frac{4 (N-1)^2}{4n} \\
\nonumber \\
\mbox{MSE } ( \max(\underline{X})) &=& \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(N - \frac{(N-1)}{n} - N\Bigg)^2 \nonumber\\
&=& \bigg(\frac{N-1}{n}\bigg)^2 + \bigg(\frac{N-1}{n} \bigg)^2  = 2*\bigg(\frac{N-1}{n} \bigg)^2 \\
\nonumber \\
\mbox{MSE } \Bigg( \bigg( \frac{n+1}{n} \bigg) \max(\underline{X})\Bigg) &=& \bigg(\frac{n+1}{n}\bigg)^2 \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(\bigg(\frac{n+1}{n}\bigg) \bigg(N - \frac{N-1}{n} \bigg) - N \Bigg)^2
\end{eqnarray}

Our rules or formulas for the estimators of N based on a sample of n (in your case 5) integers are:


\pagebreak

\normalsize
\noindent
Assuming the random variables are distributed according to a discrete uniform.   (Tbh, this model is with replacement, but the answers you get aren't much different than without replacement if $n < < N$.)
\begin{eqnarray*}
X_i \sim P(X=x | N) = \frac{1}{N} \ \ \ \ \ x = 1,2,\ldots, N \ \ \ \ i=1,2,\ldots, n
\end{eqnarray*}

\item
What is the method of moments estimator of N?

\vspace{7cm}

\item
What is the maximum likelihood estimator of N?
\vspace{4cm}


\end{enumerate}


\pagebreak

\begin{flushright}
Math 152 \\
Jo Hardin \\
\end{flushright}

\begin{center}
{\bf Mean Squared Error}
\end{center}

\noindent
Most of our estimators are made up of four basic functions of the data: the mean, the median, the min, and the max.  Fortunately, we know something about the moments of these functions:
\begin{center}
\large
\begin{tabular}{l|c|c}
g($\underline{X}$) & E[ g($\underline{X}$) ] & Var( g($\underline{X}$) )\\
\hline
& &\\
\vspace{-.3cm}
$\overline{X}$ & $\frac{N+1}{2}$ & \ \ \ $\frac{(N+1)(N-1)}{12 n}$ \ \ \ \\
& &\\
\vspace{-.3cm}
median($\underline{X}$) = M \ \ \ & $\frac{N+1}{2}$ & $\frac{(N-1)^2}{4 n}$\\
& &\\
\vspace{-.3cm}
min($\underline{X}$) & \ \ $\frac{(N-1)}{n} + 1$ \ \  & $\bigg(\frac{N-1}{n}\bigg)^2$\\
& &\\
\vspace{-.3cm}
max($\underline{X}$) & $N - \frac{(N-1)}{n}$ & $\bigg(\frac{N-1}{n}\bigg)^2$\\
\end{tabular}
\end{center}

\vspace{1cm}

\normalsize
\noindent
Using this information, we can calculate the MSE for 4 of the estimators that we have derived.  (Remember that MSE = Variance + Bias$^2$.)

\begin{eqnarray}
\mbox{MSE } ( 2 \cdot \overline{X} - 1) &=& \frac{4 (N+1) (N-1)}{12n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&=& \frac{4 (N+1) (N-1)}{12n} \\
\nonumber \\
\mbox{MSE } ( 2 \cdot M - 1) &=& \frac{4 (N-1)^2}{4n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&=& \frac{4 (N-1)^2}{4n} \\
\nonumber \\
\mbox{MSE } ( \max(\underline{X})) &=& \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(N - \frac{(N-1)}{n} - N\Bigg)^2 \nonumber\\
&=& \bigg(\frac{N-1}{n}\bigg)^2 + \bigg(\frac{N-1}{n} \bigg)^2  = 2*\bigg(\frac{N-1}{n} \bigg)^2 \\
\nonumber \\
\mbox{MSE } \Bigg( \bigg( \frac{n+1}{n} \bigg) \max(\underline{X})\Bigg) &=& \bigg(\frac{n+1}{n}\bigg)^2 \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(\bigg(\frac{n+1}{n}\bigg) \bigg(N - \frac{N-1}{n} \bigg) - N \Bigg)^2
\end{eqnarray}

\begin{table}
\begin{center}
\begin{tabular}{l|cccccccc}
&       xbar2 & med2 &xbarmed & iqr2 &  max &adjmax &max.min& maxmax\\
\hline
mean &  244.6 &244.1&   244.3 &215.5& 230.6  &247.1&   244.9  &251.7\\
median& 244.5 &245.0 &  244.0 &215.0 &235.0  &251.8 &  245.0  &245.0\\
sd     & 35.8  &58.4  &  45.6  &53.5  &13.9  & 14.8  &  20.2  & 28.2\\
min   & 111.3 & 53.0   & 92.1 & 47.0 &139.0 & 148.9   &147.0 & 148.3\\
max   & 378.2 &429.0   &400.8& 401.0 &245.0&  262.5   &359.0&  378.2\\
\end{tabular}
\caption{Sample statistics for 10,000 reps taken from a population with $N=245$ and $n=15$.}
\end{center}
\end{table}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=.5,angle=90]{tankhist245.pdf}
\end{center}
\end{figure}

\begin{figure}[ht]
\begin{center}
\includegraphics[scale=.6,angle=90]{tankmse.pdf}
\end{center}
\end{figure}

\end{document}

