<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Bayesian Estimation | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="2.1 Bayes’ Rule  Theorem 2.1 Bayes’ Rule Given events \(A\) and \(B\), \[\begin{eqnarray*} P(A|B) &amp;=&amp; \frac{P(AB)}{P(B)} = \frac{P(B|A) P(A)}{P(AB) + P(A^cB)} \nonumber \\ &amp;=&amp; \frac{P(B|A)P(A)}{...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 2 Bayesian Estimation | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="2.1 Bayes’ Rule  Theorem 2.1 Bayes’ Rule Given events \(A\) and \(B\), \[\begin{eqnarray*} P(A|B) &amp;=&amp; \frac{P(AB)}{P(B)} = \frac{P(B|A) P(A)}{P(AB) + P(A^cB)} \nonumber \\ &amp;=&amp; \frac{P(B|A)P(A)}{...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Bayesian Estimation | Statistical Theory">
<meta name="twitter:description" content="2.1 Bayes’ Rule  Theorem 2.1 Bayes’ Rule Given events \(A\) and \(B\), \[\begin{eqnarray*} P(A|B) &amp;=&amp; \frac{P(AB)}{P(B)} = \frac{P(B|A) P(A)}{P(AB) + P(A^cB)} \nonumber \\ &amp;=&amp; \frac{P(B|A)P(A)}{...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="bayes" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Bayesian Estimation<a class="anchor" aria-label="anchor" href="#bayes"><i class="fas fa-link"></i></a>
</h1>
<!--
% Great Shiny App that compares Bayesian & Frequentist:
% https://kruschke.shinyapps.io/KruschkeFreqAndBayesApp/
% https://link.springer.com/article/10.3758%2Fs13423-016-1221-4
% Psychonomic Bulletin & Review
% February 2018, Volume 25, Issue 1, pp 178?206 
% The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective
% John Kruschke
-->
<div id="bayes-rule" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Bayes’ Rule<a class="anchor" aria-label="anchor" href="#bayes-rule"><i class="fas fa-link"></i></a>
</h2>
<div class="theorem">
<p><span id="thm:unnamed-chunk-2" class="theorem"><strong>Theorem 2.1  </strong></span><strong>Bayes’ Rule</strong></p>
<p>Given events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>,
<span class="math display">\[\begin{eqnarray*}
P(A|B) &amp;=&amp; \frac{P(AB)}{P(B)} = \frac{P(B|A) P(A)}{P(AB) + P(A^cB)} \nonumber \\
&amp;=&amp; \frac{P(B|A)P(A)}{ \sum_i P(B|A_i) P(A_i)}
\end{eqnarray*}\]</span></p>
</div>
<p>Many of the following examples may be familiar to you. When reading them, work to understand both the intuition (the <strong>denominator</strong> changes when we condition!) as well as the mathematical connection to Bayes’ Rule.</p>
<div class="example">
<p><span id="exm:unlabeled-div-1" class="example"><strong>Example 2.1  </strong></span>Suppose the rate of infection with TB is 1 in 1000 (about 0.1 percent = 0.001). Suppose a TB test is used which is 90% accurate: it gives a positive result for 10 percent of people who do not actually have TB, but do have a reaction to the skin test. Also, 10% of the people who actually have TB fail to react to the test.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Example taken from &lt;em&gt;A Course in Probability&lt;/em&gt; by Neil Weiss.&lt;/p&gt;"><sup>1</sup></a></p>
<ul>
<li>What’s the chance that someone has TB if they test positive?</li>
<li>What’s the chance that a randomly chosen person tests negative and actually has TB?</li>
<li>There is another TB test which gives fewer false positives, but is more expensive. Would it be better to use that one?</li>
<li>What is the prior probability of having TB?</li>
<li>What is the posterior probability of having TB (given a positive test)?</li>
</ul>
<p><strong>Solution:</strong></p>
<p>We can use a table to figure out the probabilities. Consider a population with 10,000 people:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th align="center">Test +</th>
<th align="center">Test -</th>
<th align="center">Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>TB +</td>
<td align="center">9</td>
<td align="center">1</td>
<td align="center">10</td>
</tr>
<tr class="even">
<td>TB -</td>
<td align="center">999</td>
<td align="center">8,991</td>
<td align="center">9,990</td>
</tr>
<tr class="odd">
<td>Total</td>
<td align="center">1,008</td>
<td align="center">8,992</td>
<td align="center">10,000</td>
</tr>
</tbody>
</table></div>
<p>Alternatively, we can use probability statements which will be easier to work with in the long-run as the scenarios get more complicated. We know the following:</p>
<p><span class="math display">\[\begin{eqnarray*}
P(TB + ) &amp;=&amp; 0.001\\
P(Test + | TB - ) &amp;=&amp; 0.1 \\
P(Test - | TB + ) &amp;=&amp; 0.1
\end{eqnarray*}\]</span></p>
<ul>
<li>10 in 10,000 people will have the disease. 9 of those 10 will actually test positive for TB. However, 999 of 9990 people will be false positives. so, only <span class="math inline">\(9/(999+9) = 0.0089 \mbox{ or } \approx\)</span> 0.9% of people who test positive actually have TB.</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
P( TB + | Test + ) &amp;=&amp; \frac{P(TB + \&amp; Test + )}{P(Test + )} = \frac{P(Test  + | TB + ) P(TB+)}{P(Test + )}\\
&amp;=&amp; \frac{P(Test  + | TB + ) P(TB+)}{P(Test  + | TB - ) P(TB-) + P(Test  + | TB + ) P(TB+)} \\
&amp;=&amp; \frac{0.9 \cdot 0.001}{ 0.1 \cdot 0.999 + 0.9 \cdot 0.001}\\
&amp;=&amp; 0.0089
\end{eqnarray*}\]</span></p>
<ul>
<li>About 1 in 10,000 people will have a false negative (0.0001 = 0.01% as opposed to 9.99% false positives).</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
P( TB + | Test - ) &amp;=&amp; \frac{P(TB + \&amp; Test - )}{P(Test - )} = \frac{P(Test  - | TB + ) P(TB+)}{P(Test - )}\\
&amp;=&amp; \frac{P(Test - | TB + ) P(TB+)}{P(Test  - | TB - ) P(TB-) + P(Test  - | TB + ) P(TB+)} \\
&amp;=&amp; \frac{0.1 \cdot 0.001}{ 0.9 \cdot 0.999 + 0.1 \cdot 0.001}\\
&amp;=&amp; 0.00011121
\end{eqnarray*}\]</span></p>
<ul>
<li>Not necessarily, since it’s much worse to have a false negative than a false positive. People who test positive are then given another test with fewer false positives.</li>
<li>0.001</li>
<li>0.009</li>
</ul>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-2" class="example"><strong>Example 2.2  </strong></span>A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. Suppose you are told the following:</p>
<ul>
<li>85 percent of the cabs in the city are Green, and the remaining 15 percent are Blue.</li>
<li>A witness identified the cab as Blue (but it was dark!) The court tested the reliability of the witness under the same circumstances that existed on the night of the accident, and determined that the witness correctly identified the cab color 80% of the time, and made a mistake 20% of the time, regardless of the actual color of the cab.</li>
</ul>
<p>What’s the verdict? I.e., what is the probability that the cab involved in the hit-and-run was actually Blue?</p>
<p><strong>Solution:</strong></p>
<p>From the information given, the following probabilities are known:
<span class="math display">\[\begin{eqnarray*}
P(said B | B) &amp;=&amp; 0.8\\
P(said G | B) &amp;=&amp; 0.2\\
P(B) &amp;=&amp; 0.15 \ \ \ \mbox{prior probability!}
\end{eqnarray*}\]</span></p>
<p>The probability of interest is:
<span class="math display">\[\begin{eqnarray*}
P(B | said B) &amp;=&amp; \frac{P(said B | B) P(B)}{P(said B | G) P(G) + P(said B | B) P(B)}\\
&amp;=&amp; \frac{0.8*0.15}{0.2*0.85 + 0.8*0.15} = 0.41
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-3" class="example"><strong>Example 2.3  </strong></span>Consider the famous Monte Hall problem based on a game show, <em>Let’s Make a Deal</em>. As part of the show, the contestant is asked to pick one of three doors. Two of the doors have nothing behind them, and the third door has a car as a prize. Monte Hall (the host) opens a non-prize door that the contestant hadn’t chosen (there is always such a door available to open because there is only one prize). Monte then offers the contestant the opportunity to switch from the original door to the remaining door. Should she switch? Stay? Or it doesn’t matter? What is the probability of winning under each of the situations?</p>
<p>Define the following:
<span class="math display">\[\begin{eqnarray*}
C_i &amp;:&amp; \mbox{the car is behind Door $i$, for } i \in \{ 1, 2, 3\}\\
H_{ij} &amp;:&amp; \mbox{the host opens Door $j$ after the player has picked Door $i$, for } i, j \in \{ 1,2,3\}
\end{eqnarray*}\]</span></p>
<p>For example, <span class="math inline">\(C_2\)</span> is the situation that the car is behind Door 2, and <span class="math inline">\(H_{23}\)</span> denotes the situation that the host opened Door 3 after I chose Door 2. Note that <span class="math inline">\(H_{ij}\)</span> is the information we have (what we’ll condition on). Let’s say you start off by picking Door 2, and the host opens Door 3.</p>
<p><span class="math display">\[\begin{eqnarray*}
P(C_i) &amp;=&amp; \frac{1}{3}\\
P(C_2) &amp;=&amp; \frac{1}{3}\\
P(C_2 | H_{23}) &amp;=&amp; \frac{P(H_{23} | C_2) P(C_2)}{P(H_{23})} \ \ \ \ \ \mbox{(Bayes rule, right here!  See } P(H_{23}) \mbox{ below)}\\
&amp;=&amp; \frac{\frac{1}{2} \cdot \frac{1}{3}}{P(H_{23})} \\
\nonumber \\
P(H_{23}) &amp;=&amp;  P(H_{23} C_1) + P(H_{23} C_2) + P(H_{23} C_3)\\
&amp;=&amp;  P(H_{23} | C_1) P(C_1) + P(H_{23} | C_2) P(C_2) + P(H_{23} | C_3) P(C_3)\\
&amp;=&amp; \frac{1}{2} \cdot \frac{1}{3} + 1 \frac{1}{3} + 0 \frac{1}{3} = \frac{1}{2}\\
\nonumber\\
P(C_2 | H_{23}) &amp;=&amp; \frac{\frac{1}{2} \cdot \frac{1}{3}}{\frac{1}{2}}\\
&amp;=&amp; \frac{1}{3}\\
\end{eqnarray*}\]</span></p>
<p>Using what we know about the rules of the game (that is, if the car is behind door 3, he’d never open it for you!), we know:
<span class="math display">\[\begin{eqnarray*}
P(C_3 | H_{23}) = 0
\end{eqnarray*}\]</span></p>
<p>Because the car has to be behind one of the three doors:
<span class="math display">\[\begin{eqnarray*}
1 &amp;=&amp; P(C_1 | H_{23}) + P(C_2 | H_{23}) + P(C_3 | H_{23})\\
P(C_1 | H_{23} ) &amp;=&amp; 1 - P(C_2 | H_{23}) - P(C_3 | H_{23})\\
&amp;=&amp; 1 - \frac{1}{3} - 0\\
&amp;=&amp; \frac{2}{3}
\end{eqnarray*}\]</span></p>
<p>So…. the probability that the car is behind Door 1 is 2/3 where the probability that it’s behind Door 2 is 1/3 <span class="math inline">\(\rightarrow\)</span> you should switch doors!</p>
</div>
<p>The previous situations have all been based on discrete values for both the parameter and the data (neither of which are typically true). If we think of {} and {}, we want to find the value of {} such that <span class="math inline">\(P(A|B)\)</span> is maximized. We are going to use probability distribution functions (pdfs) instead of discrete probabilities, so we need more notation.</p>
</div>
<div id="prior-distributions" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Prior Distributions<a class="anchor" aria-label="anchor" href="#prior-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>prior distribution</strong> is the distribution of a parameter (e.g., <span class="math inline">\(\theta\)</span>) before observing any data. Note:</p>
<ul>
<li>observations come from <span class="math inline">\(f(x|\theta), \theta \in \Omega\)</span>
</li>
<li>we can express how likely <span class="math inline">\(\theta\)</span> is to be in various regions of <span class="math inline">\(\Omega\)</span> in terms of probability or a <strong>distribution</strong> on <span class="math inline">\(\theta\)</span>.</li>
<li>
<strong>Bayesians</strong> believe we should use prior distributions for all our modeling because we always know <em>something</em> about the situation at hand.</li>
<li>
<strong>Frequentists</strong> believe we should only use the data collected in the experiment or sample (and no prior information).</li>
</ul>
<div class="example">
<p><span id="exm:unnamed-chunk-3" class="example"><strong>Example 2.4  </strong></span>We want to predict the high temperature on a given day in October.
<span class="math display">\[\begin{align*}
\Omega = \{ (\theta, \sigma^2) &amp;: \theta \in I\!\!R, \sigma^2 \in I\!\!R^+\}\\
\mbox{or } &amp;: -\infty &lt; \theta &lt; \infty, \sigma^2 &gt; 0 \}\\
\mbox{or } &amp;: \theta &gt; 30, 0 &lt; \sigma^2 &lt; 625 \}\\
\end{align*}\]</span>
Suppose we know the variance <span class="math inline">\(\sigma^2 = 12^2\)</span>. The mean, <span class="math inline">\(\theta\)</span> is unknown. We might specify the prior distribution on <span class="math inline">\(\theta\)</span> as:
<span class="math display">\[\begin{eqnarray*}
\xi (\theta) &amp;\rightarrow&amp; \theta \sim N(\mu, \nu^2)\\
&amp;&amp; \mu =78^\circ, \nu^2 = (2.5^\circ)^2\\
\end{eqnarray*}\]</span></p>
</div>
<p>The value for <span class="math inline">\(\nu\)</span> is a measure of the uncertainty of our <strong>prior beliefs</strong>. That is, we have estimated <span class="math inline">\(\theta\)</span> to be <span class="math inline">\(78^\circ\)</span>, but we aren’t sure of that value, so we add some uncertainty on to our belief (<span class="math inline">\(\nu\)</span>). <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\nu\)</span> are called <strong>hyper-parameters</strong>.</p>
</div>
<div id="posterior-distributions" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Posterior Distributions<a class="anchor" aria-label="anchor" href="#posterior-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>posterior distribution</strong> is the conditional distribution of the parameter (e.g., <span class="math inline">\(\theta\)</span>) given the observed data.</p>
<p>Aside, a little probability review:</p>
<div class="example">
<p><span id="exm:unnamed-chunk-4" class="example"><strong>Example 2.5  </strong></span>Suppose you are interested in rolling two dice. Let <span class="math inline">\(X\)</span> be the larger value; let <span class="math inline">\(Y\)</span> be the sum of the two dice. Find the <strong>joint</strong> and <strong>marginal</strong> distributions of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The solution is a table of <strong>probabilities</strong>:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="8%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="7%">
<col width="8%">
</colgroup>
<thead><tr class="header">
<th align="center"><span class="math inline">\(X \backslash Y\)</span></th>
<th align="center">2</th>
<th align="center">3</th>
<th align="center">4</th>
<th align="center">5</th>
<th align="center">6</th>
<th align="center">7</th>
<th align="center">8</th>
<th align="center">9</th>
<th align="center">10</th>
<th align="center">11</th>
<th align="center">12</th>
<th align="center">P(X)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{7}{36}\)</span></td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{9}{36}\)</span></td>
</tr>
<tr class="even">
<td align="center">6</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{11}{36}\)</span></td>
</tr>
<tr class="odd">
<td align="center">P(Y)</td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{6}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{5}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{4}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{3}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{2}{36}\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{36}\)</span></td>
<td align="center"></td>
</tr>
</tbody>
</table></div>
</div>
<p><strong>More notation review</strong>. Suppose we have <span class="math inline">\(n\)</span> data points from <span class="math inline">\(f(x| \theta)\)</span>. We’ll assume it’s a simple random sample (SRS), and therefore the observations are independent.</p>
<p><span class="math display">\[\begin{eqnarray*}
f(x_1, x_2, \ldots, x_n| \theta) &amp;=&amp; f(x_1| \theta) f(x_2| \theta) \cdots f(x_n| \theta)\\
\mbox{where  } \underline{x} &amp;=&amp; \{ x_1, x_2, \ldots, x_n \}\\
f(x_1, x_2, \ldots, x_n| \theta) &amp;=&amp; f(\underline{x} | \theta)
\end{eqnarray*}\]</span></p>
<p>Remember, <span class="math inline">\(f(\underline{x} | \theta)\)</span> is the <strong>conditional</strong> distribution of <span class="math inline">\(\underline{x}\)</span> given <span class="math inline">\(\theta\)</span>. The <strong>likelihood function</strong>, <span class="math inline">\(f(\underline{x} | \theta)\)</span>, is the joint pdf of the observations (representing: how <strong>likely</strong> are the data?).</p>
<p>Define
<span class="math display">\[\begin{eqnarray*}
f(\underline{x}, \theta) &amp;=&amp; f(\underline{x} | \theta) \xi (\theta)\\
g_n(\underline{x}) &amp;=&amp; \int_\Omega f(\underline{x}, \theta) d\theta\\
&amp;=&amp; \int_\Omega f(\underline{x} | \theta) \xi(\theta) d\theta
\end{eqnarray*}\]</span></p>
<p>Remember, however, that we are interested in the probability of the parameter given the data:
<span class="math display">\[\begin{eqnarray*}
\xi(\theta| \underline{x}) = \frac{f(\underline{x} | \theta) \xi(\theta)}{g_n(\underline{x})} \ \ \ \ \ \ \ \theta \in \Omega
\end{eqnarray*}\]</span></p>
<p>(Which is Bayes’ Theorem!!!)</p>
<ul>
<li>The prior, <span class="math inline">\(\xi(\theta)\)</span> is the relative likelihood of <span class="math inline">\(\theta\)</span> any data have been observed.</li>
<li>The posterior, <span class="math inline">\(\xi(\theta | \underline{x})\)</span> is the relative likelihood of <span class="math inline">\(\theta\)</span> <strong>after</strong> <span class="math inline">\(\underline{X} = \underline{x}\)</span> have been observed.</li>
</ul>
<p>We know that <strong>the posterior is a function of <span class="math inline">\(\theta\)</span></strong>. It’s important to keep in mind what a function is, and that the posterior is <strong>not</strong> a function of the data.</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta | \underline{x}) &amp;\propto&amp; f(\underline{x} | \theta) \xi(\theta)\\
\mbox{posterior} &amp;\propto&amp; \mbox{likelihood} \cdot \mbox{prior}
\end{eqnarray*}\]</span></p>
<p>So, the posterior is proportional to the product of the likelihood and the prior. Note that <span class="math inline">\(g_n(\underline{x})\)</span> does not depend on <span class="math inline">\(\theta\)</span> and is part of the proportionality constant. But, we can always find <span class="math inline">\(g_n(\underline{x})\)</span> because we know that the posterior integrates to 1. (Sometimes <span class="math inline">\(g_n\)</span> is extraordinarily difficult to find.)
<span class="math display">\[\begin{eqnarray*}
\int_\Omega \xi(\theta | \underline{x}) d\theta = 1
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-4" class="example"><strong>Example 2.6  </strong></span>Suppose the true proportion of freethrows that Steph Curry is able to make successfully is unknown. We assume that his freethrows are <strong>distributed</strong> according to a Bernoulli process.</p>
<p><span class="math display">\[\begin{eqnarray*}
X = \left\{ \begin{array}{ll}
    1 &amp; \mbox{Curry makes the shot}\\
    0 &amp; \mbox{Curry misses the shot}\\
    \end{array} \right.
\end{eqnarray*}\]</span></p>
<p>We say,
<span class="math display">\[\begin{eqnarray*}
X &amp;\sim&amp; \mbox{Bernoulli}(\theta)\\
f(\underline{x}|\theta)&amp;=&amp;  \theta ^y (1 - \theta)^{n-y} \ \ \ \ y = \sum_{i=1}^n x_i
\end{eqnarray*}\]</span></p>
<p>
Note: here <span class="math inline">\(\underline{x} = \{x_1, x_2, \ldots, x_n\}\)</span> is a specific ordering of 0s and 1s.</p>
<p>If we have <strong>no prior</strong> information about Curry’s abilities, we put a uniform prior on <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta) = \left\{ \begin{array}{ll}
    1 &amp; 0 \leq \theta \leq 1\\
    0 &amp; \mbox{else}\\
    \end{array} \right.
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta | \underline{x}) \propto \theta^y (1-\theta)^{n-y} I_{[0,1]}(\theta)
\end{eqnarray*}\]</span></p>
<p>is the functional form of the posterior (the likelihood times the prior). We’re trying to estimate <span class="math inline">\(\theta\)</span>, what distribution does <span class="math inline">\(\theta\)</span> have given the data? Recall the Beta distribution (probability review):</p>
<p><span class="math display">\[\begin{eqnarray*}
W &amp;\sim&amp; \mbox{Beta}(\alpha,\beta)\\
f(w) &amp;=&amp; \frac{1}{B(\alpha,\beta)} w^{\alpha - 1} (1-w)^{\beta-1}\\
&amp;=&amp; \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)}w^{\alpha - 1} (1-w)^{\beta-1} \ \ \ \ w \in [0,1]
\end{eqnarray*}\]</span></p>
<p>Using the likelihood and the prior, we are able to find the full posterior distribution through integration:
<span class="math display">\[\begin{eqnarray*}
1 &amp;=&amp; \int_\Omega \xi(\theta | \underline{x}) d\theta \\
&amp;=&amp;  k \cdot \int_\Omega \theta^y (1-\theta)^{n-y} I_{[0,1]}(\theta) d\theta\\
&amp;=&amp; k \cdot \int_0^1 \theta^y (1-\theta)^{n-y} d\theta\\
&amp;=&amp; k \cdot \frac{\Gamma(y+1)\Gamma(n-y+1)}{\Gamma(y+1 + n-y + 1)} \cdot \int_0^1 \frac{\Gamma(y+1 + n-y + 1)}{\Gamma(y+1)\Gamma(n-y+1)} \theta^{y+1-1} (1-\theta)^{n-y+1-1} d\theta\\
&amp;=&amp; k \cdot \frac{\Gamma(y+1)\Gamma(n-y+1)}{\Gamma(n+2)} \cdot 1\\
k &amp;=&amp; \frac{\Gamma(n+2)}{\Gamma(y+1)\Gamma(n-y+1)}\\
g(\underline{x}) &amp;=&amp; \frac{\Gamma(y+1)\Gamma(n-y+1)}{\Gamma(n+2)}\\
\xi(\theta | \underline{x}) &amp;=&amp; \frac{\Gamma(n+2)}{\Gamma(y+1)\Gamma(n-y+1)} \theta^y (1-\theta)^{n-y} I_{[0,1]}(\theta)
\end{eqnarray*}\]</span></p>
<p>Note, however, we didn’t actually need to integrate to find <span class="math inline">\(g(\underline{x})\)</span>. Instead, we needed to note that <span class="math inline">\(\theta\)</span> takes the place of <span class="math inline">\(w\)</span> in the above Beta distribution, and we would automatically know the appropriate constant value.</p>
</div>
<p>What if we do have some information about the prior distribution on <span class="math inline">\(\theta\)</span>? Suppose we believe it is Beta(a,b). (Note: <span class="math inline">\(E[\theta] = \frac{a}{a+b}\)</span>, Var<span class="math inline">\((\theta) = \frac{ab}{(a+b)^2 (a+b+1)}\)</span>, SD$() = $ ).</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta) &amp;\propto&amp; \theta^{a-1} (1-\theta)^{b-1} I_{[0,1]}(\theta)\\
\xi(\theta | \underline{x}) &amp;\propto&amp; \theta^y (1-\theta)^{n-y} \theta^{a-1} (1-\theta)^{b-1} I_{[0,1]}(\theta)\\
&amp;\propto&amp; \theta^{y+a-1} (1-\theta)^{n-y+b-1} I_{[0,1]}(\theta)\\
\theta| \underline{x} &amp;\sim&amp; \mbox{Beta}(y+a, n-y+b)\\
\xi(\theta | \underline{x}) &amp;=&amp; \frac{\Gamma(n+a+b)}{\Gamma(y+a)\Gamma(n-y+b)}\theta^{y+a-1} (1-\theta)^{n-y+b-1}
\end{eqnarray*}\]</span></p>
<ul>
<li>Note: we didn’t need to calculate <span class="math inline">\(g(\underline{x})\)</span>!!!</li>
<li><span class="math inline">\(E[\theta | \underline{x}] = \frac{y+a}{n+a+b}\)</span></li>
<li>Var <span class="math inline">\((\theta | \underline{x}) = \frac{(y+a)(n+a+b)}{(n+a+b)^2 (n+a+b+1)}\)</span>
</li>
<li>SD <span class="math inline">\((\theta | \underline{x}) = \sqrt{\frac{(y+a)(n+a+b)}{(n+a+b)^2 (n+a+b+1)}}\)</span>
</li>
<li>Given your prior and a sample of size <span class="math inline">\(n\)</span>, what is your best guess for Curry’s ability to hit freethrows?</li>
<li>What kind of confidence do you have about that estimate?</li>
</ul>
</div>
<div id="conjugate-prior-distributions" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> Conjugate Prior Distributions<a class="anchor" aria-label="anchor" href="#conjugate-prior-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>A <strong>conjugate prior distribution</strong> is one where the prior distribution is in the same family as the posterior distribution. The Beta distribution is conjugate to the Binomial distribution (note, the <span class="math inline">\(U[0,1]\)</span> distribution is Beta(<span class="math inline">\(\alpha\)</span>=1,<span class="math inline">\(\beta\)</span>=1)).</p>
<div class="example">
<p><span id="exm:unnamed-chunk-5" class="example"><strong>Example 2.7  </strong></span>Continuing the example on temperature, <span class="math inline">\(X \sim N(\theta, \sigma^2\)</span> (known)) with a normal prior, <span class="math inline">\(\theta \sim N(\mu, \nu^2)\)</span>. Remember that typically, a prior is completely specified. In our example <span class="math inline">\(\mu=78\)</span> and <span class="math inline">\(\nu=2.5\)</span>. We have:
<span class="math display">\[\begin{eqnarray*}
f(\underline{x} | \theta) &amp;\propto&amp; \exp \bigg[ - \frac{1}{2 \sigma^2} \sum_{i=1}^n (x_i - \theta)^2 \bigg]\\
&amp;\propto&amp; \exp \Bigg[ - \frac{1}{2 \sigma^2} \bigg(n (\theta-\overline{x})^2 + \sum_{i=1}^n (x_i - \overline{x})^2 \bigg) \Bigg]\\
&amp;\propto&amp; \exp \bigg[ - \frac{n}{2 \sigma^2} (\theta - \overline{x})^2\bigg]\\
&amp;&amp;\\
\xi(\theta) &amp;\propto&amp; \exp \bigg[ - \frac{1}{2 \nu^2} (\theta - \mu)^2 \bigg]\\
&amp;&amp;\\
\xi(\theta|\underline{x}) &amp;\propto&amp; f(\underline{x} | \theta) \xi (\theta)\\
&amp;\propto&amp; \exp \bigg[ - \frac{n}{2 \sigma^2} (\theta - \overline{x})^2 -\frac{1}{2 \nu^2} (\theta - \mu)^2 \bigg]\\
\mbox{note: } &amp;&amp; \frac{n}{\sigma^2}(\theta - \overline{x})^2 + \frac{1}{\nu^2}(\theta - \mu)^2 = \frac{1}{\nu_1^2}(\theta - \mu_1)^2 + \frac{n}{\sigma^2 + n \nu^2}(\overline{x}-\mu)^2 \ \ \ \mbox{ pg 399}\\
\xi(\theta|\underline{x}) &amp;\propto&amp; \exp \bigg[ - \frac{1}{2 \nu_1^2} (\theta - \mu_1)^2 \bigg]\\
\theta | \underline{x} &amp;\sim&amp; N (\mu_1, \nu_1^2)\\
\mbox{where: } &amp;&amp; \mu_1 = \frac{\sigma^2 \mu + n \nu^2 \overline{x}}{\sigma^2 + n \nu^2} \ \ \ \ \nu_1^2 = \frac{\sigma^2 \nu^2}{\sigma^2 + n \nu^2}
\end{eqnarray*}\]</span></p>
</div>
<p>Remember, when we are computing the posterior for <span class="math inline">\(\theta\)</span>, we can ignore anything that doesn’t depend on <span class="math inline">\(\theta\)</span> (other “known” parameters, data, constants,…)</p>
<div class="inline-table"><table class="table table-sm">
<caption>Some of the conjugate families. Note that to be conjugate, the prior distribution will be of the <em>same</em> family as the posterior distribution.</caption>
<colgroup>
<col width="16%">
<col width="62%">
<col width="22%">
</colgroup>
<thead><tr class="header">
<th>Prior</th>
<th>Likelihood</th>
<th>Posterior</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Beta</td>
<td>Bernoulli, Binomial, Geometric, Negative Binomial</td>
<td>Beta</td>
</tr>
<tr class="even">
<td>Gamma</td>
<td>Poisson, Gamma (with known first parameter), exponential</td>
<td>Gamma</td>
</tr>
<tr class="odd">
<td>Normal</td>
<td>Normal (with known second parameter)</td>
<td>Normal</td>
</tr>
</tbody>
</table></div>
</div>
<div id="improper-priors" class="section level2" number="2.5">
<h2>
<span class="header-section-number">2.5</span> Improper Priors<a class="anchor" aria-label="anchor" href="#improper-priors"><i class="fas fa-link"></i></a>
</h2>
<p>Improper prior distributions are not actually probability functions, yet they lead to posterior distributions that are probability functions (that is, they integrate to 1). Improper priors capture the idea that the data are worth more than the prior belief. Often, an improper prior will lead to a Frequentist result. For example, a Beta(0,0) prior with a Bernoulli likelihood leads to:</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta) = \left\{ \begin{array}{ll}
    \theta^{-1}(1-\theta)^{-1} &amp; 0 \leq \theta \leq 1\\
    0 &amp; \mbox{else}\\
    \end{array} \right.
\end{eqnarray*}\]</span></p>
<p>Where <span class="math inline">\(\xi(\theta)\)</span> does not integrate to 1, so is not a proper pdf. However the posterior is a proper pdf,</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta | \underline{x}) &amp;\propto&amp; \theta^y (1-\theta)^{n-y} \theta^{-1} (1-\theta)^{-1} I_{[0,1]}(\theta)\\
&amp;\propto&amp; \theta^{y-1} (1-\theta)^{n-y-1} I_{[0,1]}(\theta)\\
\theta| \underline{x} &amp;\sim&amp; \mbox{Beta}(y, n-y)\\
\xi(\theta | \underline{x}) &amp;=&amp; \frac{\Gamma(n)}{\Gamma(y)\Gamma(n-y)}\theta^{y-1} (1-\theta)^{n-y-1}\\
\mbox{Note: } &amp;&amp; E[\theta | \underline{X} ] = \frac{y}{n} \ \ \ \mbox{ as expected under the frequentist model!}
\end{eqnarray*}\]</span></p>
<p>All conjugate priors have an improper prior as a limiting case: Beta(0,0), gamma(0,0), Normal(<span class="math inline">\(\mu, \nu^2 = \infty\)</span>). The normal improper prior ignores the prior constant, and becomes:</p>
<p><span class="math display">\[\begin{eqnarray*}
\xi(\theta) = \lim_{\nu^2 \rightarrow \infty} exp\bigg(-\frac{1}{2\nu^2}(\theta - \mu)^2\bigg) = 1
\end{eqnarray*}\]</span></p>
<p>That is, the improper normal prior is a flat line over all the reals. Note that if the improper normal prior is used with the normal likelihood, the posterior will be <span class="math inline">\(\theta | \underline{x} \sim N (\underline{x}, \sigma^2 /n)\)</span>. [That is, a prior indicating no knowledge of <span class="math inline">\(\theta\)</span> produces a posterior that depends only on the data.]</p>
</div>
<div id="bayes-estimators" class="section level2" number="2.6">
<h2>
<span class="header-section-number">2.6</span> Bayes’ Estimators<a class="anchor" aria-label="anchor" href="#bayes-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>Prior and posterior distributions tell us how Bayesians think about parameters. The next question we need to address is how do they think about estimators? An <strong>estimator</strong> is a function of the data that we hope is close to the true value of the parameter.</p>
<p>Note:
<span class="math display">\[\begin{eqnarray*}
\delta(X_1, X_2, \ldots, X_n) &amp;=&amp; \delta(\underline{X}) \mbox{  is our estimator}\\
\delta(x_1, x_2, \ldots, x_n) &amp;=&amp; \delta(\underline{x}) \mbox{  is our estimate}\\
\end{eqnarray*}\]</span></p>
<div id="loss-functions" class="section level3" number="2.6.1">
<h3>
<span class="header-section-number">2.6.1</span> Loss functions<a class="anchor" aria-label="anchor" href="#loss-functions"><i class="fas fa-link"></i></a>
</h3>
<p>(You are not responsible for the material on loss functions. The take away message from this section is that the Bayes estimator we will use is the expected value of the posterior distribution. However, as seen below, there are other Bayes estimators, for example, the median of the posterior distribution could be used.)</p>
<p>We want an estimator of <span class="math inline">\(\theta\)</span> that leads to an estimate which is close to the <strong>true</strong> value of <span class="math inline">\(\theta\)</span>. A <strong>loss function</strong> helps determine how far off an estimator is. For a particular estimate, <span class="math inline">\(a\)</span>:
<span class="math display">\[\begin{eqnarray*}
\mbox{squared error loss: } L(\theta, a) &amp;=&amp; (\theta - a)^2\\
\mbox{absolute error loss: } L(\theta, a) &amp;=&amp; |\theta - a|\\
\end{eqnarray*}\]</span>
We want the loss to be small (minimized).</p>
<div id="squared-error-loss" class="section level4 unnumbered">
<h4>Squared Error Loss<a class="anchor" aria-label="anchor" href="#squared-error-loss"><i class="fas fa-link"></i></a>
</h4>
<p><strong>Without data</strong>, find <span class="math inline">\(a\)</span> that minimizes:
<span class="math display">\[\begin{eqnarray*}
E[L(\theta,a)] = \int_\Omega L(\theta, a) \xi(\theta) d\theta
\end{eqnarray*}\]</span></p>
<p><strong>With data</strong>, find <span class="math inline">\(a\)</span> that minimizes:
<span class="math display">\[\begin{eqnarray*}
E[L(\theta,a) | \underline{X}] &amp;=&amp; \int_\Omega L(\theta, a) \xi(\theta| \underline{X}) d\theta\\
\end{eqnarray*}\]</span>
Let <span class="math inline">\(\delta^*(\underline{X})\)</span> be the value of a such that such that
<span class="math display">\[\begin{eqnarray*}
E[L(\theta, \delta^*(\underline{X})) | \underline{X} ] &amp;=&amp; \min_{a \in \Omega} E[ L(\theta,a) | \underline{X} ]\\
\delta^*(\underline{X}) &amp;&amp; \mbox{ is the Bayes estimator of } \theta\\
\delta^*(\underline{x}) &amp;&amp; \mbox{ is the Bayes estimate of } \theta\\
\end{eqnarray*}\]</span>
So what is <span class="math inline">\(\delta^*\)</span>?</p>
<p>Note: <span class="math inline">\(\delta^*\)</span> depends on the loss function and the prior / posterior.</p>
<p><span class="math display">\[\begin{eqnarray*}
E[L(\theta, a) | \underline{X} ] &amp;=&amp; E[ (\theta - a)^2 | \underline{X}]\\
&amp;=&amp; E[ \theta^2 - 2a\theta + a^2 | \underline{X}]\\
&amp;=&amp; E[\theta^2 | \underline{X}] - 2a E[\theta | \underline{X}] + a^2\\
&amp;&amp;\\
\frac{\partial E[ (\theta - a)^2 | \underline{X}]}{\partial a} &amp;=&amp; 0\\
&amp;&amp;\\
- 2 E[\theta | \underline{X}] + 2a &amp;=&amp; 0\\
a &amp;=&amp; E[\theta | \underline{X}] = \delta^*(\underline{X}) !!!\\
&amp;&amp;\\
\frac{\partial^2 E[ (\theta - a)^2 | \underline{X}]}{\partial a^2} &amp;=&amp; 2 &gt; 0 \rightarrow \mbox{ loss is minimized}\\
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:tape" class="example"><strong>Example 2.8  </strong></span>Let <span class="math inline">\(\theta\)</span> denote the average number of defects per 100 feet of tape. <span class="math inline">\(\theta\)</span> is unknown, but the prior on <span class="math inline">\(\theta\)</span> is a gamma distribution with <span class="math inline">\(E[\theta] = \alpha / \beta = 2/10, \alpha= 2, \beta = 10\)</span>. When a 1200 foot roll of tape is inspected, exactly 4 defects are found.</p>
<p>What is the Bayes’ estimate of the average number of defects per 100 feet?</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{Prior:     }&amp;&amp;\\
\xi(\theta) &amp;=&amp; \frac{\beta^\alpha}{\Gamma(\alpha)} \theta^{\alpha-1} e^{-\beta \theta} = \frac{10^2}{\Gamma(2)} \theta e^{-10\theta}\\
\mbox{Likelihood:     }&amp;&amp;\\
f(\underline{x} | \theta) &amp;=&amp; \prod_{i=1}^n \frac{e^{-\theta} \theta^{x_i}}{x_i!} = \frac{e^{-n\theta} \theta^{\sum x_i}}{\prod (x_i !)}\\
\mbox{Posterior:     }&amp;&amp;\\
\xi(\theta | \underline{x}) &amp;\propto&amp; \frac{ \theta e^{-10\theta} e^{-n \theta} \theta^{\sum x_i}}{\Gamma(2) 10^2 \prod (x_i !)}\\
&amp;\propto&amp; e^{-\theta(n+10)} \theta ^{\sum x_i + 1} \\
\\
\theta | \underline{x} &amp;\sim&amp; \mbox{Gamma } (\sum x_i + 2 = 6, n + 10 = 22)\\
\\
\delta^*(\underline{X}) &amp;=&amp; \frac{ \sum X_i + 2}{n+10}\\
\delta^*(\underline{x}) &amp;=&amp; \frac{6}{22} = \frac{3}{11}
\end{eqnarray*}\]</span></p>
<p>
Note: The Gamma distribution is parameterized slightly differently in DeGroot and on your sheet (as is the exponential). Make sure the expected value matches what you’ve been given in the problem.</p>
</div>
</div>
<div id="absolute-loss" class="section level4 unnumbered">
<h4>Absolute Loss<a class="anchor" aria-label="anchor" href="#absolute-loss"><i class="fas fa-link"></i></a>
</h4>
<p>How do we minimize <span class="math inline">\(E[ | \theta - a | | \underline{X}]\)</span> ? <span class="math inline">\(\rightarrow \ \ \ \delta^*(\underline{X}) = \mbox{median} (\theta | \underline{X})\)</span> (see Theorem 4.5.1 in <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>). However, it isn’t always obvious how to compute the median for non-symmetric distributions. And for symmetric distributions median = mean.</p>
</div>
</div>
</div>
<div id="evaluating-bayes-estimators" class="section level2" number="2.7">
<h2>
<span class="header-section-number">2.7</span> Evaluating Bayes Estimators<a class="anchor" aria-label="anchor" href="#evaluating-bayes-estimators"><i class="fas fa-link"></i></a>
</h2>
<div id="mean-squared-error" class="section level3" number="2.7.1">
<h3>
<span class="header-section-number">2.7.1</span> Mean Squared Error<a class="anchor" aria-label="anchor" href="#mean-squared-error"><i class="fas fa-link"></i></a>
</h3>
<p>Before we get to MSE, let’s talk about the Agresti-Coull estimate of the Binomial parameter of the probability of success. The estimator attenuates the sample proportion closer to 0.5 which has the effect of reducing the variability of the estimator. Indeed, there is evidence that the “add two successes and two failures” approach will create more precise confidence intervals.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Agresti, Alan; Coull, Brent A. (1998).”Approximate is better than ‘exact’ for interval estimation of binomial proportions”. The American Statistician. 52: 119-126.&lt;/p&gt;"><sup>2</sup></a></p>
<p>Even though there is some agreement that it is a better estimate, we’d like to show here that it is biased.</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{p} &amp;=&amp; \frac{X+2}{n+4}\\
E[\tilde{p}] &amp;=&amp; \frac{n\theta+2}{n+4}\\
bias(\tilde{p}) &amp;=&amp; \frac{n\theta+2}{n+4} - \theta\\
&amp;=&amp; \frac{n\theta + 2 - n\theta - 4\theta}{n+4}\\
&amp;=&amp; \frac{2-4\theta}{n+4}
\end{eqnarray*}\]</span></p>
<div id="mse-in-general-frequentist" class="section level4 unnumbered">
<h4>MSE in general (“Frequentist”)<a class="anchor" aria-label="anchor" href="#mse-in-general-frequentist"><i class="fas fa-link"></i></a>
</h4>
<p>The Mean Squared Error (MSE) is the expected squared difference between a parameter (<span class="math inline">\(\theta\)</span>) and its estimate (<span class="math inline">\(\hat{\theta}\)</span>), where, typically, <span class="math inline">\(\hat{\theta}\)</span> is a function of the data.</p>
<blockquote>
<p>The frequentist MSE is based on expected values taken with the likelihood pdf (<span class="math inline">\(X | \theta\)</span>).</p>
</blockquote>
<p>Note that the MSE can be written as a sum of the bias squared and the variance:
<span class="math display">\[\begin{eqnarray*}
(\mbox{MSE}_F(\hat{\theta}(\underline{X})) =) \mbox{MSE}_F(\hat{\theta}) &amp;=&amp; E [ (\hat{\theta} - \theta)^2 ]\\
&amp;=&amp; E [ ( \hat{\theta} - E(\hat{\theta}) + E(\hat{\theta}) - \theta)^2 ] \mbox{ the RV is X!} \\
&amp;=&amp; E [ (\hat{\theta} -  E(\hat{\theta}))^2 + 2(\hat{\theta} - E(\hat{\theta}))(E(\hat{\theta}) - \theta) + (E(\hat{\theta}) - \theta)^2]\\
&amp;=&amp; E [ (\hat{\theta} -  E(\hat{\theta}))^2] + 0 + E[ (E(\hat{\theta}) - \theta)^2 ] \\
&amp;=&amp; E [ (\hat{\theta} -  E(\hat{\theta}))^2] + (E(\hat{\theta}) - \theta)^2  \\
&amp;=&amp; \mbox{var}(\hat{\theta}) + (\mbox{bias}(\hat{\theta}))^2
\end{eqnarray*}\]</span></p>
<p>Note that we are taking expected values, so the resulting MSE is a function of <span class="math inline">\(\theta\)</span> only (not a function of the data).</p>
<p>For example, consider the Bernoulli situation (e.g., the Basketball shooter):
<span class="math display">\[\begin{eqnarray*}
\hat{\theta} &amp;=&amp; \frac{\sum X_i}{n}\\
Var(\hat{\theta}) &amp;=&amp; \frac{\theta}{n}\\
bias(\hat{\theta}) &amp;=&amp; E(\hat{\theta}) - \theta = 0\\
MSE_F(\hat{\theta}) &amp;=&amp; \frac{\theta}{n}\\
\end{eqnarray*}\]</span></p>
</div>
<div id="bayesian-mse" class="section level4 unnumbered">
<h4>Bayesian MSE<a class="anchor" aria-label="anchor" href="#bayesian-mse"><i class="fas fa-link"></i></a>
</h4>
<p>For Bayesians, the MSE is the expected squared error loss conditional on the data (that is, the expected value is taken on the posterior, <span class="math inline">\(\theta | \underline{X}\)</span>.)</p>
<blockquote>
<p>The Bayesian MSE is based on expected values taken with the posterior pdf <span class="math inline">\((\theta | \underline{X}).\)</span></p>
</blockquote>
<p>If we let <span class="math inline">\(\delta = \delta(\underline{X}) = E(\theta | \underline{X})\)</span> be our estimator, the MSE is:
<span class="math display">\[\begin{eqnarray*}
(\mbox{MSE}_B(\hat{\theta}(\underline{X})) =) \mbox{MSE}_B(\delta(\underline{X})) &amp;=&amp; E [ (\delta - \theta)^2 | \underline{X}]\\
&amp;=&amp; E [ ( E(\theta | \underline{X}) - \theta)^2 | \underline{X}] \mbox{the RV is } \theta \mbox{!!}\\
&amp;=&amp; \mbox{var}(\theta | \underline{X})\\
\end{eqnarray*}\]</span></p>
<p>Continuing the example, note that if <span class="math inline">\(\theta \sim Beta(a,b)\)</span>, that means <span class="math inline">\(\theta | \underline{X} \sim Beta(X + a, n - X + b)\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\delta(X) &amp;=&amp; \frac{X + a}{ n - X + b}\\
MSE_B(\delta(X)) &amp;=&amp; \frac{(X+a)(n+a+b)}{(n+a+b)^2(n+a+b+1)}\\
\end{eqnarray*}\]</span></p>
<p>Note that the Bayesian MSE is the posterior variance of the parameter of interest. This is because we’ve used the expected value as our estimate, so there is no bias. Note that the Bayesian MSE is a function of the data (and <strong>not</strong> <span class="math inline">\(\theta\)</span>), so we cannot compare the Bayesian MSE and the Frequentist MSE directly.</p>
<div class="example">
<p><span id="exm:unnamed-chunk-6" class="example"><strong>Example 2.9  </strong></span>Recall the tape example, Example <a href="bayes.html#exm:tape">2.8</a>.</p>
<ul>
<li>Prior: Gamma(2, 10) (or (2, 1/10) depending on how you parametrize))</li>
<li>Data likelihood: Poisson(<span class="math inline">\(\theta\)</span>)</li>
<li>Posterior: Gamma (<span class="math inline">\(\sum X_i + 2\)</span>, <span class="math inline">\(n\)</span> + 10)</li>
</ul>
<p>Note that the calculations done for the Frequentist MSE are <strong>not</strong> conditional on the data.</p>
<p><span class="math display">\[\begin{eqnarray*}
(\mbox{frequentist estimator}) \ \ \hat{\theta} &amp;=&amp; \frac{\sum X_i}{n}\\
(\mbox{Bayesian estimator}) \ \ \delta(\underline{X}) &amp;=&amp; \frac{\sum X_i+2}{n+10}\\
&amp; \\
MSE_F(\hat{\theta}) &amp;=&amp; var(\hat{\theta}) + bias(\hat{\theta})^2\\
&amp;=&amp; \theta/n + 0 = \theta/n\\
MSE_B(\delta(\underline{X})) &amp;= &amp; var(\theta | \underline{X})\\
&amp;=&amp; \frac{\sum X_i+2}{(n+10)^2}\\
&amp;&amp;\\
MSE_F(\delta(\underline{X})) &amp;=&amp; var(\delta(\underline{X})) + bias(\delta(\underline{X}))^2\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
bias(\delta(\underline{X})) &amp;=&amp; E\bigg[\frac{\sum X_i+2}{n+10}\bigg] - \theta\\
&amp;=&amp; \frac{n \theta +2}{n+10} - \theta = \frac{n\theta + 2 - n\theta -10\theta}{n+10}\\
&amp;=&amp; \frac{2-n\theta}{n+10}\\
var(\delta(\underline{X})) &amp;=&amp; var\bigg[\frac{\sum X_i+2}{n+10}\bigg]\\
&amp;=&amp; \frac{1}{(n+10)^2}var\bigg(\sum X_i\bigg)\\
&amp;=&amp; \frac{1}{(n+10)^2} n \  var(X_i)\\
&amp;=&amp; \frac{n}{(n+10)^2} \theta\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{align*}
MSE_F(\delta(\underline{X})) &amp;= \frac{n}{(n+10)^2} \theta + \frac{(2-n\theta)^2}{(n+10)^2}\\
&amp;= \frac{n \theta+ (2-n\theta)^2}{(n+10)^2}
\end{align*}\]</span>
Note that we couldn’t directly compare <span class="math inline">\(MSE_F\)</span> and <span class="math inline">\(MSE_B\)</span> (they are functions of different variables!). Because we’d have to come up with a prior to think about <span class="math inline">\(MSE_B(\hat{\theta})\)</span>, it seems like we can’t calculate that quantity. Instead, we take the easier route, and find <span class="math inline">\(MSE_F(\delta(\underline{X}))\)</span> in order to have a reasonable comparison of estimators.</p>
</div>
</div>
</div>
<div id="sensitivity-of-estimators" class="section level3" number="2.7.2">
<h3>
<span class="header-section-number">2.7.2</span> Sensitivity of Estimators<a class="anchor" aria-label="anchor" href="#sensitivity-of-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>How sensitive are our results to different priors?</p>
<div class="example">
<p><span id="exm:unnamed-chunk-7" class="example"><strong>Example 2.10  </strong></span>Continuing with the tape example, Example <a href="bayes.html#exm:tape">2.8</a>, below are different values for the estimate of theta depending on different priors and data values:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="17%">
<col width="27%">
<col width="27%">
<col width="27%">
</colgroup>
<thead><tr class="header">
<th align="right"></th>
<th align="center"><span class="math inline">\(\alpha = 2, \beta = 10\)</span></th>
<th align="center"><span class="math inline">\(\alpha = 8, \beta = 10\)</span></th>
<th align="center"><span class="math inline">\(\alpha = 2, \beta = 20\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right"><span class="math inline">\(\hat{\theta}\)</span></td>
<td align="center"><span class="math inline">\(\alpha / \beta = 0.2\)</span></td>
<td align="center"><span class="math inline">\(\alpha / \beta = 0.8\)</span></td>
<td align="center"><span class="math inline">\(\alpha / \beta = 0.1\)</span></td>
</tr>
<tr class="even">
<td align="right">4 in 1200 ft</td>
<td align="center">0.273</td>
<td align="center">0.545</td>
<td align="center">0.188</td>
</tr>
<tr class="odd">
<td align="right">40 in 12,000 ft</td>
<td align="center">0.323</td>
<td align="center">0.368</td>
<td align="center">0.300</td>
</tr>
</tbody>
</table></div>
<p>Note: <span class="math inline">\(E[ \theta | \underline{x} ] = \frac{\sum x_i + \alpha}{ n + \beta} = w_1 \frac{\sum x_i}{n} + w_2 \frac{\alpha}{\beta}\)</span>, where <span class="math inline">\(w_1 = \frac{n}{n+\beta}, w_2 = \frac{\beta}{n+\beta}\)</span>.\
</p>
<p>As <span class="math inline">\(n \rightarrow \infty, \hat{\theta} \rightarrow \frac{\sum x_i}{n}\)</span>, as <span class="math inline">\(n \rightarrow 0, \hat{\theta} \rightarrow \frac{\alpha}{\beta}\)</span>.</p>
</div>
</div>
<div id="consistency-of-estimators" class="section level3" number="2.7.3">
<h3>
<span class="header-section-number">2.7.3</span> Consistency of Estimators<a class="anchor" aria-label="anchor" href="#consistency-of-estimators"><i class="fas fa-link"></i></a>
</h3>
<p>A <strong>consistent</strong> estimator of <span class="math inline">\(\theta\)</span> is one that converges in probability to <span class="math inline">\(\theta\)</span>. Many of the Bayes estimators are consistent. In fact, under fairly general regularity conditions, a wide class of Bayes estimators are consistent.</p>
<p>Note, an estimator <span class="math inline">\(Y_n\)</span> converges to <span class="math inline">\(\theta\)</span> in probability if:
<span class="math display">\[\begin{eqnarray*}
\lim_{n \rightarrow \infty} P [ | Y_n - \theta | &lt; \epsilon ] &amp;=&amp; 1 \ \ \ \ \ \mbox{pg 233}\\
\mbox{or, equivalently}\\
\lim_{n \rightarrow \infty} P [ | Y_n - \theta | \geq \epsilon ] &amp;=&amp; 0\\
\end{eqnarray*}\]</span></p>
<p>(You saw this idea in the weak and strong laws of large numbers: <span class="math inline">\(\overline{X} \stackrel{P}{\rightarrow} \mu\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> is the Weak Law of Large Numbers.) [n.b. In case you are curious, the strong law of large numbers says that <span class="math inline">\(\overline{X} \stackrel{a.s.}{\rightarrow} \mu\)</span> (almost surely). That means <span class="math inline">\(\lim_{n \rightarrow \infty} P [ \overline{X} = \mu ] = 1\)</span></p>
<div class="example">
<p><span id="exm:unnamed-chunk-8" class="example"><strong>Example 2.11  </strong></span>Continuing with the tape example, Example <a href="bayes.html#exm:tape">2.8</a>:
<span class="math display">\[\begin{eqnarray*}
\delta^*(\underline{X}) &amp;=&amp; \frac{\sum X_i + \alpha}{n+\beta}\\
\overline{X} &amp;\stackrel{P}{\rightarrow}&amp; \theta \mbox{ (WLLN)}\\
\mbox{and } &amp;&amp; \\
\delta^*(\underline{X}) - \overline{X} &amp;=&amp; \frac{- \beta}{n+\beta} \overline{X} + \frac{\beta}{n+\beta} \frac{\alpha}{\beta} \stackrel{P}{\rightarrow} 0 \mbox{ (Slutsky's theorem)}\\
&amp;&amp; \\
\delta^*(\underline{X}) &amp;\stackrel{P}{\rightarrow} \theta\\
\end{eqnarray*}\]</span></p>
<p><span class="math inline">\(\delta^*(\underline{X})\)</span> is a consistent estimator of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
</div>
<div id="benefits-and-limitations-of-bayes-estimators" class="section level2" number="2.8">
<h2>
<span class="header-section-number">2.8</span> Benefits and Limitations of Bayes’ Estimators<a class="anchor" aria-label="anchor" href="#benefits-and-limitations-of-bayes-estimators"><i class="fas fa-link"></i></a>
</h2>
<div id="benefits" class="section level3" number="2.8.1">
<h3>
<span class="header-section-number">2.8.1</span> Benefits<a class="anchor" aria-label="anchor" href="#benefits"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>you can incorporate other information</li>
<li>the interpretation is more intuitive</li>
</ul>
</div>
<div id="limitations" class="section level3" number="2.8.2">
<h3>
<span class="header-section-number">2.8.2</span> Limitations<a class="anchor" aria-label="anchor" href="#limitations"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>you need prior information</li>
<li>it can be difficult to produce a prior on two parameters simultaneously (e.g., normal, gamma)</li>
<li>you need to agree on the prior information</li>
</ul>
</div>
</div>
<div id="additional-examples" class="section level2" number="2.9">
<h2>
<span class="header-section-number">2.9</span> Additional Examples<a class="anchor" aria-label="anchor" href="#additional-examples"><i class="fas fa-link"></i></a>
</h2>
<div class="example">
<p><span id="exm:unnamed-chunk-9" class="example"><strong>Example 2.12  </strong></span>Suppose there is a Beta(4,4) prior distribution on the probability <span class="math inline">\(\theta\)</span> that a coin will yield a <code>head</code> when spun in a specified manner. The coin is independently spun ten times, and <code>heads</code> appears fewer than 3 times. You are not told how many heads were seen, only that the number is less than 3. Calculate your exact posterior density for <span class="math inline">\(\theta\)</span>.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Problem taken from &lt;em&gt;Bayesian Data Analysis&lt;/em&gt; by Gelman, Carlin, Stern, and Rubin.&lt;/p&gt;"><sup>3</sup></a></p>
</div>
<div class="example">
<p><span id="exm:baseball" class="example"><strong>Example 2.13  </strong></span><strong>Baseball and Bayes</strong><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Example taken from &lt;em&gt;The Practice of Statistics: Putting the Pieces Together&lt;/em&gt; by Spurrier.&lt;/p&gt;"><sup>4</sup></a></p>
<p>You are a statistician employed by On The Ball Consulting. Veteran major-league baseball scout Rocky Chew seeks your advice regarding estimating the probability that amateur baseball player John Spurrier will get a base hit against a major-league pitcher. Rocky has arranged for Spurrier to have ten at bats against a major-league pitcher.</p>
<p>The traditional batting average, <span class="math inline">\(\hat{\theta}_f = X/n\)</span> is a frequentist estimator in that it makes use of the observed data, but ignores any prior information that might exist.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Some of you baseball enthusiasts will be a bit uncomfortable that we’re going to assume that our denominator is # of times up to bat.&lt;/p&gt;"><sup>5</sup></a> If we assume that the at bats are independent Bernoulli trials with a constant probability of getting a base hit, then</p>
<p><span class="math display">\[\begin{eqnarray*}
X \sim Bin( n=\mbox{number at bat}, \theta=\mbox{P(getting a base hit)})
\end{eqnarray*}\]</span></p>
<p><span class="math inline">\(\hat{\theta}_f\)</span>, is a good estimator of the unknown probability (of getting a base hit), but it ignores information we might have about baseball. You have the following prior information:</p>
<ul>
<li>John Spurrier appears to be a good but not great player. He is one of the better batters on a somewhat above-average American Legion (high school) baseball team.</li>
<li>The few major-league scouts who have watched him play do not believe that Spurrier’s batting ability is at the professional level.</li>
<li>A barely adequate major-league hitter has a batting average of about 0.200.</li>
<li>A very good major-league batter has a batting average of about 0.300.</li>
<li>Ty Cobb has the all-time best major-league batting average of 0.366.</li>
</ul>
<p>We’re going to use a Beta prior to incorporate our previous knowledge. What should that prior look like?</p>
<div id="the-experiment" class="section level4 unnumbered">
<h4>The Experiment<a class="anchor" aria-label="anchor" href="#the-experiment"><i class="fas fa-link"></i></a>
</h4>
<ol style="list-style-type: decimal">
<li>John Spurrier will have n=10 at bats. The random variable, <span class="math inline">\(X\)</span>, will be the number of base hits that he gets.</li>
<li>Determining the prior probability: As a class we will find <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> that are consistent with our prior information.</li>
<li>Collecting data: let’s calculate our estimates for all possible realizations of the random variable.</li>
</ol>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="8%">
<col width="23%">
<col width="65%">
<col width="3%">
</colgroup>
<thead><tr class="header">
<th><span class="math inline">\(x\)</span></th>
<th><span class="math inline">\(\hat{\theta}_f\)</span></th>
<th><span class="math inline">\(\ \ \ \ \ \ \ \ \ \hat{\theta}_b \ \ \ \ \ \ \ \ \ \)</span></th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0.00</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1</td>
<td>0.10</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>0.20</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>0.30</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>4</td>
<td>0.40</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>5</td>
<td>0.50</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>6</td>
<td>0.60</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>7</td>
<td>0.70</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>8</td>
<td>0.80</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>9</td>
<td>0.90</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>10</td>
<td>1.00</td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<ol start="4" style="list-style-type: decimal">
<li><p>Comparison of the estimators:
<span class="math display">\[ \ \ \ \ \hat{\theta}_f = \frac{x}{n} \ \ \ \ \ \ \ \hat{\theta}_b = \frac{x + \alpha}{ n + \alpha + \beta}\]</span>
To evaluate the two estimators, we might use Mean Squared Error (MSE) in the frequentist sense (that is, <span class="math inline">\(X\)</span> is the random variable, <span class="math inline">\(\theta\)</span> is no longer random) to compare estimators (apples to apples):
<span class="math display">\[\begin{eqnarray*}
MSE(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = Var(\hat{\theta}) + bias^2(\hat{\theta}) = Var(\hat{\theta}) + [E(\hat{\theta}) - \theta]^2
\end{eqnarray*}\]</span></p></li>
<li><p>Problems:</p></li>
</ol>
<ul>
<li>What are your choices of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>? What features of the plot of the prior density function made you think these were good choices?</li>
<li>Use properties of expectation of X, to find the bias (=<span class="math inline">\(E[\hat{\theta}] - \theta\)</span>) and variance (=Var(<span class="math inline">\(\hat{\theta}\)</span>)) of <span class="math inline">\(\hat{\theta}_f\)</span> and <span class="math inline">\(\hat{\theta}_b\)</span>.</li>
<li>Do you recommend using <span class="math inline">\(\hat{\theta}_f\)</span> or <span class="math inline">\(\hat{\theta}_b\)</span>? Explain.</li>
<li>If John Spurrier gets three hits in ten at bats, what is your estimate of <span class="math inline">\(\theta\)</span>?</li>
<li>Show that <span class="math inline">\(\hat{\theta}_b\)</span> is a weighted average of <span class="math inline">\(\hat{\theta}_f\)</span> and the prior mean, <span class="math inline">\(\frac{\alpha}{\alpha + \beta}\)</span>.</li>
</ul>
</div>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 2.14  </strong></span><strong>Kidney Cancer rates</strong><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Example taken from &lt;em&gt;Teaching Statistics, a bag of tricks&lt;/em&gt; by Gelman and Nolan.&lt;/p&gt;"><sup>6</sup></a> In this example, we’re going to use Bayes theory to adjust kidney cancer rates for less variability. First, we’d like to investigate the counties with the highest and lowest kidney cancer death rates in the US (white men, 1980-1989).</p>
<ul>
<li>What patterns do you see in figures 2.3 &amp; 2.4? Can you give plausible reasons for the patterns you see?</li>
<li>What if a county only had 100 people? Small counties are more variable. Keep in mind that the rates are age-adjusted.</li>
</ul>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-10"></span>
<img src="figs/highkidney.jpeg" alt="The caption reads: the counties of the United States with the highest 10% age-standardized death rates for cancer of kidney/ureter for U.S. white males, 1980-1989." width="75%" style="transform:rotate(270deg);"><p class="caption">
Figure 1.6: Figure 2.3 from Teaching Statistics, a bag of tricks by Gelman and Nolan.
</p>
</div>
<ul>
<li>Consider figure 13.4, the highest 10% of Bayes-estimated kidney cancer death rates in the US (white men, 1980-1989). Let’s assume that number of deaths is distributed Poisson(<span class="math inline">\(n_j \theta_j\)</span>) where <span class="math inline">\(n_j\)</span> is the number of people in the county, and <span class="math inline">\(\theta_j\)</span> is the true kidney cancer death rate in that county. Further, we will assume that there are no outside influences on kidney cancer (e.g., pollution) and that each county’s cancer rate comes from a Gamma distribution with parameters (<span class="math inline">\(\alpha = 61, \beta = 47000\)</span>). That is,</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{Likelihood:} &amp;&amp; y_j \sim \mbox{ Poisson}(n_j \theta_j) \ \ \ n_j = \mbox{ county population}\\
\mbox{Prior:} &amp;&amp; \theta_j \sim \mbox{ Gamma}(\alpha=61, \beta = 47000)\\
&amp;&amp; E[\theta_j] = \alpha / \beta = 1.296 \times 10^{-3} \ \ \ \ (\mbox{10 yr cancer rate})\\
\end{eqnarray*}\]</span>
We know that <span class="math inline">\(E[\theta | y] = \frac{\alpha + y}{m + \beta}\)</span>. How does this estimate compare to the frequentist estimate, <span class="math inline">\(\hat{\theta} = \frac{y}{m}\)</span>?</p>
<ul>
<li>To investigate the relationship between the Bayes estimate versus the frequentist estimate, we’re going to simulate kidney cancer death rates in a variety of counties.</li>
<li>Everyone gets a county (and population). For each county, we’ll generate the <em>true</em>, underlying, kidney cancer rate <span class="math inline">\(\theta_j\)</span>. (Note, <span class="math inline">\(\theta_j\)</span> was sampled (or <em>simulated</em>) from a Gamma(<span class="math inline">\(\alpha=61, \beta=47,000\)</span>) distribution.)</li>
<li>Using your cancer rate (<span class="math inline">\(\theta_j\)</span>) and your county’s population (<span class="math inline">\(n_j\)</span>), simulate a value for the number of people in your county who have died from kidney cancer in the last 10 years (Poisson(<span class="math inline">\(n_j \theta_j\)</span>)).</li>
<li>Report the frequentist estimate of the kidney cancer rate for your county (erase / hide the true cancer rate). Leave only the county name, the population, the number of deaths, and the estimated rate.</li>
<li>Our public officials are left with the task of guessing which counties have the highest cancer rates… what do you think?</li>
<li>Calculate the Bayes estimate of the cancer rate. Compare the underlying (<span class="math inline">\(\theta_j\)</span>), observed / frequentist (<span class="math inline">\(\frac{y_j}{n_j}\)</span>), and posterior / Bayesian (<span class="math inline">\(\hat{\theta}_j | y_j\)</span>) kidney cancer death rates.</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th>County</th>
<th align="right">2007 population</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Charleston County, SC</td>
<td align="right">342,973</td>
</tr>
<tr class="even">
<td>Divide County, ND</td>
<td align="right">2,004</td>
</tr>
<tr class="odd">
<td>Durham County, NC</td>
<td align="right">256,500</td>
</tr>
<tr class="even">
<td>Esmeralda County, NV</td>
<td align="right">695</td>
</tr>
<tr class="odd">
<td>Fayette County, KY</td>
<td align="right">279,044</td>
</tr>
<tr class="even">
<td>Franklin County, OH</td>
<td align="right">1,118,107</td>
</tr>
<tr class="odd">
<td>Hardin County, IL</td>
<td align="right">4,468</td>
</tr>
<tr class="even">
<td>King and Queen County, VA</td>
<td align="right">6,882</td>
</tr>
<tr class="odd">
<td>Lafayette County, MS</td>
<td align="right">42,716</td>
</tr>
<tr class="even">
<td>Lehigh County, PA</td>
<td align="right">337,343</td>
</tr>
<tr class="odd">
<td>Los Angeles County, CA</td>
<td align="right">9,878,554</td>
</tr>
<tr class="even">
<td>Loup County, NE</td>
<td align="right">644</td>
</tr>
<tr class="odd">
<td>Loving County, TX</td>
<td align="right">55</td>
</tr>
<tr class="even">
<td>Major County, OK</td>
<td align="right">7,190</td>
</tr>
<tr class="odd">
<td>Mercer County, NJ</td>
<td align="right">365,449</td>
</tr>
<tr class="even">
<td>New York County, NY</td>
<td align="right">1,620,867</td>
</tr>
<tr class="odd">
<td>Petroleum County, MT</td>
<td align="right">438</td>
</tr>
<tr class="even">
<td>Pima County, AZ</td>
<td align="right">967,089</td>
</tr>
<tr class="odd">
<td>Prince George’s County, MD</td>
<td align="right">828,770</td>
</tr>
<tr class="even">
<td>Putnam County, MO</td>
<td align="right">4,913</td>
</tr>
<tr class="odd">
<td>Real County, TX</td>
<td align="right">2,965</td>
</tr>
<tr class="even">
<td>Rosebud County, MT</td>
<td align="right">9,182</td>
</tr>
<tr class="odd">
<td>Sacramento County, CA</td>
<td align="right">1,386,667</td>
</tr>
<tr class="even">
<td>Slope County, ND</td>
<td align="right">659</td>
</tr>
<tr class="odd">
<td>Yellow Medicine County, MN</td>
<td align="right">10,128</td>
</tr>
</tbody>
</table></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-11"></span>
<img src="figs/lowkidney.jpeg" alt="The caption reads: the counties of the United States with the lowest 10% age-standardized death rates for cancer of kidney/ureter for U.S. white males, 1980-1989.  Surprisingly, the pattern is somewhat similar to the map of the highest rates, show in Figure 2.3." width="75%"><p class="caption">
Figure 2.1: Figure 2.4 from Teaching Statistics, a bag of tricks by Gelman and Nolan.
</p>
</div>
</div>
</div>
<div id="reflection-questions-1" class="section level2" number="2.10">
<h2>
<span class="header-section-number">2.10</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-1"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>What is a prior distribution? What is the random variable described by a prior?</li>
<li>What is a likelihood? What is the random variable described by a likelihood?</li>
<li>What is a posterior distribution? What is the random variable described by a posterior?</li>
<li>What is a conjugate prior? What is the benefit of a conjugate prior?</li>
<li>Is all hope lost if the prior is not conjugate? If it is not, how would we approach the problem of coming up with a posterior?</li>
</ol>
</div>
<div id="ethics-considerations-1" class="section level2" number="2.11">
<h2>
<span class="header-section-number">2.11</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-1"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>When does it make sense to incorporate prior information? When doesn’t it make sense to incorporate prior information?</li>
<li>What are some legitimate ways to calculate a prior? What are some illegitimate ways to calculate a prior?</li>
<li>How is an analyst able to come up with a prior or a likelihood?</li>
</ol>
</div>
<div id="r-code-bayesian-example" class="section level2" number="2.12">
<h2>
<span class="header-section-number">2.12</span> R code: Bayesian Example<a class="anchor" aria-label="anchor" href="#r-code-bayesian-example"><i class="fas fa-link"></i></a>
</h2>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 2.15  </strong></span>Recall the Baseball and Bayes example, Example <a href="bayes.html#exm:baseball">2.13</a>.</p>
<p>The functions allow us to label each plot with the parameter information.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/tidyverse/glue">glue</a></span><span class="op">)</span>

<span class="va">ex</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">a</span>,<span class="va">b</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">a</span> <span class="op">/</span> <span class="op">(</span><span class="va">a</span><span class="op">+</span><span class="va">b</span><span class="op">)</span>, <span class="fl">2</span><span class="op">)</span><span class="op">}</span>
<span class="va">sdx</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">a</span>,<span class="va">b</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">a</span><span class="op">*</span><span class="va">b</span><span class="op">/</span><span class="op">(</span><span class="op">(</span><span class="va">a</span><span class="op">+</span><span class="va">b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="op">(</span><span class="va">a</span><span class="op">+</span><span class="va">b</span><span class="op">+</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,<span class="fl">3</span><span class="op">)</span><span class="op">}</span>

<span class="va">beta_legend</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">a</span>,<span class="va">b</span><span class="op">)</span> <span class="op">{</span>
  <span class="fu">glue</span><span class="fu">::</span><span class="fu"><a href="https://glue.tidyverse.org/reference/glue.html">glue</a></span><span class="op">(</span><span class="st">'a = {a}, '</span>,
             <span class="st">'b = {b}, '</span>,
             <span class="st">'EX = {ex(a,b)}, '</span>,
             <span class="st">'SDX = {sdx(a,b)}'</span><span class="op">)</span><span class="op">}</span>

<span class="co"># see it in action:</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">17</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">3</span>,<span class="fl">17</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-bayes_files/figure-html/unnamed-chunk-12-1.png" width="672"></div>
<div id="priors" class="section level4 unnumbered">
<h4>Priors:<a class="anchor" aria-label="anchor" href="#priors"><i class="fas fa-link"></i></a>
</h4>
<p>By trying out a variety of different values for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, the prior distributions can be visualized.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://patchwork.data-imaginist.com">patchwork</a></span><span class="op">)</span>

<span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>,<span class="fl">17</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">3</span>,<span class="fl">17</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span>
<span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>,<span class="fl">15</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">2</span>,<span class="fl">15</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 
<span class="va">p3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">8</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">1</span>,<span class="fl">8</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span>
<span class="va">p4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">12</span>,<span class="fl">68</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">12</span>,<span class="fl">68</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 
<span class="va">p5</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">17</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">1</span>, <span class="fl">17</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 
<span class="va">p6</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">47</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">3</span>, <span class="fl">47</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 
<span class="va">p7</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">36</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">2</span>, <span class="fl">36</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 
<span class="va">p8</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>, mapping <span class="op">=</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_function.html">stat_function</a></span><span class="op">(</span>fun <span class="op">=</span> <span class="va">dbeta</span>, args <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">162</span><span class="op">)</span>, n <span class="op">=</span> <span class="fl">100</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="fu">beta_legend</span><span class="op">(</span><span class="fl">9</span>, <span class="fl">162</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"y"</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="st">"theta"</span><span class="op">)</span> 


<span class="op">(</span><span class="va">p1</span> <span class="op">+</span> <span class="va">p2</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">p3</span> <span class="op">+</span> <span class="va">p4</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Possible Prior Distributions I"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-bayes_files/figure-html/unnamed-chunk-13-1.png" width="672"></div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="op">(</span><span class="va">p5</span> <span class="op">+</span> <span class="va">p6</span><span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">p7</span> <span class="op">+</span> <span class="va">p8</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://patchwork.data-imaginist.com/reference/plot_annotation.html">plot_annotation</a></span><span class="op">(</span>
    title <span class="op">=</span> <span class="st">"Possible Prior Distributions II"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-bayes_files/figure-html/unnamed-chunk-13-2.png" width="672"></div>
</div>
<div id="mse" class="section level4 unnumbered">
<h4>MSE:<a class="anchor" aria-label="anchor" href="#mse"><i class="fas fa-link"></i></a>
</h4>
<p>Which prior distribution should be used? The answer is that it depends! Of course, if we have a lot of information about the situation, we should use a steep prior that contains the known information. If our information is weak, we should use a flat prior.</p>
<p>Mean Squared Error can be used to determine if the prior was the correct one to use, but only if we know the true value of <span class="math inline">\(\theta\)</span>!! In this case, we’ll compare the MSE of the Bayesian estimator with the MSE of the frequentist estimator under various <em>truth</em> conditions. Because we are comparing apples to oranges (Bayesian vs. frequentist), we are forced to use the frequentist formulation of the MSE (there is no way to find an expected value or variance of the <span class="math inline">\(\theta\)</span> random variable under the frequentist paradigm).</p>
<p>Consider <span class="math inline">\(X\)</span> to be the random variable with a Binomial(n=10, <span class="math inline">\(\theta\)</span>) distribution. In the Bayesian setting, <span class="math inline">\(\hat{\theta} = (x+\alpha) / (n+\alpha+\beta)\)</span>. Deriving the <span class="math inline">\(MSE\)</span> (as a function of <span class="math inline">\(\theta\)</span>) below is given as a homework problem.</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{MSE}_F(\hat{\theta}) &amp;=&amp; \mbox{var}(\hat{\theta}) + (\mbox{bias}(\hat{\theta}))^2\\
&amp;=&amp; \frac{(\alpha-\alpha\theta-\beta\theta)^2+n\theta(1-\theta)}{(n+\alpha+\beta)^2}
\end{eqnarray*}\]</span></p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># frequentist MSE for Bayesian estimator</span>
<span class="va">mse_b</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">t</span>,<span class="va">a</span>,<span class="va">b</span>,<span class="va">n</span><span class="op">)</span> <span class="op">{</span>
  <span class="op">(</span> <span class="op">(</span><span class="va">a</span> <span class="op">-</span> <span class="va">a</span><span class="op">*</span><span class="va">t</span> <span class="op">-</span> <span class="va">b</span><span class="op">*</span><span class="va">t</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">n</span><span class="op">*</span><span class="va">t</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">t</span><span class="op">)</span> <span class="op">)</span> <span class="op">/</span> <span class="op">(</span><span class="va">n</span> <span class="op">+</span> <span class="va">a</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span>
<span class="op">}</span>

<span class="co"># frequentist MSE for frequentist estimator</span>
<span class="va">mse_f</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">t</span>, <span class="va">n</span><span class="op">)</span><span class="op">{</span>
  <span class="va">t</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">t</span><span class="op">)</span> <span class="op">/</span> <span class="va">n</span>
<span class="op">}</span></code></pre></div>
<p>The MSE can be used to assess the estimator (which may or may not be a function of the prior information). Note that the value on the x-axis is the <strong>truth</strong>, and the value on the y-axis is how good / bad the estimator is (as measured by mean squared error).</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">t</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>theta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span><span class="op">)</span>

<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">t</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_f</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"frequentist est"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">1</span>, <span class="fl">17</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(1,17) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">3</span>, <span class="fl">47</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(3,47) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">2</span>, <span class="fl">36</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(2,36) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">9</span>, <span class="fl">162</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(9,162) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"MSE"</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"MSE for frequentist and different beta priors"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-bayes_files/figure-html/unnamed-chunk-15-1.png" width="672"></div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">t</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_f</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"frequentist est"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">1</span>, <span class="fl">17</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(1,17) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">3</span>, <span class="fl">47</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(3,47) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">2</span>, <span class="fl">36</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(2,36) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">theta</span>, y <span class="op">=</span> <span class="fu">mse_b</span><span class="op">(</span><span class="va">theta</span>, <span class="fl">9</span>, <span class="fl">162</span>, <span class="fl">10</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"beta(9,162) prior"</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ylab</a></span><span class="op">(</span><span class="st">"MSE"</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"MSE zoomed in"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">ylim</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.05</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="02-bayes_files/figure-html/unnamed-chunk-15-2.png" width="672"></div>
</div>
</div>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#bayes"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="nav-link" href="#bayes-rule"><span class="header-section-number">2.1</span> Bayes’ Rule</a></li>
<li><a class="nav-link" href="#prior-distributions"><span class="header-section-number">2.2</span> Prior Distributions</a></li>
<li><a class="nav-link" href="#posterior-distributions"><span class="header-section-number">2.3</span> Posterior Distributions</a></li>
<li><a class="nav-link" href="#conjugate-prior-distributions"><span class="header-section-number">2.4</span> Conjugate Prior Distributions</a></li>
<li><a class="nav-link" href="#improper-priors"><span class="header-section-number">2.5</span> Improper Priors</a></li>
<li>
<a class="nav-link" href="#bayes-estimators"><span class="header-section-number">2.6</span> Bayes’ Estimators</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#loss-functions"><span class="header-section-number">2.6.1</span> Loss functions</a></li></ul>
</li>
<li>
<a class="nav-link" href="#evaluating-bayes-estimators"><span class="header-section-number">2.7</span> Evaluating Bayes Estimators</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#mean-squared-error"><span class="header-section-number">2.7.1</span> Mean Squared Error</a></li>
<li><a class="nav-link" href="#sensitivity-of-estimators"><span class="header-section-number">2.7.2</span> Sensitivity of Estimators</a></li>
<li><a class="nav-link" href="#consistency-of-estimators"><span class="header-section-number">2.7.3</span> Consistency of Estimators</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#benefits-and-limitations-of-bayes-estimators"><span class="header-section-number">2.8</span> Benefits and Limitations of Bayes’ Estimators</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#benefits"><span class="header-section-number">2.8.1</span> Benefits</a></li>
<li><a class="nav-link" href="#limitations"><span class="header-section-number">2.8.2</span> Limitations</a></li>
</ul>
</li>
<li><a class="nav-link" href="#additional-examples"><span class="header-section-number">2.9</span> Additional Examples</a></li>
<li><a class="nav-link" href="#reflection-questions-1"><span class="header-section-number">2.10</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-1"><span class="header-section-number">2.11</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r-code-bayesian-example"><span class="header-section-number">2.12</span> R code: Bayesian Example</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/02-bayes.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/02-bayes.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-09-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
