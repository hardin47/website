<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Sampling Distributions of Estimators | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 4 Sampling Distributions of Estimators | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Sampling Distributions of Estimators | Statistical Theory">
<meta name="twitter:description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="active" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></li>
<li><a class="" href="intest.html"><span class="header-section-number">6</span> Interval Estimates</a></li>
<li><a class="" href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></li>
<li><a class="" href="ht.html"><span class="header-section-number">8</span> Hypothesis Testing</a></li>
<li><a class="" href="gibbs.html"><span class="header-section-number">9</span> Gibbs Sampler</a></li>
<li><a class="" href="em.html"><span class="header-section-number">10</span> The EM Algorithm</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sampdist" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Sampling Distributions of Estimators<a class="anchor" aria-label="anchor" href="#sampdist"><i class="fas fa-link"></i></a>
</h1>
<p>A <strong>statistic</strong> is a function of some observed random variables. The <strong>sampling distribution</strong> of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic is a function of the random variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, if we know the distribution of <span class="math inline">\(X\)</span>, in principle, we should be able to derive the distribution of our statistic.</p>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 4.1  </strong></span>If the data are normal, the sampling distribution of their mean is also normal. While the result may look like the Central Limit Theorem, there is no limiting behavior here. That is, the sampling distribution of <span class="math inline">\(\overline{X}\)</span> is normal regardless of the sample size. The proof come directly from the result that linear combinations of normal random variables are also normal.</p>
<p><span class="math display">\[\begin{eqnarray*}
X_i &amp;\sim&amp; N(\mu, \sigma^2)\\
\overline{X} &amp;\sim&amp; N(\mu, \sigma^2 / n)
\end{eqnarray*}\]</span></p>
</div>
<div id="the-chi-square-distribution" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> The Chi-Square Distribution<a class="anchor" aria-label="anchor" href="#the-chi-square-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The chi-square distribution is a probability distribution with the following characteristics.</p>
<p><span class="math display">\[\begin{eqnarray*}
X &amp;\sim&amp; \chi^2_n\\
f_X(x) &amp;=&amp; \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2 -1} e^{-x/2} \ \ \ \ \ \ x &gt; 0\\
E[X] &amp;=&amp; n\\
Var[X] &amp;=&amp; 2n\\
\psi_X(t) &amp;=&amp; \Bigg( \frac{1}{1-2t} \Bigg)^{n/2} \ \ \ \ \ \ t &lt; 1/2\\
( &amp;=&amp;  E[e^{tX}] )\\
\end{eqnarray*}\]</span></p>
<p>Recall Moment Generating Functions <span class="math inline">\((\psi_X(t))\)</span>, <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> page 205.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 4.1  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.1</p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots X_k \sim \chi^2_{n_i}, \ \ i=1, \ldots, k\)</span>, independently. Then, <span class="math inline">\(X_1 + X_2 + \cdots + X_k = \sum_{i=1}^k X_i \sim \chi^2_{n_1+n_2 +\cdots+n_k}\)</span>.</p>
<p>That is, if the data are independent chi-square random variables, the sampling distribution of their sum is also chi-square.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{eqnarray*}
Y&amp;=&amp; \sum_{i=1}^k X_i\\
\psi_Y (t) &amp;=&amp; E[e^{Yt} ]\\
&amp;=&amp; E[e^{t \sum_{i=1}^k X_i}]\\
&amp;=&amp; \prod_{i=1}^k E[e^{tX_i}]\\
&amp;=&amp; \prod_{i=1}^k \psi_{X_i}(t)\\
&amp;=&amp; \prod_{i=1}^k \bigg( \frac{1}{1-2t} \bigg)^{ n_i /2}\\
&amp;=&amp; \Bigg( \frac{1}{1-2t} \Bigg)^{\sum n_i /2}
\end{eqnarray*}\]</span>
See theorem 4.4.3, pg 207.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 4.2  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.1 1/2</p>
<p>If <span class="math inline">\(Z \sim N(0,1), Y=Z^2,\)</span> then <span class="math inline">\(Y \sim \chi^2_1\)</span>.</p>
<p>Note that the result here is to provide the distribution of a transformation of a random variable. There is a single data value <span class="math inline">\((Z),\)</span> and the result provides the distribution of another single value, <span class="math inline">\((Y).\)</span> The value <span class="math inline">\(Y\)</span> would typically not be referred to as a statistic because it is not a summary of observations.</p>
<p>That said, just below, <span class="math inline">\(Z\)</span> itself will be a statistic (instead of a single value) and then both <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> will have sampling distributions!</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\phi\)</span> be the cdf and pdf of Z.<br>
Let F and f be the cdf and pdf of Y.<br><span class="math display">\[\begin{eqnarray*}
F_Y(y) &amp;=&amp; P(Y \leq y) = P(Z^2 \leq y)\\
&amp;=&amp; P(-y^{1/2} \leq Z \leq y^{1/2})\\
&amp;=&amp; \Phi(y^{1/2}) -  \Phi(- y^{1/2})  \ \ \ \ \ y &gt; 0\\
\end{eqnarray*}\]</span>
<span class="math display">\[\begin{eqnarray*}
f_Y(y) &amp;=&amp; \frac{\partial F_Y(y)}{\partial y} = \phi(y^{1/2}) \cdot \frac{1}{2} y^{-1/2} - \phi(-y^{1/2}) \cdot \frac{1}{2} -y^{-1/2}\\
&amp;=&amp; \frac{1}{2} y^{-1/2} ( \phi(y^{1/2}) + \phi(-y^{1/2})) \ \ \ \ \ \ y &gt; 0\\
\mbox{we know} &amp;&amp; \phi(y^{1/2}) = \phi(-y^{1/2}) = \frac{1}{\sqrt{2 \pi}} e^{-y/2}\\
%\therefore
f_Y(y) &amp;=&amp; y^{-1/2} \frac{1}{\sqrt{2 \pi}} e^{-y/2} \ \ \ \ \ \ y &gt; 0 \\
&amp;=&amp; \frac{1}{2^{1/2}\pi^{1/2}} y^{1/2 - 1} e^{-y/2} \ \ \ \ \ \ y &gt;0\\
Y &amp;\sim&amp; \chi^2_1\\
\end{eqnarray*}\]</span>
note: <span class="math inline">\(\Gamma(1/2) = \sqrt{\pi}\)</span>.</p>
</div>
<p>By combining Theorems 7.2.1 and 7.2.1 1/2, we get:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 4.3  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.2</p>
<p>If <span class="math inline">\(X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(0,1)\)</span>,
<span class="math display">\[\begin{eqnarray*}
\sum_{i=1}^k X_i^2 \sim \chi^2_k
\end{eqnarray*}\]</span>
Note: if <span class="math inline">\(X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span>,
<span class="math display">\[\begin{eqnarray*}
\frac{X_i - \mu}{\sigma} &amp;\sim&amp; N(0,1)\\
\sum_{i=1}^k \frac{(X_i - \mu)^2}{\sigma^2} &amp;\sim&amp; \chi^2_k\\
\end{eqnarray*}\]</span></p>
<p>If the data are <span class="math inline">\(iid\)</span> normal, the sum of their squared values has a sampling distribution which is chi-square.</p>
</div>
</div>
<div id="independence-of-the-mean-and-variance-of-a-normal-random-variable" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Independence of the Mean and Variance of a Normal Random Variable<a class="anchor" aria-label="anchor" href="#independence-of-the-mean-and-variance-of-a-normal-random-variable"><i class="fas fa-link"></i></a>
</h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 4.4  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 8.3.1</p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(\frac{1}{n} \sum(X_i - \overline{X})^2\)</span> are independent. (This is <strong>only</strong> true for normal random variables. You should read through the proof in your book.)</li>
<li>
<span class="math inline">\(\overline{X} \sim N(\mu, \sigma^2/n)\)</span>. (Not the CLT, why not?)</li>
<li>
<span class="math inline">\(\frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}\)</span>. (Main idea is that we only have <span class="math inline">\(n-1\)</span> independent things.)</li>
</ol>
</div>
</div>
<div id="the-t-distribution" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> The t-distribution<a class="anchor" aria-label="anchor" href="#the-t-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>Let <span class="math inline">\(Z \sim N(0,1)\)</span> and <span class="math inline">\(Y \sim \chi^2_n\)</span>. If <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> are independent, then:</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 4.1  </strong></span><span class="math display">\[\begin{eqnarray*}
X = \frac{Z}{\sqrt{Y/n}} \sim t_n \mbox{  by definition}
\end{eqnarray*}\]</span></p>
</div>
<p><span class="math display">\[\begin{eqnarray*}
f_X(x) &amp;=&amp; \frac{\Gamma(\frac{n+1}{2})}{(n \pi)^{1/2} \Gamma(\frac{n}{2})} (1 + \frac{x^2}{n})^{-(n+1)/2} \ \ \ \ n &gt; 2\\
E[X] &amp;=&amp;0\\
Var(X) &amp;=&amp; \frac{n}{n-2}
\end{eqnarray*}\]</span></p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \sim N(\mu, \sigma^2)\)</span>, the following hold:
<span class="math display">\[\begin{eqnarray*}
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1) \mbox{ independently of } \frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{\sigma^2}/(n-1)}} &amp;=&amp; \frac{\overline{X} - \mu}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{n-1}/n}}\\
&amp;=&amp; \frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1} !
\end{eqnarray*}\]</span></p>
<p>As stated above, the t-distribution is defined as the distribution which is given when a standard normal is divided by the square root of a chi-square random variable divided by its degrees of freedom. And while that may seem obtuse at first glance, it comes in extremely handy when standardizing a sample mean by using the standard error (instead of the standard deviation) of the mean.</p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 4.2  </strong></span>According to some investors, foreign stocks have the potential for high yield, but the variability in their dividends may be greater than what is typical for American companies. Let’s say we take a random sample of 10 foreign stocks; assume also that we know the population distribution from which American stocks come (i.e., we have the American parameters). If <strong>we believe that foreign stock prices are distributed similarly (normal with the same mean and variance)</strong> to American stock prices, how likely is it that a sample of 10 foreign stocks would produce a standard deviation which is 50% bigger than American stocks?</p>
<p><span class="math display">\[\begin{eqnarray*}
P(\hat{\sigma} / \sigma &gt; 1.5 ) &amp;=&amp; ?\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &amp;\sim&amp; \chi^2_{n-1} \ \ \ \ \mbox{(normality assumption)}\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &amp;=&amp; n\frac{\sum (X_i - \overline{X})^2/n}{\sigma^2}\\
&amp;=&amp; \frac{n \hat{\sigma^2}}{\sigma^2}\\
P(\hat{\sigma} / \sigma &gt; 1.5 ) &amp;=&amp; P(\hat{\sigma}^2 / \sigma^2 &gt; 1.5^2 ) \\
&amp;=&amp; P(n \hat{\sigma}^2 / \sigma^2 &gt; n 1.5^2 )\\
&amp;=&amp; 1 - \chi^2_{n-1} (n 1.5^2)\\
&amp;=&amp; 1 - \chi^2_{n-1} (22.5)\\
&amp;=&amp; 1 - pchisq(22.5,9) = 0.00742 \ \ \ \mbox{ in R}
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 4.3  </strong></span>Suppose we take a random sample of foreign stocks (both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> unknown). Find the value of <span class="math inline">\(k\)</span> such that the sample mean is no more than <span class="math inline">\(k\)</span> sample standard deviations <span class="math inline">\((s)\)</span> above the mean <span class="math inline">\(\mu\)</span> with probability 0.90.</p>
<p>
Data: <span class="math inline">\(n=10\)</span>, <span class="math inline">\(\hat{\mu} = \overline{x}\)</span>, <span class="math inline">\(s^2 = \frac{\sum(x_i - \overline{x})^2}{n-1}\)</span>, <span class="math inline">\(s = \sigma'\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
P(\overline{X} &lt; \mu + k s) &amp;=&amp; 0.9\\
P\Bigg(\frac{\overline{X} - \mu}{s} &lt; k \Bigg) = P\Bigg(\frac{\overline{X} - \mu}{s/\sqrt{n}} &lt; k \sqrt{n}\Bigg) &amp;=&amp; 0.9\\
\frac{\overline{X} - \mu}{s / \sqrt{n}} &amp;\sim&amp; t_9\\
\sqrt{n} k &amp;=&amp; 1.383\\
k &amp;=&amp; \frac{1.383}{\sqrt{10}} = 0.437\\
\mbox{note, in R: } qt(0.9,9) &amp;=&amp; 1.383
\end{eqnarray*}\]</span></p>
<p>How would this problem have been different if we had known <span class="math inline">\(\sigma\)</span>? Or even if we had wanted the answer to the question in terms of number of population standard deviations?</p>
</div>
</div>
<div id="reflection-questions-3" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-3"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>What does it mean for a statistic to have a sampling distribution?</li>
<li>What is the difference between the theoretical MSE and the empirical MSE (e.g., in the tank example below)?</li>
<li>Why can’t a standard normal distribution be used when the statistic of interest is <span class="math inline">\(\frac{\overline{X} - \mu}{s / \sqrt{n}}?\)</span>
</li>
<li>What different tools are used to determine the distribution of a random variable? (Note, in this chapter, the majority of the random variables of interest are functions of data, also called statistics.)</li>
</ol>
</div>
<div id="ethics-considerations-3" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-3"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>How do you know which estimator to use in a consulting situation?</li>
<li>How do you respond to someone who tells you “there isn’t one right answer” to the previous question?</li>
<li>What are the technical conditions for doing inference using a t-distribution? That is, what are the conditions on the data that give rise to the t-distribution? What happens if the technical conditions are violated and the inference is done anyway? [Here, inference means confidence intervals and hypothesis testing.]</li>
</ol>
</div>
<div id="r-code-tanks-example" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> R code: Tanks Example<a class="anchor" aria-label="anchor" href="#r-code-tanks-example"><i class="fas fa-link"></i></a>
</h2>
<p>How can a random sample of integers between 1 and <span class="math inline">\(N\)</span> (with <span class="math inline">\(N\)</span> unknown to the researcher) be used to estimate <span class="math inline">\(N\)</span>? This problem is known as the German tank problem and is derived directly from a situation where the Allies used maximum likelihood estimation to determine how many tanks the Axes had produced. See .</p>
<p>The tanks are numbered from 1 to <span class="math inline">\(N\)</span>.<br>
Think about how you would use your data to estimate <span class="math inline">\(N\)</span>.</p>
<p>Some possible estimators of <span class="math inline">\(N\)</span> are:<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Note that the MOM and MLE estimators were derived under the assumption that the data are &lt;em&gt;iid&lt;/em&gt; from a population of discrete uniform values. Because our data is sampled without replacement, we don’t have an &lt;em&gt;iid&lt;/em&gt; model. However, if &lt;span class="math inline"&gt;\(n &amp;lt; &amp;lt; &amp;lt; N\)&lt;/span&gt;, the &lt;em&gt;iid&lt;/em&gt; discrete uniform is a reasonable model for the situation at hand.&lt;/p&gt;'><sup>7</sup></a><br><span class="math display">\[\begin{eqnarray*}
\hat{N}_1 &amp;=&amp; 2\cdot\overline{X} - 1 \ \ \ \mbox{the MOM}\\
\hat{N}_2 &amp;=&amp; 2\cdot \mbox{median}(\underline{X}) - 1 \\
\hat{N}_3 &amp;=&amp; \max(\underline{X})  \ \ \ \mbox{the MLE}\\
\hat{N}_4 &amp;=&amp; \frac{n+1}{n} \max(\underline{X})  \ \ \ \mbox{less biased version of the MLE}\\
\hat{N}_5 &amp;=&amp; \max(\underline{X}) + \min(\underline{X}) \\
\hat{N}_6 &amp;=&amp; \frac{n+1}{n-1}[\max(\underline{X}) - \min(\underline{X})] \\
\end{eqnarray*}\]</span></p>
<div id="theoretical-mean-squared-error" class="section level3" number="4.6.1">
<h3>
<span class="header-section-number">4.6.1</span> Theoretical Mean Squared Error<a class="anchor" aria-label="anchor" href="#theoretical-mean-squared-error"><i class="fas fa-link"></i></a>
</h3>
<p>Most of our estimators are made up of four basic functions of the data: the mean, the median, the min, and the max. Fortunately, we know something about the moments of these functions:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="41%">
<col width="26%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th>g(<span class="math inline">\(\underline{X}\)</span>)</th>
<th align="center">E( g(<span class="math inline">\(\underline{X}\)</span>) )</th>
<th align="center">Var( g(<span class="math inline">\(\underline{X}\)</span>) )</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\overline{X}\)</span></td>
<td align="center"><span class="math inline">\(\frac{N+1}{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{(N+1)(N-1)}{12 n}\)</span></td>
</tr>
<tr class="even">
<td>median(<span class="math inline">\(\underline{X}\)</span>) = M</td>
<td align="center"><span class="math inline">\(\frac{N+1}{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{(N-1)^2}{4 n}\)</span></td>
</tr>
<tr class="odd">
<td>min(<span class="math inline">\(\underline{X}\)</span>)</td>
<td align="center"><span class="math inline">\(\frac{(N-1)}{n} + 1\)</span></td>
<td align="center"><span class="math inline">\(\bigg(\frac{N-1}{n}\bigg)^2\)</span></td>
</tr>
<tr class="even">
<td>max(<span class="math inline">\(\underline{X}\)</span>)</td>
<td align="center"><span class="math inline">\(N - \frac{(N-1)}{n}\)</span></td>
<td align="center"><span class="math inline">\(\bigg(\frac{N-1}{n}\bigg)^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>Using the information on expected value and variance, we can calculate the MSE for 4 of the estimators that we have derived. (Remember that MSE = Variance + Bias<span class="math inline">\(^2\)</span>.)</p>
<p><span class="math display">\[\begin{eqnarray}
\mbox{MSE } ( 2 \cdot \overline{X} - 1) &amp;=&amp; \frac{4 (N+1) (N-1)}{12n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&amp;=&amp; \frac{4 (N+1) (N-1)}{12n} \\
\nonumber \\
\mbox{MSE } ( 2 \cdot M - 1) &amp;=&amp; \frac{4 (N-1)^2}{4n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&amp;=&amp; \frac{4 (N-1)^2}{4n} \\
\nonumber \\
\mbox{MSE } ( \max(\underline{X})) &amp;=&amp; \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(N - \frac{(N-1)}{n} - N\Bigg)^2 \nonumber\\
&amp;=&amp; \bigg(\frac{N-1}{n}\bigg)^2 + \bigg(\frac{N-1}{n} \bigg)^2  = 2*\bigg(\frac{N-1}{n} \bigg)^2 \\
\nonumber \\
\mbox{MSE } \Bigg( \bigg( \frac{n+1}{n} \bigg) \max(\underline{X})\Bigg) &amp;=&amp; \bigg(\frac{n+1}{n}\bigg)^2 \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(\bigg(\frac{n+1}{n}\bigg) \bigg(N - \frac{N-1}{n} \bigg) - N \Bigg)^2
\end{eqnarray}\]</span></p>
<div class="inline-figure"><img src="04-sampdist_files/figure-html/unnamed-chunk-1-1.png" width="672"></div>
</div>
<div id="empirical-mse" class="section level3" number="4.6.2">
<h3>
<span class="header-section-number">4.6.2</span> Empirical MSE<a class="anchor" aria-label="anchor" href="#empirical-mse"><i class="fas fa-link"></i></a>
</h3>
<p>We don’t need to know the theoretical expected value or variance of the functions to approximate the MSE. We can visualize the sampling distributions and also calculate the actual empirical MSE for any estimator we come up with.</p>
<p>By changing the population size and the sample size, we can assess how the estimators compare and whether one is particularly better under a given setting.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">calculate_N</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">nsamp</span>,<span class="va">npop</span><span class="op">)</span><span class="op">{</span>
  <span class="va">mysample</span> <span class="op">=</span>  <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">npop</span>,<span class="va">nsamp</span>,replace<span class="op">=</span><span class="cn">F</span><span class="op">)</span>  <span class="co"># what does this line do?</span>
  <span class="va">xbar2</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>
  <span class="va">median2</span> <span class="op">&lt;-</span> <span class="fl">2</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span> <span class="op">-</span> <span class="fl">1</span>
  <span class="va">samp.max</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span>
  <span class="va">mod.max</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">(</span><span class="va">nsamp</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">/</span><span class="va">nsamp</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span>
  <span class="va">sum.min.max</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span>
  <span class="va">diff.min.max</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="op">(</span><span class="va">nsamp</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="va">nsamp</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span><span class="op">*</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">mysample</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>
  <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">xbar2</span>, <span class="va">median2</span>, <span class="va">samp.max</span>, <span class="va">mod.max</span>, <span class="va">sum.min.max</span>, <span class="va">diff.min.max</span>,<span class="va">nsamp</span>,<span class="va">npop</span><span class="op">)</span>
<span class="op">}</span>

<span class="va">reps</span> <span class="op">&lt;-</span> <span class="fl">2</span>
<span class="va">nsamp_try</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">10</span>,<span class="fl">100</span>, <span class="fl">10</span>, <span class="fl">100</span><span class="op">)</span>
<span class="va">npop_try</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">147</span>, <span class="fl">147</span>, <span class="fl">447</span>, <span class="fl">447</span><span class="op">)</span>
<span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_df</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">reps</span>, <span class="op">~</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html">map2</a></span><span class="op">(</span><span class="va">nsamp_try</span>, <span class="va">npop_try</span>, <span class="va">calculate_N</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##    xbar2 median2 samp.max mod.max sum.min.max diff.min.max nsamp npop
## 1 123.40     118      140  154.00         144     166.2222    10  147
## 2 154.30     153      147  148.47         149     147.9293   100  147
## 3 514.80     459      441  485.10         582     366.6667    10  447
## 4 466.66     463      447  451.47         451     451.9495   100  447
## 5 163.80     185      140  154.00         149     160.1111    10  147
## 6 146.90     136      147  148.47         148     148.9495   100  147
## 7 474.00     581      434  477.40         445     517.0000    10  447
## 8 435.22     437      447  451.47         449     453.9899   100  447</code></pre>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">reps</span> <span class="op">&lt;-</span> <span class="fl">1000</span>
<span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://purrr.tidyverse.org/reference/map.html">map_df</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">reps</span>, <span class="op">~</span><span class="fu"><a href="https://purrr.tidyverse.org/reference/map2.html">map2</a></span><span class="op">(</span><span class="va">nsamp_try</span>, <span class="va">npop_try</span>, <span class="va">calculate_N</span><span class="op">)</span><span class="op">)</span>

<span class="co"># making the results long instead of wide:</span>
<span class="va">results_long</span> <span class="op">&lt;-</span> <span class="va">results</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://tidyr.tidyverse.org/reference/pivot_longer.html">pivot_longer</a></span><span class="op">(</span>cols <span class="op">=</span> <span class="va">xbar2</span><span class="op">:</span><span class="va">diff.min.max</span>, names_to <span class="op">=</span> <span class="st">"estimator"</span>, values_to <span class="op">=</span> <span class="st">"estimate"</span><span class="op">)</span>

<span class="co"># how is results different from results_long?  let's look at it:</span>
<span class="va">results_long</span></code></pre></div>
<pre><code>## # A tibble: 24,000 × 4
##    nsamp  npop estimator    estimate
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;
##  1    10   147 xbar2            132.
##  2    10   147 median2          117 
##  3    10   147 samp.max         133 
##  4    10   147 mod.max          146.
##  5    10   147 sum.min.max      136 
##  6    10   147 diff.min.max     159.
##  7   100   147 xbar2            144.
##  8   100   147 median2          138 
##  9   100   147 samp.max         147 
## 10   100   147 mod.max          148.
## # … with 23,990 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">results_long</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/group_by.html">group_by</a></span><span class="op">(</span><span class="va">nsamp</span>, <span class="va">npop</span>, <span class="va">estimator</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">estimate</span><span class="op">)</span>, median <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">estimate</span><span class="op">)</span>, bias <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">estimate</span> <span class="op">-</span> <span class="va">npop</span><span class="op">)</span>,
            var <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">estimate</span><span class="op">)</span>, mse <span class="op">=</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">estimate</span> <span class="op">-</span> <span class="va">npop</span><span class="op">)</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">estimate</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>## `summarise()` has grouped output by 'nsamp', 'npop'. You can override using the
## `.groups` argument.</code></pre>
<pre><code>## # A tibble: 24 × 8
## # Groups:   nsamp, npop [4]
##    nsamp  npop estimator     mean median    bias    var    mse
##    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1    10   147 diff.min.max  148.   152.   0.517   373.   374.
##  2    10   147 median2       144.   143   -2.68   1577.  1584.
##  3    10   147 mod.max       148.   152.   0.502   181.   181.
##  4    10   147 samp.max      134.   138  -12.9     149.   316.
##  5    10   147 sum.min.max   147.   147    0.49    305.   305.
##  6    10   147 xbar2         146.   146.  -0.95    698.   699.
##  7    10   447 diff.min.max  451.   461.   4.36   3491.  3510.
##  8    10   447 median2       442.   438   -4.59  13902. 13923.
##  9    10   447 mod.max       450.   463.   3.26   1498.  1508.
## 10    10   447 samp.max      409.   420. -37.7    1238.  2657.
## # … with 14 more rows
## # ℹ Use `print(n = ...)` to see more rows</code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">results_long</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">npop</span> <span class="op">==</span> <span class="fl">147</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">estimate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">npop</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_grid.html">facet_grid</a></span><span class="op">(</span><span class="va">nsamp</span> <span class="op">~</span> <span class="va">estimator</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distributions of estimators of N, pop size = 147"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="04-sampdist_files/figure-html/unnamed-chunk-5-1.png" width="672"></div>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">results_long</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/filter.html">filter</a></span><span class="op">(</span><span class="va">npop</span> <span class="op">==</span> <span class="fl">447</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">estimate</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">npop</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span>
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/facet_grid.html">facet_grid</a></span><span class="op">(</span><span class="va">nsamp</span> <span class="op">~</span> <span class="va">estimator</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"sampling distributions of estimators of N, pop size = 447"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="04-sampdist_files/figure-html/unnamed-chunk-5-2.png" width="672"></div>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></div>
<div class="next"><a href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sampdist"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="nav-link" href="#the-chi-square-distribution"><span class="header-section-number">4.1</span> The Chi-Square Distribution</a></li>
<li><a class="nav-link" href="#independence-of-the-mean-and-variance-of-a-normal-random-variable"><span class="header-section-number">4.2</span> Independence of the Mean and Variance of a Normal Random Variable</a></li>
<li><a class="nav-link" href="#the-t-distribution"><span class="header-section-number">4.3</span> The t-distribution</a></li>
<li><a class="nav-link" href="#reflection-questions-3"><span class="header-section-number">4.4</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-3"><span class="header-section-number">4.5</span>  Ethics Considerations</a></li>
<li>
<a class="nav-link" href="#r-code-tanks-example"><span class="header-section-number">4.6</span> R code: Tanks Example</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#theoretical-mean-squared-error"><span class="header-section-number">4.6.1</span> Theoretical Mean Squared Error</a></li>
<li><a class="nav-link" href="#empirical-mse"><span class="header-section-number">4.6.2</span> Empirical MSE</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/04-sampdist.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/04-sampdist.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-12-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
