<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Sampling Distributions of Estimators | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 4 Sampling Distributions of Estimators | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Sampling Distributions of Estimators | Statistical Theory">
<meta name="twitter:description" content="A statistic is a function of some observed random variables. The sampling distribution of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="active" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="sampdist" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Sampling Distributions of Estimators<a class="anchor" aria-label="anchor" href="#sampdist"><i class="fas fa-link"></i></a>
</h1>
<p>A <strong>statistic</strong> is a function of some observed random variables. The <strong>sampling distribution</strong> of a statistic tells us which values a statistic assumes and how likely those values are. Because a statistic is a function of the random variables <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>, if we know the distribution of <span class="math inline">\(X\)</span>, in principle, we should be able to derive the distribution of our statistic.</p>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 4.1  </strong></span>If the data are normal, the sampling distribution of their mean is also normal. While the result may look like the Central Limit Theorem, there is no limiting behavior here. That is, the sampling distribution of <span class="math inline">\(\overline{X}\)</span> is normal regardless of the sample size. The proof come directly from the result that linear combinations of normal random variables are also normal.</p>
<p><span class="math display">\[\begin{eqnarray*}
X_i &amp;\sim&amp; N(\mu, \sigma^2)\\
\overline{X} &amp;\sim&amp; N(\mu, \sigma^2 / n)
\end{eqnarray*}\]</span></p>
</div>
<div id="the-chi-square-distribution" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> The Chi-Square Distribution<a class="anchor" aria-label="anchor" href="#the-chi-square-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The chi-square distribution is a probability distribution with the following characteristics.</p>
<p><span class="math display">\[\begin{eqnarray*}
X &amp;\sim&amp; \chi^2_n\\
f_X(x) &amp;=&amp; \frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2 -1} e^{-x/2} \ \ \ \ \ \ x &gt; 0\\
E[X] &amp;=&amp; n\\
Var[X] &amp;=&amp; 2n\\
\psi_X(t) &amp;=&amp; \Bigg( \frac{1}{1-2t} \Bigg)^{n/2} \ \ \ \ \ \ t &lt; 1/2\\
( &amp;=&amp;  E[e^{tX}] )\\
\end{eqnarray*}\]</span></p>
<p>Recall Moment Generating Functions <span class="math inline">\((\psi_X(t))\)</span>, <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> page 205.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-19" class="theorem"><strong>Theorem 4.1  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.1</p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots X_k \sim \chi^2_{n_i}, \ \ i=1, \ldots, k\)</span>, independently. Then, <span class="math inline">\(X_1 + X_2 + \cdots + X_k = \sum_{i=1}^k X_i \sim \chi^2_{n_1+n_2 +\cdots+n_k}\)</span>.</p>
<p>That is, if the data are independent chi-square random variables, the sampling distribution of their sum is also chi-square.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-20" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{eqnarray*}
Y&amp;=&amp; \sum_{i=1}^k X_i\\
\psi_Y (t) &amp;=&amp; E[e^{Yt} ]\\
&amp;=&amp; E[e^{t \sum_{i=1}^k X_i}]\\
&amp;=&amp; \prod_{i=1}^k E[e^{tX_i}]\\
&amp;=&amp; \prod_{i=1}^k \psi_{X_i}(t)\\
&amp;=&amp; \prod_{i=1}^k \bigg( \frac{1}{1-2t} \bigg)^{ n_i /2}\\
&amp;=&amp; \Bigg( \frac{1}{1-2t} \Bigg)^{\sum n_i /2}
\end{eqnarray*}\]</span>
See theorem 4.4.3, pg 207.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 4.2  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.1 1/2</p>
<p>If <span class="math inline">\(Z \sim N(0,1), Y=Z^2,\)</span> then <span class="math inline">\(Y \sim \chi^2_1\)</span>.</p>
<p>Note that the result here is to provide the distribution of a transformation of a random variable. There is a single data value <span class="math inline">\((Z)\)</span>, and the result provides the distribution of another single value, <span class="math inline">\((Y).\)</span> The value <span class="math inline">\(Y\)</span> would typically not be referred to as a statistic because it is not a summary of observations.</p>
<p>That said, just below, <span class="math inline">\(Z\)</span> itself will be a statistic (instead of a single value) and then both <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> will have sampling distributions!</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\Phi\)</span> and <span class="math inline">\(\phi\)</span> be the cdf and pdf of Z.\
Let F and f be the cdf and pdf of Y.\
<span class="math display">\[\begin{eqnarray*}
F(y) &amp;=&amp; P(Y \leq y) = P(Z^2 \leq y)\\
&amp;=&amp; P(-y^{1/2} \leq Z \leq y^{1/2})\\
&amp;=&amp; \Phi(y^{1/2}) -  \Phi(- y^{1/2})  \ \ \ \ \ y &gt; 0\\
\end{eqnarray*}\]</span>
<span class="math display">\[\begin{eqnarray*}
f(y) &amp;=&amp; \frac{\partial F(y)}{\partial y} = \phi(y^{1/2}) \cdot \frac{1}{2} y^{-1/2} - \phi(-y^{1/2}) \cdot \frac{1}{2} -y^{-1/2}\\
&amp;=&amp; \frac{1}{2} y^{-1/2} ( \phi(y^{1/2}) + \phi(-y^{1/2})) \ \ \ \ \ \ y &gt; 0\\
\mbox{we know} &amp;&amp; \phi(y^{1/2}) = \phi(-y^{1/2}) = \frac{1}{\sqrt{2 \pi}} e^{-y/2}\\
%\therefore
f(y) &amp;=&amp; y^{-1/2} \frac{1}{\sqrt{2 \pi}} e^{-y/2} \ \ \ \ \ \ y &gt; 0 \\
&amp;=&amp; \frac{1}{2^{1/2}\pi^{1/2}} y^{1/2 - 1} e^{-y/2} \ \ \ \ \ \ y &gt;0\\
Y &amp;\sim&amp; \chi^2_1\\
\end{eqnarray*}\]</span>
note: <span class="math inline">\(\Gamma(1/2) = \sqrt{\pi}\)</span>.</p>
</div>
<p>By combining Theorems 7.2.1 and 7.2.1 1/2, we get:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-23" class="theorem"><strong>Theorem 4.3  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 7.2.2</p>
<p>If <span class="math inline">\(X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(0,1)\)</span>,
<span class="math display">\[\begin{eqnarray*}
\sum_{i=1}^k X_i^2 \sim \chi^2_k
\end{eqnarray*}\]</span>
Note: if <span class="math inline">\(X_1, X_2, \ldots, X_k \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span>,
<span class="math display">\[\begin{eqnarray*}
\frac{X_i - \mu}{\sigma} &amp;\sim&amp; N(0,1)\\
\sum_{i=1}^k \frac{(X_i - \mu)^2}{\sigma^2} &amp;\sim&amp; \chi^2_k\\
\end{eqnarray*}\]</span></p>
<p>If the data are <span class="math inline">\(iid\)</span> normal, the sum of their squared values has a sampling distribution which is chi-square.</p>
</div>
</div>
<div id="independence-of-the-mean-and-variance-of-a-normal-random-variable" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Independence of the Mean and Variance of a Normal Random Variable<a class="anchor" aria-label="anchor" href="#independence-of-the-mean-and-variance-of-a-normal-random-variable"><i class="fas fa-link"></i></a>
</h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-24" class="theorem"><strong>Theorem 4.4  </strong></span><span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 8.3.1</p>
<p>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)\)</span>.</p>
<ol style="list-style-type: decimal">
<li>
<span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(\frac{1}{n} \sum(X_i - \overline{X})^2\)</span> are independent. (This is <strong>only</strong> true for normal random variables. You should read through the proof in your book.)</li>
<li>
<span class="math inline">\(\overline{X} \sim N(\mu, \sigma^2/n)\)</span>. (Not the CLT, why not?)</li>
<li>
<span class="math inline">\(\frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}\)</span>. (Main idea is that we only have <span class="math inline">\(n-1\)</span> independent things.)</li>
</ol>
</div>
</div>
<div id="the-t-distribution" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> The t-distribution<a class="anchor" aria-label="anchor" href="#the-t-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>Let <span class="math inline">\(Z \sim N(0,1)\)</span> and <span class="math inline">\(Y \sim \chi^2_n\)</span>. If <span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> are independent, then:</p>
<div class="definition">
<p><span id="def:unlabeled-div-25" class="definition"><strong>Definition 4.1  </strong></span><span class="math display">\[\begin{eqnarray*}
X = \frac{Z}{\sqrt{Y/n}} \sim t_n \mbox{  by definition}
\end{eqnarray*}\]</span></p>
</div>
<p><span class="math display">\[\begin{eqnarray*}
f(x) &amp;=&amp; \frac{\Gamma(\frac{n+1}{2})}{(n \pi)^{1/2} \Gamma(\frac{n}{2})} (1 + \frac{x^2}{n})^{-(n+1)/2} \ \ \ \ n &gt; 2\\
E[X] &amp;=&amp;0\\
Var(X) &amp;=&amp; \frac{n}{n-2}
\end{eqnarray*}\]</span></p>
<p>Remember:
<span class="math display">\[\begin{eqnarray*}
\frac{\overline{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1) \mbox{ independently of } \frac{\sum(X_i - \overline{X})^2}{\sigma^2} \sim \chi^2_{n-1}
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\frac{\overline{X} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{\sigma^2}/(n-1)}} &amp;=&amp; \frac{\overline{X} - \mu}{\sqrt{\frac{\sum(X_i - \overline{X})^2}{n-1}/n}}\\
&amp;=&amp; \frac{\overline{X} - \mu}{s/\sqrt{n}} \sim t_{n-1} !
\end{eqnarray*}\]</span></p>
<p>As stated above, the t-distribution is defined as the distribution which is given when a standard normal is divided by the square root of a chi-square random variable divided by its degrees of freedom. And while that may seem obtuse at first glance, it comes in extremely handy when standardizing a sample mean by using the standard error (instead of the standard deviation) of the mean.</p>
<div class="example">
<p><span id="exm:unlabeled-div-26" class="example"><strong>Example 4.2  </strong></span>According to some investors, foreign stocks have the potential for high yield, but the variability in their dividends may be greater than what is typical for American companies. Letâ€™s say we take a random sample of 10 foreign stocks; assume also that we know the population distribution from which American stocks come (i.e., we have the American parameters). If <strong>we believe that foreign stock prices are distributed similarly (normal with the same mean and variance)</strong> to American stock prices, how likely is it that a sample of 10 foreign stocks would produce a standard deviation which is 50% bigger than American stocks?</p>
<p><span class="math display">\[\begin{eqnarray*}
P(\hat{\sigma} / \sigma &gt; 1.5 ) &amp;=&amp; ?\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &amp;\sim&amp; \chi^2_{n-1} \ \ \ \ \mbox{(normality assumption)}\\
\frac{\sum (X_i - \overline{X})^2}{\sigma^2} &amp;=&amp; n\frac{\sum (X_i - \overline{X})^2/n}{\sigma^2}\\
&amp;=&amp; \frac{n \hat{\sigma^2}}{\sigma^2}\\
P(\hat{\sigma} / \sigma &gt; 1.5 ) &amp;=&amp; P(\hat{\sigma}^2 / \sigma^2 &gt; 1.5^2 ) \\
&amp;=&amp; P(n \hat{\sigma}^2 / \sigma^2 &gt; n 1.5^2 )\\
&amp;=&amp; 1 - \chi^2_{n-1} (n 1.5^2)\\
&amp;=&amp; 1 - \chi^2_{n-1} (22.5)\\
&amp;=&amp; 1 - pchisq(22.5,9) = 0.00742 \ \ \ \mbox{ in R}
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-27" class="example"><strong>Example 4.3  </strong></span>Suppose we take a random sample of foreign stocks (both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> unknown). Find the value of <span class="math inline">\(k\)</span> such that the sample mean is no more than <span class="math inline">\(k\)</span> sample standard deviations <span class="math inline">\((s)\)</span> above the mean <span class="math inline">\(\mu\)</span> with probability 0.90.</p>
<p>
Data: <span class="math inline">\(n=10\)</span>, <span class="math inline">\(\hat{\mu} = \overline{x}\)</span>, <span class="math inline">\(s^2 = \frac{\sum(x_i - \overline{x})^2}{n-1}\)</span>, <span class="math inline">\(s = \sigma'\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
P(\overline{X} &lt; \mu + k s) &amp;=&amp; 0.9\\
P\Bigg(\frac{\overline{X} - \mu}{s} &lt; k \Bigg) = P\Bigg(\frac{\overline{X} - \mu}{s/\sqrt{n}} &lt; k \sqrt{n}\Bigg) &amp;=&amp; 0.9\\
\frac{\overline{X} - \mu}{s / \sqrt{n}} &amp;\sim&amp; t_9\\
\sqrt{n} k &amp;=&amp; 1.383\\
k &amp;=&amp; \frac{1.383}{\sqrt{10}} = 0.437\\
\mbox{note, in R: } qt(0.9,9) &amp;=&amp; 1.383
\end{eqnarray*}\]</span></p>
<p>How would this problem have been different if we had known <span class="math inline">\(\sigma\)</span>? Or even if we had wanted the answer to the question in terms of number of population standard deviations?</p>
</div>
</div>
<div id="reflection-questions-3" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-3"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>
</ol>
</div>
<div id="ethics-considerations-3" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-3"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>
</ol>
</div>
<div id="r-code-tanks-example" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> R code: Tanks Example<a class="anchor" aria-label="anchor" href="#r-code-tanks-example"><i class="fas fa-link"></i></a>
</h2>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#sampdist"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="nav-link" href="#the-chi-square-distribution"><span class="header-section-number">4.1</span> The Chi-Square Distribution</a></li>
<li><a class="nav-link" href="#independence-of-the-mean-and-variance-of-a-normal-random-variable"><span class="header-section-number">4.2</span> Independence of the Mean and Variance of a Normal Random Variable</a></li>
<li><a class="nav-link" href="#the-t-distribution"><span class="header-section-number">4.3</span> The t-distribution</a></li>
<li><a class="nav-link" href="#reflection-questions-3"><span class="header-section-number">4.4</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-3"><span class="header-section-number">4.5</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r-code-tanks-example"><span class="header-section-number">4.6</span> R code: Tanks Example</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/04-sampdist.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/04-sampdist.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-09-21.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
