<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 Interval Estimates | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="Up until now, we’ve used \(\hat{\theta}\) to estimate \(\theta\). But a single numerical value gives us no information about the degree of uncertainty of the estimate. A confidence interval is a...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 6 Interval Estimates | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="Up until now, we’ve used \(\hat{\theta}\) to estimate \(\theta\). But a single numerical value gives us no information about the degree of uncertainty of the estimate. A confidence interval is a...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 Interval Estimates | Statistical Theory">
<meta name="twitter:description" content="Up until now, we’ve used \(\hat{\theta}\) to estimate \(\theta\). But a single numerical value gives us no information about the degree of uncertainty of the estimate. A confidence interval is a...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></li>
<li><a class="active" href="intest.html"><span class="header-section-number">6</span> Interval Estimates</a></li>
<li><a class="" href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></li>
<li><a class="" href="ht.html"><span class="header-section-number">8</span> Hypothesis Testing</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="intest" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Interval Estimates<a class="anchor" aria-label="anchor" href="#intest"><i class="fas fa-link"></i></a>
</h1>
<p>Up until now, we’ve used <span class="math inline">\(\hat{\theta}\)</span> to estimate <span class="math inline">\(\theta\)</span>. But a single numerical value gives us no information about the degree of uncertainty of the estimate. A confidence interval is a set of values, (A,B), that we think is likely to contain <span class="math inline">\(\theta\)</span> (the true parameter). The length of the interval gives us an idea of how closely we are able to estimate <span class="math inline">\(\theta\)</span>.</p>
<div id="frequentist-confidence-intervals" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Frequentist Confidence Intervals<a class="anchor" aria-label="anchor" href="#frequentist-confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>A frequentist confidence interval (CI) is created in such a way that the interval itself contains the parameter some specified percentage of the time.</p>
<div id="ci-for-the-mean-mu-in-a-normal-random-sample" class="section level3" number="6.1.1">
<h3>
<span class="header-section-number">6.1.1</span> CI for the mean, <span class="math inline">\(\mu\)</span> in a normal random sample<a class="anchor" aria-label="anchor" href="#ci-for-the-mean-mu-in-a-normal-random-sample"><i class="fas fa-link"></i></a>
</h3>
<p>We know,
<span class="math display">\[\begin{eqnarray*}
\frac{\overline{X} - \mu}{s / \sqrt{n}} \sim t_{n-1}
\end{eqnarray*}\]</span></p>
<p>n.b., Your text uses <span class="math inline">\(\sigma' = s = \sqrt{\frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}}.\)</span> So whenever you see <span class="math inline">\(\sigma'\)</span>, think <span class="math inline">\(s.\)</span></p>
<p>Let <span class="math inline">\(c\)</span> be some constant such that <span class="math inline">\(\int_{-c}^c f_{t_{n-1}}(x) dx = \gamma\)</span> (e.g., = 0.95). Or:</p>
<p><span class="math display">\[\begin{eqnarray*}
P(-c \leq \frac{\overline{X} - \mu}{s/\sqrt{n}} \leq c) &amp;=&amp; 0.95\\
P( \overline{X} - c s/\sqrt{n} \leq \mu \leq \overline{X} + c s/\sqrt{n} ) &amp;=&amp; 0.95\\
\end{eqnarray*}\]</span></p>
<p>What is random here? <span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(s\)</span> are both random! The probability of getting <span class="math inline">\((\overline{X}\)</span>,<span class="math inline">\(s)\)</span> such that <span class="math inline">\(\overline{X} - c s/\sqrt{n} \leq \mu \leq \overline{X} + c s/\sqrt{n}\)</span> is 0.95. What is <span class="math inline">\(c?\)</span></p>
<p>As frequentists, we don’t interpret the interval as “the probability that <span class="math inline">\(\theta\)</span> is in the interval”
<span class="math display">\[\begin{eqnarray*}
X_1, X_2, \ldots, X_n &amp;\rightarrow&amp; \mbox{random}\\
\theta &amp;\rightarrow&amp; \mbox{fixed}\\
\end{eqnarray*}\]</span></p>
<blockquote>
<p>We are 95% confident that <span class="math inline">\(\mu\)</span> is between <span class="math inline">\(\overline{X} - c s/\sqrt{n}\)</span> and <span class="math inline">\(\overline{X} + c s/\sqrt{n}\)</span></p>
</blockquote>
<p>Bayesians interpret intervals as “the probability that <span class="math inline">\(\theta\)</span> is in the interval” because Bayesians treat <span class="math inline">\(\theta\)</span> as a random variable (we would need a prior, etc…) [See the next section on Bayesian Intervals.]</p>
<div class="example">
<p><span id="exm:unlabeled-div-29" class="example"><strong>Example 6.1  </strong></span>A sample of 25 statistics students reported that they spend an average of 110 minutes per week studying statistics, with a standard deviation of 40 minutes. Find a one-sided CI such that we are 98% confident that we know the <em>lower</em> bound of the true average studying time in the population. Let’s assume that the data are reasonably independent and normally distributed.</p>
<p>We need an interval <span class="math inline">\([ A,\infty)\)</span>, such that with 98% confidence, <span class="math inline">\(\mu\)</span> is in the interval. We know:
<span class="math display">\[\begin{eqnarray*}
P(c_1 \leq \frac{\overline{X} - \mu}{s/\sqrt{n}} \leq c_2) &amp;=&amp; 0.98\\
P( \overline{X} - c_2 s/\sqrt{n} \leq \mu \leq \overline{X} - c_1 s/\sqrt{n} ) &amp;=&amp; 0.98\\
P(-\infty \leq \frac{\overline{X} - \mu}{s/\sqrt{n}} \leq c_2) &amp;=&amp; 0.98\\
\\
\frac{\overline{X} - \mu}{s/\sqrt{n}} &amp;\sim&amp; t_{24} \rightarrow c_2 = 2.172\\
P( \overline{X} - 2.172 s/\sqrt{n} \leq \mu (\leq \infty) ) &amp;=&amp; 0.98\\
( \overline{X} - 2.172 s/\sqrt{n},\infty ) &amp;&amp; \mbox{is a 98% CI}\\
\end{eqnarray*}\]</span>
We are 98% confident that the true average studying time (in the population) is at least 92.62 minutes. Why can’t we plug in the numbers above and keep it as a probability?</p>
<p>2-sided interval: <span class="math inline">\(c=2.492=c_2, c_1 = -2.492\)</span>
<span class="math display">\[\begin{eqnarray*}
( \overline{X} - 2.492 s/\sqrt{n},\overline{X} + 2.492 s/\sqrt{n} ) &amp;&amp; \mbox{is a 98% CI}\\
(90.06 \mbox{ min}, 129.94 \mbox{ min}) &amp;&amp;\\
\end{eqnarray*}\]</span></p>
<p>We are 98% confident that the average number of minutes per week spent studying in the population is between 90.06 min and 129.94 min.</p>
</div>
</div>
</div>
<div id="bayesian-intervals" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Bayesian Intervals<a class="anchor" aria-label="anchor" href="#bayesian-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Note: your book calls Bayesian intervals “posterior intervals”, so we will stick to that language. However, most people in the Bayesian literature call them <a href="https://en.wikipedia.org/wiki/Credible_interval" target="_blank">“credible intervals”</a>.</p>
<p>We’d like to say “the probability that <span class="math inline">\(\theta\)</span> is in the interval is…” As a Bayesian we can do that because Bayesians think about <span class="math inline">\(\theta\)</span> as random, and they put a distribution on <span class="math inline">\(\theta | \underline{x}\)</span>.</p>
<p>A Bayesian <em>posterior</em> or <em>credible</em> interval is given by the posterior distribution. That is, a (<span class="math inline">\(1-\alpha\)</span>)% posterior interval for <span class="math inline">\(\theta\)</span> is <span class="math display">\[\Xi^{-1}_{\alpha/2} (\theta | \underline{X}), \Xi^{-1}_{1-\alpha/2} (\theta | \underline{X})\]</span>
where <span class="math inline">\(\Xi(\theta | \underline{X})\)</span> is the posterior cumulative distribution function of <span class="math inline">\(\theta\)</span> (but maybe we should use better notation). Do not focus on the <span class="math inline">\(\Xi\)</span> cdf notation. Instead, keep in mind that the inverse cdf defines the tail probabilities associated with the posterior distribution.</p>
<div class="example">
<p><span id="exm:unlabeled-div-30" class="example"><strong>Example 6.2  </strong></span>Recall the Beta-Binomial example which aims to model the true probability that Steph Curry can make a free throw, <span class="math inline">\(\theta.\)</span> Let’s say we’ll have him shoot 25 times and record the number of times he makes the shot. Also, let’s say we have a Beta(10,2) prior which induces a prior mean of 10/12 and is quite wide.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/plotDist.html">plotDist</a></span><span class="op">(</span><span class="st">'beta'</span>, params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="fl">10</span>, <span class="fl">2</span><span class="op">)</span>, main <span class="op">=</span> <span class="st">"Beta(10,2) pdf"</span><span class="op">)</span></code></pre></div>
<p><img src="06-intest_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;">
Construct a 90% posterior interval for <span class="math inline">\(\theta.\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
X &amp;\sim&amp; \mbox{Bin}(25, \theta)\\
\theta &amp;\sim&amp; \mbox{Beta}(10, 2)\\
\theta | \underline{X} &amp;\sim&amp; \mbox{Beta}(31, 6)
\end{eqnarray*}\]</span></p>
<p>(n.b., look back to Section <a href="bayes.html#bayes">2</a> for a refresher on how to get a posterior distribution from the prior and the likelihood.)</p>
<p>To build a posterior interval, we need to solve the following equation:</p>
<p><span class="math display">\[P( \_\_\_\_\_\_\_\_\_ \leq \theta \leq \_\_\_\_\_\_\_\_\_ \ | \ \underline{X}) = 0.9\]</span></p>
<div class="sourceCode" id="cb43"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqbeta</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.05</span>, <span class="fl">0.95</span><span class="op">)</span>, <span class="fl">31</span>, <span class="fl">6</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1] 0.730 0.925</code></pre>
<p>We say: there is a 0.9 probability that the true value of <span class="math inline">\(\theta\)</span> is between (0.73, 0.92).</p>
</div>
<div id="joint-posterior-distribution-for-mu-and-sigma-in-a-normal-distribution" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> Joint Posterior Distribution for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> in a Normal Distribution<a class="anchor" aria-label="anchor" href="#joint-posterior-distribution-for-mu-and-sigma-in-a-normal-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Remember, we found the posterior distribution of <span class="math inline">\(\mu | \underline{x}\)</span> with <em>known</em> <span class="math inline">\(\sigma\)</span>. But we don’t really ever know <span class="math inline">\(\sigma\)</span>. To find a joint posterior on <span class="math inline">\(\mu, \sigma | \underline{x}\)</span>, we need two priors.</p>
<p>Only to simplify calculations, let <span class="math inline">\(\tau = 1/\sigma^2\)</span>. <span class="math inline">\(\tau\)</span> is called the precision. The joint distribution is calculated using the product of the marginal normal distributions. Note that that the data are assumed to be a random sample (i.e., they are independent and identically distributed according to the same <span class="math inline">\(N(\mu, 1/\tau)\)</span> distribution).</p>
<p><span class="math display">\[\begin{eqnarray*}
f(x | \mu, \tau) &amp;=&amp; \Bigg( \frac{\tau}{2 \pi} \Bigg) ^{1/2} exp \bigg[ \frac{-1}{2} \tau (x-\mu)^2 \bigg]\\
f(\underline{x} | \mu, \tau) &amp;=&amp; \Bigg( \frac{\tau}{2 \pi} \Bigg) ^{n/2} exp \bigg[ \frac{-1}{2} \tau \sum_{i=1}^n (x_i-\mu)^2 \bigg]\\
\end{eqnarray*}\]</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-31" class="theorem"><strong>Theorem 6.1  </strong></span>(<span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> Theorem 7.6.1)
Let <span class="math inline">\(X_1, X_2, \ldots X_n \sim N(\mu, 1/\tau)\)</span> and suppose you have <strong>priors</strong> on <span class="math inline">\(\mu|\tau\)</span> and <span class="math inline">\(\tau\)</span>,
<span class="math display">\[\begin{eqnarray*}
\mu|\tau &amp;\sim&amp; N(\mu_0, 1/(\lambda_0 \tau) )\\
\tau &amp;\sim&amp; \mbox{ Gamma} (\alpha_0, \beta_0)
\end{eqnarray*}\]</span></p>
<p>Then, the <strong>posteriors</strong> on <span class="math inline">\(\mu|\tau\)</span> and <span class="math inline">\(\tau\)</span> are,
<span class="math display">\[\begin{eqnarray*}
\mu | \tau, \underline{x} &amp;\sim&amp; N(\mu_1, 1/(\lambda_1 \tau) )\\
\tau \  | \ \underline{x}  &amp;\sim&amp; \mbox{ Gamma} (\alpha_1, \beta_1)
\end{eqnarray*}\]</span>
where <span class="math inline">\(\mu_1 = \frac{\lambda_0 \mu_0 + n \overline{x}}{\lambda_0 + n}, \ \ \ \ \lambda_1 = \lambda_0 + n, \ \ \ \ \alpha_1 = \alpha_0 + \frac{n}{2}, \ \ \ \ \ \beta_1 = \beta_0 + \frac{1}{2} \sum_{i=1}^n (x_i - \overline{x})^2 + \frac{n \lambda_0 (\overline{x} - \mu_0)^2}{2(\lambda_0 +n)}.\)</span></p>
<p>Note that the prior is a joint conjugate family of distributions. <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> have a normal-gamma distribution. Note also that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\tau\)</span> are <strong>not</strong> independent.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-32" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align}
f(\underline{x} | \mu, \tau) &amp;= \Bigg( \frac{\tau}{2\pi}\Bigg)^{n/2} exp \Bigg[ -\frac{1}{2} \tau \sum_{i=1}^n (x_i - \mu)^2 \Bigg] \nonumber \\
\xi_1(\mu|\tau) &amp;= \Bigg( \frac{\lambda_0 \tau}{2\pi}\Bigg)^{1/2} exp \Bigg[ -\frac{1}{2} \lambda_0 \tau (\mu - \mu_0)^2 \Bigg]  \nonumber \\
\xi_2(\tau) &amp;= \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \tau^{\alpha_0 - 1}e^{-\beta_0 \tau} \nonumber \\
\mbox{Note, } &amp; \mu  \mbox{ and $\tau$ are not independent, and } \xi(\mu, \tau) = \xi_1(\mu|\tau) \ \xi_2(\tau) \nonumber \\
\end{align}\]</span>
<span class="math display" id="eq:one">\[\begin{align}
\xi(\mu,\tau|\underline{x}) &amp;\propto f(\underline{x} | \mu, \tau) \ \xi_1(\mu|\tau) \ \xi_2(\tau) \nonumber \\
&amp;\propto \tau^{\alpha_0 + (n+1)/2 -1} \exp \Bigg[-\frac{\tau}{2} \Bigg(\lambda_0 [\mu-\mu_0]^2 + \sum_{i=1}^{n}(x_i -\mu)^2 \Bigg) - \beta_0 \tau \Bigg]    \tag{6.1} \end{align}\]</span></p>
<p>Add and subtract <span class="math inline">\(\overline{x}\)</span> inside <span class="math inline">\((x_i -\mu)^2\)</span> to get:</p>
<p><span class="math display" id="eq:two">\[\begin{align}
\sum_{i=1}^n(x_i -\mu)^2 &amp;= \sum_{i=1}^n(x_i - \overline{x})^2 + n(\overline{x} -\mu)^2 \tag{6.2}
\end{align}\]</span></p>
<p>By adding and subtracting <span class="math inline">\(\mu_1\)</span>:</p>
<p><span class="math display" id="eq:three">\[\begin{align}
n(\overline{x} -\mu)^2 + \lambda_0 (\mu - \mu_0)^2 &amp;= (\lambda_0 + n)(\mu - \mu_1)^2 + \frac{n\lambda_0(\overline{x} - \mu_0)^2}{\lambda_0 + n} \tag{6.3}
\end{align}\]</span></p>
<p>Combining <a href="intest.html#eq:two">(6.2)</a> and <a href="intest.html#eq:three">(6.3)</a> we get:</p>
<p><span class="math display" id="eq:four">\[\begin{align}
\sum_{i=1}^n(x_i -\mu)^2 + \lambda_0 (\mu - \mu_0)^2 = (\lambda_0 + n)(\mu - \mu_1)^2 + \sum_{i=1}^n(x_i - \overline{x})^2 + \frac{n\lambda_0(\overline{x} - \mu_0)^2}{\lambda_0 + n} \tag{6.4}
\end{align}\]</span></p>
<p>By plugging <a href="intest.html#eq:four">(6.4)</a> into <a href="intest.html#eq:one">(6.1)</a> we get:</p>
<span class="math display">\[\begin{eqnarray}
\xi(\mu, \tau | \underline{x}) &amp;\propto&amp; \Bigg\{ \tau^{1/2} exp \Bigg[ -\frac{1}{2} \lambda_1 \tau (\mu - \mu_1)^2 \Bigg] \Bigg\} (\tau^{\alpha_1 -1} e^{-\beta_1 \tau})\\
\xi(\mu, \tau | \underline{x}) &amp;=&amp; \xi_1(\mu | \tau, \underline{x}) \xi_2(\tau | \underline{x})
\end{eqnarray}\]</span>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-33" class="theorem"><strong>Theorem 6.2  </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots X_n \sim N(\mu, 1/\tau)\)</span> and suppose you have priors on <span class="math inline">\(\mu|\tau\)</span> and <span class="math inline">\(\tau\)</span>,
<span class="math display">\[\begin{eqnarray*}
\mu|\tau &amp;\sim&amp; N(\mu_0, 1/(\lambda_0 \tau) )\\
\tau &amp;\sim&amp; \mbox{ Gamma} (\alpha_0, \beta_0)
\end{eqnarray*}\]</span></p>
<p>Then, the marginal posterior distribution of <span class="math inline">\(\mu\)</span> can be written as:
<span class="math display">\[\begin{eqnarray*}
\bigg(\frac{\lambda_1 \alpha_1}{\beta_1} \bigg)^{1/2} (\mu-\mu_1) \  | \ \underline{x} \sim t_{2\alpha_1}
\end{eqnarray*}\]</span>
where <span class="math inline">\(\mu_1, \lambda_1, \alpha_1,\)</span> and <span class="math inline">\(\beta_1\)</span> are given in the previous theorem.</p>
</div>
<div class="proof">
<span id="unlabeled-div-34" class="proof"><em>Proof</em>. </span>First, let
<span class="math display">\[\begin{eqnarray*}
z &amp;=&amp; (\lambda_1 \tau)^{1/2} (\mu-\mu_1) = u(\mu)\\
\mu &amp;=&amp; z (\lambda_1 \tau)^{-1/2} + \mu_1 = w(z)
\end{eqnarray*}\]</span>
We know (from the previous theorem):
<span class="math display">\[\begin{eqnarray}
\xi(\mu,\tau|\underline{x}) &amp;=&amp; \xi_1(\mu|\tau,\underline{x}) \ \ \xi_2(\tau|\underline{x})\\
\mbox{So, } \xi(z,\tau|\underline{x}) &amp;=&amp; \xi_1(w(z)|\tau,\underline{x})\ \ \Bigg| \frac{\partial w(z)}{\partial z} \Bigg| \ \ \xi_2 (\tau|\underline{x})\\
&amp;=&amp; \xi_1(z(\lambda_1 \tau)^{-1/2} + \mu_1| \tau,\underline{x}) \ \ \big| (\lambda_1 \tau)^{-1/2} \big| \ \ \xi_2 (\tau|\underline{x})\\
&amp;=&amp; \sqrt{\frac{\lambda_1 \tau}{2 \pi}} \exp \Bigg \{ \frac{-(z(\lambda_1 \tau)^{-1/2} + \mu_1 -\mu_1)^2}{2(\lambda_1 \tau)^{-1}} \Bigg\} \ \ (\lambda_1 \tau)^{-1/2} \ \ \xi_2(\tau|\underline{x})\\
&amp;=&amp; \sqrt{\frac{1}{2\pi}} \exp(-z^2/2) \ \ \xi_2(\tau|\underline{x})\\
&amp;=&amp; \Phi(z|\underline{x}) \ \ \xi_2(\tau|\underline{x})\\
\end{eqnarray}\]</span>
Which gives us:
<span class="math display">\[\begin{eqnarray*}
Z|\underline{x} &amp;\sim&amp; N(0,1) \ \ \ \ \tau| \underline{x} \sim \mbox{Gamma}(\alpha_1,\beta_1) \ \ \mbox{  (Independent!)}\\
\mbox{Let } Y &amp;=&amp; 2\beta_1\tau \rightarrow Y|\underline{x} \sim \mbox{Gamma}(\alpha_1, 1/2) \equiv \chi^2_{2\alpha_1}
\end{eqnarray*}\]</span>
So, creating a t random variable:
<span class="math display">\[\begin{eqnarray}
U = \frac{Z}{\sqrt{Y/2\alpha_1}} = \frac{(\lambda_1 \tau)^{1/2} (\mu-\mu_1)}{\sqrt{2\beta_1\tau/2\alpha_1}} = \Bigg(\frac{\lambda_1 \alpha_1}{\beta_1} \Bigg)^{1/2}(\mu-\mu_1)
\end{eqnarray}\]</span>
Which gives:
<span class="math display">\[\begin{eqnarray}
\Bigg(\frac{\lambda_1 \alpha_1}{\beta_1} \Bigg)^{1/2}(\mu-\mu_1) \  | \ \underline{x}  \sim t_{2\alpha_1}
\end{eqnarray}\]</span>
</div>
<p>Note: <span class="math inline">\(E[ U | \underline{x} ] = 0\)</span> and <span class="math inline">\(Var(U | \underline{x}) = \frac{2 \alpha_1}{2 \alpha_1 - 2} = \frac{ \alpha_1}{\alpha_1 -1}\)</span></p>
<p><span class="math inline">\(\rightarrow E[\mu| \underline{x}] = \mu_1\)</span> and <span class="math inline">\(Var(\mu | \underline{x}) = \frac{\beta_1}{\lambda_1 \alpha_1} \frac{\alpha_1}{\alpha_1 -1} = \frac{\beta_1}{\lambda_1(\alpha_1 -1)}\)</span></p>
</div>
<div id="posterior-interval-for-the-mean-mu-in-a-normal-random-sample" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Posterior Interval for the mean, <span class="math inline">\(\mu\)</span> in a normal random sample<a class="anchor" aria-label="anchor" href="#posterior-interval-for-the-mean-mu-in-a-normal-random-sample"><i class="fas fa-link"></i></a>
</h3>
<p>Let the confidence level be <span class="math inline">\(1-\alpha\)</span>. As with frequentist CI, the interval can be built by pivoting around the value of interest, <span class="math inline">\(\mu\)</span>.
<span class="math display">\[\begin{eqnarray*}
P( -c \leq U \leq c \  | \ \underline{x} ) &amp;=&amp; 1 - \alpha\\
P( -c \leq \Bigg( \frac{\lambda_1 \alpha_1}{\beta_1} \Bigg)^{1/2} (\mu - \mu_1) \leq c \  | \ \underline{x} ) &amp;=&amp; 1 - \alpha\\
P( \mu_1 - c \Bigg(\frac{\beta_1}{\lambda_1 \alpha_1} \Bigg)^{1/2} \leq \mu \leq \mu_1 + c \Bigg(\frac{\beta_1}{\lambda_1 \alpha_1} \Bigg)^{1/2} \  | \ \underline{x} ) &amp;=&amp; 1 - \alpha\\
\end{eqnarray*}\]</span></p>
<p><span class="math inline">\(\Rightarrow\)</span>    <span class="math inline">\(\mu_1 \pm c \Bigg(\frac{\beta_1}{\lambda_1 \alpha_1} \Bigg)^{1/2}\)</span> is a (<span class="math inline">\(1 - \alpha\)</span>)100% posterior interval for <span class="math inline">\(\mu\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-35" class="example"><strong>Example 6.3  </strong></span>Let’s say we are trying to estimate the total number of soft drinks a particular vending machine will sell in a typical week. We want to find a 90% posterior interval for <span class="math inline">\(\mu\)</span>. Our prior information (e.g., from past weeks) tells us:
<span class="math display">\[\begin{eqnarray*}
\mu | \tau &amp;\sim&amp; N(750, 5 / \tau = \frac{1}{(1/5)\tau} )\\
\tau &amp;\sim&amp; gamma(1, 45)
\end{eqnarray*}\]</span>
<span class="math inline">\(\mu_0 = 750\)</span>, <span class="math inline">\(\lambda_0 = 1/5\)</span>, <span class="math inline">\(\alpha_0 = 1\)</span>, <span class="math inline">\(\beta_0=45\)</span></p>
<p>Our random sample of 10 weeks gives <span class="math inline">\(\overline{x} = 692\)</span> and <span class="math inline">\(s^2 = \frac{14400}{9} = 1600\)</span>.</p>
<p>Our posterior parameters are:
<span class="math display">\[\begin{eqnarray*}
\mu_1 &amp;=&amp; \frac{\lambda_0 \mu_0 + n \overline{x}}{\lambda_0 + n} = \frac{(1/5)750 + 6920}{(1/5) + 10} = 693.14\\
\lambda_1 &amp;=&amp; \lambda_0 + n = 10.2\\
\alpha_1 &amp;=&amp; \alpha_0 + n/2 = 1+ 5 =6\\
\beta_1 &amp;=&amp; \beta_0 + \frac{1}{2} \sum_{i=1}^n (x_i - \overline{x})^2 + \frac{n \lambda_0 (\overline{x} - \mu_0)^2}{2(\lambda_0 +n)} = 45 + 14400/2 + \frac{10(1/5) (692-750)^2}{2(1/5 +10)} = 7574.8
\end{eqnarray*}\]</span></p>
<p>To find a 90% PI, find cutoff values such that <span class="math inline">\(P(-c \leq t_{2\alpha_1} \leq c) = 0.9\)</span>. <span class="math inline">\(2 \alpha_1 =12\)</span>, <span class="math inline">\(P(t_{12} \leq 1.782) = 0.95\)</span>.
<span class="math display">\[\begin{eqnarray*}
P(-1.782 \leq U \leq 1.782  \  | \ \underline{x} ) &amp;=&amp; 0.9\\
P(\mu_1 - 1.782 (\frac{\beta_1}{\lambda_1 \alpha_1})^{1/2} \leq \mu \leq \mu_1 + 1.782 (\frac{\beta_1}{\lambda_1 \alpha_1})^{1/2} \  | \ \underline{x} ) &amp;=&amp; 0.9\\
P(673.31 \leq \mu \leq 712.97 \  | \ \underline{x} ) &amp;=&amp; 0.9
\end{eqnarray*}\]</span></p>
<p>Given our prior beliefs and data, there is a 90% probability that the average number of cans sold per week is between 673.31 and 712.97 cans.</p>
</div>
<div id="improper-priors-continued" class="section level4 unnumbered">
<h4>Improper priors, continued…<a class="anchor" aria-label="anchor" href="#improper-priors-continued"><i class="fas fa-link"></i></a>
</h4>
<p>Notice that if <span class="math inline">\(\mu_0 = \beta_0 = \lambda_0 = 0\)</span> and <span class="math inline">\(\alpha_0= -1/2\)</span>, <span class="math inline">\(\mu_1 = \overline{x}\)</span>, <span class="math inline">\(\lambda_1 = n\)</span>, <span class="math inline">\(\alpha_1 = (n-1)/2\)</span>, <span class="math inline">\(\beta_1 = \sum(x_i - \overline{x})^2 / 2\)</span>. Our interval becomes:</p>
<p><span class="math display">\[\begin{eqnarray*}
\overline{x} &amp;\pm&amp; t^* \Bigg( \frac{(1/2) \sum(x_i - \overline{x})^2}{n (n-1)/2} \Bigg)^{1/2}\\
\overline{x} &amp;\pm&amp; t^* s/\sqrt{n}
\end{eqnarray*}\]</span></p>
<p>The improper prior down-weights any beliefs and gives a frequentist-like (i.e., data only) answer with a Bayesian-like interpretation.</p>
</div>
</div>
</div>
<div id="bootstrap-confidence-intervals" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Bootstrap Confidence Intervals<a class="anchor" aria-label="anchor" href="#bootstrap-confidence-intervals"><i class="fas fa-link"></i></a>
</h2>
<p>Notice that we use the phrase “confidence intervals” when describing intervals made using the bootstrap process. That is because the logic and interpretation for bootstrap confidence intervals is the same as the theory based frequentist confidence intervals.</p>
<div id="different-bootstrap-cis" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Different Bootstrap CIs<a class="anchor" aria-label="anchor" href="#different-bootstrap-cis"><i class="fas fa-link"></i></a>
</h3>
<p>There are many ways to use the bootstrap sampling distribution to create an interval estimate for <span class="math inline">\(\theta\)</span>. We will cover a few different approaches, but keep in mind that the techniques extend beyond what we will cover here.</p>
<div id="bootstrap-confidence-interval-logic" class="section level4" number="6.3.1.1">
<h4>
<span class="header-section-number">6.3.1.1</span> Bootstrap Confidence Interval Logic<a class="anchor" aria-label="anchor" href="#bootstrap-confidence-interval-logic"><i class="fas fa-link"></i></a>
</h4>
<p>Keep in mind that what we are trying to do is approximate the sampling distribution of <span class="math inline">\(\hat{\theta}\)</span>. In fact, what we are really able to do here is to estimate the sampling distribution of <span class="math inline">\(\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}\)</span>. We hope that:
<span class="math display">\[\begin{eqnarray*}
\hat{F}\bigg(\frac{\hat{\theta}^*_b - \hat{\theta}}{\hat{SE}^*_B} \bigg) \rightarrow F\bigg(\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}\bigg)
\end{eqnarray*}\]</span></p>
<p>Recall the derivation of conventional confidence intervals (for a location parameter, like <span class="math inline">\(\mu\)</span>). We somehow know the distribution of <span class="math inline">\(\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}\)</span> (which gives the cutoff vales of <span class="math inline">\(q\)</span>), and then we isolate <span class="math inline">\(\theta\)</span> in the middle of the probability statement. Keeping in mind that the randomness in the probability statement comes from the <strong>endpoints</strong>, not the parameter.
<span class="math display">\[\begin{eqnarray*}
P\bigg(q_{(\alpha/2)} \leq \frac{\hat{\theta} - \theta}{SE(\hat{\theta})} \leq q_{(1-\alpha/2)}\bigg)&amp;=&amp; 1 - \alpha\\
P\bigg(\hat{\theta} - q_{(1-\alpha/2)} SE(\hat{\theta}) \leq \theta \leq \hat{\theta} - q_{(\alpha/2)} SE(\hat{\theta})\bigg) &amp;=&amp; 1 - \alpha\\
\end{eqnarray*}\]</span></p>
<p>That is, it’s the endpoints that are random, and we have a 0.95 probability that we’ll get a random sample which will produce endpoints which will capture the true parameter.</p>
</div>
<div id="bs-se-confidence-interval" class="section level4" number="6.3.1.2">
<h4>
<span class="header-section-number">6.3.1.2</span> BS SE Confidence Interval<a class="anchor" aria-label="anchor" href="#bs-se-confidence-interval"><i class="fas fa-link"></i></a>
</h4>
<p>We could use the BS SE within the CI formula (with standard normal values like 1.96 for 95% interval as the estimated quantiles). The problem is that such an interval will only be accurate if the distribution for <span class="math inline">\(\hat{\theta}\)</span> is reasonably normal. If there is any bias or skew, the CI will not have desired coverage levels <span class="citation">(<a href="references.html#ref-efrontibs" role="doc-biblioref">Efron and Tibshirani 1993</a>, pg 161 &amp; chp 22)</span>.</p>
<p>95% BS SE CI for <span class="math inline">\(\theta\)</span>: <span class="math display">\[\hat{\theta} \pm 1.96 \cdot \hat{SE}^*_B\]</span></p>
</div>
<div id="bs-t-confidence-interval" class="section level4" number="6.3.1.3">
<h4>
<span class="header-section-number">6.3.1.3</span> BS-t Confidence Interval<a class="anchor" aria-label="anchor" href="#bs-t-confidence-interval"><i class="fas fa-link"></i></a>
</h4>
<p>(note: the BS-t is so-named because of the process by which we create the intervals – just like t-intervals! We do not use the t-distribution at all in the BS-t intervals.)</p>
<p>Previously, the bootstrap was used to estimate the (unknown) SE.
Now consider using the bootstrap to estimate the <strong>distribution</strong> for <span class="math inline">\(\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}\)</span> (and not just the SE of <span class="math inline">\(\hat{\theta}).\)</span>
<span class="math display">\[\begin{eqnarray*}
T^*(b) &amp;=&amp; \frac{\hat{\theta}^*_b - \hat{\theta}}{\hat{SE}^*(b)}
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}^*_b\)</span> is the value of <span class="math inline">\(\hat{\theta}\)</span> for the <span class="math inline">\(b^{th}\)</span> bootstrap sample, and <span class="math inline">\(\hat{SE}^*(b)\)</span> is the estimated standard error of <span class="math inline">\(\hat{\theta}^*_b\)</span> for the <span class="math inline">\(b^{th}\)</span> bootstrap sample. The <span class="math inline">\(\alpha/2^{th}\)</span> percentile of <span class="math inline">\(T^*(b)\)</span> is estimated by the value of <span class="math inline">\(\hat{q}^*_{\alpha/2}\)</span> such that
<span class="math display">\[\begin{eqnarray*}
\frac{\# \{T^*(b) \leq \hat{q}^*_{\gamma} \} }{B} = \gamma
\end{eqnarray*}\]</span></p>
<p>For example, if <span class="math inline">\(B=1000\)</span>, the estimate of the 5% point is the <span class="math inline">\(50^{th}\)</span> smallest value of the <span class="math inline">\(T^*(b)\)</span>s, and the estimate of the 95% point is the <span class="math inline">\(950^{th}\)</span> smallest value of the <span class="math inline">\(T^*(b)\)</span>s (two values that would be used to create a 90% interval).</p>
<p>Why does it make sense to estimate <span class="math inline">\(q_\gamma\)</span> using <span class="math inline">\(\hat{q}^*_{\gamma}?\)</span> Recall that the main condition of bootstrapping is:</p>
<p><span class="math display">\[\begin{eqnarray*}
\hat{F}\bigg(\frac{\hat{\theta}^*_b - \hat{\theta}}{\hat{SE}^*_B} \bigg) \rightarrow F\bigg(\frac{\hat{\theta} - \theta}{SE(\hat{\theta})}\bigg)
\end{eqnarray*}\]</span></p>
<p>That is, the needed distribution can be approximated by the bootstrap distribution. The remaining steps include the algebra which put the parameter in the center and the functions of the data as the endpoints of the interval.</p>
<p>Finally, the bootstrap-t confidence interval is:
<span class="math display" id="eq:BSt">\[\begin{align}
(\hat{\theta} - \hat{q}^*_{1-\alpha/2}\hat{SE}^*_B,  \hat{\theta} - \hat{q}^*_{\alpha/2}\hat{SE}^*_B) \tag{6.5}
\end{align}\]</span></p>
<p>Note that the multiplier <span class="math inline">\((q^*)\)</span> is given by <span class="math inline">\(T^*(b)\)</span> which requires a separate estimate of the SE for each bootstrap sample. Accordingly, to find a bootstrap-t interval, we have to bootstrap twice, see Figure <a href="intest.html#fig:BSt">6.1</a>. The algorithm is as follows:</p>
<ol style="list-style-type: decimal">
<li>Generate <span class="math inline">\(B\)</span> bootstrap samples, and for each sample <span class="math inline">\(\underline{X}^{*b}\)</span> compute the bootstrap estimate <span class="math inline">\(\hat{\theta}^*_b\)</span>.</li>
<li>
<span class="math inline">\(\rightarrow\)</span> Take <span class="math inline">\(M\)</span> bootstrap samples from <span class="math inline">\(\underline{X}^{*b}\)</span>, and estimate the standard error, <span class="math inline">\(\hat{SE}^*(b)\)</span>.</li>
<li>Find <span class="math inline">\(B\)</span> values for <span class="math inline">\(T^*(b)\)</span>. Calculate <span class="math inline">\(\hat{q}^*_\alpha\)</span> and <span class="math inline">\(\hat{q}^*_{1-\alpha}\)</span>.</li>
<li>Calculate the CI as in equation <a href="intest.html#eq:BSt">(6.5)</a>.</li>
</ol>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:BSt"></span>
<img src="figs/BStflow.png" alt="From each of the B bootstrap samples, take an additional M bootstrap samples.  The goal is to estimate the SE of the statistics B times." width="80%"><p class="caption">
Figure 6.1: From each of the B bootstrap samples, take an additional M bootstrap samples. The goal is to estimate the SE of the statistics B times.
</p>
</div>
<p>Reflecting on the BS-t interval process, we have the following considerations:</p>
<ul>
<li>If <span class="math inline">\(B\cdot \alpha\)</span> is not an integer, use <span class="math inline">\(k=\lfloor (B+1) \alpha \rfloor\)</span> and <span class="math inline">\(B+1-k\)</span>.</li>
<li>Bootstrap-t intervals are somewhat erratic and can be influenced by a few outliers. Percentile methods are more reliable when the sampling distribution for <span class="math inline">\(\hat{\theta}\)</span> is reasonably normal.</li>
<li>
<span class="math inline">\(B=100\)</span> or 200 is usually not enough for a bootstrap-t CI. However, <span class="math inline">\(M=25\)</span> may be enough to estimate the SE. <span class="math inline">\(B=1000\)</span> is needed for computing percentiles from either <span class="math inline">\(\hat{\theta}^*_b\)</span> or of <span class="math inline">\(\hat{q}^*\)</span>.</li>
<li>In choosing the appropriate multiplier:
<ul>
<li>The normal multiplier (<span class="math inline">\(Z\)</span>) is more general, and when appropriate to use, it works for all <span class="math inline">\(n\)</span> and all samples.</li>
<li>The bootstrap-t multiplier is good for <em>this</em> sample only.</li>
</ul>
</li>
<li>The resulting intervals will typically not be symmetric (that is <span class="math inline">\(\hat{q}^*_\alpha \ne - \hat{q}^*_{1-\alpha}\)</span>). The asymmetry is part of the improvement over <span class="math inline">\(Z\)</span> or <span class="math inline">\(t\)</span> intervals.</li>
<li>Bootstrap-t intervals are good for location statistics (mean, quantiles, trimmed means) but cannot be trusted for other statistics like the correlation.</li>
</ul>
</div>
<div id="bootstrap-percentile-confidence-intervals" class="section level4" number="6.3.1.4">
<h4>
<span class="header-section-number">6.3.1.4</span> Bootstrap Percentile Confidence Intervals<a class="anchor" aria-label="anchor" href="#bootstrap-percentile-confidence-intervals"><i class="fas fa-link"></i></a>
</h4>
<p>The interval between the <span class="math inline">\(\alpha/2\)</span> and <span class="math inline">\(1-\alpha/2\)</span> quantiles of the bootstrap distribution of a statistic is a <span class="math inline">\((1-\alpha)100\)</span>% bootstrap percentile confidence interval for the corresponding parameter:
<span class="math display">\[\begin{eqnarray*}
[\hat{\theta}^*_{\alpha/2}, \hat{\theta}^*_{1-\alpha/2}]  = [F^{-1}_{\hat{\theta}^*} (\alpha/2), F^{-1}_{\hat{\theta}^*} (1 - \alpha/2)]
\end{eqnarray*}\]</span></p>
<p>Why does it work? It isn’t immediately obvious that the interval above will capture the true parameter, <span class="math inline">\(\theta\)</span>, at a rate or 95%. Consider a skewed sampling distribution. If your <span class="math inline">\(\hat{\theta}\)</span> comes from the long tail, is it obvious that the short tail side of your CI will get up to the true parameter value at the correct rate? (<span class="citation">Hall (<a href="references.html#ref-hall" role="doc-biblioref">1992</a>)</span> refers to these as Efron’s “backwards” intervals.) <!-- Or, if your sampling distribution is biased, the percentiles of the bootstrap interval won't capture the parameter with the correct rate.--></p>
<p>To see how / why percentiles intervals work, we first start by considering normal sampling distributions for a function of our statistic. Let <span class="math inline">\(\phi = g(\theta), \hat{\phi} = g(\hat{\theta}), \hat{\phi}^* = g(\hat{\theta}^*)\)</span>, where <span class="math inline">\(g\)</span> is a monotonic function (assume without loss of generality that <span class="math inline">\(g\)</span> is increasing). The point is to choose (if possible) <span class="math inline">\(g(\cdot)\)</span> such that
<span class="math display" id="eq:phidist">\[\begin{eqnarray}
\hat{\phi}^* - \hat{\phi} \sim \hat{\phi} - \phi \sim N(0, \sigma^2). \tag{6.6}
\end{eqnarray}\]</span>
Again, consider the logic for the conventional frequentist confidence interval. Because <span class="math inline">\(\hat{\phi} - \phi \sim N(0, \sigma^2)\)</span>, the interval for <span class="math inline">\(\theta\)</span> is created by:
<span class="math display" id="eq:phiint">\[\begin{eqnarray}
P( (\hat{\phi} - \phi ) / \sigma &gt; z_\alpha) &amp;=&amp; 1 - \alpha \nonumber\\
P( \hat{\phi} - z_\alpha \sigma &gt; \phi) &amp;=&amp; 1 - \alpha \nonumber\\
\mbox{Interval for } \phi &amp;&amp; (-\infty, \hat{\phi} + \sigma z_{1-\alpha}) \nonumber \\
\mbox{Interval for } \theta &amp;&amp; (-\infty, g^{-1}(\hat{\phi} + \sigma z_{1-\alpha})) \tag{6.7}
\end{eqnarray}\]</span>
where <span class="math inline">\(z_{1-\alpha}\)</span> is the <span class="math inline">\(100(1-\alpha)\)</span> percent point of the standard normal distribution. Note that the interval for <span class="math inline">\(\theta\)</span> in equation <a href="intest.html#eq:phiint">(6.7)</a> is problematic in that it requires knowledge of <span class="math inline">\(g\)</span> and <span class="math inline">\(\sigma.\)</span> (Also, it doesn’t use any of the power of the bootstrap.)</p>
<p>Equation <a href="intest.html#eq:phidist">(6.6)</a> implies that <span class="math inline">\(\hat{\phi} + \sigma z_{1-\alpha} = F^{-1}_{\hat{\phi}^*}(1-\alpha)\)</span>. [Why? A picture goes a long way here: draw a normal curve describing the sampling distribution of <span class="math inline">\(\hat{\phi}^* - \hat{\phi}\)</span>.] Because of the assumption in equation <a href="intest.html#eq:phidist">(6.6)</a>, we know the following:</p>
<ul>
<li>
<span class="math inline">\(95^{th}\)</span> percentile of <span class="math inline">\(\hat{\phi}^*- \hat{\phi}\)</span> is <span class="math inline">\(\sigma \cdot z_{1-\alpha}\)</span>
</li>
<li>
<span class="math inline">\(95^{th}\)</span> percentile of <span class="math inline">\(\hat{\phi}^*\)</span> is <span class="math inline">\(\hat{\phi} + \sigma \cdot z_{1-\alpha}\)</span>
</li>
<li>
<span class="math inline">\(95^{th}\)</span> percentile of <span class="math inline">\(g(\hat{\theta}^*)\)</span> is <span class="math inline">\(\hat{\phi} + \sigma \cdot z_{1-\alpha}\)</span>
</li>
<li>
<span class="math inline">\(95^{th}\)</span> percentile of <span class="math inline">\(\hat{\theta}^*\)</span> is <span class="math inline">\(g^{-1}(\hat{\phi} + \sigma \cdot z_{1-\alpha})\)</span>
</li>
</ul>
<p>Using probability statements instead of percentiles, we get the following. Note that the key here is that we want <span class="math inline">\(\theta\)</span> in the middle of the probability statement (not <span class="math inline">\(\hat{\theta}),\)</span> and that happens through leveraging the normal distribution and condition of the distribution (equation <a href="intest.html#eq:phidist">(6.6)</a>). Note that <span class="math inline">\(z_\alpha = - z_{1-\alpha}.\)</span></p>
<p><span class="math display">\[\begin{align}
1-\alpha &amp;= P( z_\alpha &lt;(\hat{\phi} - \phi ) / \sigma  ) \\
&amp;= P(\phi &lt; \hat{\phi} - z_\alpha \sigma  ) \\
&amp;= P( \phi &lt;F^{-1}_{\hat{\phi}^*}(1-\alpha) ) \\
&amp;= P(g^{-1}(\phi)  &lt;g^{-1}(F^{-1}_{\hat{\phi}^*}(1-\alpha))  ) \\
&amp;= P(\theta &lt; F^{-1}_{\hat{\theta}^*}(1-\alpha) ) \\
\end{align}\]</span></p>
<p>which results in the percentile interval for <span class="math inline">\(\theta\)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Proof from &lt;span class="citation"&gt;Carpenter and Bithell (&lt;a href="references.html#ref-carpenter2000" role="doc-biblioref"&gt;2000&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;'><sup>8</sup></a>,
<span class="math display">\[\begin{eqnarray}
(-\infty, F^{-1}_{\hat{\theta}^*}(1-\alpha)).
\end{eqnarray}\]</span></p>
<p>As a tangible example of how a statistic might have a transformation resulting in a normal sampling distribution, consider the sample correlation. Fisher worked on the correlation coefficient and had two interesting results.</p>
<div class="example">
<p><span id="exm:unlabeled-div-36" class="example"><strong>Example 6.4  </strong></span>Consider a population of 82 law schools. Two measurements were made on the entering class of each school (in 1973!). LSAT, the average score for the class on a national law test, and GPA, the average undergraduate grade-point average for the class. A random sample of 15 schools is selected from the population, and the correlation between GPA and LSAT score was found to be 0.776.</p>
<p>In a perfect world, how would we then proceed to think about this problem? What do we want to know? What do we want to say about the population? <strong>We’d want to know the <em>sampling distribution</em> of r…<span class="math inline">\(\hat{\theta}.\)</span></strong></p>
</div>
<ol style="list-style-type: decimal">
<li><p>Fisher (1915) proved that the expected value of the correlation coefficient based on random sampling from a normal population is approximately:
<span class="math display">\[\begin{eqnarray*}
E[r] = \rho - \frac{\rho(1-\rho^2)}{2n}
\end{eqnarray*}\]</span>
Solving for <span class="math inline">\(\rho\)</span> gives an approximately unbiased estimator of the population correlation:
<span class="math display">\[\begin{eqnarray*}
\hat{\rho} = r \Big[ 1 + \frac{(1-r^2)}{2n} \Big]
\end{eqnarray*}\]</span>
Further work has been done (Fisher (1915), Kenny and Keeping (1951), Sawkins (1944), and Olkin and Pratt (1958)) the later which recommend using
<span class="math display">\[\begin{eqnarray*}
\hat{\rho} = r \Big[ 1 + \frac{(1-r^2)}{2(n-3)} \Big]
\end{eqnarray*}\]</span>
Note that the bias decreases as <span class="math inline">\(n\)</span> increases and as <span class="math inline">\(\rho\)</span> approaches zero. Note also that if the data are distributed normally:
<span class="math display">\[\begin{eqnarray*}
SE(r) = \frac{(1-\rho^2)}{\sqrt{n-2}}
\end{eqnarray*}\]</span>
(No other data distribution leads to a simple formula for the SE of the correlation.)</p></li>
<li><p>Fisher also introduced the <span class="math inline">\(r\)</span> to <span class="math inline">\(Z\)</span> transformation:
<span class="math display">\[\begin{eqnarray*}
Z = \frac{1}{2} \ln \Big[\frac{1+r}{1-r}\Big]
\end{eqnarray*}\]</span>
We think of this as the non-linear transformation that normalizes the sampling distribution of r. (Note: it is an inverse hyperbolic tangent function.)</p></li>
</ol>
<p>What do we see from Fisher? That (if the data are normal), there exists a transformation that normalizes the sampling distribution of the correlation!!</p>
<p>The formal proof which derives the percentile intervals is given in:<br><strong>Percentile interval lemma</strong> <span class="citation">(<a href="references.html#ref-efrontibs" role="doc-biblioref">Efron and Tibshirani 1993</a>, pg 173)</span> Suppose the transformation for <span class="math inline">\(\hat{\phi} = m(\hat{\theta})\)</span> perfectly normalizes the distribution of <span class="math inline">\(\hat{\theta}\)</span>:
<span class="math display">\[\begin{eqnarray*}
\hat{\phi} \sim N (\phi, 1)
\end{eqnarray*}\]</span>
Then the percentile interval based on <span class="math inline">\(\hat{\theta}\)</span> equals <span class="math inline">\([m^{-1}(\hat{\phi} - z_{1-\alpha/2} ), m^{-1}(\hat{\phi} - z_{\alpha/2} )]\)</span>.</p>
<p>And we can approximate <span class="math inline">\([m^{-1}(\hat{\phi} - z_{1-\alpha/2} ), m^{-1}(\hat{\phi} - z_{\alpha/2} )]\)</span> using <span class="math inline">\([\hat{\theta}^*_{\alpha/2}, \hat{\theta}^*_{1-\alpha/2}]\)</span></p>
<p>In order for a percentile interval to be appropriate, we simply need to know that a normalizing transformation exists. We do not need to actually find the transformation! [In complete disclosure, the transformation doesn’t have to be to a normal distribution. But it must be a monotonic transformation to a distribution which is symmetric about zero.]</p>
<div class="example">
<p><span id="exm:unlabeled-div-37" class="example"><strong>Example 6.5  </strong></span>From Charlotte Chang’s (Prof Chang in Bio!) example on bootstrapping a loess smooth for her thesis data. <!--(carryingcap.r)--> The idea was this: Prof Chang had some data that she wanted to model (using differential equations, DE). She asked me how to tell whether or not her new model was reflective of the data / population. The model was applied in repeated simulations and can be seen with the black / red lines. Additionally, we fit a loess spline to see the shape of the data (blue line). Then we bootstrapped the data and fit 1000 more loess splines. Using the percentile CI method, we created a CI for a population loess spline fit (green lines). The DE model (confidence within the red bounds) doesn’t seem to be a terrible fit, but it definitely seems to say something different about the relationship between year and pipits than the data / bootstrapping (confidence within the green bounds).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="figs/carrycap.png" alt="Model of number of birds per route as a function of time.  Blue line is loess fit; green lines are bootstrap CI for the loess fit.  The differential expression model is given by the red CI bounds under model simulation (not bootstrapping)." width="80%"><p class="caption">
Figure 1.4: Model of number of birds per route as a function of time. Blue line is loess fit; green lines are bootstrap CI for the loess fit. The differential expression model is given by the red CI bounds under model simulation (not bootstrapping).
</p>
</div>
</div>
<p>Keeping in mind that the theory we’ve covered here doesn’t exactly work for this situation (our work has been on simple parameter estimation), you can imagine that many of the ideas we’ve talked about do apply to Charlotte’s situation. To do a more precise analysis, we need to be careful about multiple comparisons and non-independent data values.</p>
<!--
\iffalse
\begin{eqnarray*}
P\bigg(q_{(\alpha/2)} \leq \frac{\hat{\theta} - \theta}{SE(\theta)} \leq q_{(1-\alpha/2)}\bigg)&=& 1 - \alpha\\
P\bigg(\hat{\theta} - q_{(1-\alpha/2)} SE(\theta) \leq \theta \leq \hat{\theta} - q_{(\alpha/2)} SE(\theta)\bigg) &=& 1 - \alpha\\
\end{eqnarray*}
Let's think about the endpoints in a way that is convenient for bootstrap CIs.  Let $\hat{\theta}^*$ indicate a random variable drawn from the distribution $N(\hat{\theta}, \hat{SE}^2)$.  That is, assume (for a minute) that the true sampling distribution for $\hat{\theta}$ (and for $\hat{\theta}^*$) is normal.
\begin{eqnarray*}
\hat{\theta}^* \sim N(\hat{\theta}, \hat{SE}^2)
\end{eqnarray*}
Then $\hat{\theta}_{lo} = \hat{\theta} - q_{(1-\alpha/2)} \hat{SE} \approx \hat{\theta} - q_{(1-\alpha/2)} SE(\theta)$ and $\hat{\theta}_{up} = \hat{\theta} - q_{(\alpha/2)} \hat{SE} \approx \hat{\theta} - q_{(\alpha/2)} SE(\theta)$ are the $100\alpha^{th}$ and $100(1-\alpha)^{th}$ percentiles for the distribution of $\hat{\theta}^*$.   In other words:
\begin{eqnarray*}
\hat{\theta}_{lo} &=& \hat{\theta}^*_\alpha = 100 \alpha^{th} \mbox{ percentile of the distribution for } \hat{\theta}^*\\
\hat{\theta}_{up} &=& \hat{\theta}^*_{1-\alpha} = 100 (1-\alpha)^{th} \mbox{ percentile of the distribution for } \hat{\theta}^*\\
\end{eqnarray*}

That is, if in fact $\hat{\theta}^*$ has a normal distribution, the percentiles will equal the appropriate CI bounds, and so we know that the probability theory will hold.
\fi
-->
</div>
<div id="bca-confidence-interval" class="section level4" number="6.3.1.5">
<h4>
<span class="header-section-number">6.3.1.5</span> BCa Confidence Interval<a class="anchor" aria-label="anchor" href="#bca-confidence-interval"><i class="fas fa-link"></i></a>
</h4>
<p>(Fall 2022: we won’t cover BCa intervals.)</p>
<p>In the percentile method, we’ve assumed that there exists a transformation of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\phi(\theta)\)</span>, such that
<span class="math display">\[\begin{eqnarray*}
\phi(\hat{\theta}) - \phi(\theta) \sim N(0,1)
\end{eqnarray*}\]</span>
The transformation assumes that neither <span class="math inline">\(\theta\)</span> nor <span class="math inline">\(\phi\)</span> are biased, and it assumes that the variance is constant for all values of the parameter. That is, in the percentage intervals, we assume the normalizing transformation creates a sampling distribution that is unbiased and variance stabilizing. Consider a monotone transformation that <em>normalizes</em> the sampling distribution (we no longer assume unbiased or constant variance).</p>
<p>We now consider the case where <span class="math inline">\(\theta\)</span> is a biased estimator. That is:
<span class="math display">\[\begin{eqnarray*}
\frac{\phi(\hat{\theta}) - \phi(\theta)}{c} \sim N(-z_0,1)
\end{eqnarray*}\]</span>
We’ve corrected for the bias, but if there is non-constant variance, we need a further adjustment to stabilize the variance:</p>
<p><span class="math display">\[\begin{eqnarray*}
\phi(\hat{\theta}) - \phi(\theta) \sim N(-z_0 \sigma_\phi,\sigma_\phi), \ \ \ \ \ \ \sigma_\phi = 1 + a \phi
\end{eqnarray*}\]</span>
That is, there must exist a monotone transformation <span class="math inline">\(\phi\)</span> such that <span class="math inline">\(\phi(\hat{\theta}) \sim N\)</span> where
<span class="math display">\[\begin{eqnarray*}
E(\phi(\hat{\theta})) = \phi(\theta) - z_0 [1 + a \phi(\theta)] &amp;&amp; SE(\phi(\hat{\theta})) = 1 + a \phi(\theta)
\end{eqnarray*}\]</span>
(Note: in the expected value and SE we’ve assumed that <span class="math inline">\(c=1\)</span>. If <span class="math inline">\(c\ne1\)</span>, then we can always choose a different transformation, <span class="math inline">\(\phi'\)</span> so that <span class="math inline">\(c=1\)</span>.) Then
<span class="math display">\[\begin{eqnarray*}
P(z_{\alpha/2} \leq \frac{\phi(\hat{\theta}) - \phi(\theta)}{1 + a \phi(\theta)} + z_0 \leq z_{1-\alpha/2}) = 1 - \alpha
\end{eqnarray*}\]</span>
A (<span class="math inline">\(1-\alpha\)</span>)100% CI for <span class="math inline">\(\phi(\theta)\)</span> is
<span class="math display">\[\begin{eqnarray*}
\bigg[ \frac{\phi(\hat{\theta}) - (z_{1-\alpha/2} - z_0)}{1 + a (z_{1-\alpha/2} - z_0)}, \frac{\phi(\hat{\theta}) - (z_{\alpha/2} - z_0)}{1 + a (z_{\alpha/2} - z_0)} \bigg]
\end{eqnarray*}\]</span>
Let’s consider an interesting probability question:
<span class="math display">\[\begin{eqnarray*}
P\bigg( \phi(\hat{\theta}^*) &amp;\leq&amp; \frac{\phi(\hat{\theta}) - (z_{1-\alpha/2} - z_0)}{(1 + a (z_{1-\alpha/2} - z_0))} \bigg) = ?\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} &amp;\leq&amp; \frac{\phi(\hat{\theta}) - (z_{1-\alpha/2} - z_0) - \phi(\hat{\theta}) - \phi(\hat{\theta})a(z_{1-\alpha/2} - z_0)}{(1 + a (z_{1-\alpha/2} - z_0))(1+a \phi(\hat{\theta}))} \bigg)\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} &amp;\leq&amp; \frac{ - (z_{1-\alpha/2} - z_0) - \phi(\hat{\theta})a(z_{1-\alpha/2} - z_0)}{(1 + a (z_{1-\alpha/2} - z_0))(1+a \phi(\hat{\theta}))} \bigg)\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} &amp;\leq&amp; \frac{ -(1+a \phi(\hat{\theta})) (z_{1-\alpha/2} - z_0) }{(1 + a (z_{1-\alpha/2} - z_0))(1+a \phi(\hat{\theta}))} \bigg)\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} &amp;\leq&amp; \frac{ - (z_{1-\alpha/2} - z_0) }{(1 + a (z_{1-\alpha/2} - z_0))} \bigg)\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} &amp;\leq&amp; \frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} \bigg)\\
= P\bigg( \frac{\phi(\hat{\theta}^*) - \phi(\hat{\theta})}{1 + a \phi(\hat{\theta})} + z_0 &amp;\leq&amp; \frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} + z_0 \bigg)\\
= P\bigg( Z &amp;\leq&amp; \frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} + z_0 \bigg) = \gamma_1\\
\mbox{where } \gamma_1 &amp;=&amp; \Phi \bigg(\frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} + z_0 \bigg)\\
&amp;=&amp; \verb;pnorm; \bigg(\frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} + z_0 \bigg)
\end{eqnarray*}\]</span></p>
<p>What we’ve shown is that the <span class="math inline">\(\gamma_1\)</span> quantile of the <span class="math inline">\(\phi(\hat{\theta}^*)\)</span> sampling distribution will be a good estimate for the lower bound of the confidence interval for <span class="math inline">\(\phi(\theta)\)</span>. Using the same argument on the upper bound, we find a <span class="math inline">\((1-\alpha)\)</span> 100% confidence interval for <span class="math inline">\(\phi(\theta)\)</span> to be:</p>
<p><span class="math display">\[\begin{eqnarray*}
&amp;&amp;[\phi(\hat{\theta}^*)_{\gamma_1}, \phi(\hat{\theta}^*)_{\gamma_2}]\\
&amp;&amp; \\
\mbox{where } \gamma_1 &amp;=&amp; \Phi\bigg(\frac{ (z_{\alpha/2} + z_0) }{(1 - a (z_{\alpha/2} + z_0))} + z_0 \bigg)\\
\gamma_2 &amp;=&amp; \Phi \bigg(\frac{ (z_{1-\alpha/2} + z_0) }{(1 - a (z_{1-\alpha/2} + z_0))} + z_0 \bigg)\\
\end{eqnarray*}\]</span></p>
<p>Using the transformation respecting property of percentile intervals, we know that a <span class="math inline">\((1-\alpha)\)</span> 100% confidence interval for <span class="math inline">\(\theta\)</span> is:</p>
<p><span class="math display">\[\begin{eqnarray*}
&amp;&amp;[\hat{\theta}^*_{\gamma_1}, \hat{\theta}^*_{\gamma_2}]
\end{eqnarray*}\]</span></p>
<p><strong>How do we estimate <span class="math inline">\(a\)</span> and <span class="math inline">\(z_0\)</span>?</strong></p>
<p><strong>bias:</strong>
<span class="math inline">\(z_0\)</span> is a measure of the bias. Recall:
<span class="math display">\[\begin{eqnarray*}
bias &amp;=&amp; E(\hat{\theta}) - \theta\\
\hat{bias} &amp;=&amp; \hat{\theta}^* - \hat{\theta}\\
\end{eqnarray*}\]</span></p>
<p>But remember that <span class="math inline">\(z_0\)</span> represents the bias for <span class="math inline">\(\phi(\hat{\theta})\)</span>, not for <span class="math inline">\(\hat{\theta}\)</span> (and we don’t know <span class="math inline">\(\phi\)</span>!). So, we use <span class="math inline">\(\theta\)</span> to see what proportion of <span class="math inline">\(\theta\)</span> values are too low, and we can map it back to the <span class="math inline">\(\phi\)</span> space using the normal distribution:
<span class="math display">\[\begin{eqnarray*}
\hat{z}_0 &amp;=&amp; \Phi^{-1} \bigg( \frac{ \# \hat{\theta}^*_b &lt; \hat{\theta}}{B} \bigg)
\end{eqnarray*}\]</span>
That is, if <span class="math inline">\(\hat{\theta}^*\)</span> underestimates <span class="math inline">\(\hat{\theta}\)</span>, then <span class="math inline">\(\hat{\theta}\)</span> likely underestimates <span class="math inline">\(\theta\)</span>; <span class="math inline">\(z_0 &gt; 0\)</span>. We think of <span class="math inline">\(z_0\)</span> and the normal quantile associated with the proportion of BS replicates less than <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p><strong>skew:</strong>
<span class="math inline">\(a\)</span> is a measure of skew.
<span class="math display">\[\begin{eqnarray*}
bias&amp;=&amp; E(\hat{\theta} - \theta)\\
var &amp;=&amp; E(\hat{\theta} - \theta)^2 = \sigma^2\\
skew &amp;=&amp; E(\hat{\theta} - \theta)^3 / \sigma^3\\
\end{eqnarray*}\]</span>
We can think of the skew as the rate of chance of the standard error on a normalized scale. If there is no skew, we will estimate <span class="math inline">\(a=0\)</span>. Our estimate of <span class="math inline">\(a\)</span> comes from a procedure known as the jackknife.
<span class="math display">\[\begin{eqnarray*}
\hat{a} = \frac{\sum_{i=1}^n (\hat{\theta} - \hat{\theta}_{(i)})^3}{6 [ \sum_{i=1}^n (\hat{\theta} - \hat{\theta}_{(i)})^2 ] ^{3/2}}
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="what-makes-a-ci-good" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> What makes a CI good?<a class="anchor" aria-label="anchor" href="#what-makes-a-ci-good"><i class="fas fa-link"></i></a>
</h3>
<p><strong>The transformation respecting property</strong> A CI is transformation respecting if, for any monotone transformation, the CI for the transformed parameter is simply the transformed CI for the unstransformed parameter. Let <span class="math inline">\(\phi = m(\theta)\)</span>.
<span class="math display">\[\begin{eqnarray*}
[\phi_{lo}, \phi_{up}] = [m(\theta_{lo}), m(\theta_{up})]
\end{eqnarray*}\]</span>
Note that the idea has to do with the process of creating the CI. That is, if we create the confidence interval using <span class="math inline">\(\phi\)</span>, we’ll get the same thing as if we created the CI using <span class="math inline">\(\theta\)</span> and then transformed it. It is straightforward to see that the percentile CI is transformation respecting. That is, for any monotone transformation of the statistic and parameter, the CI will be transformed appropriately.</p>
<p>Let
<span class="math display">\[\begin{eqnarray*}
\hat{\phi} &amp;=&amp; 0.5 \ln\bigg(\frac{1+r}{1-r}\bigg)\\
r &amp;=&amp;\frac{e^{2\phi}+1}{e^{2\phi}-1}\\
\end{eqnarray*}\]</span></p>
<p>We know that <span class="math inline">\(\hat{\phi}\)</span> does have an approximate normal distribution. So, the percentile CI for <span class="math inline">\(\phi\)</span> will approximate the normal theory CI which we know to be correct (for a given <span class="math inline">\(\alpha).\)</span> But once we have a CI for <span class="math inline">\(\phi\)</span> we can find the CI for <span class="math inline">\(\rho\)</span> by taking the inverse monotonic transformation; or rather… we can just use the r percentile CI to start with!</p>
<p><strong>The range preserving property</strong> Another advantage of the percentile interval is that it is range preserving. That is, the CI always produces endpoints that fall within the allowable range of the parameter.</p>
<p><strong>Bias</strong> The percentile interval is not, however, perfect. If the statistic is a biased estimator of the parameter, there will not exist a transformation such that the distribution is centered around the correct function of the parameter. Formally, if
<span class="math display">\[\begin{eqnarray*}
\hat{\theta} \sim N(\theta + bias, \hat{SE}^2)
\end{eqnarray*}\]</span>
no transformation <span class="math inline">\(\phi = m(\theta)\)</span> can fix things up. Keep in mind that standard intervals can fail in a variety of ways, and the percentile method has simply fixed the situation when the distribution is non-normal.</p>
<ul>
<li>Symmetry (important??): the interval is symmetric, pivotal around some value. Not necessarily a good thing. Maybe a bad thing to force?</li>
<li>Robust: BS-t is particularly not robust (can make it more robust with a variance stabilizing transformation)</li>
<li>Range preserving: the CI always contains only values that fall within an allowable range (<span class="math inline">\(p, \rho\)</span>,…)</li>
<li>Transformation respecting: for any monotone transformation, <span class="math inline">\(\phi = m(\theta)\)</span>, the interval for <span class="math inline">\(\theta\)</span> is mapped by <span class="math inline">\(m(\theta)\)</span>. If <span class="math inline">\([\hat{\theta}_{(lo)},\hat{\theta}_{(hi)}]\)</span> is a <span class="math inline">\((1-\alpha)100\)</span>% interval for <span class="math inline">\(\theta\)</span>, then
<span class="math display">\[\begin{eqnarray*}
[\hat{\phi}_{(lo)},\hat{\phi}_{(hi)}] = [m(\hat{\theta}_{(lo)}),m(\hat{\theta}_{(hi)})]
\end{eqnarray*}\]</span>
are exactly the same interval.</li>
<li>Correct level of confidence: A central (not symmetric) confidence interval, <span class="math inline">\([\hat{\theta}_{(lo)},\hat{\theta}_{(hi)}]\)</span> should have probability <span class="math inline">\(\alpha/2\)</span> of not covering <span class="math inline">\(\theta\)</span> from above or below:
<span class="math display">\[\begin{eqnarray*}
P(\theta &lt; \hat{\theta}_{(lo)})&amp;=&amp;\alpha/2\\
P(\theta &gt; \hat{\theta}_{(hi)})&amp;=&amp;\alpha/2\\
\end{eqnarray*}\]</span>
</li>
</ul>
<p>Note: all of the bootstrap intervals are approximate. We judge them based on how accurately they cover <span class="math inline">\(\theta\)</span>. (<span class="math inline">\(const_{lo}\)</span> and <span class="math inline">\(const_{hi}\)</span> are constant values that will vary depending on the data, statistic, and method. The constants will not necessarily be the same for the upper miss-rate and the lower miss-rate.) The point is whether the coverage rate converges at a rate of <span class="math inline">\(1/n\)</span> or a rate of <span class="math inline">\(1/\sqrt{n}.\)</span></p>
<p>A CI is <strong>first order accurate</strong> if:
<span class="math display">\[\begin{eqnarray*}
P(\theta &lt; \hat{\theta}_{(lo)})&amp;=&amp;\alpha/2 + \frac{const_{lo}}{\sqrt{n}}\\
P(\theta &gt; \hat{\theta}_{(hi)})&amp;=&amp;\alpha/2+ \frac{const_{hi}}{\sqrt{n}}\\
\end{eqnarray*}\]</span></p>
<p>A CI is <strong>second order accurate</strong> if:
<span class="math display">\[\begin{eqnarray*}
P(\theta &lt; \hat{\theta}_{(lo)})&amp;=&amp;\alpha/2 + \frac{const_{lo}}{n}\\
P(\theta &gt; \hat{\theta}_{(hi)})&amp;=&amp;\alpha/2+ \frac{const_{hi}}{n}\\
\end{eqnarray*}\]</span></p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="6%">
<col width="9%">
<col width="10%">
<col width="10%">
<col width="14%">
<col width="16%">
<col width="31%">
</colgroup>
<thead><tr class="header">
<th align="center">CI</th>
<th align="center">Symmetric</th>
<th align="center">Range Resp</th>
<th align="center">Trans Resp</th>
<th align="center">Accuracy</th>
<th align="center">Normal Samp Dist?</th>
<th>Other</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center">perc</td>
<td align="center">No</td>
<td align="center">Yes</td>
<td align="center">Yes</td>
<td align="center">
<span class="math inline">\(1^{st}\)</span> order</td>
<td align="center">No</td>
<td>small <span class="math inline">\(n \rightarrow\)</span> low accuracy</td>
</tr>
<tr class="even">
<td align="center">BS SE</td>
<td align="center">Yes</td>
<td align="center">No</td>
<td align="center">No</td>
<td align="center">
<span class="math inline">\(1^{st}\)</span> order</td>
<td align="center">Yes</td>
<td>param assump <span class="math inline">\(F(\hat{\theta})\)</span>
</td>
</tr>
<tr class="odd">
<td align="center">BS-t</td>
<td align="center">No</td>
<td align="center">No</td>
<td align="center">No</td>
<td align="center">
<span class="math inline">\(2^{nd}\)</span> order</td>
<td align="center">Yes-ish</td>
<td>computer intensive</td>
</tr>
<tr class="even">
<td align="center">BCa</td>
<td align="center">No</td>
<td align="center">Yes</td>
<td align="center">Yes</td>
<td align="center">
<span class="math inline">\(2^{nd}\)</span> order</td>
<td align="center">No</td>
<td>limited param assump</td>
</tr>
</tbody>
</table></div>
<div id="advantages-and-disadvantages" class="section level4" number="6.3.2.1">
<h4>
<span class="header-section-number">6.3.2.1</span> Advantages and Disadvantages<a class="anchor" aria-label="anchor" href="#advantages-and-disadvantages"><i class="fas fa-link"></i></a>
</h4>
<p>BS SE Interval</p>
<ul>
<li>
<strong>Advantages</strong>
similar to the familiar parametric approach; useful with a normally distributed <span class="math inline">\(\hat{\theta}\)</span>; requires the least computation (<span class="math inline">\(B=50-200\)</span>)</li>
<li>
<strong>Disadvantages</strong>
fails to use the entire <span class="math inline">\(\hat{F}^*(\hat{\theta}^*)\)</span>
</li>
</ul>
<p>Bootstrap-t Confidence Interval</p>
<ul>
<li>
<strong>Advantages</strong>
highly accurate CI in many cases; handles skewed <span class="math inline">\(F(\hat{\theta})\)</span> better than the percentile method;</li>
<li>
<strong>Disadvantages</strong>
not invariant to transformations; computationally expensive with the double bootstrap</li>
</ul>
<p>Percentile Interval</p>
<ul>
<li>
<strong>Advantages</strong>
uses the entire <span class="math inline">\(\hat{F}^*(\hat{\theta}^*)\)</span>; allows <span class="math inline">\(F(\hat{\theta})\)</span> to be asymmetrical; invariant to transformations; range respecting; simple to execute</li>
<li>
<strong>Disadvantages</strong>
small samples may result in low accuracy (because of the dependence on the tail behavior); assumes <span class="math inline">\(\hat{F}^*(\hat{\theta}^*)\)</span> to be unbiased</li>
</ul>
<p>BCa Interval</p>
<ul>
<li>
<strong>Advantages</strong>
all of those of the percentile method; allows for bias in <span class="math inline">\(\hat{F}^*(\hat{\theta}^*)\)</span>; <span class="math inline">\(z_0\)</span> can be calculated easily from <span class="math inline">\(\hat{F}^*(\hat{\theta}^*)\)</span>
</li>
<li>
<strong>Disadvantages</strong>
requires a limited parametric assumption; can be complicated to compute</li>
</ul>
</div>
</div>
<div id="bootstrap-ci-and-hypothesis-testing" class="section level3" number="6.3.3">
<h3>
<span class="header-section-number">6.3.3</span> Bootstrap CI and Hypothesis Testing<a class="anchor" aria-label="anchor" href="#bootstrap-ci-and-hypothesis-testing"><i class="fas fa-link"></i></a>
</h3>
<p>(Fall 2022: we won’t cover bootstrap hypothesis testing.)</p>
<p>In this class we have seen many times that if a null value for a parameter is not contained in a CI, we reject the null hypothesis; similarly, we do not reject a null value if it does lie inside the CI. Using BS CIs, we can apply the same logic, and test any hypothesis of interest (note: we can always create one-sided intervals as well!). But simply using CIs leaves out the p-value information. How do we get a p-value from a CI? Consider an alternative definition for the p-value:</p>
<p><strong>p-value:</strong> The smallest level of significance at which you would reject <span class="math inline">\(H_0\)</span>.</p>
<p>So, what we want is for the null value (<span class="math inline">\(\theta_0\)</span>) to be one of the endpoints of the confidence interval with some level of confidence <span class="math inline">\(1-2\alpha_0\)</span>. <span class="math inline">\(\alpha_0\)</span> will then be the one-sided p-value, <span class="math inline">\(2\alpha_0\)</span> will be the two-sided p-value.</p>
<p>For percentile intervals,
<span class="math display">\[\begin{eqnarray*}
p-value = \alpha_0 = \frac{\# \hat{\theta}^*_b &lt; \theta_0}{B}
\end{eqnarray*}\]</span>
(without loss of generality, assuming we set <span class="math inline">\(\hat{\theta}^*_{lo} = \theta_0\)</span>).</p>
</div>
</div>
<div id="reflection-questions-5" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-5"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>In a frequentist confidence interval, is it the parameter or the endpoints of the interval that are random?</li>
<li>What is the correct interpretation of a frequentist confidence interval?</li>
<li>Why might we create a one-sided interval vs a two-sided interval?</li>
<li>Why do you need to first find a posterior distribution in order to create a Bayesian posterior (credible) interval</li>
<li>What is the correct interpretation of a Bayesian posterior (credible) interval?</li>
<li>What is the marginal posterior distribution of <span class="math inline">\(\mu\)</span> in the situation where both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are unknown (and we have prior distributions for both)?</li>
<li>How do you use the marginal posterior distribution for <span class="math inline">\(\mu\)</span> to create a posterior (credible) interval?</li>
<li>From previous chapters, but used here: what are the different ways to determine the distribution of a statistic?</li>
</ol>
</div>
<div id="ethics-considerations-5" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-5"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>What is wrong with each of the following interpretations for a CI:
<ul>
<li>There is a 0.9 prob that the true average number of chips is between 3.7 &amp; 17.2.</li>
<li>90% of cookies have between 3.7 &amp; 17.2 chips.</li>
<li>We are 90% confident that in our sample, the sample average number of chips is between 3.7 and 17.2.</li>
<li>In many repeated samples, 90% of sample averages will be between 3.7 and 17.2.</li>
</ul>
</li>
<li>What are the technical conditions for creating confident intervals using a t-distribution? That is, what are the conditions on the data that give rise to the t-distribution? What happens if the technical conditions are violated and the confidence interval is created anyway?<br>
</li>
<li>How much influence does the prior have on the resulting posterior (credible) interval? (Hint: the answer is “it depends.”)</li>
</ol>
</div>
<div id="r-code-creating-interval-estimates" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> R code: Creating Interval Estimates<a class="anchor" aria-label="anchor" href="#r-code-creating-interval-estimates"><i class="fas fa-link"></i></a>
</h2>
<div id="finding-cutoffs" class="section level3" number="6.6.1">
<h3>
<span class="header-section-number">6.6.1</span> Finding cutoffs<a class="anchor" aria-label="anchor" href="#finding-cutoffs"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that the <code>q</code> in the distributional functions (e.g., <code><a href="https://rdrr.io/r/stats/Normal.html">qnorm()</a></code>, <code><a href="https://rdrr.io/r/stats/Binomial.html">qbinom()</a></code>, <code><a href="https://rdrr.io/r/stats/Uniform.html">qunif()</a></code>, <code><a href="https://rdrr.io/r/stats/Chisquare.html">qchisq()</a></code>, <code><a href="https://rdrr.io/r/stats/TDist.html">qt()</a></code>) indicates that the output is a <strong>quantile</strong>.</p>
<p>The <strong>mosiac</strong> package adds an <code>x</code> to the front of the function name which allows a figure to accompany the numerical value of the quantile. I highly recommend drawing pictures when finding quantiles or percentages.</p>
<p>One-sided 98% t-interval <span class="math inline">\((df = 24)\)</span> where 98% of the probability is to the left of the quantile of interest. Note that the t-distribution is symmetric.</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqt</a></span><span class="op">(</span><span class="fl">.98</span>, <span class="fl">24</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1] 2.17</code></pre>
<p>Two-sided 98% t-interval <span class="math inline">\((df = 24)\)</span> where 98% of the area is in the center, so 99% of the area is to the left. Note that the code can be written in two different ways and provides the same quantile values. Note also that the t-distribution is symmetric.</p>
<div class="sourceCode" id="cb47"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqt</a></span><span class="op">(</span><span class="fl">.99</span>, <span class="fl">24</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1] 2.49</code></pre>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.01</span>, <span class="fl">0.99</span><span class="op">)</span>, <span class="fl">24</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-6-2.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1] -2.49  2.49</code></pre>
<p>Two-sided 95% chi-square interval <span class="math inline">\((df = 9)\)</span>. Note that the chi-square distribution is not symmetric.</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqchisq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.025</span>, <span class="fl">0.975</span><span class="op">)</span>, <span class="fl">9</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1]  2.7 19.0</code></pre>
<p>To find a 90% prediction inteval cutoff, the same R code is used:</p>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu">mosaic</span><span class="fu">::</span><span class="fu"><a href="https://www.mosaic-web.org/mosaic/reference/qdist.html">xqt</a></span><span class="op">(</span><span class="fl">0.95</span>, <span class="fl">12</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;"></div>
<pre><code>## [1] 1.78</code></pre>
</div>
<div id="bootstrapping-survival-example" class="section level3" number="6.6.2">
<h3>
<span class="header-section-number">6.6.2</span> Bootstrapping Survival Example<a class="anchor" aria-label="anchor" href="#bootstrapping-survival-example"><i class="fas fa-link"></i></a>
</h3>
<p>There are many built in functions in R (and Python, Matlab, Stata, etc. for that matter) which will bootstrap a dataset and create any of a number of standard bootstrap intervals. However, in order to understand the bootstrap process, the example below uses for loops to repeated resample and calculate the statistics of interest.</p>
<div class="example">
<p><span id="exm:unlabeled-div-38" class="example"><strong>Example 6.6  </strong></span>heroin survival time</p>
<ul>
<li><p>Hesketh and Everitt (2000) report on a study by Caplehorn and Bell (1991) that investigated the times that heroin addicts remained in a clinic for methadone maintenance treatment.</p></li>
<li><p>The data include the amount of time that the subjects stayed in the facility until treatment was terminated (column 4).</p></li>
<li><p>For about 37% of the subjects, the study ended while they were still the in clinic (status=0).</p></li>
<li><p>Their survival time has been truncated. For this reason we might not want to estimate the mean survival time, but rather some other measure of typical survival time. Below we explore using the 25% trimmed mean. (From ISCAM Chance &amp; Rossman, Investigation 4.5.3)</p></li>
</ul>
</div>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidyverse.tidyverse.org">tidyverse</a></span><span class="op">)</span>
<span class="va">heroin</span> <span class="op">&lt;-</span> <span class="fu">readr</span><span class="fu">::</span><span class="fu"><a href="https://readr.tidyverse.org/reference/read_table.html">read_table</a></span><span class="op">(</span><span class="st">"http://www.rossmanchance.com/iscam2/data/heroin.txt"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">heroin</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] "id"     "clinic" "status" "times"  "prison" "dose"</code></pre>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">heroin</span><span class="op">)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 6
##      id clinic status times prison  dose
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     1      1      1   428      0    50
## 2     2      1      1   275      1    55
## 3     3      1      1   262      0    55
## 4     4      1      1   183      0    30
## 5     5      1      1   259      1    65
## 6     6      1      1   714      0    55</code></pre>
<div class="sourceCode" id="cb59"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">obs.stat</span> <span class="op">&lt;-</span> <span class="va">heroin</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>tmeantime <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">times</span>, trim<span class="op">=</span><span class="fl">0.25</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span>
<span class="va">obs.stat</span></code></pre></div>
<pre><code>## [1] 378</code></pre>
<div id="bootstrapping-the-data-1" class="section level4 unnumbered">
<h4>Bootstrapping the data<a class="anchor" aria-label="anchor" href="#bootstrapping-the-data-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb61"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4747</span><span class="op">)</span>
<span class="va">heroin.bs</span><span class="op">&lt;-</span><span class="va">heroin</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_frac</a></span><span class="op">(</span>size<span class="op">=</span><span class="fl">1</span>, replace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>

<span class="va">heroin.bs</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
  <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>tmeantime <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">times</span>, trim<span class="op">=</span><span class="fl">0.25</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 372</code></pre>
</div>
<div id="creating-a-bootstrap-sampling-distribution-for-the-trimmed-mean-1" class="section level4 unnumbered">
<h4>Creating a bootstrap sampling distribution for the trimmed mean<a class="anchor" aria-label="anchor" href="#creating-a-bootstrap-sampling-distribution-for-the-trimmed-mean-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bs.test.stat</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span>    <span class="co"># placeholder, eventually B long, check after running!</span>
<span class="va">bs.sd.test.stat</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># placeholder, eventually B long, check after running!</span>

<span class="va">B</span> <span class="op">&lt;-</span> <span class="fl">500</span>
<span class="va">M</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">4747</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw">for</span><span class="op">(</span><span class="va">b</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">B</span><span class="op">)</span><span class="op">{</span> 
  <span class="va">heroin.bs</span><span class="op">&lt;-</span><span class="va">heroin</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_frac</a></span><span class="op">(</span>size<span class="op">=</span><span class="fl">1</span>, replace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span>  <span class="co"># BS sample</span>
  <span class="va">bs.test.stat</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bs.test.stat</span>, <span class="co"># concatenate each trimmed mean each time go through loop</span>
                  <span class="va">heroin.bs</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
                    <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>tmeantime <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">times</span>, trim <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
  
  <span class="va">bsbs.test.stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># refresh the vector of double BS test statistics</span>
  
  <span class="kw">for</span><span class="op">(</span><span class="va">m</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">M</span><span class="op">)</span><span class="op">{</span>
    <span class="va">heroin.bsbs</span><span class="op">&lt;-</span><span class="va">heroin.bs</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/sample_n.html">sample_frac</a></span><span class="op">(</span>size<span class="op">=</span><span class="fl">1</span>, replace<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span> <span class="co"># BS of the BS!</span>
    <span class="va">bsbs.test.stat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bsbs.test.stat</span>, <span class="co"># concatenate the trimmed mean of the double BS</span>
                        <span class="va">heroin.bsbs</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> 
                          <span class="fu"><a href="https://dplyr.tidyverse.org/reference/summarise.html">summarize</a></span><span class="op">(</span>tmeantime <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">times</span>, trim <span class="op">=</span> <span class="fl">0.25</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://dplyr.tidyverse.org/reference/pull.html">pull</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span>
  <span class="op">}</span>
  <span class="va">bs.sd.test.stat</span><span class="op">&lt;-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">bs.sd.test.stat</span>, <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">bsbs.test.stat</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
</div>
<div id="what-do-the-data-distributions-look-like-1" class="section level4 unnumbered">
<h4>What do the <strong>data</strong> distributions look like?<a class="anchor" aria-label="anchor" href="#what-do-the-data-distributions-look-like-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb65"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">heroin</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">times</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">30</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"original sample (n=238)"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">heroin.bs</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">times</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">30</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"one bootstrap sample (n=238)"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-13-2.png" width="672" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">heroin.bsbs</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">times</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">30</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"a bootstrap sample of the one bootstrap sample (n=238)"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-13-3.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="what-do-the-sampling-distributions-look-like-1" class="section level4 unnumbered">
<h4>What do the <strong>sampling</strong> distributions look like?<a class="anchor" aria-label="anchor" href="#what-do-the-sampling-distributions-look-like-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bs.stats</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">bs.test.stat</span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">bs.stats</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">bs.test.stat</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">20</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"dist of trimmed mean"</span><span class="op">)</span> <span class="op">+</span>  
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"trimmed.mean="</span>,<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">bs.test.stat</span><span class="op">)</span>,<span class="fl">2</span><span class="op">)</span>,<span class="st">"; SE="</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">bs.test.stat</span><span class="op">)</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="what-is-the-distribution-of-the-se-of-the-statistic-1" class="section level4 unnumbered">
<h4>What is the distribution of the SE of the statistic?<a class="anchor" aria-label="anchor" href="#what-is-the-distribution-of-the-se-of-the-statistic-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bs.SE</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span><span class="va">bs.sd.test.stat</span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">bs.SE</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">bs.sd.test.stat</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">20</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"dist of SE of trimmed means"</span><span class="op">)</span> <span class="op">+</span>  
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"average SE="</span>,<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">bs.sd.test.stat</span><span class="op">)</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="what-is-the-distribution-of-the-t-statistics-1" class="section level4 unnumbered">
<h4>What is the distribution of the T statistics?<a class="anchor" aria-label="anchor" href="#what-is-the-distribution-of-the-t-statistics-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bs.T</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>T.test.stat <span class="op">=</span> <span class="op">(</span><span class="va">bs.test.stat</span> <span class="op">-</span> <span class="va">obs.stat</span><span class="op">)</span> <span class="op">/</span> <span class="va">bs.sd.test.stat</span><span class="op">)</span>
<span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">bs.T</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x<span class="op">=</span><span class="va">T.test.stat</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_histogram.html">geom_histogram</a></span><span class="op">(</span>bins<span class="op">=</span><span class="fl">20</span><span class="op">)</span> <span class="op">+</span> 
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">ggtitle</a></span><span class="op">(</span><span class="st">"dist of T statistics of trimmed means"</span><span class="op">)</span> <span class="op">+</span>  
  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">xlab</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"average T="</span>,<span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">bs.T</span><span class="op">$</span><span class="va">T.test.stat</span><span class="op">)</span>,<span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="06-intest_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;"></div>
</div>
<div id="normal-ci-with-bs-se-1" class="section level4 unnumbered">
<h4>95% normal CI with BS SE<a class="anchor" aria-label="anchor" href="#normal-ci-with-bs-se-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">obs.stat</span> <span class="op">+</span> 
  <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">qnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.025</span>,<span class="fl">.975</span><span class="op">)</span><span class="op">)</span><span class="op">*</span>
  <span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">bs.test.stat</span><span class="op">)</span></code></pre></div>
<pre><code>## [1] 334 423</code></pre>
</div>
<div id="bootstrap-t-ci-1" class="section level4 unnumbered">
<h4>95% Bootstrap-t CI<a class="anchor" aria-label="anchor" href="#bootstrap-t-ci-1"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the t-value is needed (which requires a different SE for each bootstrap sample).</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">bs.t.hat</span><span class="op">&lt;-</span><span class="op">(</span><span class="va">bs.test.stat</span> <span class="op">-</span> <span class="va">obs.stat</span><span class="op">)</span><span class="op">/</span><span class="va">bs.sd.test.stat</span>

<span class="va">bs.t.hat.95</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">bs.t.hat</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.975</span>,<span class="fl">.025</span><span class="op">)</span><span class="op">)</span>

<span class="va">obs.stat</span> <span class="op">-</span> <span class="va">bs.t.hat.95</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/stats/sd.html">sd</a></span><span class="op">(</span><span class="va">bs.test.stat</span><span class="op">)</span></code></pre></div>
<pre><code>## 97.5%  2.5% 
##   337   427</code></pre>
</div>
<div id="percentile-ci-1" class="section level4 unnumbered">
<h4>95% Percentile CI<a class="anchor" aria-label="anchor" href="#percentile-ci-1"><i class="fas fa-link"></i></a>
</h4>
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/stats/quantile.html">quantile</a></span><span class="op">(</span><span class="va">bs.test.stat</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">.025</span>, <span class="fl">.975</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<pre><code>##  2.5% 97.5% 
##   332   422</code></pre>

</div>
</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></div>
<div class="next"><a href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#intest"><span class="header-section-number">6</span> Interval Estimates</a></li>
<li>
<a class="nav-link" href="#frequentist-confidence-intervals"><span class="header-section-number">6.1</span> Frequentist Confidence Intervals</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#ci-for-the-mean-mu-in-a-normal-random-sample"><span class="header-section-number">6.1.1</span> CI for the mean, \(\mu\) in a normal random sample</a></li></ul>
</li>
<li>
<a class="nav-link" href="#bayesian-intervals"><span class="header-section-number">6.2</span> Bayesian Intervals</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#joint-posterior-distribution-for-mu-and-sigma-in-a-normal-distribution"><span class="header-section-number">6.2.1</span> Joint Posterior Distribution for \(\mu\) and \(\sigma\) in a Normal Distribution</a></li>
<li><a class="nav-link" href="#posterior-interval-for-the-mean-mu-in-a-normal-random-sample"><span class="header-section-number">6.2.2</span> Posterior Interval for the mean, \(\mu\) in a normal random sample</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#bootstrap-confidence-intervals"><span class="header-section-number">6.3</span> Bootstrap Confidence Intervals</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#different-bootstrap-cis"><span class="header-section-number">6.3.1</span> Different Bootstrap CIs</a></li>
<li><a class="nav-link" href="#what-makes-a-ci-good"><span class="header-section-number">6.3.2</span> What makes a CI good?</a></li>
<li><a class="nav-link" href="#bootstrap-ci-and-hypothesis-testing"><span class="header-section-number">6.3.3</span> Bootstrap CI and Hypothesis Testing</a></li>
</ul>
</li>
<li><a class="nav-link" href="#reflection-questions-5"><span class="header-section-number">6.4</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-5"><span class="header-section-number">6.5</span>  Ethics Considerations</a></li>
<li>
<a class="nav-link" href="#r-code-creating-interval-estimates"><span class="header-section-number">6.6</span> R code: Creating Interval Estimates</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#finding-cutoffs"><span class="header-section-number">6.6.1</span> Finding cutoffs</a></li>
<li><a class="nav-link" href="#bootstrapping-survival-example"><span class="header-section-number">6.6.2</span> Bootstrapping Survival Example</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/06-intest.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/06-intest.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-11-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
