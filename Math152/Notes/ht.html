<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Hypothesis Testing | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="8.1 Statistical Hypotheses Instead of giving bounds on parameter values (interval estimates), we are going to test specific claims about a parameter (e.g., \(\theta\)). We know \(\theta \in...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 8 Hypothesis Testing | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="8.1 Statistical Hypotheses Instead of giving bounds on parameter values (interval estimates), we are going to test specific claims about a parameter (e.g., \(\theta\)). We know \(\theta \in...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Hypothesis Testing | Statistical Theory">
<meta name="twitter:description" content="8.1 Statistical Hypotheses Instead of giving bounds on parameter values (interval estimates), we are going to test specific claims about a parameter (e.g., \(\theta\)). We know \(\theta \in...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></li>
<li><a class="" href="intest.html"><span class="header-section-number">6</span> Interval Estimates</a></li>
<li><a class="" href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></li>
<li><a class="active" href="ht.html"><span class="header-section-number">8</span> Hypothesis Testing</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ht" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Hypothesis Testing<a class="anchor" aria-label="anchor" href="#ht"><i class="fas fa-link"></i></a>
</h1>
<div id="statistical-hypotheses" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Statistical Hypotheses<a class="anchor" aria-label="anchor" href="#statistical-hypotheses"><i class="fas fa-link"></i></a>
</h2>
<p>Instead of giving bounds on parameter values (interval estimates), we are going to test specific claims about a parameter (e.g., <span class="math inline">\(\theta\)</span>). We know <span class="math inline">\(\theta \in \Omega\)</span>. Let <span class="math inline">\(\Omega\)</span> have a disjoint partition:
<span class="math display">\[\begin{eqnarray*}
\Omega_0 \cup \Omega_1 &amp;=&amp; \Omega\\
\Omega_0 \cap \Omega_1 &amp;=&amp; \emptyset\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{Claim 1:  } H_0: \theta \in \Omega_0\\
\mbox{Claim 2:  } H_1: \theta \in \Omega_1\\
\end{eqnarray*}\]</span></p>
<p>We want to decide which one is true, <span class="math inline">\(H_0\)</span> (the null) or <span class="math inline">\(H_1\)</span> (the alternative)?</p>
<ul>
<li>If <span class="math inline">\(\Omega_0\)</span> or <span class="math inline">\(\Omega_1\)</span> is a single value, then we say it is a <strong>simple hypothesis</strong>
</li>
<li>If <span class="math inline">\(\Omega_0\)</span> or <span class="math inline">\(\Omega_1\)</span> contain more than a single value, then we say it is a <strong>composite hypothesis</strong>
</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-45" class="example"><strong>Example 8.1  </strong></span>We want to test whether a coin is fair. We’ll flip the coin 20 times to collect data. We have the following two-sided test:
<span class="math display">\[\begin{eqnarray*}
H_0:&amp;&amp; \theta=0.5 \mbox{  (simple)}\\
H_1:&amp;&amp; \theta\ne0.5 \mbox{  (composite)}\\
\end{eqnarray*}\]</span></p>
<p>Often we’ll write <span class="math inline">\(\theta_0\)</span> as a placeholder for the null value:
<span class="math display">\[\begin{eqnarray*}
H_0:&amp;&amp; \theta=\theta_0 \mbox{  (simple)}\\
H_1:&amp;&amp; \theta\ne \theta_0 \mbox{  (composite)}\\
\end{eqnarray*}\]</span></p>
<p>If we are betting, and the dealer always bets on heads, we probably want a one-sided test.
<span class="math display">\[\begin{eqnarray*}
H_0: \theta=0.5 \mbox{  (simple)}&amp;  \mbox{ or } &amp; H_0: \theta \leq 0.5 \mbox{  (composite)}\\
H_1: \theta &gt; 0.5 \mbox{  (composite)}&amp;   &amp;H_1: \theta &gt; 0.5 \mbox{  (composite)}\\
\end{eqnarray*}\]</span></p>
</div>
<p>We always put our research question in the <strong>alternative</strong> hypothesis. Ask yourself: why are they/we collecting the data?</p>
<div id="what-is-an-alternative-hypothesis" class="section level3 unnumbered">
<h3>What is an Alternative Hypothesis?<a class="anchor" aria-label="anchor" href="#what-is-an-alternative-hypothesis"><i class="fas fa-link"></i></a>
</h3>
<p>Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: <a href="https://www.youtube.com/watch?v=b-U_I1DCGEY" class="uri">https://www.youtube.com/watch?v=b-U_I1DCGEY</a>.</p>
<p>In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on.</p>
<p>What is the point? Why did we see the video? How does it relate the to the material from class? What is the relationship to sampling distributions?</p>
</div>
<div id="critical-regions-and-test-statistics" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Critical Regions and Test Statistics<a class="anchor" aria-label="anchor" href="#critical-regions-and-test-statistics"><i class="fas fa-link"></i></a>
</h3>
<p>Let X count the number of heads, what would you think if your data gave X=20 (n=20) and your hypotheses were:
<span class="math display">\[\begin{eqnarray*}
H_0:&amp;&amp; \theta=0.5 \\
H_1:&amp;&amp; \theta\ne0.5
\end{eqnarray*}\]</span></p>
<p>The data help you decide which hypothesis to believe. What if X=11? X=7? We set up a critical region so that if X is in the region, we reject <span class="math inline">\(H_0\)</span>. Here, <span class="math inline">\(\exists \ \ c \ \ s.t. \ \ | x - 10| &gt; c \ \Rightarrow\)</span>   reject   <span class="math inline">\(H_0\)</span>.</p>
<p>A <strong>test statistic</strong>, <span class="math inline">\(T=r(\underline{X})\)</span> is a function of the data that will (hopefully) provide you with enough knowledge to make a decision about your hypothesis. Here, <span class="math inline">\(T=X\)</span>. Often <span class="math inline">\(T=\overline{X}\)</span> or <span class="math inline">\(T= \overline{X} - \mu_0\)</span> or <span class="math inline">\(T=(\overline{X} - \mu_0) / s/\sqrt{n}\)</span>. Usually, the <strong>critical region</strong> will be of some form like:</p>
<blockquote>
<p>“if <span class="math inline">\(T \geq c\)</span>” <span class="math inline">\(\Rightarrow\)</span> reject <span class="math inline">\(H_0\)</span></p>
</blockquote>
</div>
<div id="errors-power-size" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Errors: Power &amp; Size<a class="anchor" aria-label="anchor" href="#errors-power-size"><i class="fas fa-link"></i></a>
</h3>
<p>We can make two types of mistakes (jury example):</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th align="center">
<span class="math inline">\(H_0\)</span> true</th>
<th align="center">
<span class="math inline">\(H_1\)</span> true</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Reject <span class="math inline">\(H_0\)</span>
</td>
<td align="center">type I error</td>
<td align="center"><span class="math inline">\(\surd\)</span></td>
</tr>
<tr class="even">
<td>Do not Reject <span class="math inline">\(H_0\)</span>
</td>
<td align="center"><span class="math inline">\(\surd\)</span></td>
<td align="center">type II error</td>
</tr>
</tbody>
</table></div>
<div id="power" class="section level4" number="8.1.2.1">
<h4>
<span class="header-section-number">8.1.2.1</span> Power<a class="anchor" aria-label="anchor" href="#power"><i class="fas fa-link"></i></a>
</h4>
<p>We rate our test by considering the “power” of the test. Let <span class="math inline">\(\delta\)</span> be a test procedure that defines a critical region <span class="math inline">\(c\)</span> such that we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(T \in C\)</span>. For example:
<span class="math display">\[\begin{eqnarray*}
\delta &amp;=&amp; \{ \mbox{ always reject } H_0 \}\\
\delta &amp;=&amp; \{ \mbox{ reject } H_0 \mbox{ if } X &gt; 60 \}\\
\end{eqnarray*}\]</span></p>
<p>The <strong>power function</strong> is:
<span class="math display">\[\begin{eqnarray*}
\pi (\theta | \delta) = P(T \in C | \theta)
\end{eqnarray*}\]</span></p>
<ul>
<li>If <span class="math inline">\(\theta \in \Omega_0\)</span>, <span class="math inline">\(\pi(\theta | \delta)\)</span> is the probability of making a type I error.</li>
<li>If <span class="math inline">\(\theta \in \Omega_1\)</span>, <span class="math inline">\(1 - \pi(\theta | \delta)\)</span> is the probability of making a type II error.</li>
</ul>
<p>Note: <span class="math inline">\(\theta \in \Omega_0\)</span> OR <span class="math inline">\(\theta \in \Omega_1\)</span>, but not both! So, only one type of error is ever possible, but we never know which it is. If we have our choice between test procedures, we want one for which:
<span class="math display">\[\begin{eqnarray*}
\theta \in \Omega_0, \pi(\theta | \delta) \mbox{ is small}\\
\theta \in \Omega_1, \pi(\theta | \delta) \mbox{ is big}\\
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-46" class="example"><strong>Example 8.2  </strong></span>Let <span class="math inline">\(\delta =\)</span>“always reject <span class="math inline">\(H_0\)</span>”, <span class="math inline">\(\delta = \{ T \in I\!\!R \}\)</span>.
<span class="math display">\[\begin{eqnarray*}
\mbox{if } \theta \in \Omega_0, \pi(\theta | \delta) = 1, \mbox{ bad!!}, P(\mbox{type I error}) = 1\\
\mbox{if } \theta \in \Omega_1, \pi(\theta | \delta) = 1, \mbox{ good!!}, P(\mbox{type II error}) = 0
\end{eqnarray*}\]</span>
We need a balance.</p>
</div>
<p>Typically, we bound <span class="math inline">\(\pi(\theta | \delta) \ \ \forall \ \ \theta \in \Omega_0\)</span>. That is, <span class="math inline">\(\pi(\theta | \delta) \leq \alpha_0 \ \ \forall \ \ \theta \in \Omega_0\)</span>. Note, <span class="math inline">\(\alpha_0\)</span> is called the <strong>level of significance</strong>.</p>
<p>Among all tests that satisfy the bound for <span class="math inline">\(\theta \in \Omega_0\)</span>, we want the power function when <span class="math inline">\(\theta \in \Omega_1\)</span> to be as big as possible (probability of type II error to be as low as possible).</p>
</div>
<div id="size" class="section level4 unnumbered">
<h4>Size<a class="anchor" aria-label="anchor" href="#size"><i class="fas fa-link"></i></a>
</h4>
<p>The <strong>size</strong> of the test is:
<span class="math display">\[\begin{eqnarray*}
\alpha(\delta) = \sup_{\theta \in \Omega_0} \pi (\theta | \delta)
\end{eqnarray*}\]</span>
if <span class="math inline">\(\Omega_0 = \theta_0\)</span> (simple hypothesis), <span class="math inline">\(\alpha(\delta) = \pi(\theta_0 | \delta)\)</span>.</p>
<p>Note: you may be wondering about the <span class="math inline">\(\sup\)</span> function. Note that often the <span class="math inline">\(\sup\)</span> over a set is the same as the <span class="math inline">\(\max\)</span> over a set! So what is the difference? The <span class="math inline">\(\max\)</span> is the largest value that the function can take on over all the possible values <strong>within</strong> a set (that is, the argument of the function <em>must</em> be contained in the range over which we are evaluating the function). The <span class="math inline">\(\sup\)</span> is the largest value that the function can take on over all the possible values that <strong>bound</strong> the range of possible values. The <span class="math inline">\(\sup\)</span> may or may not happen at a value in the range. If the <span class="math inline">\(\max\)</span> exists, then the <span class="math inline">\(\sup = \max\)</span>. The hypothesis testing structure is set up to be very general and to accommodate all different types of hypotheses, thus the <span class="math inline">\(\sup\)</span> is the right choice (over the <span class="math inline">\(\max\)</span>) in defining <span class="math inline">\(\alpha(\delta).\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-47" class="example"><strong>Example 8.3  </strong></span>Back to the cheating example, n=100,
<span class="math display">\[\begin{eqnarray*}
H_0: \theta \leq 0.5 \\
H_1: \theta &gt; 0.5
\end{eqnarray*}\]</span></p>
<p>Let <span class="math inline">\(\delta = \{\)</span> reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(X&gt;55 \}.\)</span>
<span class="math display">\[\begin{eqnarray*}
\pi(\theta | \delta) &amp;=&amp; P(X &gt; 55 | \theta = 0.5)\\
&amp;=&amp; P( Z &gt; 1) \\
&amp;=&amp; 0.1587
\end{eqnarray*}\]</span>
What if <span class="math inline">\(\theta &lt; 0.5\)</span>? <span class="math inline">\(\rightarrow \pi(\theta | \delta) &lt; 0.1587\)</span> ! The <strong>size</strong> of the test is <span class="math inline">\(\alpha(\delta) = 0.1587.\)</span> Can we make the test better?? The previous statement says that if <span class="math inline">\(\theta=0.5,\)</span> we’ll reject <span class="math inline">\(H_0\)</span> 15.87% of the time. (That is a pretty big type I error). We want <span class="math inline">\(\alpha(\delta) \leq 0.05 (= \alpha_0).\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\delta &amp;=&amp; \{ \mbox{ reject } H_0 \mbox{ if } T \geq c \}\\
\alpha(\delta) &amp;=&amp; \sup_{\theta \in \Omega_0} P (T \geq c | \theta) \\
&amp;=&amp; \sup_{\theta \in \Omega_0} \pi (\theta | \delta)\\
\pi (\theta | \delta) &amp;=&amp; 1 - \Phi\Bigg( \frac{ c - 100 \theta}{\sqrt{100\theta(1-\theta)}} \Bigg)\\
\end{eqnarray*}\]</span></p>
<p>Note: <span class="math inline">\(\pi (\theta | \delta)\)</span> is an increasing function in <span class="math inline">\(\theta\)</span>. Also, <span class="math inline">\(\pi (\theta | \delta)\)</span> is a decreasing function in c (<span class="math inline">\(\Phi\)</span> is an increasing function in c).</p>
<p>Keeping in mind that we want to find a c s.t. <span class="math inline">\(\sup_{\theta \in \Omega_0} \pi(\theta | \delta) \leq 0.05\)</span>, we want to maximize <span class="math inline">\(\pi (\theta | \delta) \ \ \forall \ \ \theta \in \Omega\)</span> (make c as small as possible).</p>
<p>We already know that: <span class="math inline">\(\sup_{\theta \in \Omega_0} \pi(\theta | \delta) = \pi(\theta_0 = 0.5 | \delta)\)</span>. So,
<span class="math display">\[\begin{eqnarray*}
\pi (\theta_0 = 0.5 | \delta) &amp;\leq&amp; 0.05\\
c &amp;\geq&amp; 58.25 \mbox{ (see WU)}\\
\mbox{notice: } c &amp;=&amp; \Phi^{-1} (1 - \alpha_0) \sigma_0 + \mu_0\\
\mbox{reject if: } T &amp;\geq&amp; c\\
\\
\mbox{or, equivalently, if: } \frac{x - \mu_0}{\sigma_0} &amp;\geq&amp; \Phi^{-1} (1-\alpha_0)\\
\mbox{let: } T &amp;=&amp; \frac{X - n \theta_0}{\sqrt{\theta_0 (1-\theta_0) / n}} \ \ \ \mbox{ be our test statistic}\\
\mbox{now, reject if: } T &amp;\geq&amp; \Phi^{-1} (1-\alpha_0)\\
\end{eqnarray*}\]</span></p>
<p>Because the data are discrete, we cannot actually set our type I error rate to be 0.05. So, we end up with
<span class="math display">\[\begin{eqnarray*}
\alpha_0 &amp;=&amp; 0.05\\
\alpha(\delta) &amp;=&amp; P(X \geq 59 | \theta = 0.5)\\
&amp;=&amp; 0.04431
\end{eqnarray*}\]</span></p>
</div>
<p>Notice that we can only report P(type II error) for a simple alternative (more in section 9.2 of <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>). In addition to reporting our decision, we would like to report a measure of certainty in our decision. How <strong>strong</strong> is the evidence for the unfair coin?</p>
<p><strong>p-value</strong> is the probability of seeing your data or more extreme if the null hypothesis is true.</p>
<p>Let <span class="math inline">\(\hat{T}\)</span> be the observed test statistic</p>
<p>Let <span class="math inline">\(T\)</span> be the test statistic random variable</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{p-value } &amp;=&amp; \sup_{\theta \in \Omega_0} P(T &gt; \hat{T})\\
&amp;=&amp; \sup_{\theta \in \Omega_0} \pi (\theta | \delta_{\hat{T}}) \mbox{ the size of the test } \delta_{\hat{T}}\\
&amp;=&amp; \alpha(\delta_{\hat{T}})\\
\end{eqnarray*}\]</span></p>
<p>That is, the smallest level of significance that would reject <span class="math inline">\(H_0\)</span> with the observed data.</p>
<div class="example">
<p><span id="exm:unlabeled-div-48" class="example"><strong>Example 8.4  </strong></span>cheating dealer example continued…</p>
<p>Notice:
<span class="math display">\[\begin{eqnarray*}
0.0082 &lt; 0.05 &lt;  0.3446\\
52 &lt; 58.25 &lt; 62\\
\end{eqnarray*}\]</span>
(equivalence of tests and rejection regions)</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-49" class="example"><strong>Example 8.5  </strong></span>Assume weights of cereal in 10oz boxes are normally distributed, <span class="math inline">\(N(\mu, \sigma^2),\)</span> both unknown. To test whether or not the box label is accurate, we set up our hypotheses:
<span class="math display">\[\begin{eqnarray*}
H_0: \mu = 10\mbox{oz}\\
H_1: \mu \ne 10\mbox{oz}\\
\end{eqnarray*}\]</span></p>
<p>What does the form of our critical region look like? (Reject if less than a certain amount below 10oz, or bigger than a certain amount above 10oz, draw a number line with shaded rejection region.) We will reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\overline{X}\)</span> is too big or too small.</p>
<p>If <span class="math inline">\(|\overline{X} - 10 |\)</span> is too big, then <span class="math inline">\(\frac{|\overline{X} - 10 |}{s/\sqrt{n}}\)</span> will also be too big.
<span class="math display">\[\begin{eqnarray*}
C &amp;=&amp; \Bigg\{ \frac{|\overline{X} - 10 |}{s/\sqrt{n}} &gt; c \Bigg\}\\
\delta: &amp;&amp; \Bigg\{ \mbox{reject $H_0$ if } T \in C \mbox{ where } T = \frac{|\overline{X} - 10|}{s/\sqrt{n}} \Bigg\}
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\alpha(\delta) &amp;=&amp; \sup_{\mu \in \Omega_0} \pi(\mu | \delta)\\
&amp;=&amp; P\Bigg(\frac{|\overline{X} - 10|}{s/\sqrt{n}} &gt; c \ \  \bigg| \ \ \mu=10 \Bigg)\\
&amp;=&amp; P \bigg( -c &lt; \frac{\overline{X} - 10}{s/\sqrt{n}} &lt; c \ \  \bigg| \ \  \mu=10 \bigg) \\
&amp;=&amp; P( -c &lt; t_{15} &lt; c) = 0.95\\
c &amp;=&amp; 2.131\\
\end{eqnarray*}\]</span>
Note: we also reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\overline{X} \notin 10 \pm 0.453\)</span>. (More later on computing the power <span class="math inline">\(\pi(\mu | \delta), \ \mu \in \Omega_1.)\)</span></p>
<p>What is the p-value given our data?
<span class="math display">\[\begin{eqnarray*}
\mbox{p-value} &amp;=&amp; P (T &gt; \widehat{T})\\
&amp;=&amp; P \Bigg( \frac{|\overline{X} - 10|}{s/\sqrt{16}} &gt; \frac{|10.4 - 10|}{0.85/\sqrt{16}} \Bigg)\\
&amp;=&amp; 2 P (t_{15} &gt; 1.88)\\
&amp;=&amp; 2 (1 - \texttt{pt(1.88, 15)})\\
&amp;=&amp; 2 (1 - 0.96) \\
&amp;=&amp; 0.0797
\end{eqnarray*}\]</span>
<!--%1.753 \leq &1.88& \leq 2.131\\
%2 \cdot 0.025 \leq &\mbox{p-value}& \leq 2 \cdot 0.05\\
%0.05 \leq &\mbox{p-value}& \leq 0.10\\-->
There is moderate evidence (not strong) to say that cereal boxes weigh, on average, something other than 10oz.</p>
</div>
</div>
</div>
</div>
<div id="simple-hypotheses" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Simple Hypotheses<a class="anchor" aria-label="anchor" href="#simple-hypotheses"><i class="fas fa-link"></i></a>
</h2>
<p>Consider:
<span class="math display">\[\begin{eqnarray*}
H_0: \theta = \theta_0\\
H_1: \theta = \theta_1\\
\end{eqnarray*}\]</span></p>
<p>Let
<span class="math display">\[\begin{eqnarray*}
f_i(\underline{x}) = f(x_1 | \theta_i) f(x_2 | \theta_i) \cdots f(x_n | \theta_i)
\end{eqnarray*}\]</span>
(the joint pdf if <span class="math inline">\(\theta=\theta_i, H_i\)</span>, is true.)</p>
<p>Our errors now become:
<span class="math display">\[\begin{eqnarray*}
\alpha(\delta) = P(\mbox{reject } H_0 | \theta=\theta_0) = \pi(\theta_0 | \delta) = \mbox{size}\\
\beta(\delta) = P(\mbox{do not reject } H_0 | \theta=\theta_1) = 1 - \pi(\theta_1 | \delta) = 1 - \mbox{power}\\
\end{eqnarray*}\]</span></p>
<p>We cannot find a test for which <span class="math inline">\(\alpha(\delta)\)</span> and <span class="math inline">\(\beta(\delta)\)</span> are both arbitrarily small. Instead, we can minimize a linear combination of the two types of errors. That is, we want to find <span class="math inline">\(\delta\)</span> such that <span class="math inline">\(a \alpha(\delta) + b \beta(\delta)\)</span> is minimized. (Is <span class="math inline">\(\alpha\)</span> worse? <span class="math inline">\(\beta?)\)</span></p>
<div class="theorem">
<p><span id="thm:unlabeled-div-50" class="theorem"><strong>Theorem 8.1  </strong></span>(Theorem 9.2.1, <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>)
Let <span class="math inline">\(a &gt; 0\)</span> and <span class="math inline">\(b &gt; 0\)</span>. Consider <span class="math inline">\(\delta^*\)</span>, a test such that we will:
<span class="math display">\[\begin{eqnarray*}
\mbox{reject } H_0 \mbox{ if } a f_0(\underline{x}) &lt; b f_1(\underline{x})\\
\mbox{not reject } H_0 \mbox{ if } a f_0(\underline{x}) &gt; b f_1(\underline{x})\\
\mbox{either decision if }  a f_0(\underline{x}) = b f_1(\underline{x})\\
\end{eqnarray*}\]</span>
Then, <span class="math inline">\(\forall \delta\)</span>
<span class="math display">\[\begin{eqnarray*}
a \alpha(\delta^*) + b \beta(\delta^*) \leq a \alpha(\delta) + b \beta(\delta)
\end{eqnarray*}\]</span></p>
<p><strong>Proof:</strong>
Let <span class="math inline">\(C\)</span> be the critical region, <span class="math inline">\(C^c\)</span> is the complement of the critical region. (Note, this proof is for the discrete case, the continuous case is analogous.)</p>
<p><span class="math display">\[\begin{eqnarray*}
a \alpha(\delta) + b \beta(\delta) &amp;=&amp; a \sum_{\underline{x} \in C} f_0(\underline{x}) + b \sum_{\underline{x} \in C^c} f_1(\underline{x})\\
&amp;=&amp; a \sum_{\underline{x} \in C} f_0(\underline{x}) + b [1 - \sum_{\underline{x} \in C} f_1(\underline{x})]\\
&amp;=&amp; b + \sum_{\underline{x} \in C} [ af_0(\underline{x}) - b f_1(\underline{x})]\\
\end{eqnarray*}\]</span>
We want the parenthetical part to be negative <span class="math inline">\(\forall \underline{x} \in C\)</span> and positive <span class="math inline">\(\forall \underline{x} \in C^c\)</span>. That is, choose <span class="math inline">\(c\)</span> such that:</p>
<p><span class="math display">\[\begin{eqnarray*}
a f_0(\underline{x}) - bf_1(\underline{x}) &amp;&lt;&amp; 0 \ \ \ \forall \underline{x} \in C\\
a f_0(\underline{x}) - bf_1(\underline{x}) &amp;&gt;&amp; 0 \ \ \ \forall \underline{x} \in C^c\\
\mbox{if } a f_0(\underline{x}) - bf_1(\underline{x}) = 0 &amp;&amp; \mbox{it doesn't matter if } \underline{x} \in C \mbox{ or } \in C^c\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\Rightarrow \delta^*: \{ \mbox{reject } H_0 \mbox{ if } a f_0(\underline{x}) &lt; b f_1(\underline{x}) \} \\
\mbox{note: } \delta^*: \bigg\{ \mbox{reject } H_0 \mbox{ if } \frac{f_1(\underline{x})}{f_0(\underline{x})} &gt; \frac{a}{b} \bigg\}
\end{eqnarray*}\]</span>
(the likelihood ratio). That is, <span class="math inline">\(\delta^*\)</span> minimizes the linear combination of errors <span class="math inline">\(a \alpha(\delta) + b \beta(\delta)\)</span> over all tests, <span class="math inline">\(\delta\)</span>.</p>
</div>
<p>But what if, like before, we want to minimize the probability of a type II error (subject to <span class="math inline">\(\alpha(\delta) \leq \alpha_0\)</span>)?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-51" class="theorem"><strong>Theorem 8.2  </strong></span>(Theorem 9.2.2, <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>, Neyman-Pearson Lemma)</p>
<p>Let $ k &gt; 0$. Suppose
<span class="math display">\[\begin{eqnarray*}
\delta^*: &amp;&amp; \{ \mbox{ reject } H_0 \mbox{ if } k f_0(\underline{x}) &lt; f_1(\underline{x})\\
&amp;&amp; \mbox{ do not reject } H_0 \mbox{ if } k f_0(\underline{x}) &gt; f_1(\underline{x}) \} \\
\end{eqnarray*}\]</span></p>
<p>Let <span class="math inline">\(\delta\)</span> be another test procedure such that:
<span class="math display">\[\begin{eqnarray*}
\alpha(\delta) \leq \alpha(\delta^*)&amp;  \&amp; &amp; \alpha(\delta) &lt; \alpha(\delta^*) \\
\rightarrow \mbox{\bf{Then: }} \beta(\delta^*) \leq \beta(\delta)&amp;  \&amp; &amp; \beta(\delta^*) &lt; \beta(\delta) \\
\end{eqnarray*}\]</span>
(find k such that <span class="math inline">\(\alpha(\delta^*) \leq \alpha_0.)\)</span></p>
<p><strong>Proof:</strong></p>
<p><span class="math display">\[\begin{eqnarray*}
k \alpha(\delta^*) +  \beta(\delta^*) \leq k \alpha(\delta) +  \beta(\delta)\\
\mbox{ if } \alpha(\delta^*) \geq \alpha(\delta)\\
\mbox{ then } \beta(\delta^*) \leq \beta(\delta)
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-52" class="example"><strong>Example 8.6  </strong></span>Consider a situation where we’re trying to figure out if someone is purely guessing on a multiple choice questions with 5 possible answers. Although we wouldn’t typically think about this type of situation with a simple-vs-simple set of hypotheses, we’ll compare a random guessing hypothesis <span class="math inline">\((\theta = 0.2)\)</span> to a hypothesis where we are able to rule out one of the options <span class="math inline">\((\theta = 0.25)\)</span>. We will collect 25 observations, and we’ll use the binomial distribution to describe the probability model.</p>
<p><span class="math inline">\(H_0: \theta = 0.2\)</span>\
<span class="math inline">\(H_1: \theta = 0.25\)</span>\</p>
<p>To calculate the likelihoods, we’ll use a binomial distribution. Note that almost always, the joint distribution is the product of the <span class="math inline">\(n\)</span> marginal distributions. But with a binomial distribution, the likelihood is already a joint distribution.</p>
<p><span class="math display">\[\begin{eqnarray*}
f_0(x) &amp;=&amp; {n \choose x} (0.2)^x (0.8)^{n-x}\\
f_1(x) &amp;=&amp; {n \choose x} (0.25)^x (0.75)^{n-x}\\
\end{eqnarray*}\]</span></p>
<p>We know that if we want to find a test that minimized the type I plus type II error <span class="math inline">\((\alpha(\delta) + \beta(\delta))\)</span>, we will find the test that rejects if <span class="math display">\[\frac{f_1(x)}{f_0(x)} &gt; \frac{a}{b} = \frac{1}{1} = 1.\]</span></p>
<p>To find the actual test, use the likelihoods provided above:</p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{f_1(x)}{f_0(x)}  = (1.25)^x (0.9375)^{n-x} &amp;&gt;&amp; 1\\
x \ln(1.25) + (n-x) \ln(0.9375) &amp;&gt;&amp; 0\\
x( \ln(1.25) - \ln(0.9375) ) &amp;&gt;&amp; -n \ln(0.9375)\\
x &amp;&gt;&amp; \frac{-n \ln(0.9375)}{( \ln(1.25) - \ln(0.9375) )}\\
x &amp;&gt;&amp; 5.61\\
x &amp;\geq&amp; 6\\
\end{eqnarray*}\]</span></p>
<p>That is to say, we’ll minimize the sum of the two types of errors by creating the following test: <span class="math display">\[\delta = \{ \mbox{reject } H_0 \mbox{ if } X \geq 6 \}.\]</span></p>
<p>Which leads to:
<span class="math display">\[\begin{eqnarray*}
\alpha(\delta) = 1 - \texttt{pbinom}(5,25,0.2) = 0.383\\
\beta(\delta) = \texttt{pbinom}(5,25,0.4) = 0.029\\
\end{eqnarray*}\]</span></p>
<p>While having a type II error rate close to 3% is lovely, we find that a type I error rate of 38.3% is **WAY} too big. In fact, we need to control the type I error rate in order to be able to confidently reject <span class="math inline">\(H_0\)</span> and promote our scientific claims. So, instead of using <span class="math inline">\(a/b\)</span>, we’ll find the cutoff that keeps the type I error under 0.01. That is, find <span class="math inline">\(k\)</span> such that:</p>
<p><span class="math display">\[\delta_{NP} = \{ \mbox{reject } H_0 \mbox{ if } X \geq k \}, \ \ \ \ \  \alpha(\delta) \leq 0.01.\]</span></p>
<p>We can calculate the type I error rate directly from the binomial distribution:</p>
<p><span class="math display">\[\begin{eqnarray*}
\alpha(\delta_{NP}) = 1 - \texttt{pbinom}(10, 25, .2) = 0.00555
\end{eqnarray*}\]</span>
leads to a test of:</p>
<p><span class="math display">\[\delta_{NP} = \{ \mbox{reject } H_0 \mbox{ if } X \geq 11 \}.\]</span></p>
<p>Note that the type II error rate for <span class="math inline">\(\delta_{NP}\)</span> can be calculated: <span class="math display">\[\beta{\delta_{N}} = \texttt{pbinom}(10, 25, .25) = 0.97.\]</span></p>
<p>We aren’t very happy with such a large type II error rate, but it turns out that it is difficult to distinguish between the two hypotheses above. And so to control the type I error rate, we need to be pretty conservative, and the type II error becomes pretty large.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-53" class="example"><strong>Example 8.7  </strong></span>Let’s say we have 2 batches of paint one of which is quick-dry. The paint is unlabeled, and we forgot which was which! We paint 5 boards from batch 1 and record the drying time. We think batch 1 is quick dry. We also believe that the drying times are normally distributed with a standard deviation of 5 min.
<span class="math display">\[\begin{eqnarray*}
H_0: \theta=25 \mbox{ min}\\
H_1: \theta=10 \mbox{ min}\\
\end{eqnarray*}\]</span></p>
<p>For funzies, let’s find <span class="math inline">\(\delta^*\)</span> that minimizes <span class="math inline">\(a \alpha(\delta^*) + b \beta(\delta^*).\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
f_0(\underline{x}) &amp;=&amp; \frac{1}{\sqrt{2 \pi 25}} \exp \Bigg( \frac{-1}{2 \cdot 25} \sum(x_i - 25)^2\Bigg)\\
f_1(\underline{x}) &amp;=&amp; \frac{1}{\sqrt{2 \pi 25}} \exp \Bigg( \frac{-1}{2 \cdot 25} \sum(x_i - 10)^2\Bigg)\\
\frac{f_1(\underline{x})}{f_0(\underline{x})} &amp;=&amp; \exp\Bigg( \frac{-1}{2 \cdot 25} \sum( (x_i - 10)^2 - (x_i - 25)^2 )\Bigg)\\
&amp;=&amp;  \exp\Bigg( \frac{-1}{2 \cdot 25} \sum( x_i^2 -20x_i + 100 - x_i^2 + 50x_i - 625 ) \Bigg)\\
&amp;=&amp;  \exp\Bigg( \frac{-1}{50} \sum( 30x_i - 525 ) \Bigg)\\
&amp;=&amp;  \exp\Bigg( \frac{-30}{50} \sum( x_i - 17.5 )\Bigg)\\
&amp;=&amp;  \exp\Bigg( \frac{-3 n}{5} (\overline{x} - 17.5 ) \Bigg)\\
&amp;&gt; &amp; \frac{a}{b}\\
\overline{x} - 17.5 &amp;&lt;&amp; \frac{-5}{3n} \ln\bigg(\frac{a}{b}\bigg)\\
\overline{x} &amp;&lt;&amp; 17.5 -\frac{5}{3n} \ln\bigg(\frac{a}{b}\bigg)\\
\end{eqnarray*}\]</span></p>
<ul>
<li>If a type I error is worse <span class="math inline">\((\alpha(\delta) &lt; &lt; )\)</span> then <span class="math inline">\(\frac{a}{b} &gt; 1,\)</span> and your rejection rule would be when <span class="math inline">\(\overline{x} &lt; c_1\)</span> where <span class="math inline">\(c_1 &lt; 17.5\)</span> (we reject <span class="math inline">\(H_0\)</span> less often).</li>
<li>If a type II error is worse <span class="math inline">\((\beta(\delta) &lt; &lt; )\)</span> then <span class="math inline">\(\frac{a}{b} &lt; 1,\)</span> and your rejection rule would be when <span class="math inline">\(\overline{x} &lt; c_2\)</span> where <span class="math inline">\(c_2 &gt; 17.5\)</span> (we reject <span class="math inline">\(H_0\)</span> more often).</li>
</ul>
<p>What if <span class="math inline">\(a=k\)</span> and <span class="math inline">\(b=1\)</span> and <span class="math inline">\(\alpha(\delta) = \alpha_0\)</span>?
<span class="math display">\[\begin{eqnarray*}
\delta^*: \{ \mbox{reject } H_0 \mbox{ if } \overline{x} &lt; 17.5 - \frac{5}{3n} \ln(k) \} \\
\end{eqnarray*}\]</span>
<span class="math display">\[\begin{eqnarray*}
P(\overline{X} &lt; 17.5 - \frac{5}{3n} \ln(k)  | \theta=25) &amp;=&amp; 0.05\\
P( Z &lt; \frac{17.5 -5/3n \ln(k) - 25}{5/\sqrt{n}} ) &amp;=&amp; 0.05\\
\frac{17.5 -5/3n \ln(k) - 25}{5/\sqrt{n}} &amp;=&amp; -1.68\\
\ln(k) &amp;=&amp; -11.23\\
\delta^*: \{ \mbox{reject } H_0 \mbox{ if } \overline{x} &lt; 21.24 \} \\
\mbox{note: } P(\overline{X} &gt; 21.24 | \theta=10) &amp;=&amp; 0\\
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="uniformly-most-powerful-tests" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Uniformly Most Powerful Tests<a class="anchor" aria-label="anchor" href="#uniformly-most-powerful-tests"><i class="fas fa-link"></i></a>
</h2>
<p>In the previous section we talked about Uniformly Most Powerful (UMP) tests without explicitly defining them. Let our hypotheses be more general.
<span class="math display">\[\begin{eqnarray*}
H_0: \theta \in \Omega_0\\
H_1: \theta \in \Omega_1\\
\end{eqnarray*}\]</span></p>
<p>Let <span class="math inline">\(\delta^*\)</span> be a UMP test at level <span class="math inline">\(\alpha_0, \ \ \alpha(\delta^*) \leq \alpha_0\)</span>, then for any other <span class="math inline">\(\delta\)</span> s.t. <span class="math inline">\(\alpha(\delta) \leq \alpha_0\)</span>:
<span class="math display">\[\begin{eqnarray*}
\pi(\theta | \delta) \leq \pi(\theta | \delta^*) \ \ \forall \theta \in \Omega_1
\end{eqnarray*}\]</span>
That is, for any test, <span class="math inline">\(\delta\)</span>, of equivalent size, <span class="math inline">\(\delta^*\)</span> has more power. (Note, for <span class="math inline">\(\theta \in \Omega_1, \pi(\theta | \delta) = 1 - \beta(\delta)\)</span> if <span class="math inline">\(\Omega_1\)</span> is simple.)</p>
<div id="monotone-likelihood-ratio" class="section level3" number="8.3.1">
<h3>
<span class="header-section-number">8.3.1</span> Monotone Likelihood Ratio<a class="anchor" aria-label="anchor" href="#monotone-likelihood-ratio"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(T = r(\underline{X})\)</span> be a statistic. We say <span class="math inline">\(f(\underline{x} | \theta)\)</span> has a monotone likelihood ratio in the statistic <span class="math inline">\(T\)</span> if <span class="math inline">\(\forall \theta_1, \theta_2 \in \Omega\)</span> with <span class="math inline">\(\theta_1 &lt; \theta_2\)</span> then:
<span class="math display">\[\begin{eqnarray*}
\frac{f(\underline{x} | \theta_2)}{f(\underline{x} | \theta_1)}
\end{eqnarray*}\]</span></p>
<ul>
<li>depends on <span class="math inline">\(\underline{x}\)</span> only through <span class="math inline">\(T\)</span>
</li>
<li>is a nondecreasing function of <span class="math inline">\(T\)</span> over the range of possible values of <span class="math inline">\(T\)</span>
</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-54" class="example"><strong>Example 8.8  </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \sim Gamma(10, \theta), E[X] = 10/\theta\)</span>.
<span class="math display">\[\begin{eqnarray*}
f(x | \theta) &amp;=&amp; \frac{\theta^{10}}{\Gamma(10)} x^{10 - 1} e ^{-x \theta} \ \ \ \ 0 \leq x \leq \infty
\mbox{let } \theta_1 &lt; \theta_2\\
\frac{f(\underline{x} | \theta_2)}{f(\underline{x} | \theta_1)} &amp;=&amp; \frac{\theta_2^{10n}}{\theta_1^{10n}} e^{- \sum x_i \theta_2 + \sum x_i \theta_1}\\
&amp;=&amp; \Bigg( \frac{\theta_2}{\theta_1}\Bigg)^{10n} e^{-\sum x_i (\theta_2 - \theta_1)}\\
T &amp;=&amp; - \sum X_i\\
\end{eqnarray*}\]</span>
<span class="math inline">\(f(\underline{X} | \theta)\)</span> has a monotone likelihood ratio in <span class="math inline">\(- \sum X_i\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-55" class="theorem"><strong>Theorem 8.3  </strong></span>(<span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span> 9.3.1)</p>
<p>Without loss of generality, consider:
<span class="math display">\[\begin{eqnarray*}
H_0: \theta \leq \theta_0\\
H_1: \theta &gt; \theta_0
\end{eqnarray*}\]</span></p>
<p>Suppose that <span class="math inline">\(f(\underline{x} | \theta)\)</span> has a monotone likelihood ratio in <span class="math inline">\(T=r(\underline{X})\)</span>. Let <span class="math inline">\(c\)</span> be a constant such that <span class="math inline">\(P(T \geq c | \theta = \theta_0) = \alpha_0\)</span>.
<span class="math display">\[\begin{eqnarray*}
\delta: \{\mbox{reject } H_0 \mbox{ if } T \geq c \}
\end{eqnarray*}\]</span>
is a UMP test at level <span class="math inline">\(\alpha_0\)</span>.</p>
<p>Note: if <span class="math inline">\(H_0: \theta \geq \theta_0 \rightarrow T \leq c\)</span> is UMP.</p>
</div>
<p>Read through the proof (pages 562-563 of <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>). Because parameter spaces consist of consecutive intervals, and likelihood ratio is monotone, it is straightforward to see where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are minimized.</p>
<p>You shouldn’t have to memorize the directionality associated with the theorem for one-sided UMP tests (instead use the intuition behind the theorem). For example, consider the opposite hypotheses:
<span class="math display">\[\begin{eqnarray*}
H_0: \theta \geq \theta_0\\
H_1: \theta &lt; \theta_0
\end{eqnarray*}\]</span></p>
<ol style="list-style-type: decimal">
<li>We know that <span class="math inline">\(T \uparrow\)</span> produces <span class="math inline">\(\frac{f(\underline{X} | \theta_2)}{f(\underline{X} | \theta_1)} \uparrow\)</span>
</li>
<li>We can visualize <span class="math inline">\(\theta_2 \in \Omega_0\)</span> and <span class="math inline">\(\theta_1 \in \Omega_1\)</span> (convince yourself that this makes sense).</li>
<li>So, a small T would mean there is more evidence toward the alternative. Therefore, reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(T \leq c\)</span>.</li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-56" class="example"><strong>Example 8.9  </strong></span>Suppose <span class="math inline">\(X_1, X_2, \ldots, X_n \stackrel{iid}{\sim} N(0, \sigma^2)\)</span>, and it is desired to test:
<span class="math display">\[\begin{eqnarray*}
H_0: \sigma^2 \leq 2\\
H_1: \sigma^2 &gt; 2
\end{eqnarray*}\]</span></p>
<ol style="list-style-type: decimal">
<li>Find the UMP test at level <span class="math inline">\(\alpha_0\)</span>.</li>
<li>Assume <span class="math inline">\(n=20, \alpha_0 = 0.01\)</span>, find the power function for the test in part 1.</li>
</ol>
<p>Let <span class="math inline">\(\sigma_1^2 &lt; \sigma_2^2, \sigma_1^2, \sigma_2^2 \in \Omega\)</span>.
<span class="math display">\[\begin{eqnarray*}
\frac{f(\underline{x} | \sigma_2^2)}{f(\underline{x} | \sigma_1^2)} &amp;=&amp; \Bigg(\frac{\sigma_1}{\sigma_2} \Bigg)^n \frac{e^{-\sum x_i^2 / 2 \sigma_2^2}}{e^{-\sum x_i^2 / 2 \sigma_1^2}}\\
&amp;=&amp; \Bigg(\frac{\sigma_1}{\sigma_2} \Bigg)^n e^{\sum x_i^2 ( 1/2 \sigma_1^2 - 1/2 \sigma_2^2)}\\
( 1/2 \sigma_1^2 - 1/2 \sigma_2^2) &amp;&gt;&amp; 0 \\
T &amp;=&amp; \sum X_i^2\\
\delta: &amp;&amp; \{ \mbox{reject } H_0 \mbox{ if } \sum x_i^2 \geq c \}
\end{eqnarray*}\]</span>
is UMP at <span class="math inline">\(\alpha_0\)</span>.
For <span class="math inline">\(n=20, \alpha_0 = 0.01\)</span>, find <span class="math inline">\(c\)</span>.
<span class="math display">\[\begin{eqnarray*}
P(\sum X_i^2 \geq c | \sigma^2 = 2) &amp;=&amp; 0.01\\
P(\sum X_i^2/2 \geq c/2 | \sigma^2 = 2) &amp;=&amp; 0.01\\
c/2 &amp;=&amp; 37.57\\
c&amp;=&amp; 75.14\\
\delta: &amp;&amp; \{ \mbox{reject } H_0 \mbox{ if } \sum x_i^2 \geq 75.14 \}
\end{eqnarray*}\]</span>
The power function is found by calculating the probability of rejecting <span class="math inline">\(H_0\)</span> for various values of <span class="math inline">\(\sigma^2\)</span>.
<span class="math display">\[\begin{eqnarray*}
\pi(\sigma^2 | \delta) &amp;=&amp; P( \mbox{reject } H_0 | \sigma^2)\\
&amp;=&amp; P (\sum X_i^2 \geq 75.14 | \sigma^2)\\
&amp;=&amp; P (\sum X_i^2 / \sigma^2 \geq 75.14 / \sigma^2)\\
&amp;=&amp; 1 - \chi^2_{20}(75.14 / \sigma^2)\\
\pi(\sigma^2 = 6 | \delta) &amp;=&amp; 1 - \chi^2_{20} (12.52) \approx 0.80\\
\pi(\sigma^2 = 3 | \delta) &amp;=&amp; 1 - \chi^2_{20} (25.05) \approx 0.20\\
\pi(\sigma^2 = 4 | \delta) &amp;=&amp; 1 - \chi^2_{20} (18.79) \approx 0.45\\
\end{eqnarray*}\]</span></p>
</div>
<div id="power-functions-revisited" class="section level4" number="8.3.1.1">
<h4>
<span class="header-section-number">8.3.1.1</span> Power functions revisited<a class="anchor" aria-label="anchor" href="#power-functions-revisited"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math inline">\(X_i \sim N(\mu, \sigma^2 (\mbox{known})), \delta_1: \{ \mbox{reject } H_0 \mbox{ if } \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \geq z_{1 - \alpha_0} \}\)</span>
<span class="math display">\[\begin{eqnarray*}
H_0: \mu \leq \mu_0\\
H_1: \mu &gt; \mu_0\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\pi(\mu | \delta_1) &amp;=&amp; P\bigg(\frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \geq z_{1 - \alpha_0} | \mu \bigg)\\
&amp;=&amp; P\bigg(\frac{\overline{X} - \mu}{\sigma / \sqrt{n}} \geq \frac{z_{1 - \alpha_0} \sigma / \sqrt{n} + \mu_0 - \mu}{\sigma / \sqrt{n}} | \mu \bigg)\\
&amp;=&amp; 1 - \Phi \bigg( \frac{z_{1 - \alpha_0} \sigma / \sqrt{n} + \mu_0 - \mu}{\sigma / \sqrt{n}} \bigg)
\end{eqnarray*}\]</span></p>
<p>Alternatively, <span class="math inline">\(\delta_2: \{ \mbox{reject } H_0 \mbox{ if } \frac{\overline{X} - \mu_0}{\sigma / \sqrt{n}} \leq z_{\alpha_0} \}\)</span>
<span class="math display">\[\begin{eqnarray*}
H_0: \mu \geq \mu_0\\
H_1: \mu &lt; \mu_0\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\pi(\mu | \delta_2) &amp;=&amp;  \Phi \bigg( \frac{z_{\alpha_0} \sigma / \sqrt{n} + \mu_0 - \mu}{\sigma / \sqrt{n}} \bigg)
\end{eqnarray*}\]</span></p>
<p>Last, <span class="math inline">\(\delta_3: \{ \mbox{reject } H_0 \mbox{ if } \frac{|\overline{X} - \mu_0|}{\sigma / \sqrt{n}} \geq z_{1-{\alpha_0}/2} \}\)</span>
<span class="math display">\[\begin{eqnarray*}
H_0: \mu = \mu_0\\
H_1: \mu \ne \mu_0\\
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\pi(\mu | \delta_3) &amp;=&amp; 1 - \Phi \bigg( \frac{z_{1 - {\alpha_0}/2} \sigma / \sqrt{n} + \mu_0 - \mu}{\sigma / \sqrt{n}} \bigg)  + \Phi \bigg( \frac{ z_{ {\alpha_0}/2} \sigma / \sqrt{n} + \mu_0 - \mu}{\sigma / \sqrt{n}} \bigg)
\end{eqnarray*}\]</span></p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="figs/power.png" alt="Power as a function of mu.  Top plot describes the delta1 test; middle plot describes the delta2 test; bottom plot describes the delta3 (two-sided!) test." width="80%"><p class="caption">
Figure 1.1: Power as a function of mu. Top plot describes the delta1 test; middle plot describes the delta2 test; bottom plot describes the delta3 (two-sided!) test.
</p>
</div>
</div>
</div>
<div id="two-sided-tests" class="section level3" number="8.3.2">
<h3>
<span class="header-section-number">8.3.2</span> Two sided tests<a class="anchor" aria-label="anchor" href="#two-sided-tests"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="the-t-test" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> The t-test<a class="anchor" aria-label="anchor" href="#the-t-test"><i class="fas fa-link"></i></a>
</h2>
<p>The t-test deserves special attention. Remember, it’s a test of <span class="math inline">\(\mu\)</span>, but the variance is <strong>unknown</strong>. If you work out the ratio of the likelihoods, it’ll be pretty hard to find an MLR statistic. But before we get to likelihood ratios in discussing the t-test, first let’s think about the t-test paradigm.</p>
<div class="example">
<p><span id="exm:unlabeled-div-57" class="example"><strong>Example 8.10  </strong></span>Let’s say we have a population of radon detectors whose accuracy we’d like to test. Because we don’t want to open all the packages, we randomly select 12 of them and put them in a room with 105 picocuries per liter (pCi/l) of radon.
<span class="math display">\[\begin{eqnarray*}
H_0: \mu \geq 105 \mbox{ pCi/l}\\
H_1: \mu &lt; 105 \mbox{ pCi/l}\\
\end{eqnarray*}\]</span>
<span class="math inline">\(\overline{x} = 104.13, \sum(x_i - \overline{x})^2 = 931, s=\sqrt{\frac{\sum(x_i - \overline{x})^2}{n-1}} = \sqrt{931/11} = 9.20\)</span>.</p>
</div>
<p>We will reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\overline{X}\)</span> is too small. How small does <span class="math inline">\(\overline{X}\)</span> have to be? Let <span class="math inline">\(U = \frac{\overline{X} - \mu_0}{s/\sqrt{n}}.\)</span>
<span class="math display">\[\begin{eqnarray*}
\delta: \{ \mbox{reject } H_0 \mbox{ if } U \leq c \}
\end{eqnarray*}\]</span>
As seen in previous sections, often the test boils down to computing the constant <span class="math inline">\(c\)</span> which gives a test of size <span class="math inline">\(\alpha_0\)</span>. In particular, we need to know that the boundary value will give the maximum type I error over <span class="math inline">\(\mu \in \Omega_0\)</span>. Note that we can find <span class="math inline">\(c\)</span> such that <span class="math inline">\(P( t_{11} \leq c) = \alpha_0\)</span>. If <span class="math inline">\(\mu=\mu_0\)</span> then <span class="math inline">\(U \sim t_{11}\)</span>. But our null is composite: <span class="math inline">\(\mu \geq \mu_0\)</span>. What is the distribution of <span class="math inline">\(U\)</span> if <span class="math inline">\(\mu &gt; \mu_0?\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{let } U^* = \frac{\overline{X} - \mu}{s/\sqrt{n}} &amp;\mbox{and}&amp; W = \frac{\mu - \mu_0}{s/\sqrt{n}}\\
U = U^* + W &amp; U^* \sim t_{11} &amp; W &gt; 0\\
P(U \leq c) = P(U^* + W \leq c)\\
= P( U^* \leq c  - W)\\
&lt; P( U^* \leq c) = \alpha_0\\
\end{eqnarray*}\]</span>
That is, if <span class="math inline">\(\mu &gt; \mu_0, P(U \leq c) &lt; \alpha_0\)</span>. So,
<span class="math display">\[\begin{eqnarray*}
\delta: \{ \mbox{reject } H_0 \mbox{ if } U \leq c \}
\end{eqnarray*}\]</span>
is size <span class="math inline">\(\alpha_0\)</span> if <span class="math inline">\(c = t_{n-1}^{-1}(\alpha_0)\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-58" class="example"><strong>Example 8.11  </strong></span>(Radon continued)</p>
<p><span class="math inline">\(\alpha=0.05, df = 11\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\delta:&amp;&amp; \{ \mbox{reject } H_0 \mbox{ if } U \leq c = -1.812\}\\
U &amp;=&amp; \frac{104.13 - 105}{9.2/\sqrt{12}} = -0.327 \rightarrow \mbox{ do not reject } H_0\\
\mbox{p-value} &amp;=&amp; P( t_{11} &lt; -0.327) \approx 0.35
\end{eqnarray*}\]</span></p>
</div>
<div id="power-of-the-t-test" class="section level3" number="8.4.1">
<h3>
<span class="header-section-number">8.4.1</span> Power of the t-test<a class="anchor" aria-label="anchor" href="#power-of-the-t-test"><i class="fas fa-link"></i></a>
</h3>
<span class="math display">\[\begin{eqnarray*}
H_0: \mu \geq \mu_0\\
H_1: \mu &lt; \mu_0
\end{eqnarray*}\]</span>
<p>How would you calculate the power at <span class="math inline">\(\mu = 103.5\)</span>?
<span class="math display">\[\begin{eqnarray*}
\pi(\mu, \sigma^2 | \delta) &amp;=&amp; P( U \leq -1.812\bigg)\\
&amp;=&amp; P\bigg(\frac{\overline{X} - 105}{s/\sqrt{n}} \leq -1.812\bigg)\\
&amp;=&amp; P\bigg( \overline{X} \leq 105 - 1.812 \cdot s/\sqrt{n}\bigg)\\
&amp;=&amp; P\bigg( \frac{\overline{X} - 103.5}{s/\sqrt{n}} \leq \frac{1.5 - 1.812 \cdot s/\sqrt{n}}{s/\sqrt{n}}\bigg)\\
&amp;=&amp; P\bigg( t_{n-1} \leq \frac{1.5 - 1.812 \cdot s/\sqrt{n}}{s/\sqrt{n}}\bigg)\\
&amp;=&amp; P\bigg( t_{n-1} \leq \mbox{ a RANDOM VARIABLE!}\bigg)\\
&amp;\approx&amp; P\bigg( t_{n-1} \leq \frac{1.5 - 1.812 \cdot 9.2/\sqrt{12}}{9.2/\sqrt{12}}\bigg)\\
0.10 &amp; \approx \leq&amp; \pi(\mu=103.5 | \delta) \approx \leq 0.15\\
0.85 &amp;\approx \leq&amp; \beta \approx \leq 0.9\\
\end{eqnarray*}\]</span></p>
<p>The point is that in order to calculate the power for the t-test, you end up with a probability statement which has random variables on both sides of the inequality. It isn’t the worst idea to approximate the power by considering the righthand side to be a number. However, the true power function is based on what is called the <em>non-central t-distribution</em>.</p>
<p>What if we have a two-sided test?
<span class="math display">\[\begin{eqnarray*}
H_0: \mu = \mu_0\\
H_1: \mu \ne \mu_0
\end{eqnarray*}\]</span>
now we reject if <span class="math inline">\(|U| &gt; c\)</span>, and we want <span class="math inline">\(P(|U| &gt; c | \mu= \mu_0) = \alpha_0\)</span> (or <span class="math inline">\(P(U &gt; c_1 \mbox{ or} U &lt; c_2 | \mu=\mu_0) = \alpha_0\)</span>.)</p>
<p>If n=12, <span class="math inline">\(U = \frac{\overline{X} - \mu_0}{s/\sqrt{n}} \sim t_{11}\)</span>.
<span class="math display">\[\begin{eqnarray*}
P(|U| &gt; c | \mu= \mu_0) = \alpha_0 = 0.10 &amp;\rightarrow&amp; c=1.363\\
P(U &gt; c_1 | \mu=\mu_0)=0.025,  P(U &lt; c_2 | \mu=\mu_0) = 0.075 &amp;\rightarrow&amp; c_1=2.201, c_2 = -1.58\\
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-59" class="example"><strong>Example 8.12  </strong></span>(Radon example continued)
<span class="math display">\[\begin{eqnarray*}
H_0: \mu = 105 \mbox{ pCi/l}\\
H_1: \mu \ne 105 \mbox{ pCi/l}\\
\end{eqnarray*}\]</span>
<span class="math inline">\(\alpha_0 = 0.05, P(|U| &gt; c | \mu= \mu_0) = 0.05 \rightarrow c=2.2014\)</span>.\
<span class="math display">\[\begin{eqnarray*}
\mbox{p-value} &amp;=&amp; P(U &lt; -0.327 \mbox{ or } U &gt; 0.327)\\
&amp;=&amp; 2 P( U &gt; 0.327)\\
&amp;\approx&amp; 2 \cdot 0.35 = 0.7
\end{eqnarray*}\]</span></p>
</div>
</div>
</div>
<div id="likelihood-ratio-tests" class="section level2" number="8.5">
<h2>
<span class="header-section-number">8.5</span> Likelihood Ratio Tests<a class="anchor" aria-label="anchor" href="#likelihood-ratio-tests"><i class="fas fa-link"></i></a>
</h2>
<p>Likelihood ratio tests are arguably the most used test in statistics. There are good reasons for this!</p>
<ol style="list-style-type: decimal">
<li>As we have seen, the theory suggests that tests based on likelihoods are often quite powerful (and typically more powerful than other tests for the same hypotheses).</li>
<li>A likelihood ratio test gives a test statistic we can use to make decisions.</li>
<li>Under general regularity conditions, the asymptotic distribution of the likelihood ratio itself is known.</li>
</ol>
<p>Suppose we have
<span class="math display">\[\begin{eqnarray*}
H_0: \theta \in \Omega_0\\
H_1: \theta \in \Omega_1
\end{eqnarray*}\]</span>
<span class="math display">\[\begin{eqnarray*}
\Lambda(\underline{x}) = \frac{\sup_{\theta \in \Omega_1} f(\underline{x} | \theta)}{\sup_{\theta \in \Omega_0} f(\underline{x} | \theta)}
\end{eqnarray*}\]</span>
Just like Neyman-Pearson!! We reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\Lambda(\underline{x}) \geq k\)</span> (i.e., if the likelihood is big for <span class="math inline">\(\Omega_1\)</span> compared to <span class="math inline">\(\Omega_0).\)</span> As before, <span class="math inline">\(k\)</span> is chosen so that the test has a given size, <span class="math inline">\(\alpha_0.\)</span> However, for likelihood ratio tests, we do not have ideas of uniformly most powerful. In fact, two-sided tests are generally NOT UMP (that is, it is not typically possible to find a UMP test in a two-sided setting). The one-sided test for <span class="math inline">\(\mu &lt; \mu_0\)</span> is more powerful than the two-sided test on one side; the one-sided test for <span class="math inline">\(\mu &gt; \mu_0\)</span> is more powerful than the two-sided test on the other side.</p>
<div class="example">
<p><span id="exm:unlabeled-div-60" class="example"><strong>Example 8.13  </strong></span>Consider <span class="math inline">\(X_1, X_2, \ldots, X_n \sim N(\mu, \sigma^2)\)</span> where both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are unknown.
<span class="math display">\[\begin{eqnarray*}
H_0: \mu \geq \mu_0\\
H_1: \mu &lt; \mu_0
\end{eqnarray*}\]</span>
When is the likelihood maximized? At the MLEs!! <span class="math inline">\(\hat{\mu} = \overline{X}, \hat{\sigma}^2 = (1/n) \sum(X_i - \overline{X})^2\)</span>.
<span class="math display">\[\begin{eqnarray*}
\Lambda(\underline{x}) &amp;=&amp; \frac{\sup_{\mu &lt; \mu_0} f(\underline{x} | \mu, \sigma^2)}{\sup_{\mu \geq \mu_0} f(\underline{x} | \mu, \sigma^2)}\\
&amp;=&amp; \frac{ f(\underline{x} | \hat{\mu}_1, \hat{\sigma}_1^2)}{ f(\underline{x} | \hat{\mu_0}, \hat{\sigma}_0^2)}\\
\mbox{if } \overline{X} &lt; \mu_0 &amp;\rightarrow&amp; (\hat{\mu}, \hat{\sigma}^2) \in \Omega_1\\
(\mu, \sigma^2) &amp;\in&amp; \Omega_0 \mbox{ is maximized at } \hat{\mu}_0 = \mu_0, \hat{\sigma}_0^2 = (1/n) \sum (X_i - \mu_0)^2
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\Lambda(\underline{x}) &amp;=&amp; \frac{(1/\sqrt{2 \pi \hat{\sigma}_1^2})^n \exp(\frac{-1}{2 \hat{\sigma}_1^2} \sum(x_i - \hat{\mu}_1)^2)} {(1/\sqrt{2 \pi \hat{\sigma}_0^2})^n \exp(\frac{-1}{2 \hat{\sigma}_0^2} \sum(x_i - \hat{\mu}_0)^2)}\\
&amp;=&amp; \bigg(\frac{\hat{\sigma}_0^2}{\hat{\sigma}_1^2} \bigg)^{n/2} \frac{\exp \frac{-1}{2 (1/n) \sum(x_i - \overline{x})^2} \sum(x_i - \overline{x})^2}{\exp \frac{-1}{2 (1/n) \sum(x_i - \mu_0)^2} \sum(x_i - \mu_0)^2}\\
&amp;=&amp; \bigg( \frac{(1/n) \sum(x_i - \mu_0)^2}{(1/n) \sum(x_i - \overline{x})^2} \bigg)^{n/2}\\
\mbox{if } \Lambda(\underline{x}) &amp;\geq&amp; k' \rightarrow \mbox{ reject } H_0
\end{eqnarray*}\]</span></p>
<p>note: <span class="math inline">\(\sum(X_i - \mu_0)^2 = \sum(X_i - \overline{X})^2 + n(\overline{X} - \mu_0)^2\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\Lambda(\underline{x}) &amp;=&amp; \bigg( \frac{\sum(x_i - \overline{x})^2 + n(\overline{x} - \mu_0)^2}{\sum(x_i - \overline{x})^2} \bigg)^{n/2} \geq k' \\
&amp;=&amp; \bigg ( 1 + \frac{(\overline{x} - \mu_0)^2}{\sum(x_i - \overline{x})^2 / n} \bigg)^{n/2} \geq k'\\
&amp;\Rightarrow&amp; \frac{\overline{X} - \mu_0}{\sqrt{\sum(X_i - \overline{X})^2/(n-1)} / \sqrt{n}} \leq k
\end{eqnarray*}\]</span>
(because <span class="math inline">\(\overline{x} &lt; \mu_0\)</span>).</p>
<blockquote>
<p><strong>We derived the t-test!!!!</strong></p>
</blockquote>
</div>
</div>
<div id="tests-of-goodness-of-fit" class="section level2" number="8.6">
<h2>
<span class="header-section-number">8.6</span> Tests of Goodness-of-fit<a class="anchor" aria-label="anchor" href="#tests-of-goodness-of-fit"><i class="fas fa-link"></i></a>
</h2>
<p>Let’s say we want to test M&amp;M’s claim of the distribution of colors in their candy:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center">color</th>
<th align="center">red</th>
<th align="center">green</th>
<th align="center">blue</th>
<th align="center">brown</th>
<th align="center">yellow</th>
<th align="center">orange</th>
</tr></thead>
<tbody><tr class="odd">
<td align="center">p</td>
<td align="center">0.2</td>
<td align="center">0.1</td>
<td align="center">0.1</td>
<td align="center">0.3</td>
<td align="center">0.2</td>
<td align="center">0.1</td>
</tr></tbody>
</table></div>
<p>How are the data distributed? Multinomial. Note that the claim about the data now has to do with a distribution and not a parameter. The multinomial distribution is one of a certain situation (think back to the binomial requirements) not a spread (like with some continuous distributions). What we’re really testing is the respective probabilities for each group, so we’re testing many parameters simultaneously. (<span class="math inline">\(N_i\)</span> are the observed counts in each cell)
<span class="math display">\[\begin{eqnarray*}
f(\underline{N}) = \frac{n!}{N_1! N_2! \cdots N_m!} p_1^{N_1}p_2^{N_2} \cdots p_m^{N_m}
\end{eqnarray*}\]</span></p>
<p><span class="math display">\[\begin{eqnarray*}
H_0:&amp;&amp; p_i = p_i^0 \ \ \ \ \forall i\\
H_1:&amp;&amp; p_i \ne p_i^0 \ \ \ \ \mbox{for some } i\\
\mbox{recall LRT: } \Lambda(\underline{x}) &amp;=&amp; \frac{\max_{\Omega_1} f(\underline{x})}{\max_{\Omega_0} f(\underline{x})}
\end{eqnarray*}\]</span></p>
<p>we reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(\Lambda(\underline{x}) \geq c\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\Lambda(\underline{N}) &amp;=&amp; \frac{\frac{n!}{N_1! N_2! \cdots N_m!} \hat{p}_1^{N_1} \hat{p}_2^{N_2} \cdots \hat{p}_m^{N_m}}{\frac{n!}{N_1! N_2! \cdots N_m!} p_1^{0 N_1} p_2^{0 N_2} \cdots p_m^{0 N_m}}\\
&amp;=&amp; \prod_{i=1}^m \Big( \frac{\hat{p_i}}{p_i^0} \Big)^{N_i}\\
\hat{p}_i &amp;=&amp; \frac{N_i}{n} \ \ \ \ \mbox{(MLE)}\\
\ln \Lambda(\underline{N}) &amp;=&amp; \sum_{i=1}^m N_i \ln \Big( \frac{N_i/n}{p_i^0} \Big)\\
&amp;=&amp; \sum_{i=1}^m N_i \ln \Big( \frac{N_i}{n p_i^0} \Big)\\
\end{eqnarray*}\]</span></p>
<p>For any general LRT (under reasonable conditions) the following holds (but the proof is outside the scope of our work):
<span class="math display">\[\begin{eqnarray*}
2 \ln \Lambda(\underline{X}) \stackrel{n \rightarrow \infty}{\rightarrow} \chi^2_\nu
\end{eqnarray*}\]</span>
where <span class="math inline">\(\nu =\)</span> dim <span class="math inline">\(\Omega_1\)</span> - dim <span class="math inline">\(\Omega_0.\)</span> Here, dim <span class="math inline">\(\Omega_1 = m-1,\)</span> dim <span class="math inline">\(\Omega_0 = 0.\)</span> Note that dimension gives the dimension of the parameter space (think about the Euclidean world we live in, a piece of paper is (sort of) two dimensional, people are three dimensional). The parameter space is the possible values of the parameter. So in <span class="math inline">\(\Omega_0,\)</span> there is only one possible value of the parameter, so it is a point. A point has no dimension. In <span class="math inline">\(\Omega_1\)</span> the possible values for the parameter live in <span class="math inline">\(m-1\)</span> dimensions. If we only had two groups, <span class="math inline">\(p_1\)</span> could be anything between 0 and 1. And <span class="math inline">\(p_2\)</span> would have to be such that <span class="math inline">\(p_1 + p_2 = 1.\)</span> Which means that the space of possible values is one dimensional.</p>
<p>If we think about the problem of estimating <span class="math inline">\(\mu\)</span> (a t-test)… The alternative space is a two dimensional space <span class="math inline">\(\Omega_1 = (\mu, \sigma^2): \mu \in I\!\!R, \sigma^2 \in I\!\!R+ )\)</span>. The null space is one dimensional because we specify the value for <span class="math inline">\(\mu\)</span>, <span class="math inline">\(\Omega_0 = (\mu, \sigma^2): \mu = \mu_0, \sigma^2 \in I\!\!R+ )\)</span>. Which gives us the degrees of freedom for a <span class="math inline">\(\chi^2\)</span> test of the mean: <span class="math inline">\(\nu = dim(\Omega_1) - dim(\Omega_0) = 2-1=1.\)</span></p>
<p>For our likelihood ratio test:
<span class="math display">\[\begin{eqnarray*}
2 \ln \Lambda(\underline{N}) &amp;=&amp; 2 \sum_{i=1}^m N_i \ln \Big( \frac{N_i}{n p_i^0} \Big)\\
&amp;\stackrel{n \rightarrow \infty}{\rightarrow}&amp; \chi^2_{m-1}
\end{eqnarray*}\]</span>
Note: if <span class="math inline">\(H_0\)</span> is true and <span class="math inline">\(n\)</span> is quite large, then <span class="math inline">\(\hat{p}_i \approx p_i^0\)</span> and <span class="math inline">\(N_i \approx n p_i^0\)</span>. A Taylor expansion of <span class="math inline">\(f(x) = x \ln (x / x_0) \mbox{ about } x_0\)</span> gives <span class="math inline">\(f(x) = (x - x_0) + \frac{1}{2}(x-x_0)^2\frac{1}{x_0} + \ldots .\)</span>
<span class="math display">\[\begin{eqnarray*}
2 \ln \Lambda(\underline{N}) &amp;\approx&amp; 2 \sum_{i=1}^m (N_i - n p_i^0) + 2 \frac{1}{2} \sum_{i=1}^m \Big( \frac{(N_i - n p_i^0)^2}{n p_i^0} \Big)\\
&amp;\approx&amp; \sum_{i=1}^m \Big( \frac{(N_i - n p_i^0)^2}{n p_i^0} \Big)\\
&amp;\approx&amp; \sum_{\mbox{all cells}} \frac{(\mbox{observed} - \mbox{expected})^2}{\mbox{expected}}\\
&amp;\rightarrow&amp; \chi^2_{\nu = m-1}
\end{eqnarray*}\]</span></p>
<p>Note: not only does the derivation lead us to the <span class="math inline">\(\chi^2\)</span> test we’ve seen many times (and its equivalency to the likelihood ratio test), but it also allows us to do (said equivalent) testing without using the <span class="math inline">\(\ln\)</span> function. You might not care so much about computing logs, but certainly it used to be a big deal (many years ago when these tests were derived).</p>
<div class="example">
<p><span id="exm:unlabeled-div-61" class="example"><strong>Example 8.14  </strong></span>(M &amp; M’s)</p>
<p>Let’s say we get the following data:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="51%">
<col width="5%">
<col width="8%">
<col width="7%">
<col width="8%">
<col width="9%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th>\begin{tabular}{c|c|c|c|c|c|c}color</th>
<th>red</th>
<th>green</th>
<th>blue</th>
<th>brown</th>
<th>yellow</th>
<th>orange</th>
</tr></thead>
<tbody><tr class="odd">
<td><span class="math inline">\(N_i\)</span></td>
<td>14</td>
<td>5</td>
<td>5</td>
<td>13</td>
<td>8</td>
<td>10</td>
</tr></tbody>
</table></div>
<p><span class="math display">\[\begin{eqnarray*}
\chi^2 &amp;=&amp; \frac{(13 - .3\cdot55)^2}{.3 \cdot 55} + \frac{(8 - .2\cdot55)^2}{.2 \cdot 55} + \frac{(5 - .1\cdot55)^2}{.1 \cdot 55} +\frac{(10 - .1\cdot55)^2}{.1 \cdot 55} \\
&amp;&amp; + \frac{(5 - .1\cdot55)^2}{.1 \cdot 55} +\frac{(14 - .2\cdot55)^2}{.2 \cdot 55}\\
&amp;=&amp; 6.15\\
\mbox{p-value}&amp;=&amp; P(\chi^2_5 &gt; 6.15) &gt; 0.25\\
\end{eqnarray*}\]</span>
Do not reject the null hypothesis. There is no evidence to say that M &amp; M’s is lying about the color distribution of their candy.</p>
</div>
<p>In general, the asymptotics associated with <span class="math inline">\(\chi^2\)</span> tests hold if there are at least 5 observations in each cell.</p>
<div id="goodness-of-fit-for-composite-hypotheses" class="section level3" number="8.6.1">
<h3>
<span class="header-section-number">8.6.1</span> Goodness-of-Fit for Composite Hypotheses<a class="anchor" aria-label="anchor" href="#goodness-of-fit-for-composite-hypotheses"><i class="fas fa-link"></i></a>
</h3>
<div id="testing-distributions" class="section level4" number="8.6.1.1">
<h4>
<span class="header-section-number">8.6.1.1</span> Testing distributions<a class="anchor" aria-label="anchor" href="#testing-distributions"><i class="fas fa-link"></i></a>
</h4>
<p>Notice that we can use the same results from above to test a particular distribution of the data. For example,
<span class="math display">\[\begin{eqnarray*}
H_0:&amp;&amp; \mbox{ pdf is uniform / normal / exponential...}\\
H_1: &amp;&amp;\mbox{not } H_0
\end{eqnarray*}\]</span>
Here, the degrees of freedom will be $= $ # cells - # parameters - 1. (Note dim <span class="math inline">\(\Omega_0 =\)</span> # parameters, dim <span class="math inline">\(\Omega_1 = m-1\)</span>.)</p>
</div>
</div>
</div>
<div id="bayes-test-procedures" class="section level2" number="8.7">
<h2>
<span class="header-section-number">8.7</span> Bayes Test Procedures<a class="anchor" aria-label="anchor" href="#bayes-test-procedures"><i class="fas fa-link"></i></a>
</h2>
<p>Recall that using Bayesian procedures, we can find the posterior distribution of the parameter. That is, we can create a distribution which gives us the probability associated with any interval of interest for the parameter. Seemingly, a natural Bayesian hypothesis test would be of the form:</p>
<p><span class="math display">\[\begin{eqnarray*}
\delta = \{ \mbox{reject } H_0 \mbox{ if } P(H_0 \mbox{ true } | \underline{X}) \leq ? \}
\end{eqnarray*}\]</span></p>
<p>The problem is that because the structure has changed (<span class="math inline">\(\mu\)</span> is now the random variable!), we can’t talk about the distribution of <span class="math inline">\(\mu\)</span> under the assumption that <span class="math inline">\(\mu \in \Omega_0\)</span> or <span class="math inline">\(\mu \in \Omega_1.\)</span> That makes it hard to talk about type I and type II errors with only the posterior distribution. We might consider a test that rejects <span class="math inline">\(H_0\)</span> when <span class="math inline">\(P(H_0 \mbox{ true } | \underline{X}) \leq \alpha_0\)</span>, but we haven’t considered whether <span class="math inline">\(H_0\)</span> is true or false.</p>
<p>Instead, the general idea is to choose the decision that leads to the smaller posterior expected loss. We assume that the loss in making the right decision is zero (that is, we don’t have an idea of “gain”). Let:</p>
<p>Yikes! Now our conclusions are slightly different. It’s because the logic is different and we actually measure the probability of the null hypothesis being true!</p>
<p>Our loss matrix can be written as (<span class="math inline">\(L(\theta, d_i)\)</span> is the loss in units of <span class="math inline">\(\omega_0\)</span> or <span class="math inline">\(\omega_1).\)</span></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="right"></th>
<th align="center"><span class="math inline">\(d_0\)</span></th>
<th align="center"><span class="math inline">\(d_1\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="right">
<span class="math inline">\(H_0\)</span> true</td>
<td align="center">0</td>
<td align="center"><span class="math inline">\(\omega_0\)</span></td>
</tr>
<tr class="even">
<td align="right">
<span class="math inline">\(H_1\)</span> true</td>
<td align="center"><span class="math inline">\(\omega_1\)</span></td>
<td align="center">0</td>
</tr>
</tbody>
</table></div>
<p>Assume <span class="math inline">\(\Omega_0\)</span> and <span class="math inline">\(\Omega_1\)</span> are simple hypotheses:
<span class="math display">\[\begin{eqnarray*}
H_0: \theta = \theta_0\\
H_1: \theta = \theta_1\\
\end{eqnarray*}\]</span></p>
<p>Let:</p>
<ul>
<li>
<span class="math inline">\(\xi_0\)</span> be the prior probability that <span class="math inline">\(H_0\)</span> is true</li>
<li>
<span class="math inline">\(\xi_1\)</span> be the prior probability that <span class="math inline">\(H_1\)</span> is true</li>
</ul>
<p>The <strong>expected loss</strong> of the test procedure based on the prior information is:
<span class="math display">\[\begin{eqnarray*}
r(\delta) &amp;=&amp; \xi_0 E[loss | \theta=\theta_0] + \xi_1 E[loss | \theta=\theta_1]\\
E[loss | \theta=\theta_0] &amp;=&amp; \omega_0 P(d_1 | \theta=\theta_0) = \omega_0 \alpha(\delta)\\
E[loss | \theta=\theta_1] &amp;=&amp; \omega_1 P(d_0 | \theta=\theta_1) = \omega_1 \beta(\delta)\\
r(\delta) &amp;=&amp; \xi_o \omega_0 \alpha(\delta) + \xi_1 \omega_1 \beta(\delta)\\
\end{eqnarray*}\]</span></p>
<p>A procedure which minimizes <span class="math inline">\(r(\delta)\)</span> is called a <strong>Bayes Test Procedure</strong>. (Notice that <span class="math inline">\(r(\delta)\)</span> is just a linear combination of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>!! A Bayes test procedure can immediately be determined from Theorem 8.2.1 of <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>:</p>
<p><span class="math display">\[\begin{eqnarray*}
\delta: \{ \mbox{reject } H_0 \mbox{ if } \xi_0 \omega_0 f_0(\underline{x}) &lt; \xi_1 \omega_1 f_1(\underline{x})\}\\
\end{eqnarray*}\]</span></p>
<p>However, if we want to follow our previous Bayesian ideas, it makes more sense to think about posterior distributions and minimize the posterior expected loss. Considering composite hypotheses, the posterior expected loss is:</p>
<p><span class="math display">\[\begin{eqnarray*}
r(d_i | \underline{x}) &amp;=&amp; \int_{\Omega_i} L(\theta, d_i) \xi(\theta | \underline{x}) d\theta\\
r(d_0 | \underline{x}) &amp;=&amp; \int_{\Omega_1} \omega_1 \xi(\theta | \underline{x}) d\theta = \omega_1[1-P(H_0 true | \underline{x})]\\
r(d_1 | \underline{x}) &amp;=&amp; \int_{\Omega_0} \omega_0 \xi(\theta | \underline{x}) d\theta = \omega_0 P(H_0 true | \underline{x}) \\
\end{eqnarray*}\]</span></p>
<p>A Bayes test procedure is one that chooses the decision that has the smaller posterior expected loss:
<span class="math display">\[\begin{eqnarray*}
\mbox{choose } d_0 \mbox{ if } r(d_0 | \underline{x}) &lt; r(d_1 | \underline{x})\\
\mbox{choose } d_1 \mbox{ if } r(d_0 | \underline{x}) &gt; r(d_1 | \underline{x})\\
\delta: \{ \mbox{reject } H_0 \mbox{ if } P(H_0 \mbox{ true } | \underline{x}) \leq \frac{\omega_1}{\omega_0 + \omega_1} \}
\end{eqnarray*}\]</span></p>
<p>is the Bayes test for all situations given by the loss table (with zeros for no loss). The result holds whether or not MLR, one vs. two-sided, discrete vs. continuous parameters, etc.</p>
<div id="two-sided-alternatives" class="section level4" number="8.7.0.1">
<h4>
<span class="header-section-number">8.7.0.1</span> Two-sided alternatives<a class="anchor" aria-label="anchor" href="#two-sided-alternatives"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
H_0: \theta = \theta_0\\
H_1: \theta \ne \theta_0\\
P(H_0 \mbox{ true }| \underline{x}) = 0
\end{eqnarray*}\]</span>
we don’t even need to look at the data…</p>
<p><span class="math display">\[\begin{eqnarray*}
H_0: |\theta - \theta_0| \leq d\\
H_1: |\theta - \theta_0| &gt; d\\
\end{eqnarray*}\]</span>
We might want to choose such hypotheses as more meaningful. But we’d need to choose d. Note, we’re trying to balance practical versus statistical significance.</p>
</div>
<div id="improper-priors-1" class="section level4" number="8.7.0.2">
<h4>
<span class="header-section-number">8.7.0.2</span> Improper Priors<a class="anchor" aria-label="anchor" href="#improper-priors-1"><i class="fas fa-link"></i></a>
</h4>
<p>With improper priors, t-tests, F-tests, etc. are exactly the same as the Frequentist likelihood ratio tests we saw previously. (Using <span class="math inline">\(\omega_0\)</span> and <span class="math inline">\(\omega_1\)</span> to set <span class="math inline">\(\alpha\)</span>, in particular, <span class="math inline">\(\alpha_0 = \frac{\omega_1}{\omega_0 + \omega_1}.)\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-62" class="example"><strong>Example 8.15  </strong></span>Suppose an electronic failure can occur because of either a major or a minor defect. 80% of the defects are minor, 20% are major. When a failure occurs, <span class="math inline">\(n\)</span> independent tests are made on the system. If the failure was due to a minor defect, <span class="math inline">\(X \sim\)</span> Poisson(3). If the failure was major, <span class="math inline">\(X \sim\)</span> Poisson(7). The cost of deciding the failure was major if it was actually minor is $400. The cost of deciding the failure was minor if it was actually major is $2500. What decision minimizes the cost?</p>
<p><span class="math display">\[\begin{eqnarray*}
H_0: \mbox{minor } \lambda=3\\
H_1: \mbox{major } \lambda=7\\
\end{eqnarray*}\]</span></p>
<p>Reject if:
<span class="math display">\[\begin{eqnarray*}
0.8 \cdot 400 f(\underline{x} | 3) &amp;&lt;&amp; 0.2 \cdot 2500 f(\underline{x} | 7)\\
320 \frac{e^{-3n}3^{\Sigma x_i}}{\prod x_i !} &amp;&lt;&amp; 500 \frac{e^{-7n}7^{\Sigma x_i}}{\prod x_i !}\\
0.64 e^{4n} &amp;&lt;&amp; (7/3)^{\Sigma x_i}\\
\Sigma x_i &amp;&gt;&amp; \frac{ \ln(0.64) + 4n}{\ln(7/3)}
\end{eqnarray*}\]</span></p>
</div>
<!--% \# 7, section 8.8, pg 526-->
<div class="example">
<p><span id="exm:unlabeled-div-63" class="example"><strong>Example 8.16  </strong></span>Suppose that we have a situation in which the Bayes test that rejects <span class="math inline">\(H_0\)</span> when P(<span class="math inline">\(H_0\)</span> true <span class="math inline">\(| \underline{x}) \leq \alpha_0\)</span> is the same as the level <span class="math inline">\(\alpha_0\)</span> test of <span class="math inline">\(H_0\)</span> for all <span class="math inline">\(\alpha_0\)</span> [“same” here means that for any sample <span class="math inline">\(\underline{x}\)</span>, both tests would come to the same conclusion reject <span class="math inline">\(H_0\)</span> or don’t reject <span class="math inline">\(H_0.]\)</span> Prove that the p-value equals the posterior probability that <span class="math inline">\(H_0\)</span> is true.</p>
<p><strong>Solution:</strong>
Argue by contradiction. Suppose that there is an <span class="math inline">\(\underline{x}\)</span> such that the p-value is not equal to the posterior probability that <span class="math inline">\(H_0\)</span> is true. First, suppose that the p-value is greater. Let <span class="math inline">\(\alpha_0\)</span> be greater than the posterior probability and less than the p-value. Then the tst that rejects <span class="math inline">\(H_0\)</span> when <span class="math inline">\(P(H_0 \mbox{ is true } \leq \alpha_0)\)</span> will reject <span class="math inline">\(H_0\)</span>, but the level <span class="math inline">\(\alpha_0\)</span> test will not reject <span class="math inline">\(H_0\)</span> because the p-value is greater than <span class="math inline">\(\alpha_0\)</span>. This contradicts the fact that the two tests are the same. If the p-value is smaller, the argument is identical.</p>
</div>
</div>
</div>
<div id="foundational-ideas" class="section level2" number="8.8">
<h2>
<span class="header-section-number">8.8</span> Foundational Ideas<a class="anchor" aria-label="anchor" href="#foundational-ideas"><i class="fas fa-link"></i></a>
</h2>
<p>Section 9.9 <span class="citation">(<a href="references.html#ref-degroot" role="doc-biblioref">DeGroot and Schervish 2011</a>)</span> has lots of important ideas that we’ve talked about in the last few weeks. All fair game on the final, you should probably read that section.</p>
<ul>
<li>the relationship between level of significance and sample size</li>
<li>statistically significant results</li>
</ul>
<!--% \# 2, section 9.9, pg 620--><div class="example">
<p><span id="exm:unlabeled-div-64" class="example"><strong>Example 8.17  </strong></span>Suppose that a random sample of 10,000 observations is taken from the normal distribution with unknown mean <span class="math inline">\(\mu\)</span> and known variance is 1, and it is desired to test the following hypotheses at the level of significance 0.05.
<span class="math display">\[\begin{eqnarray*}
H_0: &amp;&amp; \mu=0\\
H_1: &amp;&amp; \mu \ne 0
\end{eqnarray*}\]</span>
Suppose that the test procedure specifies rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(|\overline{X}| \geq c\)</span>, where the constant <span class="math inline">\(c\)</span> is chosen so that <span class="math inline">\(P(|\overline{X}| \geq c | \mu=0) = 0.05.\)</span> Find the probability that the test will reject <span class="math inline">\(H_0\)</span> if (a) the actual value of <span class="math inline">\(\mu\)</span> is 0.01, and (b) the actual value of <span class="math inline">\(\mu\)</span> is 0.02.</p>
<p><strong>Solution:</strong></p>
<p>When <span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\frac{\overline{X}-0}{1/\sqrt{10,000}}\)</span> has a standard normal distribution. Therefore, <span class="math inline">\(P(100 | \overline{X}| &gt; 1.96 | \mu=0) = 0.05\)</span>. Therefore, <span class="math inline">\(c=1.96/100 = 0.0196\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>When <span class="math inline">\(\mu=0.01\)</span>, the random variable <span class="math inline">\(\frac{\overline{X} - 0.01}{1/\sqrt{10,000}}\)</span> has a standard normal distribution.
<span class="math display">\[\begin{eqnarray*}
P(|\overline{X}| &lt; c) &amp;=&amp; P(-1.96 &lt; 100 \overline{X} &lt; 1.96 | \mu=0.01)\\
&amp;=&amp; P(-2.96 &lt; Z &lt; 0.96)\\
&amp;=&amp; 0.8315 - 0.0015 = 0.83
\end{eqnarray*}\]</span>
Therefore, <span class="math inline">\(P(|\overline{X}| \geq c | \mu = 0.01) = 0.17\)</span></p></li>
<li><p>When <span class="math inline">\(\mu=0.02\)</span>, the random variable <span class="math inline">\(\frac{\overline{X} - 0.02}{1/\sqrt{10,000}}\)</span> has a standard normal distribution.
<span class="math display">\[\begin{eqnarray*}
P(|\overline{X}| &lt; c) &amp;=&amp; P(-1.96 &lt; 100 \overline{X} &lt; 1.96 | \mu=0.02)\\
&amp;=&amp; P(-3.96 &lt; Z &lt; -.04)\\
&amp;=&amp; 0.484
\end{eqnarray*}\]</span>
Therefore, <span class="math inline">\(P(|\overline{X}| \geq c | \mu = 0.02) = 0.516.\)</span></p></li>
</ol>
</div>
</div>
<div id="reflection-questions-7" class="section level2" number="8.9">
<h2>
<span class="header-section-number">8.9</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-7"><i class="fas fa-link"></i></a>
</h2>
<p>One of the most important tools that statisticians can bring to the table is understanding about variability. That is, we don’t take the results of the test, CI, or model as <em>truth</em>. Instead, we think about how likely our results would be under one setting and under another setting. We use power as a way of differentiating some processes (tests, algorithms, statistics) in comparison with others.</p>
<ol style="list-style-type: decimal">
<li>What happens with power as the sample size changes (increases / decreases)?</li>
<li>What happens to the power curve for a two-sided test?</li>
<li>Why is (for some values) a one-sided test more powerful if there is a two-sided hypothesis?</li>
<li>Why can’t we find the exact power associated with the t-test? What does it mean to have a probability statement associated with two different random variables?</li>
<li>What is the difference between size, level of significance, and power?</li>
<li>What does it mean for a result to be “significant”?</li>
<li>How can hypothesis testing be used to assess whether the data come from a particular probability model (i.e., a population given by a pdf)?</li>
</ol>
</div>
<div id="ethics-considerations-7" class="section level2" number="8.10">
<h2>
<span class="header-section-number">8.10</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-7"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Which is worse, type I or type II errors?</li>
<li>N-P, MLR, and LRT provide tests that are “best” (or close to best) in some way. What way? And is that way the only criteria for assessing whether or not a test is good?</li>
<li>Is the p-value the probability that the null hypothesis is true? Explain.</li>
</ol>
</div>
<div id="r-code" class="section level2" number="8.11">
<h2>
<span class="header-section-number">8.11</span> R code:<a class="anchor" aria-label="anchor" href="#r-code"><i class="fas fa-link"></i></a>
</h2>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ht"><span class="header-section-number">8</span> Hypothesis Testing</a></li>
<li>
<a class="nav-link" href="#statistical-hypotheses"><span class="header-section-number">8.1</span> Statistical Hypotheses</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#what-is-an-alternative-hypothesis">What is an Alternative Hypothesis?</a></li>
<li><a class="nav-link" href="#critical-regions-and-test-statistics"><span class="header-section-number">8.1.1</span> Critical Regions and Test Statistics</a></li>
<li><a class="nav-link" href="#errors-power-size"><span class="header-section-number">8.1.2</span> Errors: Power &amp; Size</a></li>
</ul>
</li>
<li><a class="nav-link" href="#simple-hypotheses"><span class="header-section-number">8.2</span> Simple Hypotheses</a></li>
<li>
<a class="nav-link" href="#uniformly-most-powerful-tests"><span class="header-section-number">8.3</span> Uniformly Most Powerful Tests</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#monotone-likelihood-ratio"><span class="header-section-number">8.3.1</span> Monotone Likelihood Ratio</a></li>
<li><a class="nav-link" href="#two-sided-tests"><span class="header-section-number">8.3.2</span> Two sided tests</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#the-t-test"><span class="header-section-number">8.4</span> The t-test</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#power-of-the-t-test"><span class="header-section-number">8.4.1</span> Power of the t-test</a></li></ul>
</li>
<li><a class="nav-link" href="#likelihood-ratio-tests"><span class="header-section-number">8.5</span> Likelihood Ratio Tests</a></li>
<li>
<a class="nav-link" href="#tests-of-goodness-of-fit"><span class="header-section-number">8.6</span> Tests of Goodness-of-fit</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#goodness-of-fit-for-composite-hypotheses"><span class="header-section-number">8.6.1</span> Goodness-of-Fit for Composite Hypotheses</a></li></ul>
</li>
<li><a class="nav-link" href="#bayes-test-procedures"><span class="header-section-number">8.7</span> Bayes Test Procedures</a></li>
<li><a class="nav-link" href="#foundational-ideas"><span class="header-section-number">8.8</span> Foundational Ideas</a></li>
<li><a class="nav-link" href="#reflection-questions-7"><span class="header-section-number">8.9</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-7"><span class="header-section-number">8.10</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r-code"><span class="header-section-number">8.11</span> R code:</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/08-ht.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/08-ht.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-11-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
