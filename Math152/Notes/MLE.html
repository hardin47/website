<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 Maximum Likelihood Estimation | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="Maximum likelihood estimation is a method for choosing estimators of parameters that avoids using prior distributions or loss functions. MLE chooses \(\hat{\theta}\) as the estimate of \(\theta\)...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 3 Maximum Likelihood Estimation | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="Maximum likelihood estimation is a method for choosing estimators of parameters that avoids using prior distributions or loss functions. MLE chooses \(\hat{\theta}\) as the estimate of \(\theta\)...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 3 Maximum Likelihood Estimation | Statistical Theory">
<meta name="twitter:description" content="Maximum likelihood estimation is a method for choosing estimators of parameters that avoids using prior distributions or loss functions. MLE chooses \(\hat{\theta}\) as the estimate of \(\theta\)...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="active" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="MLE" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#MLE"><i class="fas fa-link"></i></a>
</h1>
<p>Maximum likelihood estimation is a method for choosing estimators of parameters that avoids using prior distributions or loss functions. MLE chooses <span class="math inline">\(\hat{\theta}\)</span> as the estimate of <span class="math inline">\(\theta\)</span> that <strong>maximizes</strong> the likelihood function (easily the most widely used estimation method in statistics).</p>
<p>Let’s say <span class="math inline">\(X \sim U[0, \theta].\)</span> We collect <span class="math inline">\(X=47.\)</span> Would you ever pick <span class="math inline">\(\theta=10?\)</span> No! <span class="math inline">\(10 \notin \Omega!\)</span></p>
<p>Let’s say that <span class="math inline">\(X \sim\)</span> Bin(<span class="math inline">\(\theta,\)</span>n=4). If we have 4 independent trials, and X=1, would you chose <span class="math inline">\(\theta=0.99?\)</span> But <span class="math inline">\(0.99 \in \Omega???\)</span> You would choose <span class="math inline">\(\theta = 0.25.\)</span> Why? Because you maximized the likelihood.</p>
<p><span class="math display">\[\begin{eqnarray*}
P(X=1 | \theta = 0.25) &amp;=&amp; 0.422\\
P(X=1 | \theta = 0.5) &amp;=&amp; 0.25\\
P(X=1 | \theta = 0.05)  &amp;=&amp; 0.171\\
P(X=1 | \theta = 0.15) &amp;=&amp; 0.368\\
\end{eqnarray*}\]</span>
You maximized the probability of seeing your data!</p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 3.1  </strong></span>Let’s say that houses on a block have their electricity connected in such a way that it only works for house #47 if all the neighbor’s electricity works (in sequence). Sometimes a house’s electricity will fail (w/prob p). To how many houses can we expect to provide electricity? What is our best guess of p?</p>
<p>Data: let’s say we have <span class="math inline">\(n\)</span> neighborhoods with information on which house lost electricity. [What is the distribution of <span class="math inline">\(X?\)</span> <span class="math inline">\(X \sim geometric(p),\)</span> which means <span class="math inline">\(E[X] = 1/p.]\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
f(x_i | p) &amp;=&amp; p (1-p)^{x_i -1}\\
f(\underline{x} | p) &amp;=&amp; \prod_{i=1}^n p (1-p)^{x_i -1}\\
&amp;=&amp; p^n (1-p)^{\sum x_i -n}\\
&amp;&amp; \mbox{we want to maximize wrt } p
\end{eqnarray*}\]</span></p>
<p>Often, the log-likelihood is easier to maximize than the likelihood. We define the log likelihood as
<span class="math display">\[\begin{eqnarray*}
L(\theta) = \ln f(\underline{X} | \theta).
\end{eqnarray*}\]</span></p>
<p>For example,
<span class="math display">\[\begin{eqnarray*}
L(p) &amp;=&amp; \ln f(\underline{x} | p) = n \ln(p) + (\sum x_i -n) \ln(1-p)\\
\frac{\partial L(p)}{\partial p} &amp;=&amp; \frac{n}{p} + \frac{(\sum x_i -n)(-1)}{(1-p)} = 0\\
\hat{p} &amp;=&amp; \frac{1}{\overline{X}}
\end{eqnarray*}\]</span></p>
<p>If <span class="math inline">\(\overline{x} = 10\)</span>, <span class="math inline">\(\hat{p} = 1/10\)</span> (1 failure at 10 homes). E[X] = 1/p. (If we let <span class="math inline">\(\theta = E[X] -1\)</span>, then a good estimate of the expected (average) number of homes to which we can provide electricity is <span class="math inline">\(1/\hat{p} -1 = 10 - 1\)</span>. It turns out that the MLE has the invariance property which says that a function of the MLE is the MLE of that function of the parameter.)</p>
<p>Note that we’ve found a maximum:
<span class="math display">\[\begin{eqnarray*}
\frac{\partial^2 L(p)}{\partial p^2} &amp;=&amp; \frac{2np -n -p^2\sum x_i}{p^2 (1-p)^2}\\
&amp;\leq&amp; \frac{2p n - n -p^2n}{p^2 (1-p)^2}\\
(\mbox{because } n &amp;\leq&amp; \sum x_i)\\
&amp;=&amp; \frac{n ( -2p + 1 +p^2}{p^2 (1-p)^2}\\
&amp;=&amp; \frac{-n}{p^2} &lt; 0\\
\end{eqnarray*}\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>Definition 3.1  </strong></span>Maximum Likelihood Estimator (Def 7.5.2).<br>
For each possible observed vector <span class="math inline">\(\underline{x}\)</span>, let <span class="math inline">\(\delta(\underline{x}) \in \Omega\)</span> denote a value of <span class="math inline">\(\theta \in \Omega\)</span> for which the likelihood function, <span class="math inline">\(f(\underline{x}|\theta)\)</span> is a maximum, and let <span class="math inline">\(\hat{\theta} = \delta(\underline{X})\)</span> be the estimator of <span class="math inline">\(\theta\)</span> defined in this way. The estimator for <span class="math inline">\(\hat{\theta}\)</span> is called a <strong>maximum likelihood estimator</strong> of <span class="math inline">\(\theta.\)</span> After <span class="math inline">\(\underline{X} = \underline{x}\)</span> is observed, the value <span class="math inline">\(\delta(\underline{x})\)</span> is called a <em>maximum likelihood estimate</em> of <span class="math inline">\(\theta.\)</span></p>
</div>
<div id="sampling-from-a-normal-distribution" class="section level4 unnumbered">
<h4>Sampling from a Normal Distribution<a class="anchor" aria-label="anchor" href="#sampling-from-a-normal-distribution"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
X_1, X_2, \ldots X_n &amp;\sim&amp; N(\mu, \sigma^2)  \ \ \ \mu \ \&amp; \ \sigma^2 \mbox{ are fixed but unknown}\\
f(x_i | \mu, \sigma^2) &amp;=&amp; \frac{1}{\sqrt{2 \pi \sigma^2}} \exp [ \frac{-1}{2\sigma^2}(x_i - \mu)^2 ]\\
f(\underline{x} | \mu, \sigma^2) &amp;=&amp; \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp [ \frac{-1}{2\sigma^2}\sum_{i=1}^n(x_i - \mu)^2 ]\\
L(\mu, \sigma^2) &amp;=&amp; \frac{-n}{2}\ln(2\pi\sigma^2) - \frac{1}{2 \sigma^2} \sum(x_i - \mu)^2\\
\end{eqnarray*}\]</span></p>
<p>We want to maximize <span class="math inline">\(f(\underline{x}|\mu, \sigma^2)\)</span> with respect to <span class="math inline">\(\mu;\)</span> equivalently, we can minimize <span class="math inline">\(\sum (x_i - \mu)^2.\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{Let } Q(\mu) &amp;=&amp; \sum(x_i - \mu)^2\\
\frac{\partial Q(\mu)}{\partial \mu} &amp;=&amp; -2 \sum(x_i - \mu) = 0\\
-2 \sum x_i &amp;=&amp; -2 n \mu\\
\hat{\mu} &amp;=&amp; \overline{x}
\end{eqnarray*}\]</span></p>
<p>Because the MLE for <span class="math inline">\(\mu\)</span> doesn’t depend on <span class="math inline">\(\sigma^2\)</span>, we know that the joint MLE, <span class="math inline">\(\hat{\theta} = (\mu, \sigma^2)\)</span>, will be <span class="math inline">\(\hat{\theta}\)</span> such that <span class="math inline">\(\hat{\sigma}^2\)</span> maximizes (where <span class="math inline">\(\theta' = (\overline{x}, \sigma^2) ):\)</span>
<span class="math display">\[\begin{eqnarray*}
L(\theta') &amp;=&amp; \frac{-n}{2} \ln (2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{1}{2\sigma^2} \sum(x_i - \overline{x})^2\\
\frac{\partial L(\theta')}{\partial \sigma^2} &amp;=&amp; \frac{-n}{2 \sigma^2} - \frac{1(-1)}{2 (\sigma^2)^2} \sum(x_i - \overline{x}) = 0\\
\sum (x_i - \overline{x})^2 &amp;=&amp; \frac{n(\sigma^2)^2}{\sigma^2}\\
\hat{\sigma^2} &amp;=&amp; \frac{\sum(x_i - \overline{x})^2}{n}\\
\hat{\theta} &amp;=&amp; (\overline{x}, \frac{\sum(x_i - \overline{x})^2}{n})
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 3.2  </strong></span>Non-existence of the MLE</p>
<div class="inline-figure"><img src="03-MLE_files/figure-html/unnamed-chunk-2-1.png" width="672"></div>
<p>Suppose <span class="math inline">\(X_1, X_2, \ldots X_n \sim f(x | \theta)\)</span> where:
<span class="math display">\[\begin{eqnarray*}
f(x) = \left\{ \begin{array}{ll}
    e^{\theta - x} &amp; x &gt; \theta\\
    0 &amp; x \leq \theta \\
    \end{array} \right.
    \end{eqnarray*}\]</span></p>
<p><span class="math inline">\(\theta\)</span> is unknown, but <span class="math inline">\(-\infty &lt; \theta &lt; \infty\)</span>. Does the MLE exist?
<span class="math display">\[\begin{eqnarray*}
f(\underline{x}|\theta) &amp;=&amp; e^{n \theta - \sum x_i} \ \ \ \ \ \forall x_i &gt; \theta\\
L(\theta) &amp;=&amp; n \theta - \sum x_i \ \ \ \ \ \forall x_i &gt; \theta\\
\frac{\partial L(\theta)}{\partial \theta} &amp;=&amp; n = 0 ?!?!?!\\
\end{eqnarray*}\]</span></p>
<p>We want to find the largest <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(\theta &lt; x_i \ \ \forall x_i\)</span>. Is <span class="math inline">\(\hat{\theta} = \min x_i\)</span>? No, because <span class="math inline">\(\theta &lt; \min x_i!\)</span> If, instead,
<span class="math display">\[\begin{eqnarray*}
f(x) = \left\{ \begin{array}{ll}
    e^{\theta - x} &amp; x \geq \theta\\
    0 &amp; x &lt; \theta \\
    \end{array} \right.
    \end{eqnarray*}\]</span></p>
<p>MLE of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \min x_i\)</span>.</p>
</div>
</div>
<div id="qualities-of-the-mle" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> Qualities of the MLE<a class="anchor" aria-label="anchor" href="#qualities-of-the-mle"><i class="fas fa-link"></i></a>
</h2>
<div id="invariance-of-the-mle" class="section level3" number="3.1.1">
<h3>
<span class="header-section-number">3.1.1</span> Invariance of the MLE<a class="anchor" aria-label="anchor" href="#invariance-of-the-mle"><i class="fas fa-link"></i></a>
</h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-9" class="theorem"><strong>Theorem 3.1  </strong></span>(D&amp;S Theorem 6.6.1) Let <span class="math inline">\(\hat{\theta}\)</span> be the MLE of <span class="math inline">\(\theta\)</span>, and let <span class="math inline">\(g(\theta)\)</span> be a function of <span class="math inline">\(\theta\)</span>. Then an MLE of <span class="math inline">\(g(\theta)\)</span> is <span class="math inline">\(g(\hat{\theta})\)</span>. (Proof: see page 427 in DeGroot &amp; Schervish)</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-10" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(\hat{\theta}\)</span> be the MLE of <span class="math inline">\(\theta\)</span>. Then we know that <span class="math display">\[L(\hat{\theta}) = \max_{\theta \in \Omega} L(\theta).\]</span></p>
<p>We let <span class="math inline">\(g\)</span> be a one-to-one function such that <span class="math inline">\(\psi = g(\theta) \in \Gamma\)</span> (the image of <span class="math inline">\(\Omega\)</span> under g). Then we can denote the inverse function of <span class="math inline">\(g\)</span> as <span class="math display">\[\theta = h(\psi).\]</span></p>
<p>We know that the MLE of <span class="math inline">\(\psi\)</span> is <span class="math inline">\(\hat{\psi}\)</span> where <span class="math display">\[L(h(\hat{\psi})) = \max_{\psi \in \Gamma} L(h(\psi)).\]</span></p>
<p>But we know that <span class="math inline">\(L(\theta)\)</span> is maximized at <span class="math inline">\(\hat{\theta}\)</span>. So <span class="math inline">\(L(h(\psi))\)</span> must also be maximized at <span class="math inline">\(h(\psi) = \hat{\theta}\)</span>. <span class="math display">\[\therefore h(\hat{\psi}) = \hat{\theta} \ \ \ \mbox{ or } \ \ \ \hat{\psi} = g(\hat{\theta}).\]</span></p>
<p>If <span class="math inline">\(g\)</span> is not one-to-one, however, we need to redefine what we mean by likelihood. If <span class="math inline">\(g(\theta)\)</span> is not one-to-one there may be many values of <span class="math inline">\(\psi\)</span> that satisfy <span class="math display">\[g(\theta) = \psi.\]</span> (For example, the square function. n.b. <span class="math inline">\(g\)</span> still has to be a function, so it can’t map to multiple values.)</p>
<p>In that case, the correspondence between the maximum over <span class="math inline">\(\psi\)</span> and the maximum over <span class="math inline">\(\theta\)</span> breaks down. E.g., if <span class="math inline">\(\hat{\theta}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>, there may be another value of <span class="math inline">\(\theta\)</span>, say <span class="math inline">\(\theta_0\)</span> for which <span class="math inline">\(g(\hat{\theta}) = g(\theta_0).\)</span></p>
<p>We define the induced log likelihood function: <span class="math display">\[L^*(t) = \max_{\theta \in G_t} \ln f(\underline{x} | \theta).\]</span></p>
<p>Let <span class="math inline">\(G\)</span> be the image of <span class="math inline">\(\Omega\)</span> under g, then <span class="math display">\[G_t = \{ \theta: g(\theta) = t\} \ \ \forall t \in G.\]</span></p>
<p>We define the MLE of <span class="math inline">\(g(\theta)\)</span> to be <span class="math inline">\(\hat{t}\)</span> where <span class="math display">\[L^*(\hat{t}) = \max_{t \in G} L^* (t).\]</span></p>
<p>The proof follows (page 427). [Note that above there are <em>two</em> maximizations. That is, the first maximization created the log likelihood function. The second maximization found the maximum of the function over the parameter space.]</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 3.3  </strong></span>Functions of MLEs are MLEs also!</p>
<ol style="list-style-type: decimal">
<li>standard deviation of a normal:
<span class="math display">\[\begin{eqnarray*}
\sigma &amp;=&amp; \sqrt{\sigma^2}\\
\hat{\sigma} &amp;=&amp; \mbox{MLE}(\sigma) = \sqrt{\frac{\sum (x_i - \overline{x})^2}{n}}
\end{eqnarray*}\]</span>
</li>
<li>mean of a uniform:
<span class="math display">\[\begin{eqnarray*}
X &amp;\sim&amp; U [\theta_1, \theta_2]\\
\mu &amp;=&amp; \frac{\theta_1 + \theta_2}{2}\\
\hat{\mu} &amp;=&amp; \mbox{MLE}(\mu) = \frac{\max(x_i) + \min(x_i)}{2}
\end{eqnarray*}\]</span>
</li>
</ol>
</div>
</div>
<div id="consistency-of-the-mle" class="section level3" number="3.1.2">
<h3>
<span class="header-section-number">3.1.2</span> Consistency of the MLE<a class="anchor" aria-label="anchor" href="#consistency-of-the-mle"><i class="fas fa-link"></i></a>
</h3>
<p>Under certain regularity conditions, the MLE of <span class="math inline">\(\theta\)</span> is consistent for <span class="math inline">\(\theta\)</span>. That is,
<span class="math display">\[\begin{eqnarray*}
\hat{\theta} \stackrel{P}{\rightarrow} \theta\\
\stackrel{\lim}{ n \rightarrow \infty} P [ | \hat{\theta} - \theta | &gt; \epsilon ] = 0
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}\)</span> is a function of <span class="math inline">\(X\)</span>.</p>
</div>
<div id="bias-of-the-mle-section-7.7" class="section level3" number="3.1.3">
<h3>
<span class="header-section-number">3.1.3</span> Bias of the MLE (section 7.7)<a class="anchor" aria-label="anchor" href="#bias-of-the-mle-section-7.7"><i class="fas fa-link"></i></a>
</h3>
<p>Previously, we discussed briefly the concept of bias:</p>
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>Definition 3.2  </strong></span>Unbiased (page 428 of DeGroot &amp; Schervish): Let <span class="math inline">\(\delta(X_1, X_2, \ldots, X_n)\)</span> be an estimator of <span class="math inline">\(g(\theta)\)</span>. We say <span class="math inline">\(\delta(X_1, X_2, \ldots, X_n)\)</span> is <strong>unbiased</strong> if:
<span class="math display">\[\begin{eqnarray*}
E[ \delta(X_1, X_2, \ldots, X_n)] = g(\theta)
\end{eqnarray*}\]</span></p>
</div>
<p>Note: bias <span class="math inline">\((\delta(\underline{X}) ) = E[ \delta(\underline{X})] - g(\theta)\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 3.4  </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \sim N(\mu, \sigma^2)\)</span>. Recall, <span class="math inline">\(\hat{\sigma^2} = \frac{\sum(X_i - \overline{X})^2}{n}\)</span> is the MLE.
<span class="math display">\[\begin{eqnarray*}
E[\hat{\sigma^2}] &amp;=&amp; E \bigg[ \frac{\sum(X_i - \overline{X})^2}{n}\bigg]\\
&amp;=&amp; \frac{1}{n} E\bigg[ \sum (X_i - \mu + \mu - \overline{X})^2 \bigg]\\
&amp;=&amp; \frac{1}{n} E\bigg[ \sum (X_i - \mu)^2 + 2 \sum (X_i - \mu)(\mu - \overline{X}) + n(\mu - \overline{X})^2 \bigg]\\
&amp;=&amp; \frac{1}{n}\bigg\{ E\bigg[ \sum (X_i - \mu)^2 -  n [(\overline{X} - \mu)^2 ] \bigg] \bigg\}\\
&amp;=&amp; \frac{1}{n} \bigg\{  \sum E (X_i - \mu)^2 -  n E [(\overline{X} - \mu)^2 ] \bigg\}\\
&amp;=&amp; \frac{1}{n}\{ n \sigma^2 - n \frac{\sigma^2}{n}  \}\\
&amp;=&amp; \frac{n-1}{n} \sigma^2 \Rightarrow \mbox{Biased!}
\end{eqnarray*}\]</span></p>
<div id="but-consistent-yes." class="section level4 unnumbered">
<h4>But consistent? Yes.<a class="anchor" aria-label="anchor" href="#but-consistent-yes."><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\sum(X_i - \overline{X})^2}{n} &amp;=&amp; \frac{1}{n}\bigg[ \sum (X_i - \mu)^2 -  n [(\overline{X} - \mu)^2] \bigg] \\
&amp;=&amp; \frac{\sum(X_i - \mu)^2}{n} - \frac{1}{n} \frac{\sum n (\overline{X} - \mu)^2}{n}\\
&amp;=&amp; \stackrel{P}{\rightarrow} \sigma^2   \ \ \ \ \ \ \ \ \ \ \ \rightarrow 0 \ \ \ \stackrel{P}{\rightarrow} \sigma^2\\
&amp;\stackrel{P}{\rightarrow}&amp; \sigma^2\\
\end{eqnarray*}\]</span>
<!--
%The above limits are due to the WLLN.  The sample average of a random variable will converge in probability to its expected value. $$ Var(\overline{X}) = E[(\overline{X} - \mu)^2] = \frac{\sigma^2}{n}.$$  If we let $Y_n$ be the value of the sum minus the mean squared, then $Y_n$ changes as the sample size changes, specifically
%\begin{eqnarray*}
%Y_n &= (\overline{X}_n - \mu)^2\\
%E [Y_n] &= E[ (\overline{X}_n - \mu)^2 ] = \frac{\sigma^2}{n}\\
%n \frac{\sum(Y_n)}{n} &\stackrel{P}{\rightarrow} \sigma^2
%\end{eqnarray*}--></p>
</div>
</div>
</div>
<div id="benefits-and-limitations-of-maximum-likelihood-estimation" class="section level3" number="3.1.4">
<h3>
<span class="header-section-number">3.1.4</span> Benefits and Limitations of Maximum Likelihood Estimation<a class="anchor" aria-label="anchor" href="#benefits-and-limitations-of-maximum-likelihood-estimation"><i class="fas fa-link"></i></a>
</h3>
<div id="benefits-1" class="section level4 unnumbered">
<h4>Benefits<a class="anchor" aria-label="anchor" href="#benefits-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>functions of MLEs are MLEs (invariance of the MLE)</li>
<li>under certain regularity conditions, MLEs are asymptotically distributed normally.</li>
</ul>
</div>
<div id="limitations-1" class="section level4 unnumbered">
<h4>Limitations<a class="anchor" aria-label="anchor" href="#limitations-1"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>MLE does not always exist</li>
<li>MLE is not always unique</li>
<li>MLE is the value of the parameter <span class="math inline">\(\theta\)</span> that maximizes the distribution of <span class="math inline">\(X|\theta\)</span> (most likely to have produced the observed data). The MLE is <strong>not</strong> the most likely parameter given the data: <span class="math inline">\(E[ \theta | X]\)</span> (that’s the Bayesian estimator!).</li>
</ul>
</div>
</div>
</div>
<div id="method-of-moments" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Method of Moments<a class="anchor" aria-label="anchor" href="#method-of-moments"><i class="fas fa-link"></i></a>
</h2>
<p>Method of Moments (MOM) is another parameter estimation technique. To find the MOM estimate, set the expected moment equal to the sample moment and solve for the parameter value of interest.</p>
<p><span class="math display">\[\begin{eqnarray*}
E[X^k] &amp;=&amp; k^{th} \mbox{ expected moment}\\
\frac{1}{n} X_i^k &amp;=&amp; k^{th} \mbox{ sample moment}\\
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 3.5  </strong></span>Find the MOM for both parameters using a random sample, <span class="math inline">\(X_1, X_2, \ldots, X_n \sim N(\mu, \sigma^2)\)</span>.
<span class="math display">\[\begin{eqnarray*}
\tilde{\mu} &amp;=&amp; \overline{X}\\
\tilde{\sigma^2} &amp;=&amp; \frac{\sum X_i^2}{n} - \overline{X}^2\\
&amp;=&amp; \frac{1}{n} \sum(X_i - \overline{X})^2 \mbox{   MLE!!}
\end{eqnarray*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-15" class="example"><strong>Example 3.6  </strong></span>Find the MOM for both parameters using a random sample, <span class="math inline">\(X_1, X_2, \ldots, X_n \sim Gamma(\alpha,\beta)\)</span>.
<span class="math display">\[\begin{eqnarray*}
\alpha / \beta &amp;=&amp; \overline{X}\\
\alpha / \beta^2 &amp;=&amp; \sum X_i^2 /n - \overline{X}^2\\
\alpha &amp;=&amp; \overline{X} \beta\\
\frac{\overline{X} \beta}{\beta^2} &amp;=&amp; \sum X_i^2 /n - \overline{X}^2\\
\tilde{\beta} &amp;=&amp; \frac{\overline{X}}{(\sum X_i^2 / n - \overline{X}^2)}\\
&amp;=&amp; \overline{X} / \hat{\sigma^2}\\
\tilde{\alpha} &amp;=&amp; \overline{X}^2 / \hat{\sigma^2}
\end{eqnarray*}\]</span></p>
</div>
<div id="benefits-and-limitations-of-method-of-moments-estimation" class="section level3" number="3.2.1">
<h3>
<span class="header-section-number">3.2.1</span> Benefits and Limitations of Method of Moments Estimation<a class="anchor" aria-label="anchor" href="#benefits-and-limitations-of-method-of-moments-estimation"><i class="fas fa-link"></i></a>
</h3>
<div id="benefits-2" class="section level4 unnumbered">
<h4>Benefits<a class="anchor" aria-label="anchor" href="#benefits-2"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Often easier to compute than MLEs (e.g., MLE for <span class="math inline">\(\alpha\)</span> in the Gamma distribution is intractable).</li>
<li>Estimates by MOM can be used as first approximations of the likelihood.</li>
<li>Can easily estimate multiple parameter families.</li>
</ul>
</div>
<div id="limitations-2" class="section level4 unnumbered">
<h4>Limitations<a class="anchor" aria-label="anchor" href="#limitations-2"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>Can sometimes (usually in cases of small sample sizes) produce estimates outside the parameter space.</li>
<li>Doesn’t work if the moments don’t exist (e.g., Cauchy distribution)</li>
<li>MLEs are typically closer to the quantity being estimated.</li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>Example 3.7  </strong></span>Tank Estimators</p>
<p>How can a random sample of integers between 1 and N (with N unknown to the researcher) be used to estimate N?</p>
<ol style="list-style-type: decimal">
<li>The tanks are numbered from 1 to N. Working with your group, randomly select five tanks, without replacement, from the bowl. The tanks are numbered:</li>
<li>Think about how you would use your data to estimate N. (Come up with at least 3 estimators.) Come to a consensus within the group as to how this should be done.</li>
<li>Our estimates of N are:</li>
<li>Our rules or formulas for the estimators of N based on a sample of n (in your case 5) integers are:</li>
<li>Assuming the random variables are distributed according to a discrete uniform:
<span class="math display">\[\begin{eqnarray*}
X_i \sim P(X=x | N) = \frac{1}{N} \ \ \ \ \ x = 1,2,\ldots, N \ \ \ \ i=1,2,\ldots, n
\end{eqnarray*}\]</span>
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>What is the method of moments estimator of N?</li>
<li>What is the maximum likelihood estimator of N?</li>
</ol>
<div id="mean-squared-error-1" class="section level4 unnumbered">
<h4>Mean Squared Error<a class="anchor" aria-label="anchor" href="#mean-squared-error-1"><i class="fas fa-link"></i></a>
</h4>
<p>Most of our estimators are made up of four basic functions of the data: the mean, the median, the min, and the max. Fortunately, we know something about the moments of these functions:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="41%">
<col width="26%">
<col width="32%">
</colgroup>
<thead><tr class="header">
<th>g(<span class="math inline">\(\underline{X}\)</span>)</th>
<th align="center">E[ g(<span class="math inline">\(\underline{X}\)</span>) ]</th>
<th align="center">Var( g(<span class="math inline">\(\underline{X}\)</span>) )</th>
</tr></thead>
<tbody>
<tr class="odd">
<td></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\overline{X}\)</span></td>
<td align="center"><span class="math inline">\(\frac{N+1}{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{(N+1)(N-1)}{12 n}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>median(<span class="math inline">\(\underline{X}\)</span>) = M</td>
<td align="center"><span class="math inline">\(\frac{N+1}{2}\)</span></td>
<td align="center"><span class="math inline">\(\frac{(N-1)^2}{4 n}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>min(<span class="math inline">\(\underline{X}\)</span>)</td>
<td align="center"><span class="math inline">\(\frac{(N-1)}{n} + 1\)</span></td>
<td align="center"><span class="math inline">\(\bigg(\frac{N-1}{n}\bigg)^2\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td>max(<span class="math inline">\(\underline{X}\)</span>)</td>
<td align="center"><span class="math inline">\(N - \frac{(N-1)}{n}\)</span></td>
<td align="center"><span class="math inline">\(\bigg(\frac{N-1}{n}\bigg)^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>Using this information, we can calculate the MSE for 4 of the estimators that we have derived. (Remember that MSE = Variance + Bias<span class="math inline">\(^2\)</span>.)</p>
<p><span class="math display">\[\begin{eqnarray}
\mbox{MSE } ( 2 \cdot \overline{X} - 1) &amp;=&amp; \frac{4 (N+1) (N-1)}{12n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&amp;=&amp; \frac{4 (N+1) (N-1)}{12n} \\
\nonumber \\
\mbox{MSE } ( 2 \cdot M - 1) &amp;=&amp; \frac{4 (N-1)^2}{4n} + \Bigg(2 \bigg(\frac{N+1}{2}\bigg) - 1 - N\Bigg)^2 \nonumber \\
&amp;=&amp; \frac{4 (N-1)^2}{4n} \\
\nonumber \\
\mbox{MSE } ( \max(\underline{X})) &amp;=&amp; \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(N - \frac{(N-1)}{n} - N\Bigg)^2 \nonumber\\
&amp;=&amp; \bigg(\frac{N-1}{n}\bigg)^2 + \bigg(\frac{N-1}{n} \bigg)^2  = 2*\bigg(\frac{N-1}{n} \bigg)^2 \\
\nonumber \\
\mbox{MSE } \Bigg( \bigg( \frac{n+1}{n} \bigg) \max(\underline{X})\Bigg) &amp;=&amp; \bigg(\frac{n+1}{n}\bigg)^2 \bigg(\frac{N-1}{n}\bigg)^2 + \Bigg(\bigg(\frac{n+1}{n}\bigg) \bigg(N - \frac{N-1}{n} \bigg) - N \Bigg)^2
\end{eqnarray}\]</span></p>
</div>
</div>
</div>
</div>
</div>
<div id="reflection-questions-2" class="section level2" number="3.3">
<h2>
<span class="header-section-number">3.3</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-2"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>What is a prior distribution? What is the random variable described by a prior?</li>
<li>What is a likelihood? What is the random variable described by a likelihood?</li>
<li>What is a posterior distribution? What is the random variable described by a posterior?</li>
<li>What is a conjugate prior? What is the benefit of a conjugate prior?</li>
<li>Is all hope lost if the prior is not conjugate? If it is not, how would we approach the problem of coming up with a posterior?</li>
</ol>
</div>
<div id="ethics-considerations-2" class="section level2" number="3.4">
<h2>
<span class="header-section-number">3.4</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-2"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>When does it make sense to incorporate prior information? When doesn’t it make sense to incorporate prior information?</li>
<li>What are some legitimate ways to calculate a prior? What are some illegitimate ways to calculate a prior?</li>
<li>How is an analyst able to come up with a prior or a likelihood?</li>
</ol>
</div>
<div id="r-code-mle-example" class="section level2" number="3.5">
<h2>
<span class="header-section-number">3.5</span> R code: MLE Example<a class="anchor" aria-label="anchor" href="#r-code-mle-example"><i class="fas fa-link"></i></a>
</h2>
<p>Example 7.6.5 in the text looks at the MLE for the center of of a Cauchy distribution.
The Cauchy distribution is interesting because the tails decay at a rate of <span class="math inline">\(1/x^2\)</span>, so that when you try to take the expected value, you end up integrating something that looks like <span class="math inline">\(1/x\)</span> over the real line.
Hence, the expected value does not exist.
Thus, method of moments estimators are of no use.
The MLE is still useful, though not easy to find. As stated in the text, the likelihood is proportional to
<span class="math display">\[\prod_{i=1}^n [1 + (x_i - \theta)^2]^{-1}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Compute the first and second derivative of the log likelihood.</li>
<li>Consider trying to find the root of a function f(x). Suppose your current
guess is some value <span class="math inline">\(x_0\)</span>. We might approximate the function by the tangent line (first order Taylor approximation) at x0 and take our next guess as the root of that line. Use the Taylor expansion to find the next guess of <span class="math inline">\(x_1\)</span> as</li>
</ol>
<p><span class="math display">\[x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}\]</span></p>
<p>Continually updating our guesses via this method is known as Newton’s Method
or the Newton-Raphson Method.</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>Generate 50 observations from a Cauchy distribution centered at <span class="math inline">\(\theta = 10\)</span>. Based on these 50 observations, use parts (a) and (b) to estimate <span class="math inline">\(\theta\)</span>. Remember, we’re trying to maximize the likelihood, so the function that we are trying to find the root of is derivative of the log-likelihood. The R code might look something like this:</li>
</ol>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="MLE.html#cb7-1" aria-hidden="true" tabindex="-1"></a>x<span class="ot">=</span><span class="fu">rt</span>(<span class="dv">50</span>,<span class="dv">1</span>)<span class="sc">+</span><span class="dv">10</span>  <span class="co"># 50 random Cauchy variables centered at 10</span></span>
<span id="cb7-2"><a href="MLE.html#cb7-2" aria-hidden="true" tabindex="-1"></a>vect.theta <span class="ot">=</span> <span class="fu">c</span>()  <span class="co"># placeholder</span></span>
<span id="cb7-3"><a href="MLE.html#cb7-3" aria-hidden="true" tabindex="-1"></a>theta.guess<span class="ot">=</span>((pick a starting value, you might try different ones))  <span class="co"># just a number</span></span>
<span id="cb7-4"><a href="MLE.html#cb7-4" aria-hidden="true" tabindex="-1"></a>theta.gues <span class="ot">=</span> <span class="dv">10</span>  <span class="co"># what happens when your initial guess is far from 10?</span></span>
<span id="cb7-5"><a href="MLE.html#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="MLE.html#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>) {  <span class="co"># play around with how many times you loop through.  10 is likely too small.</span></span>
<span id="cb7-7"><a href="MLE.html#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="MLE.html#cb7-8" aria-hidden="true" tabindex="-1"></a>    f1<span class="ot">=</span>((compute first derivative of log<span class="sc">-</span>likelihood evaluated at theta.guess))</span>
<span id="cb7-9"><a href="MLE.html#cb7-9" aria-hidden="true" tabindex="-1"></a>    f2<span class="ot">=</span>((compute second derivative at theta.guess))</span>
<span id="cb7-10"><a href="MLE.html#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="MLE.html#cb7-11" aria-hidden="true" tabindex="-1"></a>    f1<span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sum</span>((x<span class="sc">-</span>theta.guess)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>(x<span class="sc">-</span>theta.guess)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb7-12"><a href="MLE.html#cb7-12" aria-hidden="true" tabindex="-1"></a>    f2<span class="ot">=</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">sum</span>(((<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span>(x<span class="sc">-</span>theta.guess)<span class="sc">^</span><span class="dv">2</span>))<span class="sc">+</span>((<span class="dv">2</span><span class="sc">*</span>(x<span class="sc">-</span>theta.guess)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>((<span class="dv">1</span><span class="sc">+</span>(x<span class="sc">-</span>theta.guess)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">^</span><span class="dv">2</span>)))</span>
<span id="cb7-13"><a href="MLE.html#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="MLE.html#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="MLE.html#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># f1 and f2 need to be written as R functions of theta.guess</span></span>
<span id="cb7-16"><a href="MLE.html#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="MLE.html#cb7-17" aria-hidden="true" tabindex="-1"></a>    theta.guess<span class="ot">=</span>theta.guess <span class="sc">-</span> f1<span class="sc">/</span>f2</span>
<span id="cb7-18"><a href="MLE.html#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">print</span>(theta.guess)</span>
<span id="cb7-19"><a href="MLE.html#cb7-19" aria-hidden="true" tabindex="-1"></a>    vect.theta <span class="ot">=</span> <span class="fu">c</span>(vect.theta, theta.guess) <span class="co"># keeping track of all your guesses</span></span>
<span id="cb7-20"><a href="MLE.html#cb7-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-21"><a href="MLE.html#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="MLE.html#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(vect.theta)</span></code></pre></div>
<p>where the actual formulas for f1 and f2 will come from part (a). Everything in
double parentheses needs to be replaced by proper R syntax. The rest of it will
run in R. In case you are curious, <code><a href="https://rdrr.io/r/base/sum.html">sum()</a></code> is the R function for summing a vector.</p>
<p>Indeed, if your initial guess is far from 10, the Newton-Raphson procedure won’t converge to the MLE (it is a first approximation). Consider the following figure that describes the behavior of the derivative of the log-likelihood function. Note that the N-R procedure will go off to infinity or negative infinity if it starts at a value far from 10 (see purple line). If the initial value is close to 10, it will converge to the MLE (see red line).</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#MLE"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li>
<a class="nav-link" href="#qualities-of-the-mle"><span class="header-section-number">3.1</span> Qualities of the MLE</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#invariance-of-the-mle"><span class="header-section-number">3.1.1</span> Invariance of the MLE</a></li>
<li><a class="nav-link" href="#consistency-of-the-mle"><span class="header-section-number">3.1.2</span> Consistency of the MLE</a></li>
<li><a class="nav-link" href="#bias-of-the-mle-section-7.7"><span class="header-section-number">3.1.3</span> Bias of the MLE (section 7.7)</a></li>
<li><a class="nav-link" href="#benefits-and-limitations-of-maximum-likelihood-estimation"><span class="header-section-number">3.1.4</span> Benefits and Limitations of Maximum Likelihood Estimation</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#method-of-moments"><span class="header-section-number">3.2</span> Method of Moments</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#benefits-and-limitations-of-method-of-moments-estimation"><span class="header-section-number">3.2.1</span> Benefits and Limitations of Method of Moments Estimation</a></li></ul>
</li>
<li><a class="nav-link" href="#reflection-questions-2"><span class="header-section-number">3.3</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-2"><span class="header-section-number">3.4</span>  Ethics Considerations</a></li>
<li><a class="nav-link" href="#r-code-mle-example"><span class="header-section-number">3.5</span> R code: MLE Example</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/03-MLE.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/03-MLE.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-09-13.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
