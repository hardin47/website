[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 152 Pomona College: Computational Statistics. notes based extensively Probability & Statistics (DeGroot Schervish 2011).responsible reading relevant chapters text. text good & readable, use . make sure coming class also re-visiting warm-ups feedback provided.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"","code":""},{"path":"intro.html","id":"what-is-statistics","chapter":"1 Introduction","heading":"1.1.0.1 What is Statistics?","text":"Generally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims population","code":""},{"path":"intro.html","id":"what-is-the-content-of-math-152","chapter":"1 Introduction","heading":"1.1.0.2 What is the content of Math 152?","text":"Statistical Theory introduction statistics students background probability theory, calculus, linear algebra. statistics prerequisite class. course focused theoretical aspects material, though real world applications class homework assignments. idea strong mathematical understanding concepts also understanding concepts applied real world.completion course, students :able derive methods introductory statistics using tools mathematics (.e., calculus probability).able justify use particular method (technical conditions).able weigh advantages disadvantages different estimation techniques (e.g., bias, variability, resistance outliers).know communicate results effectively.","code":""},{"path":"intro.html","id":"who-should-take-math-152","chapter":"1 Introduction","heading":"1.1.0.3 Who should take Math 152?","text":"Linear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.","code":""},{"path":"intro.html","id":"what-are-the-prerequisites-for-math-152","chapter":"1 Introduction","heading":"1.1.0.4 What are the prerequisites for Math 152?","text":"prerequisites Statistical Theory Probability (Math 151 equivalent) completion three semester sequence calculus linear algebra. rely heavily prerequisites, students background probability multivariable calculus find trying catch throughout semester. familiar topics conditional probabilities expectations, Central Limit Theorem, moment generating functions, probability density functions.","code":""},{"path":"intro.html","id":"is-there-overlap-with-other-classes","chapter":"1 Introduction","heading":"1.1.0.5 Is there overlap with other classes?","text":"Statistical Theory covers many topics Introductory Statistics (e.g., Math 58), however, treatment topics completely different.\nStatistical Theory uses mathematical tools (e.g., probability theory calculus) derive topics prove , many ways, optimal.","code":""},{"path":"intro.html","id":"when-should-i-take-math-152","chapter":"1 Introduction","heading":"1.1.0.6 When should I take Math 152?","text":"prerequisite structure, students Math 152 juniors seniors.","code":""},{"path":"intro.html","id":"what-is-the-workload-for-math-152","chapter":"1 Introduction","heading":"1.1.0.7 What is the workload for Math 152?","text":"one homework assignment per week, two -class midterm exams, two take-home midterm exams, final exam. Many students report working 8-10 hours per week outside class.##software use? real world applications? mathematics? CS?majority work Statistical Theory done pencil (/ pen / LaTeX).\nHowever, use R way investigating theorecial concepts computationally.\nR work done using RStudio IDE.\nneed either download R RStudio (free) onto computer use Pomona’s server.\nreal-world applications motivate theory, cours applied focus data analysis.can use R Pomona server: https://rstudio.pomona.edu/ (Pomona students able log immediately.\nNon-Pomona students need get Pomona login information.)Alternatively, feel free download R onto computer.\nR freely available http://www.r-project.org/ already installed college computers.\nAdditionally, required install RStudio turn R assignments using RMarkdown. http://rstudio.org/.\n(can use LaTeX compiler : https://yihui.name/tinytex/)\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Although Statistical Theory statistics prerequisite, students seen many building blocks high school classes Probability Theory.\nreview vocabulary covered previous contexts.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"","code":""},{"path":"intro.html","id":"probability","chapter":"1 Introduction","heading":"Probability","text":"probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).experiment activity situation uncertainty outcome.experiment activity situation uncertainty outcome.sample space list possible outcomes random trial. (Called S.)sample space list possible outcomes random trial. (Called S.)event potential subset sample space.event potential subset sample space.set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).simple event event consisting exactly one outcome.simple event event consisting exactly one outcome.Two events mutually exclusive disjoint occur simultaneously.Two events mutually exclusive disjoint occur simultaneously.Two events independent occurrence one change probability second occur.Two events independent occurrence one change probability second occur.random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]","code":""},{"path":"intro.html","id":"statistics","chapter":"1 Introduction","heading":"Statistics","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]\\(\\Omega\\) parameter space, set values contains possible realizations parameter.many statistical problems, assume distribution generated data completely known except parameters. goal statistics make statements (inference) population using sample (data). course broken two main parts: parameter estimation tests hypotheses.","code":""},{"path":"intro.html","id":"estimation","chapter":"1 Introduction","heading":"1.2.2 Estimation","text":"many statistical problems, assume distribution generated data completely known except parameter(s). first half course, ’re going learn different ways estimate parameters, properties estimators, makes good estimator.","code":""},{"path":"intro.html","id":"example","chapter":"1 Introduction","heading":"example","text":"Consider trying estimate true average weight dozen eggs particular farm. add random 12 values pdf different multiplying one observation 12. Clearly 12*one egg much variable weight dozen eggs.\n\\[\\begin{align*}\n\\mbox{statistic } 1 &= 12 X\\\\\n\\mbox{statistic } 2 &= \\sum_{=1}^{12} X_i\\\\\n\\Omega &= \\{ \\mu: \\mu \\\\!\\!R+\\}\\\\\n\\end{align*}\\]","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3  Reflection Questions","text":"difference sample population?difference discrete random variable continuous random variable?difference pdf cdf?can find expected value discrete random variable? can find expected value continuous random variable?difference R RStudio?","code":""},{"path":"intro.html","id":"ethics-considerations","chapter":"1 Introduction","heading":"1.4  Ethics Considerations","text":"important distinguish sample population?ever okay model discrete random variable using continuous probability model? , ? , ?better understand method’s properties theoretically better use computational tools investigate method’s properties? [question probably won’t able answer end semester.]","code":""},{"path":"intro.html","id":"r-code-reproduciblity","chapter":"1 Introduction","heading":"1.5 R code: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.5.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.5.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.5.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"bayes.html","id":"bayes","chapter":"2 Bayesian Estimation","heading":"2 Bayesian Estimation","text":"","code":""},{"path":"bayes.html","id":"bayes-rule","chapter":"2 Bayesian Estimation","heading":"2.1 Bayes’ Rule","text":"Theorem 2.1  Bayes’ RuleGiven events \\(\\) \\(B\\),\n\\[\\begin{eqnarray*}\nP(|B) &=& \\frac{P(AB)}{P(B)} = \\frac{P(B|) P()}{P(AB) + P(^cB)} \\nonumber \\\\\n&=& \\frac{P(B|)P()}{ \\sum_i P(B|A_i) P(A_i)}\n\\end{eqnarray*}\\]Many following examples may familiar . reading , work understand intuition (denominator changes condition!) well mathematical connection Bayes’ Rule.Example 2.1  Suppose rate infection TB 1 1000 (0.1 percent = 0.001). Suppose TB test used 90% accurate: gives positive result 10 percent people actually TB, reaction skin test. Also, 10% people actually TB fail react test.1What’s chance someone TB test positive?’s chance randomly chosen person tests negative actually TB?another TB test gives fewer false positives, expensive. better use one?prior probability TB?posterior probability TB (given positive test)?Solution:can use table figure probabilities. Consider population 10,000 people:Alternatively, can use probability statements easier work long-run scenarios get complicated. know following:\\[\\begin{eqnarray*}\nP(TB + ) &=& 0.001\\\\\nP(Test + | TB - ) &=& 0.1 \\\\\nP(Test - | TB + ) &=& 0.1\n\\end{eqnarray*}\\]10 10,000 people disease. 9 10 actually test positive TB. However, 999 9990 people false positives. , \\(9/(999+9) = 0.0089 \\mbox{ } \\approx\\) 0.9% people test positive actually TB.\\[\\begin{eqnarray*}\nP( TB + | Test + ) &=& \\frac{P(TB + \\& Test + )}{P(Test + )} = \\frac{P(Test  + | TB + ) P(TB+)}{P(Test + )}\\\\\n&=& \\frac{P(Test  + | TB + ) P(TB+)}{P(Test  + | TB - ) P(TB-) + P(Test  + | TB + ) P(TB+)} \\\\\n&=& \\frac{0.9 \\cdot 0.001}{ 0.1 \\cdot 0.999 + 0.9 \\cdot 0.001}\\\\\n&=& 0.0089\n\\end{eqnarray*}\\]1 10,000 people false negative (0.0001 = 0.01% opposed 9.99% false positives).\\[\\begin{eqnarray*}\nP( TB + | Test - ) &=& \\frac{P(TB + \\& Test - )}{P(Test - )} = \\frac{P(Test  - | TB + ) P(TB+)}{P(Test - )}\\\\\n&=& \\frac{P(Test - | TB + ) P(TB+)}{P(Test  - | TB - ) P(TB-) + P(Test  - | TB + ) P(TB+)} \\\\\n&=& \\frac{0.1 \\cdot 0.001}{ 0.9 \\cdot 0.999 + 0.1 \\cdot 0.001}\\\\\n&=& 0.00011121\n\\end{eqnarray*}\\]necessarily, since ’s much worse false negative false positive. People test positive given another test fewer false positives.0.0010.009Example 2.2  cab involved hit run accident night. Two cab companies, Green Blue, operate city. Suppose told following:85 percent cabs city Green, remaining 15 percent Blue.witness identified cab Blue (dark!) court tested reliability witness circumstances existed night accident, determined witness correctly identified cab color 80% time, made mistake 20% time, regardless actual color cab.’s verdict? .e., probability cab involved hit--run actually Blue?Solution:information given, following probabilities known:\n\\[\\begin{eqnarray*}\nP(said B | B) &=& 0.8\\\\\nP(said G | B) &=& 0.2\\\\\nP(B) &=& 0.15 \\ \\ \\ \\mbox{prior probability!}\n\\end{eqnarray*}\\]probability interest :\n\\[\\begin{eqnarray*}\nP(B | said B) &=& \\frac{P(said B | B) P(B)}{P(said B | G) P(G) + P(said B | B) P(B)}\\\\\n&=& \\frac{0.8*0.15}{0.2*0.85 + 0.8*0.15} = 0.41\n\\end{eqnarray*}\\]Example 2.3  Consider famous Monte Hall problem based game show, Let’s Make Deal. part show, contestant asked pick one three doors. Two doors nothing behind , third door car prize. Monte Hall (host) opens non-prize door contestant hadn’t chosen (always door available open one prize). Monte offers contestant opportunity switch original door remaining door. switch? Stay? doesn’t matter? probability winning situations?Define following:\n\\[\\begin{eqnarray*}\nC_i &:& \\mbox{car behind Door $$, } \\\\{ 1, 2, 3\\}\\\\\nH_{ij} &:& \\mbox{host opens Door $j$ player picked Door $$, } , j \\\\{ 1,2,3\\}\n\\end{eqnarray*}\\]example, \\(C_2\\) situation car behind Door 2, \\(H_{23}\\) denotes situation host opened Door 3 chose Door 2. Note \\(H_{ij}\\) information (’ll condition ). Let’s say start picking Door 2, host opens Door 3.\\[\\begin{eqnarray*}\nP(C_i) &=& \\frac{1}{3}\\\\\nP(C_2) &=& \\frac{1}{3}\\\\\nP(C_2 | H_{23}) &=& \\frac{P(H_{23} | C_2) P(C_2)}{P(H_{23})} \\ \\ \\ \\ \\ \\mbox{(Bayes rule, right !  See } P(H_{23}) \\mbox{ )}\\\\\n&=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{P(H_{23})} \\\\\n\\nonumber \\\\\nP(H_{23}) &=&  P(H_{23} C_1) + P(H_{23} C_2) + P(H_{23} C_3)\\\\\n&=&  P(H_{23} | C_1) P(C_1) + P(H_{23} | C_2) P(C_2) + P(H_{23} | C_3) P(C_3)\\\\\n&=& \\frac{1}{2} \\cdot \\frac{1}{3} + 1 \\frac{1}{3} + 0 \\frac{1}{3} = \\frac{1}{2}\\\\\n\\nonumber\\\\\nP(C_2 | H_{23}) &=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}}\\\\\n&=& \\frac{1}{3}\\\\\n\\end{eqnarray*}\\]Using know rules game (, car behind door 3, ’d never open !), know:\n\\[\\begin{eqnarray*}\nP(C_3 | H_{23}) = 0\n\\end{eqnarray*}\\]car behind one three doors:\n\\[\\begin{eqnarray*}\n1 &=& P(C_1 | H_{23}) + P(C_2 | H_{23}) + P(C_3 | H_{23})\\\\\nP(C_1 | H_{23} ) &=& 1 - P(C_2 | H_{23}) - P(C_3 | H_{23})\\\\\n&=& 1 - \\frac{1}{3} - 0\\\\\n&=& \\frac{2}{3}\n\\end{eqnarray*}\\]…. probability car behind Door 1 2/3 probability ’s behind Door 2 1/3 \\(\\rightarrow\\) switch doors!previous situations based discrete values parameter data (neither typically true). think {} {}, want find value {} \\(P(|B)\\) maximized. going use probability distribution functions (pdfs) instead discrete probabilities, need notation.","code":""},{"path":"bayes.html","id":"prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.2 Prior Distributions","text":"prior distribution distribution parameter (e.g., \\(\\theta\\)) observing data. Note:observations come \\(f(x|\\theta), \\theta \\\\Omega\\)can express likely \\(\\theta\\) various regions \\(\\Omega\\) terms probability distribution \\(\\theta\\).Bayesians believe use prior distributions modeling always know something situation hand.Frequentists believe use data collected experiment sample (prior information).Example 2.4  want predict high temperature given day October.\n\\[\\begin{align*}\n\\Omega = \\{ (\\theta, \\sigma^2) &: \\theta \\\\!\\!R, \\sigma^2 \\\\!\\!R^+\\}\\\\\n\\mbox{} &: -\\infty < \\theta < \\infty, \\sigma^2 > 0 \\}\\\\\n\\mbox{} &: \\theta > 30, 0 < \\sigma^2 < 625 \\}\\\\\n\\end{align*}\\]\nSuppose know variance \\(\\sigma^2 = 12^2\\). mean, \\(\\theta\\) unknown. might specify prior distribution \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\xi (\\theta) &\\rightarrow& \\theta \\sim N(\\mu, \\nu^2)\\\\\n&& \\mu =78^\\circ, \\nu^2 = (2.5^\\circ)^2\\\\\n\\end{eqnarray*}\\]value \\(\\nu\\) measure uncertainty prior beliefs. , estimated \\(\\theta\\) \\(78^\\circ\\), aren’t sure value, add uncertainty belief (\\(\\nu\\)). \\(\\mu\\) \\(\\nu\\) called hyper-parameters.","code":""},{"path":"bayes.html","id":"posterior-distributions","chapter":"2 Bayesian Estimation","heading":"2.3 Posterior Distributions","text":"posterior distribution conditional distribution parameter (e.g., \\(\\theta\\)) given observed data.Aside, little probability review:Example 2.5  Suppose interested rolling two dice. Let \\(X\\) larger value; let \\(Y\\) sum two dice. Find joint marginal distributions \\(X\\) \\(Y\\). solution table probabilities:notation review. Suppose \\(n\\) data points \\(f(x| \\theta)\\). ’ll assume ’s simple random sample (SRS), therefore observations independent.\\[\\begin{eqnarray*}\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(x_1| \\theta) f(x_2| \\theta) \\cdots f(x_n| \\theta)\\\\\n\\mbox{ } \\underline{x} &=& \\{ x_1, x_2, \\ldots, x_n \\}\\\\\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(\\underline{x} | \\theta)\n\\end{eqnarray*}\\]Remember, \\(f(\\underline{x} | \\theta)\\) conditional distribution \\(\\underline{x}\\) given \\(\\theta\\). likelihood function, \\(f(\\underline{x} | \\theta)\\), joint pdf observations (representing: likely data?).Define\n\\[\\begin{eqnarray*}\nf(\\underline{x}, \\theta) &=& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\ng_n(\\underline{x}) &=& \\int_\\Omega f(\\underline{x}, \\theta) d\\theta\\\\\n&=& \\int_\\Omega f(\\underline{x} | \\theta) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]Remember, however, interested probability parameter given data:\n\\[\\begin{eqnarray*}\n\\xi(\\theta| \\underline{x}) = \\frac{f(\\underline{x} | \\theta) \\xi(\\theta)}{g_n(\\underline{x})} \\ \\ \\ \\ \\ \\ \\ \\theta \\\\Omega\n\\end{eqnarray*}\\](Bayes’ Theorem!!!)prior, \\(\\xi(\\theta)\\) relative likelihood \\(\\theta\\) data observed.posterior, \\(\\xi(\\theta | \\underline{x})\\) relative likelihood \\(\\theta\\) \\(\\underline{X} = \\underline{x}\\) observed.know posterior function \\(\\theta\\). ’s important keep mind function , posterior function data.\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi(\\theta)\\\\\n\\mbox{posterior} &\\propto& \\mbox{likelihood} \\cdot \\mbox{prior}\n\\end{eqnarray*}\\], posterior proportional product likelihood prior. Note \\(g_n(\\underline{x})\\) depend \\(\\theta\\) part proportionality constant. , can always find \\(g_n(\\underline{x})\\) know posterior integrates 1. (Sometimes \\(g_n\\) extraordinarily difficult find.)\n\\[\\begin{eqnarray*}\n\\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta = 1\n\\end{eqnarray*}\\]Example 2.6  Suppose true proportion freethrows Steph Curry able make successfully unknown. assume freethrows distributed according Bernoulli process.\\[\\begin{eqnarray*}\nX = \\left\\{ \\begin{array}{ll}\n    1 & \\mbox{Curry makes shot}\\\\\n    0 & \\mbox{Curry misses shot}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]say,\n\\[\\begin{eqnarray*}\nX &\\sim& \\mbox{Bernoulli}(\\theta)\\\\\nf(\\underline{x}|\\theta)&=&  \\theta ^y (1 - \\theta)^{n-y} \\ \\ \\ \\ y = \\sum_{=1}^n x_i\n\\end{eqnarray*}\\]\nNote: \\(\\underline{x} = \\{x_1, x_2, \\ldots, x_n\\}\\) specific ordering 0s 1s.prior information Curry’s abilities, put uniform prior \\(\\theta\\).\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    1 & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) \\propto \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]functional form posterior (likelihood times prior). ’re trying estimate \\(\\theta\\), distribution \\(\\theta\\) given data? Recall Beta distribution (probability review):\\[\\begin{eqnarray*}\nW &\\sim& \\mbox{Beta}(\\alpha,\\beta)\\\\\nf(w) &=& \\frac{1}{B(\\alpha,\\beta)} w^{\\alpha - 1} (1-w)^{\\beta-1}\\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}w^{\\alpha - 1} (1-w)^{\\beta-1} \\ \\ \\ \\ w \\[0,1]\n\\end{eqnarray*}\\]Using likelihood prior, able find full posterior distribution integration:\n\\[\\begin{eqnarray*}\n1 &=& \\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta \\\\\n&=&  k \\cdot \\int_\\Omega \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta) d\\theta\\\\\n&=& k \\cdot \\int_0^1 \\theta^y (1-\\theta)^{n-y} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(y+1 + n-y + 1)} \\cdot \\int_0^1 \\frac{\\Gamma(y+1 + n-y + 1)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^{y+1-1} (1-\\theta)^{n-y+1-1} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)} \\cdot 1\\\\\nk &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\\\\ng(\\underline{x}) &=& \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)}\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]Note, however, didn’t actually need integrate find \\(g(\\underline{x})\\). simply needed note \\(\\theta\\) takes place \\(w\\) Beta distribution, automatically know appropriate constant value.information prior distribution \\(\\theta\\)? Suppose believe Beta(,b). (Note: \\(E[\\theta] = \\frac{}{+b}\\), Var\\((\\theta) = \\frac{ab}{(+b)^2 (+b+1)}\\), SD$() = $ ).\\[\\begin{eqnarray*}\n\\xi(\\theta) &\\propto& \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y+-1} (1-\\theta)^{n-y+b-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y+, n-y+b)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n++b)}{\\Gamma(y+)\\Gamma(n-y+b)}\\theta^{y+-1} (1-\\theta)^{n-y+b-1}\n\\end{eqnarray*}\\]Note: didn’t need calculate \\(g(\\underline{x})\\)!!!\\(E[\\theta | \\underline{x}] = \\frac{y+}{n++b}\\)Var \\((\\theta | \\underline{x}) = \\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}\\)SD \\((\\theta | \\underline{x}) = \\sqrt{\\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}}\\)Given prior sample size \\(n\\), best guess Curry’s ability hit freethrows?kind confidence estimate?","code":""},{"path":"bayes.html","id":"conjugate-prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.4 Conjugate Prior Distributions","text":"conjugate prior distribution one prior distribution family posterior distribution. Beta distribution conjugate Binomial distribution (note, \\(U[0,1]\\) distribution Beta(\\(\\alpha\\)=1,\\(\\beta\\)=1)).Example 2.7  Continuing example temperature, \\(X \\sim N(\\theta, \\sigma^2\\) (known)) normal prior, \\(\\theta \\sim N(\\mu, \\nu^2)\\). Remember typically, prior completely specified. example \\(\\mu=78\\) \\(\\nu=2.5\\). :\n\\[\\begin{eqnarray*}\nf(\\underline{x} | \\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\theta)^2 \\bigg]\\\\\n&\\propto& \\exp \\Bigg[ - \\frac{1}{2 \\sigma^2} \\bigg(n (\\theta-\\overline{x})^2 + \\sum_{=1}^n (x_i - \\overline{x})^2 \\bigg) \\Bigg]\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2\\bigg]\\\\\n&&\\\\\n\\xi(\\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n&&\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2 -\\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n\\mbox{note: } && \\frac{n}{\\sigma^2}(\\theta - \\overline{x})^2 + \\frac{1}{\\nu^2}(\\theta - \\mu)^2 = \\frac{1}{\\nu_1^2}(\\theta - \\mu_1)^2 + \\frac{n}{\\sigma^2 + n \\nu^2}(\\overline{x}-\\mu)^2 \\ \\ \\ \\mbox{ pg 399}\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu_1^2} (\\theta - \\mu_1)^2 \\bigg]\\\\\n\\theta | \\underline{x} &\\sim& N (\\mu_1, \\nu_1^2)\\\\\n\\mbox{: } && \\mu_1 = \\frac{\\sigma^2 \\mu + n \\nu^2 \\overline{x}}{\\sigma^2 + n \\nu^2} \\ \\ \\ \\ \\nu_1^2 = \\frac{\\sigma^2 \\nu^2}{\\sigma^2 + n \\nu^2}\n\\end{eqnarray*}\\]Remember, computing posterior \\(\\theta\\), can ignore anything doesn’t depend \\(\\theta\\) (“known” parameters, data, constants,…)conjugate families. Note conjugate, prior distribution family posterior distribution.","code":""},{"path":"bayes.html","id":"improper-priors","chapter":"2 Bayesian Estimation","heading":"2.5 Improper Priors","text":"Improper prior distributions actually probability functions, yet lead posterior distributions probability functions (, integrate 1). Improper priors capture idea data worth prior belief. Often, improper prior lead Frequentist result. example, Beta(0,0) prior Bernoulli likelihood leads :\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    \\theta^{-1}(1-\\theta)^{-1} & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\(\\xi(\\theta)\\) integrate 1, proper pdf. However posterior proper pdf,\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y-1} (1-\\theta)^{n-y-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y, n-y)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)}\\theta^{y-1} (1-\\theta)^{n-y-1}\\\\\n\\mbox{Note: } && E[\\theta | \\underline{X} ] = \\frac{y}{n} \\ \\ \\ \\mbox{ expected frequentist model!}\n\\end{eqnarray*}\\]conjugate priors improper prior limiting case: Beta(0,0), gamma(0,0), Normal(\\(\\mu, \\nu^2 = \\infty\\)). normal improper prior ignores prior constant, becomes:\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\lim_{\\nu^2 \\rightarrow \\infty} exp\\bigg(-\\frac{1}{2\\nu^2}(\\theta - \\mu)^2\\bigg) = 1\n\\end{eqnarray*}\\], improper normal prior flat line reals. Note improper normal prior used normal likelihood, posterior \\(\\theta | \\underline{x} \\sim N (\\underline{x}, \\sigma^2 /n)\\). [, prior indicating knowledge \\(\\theta\\) produces posterior depends data.]","code":""},{"path":"bayes.html","id":"bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.6 Bayes’ Estimators","text":"Prior posterior distributions tell us Bayesians think parameters. next question need address think estimators? estimator function data hope close true value parameter.Note:\n\\[\\begin{eqnarray*}\n\\delta(X_1, X_2, \\ldots, X_n) &=& \\delta(\\underline{X}) \\mbox{  estimator}\\\\\n\\delta(x_1, x_2, \\ldots, x_n) &=& \\delta(\\underline{x}) \\mbox{  estimate}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"loss-functions","chapter":"2 Bayesian Estimation","heading":"2.6.1 Loss functions","text":"(responsible material loss functions. take away message section Bayes estimator use expected value posterior distribution. However, seen , Bayes estimators, example, median posterior distribution used.)want estimator \\(\\theta\\) leads estimate close true value \\(\\theta\\). loss function helps determine far estimator . particular estimate, \\(\\):\n\\[\\begin{eqnarray*}\n\\mbox{squared error loss: } L(\\theta, ) &=& (\\theta - )^2\\\\\n\\mbox{absolute error loss: } L(\\theta, ) &=& |\\theta - |\\\\\n\\end{eqnarray*}\\]\nwant loss small (minimized).","code":""},{"path":"bayes.html","id":"squared-error-loss","chapter":"2 Bayesian Estimation","heading":"Squared Error Loss","text":"Without data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,)] = \\int_\\Omega L(\\theta, ) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,) | \\underline{X}] &=& \\int_\\Omega L(\\theta, ) \\xi(\\theta| \\underline{X}) d\\theta\\\\\n\\end{eqnarray*}\\]\nLet \\(\\delta^*(\\underline{X})\\) value \n\\[\\begin{eqnarray*}\nE[L(\\theta, \\delta^*(\\underline{X})) | \\underline{X} ] &=& \\min_{\\\\Omega} E[ L(\\theta,) | \\underline{X} ]\\\\\n\\delta^*(\\underline{X}) && \\mbox{ Bayes estimator } \\theta\\\\\n\\delta^*(\\underline{x}) && \\mbox{ Bayes estimate } \\theta\\\\\n\\end{eqnarray*}\\]\n\\(\\delta^*\\)?Note: \\(\\delta^*\\) depends loss function prior / posterior.\\[\\begin{eqnarray*}\nE[L(\\theta, ) | \\underline{X} ] &=& E[ (\\theta - )^2 | \\underline{X}]\\\\\n&=& E[ \\theta^2 - 2a\\theta + ^2 | \\underline{X}]\\\\\n&=& E[\\theta^2 | \\underline{X}] - 2a E[\\theta | \\underline{X}] + ^2\\\\\n&&\\\\\n\\frac{\\partial E[ (\\theta - )^2 | \\underline{X}]}{\\partial } &=& 0\\\\\n&&\\\\\n- 2 E[\\theta | \\underline{X}] + 2a &=& 0\\\\\n&=& E[\\theta | \\underline{X}] = \\delta^*(\\underline{X}) !!!\\\\\n&&\\\\\n\\frac{\\partial^2 E[ (\\theta - )^2 | \\underline{X}]}{\\partial ^2} &=& 2 > 0 \\rightarrow \\mbox{ loss minimized}\\\\\n\\end{eqnarray*}\\]Example 2.8  Let \\(\\theta\\) denote average number defects per 100 feet tape. \\(\\theta\\) unknown, prior \\(\\theta\\) gamma distribution \\(E[\\theta] = \\alpha / \\beta = 2/10, \\alpha= 2, \\beta = 10\\). 1200 foot roll tape inspected, exactly 4 defects found.Bayes’ estimate average number defects per 100 feet?\\[\\begin{eqnarray*}\n\\mbox{Prior:     }&&\\\\\n\\xi(\\theta) &=& \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta \\theta} = \\frac{10^2}{\\Gamma(2)} \\theta e^{-10\\theta}\\\\\n\\mbox{Likelihood:     }&&\\\\\nf(\\underline{x} | \\theta) &=& \\prod_{=1}^n \\frac{e^{-\\theta} \\theta^{x_i}}{x_i!} = \\frac{e^{-n\\theta} \\theta^{\\sum x_i}}{\\prod (x_i !)}\\\\\n\\mbox{Posterior:     }&&\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\frac{ \\theta e^{-10\\theta} e^{-n \\theta} \\theta^{\\sum x_i}}{\\Gamma(2) 10^2 \\prod (x_i !)}\\\\\n&\\propto& e^{-\\theta(n+10)} \\theta ^{\\sum x_i + 1} \\\\\n\\\\\n\\theta | \\underline{x} &\\sim& \\mbox{Gamma } (\\sum x_i + 2 = 6, n + 10 = 22)\\\\\n\\\\\n\\delta^*(\\underline{X}) &=& \\frac{ \\sum X_i + 2}{n+10}\\\\\n\\delta^*(\\underline{x}) &=& \\frac{6}{22} = \\frac{3}{11}\n\\end{eqnarray*}\\]\nNote: Gamma distribution parameterized slightly differently DeGroot sheet (exponential). Make sure expected value matches ’ve given problem.","code":""},{"path":"bayes.html","id":"absolute-loss","chapter":"2 Bayesian Estimation","heading":"Absolute Loss","text":"minimize \\(E[ | \\theta - | | \\underline{X}]\\) ? \\(\\rightarrow \\ \\ \\ \\delta^*(\\underline{X}) = \\mbox{median} (\\theta | \\underline{X})\\) (see Theorem 4.5.1 DeGroot Schervish (2011)). However, isn’t always obvious compute median non-symmetric distributions. symmetric distributions median = mean.","code":""},{"path":"bayes.html","id":"evaluating-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.7 Evaluating Bayes Estimators","text":"","code":""},{"path":"bayes.html","id":"mean-squared-error","chapter":"2 Bayesian Estimation","heading":"2.7.1 Mean Squared Error","text":"get MSE, let’s talk Agresti-Coull estimate Binomial parameter probability success. estimator attenuates sample proportion closer 0.5 effect reducing variability estimator. Indeed, evidence “add two successes two failures” approach create precise confidence intervals.2Even though agreement better estimate, ’d like show biased.\\[\\begin{eqnarray*}\n\\tilde{p} &=& \\frac{X+2}{n+4}\\\\\nE[\\tilde{p}] &=& \\frac{n\\theta+2}{n+4}\\\\\nbias(\\tilde{p}) &=& \\frac{n\\theta+2}{n+4} - \\theta\\\\\n&=& \\frac{n\\theta + 2 - n\\theta - 4\\theta}{n+4}\\\\\n&=& \\frac{2-4\\theta}{n+4}\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"mse-in-general-frequentist","chapter":"2 Bayesian Estimation","heading":"MSE in general (“Frequentist”)","text":"Mean Squared Error (MSE) expected squared difference parameter (\\(\\theta\\)) estimate (\\(\\hat{\\theta}\\)), , typically, \\(\\hat{\\theta}\\) function data.frequentist MSE based expected values taken likelihood pdf (\\(X | \\theta\\)).Note MSE can written sum bias squared variance:\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_F(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_F(\\hat{\\theta}) &=& E [ (\\hat{\\theta} - \\theta)^2 ]\\\\\n&=& E [ ( \\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2 ] \\mbox{ RV X!} \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2 + 2(\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2]\\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + 0 + E[ (E(\\hat{\\theta}) - \\theta)^2 ] \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + (E(\\hat{\\theta}) - \\theta)^2  \\\\\n&=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\n\\end{eqnarray*}\\]Note taking expected values, resulting MSE function \\(\\theta\\) (function data).example, consider Bernoulli situation (e.g., Basketball shooter):\n\\[\\begin{eqnarray*}\n\\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\nVar(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\nbias(\\hat{\\theta}) &=& E(\\hat{\\theta}) - \\theta = 0\\\\\nMSE_F(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"bayesian-mse","chapter":"2 Bayesian Estimation","heading":"Bayesian MSE","text":"Bayesians, MSE simply expected squared error loss conditional data (, expected value taken posterior, \\(\\theta | \\underline{X}\\).)Bayesian MSE based expected values taken posterior pdf \\((\\theta | \\underline{X}).\\)let \\(\\delta = \\delta(\\underline{X}) = E(\\theta | \\underline{X})\\) estimator, MSE :\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_B(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_B(\\delta(\\underline{X})) &=& E [ (\\delta - \\theta)^2 | \\underline{X}]\\\\\n&=& E [ ( E(\\theta | \\underline{X}) - \\theta)^2 | \\underline{X}] \\mbox{RV } \\theta \\mbox{!!}\\\\\n&=& \\mbox{var}(\\theta | \\underline{X})\\\\\n\\end{eqnarray*}\\]Continuing example, note \\(\\theta \\sim Beta(,b)\\), means \\(\\theta | \\underline{X} \\sim Beta(X + , n - X + b)\\).\\[\\begin{eqnarray*}\n\\delta(X) &=& \\frac{X + }{n ++b}\\\\\nMSE_B(\\delta(X)) &=& \\frac{(X+)(n++b)}{(n++b)^2(n++b+1)}\\\\\n\\end{eqnarray*}\\]Note Bayesian MSE simply posterior variance parameter interest. ’ve used expected value estimate, bias. Note Bayesian MSE function data (\\(\\theta\\)), compare Bayesian MSE Frequentist MSE directly.Example 2.9  Recall tape example, Example 2.8.Prior: Gamma(2, 10) ((2, 1/10) depending parametrize))Data likelihood: Poisson(\\(\\theta\\))Posterior: Gamma (\\(\\sum X_i + 2\\), \\(n\\) + 10)Note calculations done Frequentist MSE conditional data.\\[\\begin{eqnarray*}\n(\\mbox{frequentist estimator}) \\ \\ \\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\n(\\mbox{Bayesian estimator}) \\ \\ \\delta(\\underline{X}) &=& \\frac{\\sum X_i+2}{n+10}\\\\\n& \\\\\nMSE_F(\\hat{\\theta}) &=& var(\\hat{\\theta}) + bias(\\hat{\\theta})^2\\\\\n&=& \\theta/n + 0 = \\theta/n\\\\\nMSE_B(\\delta(\\underline{X})) &= & var(\\theta | \\underline{X})\\\\\n&=& \\frac{\\sum X_i+2}{(n+10)^2}\\\\\n&&\\\\\nMSE_F(\\delta(\\underline{X})) &=& var(\\delta(\\underline{X})) + bias(\\delta(\\underline{X}))^2\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nbias(\\delta(\\underline{X})) &=& E\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg] - \\theta\\\\\n&=& \\frac{n \\theta +2}{n+10} - \\theta = \\frac{n\\theta + 2 - n\\theta -10\\theta}{n+10}\\\\\n&=& \\frac{2-n\\theta}{n+10}\\\\\nvar(\\delta(\\underline{X})) &=& var\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg]\\\\\n&=& \\frac{1}{(n+10)^2}var\\bigg(\\sum X_i\\bigg)\\\\\n&=& \\frac{1}{(n+10)^2} n \\  var(X_i)\\\\\n&=& \\frac{n}{(n+10)^2} \\theta\\\\\n\\end{eqnarray*}\\]\\[\\begin{align*}\nMSE_F(\\delta(\\underline{X})) &= \\frac{n}{(n+10)^2} \\theta + \\frac{(2-n\\theta)^2}{(n+10)^2}\\\\\n&= \\frac{n \\theta+ (2-n\\theta)^2}{(n+10)^2}\n\\end{align*}\\]\nNote couldn’t directly compare \\(MSE_F\\) \\(MSE_B\\) (functions different variables!). ’d come prior think \\(MSE_B(\\hat{\\theta})\\), seems like can’t calculate quantity. Instead, take easier route, find \\(MSE_F(\\delta(\\underline{X}))\\) order reasonable comparison estimators.","code":""},{"path":"bayes.html","id":"sensitivity-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.2 Sensitivity of Estimators","text":"sensitive results different priors?Example 2.10  Continuing tape example, Example 2.8, different values estimate theta depending different priors data values:Note: \\(E[ \\theta | \\underline{x} ] = \\frac{\\sum x_i + \\alpha}{ n + \\beta} = w_1 \\frac{\\sum x_i}{n} + w_2 \\frac{\\alpha}{\\beta}\\), \\(w_1 = \\frac{n}{n+\\beta}, w_2 = \\frac{\\beta}{n+\\beta}\\).\\\n\\(n \\rightarrow \\infty, \\hat{\\theta} \\rightarrow \\frac{\\sum x_i}{n}\\), \\(n \\rightarrow 0, \\hat{\\theta} \\rightarrow \\frac{\\alpha}{\\beta}\\).","code":""},{"path":"bayes.html","id":"consistency-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.3 Consistency of Estimators","text":"consistent estimator \\(\\theta\\) one converges probability \\(\\theta\\). Many Bayes estimators consistent. fact, fairly general regularity conditions, wide class Bayes estimators consistent.Note, estimator \\(Y_n\\) converges \\(\\theta\\) probability :\n\\[\\begin{eqnarray*}\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | < \\epsilon ] &=& 1 \\ \\ \\ \\ \\ \\mbox{pg 233}\\\\\n\\mbox{, equivalently}\\\\\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | \\geq \\epsilon ] &=& 0\\\\\n\\end{eqnarray*}\\](saw idea weak strong laws large numbers: \\(\\overline{X} \\stackrel{P}{\\rightarrow} \\mu\\) \\(n \\rightarrow \\infty\\) Weak Law Large Numbers.) [n.b. case curious, strong law large numbers says \\(\\overline{X} \\stackrel{.s.}{\\rightarrow} \\mu\\) (almost surely). means \\(\\lim_{n \\rightarrow \\infty} P [ \\overline{X} = \\mu ] = 1\\)Example 2.11  Continuing tape example, Example 2.8:\n\\[\\begin{eqnarray*}\n\\delta^*(\\underline{X}) &=& \\frac{\\sum X_i + \\alpha}{n+\\beta}\\\\\n\\overline{X} &\\stackrel{P}{\\rightarrow}& \\theta \\mbox{ (WLLN)}\\\\\n\\mbox{} && \\\\\n\\delta^*(\\underline{X}) - \\overline{X} &=& \\frac{- \\beta}{n+\\beta} \\overline{X} + \\frac{\\beta}{n+\\beta} \\frac{\\alpha}{\\beta} \\stackrel{P}{\\rightarrow} 0 \\mbox{ (Slutsky's theorem)}\\\\\n&& \\\\\n\\delta^*(\\underline{X}) &\\stackrel{P}{\\rightarrow} \\theta\\\\\n\\end{eqnarray*}\\]\\(\\delta^*(\\underline{X})\\) consistent estimator \\(\\theta\\).","code":""},{"path":"bayes.html","id":"benefits-and-limitations-of-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.8 Benefits and Limitations of Bayes’ Estimators","text":"","code":""},{"path":"bayes.html","id":"benefits","chapter":"2 Bayesian Estimation","heading":"2.8.1 Benefits","text":"can incorporate informationthe interpretation intuitive","code":""},{"path":"bayes.html","id":"limitations","chapter":"2 Bayesian Estimation","heading":"2.8.2 Limitations","text":"need prior informationit can difficult produce prior two parameters simultaneously (e.g., normal, gamma)need agree prior information","code":""},{"path":"bayes.html","id":"additional-examples","chapter":"2 Bayesian Estimation","heading":"2.9 Additional Examples","text":"Example 2.12  Suppose Beta(4,4) prior distribution probability \\(\\theta\\) coin yield head spun specified manner. coin independently spun ten times, heads appears fewer 3 times. told many heads seen, number less 3. Calculate exact posterior density \\(\\theta\\).3Example 2.13  Baseball Bayes4You statistician employed Ball Consulting. Veteran major-league baseball scout Rocky Chew seeks advice regarding estimating probability amateur baseball player John Spurrier get base hit major-league pitcher. Rocky arranged Spurrier ten bats major-league pitcher.traditional batting average, \\(\\hat{\\theta}_f = X/n\\) frequentist estimator makes use observed data, ignores prior information might exist.5 assume bats independent Bernoulli trials constant probability getting base hit, \\[\\begin{eqnarray*}\nX \\sim Bin( n=\\mbox{number bat}, \\theta=\\mbox{P(getting base hit)})\n\\end{eqnarray*}\\]\\(\\hat{\\theta}_f\\), good estimator unknown probability (getting base hit), ignores information might baseball. following prior information:John Spurrier appears good great player. one better batters somewhat -average American Legion (high school) baseball team.major-league scouts watched play believe Spurrier’s batting ability professional level.barely adequate major-league hitter batting average 0.200.good major-league batter batting average 0.300.Ty Cobb -time best major-league batting average 0.366.’re going use Beta prior incorporate previous knowledge. prior look like?John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).Example 2.14  Kidney Cancer rates6 example, ’re going use Bayes theory adjust kidney cancer rates less variability. First, ’d like investigate counties highest lowest kidney cancer death rates US (white men, 1980-1989).patterns see figures 2.3 & 2.4? Can give plausible reasons patterns see?county 100 people? Small counties variable. Keep mind rates age-adjusted.\nFigure 2.1: Figure 2.3 Teaching Statistics, bag tricks Gelman Nolan.\nConsider figure 13.4, highest 10% Bayes-estimated kidney cancer death rates US (white men, 1980-1989). Let’s assume number deaths distributed Poisson(\\(n_j \\theta_j\\)) \\(n_j\\) number people county, \\(\\theta_j\\) true kidney cancer death rate county. , assume outside influences kidney cancer (e.g., pollution) county’s cancer rate comes Gamma distribution parameters (\\(\\alpha = 61, \\beta = 47000\\)). ,\\[\\begin{eqnarray*}\n\\mbox{Likelihood:} && y_j \\sim \\mbox{ Poisson}(n_j \\theta_j) \\ \\ \\ n_j = \\mbox{ county population}\\\\\n\\mbox{Prior:} && \\theta_j \\sim \\mbox{ Gamma}(\\alpha=61, \\beta = 47000)\\\\\n&& E[\\theta_j] = \\alpha / \\beta = 1.296 \\times 10^{-3} \\ \\ \\ \\ (\\mbox{10 yr cancer rate})\\\\\n\\end{eqnarray*}\\]\nknow \\(E[\\theta | y] = \\frac{\\alpha + y}{m + \\beta}\\). estimate compare frequentist estimate, \\(\\hat{\\theta} = \\frac{y}{m}\\)?investigate relationship Bayes estimate versus frequentist estimate, ’re going simulate kidney cancer death rates variety counties.Everyone gets county (population). county, ’ll generate true, underlying, kidney cancer rate \\(\\theta_j\\). (Note, \\(\\theta_j\\) sampled (simulated) Gamma(\\(\\alpha=61, \\beta=47,000\\)) distribution.)Using cancer rate (\\(\\theta_j\\)) county’s population (\\(n_j\\)), simulate value number people county died kidney cancer last 10 years (Poisson(\\(n_j \\theta_j\\))).Report frequentist estimate kidney cancer rate county (erase / hide true cancer rate). Leave county name, population, number deaths, estimated rate.public officials left task guessing counties highest cancer rates… think?Calculate Bayes estimate cancer rate. Compare underlying (\\(\\theta_j\\)), observed / frequentist (\\(\\frac{y_j}{n_j}\\)), posterior / Bayesian (\\(\\hat{\\theta}_j | y_j\\)) kidney cancer death rates.\nFigure 2.2: Figure 2.4 Teaching Statistics, bag tricks Gelman Nolan.\n","code":""},{"path":"bayes.html","id":"the-experiment","chapter":"2 Bayesian Estimation","heading":"The Experiment","text":"John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).","code":""},{"path":"bayes.html","id":"reflection-questions-1","chapter":"2 Bayesian Estimation","heading":"2.10  Reflection Questions","text":"prior distribution? random variable described prior?likelihood? random variable described likelihood?posterior distribution? random variable described posterior?conjugate prior? benefit conjugate prior?hope lost prior conjugate? , approach problem coming posterior?","code":""},{"path":"bayes.html","id":"ethics-considerations-1","chapter":"2 Bayesian Estimation","heading":"2.11  Ethics Considerations","text":"make sense incorporate prior information? doesn’t make sense incorporate prior information?legitimate ways calculate prior? illegitimate ways calculate prior?analyst able come prior likelihood?","code":""},{"path":"bayes.html","id":"r-code-bayesian-example","chapter":"2 Bayesian Estimation","heading":"2.12 R code: Bayesian Example","text":"Example 2.15  Recall Baseball Bayes example, Example 2.13.functions allow us label plot parameter information.trying variety different values \\(\\) \\(b\\), prior distributions can visualized.prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\nlibrary(tidyverse)\nlibrary(glue)\n\nex <- function(a,b) {round(a / (a+b), 2)}\nsdx <- function(a,b) {round(sqrt(a*b/((a+b)^2 * (a+b+1))),3)}\n\nbeta_legend <- function(a,b) {\n  glue::glue('a = {a}, ',\n             'b = {b}, ',\n             'EX = {ex(a,b)}, ',\n             'SDX = {sdx(a,b)}')}\n\n# see it in action:\nggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"bayes.html","id":"priors","chapter":"2 Bayesian Estimation","heading":"Priors:","text":"trying variety different values \\(\\) \\(b\\), prior distributions can visualized.","code":"\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")"},{"path":"bayes.html","id":"mse","chapter":"2 Bayesian Estimation","heading":"MSE:","text":"prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"MLE.html","id":"MLE","chapter":"3 Maximum Likelihood Estimation","heading":"3 Maximum Likelihood Estimation","text":"Maximum likelihood estimation method choosing estimators parameters avoids using prior distributions loss functions. MLE chooses \\(\\hat{\\theta}\\) estimate \\(\\theta\\) maximizes likelihood function (easily widely used estimation method statistics).Let’s say \\(X \\sim U[0, \\theta].\\) collect \\(X=47.\\) ever pick \\(\\theta=10?\\) ! \\(10 \\notin \\Omega!\\)Let’s say \\(X \\sim\\) Bin(\\(\\theta,\\)n=4). 4 independent trials, X=1, chose \\(\\theta=0.99?\\) \\(0.99 \\\\Omega???\\) choose \\(\\theta = 0.25.\\) ? maximized likelihood.\\[\\begin{eqnarray*}\nP(X=1 | \\theta = 0.25) &=& 0.422\\\\\nP(X=1 | \\theta = 0.5) &=& 0.25\\\\\nP(X=1 | \\theta = 0.05)  &=& 0.171\\\\\nP(X=1 | \\theta = 0.15) &=& 0.368\\\\\n\\end{eqnarray*}\\]\nmaximized probability seeing data!Example 3.1  Let’s say houses block electricity connected way works house #47 neighbor’s electricity works (sequence). Sometimes house’s electricity fail (w/prob p). many houses can expect provide electricity? best guess p?Data: let’s say \\(n\\) neighborhoods information house lost electricity. [distribution \\(X?\\) \\(X \\sim geometric(p),\\) means \\(E[X] = 1/p.]\\)\\[\\begin{eqnarray*}\nf(x_i | p) &=& p (1-p)^{x_i -1}\\\\\nf(\\underline{x} | p) &=& \\prod_{=1}^n p (1-p)^{x_i -1}\\\\\n&=& p^n (1-p)^{\\sum x_i -n}\\\\\n&& \\mbox{want maximize wrt } p\n\\end{eqnarray*}\\]Often, log-likelihood easier maximize likelihood. define log likelihood \n\\[\\begin{eqnarray*}\nL(\\theta) = \\ln f(\\underline{X} | \\theta).\n\\end{eqnarray*}\\]example,\n\\[\\begin{eqnarray*}\nL(p) &=& \\ln f(\\underline{x} | p) = n \\ln(p) + (\\sum x_i -n) \\ln(1-p)\\\\\n\\frac{\\partial L(p)}{\\partial p} &=& \\frac{n}{p} + \\frac{(\\sum x_i -n)(-1)}{(1-p)} = 0\\\\\n\\hat{p} &=& \\frac{1}{\\overline{X}}\n\\end{eqnarray*}\\]\\(\\overline{x} = 10\\), \\(\\hat{p} = 1/10\\) (1 failure 10 homes). E[X] = 1/p. (let \\(\\theta = E[X] -1\\), good estimate expected (average) number homes can provide electricity \\(1/\\hat{p} -1 = 10 - 1\\). turns MLE invariance property says function MLE MLE function parameter.)Note ’ve found maximum:\n\\[\\begin{eqnarray*}\n\\frac{\\partial^2 L(p)}{\\partial p^2} &=& \\frac{2np -n -p^2\\sum x_i}{p^2 (1-p)^2}\\\\\n&\\leq& \\frac{2p n - n -p^2n}{p^2 (1-p)^2}\\\\\n(\\mbox{} n &\\leq& \\sum x_i)\\\\\n&=& \\frac{n ( -2p + 1 +p^2}{p^2 (1-p)^2}\\\\\n&=& \\frac{-n}{p^2} < 0\\\\\n\\end{eqnarray*}\\]Definition 3.1  Maximum Likelihood Estimator (Def 7.5.2).\npossible observed vector \\(\\underline{x}\\), let \\(\\delta(\\underline{x}) \\\\Omega\\) denote value \\(\\theta \\\\Omega\\) likelihood function, \\(f(\\underline{x}|\\theta)\\) maximum, let \\(\\hat{\\theta} = \\delta(\\underline{X})\\) estimator \\(\\theta\\) defined way. estimator \\(\\hat{\\theta}\\) called maximum likelihood estimator \\(\\theta.\\) \\(\\underline{X} = \\underline{x}\\) observed, value \\(\\delta(\\underline{x})\\) called maximum likelihood estimate \\(\\theta.\\)","code":""},{"path":"MLE.html","id":"sampling-from-a-normal-distribution","chapter":"3 Maximum Likelihood Estimation","heading":"Sampling from a Normal Distribution","text":"\\[\\begin{eqnarray*}\nX_1, X_2, \\ldots X_n &\\sim& N(\\mu, \\sigma^2)  \\ \\ \\ \\mu \\ \\& \\ \\sigma^2 \\mbox{ fixed unknown}\\\\\nf(x_i | \\mu, \\sigma^2) &=& \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp [ \\frac{-1}{2\\sigma^2}(x_i - \\mu)^2 ]\\\\\nf(\\underline{x} | \\mu, \\sigma^2) &=& \\frac{1}{(2 \\pi \\sigma^2)^{n/2}} \\exp [ \\frac{-1}{2\\sigma^2}\\sum_{=1}^n(x_i - \\mu)^2 ]\\\\\nL(\\mu, \\sigma^2) &=& \\frac{-n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum(x_i - \\mu)^2\\\\\n\\end{eqnarray*}\\]want maximize \\(f(\\underline{x}|\\mu, \\sigma^2)\\) respect \\(\\mu;\\) equivalently, can minimize \\(\\sum (x_i - \\mu)^2.\\)\\[\\begin{eqnarray*}\n\\mbox{Let } Q(\\mu) &=& \\sum(x_i - \\mu)^2\\\\\n\\frac{\\partial Q(\\mu)}{\\partial \\mu} &=& -2 \\sum(x_i - \\mu) = 0\\\\\n-2 \\sum x_i &=& -2 n \\mu\\\\\n\\hat{\\mu} &=& \\overline{x}\n\\end{eqnarray*}\\]MLE \\(\\mu\\) doesn’t depend \\(\\sigma^2\\), know joint MLE, \\(\\hat{\\theta} = (\\mu, \\sigma^2)\\), \\(\\hat{\\theta}\\) \\(\\hat{\\sigma}^2\\) maximizes (\\(\\theta' = (\\overline{x}, \\sigma^2) ):\\)\n\\[\\begin{eqnarray*}\nL(\\theta') &=& \\frac{-n}{2} \\ln (2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum(x_i - \\overline{x})^2\\\\\n\\frac{\\partial L(\\theta')}{\\partial \\sigma^2} &=& \\frac{-n}{2 \\sigma^2} - \\frac{1(-1)}{2 (\\sigma^2)^2} \\sum(x_i - \\overline{x}) = 0\\\\\n\\sum (x_i - \\overline{x})^2 &=& \\frac{n(\\sigma^2)^2}{\\sigma^2}\\\\\n\\hat{\\sigma^2} &=& \\frac{\\sum(x_i - \\overline{x})^2}{n}\\\\\n\\hat{\\theta} &=& (\\overline{x}, \\frac{\\sum(x_i - \\overline{x})^2}{n})\n\\end{eqnarray*}\\]Example 3.2  Non-existence MLESuppose \\(X_1, X_2, \\ldots X_n \\sim f(x | \\theta)\\) :\n\\[\\begin{eqnarray*}\nf(x) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x > \\theta\\\\\n    0 & x \\leq \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]\\(\\theta\\) unknown, \\(-\\infty < \\theta < \\infty\\). MLE exist?\n\\[\\begin{eqnarray*}\nf(\\underline{x}|\\theta) &=& e^{n \\theta - \\sum x_i} \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\nL(\\theta) &=& n \\theta - \\sum x_i \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\n\\frac{\\partial L(\\theta)}{\\partial \\theta} &=& n = 0 ?!?!?!\\\\\n\\end{eqnarray*}\\]want find largest \\(\\theta\\) \\(\\theta < x_i \\ \\ \\forall x_i\\). \\(\\hat{\\theta} = \\min x_i\\)? , \\(\\theta < \\min x_i!\\) , instead,\n\\[\\begin{eqnarray*}\nf(x) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x \\geq \\theta\\\\\n    0 & x < \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]MLE \\(\\theta\\) \\(\\hat{\\theta} = \\min x_i\\).","code":""},{"path":"MLE.html","id":"qualities-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1 Qualities of the MLE","text":"","code":""},{"path":"MLE.html","id":"invariance-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.1 Invariance of the MLE","text":"Theorem 3.1  (D&S Theorem 6.6.1) Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\), let \\(g(\\theta)\\) function \\(\\theta\\). MLE \\(g(\\theta)\\) \\(g(\\hat{\\theta})\\). (Proof: see page 427 DeGroot & Schervish)Proof. Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\). know \\[L(\\hat{\\theta}) = \\max_{\\theta \\\\Omega} L(\\theta).\\]let \\(g\\) one--one function \\(\\psi = g(\\theta) \\\\Gamma\\) (image \\(\\Omega\\) g). can denote inverse function \\(g\\) \\[\\theta = h(\\psi).\\]know MLE \\(\\psi\\) \\(\\hat{\\psi}\\) \\[L(h(\\hat{\\psi})) = \\max_{\\psi \\\\Gamma} L(h(\\psi)).\\]know \\(L(\\theta)\\) maximized \\(\\hat{\\theta}\\). \\(L(h(\\psi))\\) must also maximized \\(h(\\psi) = \\hat{\\theta}\\). \\[\\therefore h(\\hat{\\psi}) = \\hat{\\theta} \\ \\ \\ \\mbox{ } \\ \\ \\ \\hat{\\psi} = g(\\hat{\\theta}).\\]\\(g\\) one--one, however, need redefine mean likelihood. \\(g(\\theta)\\) one--one may many values \\(\\psi\\) satisfy \\[g(\\theta) = \\psi.\\] (example, square function. n.b. \\(g\\) still function, can’t map multiple values.)case, correspondence maximum \\(\\psi\\) maximum \\(\\theta\\) breaks . E.g., \\(\\hat{\\theta}\\) MLE \\(\\theta\\), may another value \\(\\theta\\), say \\(\\theta_0\\) \\(g(\\hat{\\theta}) = g(\\theta_0).\\)define induced log likelihood function: \\[L^*(t) = \\max_{\\theta \\G_t} \\ln f(\\underline{x} | \\theta).\\]Let \\(G\\) image \\(\\Omega\\) g, \\[G_t = \\{ \\theta: g(\\theta) = t\\} \\ \\ \\forall t \\G.\\]define MLE \\(g(\\theta)\\) \\(\\hat{t}\\) \\[L^*(\\hat{t}) = \\max_{t \\G} L^* (t).\\]proof follows (page 427). [Note two maximizations. , first maximization created log likelihood function. second maximization found maximum function parameter space.]Example 3.3  Functions MLEs MLEs also!standard deviation normal:\n\\[\\begin{eqnarray*}\n\\sigma &=& \\sqrt{\\sigma^2}\\\\\n\\hat{\\sigma} &=& \\mbox{MLE}(\\sigma) = \\sqrt{\\frac{\\sum (x_i - \\overline{x})^2}{n}}\n\\end{eqnarray*}\\]mean uniform:\n\\[\\begin{eqnarray*}\nX &\\sim& U [\\theta_1, \\theta_2]\\\\\n\\mu &=& \\frac{\\theta_1 + \\theta_2}{2}\\\\\n\\hat{\\mu} &=& \\mbox{MLE}(\\mu) = \\frac{\\max(x_i) + \\min(x_i)}{2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"consistency-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.2 Consistency of the MLE","text":"certain regularity conditions, MLE \\(\\theta\\) consistent \\(\\theta\\). ,\n\\[\\begin{eqnarray*}\n\\hat{\\theta} \\stackrel{P}{\\rightarrow} \\theta\\\\\n\\stackrel{\\lim}{ n \\rightarrow \\infty} P [ | \\hat{\\theta} - \\theta | > \\epsilon ] = 0\n\\end{eqnarray*}\\]\\(\\hat{\\theta}\\) function \\(X\\).","code":""},{"path":"MLE.html","id":"bias-of-the-mle-section-7.7","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.3 Bias of the MLE (section 7.7)","text":"Previously, discussed briefly concept bias:Definition 3.2  Unbiased (page 428 DeGroot & Schervish): Let \\(\\delta(X_1, X_2, \\ldots, X_n)\\) estimator \\(g(\\theta)\\). say \\(\\delta(X_1, X_2, \\ldots, X_n)\\) unbiased :\n\\[\\begin{eqnarray*}\nE[ \\delta(X_1, X_2, \\ldots, X_n)] = g(\\theta)\n\\end{eqnarray*}\\]Note: bias \\((\\delta(\\underline{X}) ) = E[ \\delta(\\underline{X})] - g(\\theta)\\).Example 3.4  Let \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\). Recall, \\(\\hat{\\sigma^2} = \\frac{\\sum(X_i - \\overline{X})^2}{n}\\) MLE.\n\\[\\begin{eqnarray*}\nE[\\hat{\\sigma^2}] &=& E \\bigg[ \\frac{\\sum(X_i - \\overline{X})^2}{n}\\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu + \\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu)^2 + 2 \\sum (X_i - \\mu)(\\mu - \\overline{X}) + n(\\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n}\\bigg\\{ E\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2 ] \\bigg] \\bigg\\}\\\\\n&=& \\frac{1}{n} \\bigg\\{  \\sum E (X_i - \\mu)^2 -  n E [(\\overline{X} - \\mu)^2 ] \\bigg\\}\\\\\n&=& \\frac{1}{n}\\{ n \\sigma^2 - n \\frac{\\sigma^2}{n}  \\}\\\\\n&=& \\frac{n-1}{n} \\sigma^2 \\Rightarrow \\mbox{Biased!}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"but-consistent-yes.","chapter":"3 Maximum Likelihood Estimation","heading":"But consistent? Yes.","text":"\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-maximum-likelihood-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.4 Benefits and Limitations of Maximum Likelihood Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-1","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"functions MLEs MLEs (invariance MLE)certain regularity conditions, MLEs asymptotically distributed normally.","code":""},{"path":"MLE.html","id":"limitations-1","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"MLE always existMLE always uniqueMLE value parameter \\(\\theta\\) maximizes distribution \\(X|\\theta\\) (likely produced observed data). MLE likely parameter given data: \\(E[ \\theta | X]\\) (’s Bayesian estimator!).","code":""},{"path":"MLE.html","id":"method-of-moments","chapter":"3 Maximum Likelihood Estimation","heading":"3.2 Method of Moments","text":"Method Moments (MOM) another parameter estimation technique. find MOM estimate, set expected moment equal sample moment solve parameter value interest.\\[\\begin{eqnarray*}\nE[X^k] &=& k^{th} \\mbox{ expected moment}\\\\\n\\frac{1}{n} X_i^k &=& k^{th} \\mbox{ sample moment}\\\\\n\\end{eqnarray*}\\]Example 3.5  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\).\n\\[\\begin{eqnarray*}\n\\tilde{\\mu} &=& \\overline{X}\\\\\n\\tilde{\\sigma^2} &=& \\frac{\\sum X_i^2}{n} - \\overline{X}^2\\\\\n&=& \\frac{1}{n} \\sum(X_i - \\overline{X})^2 \\mbox{   MLE!!}\n\\end{eqnarray*}\\]Example 3.6  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim Gamma(\\alpha,\\beta)\\).\n\\[\\begin{eqnarray*}\n\\alpha / \\beta &=& \\overline{X}\\\\\n\\alpha / \\beta^2 &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\alpha &=& \\overline{X} \\beta\\\\\n\\frac{\\overline{X} \\beta}{\\beta^2} &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\tilde{\\beta} &=& \\frac{\\overline{X}}{(\\sum X_i^2 / n - \\overline{X}^2)}\\\\\n&=& \\overline{X} / \\hat{\\sigma^2}\\\\\n\\tilde{\\alpha} &=& \\overline{X}^2 / \\hat{\\sigma^2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-method-of-moments-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.2.1 Benefits and Limitations of Method of Moments Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-2","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"Often easier compute MLEs (e.g., MLE \\(\\alpha\\) Gamma distribution intractable).Estimates MOM can used first approximations likelihood.Can easily estimate multiple parameter families.","code":""},{"path":"MLE.html","id":"limitations-2","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"Can sometimes (usually cases small sample sizes) produce estimates outside parameter space.Doesn’t work moments don’t exist (e.g., Cauchy distribution)MLEs typically closer quantity estimated.Example 3.7  Tank EstimatorsHow can random sample integers 1 N (N unknown researcher) used estimate N?tanks numbered 1 N. Working group, randomly select five tanks, without replacement, bowl. tanks numbered:Think use data estimate N. (Come least 3 estimators.) Come consensus within group done.estimates N :rules formulas estimators N based sample n (case 5) integers :Assuming random variables distributed according discrete uniform:\n\\[\\begin{eqnarray*}\nX_i \\sim P(X=x | N) = \\frac{1}{N} \\ \\ \\ \\ \\ x = 1,2,\\ldots, N \\ \\ \\ \\ =1,2,\\ldots, n\n\\end{eqnarray*}\\]method moments estimator N?maximum likelihood estimator N?estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"mean-squared-error-1","chapter":"3 Maximum Likelihood Estimation","heading":"Mean Squared Error","text":"estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"reflection-questions-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.3  Reflection Questions","text":"prior distribution? random variable described prior?likelihood? random variable described likelihood?posterior distribution? random variable described posterior?conjugate prior? benefit conjugate prior?hope lost prior conjugate? , approach problem coming posterior?","code":""},{"path":"MLE.html","id":"ethics-considerations-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.4  Ethics Considerations","text":"make sense incorporate prior information? doesn’t make sense incorporate prior information?legitimate ways calculate prior? illegitimate ways calculate prior?analyst able come prior likelihood?","code":""},{"path":"MLE.html","id":"r-code-mle-example","chapter":"3 Maximum Likelihood Estimation","heading":"3.5 R code: MLE Example","text":"Example 7.6.5 text looks MLE center Cauchy distribution.\nCauchy distribution interesting tails decay rate \\(1/x^2\\), try take expected value, end integrating something looks like \\(1/x\\) real line.\nHence, expected value exist.\nThus, method moments estimators use.\nMLE still useful, though easy find. stated text, likelihood proportional \n\\[\\prod_{=1}^n [1 + (x_i - \\theta)^2]^{-1}\\]Compute first second derivative log likelihood.Consider trying find root function f(x). Suppose current\nguess value \\(x_0\\). might approximate function tangent line (first order Taylor approximation) x0 take next guess root line. Use Taylor expansion find next guess \\(x_1\\) \\[x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}\\]Continually updating guesses via method known Newton’s Method\nNewton-Raphson Method.Generate 50 observations Cauchy distribution centered \\(\\theta = 10\\). Based 50 observations, use parts () (b) estimate \\(\\theta\\). Remember, ’re trying maximize likelihood, function trying find root derivative log-likelihood. R code might look something like :actual formulas f1 f2 come part (). Everything \ndouble parentheses needs replaced proper R syntax. rest \nrun R. case curious, sum() R function summing vector.Indeed, initial guess far 10, Newton-Raphson procedure won’t converge MLE (first approximation). Consider following figure describes behavior derivative log-likelihood function. Note N-R procedure go infinity negative infinity starts value far 10 (see purple line). initial value close 10, converge MLE (see red line).","code":"x=rt(50,1)+10  # 50 random Cauchy variables centered at 10\nvect.theta = c()  # placeholder\ntheta.guess=((pick a starting value, you might try different ones))  # just a number\ntheta.gues = 10  # what happens when your initial guess is far from 10?\n\nfor (i in 1:50) {  # play around with how many times you loop through.  10 is likely too small.\n\n    f1=((compute first derivative of log-likelihood evaluated at theta.guess))\n    f2=((compute second derivative at theta.guess))\n\n    f1= 2*sum((x-theta.guess)/(1+(x-theta.guess)^2))\n    f2= 2*sum(((-1)/(1+(x-theta.guess)^2))+((2*(x-theta.guess)^2)/((1+(x-theta.guess)^2)^2)))\n\n\n# f1 and f2 need to be written as R functions of theta.guess\n\n    theta.guess=theta.guess - f1/f2\n    print(theta.guess)\n    vect.theta = c(vect.theta, theta.guess) # keeping track of all your guesses\n}\n\nplot(vect.theta)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
