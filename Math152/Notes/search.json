[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 152 Pomona College: Computational Statistics. notes based extensively Probability & Statistics (DeGroot Schervish 2011).responsible reading relevant chapters text. text good & readable, use . make sure coming class also re-visiting warm-ups feedback provided.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"","code":""},{"path":"intro.html","id":"what-is-statistics","chapter":"1 Introduction","heading":"1.1.0.1 What is Statistics?","text":"Generally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims population","code":""},{"path":"intro.html","id":"what-is-the-content-of-math-152","chapter":"1 Introduction","heading":"1.1.0.2 What is the content of Math 152?","text":"Statistical Theory introduction statistics students background probability theory, calculus, linear algebra. statistics prerequisite class. course focused theoretical aspects material, though real world applications class homework assignments. idea strong mathematical understanding concepts also understanding concepts applied real world.completion course, students :able derive methods introductory statistics using tools mathematics (.e., calculus probability).able justify use particular method (technical conditions).able weigh advantages disadvantages different estimation techniques (e.g., bias, variability, resistance outliers).know communicate results effectively.","code":""},{"path":"intro.html","id":"who-should-take-math-152","chapter":"1 Introduction","heading":"1.1.0.3 Who should take Math 152?","text":"Linear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.","code":""},{"path":"intro.html","id":"what-are-the-prerequisites-for-math-152","chapter":"1 Introduction","heading":"1.1.0.4 What are the prerequisites for Math 152?","text":"prerequisites Statistical Theory Probability (Math 151 equivalent) completion three semester sequence calculus linear algebra. rely heavily prerequisites, students background probability multivariable calculus find trying catch throughout semester. familiar topics conditional probabilities expectations, Central Limit Theorem, moment generating functions, probability density functions.","code":""},{"path":"intro.html","id":"is-there-overlap-with-other-classes","chapter":"1 Introduction","heading":"1.1.0.5 Is there overlap with other classes?","text":"Statistical Theory covers many topics Introductory Statistics (e.g., Math 58), however, treatment topics completely different.\nStatistical Theory uses mathematical tools (e.g., probability theory calculus) derive topics prove , many ways, optimal.","code":""},{"path":"intro.html","id":"when-should-i-take-math-152","chapter":"1 Introduction","heading":"1.1.0.6 When should I take Math 152?","text":"prerequisite structure, students Math 152 juniors seniors.","code":""},{"path":"intro.html","id":"what-is-the-workload-for-math-152","chapter":"1 Introduction","heading":"1.1.0.7 What is the workload for Math 152?","text":"one homework assignment per week, two -class midterm exams, two take-home midterm exams, final exam. Many students report working 8-10 hours per week outside class.##software use? real world applications? mathematics? CS?majority work Statistical Theory done pencil (/ pen / LaTeX).\nHowever, use R way investigating theorecial concepts computationally.\nR work done using RStudio IDE.\nneed either download R RStudio (free) onto computer use Pomona’s server.\nreal-world applications motivate theory, cours applied focus data analysis.can use R Pomona server: https://rstudio.pomona.edu/ (Pomona students able log immediately.\nNon-Pomona students need get Pomona login information.)Alternatively, feel free download R onto computer.\nR freely available http://www.r-project.org/ already installed college computers.\nAdditionally, required install RStudio turn R assignments using RMarkdown. http://rstudio.org/.\n(can use LaTeX compiler : https://yihui.name/tinytex/)\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Although Statistical Theory statistics prerequisite, students seen many building blocks high school classes Probability Theory.\nreview vocabulary covered previous contexts.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"","code":""},{"path":"intro.html","id":"probability","chapter":"1 Introduction","heading":"Probability","text":"probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).experiment activity situation uncertainty outcome.experiment activity situation uncertainty outcome.sample space list possible outcomes random trial. (Called S.)sample space list possible outcomes random trial. (Called S.)event potential subset sample space.event potential subset sample space.set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).simple event event consisting exactly one outcome.simple event event consisting exactly one outcome.Two events mutually exclusive disjoint occur simultaneously.Two events mutually exclusive disjoint occur simultaneously.Two events independent occurrence one change probability second occur.Two events independent occurrence one change probability second occur.random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]","code":""},{"path":"intro.html","id":"statistics","chapter":"1 Introduction","heading":"Statistics","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]\\(\\Omega\\) parameter space, set values contains possible realizations parameter.many statistical problems, assume distribution generated data completely known except parameters. goal statistics make statements (inference) population using sample (data). course broken two main parts: parameter estimation tests hypotheses.","code":""},{"path":"intro.html","id":"estimation","chapter":"1 Introduction","heading":"1.2.2 Estimation","text":"many statistical problems, assume distribution generated data completely known except parameter(s). first half course, ’re going learn different ways estimate parameters, properties estimators, makes good estimator.","code":""},{"path":"intro.html","id":"example","chapter":"1 Introduction","heading":"example","text":"Consider trying estimate true average weight dozen eggs particular farm. add random 12 values pdf different multiplying one observation 12. Clearly 12*one egg much variable weight dozen eggs.\n\\[\\begin{align*}\n\\mbox{statistic } 1 &= 12 X\\\\\n\\mbox{statistic } 2 &= \\sum_{=1}^{12} X_i\\\\\n\\Omega &= \\{ \\mu: \\mu \\\\!\\!R+\\}\\\\\n\\end{align*}\\]","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3  Reflection Questions","text":"difference sample population?difference discrete random variable continuous random variable?difference pdf cdf?can find expected value discrete random variable? can find expected value continuous random variable?difference R RStudio?","code":""},{"path":"intro.html","id":"ethics-considerations","chapter":"1 Introduction","heading":"1.4  Ethics Considerations","text":"important distinguish sample population?ever okay model discrete random variable using continuous probability model? , ? , ?better understand method’s properties theoretically better use computational tools investigate method’s properties? [question probably won’t able answer end semester.]","code":""},{"path":"intro.html","id":"r-code-reproduciblity","chapter":"1 Introduction","heading":"1.5 R code: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.5.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.5.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.5.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"bayes.html","id":"bayes","chapter":"2 Bayesian Estimation","heading":"2 Bayesian Estimation","text":"","code":""},{"path":"bayes.html","id":"bayes-rule","chapter":"2 Bayesian Estimation","heading":"2.1 Bayes’ Rule","text":"Theorem 2.1  Bayes’ RuleGiven events \\(\\) \\(B\\),\n\\[\\begin{eqnarray*}\nP(|B) &=& \\frac{P(AB)}{P(B)} = \\frac{P(B|) P()}{P(AB) + P(^cB)} \\nonumber \\\\\n&=& \\frac{P(B|)P()}{ \\sum_i P(B|A_i) P(A_i)}\n\\end{eqnarray*}\\]Many following examples may familiar . reading , work understand intuition (denominator changes condition!) well mathematical connection Bayes’ Rule.Example 2.1  Suppose rate infection TB 1 1000 (0.1 percent = 0.001). Suppose TB test used 90% accurate: gives positive result 10 percent people actually TB, reaction skin test. Also, 10% people actually TB fail react test.1What’s chance someone TB test positive?’s chance randomly chosen person tests negative actually TB?another TB test gives fewer false positives, expensive. better use one?prior probability TB?posterior probability TB (given positive test)?Solution:can use table figure probabilities. Consider population 10,000 people:Alternatively, can use probability statements easier work long-run scenarios get complicated. know following:\\[\\begin{eqnarray*}\nP(TB + ) &=& 0.001\\\\\nP(Test + | TB - ) &=& 0.1 \\\\\nP(Test - | TB + ) &=& 0.1\n\\end{eqnarray*}\\]10 10,000 people disease. 9 10 actually test positive TB. However, 999 9990 people false positives. , \\(9/(999+9) = 0.0089 \\mbox{ } \\approx\\) 0.9% people test positive actually TB.\\[\\begin{eqnarray*}\nP( TB + | Test + ) &=& \\frac{P(TB + \\& Test + )}{P(Test + )} = \\frac{P(Test  + | TB + ) P(TB+)}{P(Test + )}\\\\\n&=& \\frac{P(Test  + | TB + ) P(TB+)}{P(Test  + | TB - ) P(TB-) + P(Test  + | TB + ) P(TB+)} \\\\\n&=& \\frac{0.9 \\cdot 0.001}{ 0.1 \\cdot 0.999 + 0.9 \\cdot 0.001}\\\\\n&=& 0.0089\n\\end{eqnarray*}\\]1 10,000 people false negative (0.0001 = 0.01% opposed 9.99% false positives).\\[\\begin{eqnarray*}\nP( TB + | Test - ) &=& \\frac{P(TB + \\& Test - )}{P(Test - )} = \\frac{P(Test  - | TB + ) P(TB+)}{P(Test - )}\\\\\n&=& \\frac{P(Test - | TB + ) P(TB+)}{P(Test  - | TB - ) P(TB-) + P(Test  - | TB + ) P(TB+)} \\\\\n&=& \\frac{0.1 \\cdot 0.001}{ 0.9 \\cdot 0.999 + 0.1 \\cdot 0.001}\\\\\n&=& 0.00011121\n\\end{eqnarray*}\\]necessarily, since ’s much worse false negative false positive. People test positive given another test fewer false positives.0.0010.009Example 2.2  cab involved hit run accident night. Two cab companies, Green Blue, operate city. Suppose told following:85 percent cabs city Green, remaining 15 percent Blue.witness identified cab Blue (dark!) court tested reliability witness circumstances existed night accident, determined witness correctly identified cab color 80% time, made mistake 20% time, regardless actual color cab.’s verdict? .e., probability cab involved hit--run actually Blue?Solution:information given, following probabilities known:\n\\[\\begin{eqnarray*}\nP(said B | B) &=& 0.8\\\\\nP(said G | B) &=& 0.2\\\\\nP(B) &=& 0.15 \\ \\ \\ \\mbox{prior probability!}\n\\end{eqnarray*}\\]probability interest :\n\\[\\begin{eqnarray*}\nP(B | said B) &=& \\frac{P(said B | B) P(B)}{P(said B | G) P(G) + P(said B | B) P(B)}\\\\\n&=& \\frac{0.8*0.15}{0.2*0.85 + 0.8*0.15} = 0.41\n\\end{eqnarray*}\\]Example 2.3  Consider famous Monte Hall problem based game show, Let’s Make Deal. part show, contestant asked pick one three doors. Two doors nothing behind , third door car prize. Monte Hall (host) opens non-prize door contestant hadn’t chosen (always door available open one prize). Monte offers contestant opportunity switch original door remaining door. switch? Stay? doesn’t matter? probability winning situations?Define following:\n\\[\\begin{eqnarray*}\nC_i &:& \\mbox{car behind Door $$, } \\\\{ 1, 2, 3\\}\\\\\nH_{ij} &:& \\mbox{host opens Door $j$ player picked Door $$, } , j \\\\{ 1,2,3\\}\n\\end{eqnarray*}\\]example, \\(C_2\\) situation car behind Door 2, \\(H_{23}\\) denotes situation host opened Door 3 chose Door 2. Note \\(H_{ij}\\) information (’ll condition ). Let’s say start picking Door 2, host opens Door 3.\\[\\begin{eqnarray*}\nP(C_i) &=& \\frac{1}{3}\\\\\nP(C_2) &=& \\frac{1}{3}\\\\\nP(C_2 | H_{23}) &=& \\frac{P(H_{23} | C_2) P(C_2)}{P(H_{23})} \\ \\ \\ \\ \\ \\mbox{(Bayes rule, right !  See } P(H_{23}) \\mbox{ )}\\\\\n&=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{P(H_{23})} \\\\\n\\nonumber \\\\\nP(H_{23}) &=&  P(H_{23} C_1) + P(H_{23} C_2) + P(H_{23} C_3)\\\\\n&=&  P(H_{23} | C_1) P(C_1) + P(H_{23} | C_2) P(C_2) + P(H_{23} | C_3) P(C_3)\\\\\n&=& \\frac{1}{2} \\cdot \\frac{1}{3} + 1 \\frac{1}{3} + 0 \\frac{1}{3} = \\frac{1}{2}\\\\\n\\nonumber\\\\\nP(C_2 | H_{23}) &=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}}\\\\\n&=& \\frac{1}{3}\\\\\n\\end{eqnarray*}\\]Using know rules game (, car behind door 3, ’d never open !), know:\n\\[\\begin{eqnarray*}\nP(C_3 | H_{23}) = 0\n\\end{eqnarray*}\\]car behind one three doors:\n\\[\\begin{eqnarray*}\n1 &=& P(C_1 | H_{23}) + P(C_2 | H_{23}) + P(C_3 | H_{23})\\\\\nP(C_1 | H_{23} ) &=& 1 - P(C_2 | H_{23}) - P(C_3 | H_{23})\\\\\n&=& 1 - \\frac{1}{3} - 0\\\\\n&=& \\frac{2}{3}\n\\end{eqnarray*}\\]…. probability car behind Door 1 2/3 probability ’s behind Door 2 1/3 \\(\\rightarrow\\) switch doors!previous situations based discrete values parameter data (neither typically true). think {} {}, want find value {} \\(P(|B)\\) maximized. going use probability distribution functions (pdfs) instead discrete probabilities, need notation.","code":""},{"path":"bayes.html","id":"prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.2 Prior Distributions","text":"prior distribution distribution parameter (e.g., \\(\\theta\\)) observing data. Note:observations come \\(f(x|\\theta), \\theta \\\\Omega\\)can express likely \\(\\theta\\) various regions \\(\\Omega\\) terms probability distribution \\(\\theta\\).Bayesians believe use prior distributions modeling always know something situation hand.Frequentists believe use data collected experiment sample (prior information).Example 2.4  want predict high temperature given day October.\n\\[\\begin{align*}\n\\Omega = \\{ (\\theta, \\sigma^2) &: \\theta \\\\!\\!R, \\sigma^2 \\\\!\\!R^+\\}\\\\\n\\mbox{} &: -\\infty < \\theta < \\infty, \\sigma^2 > 0 \\}\\\\\n\\mbox{} &: \\theta > 30, 0 < \\sigma^2 < 625 \\}\\\\\n\\end{align*}\\]\nSuppose know variance \\(\\sigma^2 = 12^2\\). mean, \\(\\theta\\) unknown. might specify prior distribution \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\xi (\\theta) &\\rightarrow& \\theta \\sim N(\\mu, \\nu^2)\\\\\n&& \\mu =78^\\circ, \\nu^2 = (2.5^\\circ)^2\\\\\n\\end{eqnarray*}\\]value \\(\\nu\\) measure uncertainty prior beliefs. , estimated \\(\\theta\\) \\(78^\\circ\\), aren’t sure value, add uncertainty belief (\\(\\nu\\)). \\(\\mu\\) \\(\\nu\\) called hyper-parameters.","code":""},{"path":"bayes.html","id":"posterior-distributions","chapter":"2 Bayesian Estimation","heading":"2.3 Posterior Distributions","text":"posterior distribution conditional distribution parameter (e.g., \\(\\theta\\)) given observed data.Aside, little probability review:Example 2.5  Suppose interested rolling two dice. Let \\(X\\) larger value; let \\(Y\\) sum two dice. Find joint marginal distributions \\(X\\) \\(Y\\). solution table probabilities:notation review. Suppose \\(n\\) data points \\(f(x| \\theta)\\). ’ll assume ’s simple random sample (SRS), therefore observations independent.\\[\\begin{eqnarray*}\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(x_1| \\theta) f(x_2| \\theta) \\cdots f(x_n| \\theta)\\\\\n\\mbox{ } \\underline{x} &=& \\{ x_1, x_2, \\ldots, x_n \\}\\\\\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(\\underline{x} | \\theta)\n\\end{eqnarray*}\\]Remember, \\(f(\\underline{x} | \\theta)\\) conditional distribution \\(\\underline{x}\\) given \\(\\theta\\). likelihood function, \\(f(\\underline{x} | \\theta)\\), joint pdf observations (representing: likely data?).Define\n\\[\\begin{eqnarray*}\nf(\\underline{x}, \\theta) &=& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\ng_n(\\underline{x}) &=& \\int_\\Omega f(\\underline{x}, \\theta) d\\theta\\\\\n&=& \\int_\\Omega f(\\underline{x} | \\theta) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]Remember, however, interested probability parameter given data:\n\\[\\begin{eqnarray*}\n\\xi(\\theta| \\underline{x}) = \\frac{f(\\underline{x} | \\theta) \\xi(\\theta)}{g_n(\\underline{x})} \\ \\ \\ \\ \\ \\ \\ \\theta \\\\Omega\n\\end{eqnarray*}\\](Bayes’ Theorem!!!)prior, \\(\\xi(\\theta)\\) relative likelihood \\(\\theta\\) data observed.posterior, \\(\\xi(\\theta | \\underline{x})\\) relative likelihood \\(\\theta\\) \\(\\underline{X} = \\underline{x}\\) observed.know posterior function \\(\\theta\\). ’s important keep mind function , posterior function data.\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi(\\theta)\\\\\n\\mbox{posterior} &\\propto& \\mbox{likelihood} \\cdot \\mbox{prior}\n\\end{eqnarray*}\\], posterior proportional product likelihood prior. Note \\(g_n(\\underline{x})\\) depend \\(\\theta\\) part proportionality constant. , can always find \\(g_n(\\underline{x})\\) know posterior integrates 1. (Sometimes \\(g_n\\) extraordinarily difficult find.)\n\\[\\begin{eqnarray*}\n\\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta = 1\n\\end{eqnarray*}\\]Example 2.6  Suppose true proportion free throws Steph Curry able make successfully unknown. assume freethrows distributed according Bernoulli process.\\[\\begin{eqnarray*}\nX = \\left\\{ \\begin{array}{ll}\n    1 & \\mbox{Curry makes shot}\\\\\n    0 & \\mbox{Curry misses shot}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]say,\n\\[\\begin{eqnarray*}\nX &\\sim& \\mbox{Bernoulli}(\\theta)\\\\\nf(\\underline{x}|\\theta)&=&  \\theta ^y (1 - \\theta)^{n-y} \\ \\ \\ \\ y = \\sum_{=1}^n x_i\n\\end{eqnarray*}\\]\nNote: \\(\\underline{x} = \\{x_1, x_2, \\ldots, x_n\\}\\) specific ordering 0s 1s.prior information Curry’s abilities, put uniform prior \\(\\theta\\).\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    1 & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) \\propto \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]functional form posterior (likelihood times prior). ’re trying estimate \\(\\theta\\), distribution \\(\\theta\\) given data? Recall Beta distribution (probability review):\\[\\begin{eqnarray*}\nW &\\sim& \\mbox{Beta}(\\alpha,\\beta)\\\\\nf(w) &=& \\frac{1}{B(\\alpha,\\beta)} w^{\\alpha - 1} (1-w)^{\\beta-1}\\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}w^{\\alpha - 1} (1-w)^{\\beta-1} \\ \\ \\ \\ w \\[0,1]\n\\end{eqnarray*}\\]Using likelihood prior, able find full posterior distribution integration:\n\\[\\begin{eqnarray*}\n1 &=& \\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta \\\\\n&=&  k \\cdot \\int_\\Omega \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta) d\\theta\\\\\n&=& k \\cdot \\int_0^1 \\theta^y (1-\\theta)^{n-y} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(y+1 + n-y + 1)} \\cdot \\int_0^1 \\frac{\\Gamma(y+1 + n-y + 1)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^{y+1-1} (1-\\theta)^{n-y+1-1} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)} \\cdot 1\\\\\nk &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\\\\ng(\\underline{x}) &=& \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)}\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]Note, however, didn’t actually need integrate find \\(g(\\underline{x})\\). Instead, needed note \\(\\theta\\) takes place \\(w\\) Beta distribution, automatically know appropriate constant value.information prior distribution \\(\\theta\\)? Suppose believe Beta(,b). \\(\\Big(\\)Note: \\(E[\\theta] = \\frac{}{+b}\\), \\(\\mbox{Var}(\\theta) = \\frac{ab}{(+b)^2 (+b+1)}\\), \\(\\mbox{SD}(\\theta) = \\sqrt{\\frac{ab}{(+b)^2 (+b+1) }} \\Big).\\)\\[\\begin{eqnarray*}\n\\xi(\\theta) &\\propto& \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y+-1} (1-\\theta)^{n-y+b-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y+, n-y+b)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n++b)}{\\Gamma(y+)\\Gamma(n-y+b)}\\theta^{y+-1} (1-\\theta)^{n-y+b-1}\n\\end{eqnarray*}\\]Note: didn’t need calculate \\(g(\\underline{x})\\)!!!\\(E[\\theta | \\underline{x}] = \\frac{y+}{n++b}\\)Var \\((\\theta | \\underline{x}) = \\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}\\)SD \\((\\theta | \\underline{x}) = \\sqrt{\\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}}\\)Given prior sample size \\(n\\), best guess Curry’s ability hit free throws?kind confidence estimate?","code":""},{"path":"bayes.html","id":"conjugate-prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.4 Conjugate Prior Distributions","text":"conjugate prior distribution one prior distribution family posterior distribution. Beta distribution conjugate Binomial distribution (note, \\(U[0,1]\\) distribution Beta(\\(\\alpha\\)=1,\\(\\beta\\)=1)).Example 2.7  Continuing example temperature, \\(X \\sim N(\\theta, \\sigma^2\\) (known)) normal prior, \\(\\theta \\sim N(\\mu, \\nu^2)\\). Remember typically, prior completely specified. example \\(\\mu=78\\) \\(\\nu=2.5\\). :\n\\[\\begin{eqnarray*}\nf(\\underline{x} | \\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\theta)^2 \\bigg]\\\\\n&\\propto& \\exp \\Bigg[ - \\frac{1}{2 \\sigma^2} \\bigg(n (\\theta-\\overline{x})^2 + \\sum_{=1}^n (x_i - \\overline{x})^2 \\bigg) \\Bigg]\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2\\bigg]\\\\\n&&\\\\\n\\xi(\\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n&&\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2 -\\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n\\mbox{note: } && \\frac{n}{\\sigma^2}(\\theta - \\overline{x})^2 + \\frac{1}{\\nu^2}(\\theta - \\mu)^2 = \\frac{1}{\\nu_1^2}(\\theta - \\mu_1)^2 + \\frac{n}{\\sigma^2 + n \\nu^2}(\\overline{x}-\\mu)^2 \\ \\ \\ \\mbox{ pg 399}\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu_1^2} (\\theta - \\mu_1)^2 \\bigg]\\\\\n\\theta | \\underline{x} &\\sim& N (\\mu_1, \\nu_1^2)\\\\\n\\mbox{: } && \\mu_1 = \\frac{\\sigma^2 \\mu + n \\nu^2 \\overline{x}}{\\sigma^2 + n \\nu^2} \\ \\ \\ \\ \\nu_1^2 = \\frac{\\sigma^2 \\nu^2}{\\sigma^2 + n \\nu^2}\n\\end{eqnarray*}\\]Remember, computing posterior \\(\\theta\\), can ignore anything doesn’t depend \\(\\theta\\) (“known” parameters, data, constants,…)conjugate families. Note conjugate, prior distribution family posterior distribution.","code":""},{"path":"bayes.html","id":"improper-priors","chapter":"2 Bayesian Estimation","heading":"2.5 Improper Priors","text":"Improper prior distributions actually probability functions, yet lead posterior distributions probability functions (, integrate 1). Improper priors capture idea data worth prior belief. Often, improper prior lead Frequentist result. example, Beta(0,0) prior Bernoulli likelihood leads :\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    \\theta^{-1}(1-\\theta)^{-1} & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\(\\xi(\\theta)\\) integrate 1, proper pdf. However posterior proper pdf,\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y-1} (1-\\theta)^{n-y-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y, n-y)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)}\\theta^{y-1} (1-\\theta)^{n-y-1}\\\\\n\\mbox{Note: } && E[\\theta | \\underline{X} ] = \\frac{y}{n} \\ \\ \\ \\mbox{ expected frequentist model!}\n\\end{eqnarray*}\\]conjugate priors improper prior limiting case: Beta(0,0), gamma(0,0), Normal(\\(\\mu, \\nu^2 = \\infty\\)). normal improper prior ignores prior constant, becomes:\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\lim_{\\nu^2 \\rightarrow \\infty} exp\\bigg(-\\frac{1}{2\\nu^2}(\\theta - \\mu)^2\\bigg) = 1\n\\end{eqnarray*}\\], improper normal prior flat line reals. Note improper normal prior used normal likelihood, posterior \\(\\theta | \\underline{x} \\sim N (\\underline{x}, \\sigma^2 /n)\\). [, prior indicating knowledge \\(\\theta\\) produces posterior depends data.]","code":""},{"path":"bayes.html","id":"bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.6 Bayes’ Estimators","text":"Prior posterior distributions tell us Bayesians think parameters. next question need address think estimators? estimator function data hope close true value parameter.Note:\n\\[\\begin{eqnarray*}\n\\delta(X_1, X_2, \\ldots, X_n) &=& \\delta(\\underline{X}) \\mbox{  estimator}\\\\\n\\delta(x_1, x_2, \\ldots, x_n) &=& \\delta(\\underline{x}) \\mbox{  estimate}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"loss-functions","chapter":"2 Bayesian Estimation","heading":"2.6.1 Loss functions","text":"(responsible material loss functions. take away message section Bayes estimator use expected value posterior distribution. However, seen , Bayes estimators, example, median posterior distribution used.)want estimator \\(\\theta\\) leads estimate close true value \\(\\theta\\). loss function helps determine far estimator . particular estimate, \\(\\):\n\\[\\begin{eqnarray*}\n\\mbox{squared error loss: } L(\\theta, ) &=& (\\theta - )^2\\\\\n\\mbox{absolute error loss: } L(\\theta, ) &=& |\\theta - |\\\\\n\\end{eqnarray*}\\]\nwant loss small (minimized).","code":""},{"path":"bayes.html","id":"squared-error-loss","chapter":"2 Bayesian Estimation","heading":"Squared Error Loss","text":"Without data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,)] = \\int_\\Omega L(\\theta, ) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,) | \\underline{X}] &=& \\int_\\Omega L(\\theta, ) \\xi(\\theta| \\underline{X}) d\\theta\\\\\n\\end{eqnarray*}\\]\nLet \\(\\delta^*(\\underline{X})\\) value \n\\[\\begin{eqnarray*}\nE[L(\\theta, \\delta^*(\\underline{X})) | \\underline{X} ] &=& \\min_{\\\\Omega} E[ L(\\theta,) | \\underline{X} ]\\\\\n\\delta^*(\\underline{X}) && \\mbox{ Bayes estimator } \\theta\\\\\n\\delta^*(\\underline{x}) && \\mbox{ Bayes estimate } \\theta\\\\\n\\end{eqnarray*}\\]\n\\(\\delta^*\\)?Note: \\(\\delta^*\\) depends loss function prior / posterior.\\[\\begin{eqnarray*}\nE[L(\\theta, ) | \\underline{X} ] &=& E[ (\\theta - )^2 | \\underline{X}]\\\\\n&=& E[ \\theta^2 - 2a\\theta + ^2 | \\underline{X}]\\\\\n&=& E[\\theta^2 | \\underline{X}] - 2a E[\\theta | \\underline{X}] + ^2\\\\\n&&\\\\\n\\frac{\\partial E[ (\\theta - )^2 | \\underline{X}]}{\\partial } &=& 0\\\\\n&&\\\\\n- 2 E[\\theta | \\underline{X}] + 2a &=& 0\\\\\n&=& E[\\theta | \\underline{X}] = \\delta^*(\\underline{X}) !!!\\\\\n&&\\\\\n\\frac{\\partial^2 E[ (\\theta - )^2 | \\underline{X}]}{\\partial ^2} &=& 2 > 0 \\rightarrow \\mbox{ loss minimized}\\\\\n\\end{eqnarray*}\\]Example 2.8  Let \\(\\theta\\) denote average number defects per 100 feet tape. \\(\\theta\\) unknown, prior \\(\\theta\\) gamma distribution \\(E[\\theta] = \\alpha / \\beta = 2/10, \\alpha= 2, \\beta = 10\\). 1200 foot roll tape inspected, exactly 4 defects found.Bayes’ estimate average number defects per 100 feet?\\[\\begin{eqnarray*}\n\\mbox{Prior:     }&&\\\\\n\\xi(\\theta) &=& \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta \\theta} = \\frac{10^2}{\\Gamma(2)} \\theta e^{-10\\theta}\\\\\n\\mbox{Likelihood:     }&&\\\\\nf(\\underline{x} | \\theta) &=& \\prod_{=1}^n \\frac{e^{-\\theta} \\theta^{x_i}}{x_i!} = \\frac{e^{-n\\theta} \\theta^{\\sum x_i}}{\\prod (x_i !)}\\\\\n\\mbox{Posterior:     }&&\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\frac{ \\theta e^{-10\\theta} e^{-n \\theta} \\theta^{\\sum x_i}}{\\Gamma(2) 10^2 \\prod (x_i !)}\\\\\n&\\propto& e^{-\\theta(n+10)} \\theta ^{\\sum x_i + 1} \\\\\n\\\\\n\\theta | \\underline{x} &\\sim& \\mbox{Gamma } (\\sum x_i + 2 = 6, n + 10 = 22)\\\\\n\\\\\n\\delta^*(\\underline{X}) &=& \\frac{ \\sum X_i + 2}{n+10}\\\\\n\\delta^*(\\underline{x}) &=& \\frac{6}{22} = \\frac{3}{11}\n\\end{eqnarray*}\\]\nNote: Gamma distribution parameterized slightly differently DeGroot sheet (exponential). Make sure expected value matches ’ve given problem.","code":""},{"path":"bayes.html","id":"absolute-loss","chapter":"2 Bayesian Estimation","heading":"Absolute Loss","text":"minimize \\(E[ | \\theta - | | \\underline{X}]\\) ? \\(\\rightarrow \\ \\ \\ \\delta^*(\\underline{X}) = \\mbox{median} (\\theta | \\underline{X})\\) (see Theorem 4.5.1 DeGroot Schervish (2011)). However, isn’t always obvious compute median non-symmetric distributions. symmetric distributions median = mean.","code":""},{"path":"bayes.html","id":"evaluating-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.7 Evaluating Bayes Estimators","text":"","code":""},{"path":"bayes.html","id":"mean-squared-error","chapter":"2 Bayesian Estimation","heading":"2.7.1 Mean Squared Error","text":"get MSE, let’s talk Agresti-Coull estimate Binomial parameter probability success. estimator attenuates sample proportion closer 0.5 effect reducing variability estimator. Indeed, evidence “add two successes two failures” approach create precise confidence intervals.2Even though agreement better estimate, ’d like show biased.\\[\\begin{eqnarray*}\n\\tilde{p} &=& \\frac{X+2}{n+4}\\\\\nE[\\tilde{p}] &=& \\frac{n\\theta+2}{n+4}\\\\\nbias(\\tilde{p}) &=& \\frac{n\\theta+2}{n+4} - \\theta\\\\\n&=& \\frac{n\\theta + 2 - n\\theta - 4\\theta}{n+4}\\\\\n&=& \\frac{2-4\\theta}{n+4}\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"mse-in-general-frequentist","chapter":"2 Bayesian Estimation","heading":"MSE in general (“Frequentist”)","text":"Mean Squared Error (MSE) expected squared difference parameter (\\(\\theta\\)) estimate (\\(\\hat{\\theta}\\)), , typically, \\(\\hat{\\theta}\\) function data.frequentist MSE based expected values taken likelihood pdf (\\(X | \\theta\\)).Note MSE can written sum bias squared variance:\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_F(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_F(\\hat{\\theta}) &=& E [ (\\hat{\\theta} - \\theta)^2 ]\\\\\n&=& E [ ( \\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2 ] \\mbox{ RV X!} \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2 + 2(\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2]\\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + 0 + E[ (E(\\hat{\\theta}) - \\theta)^2 ] \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + (E(\\hat{\\theta}) - \\theta)^2  \\\\\n&=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\n\\end{eqnarray*}\\]Note taking expected values, resulting MSE function \\(\\theta\\) (function data).example, consider Bernoulli situation (e.g., Basketball shooter):\n\\[\\begin{eqnarray*}\n\\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\nVar(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\nbias(\\hat{\\theta}) &=& E(\\hat{\\theta}) - \\theta = 0\\\\\nMSE_F(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"bayesian-mse","chapter":"2 Bayesian Estimation","heading":"Bayesian MSE","text":"Bayesians, MSE expected squared error loss conditional data (, expected value taken posterior, \\(\\theta | \\underline{X}\\).)Bayesian MSE based expected values taken posterior pdf \\((\\theta | \\underline{X}).\\)let \\(\\delta = \\delta(\\underline{X}) = E(\\theta | \\underline{X})\\) estimator, MSE :\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_B(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_B(\\delta(\\underline{X})) &=& E [ (\\delta - \\theta)^2 | \\underline{X}]\\\\\n&=& E [ ( E(\\theta | \\underline{X}) - \\theta)^2 | \\underline{X}] \\mbox{RV } \\theta \\mbox{!!}\\\\\n&=& \\mbox{var}(\\theta | \\underline{X})\\\\\n\\end{eqnarray*}\\]Continuing example, note \\(\\theta \\sim Beta(,b)\\), means \\(\\theta | \\underline{X} \\sim Beta(X + , n - X + b)\\).\\[\\begin{eqnarray*}\n\\delta(X) &=& \\frac{X + }{ n - X + b}\\\\\nMSE_B(\\delta(X)) &=& \\frac{(X+)(n++b)}{(n++b)^2(n++b+1)}\\\\\n\\end{eqnarray*}\\]Note Bayesian MSE posterior variance parameter interest. ’ve used expected value estimate, bias. Note Bayesian MSE function data (\\(\\theta\\)), compare Bayesian MSE Frequentist MSE directly.Example 2.9  Recall tape example, Example 2.8.Prior: Gamma(2, 10) ((2, 1/10) depending parametrize))Data likelihood: Poisson(\\(\\theta\\))Posterior: Gamma (\\(\\sum X_i + 2\\), \\(n\\) + 10)Note calculations done Frequentist MSE conditional data.\\[\\begin{eqnarray*}\n(\\mbox{frequentist estimator}) \\ \\ \\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\n(\\mbox{Bayesian estimator}) \\ \\ \\delta(\\underline{X}) &=& \\frac{\\sum X_i+2}{n+10}\\\\\n& \\\\\nMSE_F(\\hat{\\theta}) &=& var(\\hat{\\theta}) + bias(\\hat{\\theta})^2\\\\\n&=& \\theta/n + 0 = \\theta/n\\\\\nMSE_B(\\delta(\\underline{X})) &= & var(\\theta | \\underline{X})\\\\\n&=& \\frac{\\sum X_i+2}{(n+10)^2}\\\\\n&&\\\\\nMSE_F(\\delta(\\underline{X})) &=& var(\\delta(\\underline{X})) + bias(\\delta(\\underline{X}))^2\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nbias(\\delta(\\underline{X})) &=& E\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg] - \\theta\\\\\n&=& \\frac{n \\theta +2}{n+10} - \\theta = \\frac{n\\theta + 2 - n\\theta -10\\theta}{n+10}\\\\\n&=& \\frac{2-n\\theta}{n+10}\\\\\nvar(\\delta(\\underline{X})) &=& var\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg]\\\\\n&=& \\frac{1}{(n+10)^2}var\\bigg(\\sum X_i\\bigg)\\\\\n&=& \\frac{1}{(n+10)^2} n \\  var(X_i)\\\\\n&=& \\frac{n}{(n+10)^2} \\theta\\\\\n\\end{eqnarray*}\\]\\[\\begin{align*}\nMSE_F(\\delta(\\underline{X})) &= \\frac{n}{(n+10)^2} \\theta + \\frac{(2-n\\theta)^2}{(n+10)^2}\\\\\n&= \\frac{n \\theta+ (2-n\\theta)^2}{(n+10)^2}\n\\end{align*}\\]\nNote couldn’t directly compare \\(MSE_F\\) \\(MSE_B\\) (functions different variables!). ’d come prior think \\(MSE_B(\\hat{\\theta})\\), seems like can’t calculate quantity. Instead, take easier route, find \\(MSE_F(\\delta(\\underline{X}))\\) order reasonable comparison estimators.","code":""},{"path":"bayes.html","id":"sensitivity-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.2 Sensitivity of Estimators","text":"sensitive results different priors?Example 2.10  Continuing tape example, Example 2.8, different values estimate theta depending different priors data values:Note: \\(E[ \\theta | \\underline{x} ] = \\frac{\\sum x_i + \\alpha}{ n + \\beta} = w_1 \\frac{\\sum x_i}{n} + w_2 \\frac{\\alpha}{\\beta}\\), \\(w_1 = \\frac{n}{n+\\beta}, w_2 = \\frac{\\beta}{n+\\beta}\\).\\\n\\(n \\rightarrow \\infty, \\hat{\\theta} \\rightarrow \\frac{\\sum x_i}{n}\\), \\(n \\rightarrow 0, \\hat{\\theta} \\rightarrow \\frac{\\alpha}{\\beta}\\).","code":""},{"path":"bayes.html","id":"consistency-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.3 Consistency of Estimators","text":"consistent estimator \\(\\theta\\) one converges probability \\(\\theta\\). Many Bayes estimators consistent. fact, fairly general regularity conditions, wide class Bayes estimators consistent.Note, estimator \\(Y_n\\) converges \\(\\theta\\) probability :\n\\[\\begin{eqnarray*}\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | < \\epsilon ] &=& 1 \\ \\ \\ \\ \\ \\mbox{pg 233}\\\\\n\\mbox{, equivalently}\\\\\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | \\geq \\epsilon ] &=& 0\\\\\n\\end{eqnarray*}\\](saw idea weak strong laws large numbers: \\(\\overline{X} \\stackrel{P}{\\rightarrow} \\mu\\) \\(n \\rightarrow \\infty\\) Weak Law Large Numbers.) [n.b. case curious, strong law large numbers says \\(\\overline{X} \\stackrel{.s.}{\\rightarrow} \\mu\\) (almost surely). means \\(\\lim_{n \\rightarrow \\infty} P [ \\overline{X} = \\mu ] = 1\\)Example 2.11  Continuing tape example, Example 2.8:\n\\[\\begin{eqnarray*}\n\\delta^*(\\underline{X}) &=& \\frac{\\sum X_i + \\alpha}{n+\\beta}\\\\\n\\overline{X} &\\stackrel{P}{\\rightarrow}& \\theta \\mbox{ (WLLN)}\\\\\n\\mbox{} && \\\\\n\\delta^*(\\underline{X}) - \\overline{X} &=& \\frac{- \\beta}{n+\\beta} \\overline{X} + \\frac{\\beta}{n+\\beta} \\frac{\\alpha}{\\beta} \\stackrel{P}{\\rightarrow} 0 \\mbox{ (Slutsky's theorem)}\\\\\n&& \\\\\n\\delta^*(\\underline{X}) &\\stackrel{P}{\\rightarrow} \\theta\\\\\n\\end{eqnarray*}\\]\\(\\delta^*(\\underline{X})\\) consistent estimator \\(\\theta\\).","code":""},{"path":"bayes.html","id":"benefits-and-limitations-of-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.8 Benefits and Limitations of Bayes’ Estimators","text":"","code":""},{"path":"bayes.html","id":"benefits","chapter":"2 Bayesian Estimation","heading":"2.8.1 Benefits","text":"can incorporate informationthe interpretation intuitive","code":""},{"path":"bayes.html","id":"limitations","chapter":"2 Bayesian Estimation","heading":"2.8.2 Limitations","text":"need prior informationit can difficult produce prior two parameters simultaneously (e.g., normal, gamma)need agree prior information","code":""},{"path":"bayes.html","id":"additional-examples","chapter":"2 Bayesian Estimation","heading":"2.9 Additional Examples","text":"Example 2.12  Suppose Beta(4,4) prior distribution probability \\(\\theta\\) coin yield head spun specified manner. coin independently spun ten times, heads appears fewer 3 times. told many heads seen, number less 3. Calculate exact posterior density \\(\\theta\\).3Example 2.13  Baseball Bayes4You statistician employed Ball Consulting. Veteran major-league baseball scout Rocky Chew seeks advice regarding estimating probability amateur baseball player John Spurrier get base hit major-league pitcher. Rocky arranged Spurrier ten bats major-league pitcher.traditional batting average, \\(\\hat{\\theta}_f = X/n\\) frequentist estimator makes use observed data, ignores prior information might exist.5 assume bats independent Bernoulli trials constant probability getting base hit, \\[\\begin{eqnarray*}\nX \\sim Bin( n=\\mbox{number bat}, \\theta=\\mbox{P(getting base hit)})\n\\end{eqnarray*}\\]\\(\\hat{\\theta}_f\\), good estimator unknown probability (getting base hit), ignores information might baseball. following prior information:John Spurrier appears good great player. one better batters somewhat -average American Legion (high school) baseball team.major-league scouts watched play believe Spurrier’s batting ability professional level.barely adequate major-league hitter batting average 0.200.good major-league batter batting average 0.300.Ty Cobb -time best major-league batting average 0.366.’re going use Beta prior incorporate previous knowledge. prior look like?John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).Example 2.14  Kidney Cancer rates6 example, ’re going use Bayes theory adjust kidney cancer rates less variability. First, ’d like investigate counties highest lowest kidney cancer death rates US (white men, 1980-1989).patterns see figures 2.3 & 2.4? Can give plausible reasons patterns see?county 100 people? Small counties variable. Keep mind rates age-adjusted.\nFigure 1.6: Figure 2.3 Teaching Statistics, bag tricks Gelman Nolan.\nConsider figure 13.4, highest 10% Bayes-estimated kidney cancer death rates US (white men, 1980-1989). Let’s assume number deaths distributed Poisson(\\(n_j \\theta_j\\)) \\(n_j\\) number people county, \\(\\theta_j\\) true kidney cancer death rate county. , assume outside influences kidney cancer (e.g., pollution) county’s cancer rate comes Gamma distribution parameters (\\(\\alpha = 61, \\beta = 47000\\)). ,\\[\\begin{eqnarray*}\n\\mbox{Likelihood:} && y_j \\sim \\mbox{ Poisson}(n_j \\theta_j) \\ \\ \\ n_j = \\mbox{ county population}\\\\\n\\mbox{Prior:} && \\theta_j \\sim \\mbox{ Gamma}(\\alpha=61, \\beta = 47000)\\\\\n&& E[\\theta_j] = \\alpha / \\beta = 1.296 \\times 10^{-3} \\ \\ \\ \\ (\\mbox{10 yr cancer rate})\\\\\n\\end{eqnarray*}\\]\nknow \\(E[\\theta | y] = \\frac{\\alpha + y}{m + \\beta}\\). estimate compare frequentist estimate, \\(\\hat{\\theta} = \\frac{y}{m}\\)?investigate relationship Bayes estimate versus frequentist estimate, ’re going simulate kidney cancer death rates variety counties.Everyone gets county (population). county, ’ll generate true, underlying, kidney cancer rate \\(\\theta_j\\). (Note, \\(\\theta_j\\) sampled (simulated) Gamma(\\(\\alpha=61, \\beta=47,000\\)) distribution.)Using cancer rate (\\(\\theta_j\\)) county’s population (\\(n_j\\)), simulate value number people county died kidney cancer last 10 years (Poisson(\\(n_j \\theta_j\\))).Report frequentist estimate kidney cancer rate county (erase / hide true cancer rate). Leave county name, population, number deaths, estimated rate.public officials left task guessing counties highest cancer rates… think?Calculate Bayes estimate cancer rate. Compare underlying (\\(\\theta_j\\)), observed / frequentist (\\(\\frac{y_j}{n_j}\\)), posterior / Bayesian (\\(\\hat{\\theta}_j | y_j\\)) kidney cancer death rates.\nFigure 2.1: Figure 2.4 Teaching Statistics, bag tricks Gelman Nolan.\n","code":""},{"path":"bayes.html","id":"the-experiment","chapter":"2 Bayesian Estimation","heading":"The Experiment","text":"John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).","code":""},{"path":"bayes.html","id":"reflection-questions-1","chapter":"2 Bayesian Estimation","heading":"2.10  Reflection Questions","text":"prior distribution? random variable described prior?likelihood? random variable described likelihood?posterior distribution? random variable described posterior?conjugate prior? benefit conjugate prior?hope lost prior conjugate? , approach problem coming posterior?","code":""},{"path":"bayes.html","id":"ethics-considerations-1","chapter":"2 Bayesian Estimation","heading":"2.11  Ethics Considerations","text":"make sense incorporate prior information? doesn’t make sense incorporate prior information?legitimate ways calculate prior? illegitimate ways calculate prior?analyst able come prior likelihood?","code":""},{"path":"bayes.html","id":"r-code-bayesian-example","chapter":"2 Bayesian Estimation","heading":"2.12 R code: Bayesian Example","text":"Example 2.15  Recall Baseball Bayes example, Example 2.13.functions allow us label plot parameter information.trying variety different values \\(\\) \\(b\\), prior distributions can visualized.prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\nlibrary(tidyverse)\nlibrary(glue)\n\nex <- function(a,b) {round(a / (a+b), 2)}\nsdx <- function(a,b) {round(sqrt(a*b/((a+b)^2 * (a+b+1))),3)}\n\nbeta_legend <- function(a,b) {\n  glue::glue('a = {a}, ',\n             'b = {b}, ',\n             'EX = {ex(a,b)}, ',\n             'SDX = {sdx(a,b)}')}\n\n# see it in action:\nggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"bayes.html","id":"priors","chapter":"2 Bayesian Estimation","heading":"Priors:","text":"trying variety different values \\(\\) \\(b\\), prior distributions can visualized.","code":"\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")"},{"path":"bayes.html","id":"mse","chapter":"2 Bayesian Estimation","heading":"MSE:","text":"prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"MLE.html","id":"MLE","chapter":"3 Maximum Likelihood Estimation","heading":"3 Maximum Likelihood Estimation","text":"Maximum likelihood estimation method choosing estimators parameters avoids using prior distributions loss functions. MLE chooses \\(\\hat{\\theta}\\) estimate \\(\\theta\\) maximizes likelihood function (easily widely used estimation method statistics).Let’s say \\(X \\sim U[0, \\theta].\\) collect \\(X=47.\\) ever pick \\(\\theta=10?\\) ! \\(10 \\notin \\Omega!\\)Let’s say \\(X \\sim\\) Bin(\\(\\theta,\\)n=4). 4 independent trials, X=1, chose \\(\\theta=0.99?\\) \\(0.99 \\\\Omega???\\) choose \\(\\theta = 0.25.\\) ? maximized likelihood.\\[\\begin{eqnarray*}\nP(X=1 | \\theta = 0.25) &=& 0.422\\\\\nP(X=1 | \\theta = 0.5) &=& 0.25\\\\\nP(X=1 | \\theta = 0.05)  &=& 0.171\\\\\nP(X=1 | \\theta = 0.15) &=& 0.368\\\\\n\\end{eqnarray*}\\]\nmaximized probability seeing data!Example 3.1  Let’s say houses block electricity connected way works house #47 neighbor’s electricity works (sequence). Sometimes house’s electricity fail (w/prob p). many houses can expect provide electricity? best guess p?Data: let’s say \\(n\\) neighborhoods information house lost electricity. [distribution \\(X?\\) \\(X \\sim geometric(p),\\) means \\(E[X] = 1/p.]\\)\\[\\begin{eqnarray*}\nf(x_i | p) &=& p (1-p)^{x_i -1}\\\\\nf(\\underline{x} | p) &=& \\prod_{=1}^n p (1-p)^{x_i -1}\\\\\n&=& p^n (1-p)^{\\sum x_i -n}\\\\\n&& \\mbox{want maximize wrt } p\n\\end{eqnarray*}\\]Often, log-likelihood easier maximize likelihood. define log likelihood \n\\[\\begin{eqnarray*}\nL(\\theta) = \\ln f(\\underline{X} | \\theta).\n\\end{eqnarray*}\\]example,\n\\[\\begin{eqnarray*}\nL(p) &=& \\ln f(\\underline{x} | p) = n \\ln(p) + (\\sum x_i -n) \\ln(1-p)\\\\\n\\frac{\\partial L(p)}{\\partial p} &=& \\frac{n}{p} + \\frac{(\\sum x_i -n)(-1)}{(1-p)} = 0\\\\\n\\hat{p} &=& \\frac{1}{\\overline{X}}\n\\end{eqnarray*}\\]\\(\\overline{x} = 10\\), \\(\\hat{p} = 1/10\\) (1 failure 10 homes). E[X] = 1/p. (let \\(\\theta = E[X] -1\\), good estimate expected (average) number homes can provide electricity \\(1/\\hat{p} -1 = 10 - 1\\). turns MLE invariance property says function MLE MLE function parameter.)Note ’ve found maximum:\n\\[\\begin{eqnarray*}\n\\frac{\\partial^2 L(p)}{\\partial p^2} &=& \\frac{2np -n -p^2\\sum x_i}{p^2 (1-p)^2}\\\\\n&\\leq& \\frac{2p n - n -p^2n}{p^2 (1-p)^2}\\\\\n(\\mbox{} n &\\leq& \\sum x_i)\\\\\n&=& \\frac{n ( -2p + 1 +p^2}{p^2 (1-p)^2}\\\\\n&=& \\frac{-n}{p^2} < 0\\\\\n\\end{eqnarray*}\\]Definition 3.1  Maximum Likelihood Estimator (Def 7.5.2).\npossible observed vector \\(\\underline{x}\\), let \\(\\delta(\\underline{x}) \\\\Omega\\) denote value \\(\\theta \\\\Omega\\) likelihood function, \\(f(\\underline{x}|\\theta)\\) maximum, let \\(\\hat{\\theta} = \\delta(\\underline{X})\\) estimator \\(\\theta\\) defined way. estimator \\(\\hat{\\theta}\\) called maximum likelihood estimator \\(\\theta.\\) \\(\\underline{X} = \\underline{x}\\) observed, value \\(\\delta(\\underline{x})\\) called maximum likelihood estimate \\(\\theta.\\)","code":""},{"path":"MLE.html","id":"sampling-from-a-normal-distribution","chapter":"3 Maximum Likelihood Estimation","heading":"Sampling from a Normal Distribution","text":"\\[\\begin{eqnarray*}\nX_1, X_2, \\ldots X_n &\\stackrel{iid}{\\sim}& N(\\mu, \\sigma^2)  \\ \\ \\ \\mu \\ \\& \\ \\sigma^2 \\mbox{ fixed unknown}\\\\\nf(x_i | \\mu, \\sigma^2) &=& \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp [ \\frac{-1}{2\\sigma^2}(x_i - \\mu)^2 ]\\\\\nf(\\underline{x} | \\mu, \\sigma^2) &=& \\frac{1}{(2 \\pi \\sigma^2)^{n/2}} \\exp [ \\frac{-1}{2\\sigma^2}\\sum_{=1}^n(x_i - \\mu)^2 ]\\\\\nL(\\mu, \\sigma^2) &=& \\frac{-n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum(x_i - \\mu)^2\\\\\n\\end{eqnarray*}\\]want maximize \\(f(\\underline{x}|\\mu, \\sigma^2)\\) respect \\(\\mu;\\) equivalently, can minimize \\(\\sum (x_i - \\mu)^2.\\)\\[\\begin{eqnarray*}\n\\mbox{Let } Q(\\mu) &=& \\sum(x_i - \\mu)^2\\\\\n\\frac{\\partial Q(\\mu)}{\\partial \\mu} &=& -2 \\sum(x_i - \\mu) = 0\\\\\n-2 \\sum x_i &=& -2 n \\mu\\\\\n\\hat{\\mu} &=& \\overline{x}\n\\end{eqnarray*}\\]MLE \\(\\mu\\) doesn’t depend \\(\\sigma^2\\), know joint MLE, \\(\\hat{\\theta} = (\\mu, \\sigma^2)\\), \\(\\hat{\\theta}\\) \\(\\hat{\\sigma}^2\\) maximizes (\\(\\theta' = (\\overline{x}, \\sigma^2) ):\\)\n\\[\\begin{eqnarray*}\nL(\\theta') &=& \\frac{-n}{2} \\ln (2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum(x_i - \\overline{x})^2\\\\\n\\frac{\\partial L(\\theta')}{\\partial \\sigma^2} &=& \\frac{-n}{2 \\sigma^2} - \\frac{1(-1)}{2 (\\sigma^2)^2} \\sum(x_i - \\overline{x}) = 0\\\\\n\\sum (x_i - \\overline{x})^2 &=& \\frac{n(\\sigma^2)^2}{\\sigma^2}\\\\\n\\hat{\\sigma^2} &=& \\frac{\\sum(x_i - \\overline{x})^2}{n}\\\\\n\\hat{\\theta} &=& (\\overline{x}, \\frac{\\sum(x_i - \\overline{x})^2}{n})\n\\end{eqnarray*}\\]Example 3.2  Non-existence MLESuppose \\(X_1, X_2, \\ldots X_n \\stackrel{iid}{\\sim} f(x | \\theta)\\) :\n\\[\\begin{eqnarray*}\nf(x | \\theta) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x > \\theta\\\\\n    0 & x \\leq \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]\\(\\theta\\) unknown, \\(-\\infty < \\theta < \\infty\\). MLE exist?\n\\[\\begin{eqnarray*}\nf(\\underline{x}|\\theta) &=& e^{n \\theta - \\sum x_i} \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\nL(\\theta) &=& n \\theta - \\sum x_i \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\n\\frac{\\partial L(\\theta)}{\\partial \\theta} &=& n = 0 ?!?!?!\\\\\n\\end{eqnarray*}\\]want find largest \\(\\theta\\) \\(\\theta < x_i \\ \\ \\forall x_i\\). \\(\\hat{\\theta} = \\min x_i\\)? , \\(\\theta < \\min x_i!\\) , instead,\n\\[\\begin{eqnarray*}\nf (x | \\theta) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x \\geq \\theta\\\\\n    0 & x < \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]MLE \\(\\theta\\) \\(\\hat{\\theta} = \\min x_i\\).","code":""},{"path":"MLE.html","id":"qualities-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1 Qualities of the MLE","text":"","code":""},{"path":"MLE.html","id":"invariance-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.1 Invariance of the MLE","text":"Theorem 3.1  (DeGroot Schervish (2011) Theorem 6.6.1) Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\), let \\(g(\\theta)\\) function \\(\\theta\\). MLE \\(g(\\theta)\\) \\(g(\\hat{\\theta})\\). (Proof: see page 427 DeGroot & Schervish)Proof. Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\). know \\[L(\\hat{\\theta}) = \\max_{\\theta \\\\Omega} L(\\theta).\\]let \\(g\\) one--one function \\(\\psi = g(\\theta) \\\\Gamma\\) (image \\(\\Omega\\) g). can denote inverse function \\(g\\) \\[\\theta = h(\\psi).\\]know MLE \\(\\psi\\) \\(\\hat{\\psi}\\) \\[L(h(\\hat{\\psi})) = \\max_{\\psi \\\\Gamma} L(h(\\psi)).\\]know \\(L(\\theta)\\) maximized \\(\\hat{\\theta}\\). \\(L(h(\\psi))\\) must also maximized \\(h(\\psi) = \\hat{\\theta}\\). \\[\\therefore h(\\hat{\\psi}) = \\hat{\\theta} \\ \\ \\ \\mbox{ } \\ \\ \\ \\hat{\\psi} = g(\\hat{\\theta}).\\]\\(g\\) one--one, however, need redefine mean likelihood. \\(g(\\theta)\\) one--one may many values \\(\\psi\\) satisfy \\[g(\\theta) = \\psi.\\] (example, square function. n.b. \\(g\\) still function, can’t map multiple values.)case, correspondence maximum \\(\\psi\\) maximum \\(\\theta\\) breaks . E.g., \\(\\hat{\\theta}\\) MLE \\(\\theta\\), may another value \\(\\theta\\), say \\(\\theta_0\\) \\(g(\\hat{\\theta}) = g(\\theta_0).\\)define induced log likelihood function: \\[L^*(t) = \\max_{\\theta \\G_t} \\ln f(\\underline{x} | \\theta).\\]Let \\(G\\) image \\(\\Omega\\) g, \\[G_t = \\{ \\theta: g(\\theta) = t\\} \\ \\ \\forall t \\G.\\]define MLE \\(g(\\theta)\\) \\(\\hat{t}\\) \\[L^*(\\hat{t}) = \\max_{t \\G} L^* (t).\\]proof follows (page 427). [Note two maximizations. , first maximization created log likelihood function. second maximization found maximum function parameter space.]Example 3.3  Functions MLEs MLEs also!standard deviation normal:\n\\[\\begin{eqnarray*}\n\\sigma &=& \\sqrt{\\sigma^2}\\\\\n\\hat{\\sigma} &=& \\mbox{MLE}(\\sigma) = \\sqrt{\\frac{\\sum (x_i - \\overline{x})^2}{n}}\n\\end{eqnarray*}\\]mean uniform:\n\\[\\begin{eqnarray*}\nX &\\sim& U [\\theta_1, \\theta_2]\\\\\n\\mu &=& \\frac{\\theta_1 + \\theta_2}{2}\\\\\n\\hat{\\mu} &=& \\mbox{MLE}(\\mu) = \\frac{\\max(x_i) + \\min(x_i)}{2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"consistency-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.2 Consistency of the MLE","text":"certain regularity conditions, MLE \\(\\theta\\) consistent \\(\\theta\\). ,\n\\[\\begin{eqnarray*}\n\\hat{\\theta} \\stackrel{P}{\\rightarrow} \\theta\\\\\n\\stackrel{\\lim}{ n \\rightarrow \\infty} P [ | \\hat{\\theta} - \\theta | > \\epsilon ] = 0\n\\end{eqnarray*}\\]\\(\\hat{\\theta}\\) function \\(X\\).","code":""},{"path":"MLE.html","id":"bias-of-the-mle-section-7.7","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.3 Bias of the MLE (section 7.7)","text":"Previously, discussed briefly concept bias:Definition 3.2  Unbiased (page 428 DeGroot & Schervish): Let \\(\\delta(X_1, X_2, \\ldots, X_n)\\) estimator \\(g(\\theta)\\). say \\(\\delta(X_1, X_2, \\ldots, X_n)\\) unbiased :\n\\[\\begin{eqnarray*}\nE[ \\delta(X_1, X_2, \\ldots, X_n)] = g(\\theta)\n\\end{eqnarray*}\\]Note: bias \\((\\delta(\\underline{X}) ) = E[ \\delta(\\underline{X})] - g(\\theta)\\).Example 3.4  Let \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\). Recall, \\(\\hat{\\sigma^2} = \\frac{\\sum(X_i - \\overline{X})^2}{n}\\) MLE.\n\\[\\begin{eqnarray*}\nE[\\hat{\\sigma^2}] &=& E \\bigg[ \\frac{\\sum(X_i - \\overline{X})^2}{n}\\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu + \\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu)^2 + 2 \\sum (X_i - \\mu)(\\mu - \\overline{X}) + n(\\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n}\\bigg\\{ E\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2 ] \\bigg] \\bigg\\}\\\\\n&=& \\frac{1}{n} \\bigg\\{  \\sum E (X_i - \\mu)^2 -  n E [(\\overline{X} - \\mu)^2 ] \\bigg\\}\\\\\n&=& \\frac{1}{n}\\{ n \\sigma^2 - n \\frac{\\sigma^2}{n}  \\}\\\\\n&=& \\frac{n-1}{n} \\sigma^2 \\Rightarrow \\mbox{Biased!}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"but-consistent-yes.","chapter":"3 Maximum Likelihood Estimation","heading":"But consistent? Yes.","text":"\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-maximum-likelihood-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.4 Benefits and Limitations of Maximum Likelihood Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-1","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"functions MLEs MLEs (invariance MLE)certain regularity conditions, MLEs asymptotically distributed normally.","code":""},{"path":"MLE.html","id":"limitations-1","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"MLE always existMLE always uniqueMLE value parameter \\(\\theta\\) maximizes distribution \\(X|\\theta\\) (likely produced observed data). MLE likely parameter given data: \\(E[ \\theta | X]\\) (’s Bayesian estimator!).","code":""},{"path":"MLE.html","id":"method-of-moments","chapter":"3 Maximum Likelihood Estimation","heading":"3.2 Method of Moments","text":"Method Moments (MOM) another parameter estimation technique. find MOM estimate, set expected moment equal sample moment solve parameter value interest.\\[\\begin{eqnarray*}\nE[X^k] &=& k^{th} \\mbox{ expected moment}\\\\\n\\frac{1}{n} X_i^k &=& k^{th} \\mbox{ sample moment}\\\\\n\\end{eqnarray*}\\]Example 3.5  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\).\n\\[\\begin{eqnarray*}\n\\tilde{\\mu} &=& \\overline{X}\\\\\n\\tilde{\\sigma^2} &=& \\frac{\\sum X_i^2}{n} - \\overline{X}^2\\\\\n&=& \\frac{1}{n} \\sum(X_i - \\overline{X})^2 \\mbox{   MLE!!}\n\\end{eqnarray*}\\]Example 3.6  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim Gamma(\\alpha,\\beta)\\).\n\\[\\begin{eqnarray*}\n\\alpha / \\beta &=& \\overline{X}\\\\\n\\alpha / \\beta^2 &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\alpha &=& \\overline{X} \\beta\\\\\n\\frac{\\overline{X} \\beta}{\\beta^2} &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\tilde{\\beta} &=& \\frac{\\overline{X}}{(\\sum X_i^2 / n - \\overline{X}^2)}\\\\\n&=& \\overline{X} / \\hat{\\sigma^2}\\\\\n\\tilde{\\alpha} &=& \\overline{X}^2 / \\hat{\\sigma^2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-method-of-moments-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.2.1 Benefits and Limitations of Method of Moments Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-2","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"Often easier compute MLEs (e.g., MLE \\(\\alpha\\) Gamma distribution intractable).Estimates MOM can used first approximations likelihood.Can easily estimate multiple parameter families.","code":""},{"path":"MLE.html","id":"limitations-2","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"Can sometimes (usually cases small sample sizes) produce estimates outside parameter space.Doesn’t work moments don’t exist (e.g., Cauchy distribution)MLEs typically closer quantity estimated.Example 3.7  Tank EstimatorsHow can random sample integers 1 N (N unknown researcher) used estimate N?tanks numbered 1 N. Working group, randomly select five tanks, without replacement, bowl. tanks numbered:Think use data estimate N. (Come least 3 estimators.) Come consensus within group done.estimates N :rules formulas estimators N based sample n (case 5) integers :Assuming random variables distributed according discrete uniform:\n\\[\\begin{eqnarray*}\nX_i \\sim P(X=x | N) = \\frac{1}{N} \\ \\ \\ \\ \\ x = 1,2,\\ldots, N \\ \\ \\ \\ =1,2,\\ldots, n\n\\end{eqnarray*}\\]method moments estimator N?maximum likelihood estimator N?estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"mean-squared-error-1","chapter":"3 Maximum Likelihood Estimation","heading":"Mean Squared Error","text":"estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"reflection-questions-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.3  Reflection Questions","text":"function maximized MLE found? Derivative respect ? , explain intuition behind MLE good estimator \\(\\theta.\\)MOM estimator found? Explain MOM good estimator \\(\\theta.\\)asymptotic properties MLE make nice work ?benefits limitations MLE?benefits limitations MOM?","code":""},{"path":"MLE.html","id":"ethics-considerations-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.4  Ethics Considerations","text":"derivative always work find MLE? , ways can find maximum?MLE always best estimator \\(\\theta\\)?MOM always best estimator \\(\\theta\\)?makes good estimator?consulting, given dataset, asked find \\(\\theta\\), single right answer? know give boss?dataset don’t know likelihood?","code":""},{"path":"MLE.html","id":"r-code-mle-example","chapter":"3 Maximum Likelihood Estimation","heading":"3.5 R code: MLE Example","text":"Example 7.6.5 text looks MLE center Cauchy distribution.\nCauchy distribution interesting tails decay rate \\(1/x^2\\), try take expected value, end integrating something looks like \\(1/x\\) real line.\nHence, expected value exist.\nThus, method moments estimators use.\nMLE still useful, though easy find. stated text, likelihood proportional \n\\[\\prod_{=1}^n [1 + (x_i - \\theta)^2]^{-1}\\]Compute first second derivative log likelihood.Consider trying find root function \\(h(x)\\). Suppose current\nguess value \\(x_0\\). might approximate function tangent line (first order Taylor approximation) \\(x_0\\) take next guess root line. Use Taylor expansion find next guess \\(x_1\\) \\[x_1 = x_0 - \\frac{h(x_0)}{h'(x_0)}\\]Continually updating guess via method known Newton’s Method Newton-Raphson Method.Generate 50 observations Cauchy distribution centered \\(\\theta = 10\\). Based 50 observations, use parts () (b) estimate \\(\\theta\\). Remember, ’re trying maximize likelihood, function trying find root derivative log-likelihood. R code might look something like :Cauchy distribution sensitive initial value used Newton’s method, theta_guess close 10, probably MLE estimate infinite.","code":"library(tidyverse)\n\n# Use Newton's method to find the MLE for a Cauchy distribution\ncauchy_mle <- function(guess, data){\n    l1 <- ((compute 1st derivative of log-likelihood evaluated at \"guess\")) # 1st deriv\n    l2 <- ((compute 2nd derivative of log-likelihood evaluated at \"guess\")) # 2nd deriv\n    guess <- guess - l1/l2\n    guess\n}\n\n# set up the initial conditions (including the dataset to use)\nset.seed(54321)  # to get the same answer each time the .Rmd is knit\n\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\ntheta_guess <- ((pick a starting value, you might try different ones)) # just a number"},{"path":"MLE.html","id":"running-cauchy_mle-recursively","chapter":"3 Maximum Likelihood Estimation","heading":"Running cauchy_mle recursively","text":"","code":"theta_guess <- 10\nguess_vec <- numeric()\nreps <- 10  # play around with how many times you loop through. \n# you probably don't need very many times through to see what happens.\n# what happens might seem unsettling!\n\nfor(i in 1:reps){\n  theta_guess <- cauchy_mle(___, ___)  # what changes for each rep?\n  guess_vec[i] <- theta_guess\n}\nguess_vec\n\ndata.frame(guess_vec) %>%  # ggplot needs a data frame instead of a vector\n  ggplot(aes(y = guess_vec, x = 1:reps)) + \n  geom_line()  # line plot connecting the guesses for each rep"},{"path":"MLE.html","id":"reflection","chapter":"3 Maximum Likelihood Estimation","heading":"Reflection","text":"Just kicks, ’ve plotted derivative log-likelihood (red, trying find zero derivative log-likelihood) well two different guess trajectories (green start 10 blue start 3).\ngreen line (hard see, look right theta = 10) indicates get maximum close 10 start. blue line indicates happens initial value far 10 (MLE estimated infinite).’ve left R code, don’t worry understanding specific lines code. Instead focus understanding image . tell function maximum (.e., derivative goes zero!), x-value derivative equals zero difficult find. Using Newton-Raphson method requires initial guess quite close truth, otherwise, estimate go positive negative infinity.","code":"\nset.seed(470)\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\n\n\n# print the derivative of the log likelihood function\n# this is what you're trying to find the zero of\nthetas <-  seq(-100,50,by = .01)\nllfun <-  c()\nfor (k in 1:length(thetas))\n{\nllfun[k] <- 2*sum((observations-thetas[k])/(1+(observations-thetas[k])^2))\n}\n\n\n# draw on top two instances of Newton's method:\n# starting at theta_guess = 10  (RED)\ntheta_guess <- 10\ntheta_10 <- 10\nl1_vals_10 <- c(0)\nfor (i in 1:5)\n    {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_10[i + 1] <- theta_guess\n    l1_vals_10 <- c(l1_vals_10, l1, 0)\n    }\n\n\n# then starting at theta_guess = 3 (PURPLE)\ntheta_guess <- 3\ntheta_3 <- 3\nl1_vals_3 <- c(0)\nfor (i in 1:5)\n  {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_3[i + 1] <- theta_guess\n    l1_vals_3 <- c(l1_vals_3, l1, 0)\n}\n\n\nguesses_10 <- data.frame(thetas = rep(theta_10, each = 2),\n                        guess = c(l1_vals_10,0))\nguesses_3 <- data.frame(thetas = rep(theta_3, each = 2),\n                        guess = c(l1_vals_3,0))\ncauchy_data <- data.frame(thetas, llfun)\n\n\n\ncauchy_data %>%\n  ggplot() + \n  geom_hline(yintercept = 0) +\n  geom_line(aes(x = thetas, y = llfun, color = \"deriv loglik\")) +\n  geom_line(data = guesses_3, aes(x = thetas, y = guess, color = \"guesses, init is 3\")) +\n  geom_line(data = guesses_10, aes(x = thetas, y = guess, color = \"guesses, init is 10\")) +\n  xlim(c(-100, 50))"},{"path":"MLE.html","id":"odd-things-happen-sometimes","chapter":"3 Maximum Likelihood Estimation","heading":"Odd things happen sometimes","text":"’ve run code many many many times years. every time, initial guess super close 10, estimate doesn’t converge. just , set seed particular way, start value 3 actually converged answer! Randomness wild.","code":"\nset.seed(4747)\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\n\n\n# print the derivative of the log likelihood function\n# this is what you're trying to find the zero of\nthetas <-  seq(-100,50,by = .01)\nllfun <-  c()\nfor (k in 1:length(thetas))\n{\nllfun[k] <- 2*sum((observations-thetas[k])/(1+(observations-thetas[k])^2))\n}\n\n\n# draw on top two instances of Newton's method:\n# starting at theta_guess = 10  (RED)\ntheta_guess <- 10\ntheta_10 <- 10\nl1_vals_10 <- c(0)\nfor (i in 1:5)\n    {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_10[i + 1] <- theta_guess\n    l1_vals_10 <- c(l1_vals_10, l1, 0)\n    }\n\n\n# then starting at theta_guess = 3 (PURPLE)\ntheta_guess <- 3\ntheta_3 <- 3\nl1_vals_3 <- c(0)\nfor (i in 1:5)\n  {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_3[i + 1] <- theta_guess\n    l1_vals_3 <- c(l1_vals_3, l1, 0)\n}\n\n\nguesses_10 <- data.frame(thetas = rep(theta_10, each = 2),\n                        guess = c(l1_vals_10,0))\nguesses_3 <- data.frame(thetas = rep(theta_3, each = 2),\n                        guess = c(l1_vals_3,0))\ncauchy_data <- data.frame(thetas, llfun)\n\n\n\ncauchy_data %>%\n  ggplot() + \n  geom_hline(yintercept = 0) +\n  geom_line(aes(x = thetas, y = llfun, color = \"deriv loglik\")) +\n  geom_line(data = guesses_3, aes(x = thetas, y = guess, color = \"guesses, init is 3\")) +\n  geom_line(data = guesses_10, aes(x = thetas, y = guess, color = \"guesses, init is 10\")) +\n  xlim(c(-100, 50))"},{"path":"sampdist.html","id":"sampdist","chapter":"4 Sampling Distributions of Estimators","heading":"4 Sampling Distributions of Estimators","text":"statistic function observed random variables. sampling distribution statistic tells us values statistic assumes likely values . statistic function random variables \\(X_1, X_2, \\ldots, X_n\\), know distribution \\(X\\), principle, able derive distribution statistic.Example 4.1  data normal, sampling distribution mean also normal. result may look like Central Limit Theorem, limiting behavior . , sampling distribution \\(\\overline{X}\\) normal regardless sample size. proof come directly result linear combinations normal random variables also normal.\\[\\begin{eqnarray*}\nX_i &\\sim& N(\\mu, \\sigma^2)\\\\\n\\overline{X} &\\sim& N(\\mu, \\sigma^2 / n)\n\\end{eqnarray*}\\]","code":""},{"path":"sampdist.html","id":"the-chi-square-distribution","chapter":"4 Sampling Distributions of Estimators","heading":"4.1 The Chi-Square Distribution","text":"chi-square distribution probability distribution following characteristics.\\[\\begin{eqnarray*}\nX &\\sim& \\chi^2_n\\\\\nf_X(x) &=& \\frac{1}{2^{n/2} \\Gamma(n/2)} x^{n/2 -1} e^{-x/2} \\ \\ \\ \\ \\ \\ x > 0\\\\\nE[X] &=& n\\\\\nVar[X] &=& 2n\\\\\n\\psi_X(t) &=& \\Bigg( \\frac{1}{1-2t} \\Bigg)^{n/2} \\ \\ \\ \\ \\ \\ t < 1/2\\\\\n( &=&  E[e^{tX}] )\\\\\n\\end{eqnarray*}\\]Recall Moment Generating Functions \\((\\psi_X(t))\\), DeGroot Schervish (2011) page 205.Theorem 4.1  DeGroot Schervish (2011) 7.2.1Let \\(X_1, X_2, \\ldots X_k \\sim \\chi^2_{n_i}, \\ \\ =1, \\ldots, k\\), independently. , \\(X_1 + X_2 + \\cdots + X_k = \\sum_{=1}^k X_i \\sim \\chi^2_{n_1+n_2 +\\cdots+n_k}\\)., data independent chi-square random variables, sampling distribution sum also chi-square.Proof. \\[\\begin{eqnarray*}\nY&=& \\sum_{=1}^k X_i\\\\\n\\psi_Y (t) &=& E[e^{Yt} ]\\\\\n&=& E[e^{t \\sum_{=1}^k X_i}]\\\\\n&=& \\prod_{=1}^k E[e^{tX_i}]\\\\\n&=& \\prod_{=1}^k \\psi_{X_i}(t)\\\\\n&=& \\prod_{=1}^k \\bigg( \\frac{1}{1-2t} \\bigg)^{ n_i /2}\\\\\n&=& \\Bigg( \\frac{1}{1-2t} \\Bigg)^{\\sum n_i /2}\n\\end{eqnarray*}\\]\nSee theorem 4.4.3, pg 207.Theorem 4.2  DeGroot Schervish (2011) 7.2.1 1/2If \\(Z \\sim N(0,1), Y=Z^2,\\) \\(Y \\sim \\chi^2_1\\).Note result provide distribution transformation random variable. single data value \\((Z),\\) result provides distribution another single value, \\((Y).\\) value \\(Y\\) typically referred statistic summary observations.said, just , \\(Z\\) statistic (instead single value) \\(Z\\) \\(Y\\) sampling distributions!Proof. Let \\(\\Phi\\) \\(\\phi\\) cdf pdf Z.\nLet F f cdf pdf Y.\\[\\begin{eqnarray*}\nF_Y(y) &=& P(Y \\leq y) = P(Z^2 \\leq y)\\\\\n&=& P(-y^{1/2} \\leq Z \\leq y^{1/2})\\\\\n&=& \\Phi(y^{1/2}) -  \\Phi(- y^{1/2})  \\ \\ \\ \\ \\ y > 0\\\\\n\\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\nf_Y(y) &=& \\frac{\\partial F_Y(y)}{\\partial y} = \\phi(y^{1/2}) \\cdot \\frac{1}{2} y^{-1/2} - \\phi(-y^{1/2}) \\cdot \\frac{1}{2} -y^{-1/2}\\\\\n&=& \\frac{1}{2} y^{-1/2} ( \\phi(y^{1/2}) + \\phi(-y^{1/2})) \\ \\ \\ \\ \\ \\ y > 0\\\\\n\\mbox{know} && \\phi(y^{1/2}) = \\phi(-y^{1/2}) = \\frac{1}{\\sqrt{2 \\pi}} e^{-y/2}\\\\\n%\\therefore\nf_Y(y) &=& y^{-1/2} \\frac{1}{\\sqrt{2 \\pi}} e^{-y/2} \\ \\ \\ \\ \\ \\ y > 0 \\\\\n&=& \\frac{1}{2^{1/2}\\pi^{1/2}} y^{1/2 - 1} e^{-y/2} \\ \\ \\ \\ \\ \\ y >0\\\\\nY &\\sim& \\chi^2_1\\\\\n\\end{eqnarray*}\\]\nnote: \\(\\Gamma(1/2) = \\sqrt{\\pi}\\).combining Theorems 7.2.1 7.2.1 1/2, get:Theorem 4.3  DeGroot Schervish (2011) 7.2.2If \\(X_1, X_2, \\ldots, X_k \\stackrel{iid}{\\sim} N(0,1)\\),\n\\[\\begin{eqnarray*}\n\\sum_{=1}^k X_i^2 \\sim \\chi^2_k\n\\end{eqnarray*}\\]\nNote: \\(X_1, X_2, \\ldots, X_k \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\),\n\\[\\begin{eqnarray*}\n\\frac{X_i - \\mu}{\\sigma} &\\sim& N(0,1)\\\\\n\\sum_{=1}^k \\frac{(X_i - \\mu)^2}{\\sigma^2} &\\sim& \\chi^2_k\\\\\n\\end{eqnarray*}\\]data \\(iid\\) normal, sum squared values sampling distribution chi-square.","code":""},{"path":"sampdist.html","id":"independence-of-the-mean-and-variance-of-a-normal-random-variable","chapter":"4 Sampling Distributions of Estimators","heading":"4.2 Independence of the Mean and Variance of a Normal Random Variable","text":"Theorem 4.4  DeGroot Schervish (2011) 8.3.1Let \\(X_1, X_2, \\ldots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\).\\(\\overline{X}\\) \\(\\frac{1}{n} \\sum(X_i - \\overline{X})^2\\) independent. (true normal random variables. read proof book.)\\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\). (CLT, ?)\\(\\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\). (Main idea \\(n-1\\) independent things.)","code":""},{"path":"sampdist.html","id":"the-t-distribution","chapter":"4 Sampling Distributions of Estimators","heading":"4.3 The t-distribution","text":"Let \\(Z \\sim N(0,1)\\) \\(Y \\sim \\chi^2_n\\). \\(Z\\) \\(Y\\) independent, :Definition 4.1  \\[\\begin{eqnarray*}\nX = \\frac{Z}{\\sqrt{Y/n}} \\sim t_n \\mbox{  definition}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nf_X(x) &=& \\frac{\\Gamma(\\frac{n+1}{2})}{(n \\pi)^{1/2} \\Gamma(\\frac{n}{2})} (1 + \\frac{x^2}{n})^{-(n+1)/2} \\ \\ \\ \\ n > 2\\\\\nE[X] &=&0\\\\\nVar(X) &=& \\frac{n}{n-2}\n\\end{eqnarray*}\\]Let \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\), following hold:\n\\[\\begin{eqnarray*}\n\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\mbox{ independently } \\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\frac{\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2}/(n-1)}} &=& \\frac{\\overline{X} - \\mu}{\\sqrt{\\frac{\\sum(X_i - \\overline{X})^2}{n-1}/n}}\\\\\n&=& \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1} !\n\\end{eqnarray*}\\]stated , t-distribution defined distribution given standard normal divided square root chi-square random variable divided degrees freedom. may seem obtuse first glance, comes extremely handy standardizing sample mean using standard error (instead standard deviation) mean.Example 4.2  According investors, foreign stocks potential high yield, variability dividends may greater typical American companies. Let’s say take random sample 10 foreign stocks; assume also know population distribution American stocks come (.e., American parameters). believe foreign stock prices distributed similarly (normal mean variance) American stock prices, likely sample 10 foreign stocks produce standard deviation 50% bigger American stocks?\\[\\begin{eqnarray*}\nP(\\hat{\\sigma} / \\sigma > 1.5 ) &=& ?\\\\\n\\frac{\\sum (X_i - \\overline{X})^2}{\\sigma^2} &\\sim& \\chi^2_{n-1} \\ \\ \\ \\ \\mbox{(normality assumption)}\\\\\n\\frac{\\sum (X_i - \\overline{X})^2}{\\sigma^2} &=& n\\frac{\\sum (X_i - \\overline{X})^2/n}{\\sigma^2}\\\\\n&=& \\frac{n \\hat{\\sigma^2}}{\\sigma^2}\\\\\nP(\\hat{\\sigma} / \\sigma > 1.5 ) &=& P(\\hat{\\sigma}^2 / \\sigma^2 > 1.5^2 ) \\\\\n&=& P(n \\hat{\\sigma}^2 / \\sigma^2 > n 1.5^2 )\\\\\n&=& 1 - \\chi^2_{n-1} (n 1.5^2)\\\\\n&=& 1 - \\chi^2_{n-1} (22.5)\\\\\n&=& 1 - pchisq(22.5,9) = 0.00742 \\ \\ \\ \\mbox{ R}\n\\end{eqnarray*}\\]Example 4.3  Suppose take random sample foreign stocks (\\(\\mu\\) \\(\\sigma^2\\) unknown). Find value \\(k\\) sample mean \\(k\\) sample standard deviations \\((s)\\) mean \\(\\mu\\) probability 0.90.\nData: \\(n=10\\), \\(\\hat{\\mu} = \\overline{x}\\), \\(s^2 = \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\), \\(s = \\sigma'\\).\\[\\begin{eqnarray*}\nP(\\overline{X} < \\mu + k s) &=& 0.9\\\\\nP\\Bigg(\\frac{\\overline{X} - \\mu}{s} < k \\Bigg) = P\\Bigg(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} < k \\sqrt{n}\\Bigg) &=& 0.9\\\\\n\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} &\\sim& t_9\\\\\n\\sqrt{n} k &=& 1.383\\\\\nk &=& \\frac{1.383}{\\sqrt{10}} = 0.437\\\\\n\\mbox{note, R: } qt(0.9,9) &=& 1.383\n\\end{eqnarray*}\\]problem different known \\(\\sigma\\)? even wanted answer question terms number population standard deviations?","code":""},{"path":"sampdist.html","id":"reflection-questions-3","chapter":"4 Sampling Distributions of Estimators","heading":"4.4  Reflection Questions","text":"mean statistic sampling distribution?difference theoretical MSE empirical MSE (e.g., tank example )?can’t standard normal distribution used statistic interest \\(\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}}?\\)different tools used determine distribution random variable? (Note, chapter, majority random variables interest functions data, also called statistics.)","code":""},{"path":"sampdist.html","id":"ethics-considerations-3","chapter":"4 Sampling Distributions of Estimators","heading":"4.5  Ethics Considerations","text":"know estimator use consulting situation?respond someone tells “isn’t one right answer” previous question?technical conditions inference using t-distribution? , conditions data give rise t-distribution? happens technical conditions violated inference done anyway? [, inference means confidence intervals hypothesis testing.]","code":""},{"path":"sampdist.html","id":"r-code-tanks-example","chapter":"4 Sampling Distributions of Estimators","heading":"4.6 R code: Tanks Example","text":"can random sample integers 1 \\(N\\) (\\(N\\) unknown researcher) used estimate \\(N\\)? problem known German tank problem derived directly situation Allies used maximum likelihood estimation determine many tanks Axes produced. See .tanks numbered 1 \\(N\\).\nThink use data estimate \\(N\\).possible estimators \\(N\\) :7\\[\\begin{eqnarray*}\n\\hat{N}_1 &=& 2\\cdot\\overline{X} - 1 \\ \\ \\ \\mbox{MOM}\\\\\n\\hat{N}_2 &=& 2\\cdot \\mbox{median}(\\underline{X}) - 1 \\\\\n\\hat{N}_3 &=& \\max(\\underline{X})  \\ \\ \\ \\mbox{MLE}\\\\\n\\hat{N}_4 &=& \\frac{n+1}{n} \\max(\\underline{X})  \\ \\ \\ \\mbox{less biased version MLE}\\\\\n\\hat{N}_5 &=& \\max(\\underline{X}) + \\min(\\underline{X}) \\\\\n\\hat{N}_6 &=& \\frac{n+1}{n-1}[\\max(\\underline{X}) - \\min(\\underline{X})] \\\\\n\\end{eqnarray*}\\]","code":""},{"path":"sampdist.html","id":"theoretical-mean-squared-error","chapter":"4 Sampling Distributions of Estimators","heading":"4.6.1 Theoretical Mean Squared Error","text":"estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information expected value variance, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"sampdist.html","id":"empirical-mse","chapter":"4 Sampling Distributions of Estimators","heading":"4.6.2 Empirical MSE","text":"don’t need know theoretical expected value variance functions approximate MSE. can visualize sampling distributions also calculate actual empirical MSE estimator come .changing population size sample size, can assess estimators compare whether one particularly better given setting.","code":"\ncalculate_N <- function(nsamp,npop){\n  mysample =  sample(1:npop,nsamp,replace=F)  # what does this line do?\n  xbar2 <- 2 * mean(mysample) - 1\n  median2 <- 2 * median(mysample) - 1\n  samp.max <- max(mysample)\n  mod.max <- ((nsamp + 1)/nsamp) * max(mysample)\n  sum.min.max <- min(mysample) + max(mysample)\n  diff.min.max <- ((nsamp + 1)/(nsamp - 1)* (max(mysample) - min(mysample)))\n  data.frame(xbar2, median2, samp.max, mod.max, sum.min.max, diff.min.max,nsamp,npop)\n}\n\nreps <- 2\nnsamp_try <- c(10,100, 10, 100)\nnpop_try <- c(147, 147, 447, 447)\nmap_df(1:reps, ~map2(nsamp_try, npop_try, calculate_N))##    xbar2 median2 samp.max mod.max sum.min.max diff.min.max nsamp npop\n## 1 135.00     118      134  147.40         139     157.6667    10  147\n## 2 149.64     156      146  147.46         147     147.9293   100  147\n## 3 410.00     449      380  418.00         397     443.6667    10  447\n## 4 449.62     473      443  447.43         447     447.8687   100  447\n## 5 150.20     172      146  160.60         150     173.5556    10  147\n## 6 143.58     142      146  147.46         148     146.9091   100  147\n## 7 402.40     355      394  433.40         402     471.7778    10  447\n## 8 484.72     485      446  450.46         450     450.9293   100  447\nreps <- 1000\nresults <- map_df(1:reps, ~map2(nsamp_try, npop_try, calculate_N))\n\n# making the results long instead of wide:\nresults_long <- results %>%\n  pivot_longer(cols = xbar2:diff.min.max, names_to = \"estimator\", values_to = \"estimate\")\n\n# how is results different from results_long?  let's look at it:\nresults_long## # A tibble: 24,000 × 4\n##    nsamp  npop estimator    estimate\n##    <dbl> <dbl> <chr>           <dbl>\n##  1    10   147 xbar2            106.\n##  2    10   147 median2           94 \n##  3    10   147 samp.max          98 \n##  4    10   147 mod.max          108.\n##  5    10   147 sum.min.max       99 \n##  6    10   147 diff.min.max     119.\n##  7   100   147 xbar2            147.\n##  8   100   147 median2          150 \n##  9   100   147 samp.max         147 \n## 10   100   147 mod.max          148.\n## # … with 23,990 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nresults_long %>%\n  group_by(nsamp, npop, estimator) %>%\n  summarize(mean = mean(estimate), median = median(estimate), bias = mean(estimate - npop),\n            var = var(estimate), mse = (mean(estimate - npop))^2 + var(estimate))## `summarise()` has grouped output by 'nsamp', 'npop'. You can override using the\n## `.groups` argument.## # A tibble: 24 × 8\n## # Groups:   nsamp, npop [4]\n##    nsamp  npop estimator     mean median    bias    var    mse\n##    <dbl> <dbl> <chr>        <dbl>  <dbl>   <dbl>  <dbl>  <dbl>\n##  1    10   147 diff.min.max  148.   152.   0.756   369.   369.\n##  2    10   147 median2       148.   148    1.03   1548.  1549.\n##  3    10   147 mod.max       148.   151.   0.892   175.   175.\n##  4    10   147 samp.max      134.   138. -12.6     144.   302.\n##  5    10   147 sum.min.max   148.   148    1.00    316.   317.\n##  6    10   147 xbar2         147.   147.   0.257   661.   661.\n##  7    10   447 diff.min.max  453.   461.   5.53   3338.  3368.\n##  8    10   447 median2       446.   444   -0.8   14463. 14464.\n##  9    10   447 mod.max       451.   462    3.60   1480.  1493.\n## 10    10   447 samp.max      410.   420  -37.4    1223.  2619.\n## # … with 14 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nresults_long %>%\n  filter(npop == 147) %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = npop), color = \"red\") +\n  facet_grid(nsamp ~ estimator) + \n  ggtitle(\"sampling distributions of estimators of N, pop size = 147\")\nresults_long %>%\n  filter(npop == 447) %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = npop), color = \"red\") +\n  facet_grid(nsamp ~ estimator) + \n  ggtitle(\"sampling distributions of estimators of N, pop size = 447\")"},{"path":"bootdist.html","id":"bootdist","chapter":"5 Bootstrap Distributions","heading":"5 Bootstrap Distributions","text":"","code":""},{"path":"bootdist.html","id":"bootstrap-sampling-distributions","chapter":"5 Bootstrap Distributions","heading":"5.1 Bootstrap Sampling Distributions","text":"introducing bootstrap, let’s reflect work ’ve done far. Note one big goals understand variability data (samples) affects estimation. Consider different approaches ’ve taken understanding variability:Bayesian, \\(\\theta | \\underline{X} \\sim\\) according posterior distribution describes variability \\(\\theta\\) conditional data.Frequentist, focus variability statistic instead variability parameter.\n\\(f(x|\\theta)\\) known, can simulate data population use empirical sampling distribution see estimates vary sample sample. example, consider work tanks.\n\\(f(x|\\theta)\\) statistic lovely, can use mathematical theory. Mostly (far) “lovely” meant data normal statistic one produced either \\(\\chi^2\\) \\(t\\) distribution.\n\\(f(x|\\theta)\\) unknown \\(n\\) large statistic interest sample mean (note: proportion sample mean!), can use Central Limit Theorem.\n\\(f(x|\\theta)\\) unknown statistic arbitrary, use Bootstrap.\n\\(f(x|\\theta)\\) known, can simulate data population use empirical sampling distribution see estimates vary sample sample. example, consider work tanks.\\(f(x|\\theta)\\) statistic lovely, can use mathematical theory. Mostly (far) “lovely” meant data normal statistic one produced either \\(\\chi^2\\) \\(t\\) distribution.\\(f(x|\\theta)\\) unknown \\(n\\) large statistic interest sample mean (note: proportion sample mean!), can use Central Limit Theorem.\\(f(x|\\theta)\\) unknown statistic arbitrary, use Bootstrap.","code":""},{"path":"bootdist.html","id":"introduction","chapter":"5 Bootstrap Distributions","heading":"5.1.1 Introduction","text":"Main idea: able estimate variability estimator, using single random sample describe population (hey, ’s always usually one sample use!).’s strange get \\(\\hat{\\theta}\\) SE(\\(\\hat{\\theta}\\)) data (consider \\(\\hat{p}\\) & \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) \\(\\overline{X}\\) & \\(s/\\sqrt{n}\\)).’ll consider confidence intervals now.Bootstrapping doesn’t help get around small samples.following applets may helpful:Bootstrapping actual datasets: http://lock5stat.com/statkey/index.htmlBootstrapping contrast sampling population: https://www.rossmanchance.com/applets/OneSample.html?population=bootstrapThe logic confidence intervals: http://www.rossmanchance.com/applets/ConfSim.html","code":""},{"path":"bootdist.html","id":"basics-notation","chapter":"5 Bootstrap Distributions","heading":"5.1.2 Basics & Notation","text":"Let \\(\\theta\\) parameter interest, let \\(\\hat{\\theta}\\) estimate \\(\\theta\\). , ’d take lots samples size \\(n\\) population create \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples \\(F\\):\n\\[\\begin{eqnarray*}\n\\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}_b\n\\end{eqnarray*}\\]\nbest guess \\(\\theta\\). \\(\\hat{\\theta}\\) different \\(\\theta\\), call .\n\\[\\begin{eqnarray*}\nSE(\\hat{\\theta}) &=& \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B(\\hat{\\theta}_b - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\\nq_1 &=& [0.25 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\\nq_3 &=& [0.75 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\\n\\end{eqnarray*}\\], completely characterize sampling distribution (function \\(\\theta\\)) allow us make inference \\(\\theta\\) \\(\\hat{\\theta}\\).\nFigure 1.1: Image credit: Hesterberg: supplemental chapter Introduction Practice Statistics, 5th Edition Moore McCabe. https://www.timhesterberg.net/bootstrap--resampling\n","code":""},{"path":"bootdist.html","id":"the-plug-in-principle","chapter":"5 Bootstrap Distributions","heading":"The Plug-in Principle","text":"Recall\n\\[\\begin{eqnarray*}\nF(x) &=& P(X \\leq x)\\\\\n\\hat{F}(x) &=& S(x) = \\frac{\\# X_i \\leq x}{n}\n\\end{eqnarray*}\\]\n\\(\\hat{F}(x)\\) sufficient statistic \\(F(x)\\). , information \\(F\\) data contained \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) MLE \\(F(x)\\). (probabilities, ’s multinomial argument. similar binomial argument, maximization happens additional constraint probabilities sum one, Lagrangian multipliers used.)Note , general, interested parameter, \\(\\theta\\) functional \\(F\\) (functional means function).\n\\[\\begin{eqnarray*}\nt(F) = \\theta \\ \\ \\ \\ \\Bigg[\\mbox{e.g., } \\int x f(x) dx = \\mu \\Bigg]\n\\end{eqnarray*}\\]plug-estimate \\(\\theta\\) :\n\\[\\begin{eqnarray*}\nt(\\hat{F}) = \\hat{\\theta}  \\ \\ \\ \\ \\Bigg[\\mbox{e.g., }  \\sum_x x \\hat{f}(x) = \\sum_{=1}^n x_i \\bigg(\\frac{1}{n}\\bigg) =  \\frac{1}{n} \\sum x_i  = \\overline{x} \\Bigg]\n\\end{eqnarray*}\\]: estimate parameter, use statistic corresponding quantity sample.idea boostrapping (fact, bootstrap samples ), depends double arrow . must random sample: , \\(\\hat{F}\\) must good job estimating \\(F\\) order bootstrap concepts meaningful.\\[\\begin{eqnarray*}\n\\underline{\\mbox{Real World}} && \\underline{\\mbox{Boostrap World}}\\\\\nF \\rightarrow x &\\Rightarrow& \\hat{F} \\rightarrow x^*\\\\\n\\downarrow & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow\\\\\n\\hat{\\theta} & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}^*\n\\end{eqnarray*}\\]Note ’ve seen plug-principle :\n\\[\\begin{eqnarray*}\n\\sqrt{\\frac{p(1-p)}{n}} &\\approx& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\mbox{Fisher's Information: } (\\theta) &\\approx& (\\hat{\\theta})\n\\end{eqnarray*}\\]Okay, okay, haven’t seen Fisher’s Information yet, ’ll see weeks, plug-principle apply.","code":""},{"path":"bootdist.html","id":"the-bootstrap-idea","chapter":"5 Bootstrap Distributions","heading":"The Bootstrap Idea","text":"can resample sample represent samples actual population! boostrap distribution statistic, based many resamples, represents sampling distribution statistic based many samples. okay?? assuming?\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)","code":""},{"path":"bootdist.html","id":"bootstrap-procedure","chapter":"5 Bootstrap Distributions","heading":"Bootstrap Procedure","text":"Resample data replacement.Calculate statistic interest resample.Repeat 1. 2. \\(B\\) times.Use bootstrap distribution inference.","code":""},{"path":"bootdist.html","id":"bootstrap-notation","chapter":"5 Bootstrap Distributions","heading":"Bootstrap Notation","text":"Take lots (\\(B\\)) resamples sample size n sample, \\(\\hat{F}(x)\\) (instead population, \\(F(x)\\) ) create bootstrap distribution \\(\\hat{\\theta}^*\\) (instead sampling distribution \\(\\hat{\\theta}\\)).Let \\(\\hat{\\theta}^*_b\\) calculated statistic interest \\(b^{th}\\) bootstrap sample. best guess \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b\n\\end{eqnarray*}\\]\n(\\(\\hat{\\theta}^*\\) different \\(\\hat{\\theta}\\), call biased.) estimated value standard error estimate \n\\[\\begin{eqnarray*}\n\\hat{SE}^*_B = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*_b - \\hat{\\theta}^*)^2 \\bigg]^{1/2}\n\\end{eqnarray*}\\]Just like repeatedly taking samples population, taking resamples sample allows us characterize bootstrap distribution approximates sampling distribution. bootstrap distribution approximates shape, spread, & bias actual sampling distribution. bootstrap sampling distribution estimate center true sampling distribution.\nFigure 1.2: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig. left image represents mean n=50. center image represents mean n=9. right image represents median n=15.\n","code":""},{"path":"bootdist.html","id":"finding-the-bootstrap-distribution","chapter":"5 Bootstrap Distributions","heading":"5.1.3 Finding the Bootstrap Distribution","text":"Consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{eqnarray*}\nT^*(b) &=& \\frac{\\hat{\\theta}^*_b - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{eqnarray*}\\]\n\\(\\hat{\\theta}^*_b\\) value \\(\\hat{\\theta}\\) \\(b^{th}\\) bootstrap sample, \\(\\hat{SE}^*(b)\\) estimated standard error \\(\\hat{\\theta}^*_b\\) \\(b^{th}\\) bootstrap sample. \\(\\alpha/2^{th}\\) percentile \\(T^*(b)\\) estimated value \\(\\hat{q}^*_{\\alpha/2}\\) \n\\[\\begin{eqnarray*}\n\\frac{\\# \\{T^*(b) \\leq \\hat{q}^*_{\\alpha/2} \\} }{B} = \\alpha/2\n\\end{eqnarray*}\\]\nexample, \\(B=1000\\), estimate 5% point \\(50^{th}\\) smallest value \\(T^*(b)\\)s, estimate 95% point \\(950^{th}\\) smallest value \\(T^*(b)\\)s.find different SE bootstrapped dataset, bootstrap twice. algorithm follows:Generate \\(B\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*_b\\).Take \\(M\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Find \\(B\\) values \\(T^*(b)\\). Calculate \\(\\hat{q}^*_\\alpha\\) \\(\\hat{q}^*_{1-\\alpha}\\) (\\(\\alpha/2\\)).","code":""},{"path":"bootdist.html","id":"reflection-questions-4","chapter":"5 Bootstrap Distributions","heading":"5.2  Reflection Questions","text":"bootstrap? , bootstrapping provide?technical conditions bootstrapping?bootstrap sampling distribution?bootstrap standard error?mean statistic sampling distribution? can conceptualize sampling distribution (always!) one sample? can approximate sampling distribution (always!) one sample?","code":""},{"path":"bootdist.html","id":"ethics-considerations-4","chapter":"5 Bootstrap Distributions","heading":"5.3  Ethics Considerations","text":"friend tells planning bootstrap don’t enough data central limit theorem kick . Explain wrong logic.different methods ’ve used develop sampling distributions? one use setting?SE statistic valuable: skewed symmetric sampling distribution? ?dataset hand missing observations, remove missing data bootstrap bootstrap missing observations? answer “depends”? one missing data anyway????","code":""},{"path":"bootdist.html","id":"r-code-bootstrapping-survival-example","chapter":"5 Bootstrap Distributions","heading":"5.4 R code: Bootstrapping Survival Example","text":"many built functions R (Python, Matlab, Stata, etc. matter) bootstrap dataset create number standard bootstrap intervals. However, order understand bootstrap process, example uses loops repeated resample calculate statistics interest.Example 5.1  heroin survival timeHesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.Hesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.data include amount time subjects stayed facility treatment terminated (column 4).data include amount time subjects stayed facility treatment terminated (column 4).37% subjects, study ended still clinic (status=0).37% subjects, study ended still clinic (status=0).survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)","code":"\nlibrary(tidyverse)\nheroin <- readr::read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nnames(heroin)## [1] \"id\"     \"clinic\" \"status\" \"times\"  \"prison\" \"dose\"\nhead(heroin)## # A tibble: 6 × 6\n##      id clinic status times prison  dose\n##   <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>\n## 1     1      1      1   428      0    50\n## 2     2      1      1   275      1    55\n## 3     3      1      1   262      0    55\n## 4     4      1      1   183      0    30\n## 5     5      1      1   259      1    65\n## 6     6      1      1   714      0    55\nobs.stat <- heroin %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()\nobs.stat## [1] 378.3"},{"path":"bootdist.html","id":"bootstrapping-the-data","chapter":"5 Bootstrap Distributions","heading":"Bootstrapping the data","text":"","code":"\nset.seed(4747)\nheroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)\n\nheroin.bs %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()## [1] 372.2583"},{"path":"bootdist.html","id":"creating-a-bootstrap-sampling-distribution-for-the-trimmed-mean","chapter":"5 Bootstrap Distributions","heading":"Creating a bootstrap sampling distribution for the trimmed mean","text":"","code":"\nbs.test.stat<-c()    # placeholder, eventually B long, check after running!\nbs.sd.test.stat<-c() # placeholder, eventually B long, check after running!\n\nB <- 500\nM <- 100\nset.seed(4747)\nfor(b in 1:B){ \n  heroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)  # BS sample\n  bs.test.stat<-c(bs.test.stat, # concatenate each trimmed mean each time go through loop\n                  heroin.bs %>% \n                    summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  \n  bsbs.test.stat <- c() # refresh the vector of double BS test statistics\n  \n  for(m in 1:M){\n    heroin.bsbs<-heroin.bs %>% sample_frac(size=1, replace=TRUE) # BS of the BS!\n    bsbs.test.stat <- c(bsbs.test.stat, # concatenate the trimmed mean of the double BS\n                        heroin.bsbs %>% \n                          summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  }\n  bs.sd.test.stat<-c(bs.sd.test.stat, sd(bsbs.test.stat))\n}"},{"path":"bootdist.html","id":"what-do-the-data-distributions-look-like","chapter":"5 Bootstrap Distributions","heading":"What do the data distributions look like?","text":"","code":"\nggplot(heroin, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"original sample (n=238)\")\nggplot(heroin.bs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"one bootstrap sample (n=238)\")\nggplot(heroin.bsbs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"a bootstrap sample of the one bootstrap sample (n=238)\")"},{"path":"bootdist.html","id":"what-do-the-sampling-distributions-look-like","chapter":"5 Bootstrap Distributions","heading":"What do the sampling distributions look like?","text":"","code":"\nbs.stats <- data.frame(bs.test.stat)\nggplot(bs.stats, aes(x=bs.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of trimmed mean\") +  \n  xlab(paste(\"trimmed.mean=\",round(mean(bs.test.stat),2),\"; SE=\", round(sd(bs.test.stat),2)))"},{"path":"bootdist.html","id":"what-is-the-distribution-of-the-se-of-the-statistic","chapter":"5 Bootstrap Distributions","heading":"What is the distribution of the SE of the statistic?","text":"","code":"\nbs.SE <- data.frame(bs.sd.test.stat)\nggplot(bs.SE, aes(x=bs.sd.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of SE of trimmed means\") +  \n  xlab(paste(\"average SE=\",round(mean(bs.sd.test.stat),2)))"},{"path":"bootdist.html","id":"what-is-the-distribution-of-the-t-statistics","chapter":"5 Bootstrap Distributions","heading":"What is the distribution of the T statistics?","text":"","code":"\nbs.T <- data.frame(T.test.stat = (bs.test.stat - obs.stat) / bs.sd.test.stat)\nggplot(bs.T, aes(x=T.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of T statistics of trimmed means\") +  \n  xlab(paste(\"average T=\",round(mean(bs.T$T.test.stat),2)))"},{"path":"bootdist.html","id":"normal-ci-with-bs-se","chapter":"5 Bootstrap Distributions","heading":"95% normal CI with BS SE","text":"","code":"\nobs.stat + \n  qnorm(c(.025,.975))*\n  sd(bs.test.stat)## [1] 334.0961 422.5039"},{"path":"bootdist.html","id":"bootstrap-t-ci","chapter":"5 Bootstrap Distributions","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample).","code":"\nbs.t.hat<-(bs.test.stat - obs.stat)/bs.sd.test.stat\n\nbs.t.hat.95 = quantile(bs.t.hat, c(.975,.025))\n\nobs.stat - bs.t.hat.95*sd(bs.test.stat)##    97.5%     2.5% \n## 336.5108 426.8502"},{"path":"bootdist.html","id":"percentile-ci","chapter":"5 Bootstrap Distributions","heading":"95% Percentile CI","text":"","code":"\nquantile(bs.test.stat, c(.025, .975))##     2.5%    97.5% \n## 332.2373 422.3135"},{"path":"intest.html","id":"intest","chapter":"6 Interval Estimates","heading":"6 Interval Estimates","text":"now, ’ve used \\(\\hat{\\theta}\\) estimate \\(\\theta\\). single numerical value gives us information degree uncertainty estimate. confidence interval set values, (,B), think likely contain \\(\\theta\\) (true parameter). length interval gives us idea closely able estimate \\(\\theta\\).","code":""},{"path":"intest.html","id":"frequentist-confidence-intervals","chapter":"6 Interval Estimates","heading":"6.1 Frequentist Confidence Intervals","text":"frequentist confidence interval (CI) created way interval contains parameter specified percentage time.","code":""},{"path":"intest.html","id":"ci-for-the-mean-mu-in-a-normal-random-sample","chapter":"6 Interval Estimates","heading":"6.1.1 CI for the mean, \\(\\mu\\) in a normal random sample","text":"know,\n\\[\\begin{eqnarray*}\n\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}\n\\end{eqnarray*}\\]n.b., text uses \\(\\sigma' = s = \\sqrt{\\frac{\\sum_{=1}^{n}(X_i - \\overline{X})^2}{n-1}}.\\) whenever see \\(\\sigma'\\), think \\(s.\\)Let \\(c\\) constant \\(\\int_{-c}^c f_{t_{n-1}}(x) dx = \\gamma\\) (e.g., = 0.95). :\\[\\begin{eqnarray*}\nP(-c \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c) &=& 0.95\\\\\nP( \\overline{X} - c s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} + c s/\\sqrt{n} ) &=& 0.95\\\\\n\\end{eqnarray*}\\]random ? \\(\\overline{X}\\) \\(s\\) random! probability getting \\((\\overline{X}\\),\\(s)\\) \\(\\overline{X} - c s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} + c s/\\sqrt{n}\\) 0.95. \\(c?\\)frequentists, don’t interpret interval “probability \\(\\theta\\) interval”\n\\[\\begin{eqnarray*}\nX_1, X_2, \\ldots, X_n &\\rightarrow& \\mbox{random}\\\\\n\\theta &\\rightarrow& \\mbox{fixed}\\\\\n\\end{eqnarray*}\\]95% confident \\(\\mu\\) \\(\\overline{X} - c s/\\sqrt{n}\\) \\(\\overline{X} + c s/\\sqrt{n}\\)Bayesians interpret intervals “probability \\(\\theta\\) interval” Bayesians treat \\(\\theta\\) random variable (need prior, etc…) [See next section Bayesian Intervals.]Example 6.1  sample 25 statistics students reported spend average 110 minutes per week studying statistics, standard deviation 40 minutes. Find one-sided CI 98% confident know lower bound true average studying time population. Let’s assume data reasonably independent normally distributed.need interval \\([ ,\\infty)\\), 98% confidence, \\(\\mu\\) interval. know:\n\\[\\begin{eqnarray*}\nP(c_1 \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c_2) &=& 0.98\\\\\nP( \\overline{X} - c_2 s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} - c_1 s/\\sqrt{n} ) &=& 0.98\\\\\nP(-\\infty \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c_2) &=& 0.98\\\\\n\\\\\n\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} &\\sim& t_{24} \\rightarrow c_2 = 2.172\\\\\nP( \\overline{X} - 2.172 s/\\sqrt{n} \\leq \\mu (\\leq \\infty) ) &=& 0.98\\\\\n( \\overline{X} - 2.172 s/\\sqrt{n},\\infty ) && \\mbox{98% CI}\\\\\n\\end{eqnarray*}\\]\n98% confident true average studying time (population) least 92.62 minutes. can’t plug numbers keep probability?2-sided interval: \\(c=2.492=c_2, c_1 = -2.492\\)\n\\[\\begin{eqnarray*}\n( \\overline{X} - 2.492 s/\\sqrt{n},\\overline{X} + 2.492 s/\\sqrt{n} ) && \\mbox{98% CI}\\\\\n(90.06 \\mbox{ min}, 129.94 \\mbox{ min}) &&\\\\\n\\end{eqnarray*}\\]98% confident average number minutes per week spent studying population 90.06 min 129.94 min.","code":""},{"path":"intest.html","id":"bayesian-intervals","chapter":"6 Interval Estimates","heading":"6.2 Bayesian Intervals","text":"Note: book calls Bayesian intervals “posterior intervals”, stick language. However, people Bayesian literature call “credible intervals”.’d like say “probability \\(\\theta\\) interval …” Bayesian can Bayesians think \\(\\theta\\) random, put distribution \\(\\theta | \\underline{x}\\).Bayesian posterior credible interval given posterior distribution. , (\\(1-\\alpha\\))% posterior interval \\(\\theta\\) \\[\\Xi^{-1}_{\\alpha/2} (\\theta | \\underline{X}), \\Xi^{-1}_{1-\\alpha/2} (\\theta | \\underline{X})\\]\n\\(\\Xi(\\theta | \\underline{X})\\) posterior cumulative distribution function \\(\\theta\\) (maybe use better notation). focus \\(\\Xi\\) cdf notation. Instead, keep mind inverse cdf defines tail probabilities associated posterior distribution.Example 6.2  Recall Beta-Binomial example aims model true probability Steph Curry can make free throw, \\(\\theta.\\) Let’s say ’ll shoot 25 times record number times makes shot. Also, let’s say Beta(10,2) prior induces prior mean 10/12 quite wide.\nConstruct 90% posterior interval \\(\\theta.\\)\\[\\begin{eqnarray*}\nX &\\sim& \\mbox{Bin}(25, \\theta)\\\\\n\\theta &\\sim& \\mbox{Beta}(10, 2)\\\\\n\\theta | \\underline{X} &\\sim& \\mbox{Beta}(31, 6)\n\\end{eqnarray*}\\](n.b., look back Section 2 refresher get posterior distribution prior likelihood.)build posterior interval, need solve following equation:\\[P( \\_\\_\\_\\_\\_\\_\\_\\_\\_ \\leq \\theta \\leq \\_\\_\\_\\_\\_\\_\\_\\_\\_ \\ | \\ \\underline{X}) = 0.9\\]say: 0.9 probability true value \\(\\theta\\) (0.73, 0.92).","code":"\nmosaic::plotDist('beta', params = list(10, 2), main = \"Beta(10,2) pdf\")\nmosaic::xqbeta(c(0.05, 0.95), 31, 6)## [1] 0.730 0.925"},{"path":"intest.html","id":"joint-posterior-distribution-for-mu-and-sigma-in-a-normal-distribution","chapter":"6 Interval Estimates","heading":"6.2.1 Joint Posterior Distribution for \\(\\mu\\) and \\(\\sigma\\) in a Normal Distribution","text":"Remember, found posterior distribution \\(\\mu | \\underline{x}\\) known \\(\\sigma\\). don’t really ever know \\(\\sigma\\). find joint posterior \\(\\mu, \\sigma | \\underline{x}\\), need two priors.simplify calculations, let \\(\\tau = 1/\\sigma^2\\). \\(\\tau\\) called precision. joint distribution calculated using product marginal normal distributions. Note data assumed random sample (.e., independent identically distributed according \\(N(\\mu, 1/\\tau)\\) distribution).\\[\\begin{eqnarray*}\nf(x | \\mu, \\tau) &=& \\Bigg( \\frac{\\tau}{2 \\pi} \\Bigg) ^{1/2} exp \\bigg[ \\frac{-1}{2} \\tau (x-\\mu)^2 \\bigg]\\\\\nf(\\underline{x} | \\mu, \\tau) &=& \\Bigg( \\frac{\\tau}{2 \\pi} \\Bigg) ^{n/2} exp \\bigg[ \\frac{-1}{2} \\tau \\sum_{=1}^n (x_i-\\mu)^2 \\bigg]\\\\\n\\end{eqnarray*}\\]Theorem 6.1  (DeGroot Schervish (2011) Theorem 7.6.1)\nLet \\(X_1, X_2, \\ldots X_n \\sim N(\\mu, 1/\\tau)\\) suppose priors \\(\\mu|\\tau\\) \\(\\tau\\),\n\\[\\begin{eqnarray*}\n\\mu|\\tau &\\sim& N(\\mu_0, 1/(\\lambda_0 \\tau) )\\\\\n\\tau &\\sim& \\mbox{ Gamma} (\\alpha_0, \\beta_0)\n\\end{eqnarray*}\\], posteriors \\(\\mu|\\tau\\) \\(\\tau\\) ,\n\\[\\begin{eqnarray*}\n\\mu | \\tau, \\underline{x} &\\sim& N(\\mu_1, 1/(\\lambda_1 \\tau) )\\\\\n\\tau \\  | \\ \\underline{x}  &\\sim& \\mbox{ Gamma} (\\alpha_1, \\beta_1)\n\\end{eqnarray*}\\]\n\\(\\mu_1 = \\frac{\\lambda_0 \\mu_0 + n \\overline{x}}{\\lambda_0 + n}, \\ \\ \\ \\ \\lambda_1 = \\lambda_0 + n, \\ \\ \\ \\ \\alpha_1 = \\alpha_0 + \\frac{n}{2}, \\ \\ \\ \\ \\ \\beta_1 = \\beta_0 + \\frac{1}{2} \\sum_{=1}^n (x_i - \\overline{x})^2 + \\frac{n \\lambda_0 (\\overline{x} - \\mu_0)^2}{2(\\lambda_0 +n)}.\\)Note prior joint conjugate family distributions. \\(\\mu\\) \\(\\tau\\) normal-gamma distribution. Note also \\(\\mu\\) \\(\\tau\\) independent.Proof. \\[\\begin{align}\nf(\\underline{x} | \\mu, \\tau) &= \\Bigg( \\frac{\\tau}{2\\pi}\\Bigg)^{n/2} exp \\Bigg[ -\\frac{1}{2} \\tau \\sum_{=1}^n (x_i - \\mu)^2 \\Bigg] \\nonumber \\\\\n\\xi_1(\\mu|\\tau) &= \\Bigg( \\frac{\\lambda_0 \\tau}{2\\pi}\\Bigg)^{1/2} exp \\Bigg[ -\\frac{1}{2} \\lambda_0 \\tau (\\mu - \\mu_0)^2 \\Bigg]  \\nonumber \\\\\n\\xi_2(\\tau) &= \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\tau^{\\alpha_0 - 1}e^{-\\beta_0 \\tau} \\nonumber \\\\\n\\mbox{Note, } & \\mu  \\mbox{ $\\tau$ independent, } \\xi(\\mu, \\tau) = \\xi_1(\\mu|\\tau) \\ \\xi_2(\\tau) \\nonumber \\\\\n\\end{align}\\]\n\\[\\begin{align}\n\\xi(\\mu,\\tau|\\underline{x}) &\\propto f(\\underline{x} | \\mu, \\tau) \\ \\xi_1(\\mu|\\tau) \\ \\xi_2(\\tau) \\nonumber \\\\\n&\\propto \\tau^{\\alpha_0 + (n+1)/2 -1} \\exp \\Bigg[-\\frac{\\tau}{2} \\Bigg(\\lambda_0 [\\mu-\\mu_0]^2 + \\sum_{=1}^{n}(x_i -\\mu)^2 \\Bigg) - \\beta_0 \\tau \\Bigg]    \\tag{6.1} \\end{align}\\]Add subtract \\(\\overline{x}\\) inside \\((x_i -\\mu)^2\\) get:\\[\\begin{align}\n\\sum_{=1}^n(x_i -\\mu)^2 &= \\sum_{=1}^n(x_i - \\overline{x})^2 + n(\\overline{x} -\\mu)^2 \\tag{6.2}\n\\end{align}\\]adding subtracting \\(\\mu_1\\):\\[\\begin{align}\nn(\\overline{x} -\\mu)^2 + \\lambda_0 (\\mu - \\mu_0)^2 &= (\\lambda_0 + n)(\\mu - \\mu_1)^2 + \\frac{n\\lambda_0(\\overline{x} - \\mu_0)^2}{\\lambda_0 + n} \\tag{6.3}\n\\end{align}\\]Combining (6.2) (6.3) get:\\[\\begin{align}\n\\sum_{=1}^n(x_i -\\mu)^2 + \\lambda_0 (\\mu - \\mu_0)^2 = (\\lambda_0 + n)(\\mu - \\mu_1)^2 + \\sum_{=1}^n(x_i - \\overline{x})^2 + \\frac{n\\lambda_0(\\overline{x} - \\mu_0)^2}{\\lambda_0 + n} \\tag{6.4}\n\\end{align}\\]plugging (6.4) (6.1) get:Theorem 6.2  Let \\(X_1, X_2, \\ldots X_n \\sim N(\\mu, 1/\\tau)\\) suppose priors \\(\\mu|\\tau\\) \\(\\tau\\),\n\\[\\begin{eqnarray*}\n\\mu|\\tau &\\sim& N(\\mu_0, 1/(\\lambda_0 \\tau) )\\\\\n\\tau &\\sim& \\mbox{ Gamma} (\\alpha_0, \\beta_0)\n\\end{eqnarray*}\\], marginal posterior distribution \\(\\mu\\) can written :\n\\[\\begin{eqnarray*}\n\\bigg(\\frac{\\lambda_1 \\alpha_1}{\\beta_1} \\bigg)^{1/2} (\\mu-\\mu_1) \\  | \\ \\underline{x} \\sim t_{2\\alpha_1}\n\\end{eqnarray*}\\]\n\\(\\mu_1, \\lambda_1, \\alpha_1,\\) \\(\\beta_1\\) given previous theorem.Note: \\(E[ U | \\underline{x} ] = 0\\) \\(Var(U | \\underline{x}) = \\frac{2 \\alpha_1}{2 \\alpha_1 - 2} = \\frac{ \\alpha_1}{\\alpha_1 -1}\\)\\(\\rightarrow E[\\mu| \\underline{x}] = \\mu_1\\) \\(Var(\\mu | \\underline{x}) = \\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\frac{\\alpha_1}{\\alpha_1 -1} = \\frac{\\beta_1}{\\lambda_1(\\alpha_1 -1)}\\)","code":""},{"path":"intest.html","id":"posterior-interval-for-the-mean-mu-in-a-normal-random-sample","chapter":"6 Interval Estimates","heading":"6.2.2 Posterior Interval for the mean, \\(\\mu\\) in a normal random sample","text":"Let confidence level \\(1-\\alpha\\). frequentist CI, interval can built pivoting around value interest, \\(\\mu\\).\n\\[\\begin{eqnarray*}\nP( -c \\leq U \\leq c \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\nP( -c \\leq \\Bigg( \\frac{\\lambda_1 \\alpha_1}{\\beta_1} \\Bigg)^{1/2} (\\mu - \\mu_1) \\leq c \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\nP( \\mu_1 - c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2} \\leq \\mu \\leq \\mu_1 + c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2} \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\n\\end{eqnarray*}\\]\\(\\Rightarrow\\)    \\(\\mu_1 \\pm c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2}\\) (\\(1 - \\alpha\\))100% posterior interval \\(\\mu\\).Example 6.3  Let’s say trying estimate total number soft drinks particular vending machine sell typical week. want find 90% posterior interval \\(\\mu\\). prior information (e.g., past weeks) tells us:\n\\[\\begin{eqnarray*}\n\\mu | \\tau &\\sim& N(750, 5 / \\tau = \\frac{1}{(1/5)\\tau} )\\\\\n\\tau &\\sim& gamma(1, 45)\n\\end{eqnarray*}\\]\n\\(\\mu_0 = 750\\), \\(\\lambda_0 = 1/5\\), \\(\\alpha_0 = 1\\), \\(\\beta_0=45\\)random sample 10 weeks gives \\(\\overline{x} = 692\\) \\(s^2 = \\frac{14400}{9} = 1600\\).posterior parameters :\n\\[\\begin{eqnarray*}\n\\mu_1 &=& \\frac{\\lambda_0 \\mu_0 + n \\overline{x}}{\\lambda_0 + n} = \\frac{(1/5)750 + 6920}{(1/5) + 10} = 693.14\\\\\n\\lambda_1 &=& \\lambda_0 + n = 10.2\\\\\n\\alpha_1 &=& \\alpha_0 + n/2 = 1+ 5 =6\\\\\n\\beta_1 &=& \\beta_0 + \\frac{1}{2} \\sum_{=1}^n (x_i - \\overline{x})^2 + \\frac{n \\lambda_0 (\\overline{x} - \\mu_0)^2}{2(\\lambda_0 +n)} = 45 + 14400/2 + \\frac{10(1/5) (692-750)^2}{2(1/5 +10)} = 7574.8\n\\end{eqnarray*}\\]find 90% PI, find cutoff values \\(P(-c \\leq t_{2\\alpha_1} \\leq c) = 0.9\\). \\(2 \\alpha_1 =12\\), \\(P(t_{12} \\leq 1.782) = 0.95\\).\n\\[\\begin{eqnarray*}\nP(-1.782 \\leq U \\leq 1.782  \\  | \\ \\underline{x} ) &=& 0.9\\\\\nP(\\mu_1 - 1.782 (\\frac{\\beta_1}{\\lambda_1 \\alpha_1})^{1/2} \\leq \\mu \\leq \\mu_1 + 1.782 (\\frac{\\beta_1}{\\lambda_1 \\alpha_1})^{1/2} \\  | \\ \\underline{x} ) &=& 0.9\\\\\nP(673.31 \\leq \\mu \\leq 712.97 \\  | \\ \\underline{x} ) &=& 0.9\n\\end{eqnarray*}\\]Given prior beliefs data, 90% probability average number cans sold per week 673.31 712.97 cans.","code":""},{"path":"intest.html","id":"improper-priors-continued","chapter":"6 Interval Estimates","heading":"Improper priors, continued…","text":"Notice \\(\\mu_0 = \\beta_0 = \\lambda_0 = 0\\) \\(\\alpha_0= -1/2\\), \\(\\mu_1 = \\overline{x}\\), \\(\\lambda_1 = n\\), \\(\\alpha_1 = (n-1)/2\\), \\(\\beta_1 = \\sum(x_i - \\overline{x})^2 / 2\\). interval becomes:\\[\\begin{eqnarray*}\n\\overline{x} &\\pm& t^* \\Bigg( \\frac{(1/2) \\sum(x_i - \\overline{x})^2}{n (n-1)/2} \\Bigg)^{1/2}\\\\\n\\overline{x} &\\pm& t^* s/\\sqrt{n}\n\\end{eqnarray*}\\]improper prior -weights beliefs gives frequentist-like (.e., data ) answer Bayesian-like interpretation.","code":""},{"path":"intest.html","id":"bootstrap-confidence-intervals","chapter":"6 Interval Estimates","heading":"6.3 Bootstrap Confidence Intervals","text":"Notice use phrase “confidence intervals” describing intervals made using bootstrap process. logic interpretation bootstrap confidence intervals theory based frequentist confidence intervals.","code":""},{"path":"intest.html","id":"different-bootstrap-cis","chapter":"6 Interval Estimates","heading":"6.3.1 Different Bootstrap CIs","text":"many ways use bootstrap sampling distribution create interval estimate \\(\\theta\\). cover different approaches, keep mind techniques extend beyond cover .","code":""},{"path":"intest.html","id":"bootstrap-confidence-interval-logic","chapter":"6 Interval Estimates","heading":"6.3.1.1 Bootstrap Confidence Interval Logic","text":"Keep mind trying approximate sampling distribution \\(\\hat{\\theta}\\). fact, really able estimate sampling distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\). hope :\n\\[\\begin{eqnarray*}\n\\hat{F}\\bigg(\\frac{\\hat{\\theta}^*_b - \\hat{\\theta}}{\\hat{SE}^*_B} \\bigg) \\rightarrow F\\bigg(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\bigg)\n\\end{eqnarray*}\\]Recall derivation conventional confidence intervals (location parameter, like \\(\\mu\\)). somehow know distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\) (gives cutoff vales \\(q\\)), isolate \\(\\theta\\) middle probability statement. Keeping mind randomness probability statement comes endpoints, parameter.\n\\[\\begin{eqnarray*}\nP\\bigg(q_{(\\alpha/2)} \\leq \\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})} \\leq q_{(1-\\alpha/2)}\\bigg)&=& 1 - \\alpha\\\\\nP\\bigg(\\hat{\\theta} - q_{(1-\\alpha/2)} SE(\\hat{\\theta}) \\leq \\theta \\leq \\hat{\\theta} - q_{(\\alpha/2)} SE(\\hat{\\theta})\\bigg) &=& 1 - \\alpha\\\\\n\\end{eqnarray*}\\], ’s endpoints random, 0.95 probability ’ll get random sample produce endpoints capture true parameter.","code":""},{"path":"intest.html","id":"bs-se-confidence-interval","chapter":"6 Interval Estimates","heading":"6.3.1.2 BS SE Confidence Interval","text":"use BS SE within CI formula (standard normal values like 1.96 95% interval estimated quantiles). problem interval accurate distribution \\(\\hat{\\theta}\\) reasonably normal. bias skew, CI desired coverage levels (Efron Tibshirani 1993, pg 161 & chp 22).95% BS SE CI \\(\\theta\\): \\[\\hat{\\theta} \\pm 1.96 \\cdot \\hat{SE}^*_B\\]","code":""},{"path":"intest.html","id":"bs-t-confidence-interval","chapter":"6 Interval Estimates","heading":"6.3.1.3 BS-t Confidence Interval","text":"(note: BS-t -named process create intervals – just like t-intervals! use t-distribution BS-t intervals.)Previously, bootstrap used estimate (unknown) SE.\nNow consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\) (just SE \\(\\hat{\\theta}).\\)\n\\[\\begin{eqnarray*}\nT^*(b) &=& \\frac{\\hat{\\theta}^*_b - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{eqnarray*}\\]\\(\\hat{\\theta}^*_b\\) value \\(\\hat{\\theta}\\) \\(b^{th}\\) bootstrap sample, \\(\\hat{SE}^*(b)\\) estimated standard error \\(\\hat{\\theta}^*_b\\) \\(b^{th}\\) bootstrap sample. \\(\\alpha/2^{th}\\) percentile \\(T^*(b)\\) estimated value \\(\\hat{q}^*_{\\alpha/2}\\) \n\\[\\begin{eqnarray*}\n\\frac{\\# \\{T^*(b) \\leq \\hat{q}^*_{\\gamma} \\} }{B} = \\gamma\n\\end{eqnarray*}\\]example, \\(B=1000\\), estimate 5% point \\(50^{th}\\) smallest value \\(T^*(b)\\)s, estimate 95% point \\(950^{th}\\) smallest value \\(T^*(b)\\)s (two values used create 90% interval).make sense estimate \\(q_\\gamma\\) using \\(\\hat{q}^*_{\\gamma}?\\) Recall main condition bootstrapping :\\[\\begin{eqnarray*}\n\\hat{F}\\bigg(\\frac{\\hat{\\theta}^*_b - \\hat{\\theta}}{\\hat{SE}^*_B} \\bigg) \\rightarrow F\\bigg(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\bigg)\n\\end{eqnarray*}\\], needed distribution can approximated bootstrap distribution. remaining steps include algebra put parameter center functions data endpoints interval.Finally, bootstrap-t confidence interval :\n\\[\\begin{align}\n(\\hat{\\theta} - \\hat{q}^*_{1-\\alpha/2}\\hat{SE}^*_B,  \\hat{\\theta} - \\hat{q}^*_{\\alpha/2}\\hat{SE}^*_B) \\tag{6.5}\n\\end{align}\\]Note multiplier \\((q^*)\\) given \\(T^*(b)\\) requires separate estimate SE bootstrap sample. Accordingly, find bootstrap-t interval, bootstrap twice, see Figure 6.1. algorithm follows:Generate \\(B\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*_b\\).\\(\\rightarrow\\) Take \\(M\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Find \\(B\\) values \\(T^*(b)\\). Calculate \\(\\hat{q}^*_\\alpha\\) \\(\\hat{q}^*_{1-\\alpha}\\).Calculate CI equation (6.5).\nFigure 6.1: B bootstrap samples, take additional M bootstrap samples. goal estimate SE statistics B times.\nReflecting BS-t interval process, following considerations:\\(B\\cdot \\alpha\\) integer, use \\(k=\\lfloor (B+1) \\alpha \\rfloor\\) \\(B+1-k\\).Bootstrap-t intervals somewhat erratic can influenced outliers. Percentile methods reliable sampling distribution \\(\\hat{\\theta}\\) reasonably normal.\\(B=100\\) 200 usually enough bootstrap-t CI. However, \\(M=25\\) may enough estimate SE. \\(B=1000\\) needed computing percentiles either \\(\\hat{\\theta}^*_b\\) \\(\\hat{q}^*\\).choosing appropriate multiplier:\nnormal multiplier (\\(Z\\)) general, appropriate use, works \\(n\\) samples.\nbootstrap-t multiplier good sample .\nnormal multiplier (\\(Z\\)) general, appropriate use, works \\(n\\) samples.bootstrap-t multiplier good sample .resulting intervals typically symmetric (\\(\\hat{q}^*_\\alpha \\ne - \\hat{q}^*_{1-\\alpha}\\)). asymmetry part improvement \\(Z\\) \\(t\\) intervals.Bootstrap-t intervals good location statistics (mean, quantiles, trimmed means) trusted statistics like correlation.","code":""},{"path":"intest.html","id":"bootstrap-percentile-confidence-intervals","chapter":"6 Interval Estimates","heading":"6.3.1.4 Bootstrap Percentile Confidence Intervals","text":"interval \\(\\alpha/2\\) \\(1-\\alpha/2\\) quantiles bootstrap distribution statistic \\((1-\\alpha)100\\)% bootstrap percentile confidence interval corresponding parameter:\n\\[\\begin{eqnarray*}\n[\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}]  = [F^{-1}_{\\hat{\\theta}^*} (\\alpha/2), F^{-1}_{\\hat{\\theta}^*} (1 - \\alpha/2)]\n\\end{eqnarray*}\\]work? isn’t immediately obvious interval capture true parameter, \\(\\theta\\), rate 95%. Consider skewed sampling distribution. \\(\\hat{\\theta}\\) comes long tail, obvious short tail side CI get true parameter value correct rate? (Hall (1992) refers Efron’s “backwards” intervals.) see / percentiles intervals work, first start considering normal sampling distributions function statistic. Let \\(\\phi = g(\\theta), \\hat{\\phi} = g(\\hat{\\theta}), \\hat{\\phi}^* = g(\\hat{\\theta}^*)\\), \\(g\\) monotonic function (assume without loss generality \\(g\\) increasing). point choose (possible) \\(g(\\cdot)\\) \n\\[\\begin{eqnarray}\n\\hat{\\phi}^* - \\hat{\\phi} \\sim \\hat{\\phi} - \\phi \\sim N(0, \\sigma^2). \\tag{6.6}\n\\end{eqnarray}\\]\n, consider logic conventional frequentist confidence interval. \\(\\hat{\\phi} - \\phi \\sim N(0, \\sigma^2)\\), interval \\(\\theta\\) created :\n\\[\\begin{eqnarray}\nP( (\\hat{\\phi} - \\phi ) / \\sigma > z_\\alpha) &=& 1 - \\alpha \\nonumber\\\\\nP( \\hat{\\phi} - z_\\alpha \\sigma > \\phi) &=& 1 - \\alpha \\nonumber\\\\\n\\mbox{Interval } \\phi && (-\\infty, \\hat{\\phi} + \\sigma z_{1-\\alpha}) \\nonumber \\\\\n\\mbox{Interval } \\theta && (-\\infty, g^{-1}(\\hat{\\phi} + \\sigma z_{1-\\alpha})) \\tag{6.7}\n\\end{eqnarray}\\]\n\\(z_{1-\\alpha}\\) \\(100(1-\\alpha)\\) percent point standard normal distribution. Note interval \\(\\theta\\) equation (6.7) problematic requires knowledge \\(g\\) \\(\\sigma.\\) (Also, doesn’t use power bootstrap.)Equation (6.6) implies \\(\\hat{\\phi} + \\sigma z_{1-\\alpha} = F^{-1}_{\\hat{\\phi}^*}(1-\\alpha)\\). [? picture goes long way : draw normal curve describing sampling distribution \\(\\hat{\\phi}^* - \\hat{\\phi}\\).] assumption equation (6.6), know following:\\(95^{th}\\) percentile \\(\\hat{\\phi}^*- \\hat{\\phi}\\) \\(\\sigma \\cdot z_{1-\\alpha}\\)\\(95^{th}\\) percentile \\(\\hat{\\phi}^*\\) \\(\\hat{\\phi} + \\sigma \\cdot z_{1-\\alpha}\\)\\(95^{th}\\) percentile \\(g(\\hat{\\theta}^*)\\) \\(\\hat{\\phi} + \\sigma \\cdot z_{1-\\alpha}\\)\\(95^{th}\\) percentile \\(\\hat{\\theta}^*\\) \\(g^{-1}(\\hat{\\phi} + \\sigma \\cdot z_{1-\\alpha})\\)Using probability statements instead percentiles, get following. Note key want \\(\\theta\\) middle probability statement (\\(\\hat{\\theta}),\\) happens leveraging normal distribution condition distribution (equation (6.6)). Note \\(z_\\alpha = - z_{1-\\alpha}.\\)\\[\\begin{align}\n1-\\alpha &= P( z_\\alpha <(\\hat{\\phi} - \\phi ) / \\sigma  ) \\\\\n&= P(\\phi < \\hat{\\phi} - z_\\alpha \\sigma  ) \\\\\n&= P( \\phi <F^{-1}_{\\hat{\\phi}^*}(1-\\alpha) ) \\\\\n&= P(g^{-1}(\\phi)  <g^{-1}(F^{-1}_{\\hat{\\phi}^*}(1-\\alpha))  ) \\\\\n&= P(\\theta < F^{-1}_{\\hat{\\theta}^*}(1-\\alpha) ) \\\\\n\\end{align}\\]results percentile interval \\(\\theta\\)8,\n\\[\\begin{eqnarray}\n(-\\infty, F^{-1}_{\\hat{\\theta}^*}(1-\\alpha)).\n\\end{eqnarray}\\]tangible example statistic might transformation resulting normal sampling distribution, consider sample correlation. Fisher worked correlation coefficient two interesting results.Example 6.4  Consider population 82 law schools. Two measurements made entering class school (1973!). LSAT, average score class national law test, GPA, average undergraduate grade-point average class. random sample 15 schools selected population, correlation GPA LSAT score found 0.776.perfect world, proceed think problem? want know? want say population? ’d want know sampling distribution r…\\(\\hat{\\theta}.\\)Fisher (1915) proved expected value correlation coefficient based random sampling normal population approximately:\n\\[\\begin{eqnarray*}\nE[r] = \\rho - \\frac{\\rho(1-\\rho^2)}{2n}\n\\end{eqnarray*}\\]\nSolving \\(\\rho\\) gives approximately unbiased estimator population correlation:\n\\[\\begin{eqnarray*}\n\\hat{\\rho} = r \\Big[ 1 + \\frac{(1-r^2)}{2n} \\Big]\n\\end{eqnarray*}\\]\nwork done (Fisher (1915), Kenny Keeping (1951), Sawkins (1944), Olkin Pratt (1958)) later recommend using\n\\[\\begin{eqnarray*}\n\\hat{\\rho} = r \\Big[ 1 + \\frac{(1-r^2)}{2(n-3)} \\Big]\n\\end{eqnarray*}\\]\nNote bias decreases \\(n\\) increases \\(\\rho\\) approaches zero. Note also data distributed normally:\n\\[\\begin{eqnarray*}\nSE(r) = \\frac{(1-\\rho^2)}{\\sqrt{n-2}}\n\\end{eqnarray*}\\]\n(data distribution leads simple formula SE correlation.)Fisher (1915) proved expected value correlation coefficient based random sampling normal population approximately:\n\\[\\begin{eqnarray*}\nE[r] = \\rho - \\frac{\\rho(1-\\rho^2)}{2n}\n\\end{eqnarray*}\\]\nSolving \\(\\rho\\) gives approximately unbiased estimator population correlation:\n\\[\\begin{eqnarray*}\n\\hat{\\rho} = r \\Big[ 1 + \\frac{(1-r^2)}{2n} \\Big]\n\\end{eqnarray*}\\]\nwork done (Fisher (1915), Kenny Keeping (1951), Sawkins (1944), Olkin Pratt (1958)) later recommend using\n\\[\\begin{eqnarray*}\n\\hat{\\rho} = r \\Big[ 1 + \\frac{(1-r^2)}{2(n-3)} \\Big]\n\\end{eqnarray*}\\]\nNote bias decreases \\(n\\) increases \\(\\rho\\) approaches zero. Note also data distributed normally:\n\\[\\begin{eqnarray*}\nSE(r) = \\frac{(1-\\rho^2)}{\\sqrt{n-2}}\n\\end{eqnarray*}\\]\n(data distribution leads simple formula SE correlation.)Fisher also introduced \\(r\\) \\(Z\\) transformation:\n\\[\\begin{eqnarray*}\nZ = \\frac{1}{2} \\ln \\Big[\\frac{1+r}{1-r}\\Big]\n\\end{eqnarray*}\\]\nthink non-linear transformation normalizes sampling distribution r. (Note: inverse hyperbolic tangent function.)Fisher also introduced \\(r\\) \\(Z\\) transformation:\n\\[\\begin{eqnarray*}\nZ = \\frac{1}{2} \\ln \\Big[\\frac{1+r}{1-r}\\Big]\n\\end{eqnarray*}\\]\nthink non-linear transformation normalizes sampling distribution r. (Note: inverse hyperbolic tangent function.)see Fisher? (data normal), exists transformation normalizes sampling distribution correlation!!formal proof derives percentile intervals given :Percentile interval lemma (Efron Tibshirani 1993, pg 173) Suppose transformation \\(\\hat{\\phi} = m(\\hat{\\theta})\\) perfectly normalizes distribution \\(\\hat{\\theta}\\):\n\\[\\begin{eqnarray*}\n\\hat{\\phi} \\sim N (\\phi, 1)\n\\end{eqnarray*}\\]\npercentile interval based \\(\\hat{\\theta}\\) equals \\([m^{-1}(\\hat{\\phi} - z_{1-\\alpha/2} ), m^{-1}(\\hat{\\phi} - z_{\\alpha/2} )]\\).can approximate \\([m^{-1}(\\hat{\\phi} - z_{1-\\alpha/2} ), m^{-1}(\\hat{\\phi} - z_{\\alpha/2} )]\\) using \\([\\hat{\\theta}^*_{\\alpha/2}, \\hat{\\theta}^*_{1-\\alpha/2}]\\)order percentile interval appropriate, simply need know normalizing transformation exists. need actually find transformation! [complete disclosure, transformation doesn’t normal distribution. must monotonic transformation distribution symmetric zero.]Example 6.5  Charlotte Chang’s (Prof Chang Bio!) example bootstrapping loess smooth thesis data.  idea : Prof Chang data wanted model (using differential equations, DE). asked tell whether new model reflective data / population. model applied repeated simulations can seen black / red lines. Additionally, fit loess spline see shape data (blue line). bootstrapped data fit 1000 loess splines. Using percentile CI method, created CI population loess spline fit (green lines). DE model (confidence within red bounds) doesn’t seem terrible fit, definitely seems say something different relationship year pipits data / bootstrapping (confidence within green bounds).\nFigure 1.4: Model number birds per route function time. Blue line loess fit; green lines bootstrap CI loess fit. differential expression model given red CI bounds model simulation (bootstrapping).\nKeeping mind theory ’ve covered doesn’t exactly work situation (work simple parameter estimation), can imagine many ideas ’ve talked apply Charlotte’s situation. precise analysis, need careful multiple comparisons non-independent data values.","code":""},{"path":"intest.html","id":"bca-confidence-interval","chapter":"6 Interval Estimates","heading":"6.3.1.5 BCa Confidence Interval","text":"(Fall 2022: won’t cover BCa intervals.)percentile method, ’ve assumed exists transformation \\(\\theta\\), \\(\\phi(\\theta)\\), \n\\[\\begin{eqnarray*}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(0,1)\n\\end{eqnarray*}\\]\ntransformation assumes neither \\(\\theta\\) \\(\\phi\\) biased, assumes variance constant values parameter. , percentage intervals, assume normalizing transformation creates sampling distribution unbiased variance stabilizing. Consider monotone transformation normalizes sampling distribution (longer assume unbiased constant variance).now consider case \\(\\theta\\) biased estimator. :\n\\[\\begin{eqnarray*}\n\\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{c} \\sim N(-z_0,1)\n\\end{eqnarray*}\\]\n’ve corrected bias, non-constant variance, need adjustment stabilize variance:\\[\\begin{eqnarray*}\n\\phi(\\hat{\\theta}) - \\phi(\\theta) \\sim N(-z_0 \\sigma_\\phi,\\sigma_\\phi), \\ \\ \\ \\ \\ \\ \\sigma_\\phi = 1 + \\phi\n\\end{eqnarray*}\\]\n, must exist monotone transformation \\(\\phi\\) \\(\\phi(\\hat{\\theta}) \\sim N\\) \n\\[\\begin{eqnarray*}\nE(\\phi(\\hat{\\theta})) = \\phi(\\theta) - z_0 [1 + \\phi(\\theta)] && SE(\\phi(\\hat{\\theta})) = 1 + \\phi(\\theta)\n\\end{eqnarray*}\\]\n(Note: expected value SE ’ve assumed \\(c=1\\). \\(c\\ne1\\), can always choose different transformation, \\(\\phi'\\) \\(c=1\\).) \n\\[\\begin{eqnarray*}\nP(z_{\\alpha/2} \\leq \\frac{\\phi(\\hat{\\theta}) - \\phi(\\theta)}{1 + \\phi(\\theta)} + z_0 \\leq z_{1-\\alpha/2}) = 1 - \\alpha\n\\end{eqnarray*}\\]\n(\\(1-\\alpha\\))100% CI \\(\\phi(\\theta)\\) \n\\[\\begin{eqnarray*}\n\\bigg[ \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{1 + (z_{1-\\alpha/2} - z_0)}, \\frac{\\phi(\\hat{\\theta}) - (z_{\\alpha/2} - z_0)}{1 + (z_{\\alpha/2} - z_0)} \\bigg]\n\\end{eqnarray*}\\]\nLet’s consider interesting probability question:\n\\[\\begin{eqnarray*}\nP\\bigg( \\phi(\\hat{\\theta}^*) &\\leq& \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg) = ?\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq& \\frac{\\phi(\\hat{\\theta}) - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta}) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq& \\frac{ - (z_{1-\\alpha/2} - z_0) - \\phi(\\hat{\\theta})(z_{1-\\alpha/2} - z_0)}{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq& \\frac{ -(1+\\phi(\\hat{\\theta})) (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))(1+\\phi(\\hat{\\theta}))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq& \\frac{ - (z_{1-\\alpha/2} - z_0) }{(1 + (z_{1-\\alpha/2} - z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} &\\leq& \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} \\bigg)\\\\\n= P\\bigg( \\frac{\\phi(\\hat{\\theta}^*) - \\phi(\\hat{\\theta})}{1 + \\phi(\\hat{\\theta})} + z_0 &\\leq& \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n= P\\bigg( Z &\\leq& \\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg) = \\gamma_1\\\\\n\\mbox{} \\gamma_1 &=& \\Phi \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n&=& \\verb;pnorm; \\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\n\\end{eqnarray*}\\]’ve shown \\(\\gamma_1\\) quantile \\(\\phi(\\hat{\\theta}^*)\\) sampling distribution good estimate lower bound confidence interval \\(\\phi(\\theta)\\). Using argument upper bound, find \\((1-\\alpha)\\) 100% confidence interval \\(\\phi(\\theta)\\) :\\[\\begin{eqnarray*}\n&&[\\phi(\\hat{\\theta}^*)_{\\gamma_1}, \\phi(\\hat{\\theta}^*)_{\\gamma_2}]\\\\\n&& \\\\\n\\mbox{} \\gamma_1 &=& \\Phi\\bigg(\\frac{ (z_{\\alpha/2} + z_0) }{(1 - (z_{\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\gamma_2 &=& \\Phi \\bigg(\\frac{ (z_{1-\\alpha/2} + z_0) }{(1 - (z_{1-\\alpha/2} + z_0))} + z_0 \\bigg)\\\\\n\\end{eqnarray*}\\]Using transformation respecting property percentile intervals, know \\((1-\\alpha)\\) 100% confidence interval \\(\\theta\\) :\\[\\begin{eqnarray*}\n&&[\\hat{\\theta}^*_{\\gamma_1}, \\hat{\\theta}^*_{\\gamma_2}]\n\\end{eqnarray*}\\]estimate \\(\\) \\(z_0\\)?bias:\n\\(z_0\\) measure bias. Recall:\n\\[\\begin{eqnarray*}\nbias &=& E(\\hat{\\theta}) - \\theta\\\\\n\\hat{bias} &=& \\hat{\\theta}^* - \\hat{\\theta}\\\\\n\\end{eqnarray*}\\]remember \\(z_0\\) represents bias \\(\\phi(\\hat{\\theta})\\), \\(\\hat{\\theta}\\) (don’t know \\(\\phi\\)!). , use \\(\\theta\\) see proportion \\(\\theta\\) values low, can map back \\(\\phi\\) space using normal distribution:\n\\[\\begin{eqnarray*}\n\\hat{z}_0 &=& \\Phi^{-1} \\bigg( \\frac{ \\# \\hat{\\theta}^*_b < \\hat{\\theta}}{B} \\bigg)\n\\end{eqnarray*}\\]\n, \\(\\hat{\\theta}^*\\) underestimates \\(\\hat{\\theta}\\), \\(\\hat{\\theta}\\) likely underestimates \\(\\theta\\); \\(z_0 > 0\\). think \\(z_0\\) normal quantile associated proportion BS replicates less \\(\\hat{\\theta}\\).skew:\n\\(\\) measure skew.\n\\[\\begin{eqnarray*}\nbias&=& E(\\hat{\\theta} - \\theta)\\\\\nvar &=& E(\\hat{\\theta} - \\theta)^2 = \\sigma^2\\\\\nskew &=& E(\\hat{\\theta} - \\theta)^3 / \\sigma^3\\\\\n\\end{eqnarray*}\\]\ncan think skew rate chance standard error normalized scale. skew, estimate \\(=0\\). estimate \\(\\) comes procedure known jackknife.\n\\[\\begin{eqnarray*}\n\\hat{} = \\frac{\\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^3}{6 [ \\sum_{=1}^n (\\hat{\\theta} - \\hat{\\theta}_{()})^2 ] ^{3/2}}\n\\end{eqnarray*}\\]","code":""},{"path":"intest.html","id":"what-makes-a-ci-good","chapter":"6 Interval Estimates","heading":"6.3.2 What makes a CI good?","text":"transformation respecting property CI transformation respecting , monotone transformation, CI transformed parameter simply transformed CI unstransformed parameter. Let \\(\\phi = m(\\theta)\\).\n\\[\\begin{eqnarray*}\n[\\phi_{lo}, \\phi_{}] = [m(\\theta_{lo}), m(\\theta_{})]\n\\end{eqnarray*}\\]\nNote idea process creating CI. , create confidence interval using \\(\\phi\\), ’ll get thing created CI using \\(\\theta\\) transformed . straightforward see percentile CI transformation respecting. , monotone transformation statistic parameter, CI transformed appropriately.Let\n\\[\\begin{eqnarray*}\n\\hat{\\phi} &=& 0.5 \\ln\\bigg(\\frac{1+r}{1-r}\\bigg)\\\\\nr &=&\\frac{e^{2\\phi}+1}{e^{2\\phi}-1}\\\\\n\\end{eqnarray*}\\]know \\(\\hat{\\phi}\\) approximate normal distribution. , percentile CI \\(\\phi\\) approximate normal theory CI know correct (given \\(\\alpha).\\) CI \\(\\phi\\) can find CI \\(\\rho\\) taking inverse monotonic transformation; rather… can just use r percentile CI start !range preserving property Another advantage percentile interval range preserving. , CI always produces endpoints fall within allowable range parameter.Bias percentile interval , however, perfect. statistic biased estimator parameter, exist transformation distribution centered around correct function parameter. Formally, \n\\[\\begin{eqnarray*}\n\\hat{\\theta} \\sim N(\\theta + bias, \\hat{SE}^2)\n\\end{eqnarray*}\\]\ntransformation \\(\\phi = m(\\theta)\\) can fix things . Keep mind standard intervals can fail variety ways, percentile method simply fixed situation distribution non-normal.Symmetry (important??): interval symmetric, pivotal around value. necessarily good thing. Maybe bad thing force?Robust: BS-t particularly robust (can make robust variance stabilizing transformation)Range preserving: CI always contains values fall within allowable range (\\(p, \\rho\\),…)Transformation respecting: monotone transformation, \\(\\phi = m(\\theta)\\), interval \\(\\theta\\) mapped \\(m(\\theta)\\). \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) \\((1-\\alpha)100\\)% interval \\(\\theta\\), \n\\[\\begin{eqnarray*}\n[\\hat{\\phi}_{(lo)},\\hat{\\phi}_{(hi)}] = [m(\\hat{\\theta}_{(lo)}),m(\\hat{\\theta}_{(hi)})]\n\\end{eqnarray*}\\]\nexactly interval.Correct level confidence: central (symmetric) confidence interval, \\([\\hat{\\theta}_{(lo)},\\hat{\\theta}_{(hi)}]\\) probability \\(\\alpha/2\\) covering \\(\\theta\\) :\n\\[\\begin{eqnarray*}\nP(\\theta < \\hat{\\theta}_{(lo)})&=&\\alpha/2\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=&\\alpha/2\\\\\n\\end{eqnarray*}\\]Note: bootstrap intervals approximate. judge based accurately cover \\(\\theta\\). (\\(const_{lo}\\) \\(const_{hi}\\) constant values vary depending data, statistic, method. constants necessarily upper miss-rate lower miss-rate.) point whether coverage rate converges rate \\(1/n\\) rate \\(1/\\sqrt{n}.\\)CI first order accurate :\n\\[\\begin{eqnarray*}\nP(\\theta < \\hat{\\theta}_{(lo)})&=&\\alpha/2 + \\frac{const_{lo}}{\\sqrt{n}}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=&\\alpha/2+ \\frac{const_{hi}}{\\sqrt{n}}\\\\\n\\end{eqnarray*}\\]CI second order accurate :\n\\[\\begin{eqnarray*}\nP(\\theta < \\hat{\\theta}_{(lo)})&=&\\alpha/2 + \\frac{const_{lo}}{n}\\\\\nP(\\theta > \\hat{\\theta}_{(hi)})&=&\\alpha/2+ \\frac{const_{hi}}{n}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"intest.html","id":"advantages-and-disadvantages","chapter":"6 Interval Estimates","heading":"6.3.2.1 Advantages and Disadvantages","text":"BS SE IntervalAdvantages\nsimilar familiar parametric approach; useful normally distributed \\(\\hat{\\theta}\\); requires least computation (\\(B=50-200\\))Disadvantages\nfails use entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\)Bootstrap-t Confidence IntervalAdvantages\nhighly accurate CI many cases; handles skewed \\(F(\\hat{\\theta})\\) better percentile method;Disadvantages\ninvariant transformations; computationally expensive double bootstrapPercentile IntervalAdvantages\nuses entire \\(\\hat{F}^*(\\hat{\\theta}^*)\\); allows \\(F(\\hat{\\theta})\\) asymmetrical; invariant transformations; range respecting; simple executeDisadvantages\nsmall samples may result low accuracy (dependence tail behavior); assumes \\(\\hat{F}^*(\\hat{\\theta}^*)\\) unbiasedBCa IntervalAdvantages\npercentile method; allows bias \\(\\hat{F}^*(\\hat{\\theta}^*)\\); \\(z_0\\) can calculated easily \\(\\hat{F}^*(\\hat{\\theta}^*)\\)Disadvantages\nrequires limited parametric assumption; can complicated compute","code":""},{"path":"intest.html","id":"bootstrap-ci-and-hypothesis-testing","chapter":"6 Interval Estimates","heading":"6.3.3 Bootstrap CI and Hypothesis Testing","text":"(Fall 2022: won’t cover bootstrap hypothesis testing.)class seen many times null value parameter contained CI, reject null hypothesis; similarly, reject null value lie inside CI. Using BS CIs, can apply logic, test hypothesis interest (note: can always create one-sided intervals well!). simply using CIs leaves p-value information. get p-value CI? Consider alternative definition p-value:p-value: smallest level significance reject \\(H_0\\)., want null value (\\(\\theta_0\\)) one endpoints confidence interval level confidence \\(1-2\\alpha_0\\). \\(\\alpha_0\\) one-sided p-value, \\(2\\alpha_0\\) two-sided p-value.percentile intervals,\n\\[\\begin{eqnarray*}\np-value = \\alpha_0 = \\frac{\\# \\hat{\\theta}^*_b < \\theta_0}{B}\n\\end{eqnarray*}\\]\n(without loss generality, assuming set \\(\\hat{\\theta}^*_{lo} = \\theta_0\\)).","code":""},{"path":"intest.html","id":"reflection-questions-5","chapter":"6 Interval Estimates","heading":"6.4  Reflection Questions","text":"frequentist confidence interval, parameter endpoints interval random?correct interpretation frequentist confidence interval?might create one-sided interval vs two-sided interval?need first find posterior distribution order create Bayesian posterior (credible) intervalWhat correct interpretation Bayesian posterior (credible) interval?marginal posterior distribution \\(\\mu\\) situation \\(\\mu\\) \\(\\sigma^2\\) unknown (prior distributions )?use marginal posterior distribution \\(\\mu\\) create posterior (credible) interval?previous chapters, used : different ways determine distribution statistic?","code":""},{"path":"intest.html","id":"ethics-considerations-5","chapter":"6 Interval Estimates","heading":"6.5  Ethics Considerations","text":"wrong following interpretations CI:\n0.9 prob true average number chips 3.7 & 17.2.\n90% cookies 3.7 & 17.2 chips.\n90% confident sample, sample average number chips 3.7 17.2.\nmany repeated samples, 90% sample averages 3.7 17.2.\n0.9 prob true average number chips 3.7 & 17.2.90% cookies 3.7 & 17.2 chips.90% confident sample, sample average number chips 3.7 17.2.many repeated samples, 90% sample averages 3.7 17.2.technical conditions creating confident intervals using t-distribution? , conditions data give rise t-distribution? happens technical conditions violated confidence interval created anyway?much influence prior resulting posterior (credible) interval? (Hint: answer “depends.”)","code":""},{"path":"intest.html","id":"r-code-creating-interval-estimates","chapter":"6 Interval Estimates","heading":"6.6 R code: Creating Interval Estimates","text":"","code":""},{"path":"intest.html","id":"finding-cutoffs","chapter":"6 Interval Estimates","heading":"6.6.1 Finding cutoffs","text":"Recall q distributional functions (e.g., qnorm(), qbinom(), qunif(), qchisq(), qt()) indicates output quantile.mosiac package adds x front function name allows figure accompany numerical value quantile. highly recommend drawing pictures finding quantiles percentages.One-sided 98% t-interval \\((df = 24)\\) 98% probability left quantile interest. Note t-distribution symmetric.Two-sided 98% t-interval \\((df = 24)\\) 98% area center, 99% area left. Note code can written two different ways provides quantile values. Note also t-distribution symmetric.Two-sided 95% chi-square interval \\((df = 9)\\). Note chi-square distribution symmetric.find 90% prediction inteval cutoff, R code used:","code":"\nmosaic::xqt(.98, 24)## [1] 2.17\nmosaic::xqt(.99, 24)## [1] 2.49\nmosaic::xqt(c(0.01, 0.99), 24)## [1] -2.49  2.49\nmosaic::xqchisq(c(0.025, 0.975), 9)## [1]  2.7 19.0\nmosaic::xqt(0.95, 12)## [1] 1.78"},{"path":"intest.html","id":"bootstrapping-survival-example","chapter":"6 Interval Estimates","heading":"6.6.2 Bootstrapping Survival Example","text":"many built functions R (Python, Matlab, Stata, etc. matter) bootstrap dataset create number standard bootstrap intervals. However, order understand bootstrap process, example uses loops repeated resample calculate statistics interest.Example 6.6  heroin survival timeHesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.Hesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.data include amount time subjects stayed facility treatment terminated (column 4).data include amount time subjects stayed facility treatment terminated (column 4).37% subjects, study ended still clinic (status=0).37% subjects, study ended still clinic (status=0).survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)","code":"\nlibrary(tidyverse)\nheroin <- readr::read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nnames(heroin)## [1] \"id\"     \"clinic\" \"status\" \"times\"  \"prison\" \"dose\"\nhead(heroin)## # A tibble: 6 × 6\n##      id clinic status times prison  dose\n##   <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>\n## 1     1      1      1   428      0    50\n## 2     2      1      1   275      1    55\n## 3     3      1      1   262      0    55\n## 4     4      1      1   183      0    30\n## 5     5      1      1   259      1    65\n## 6     6      1      1   714      0    55\nobs.stat <- heroin %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()\nobs.stat## [1] 378"},{"path":"intest.html","id":"bootstrapping-the-data-1","chapter":"6 Interval Estimates","heading":"Bootstrapping the data","text":"","code":"\nset.seed(4747)\nheroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)\n\nheroin.bs %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()## [1] 372"},{"path":"intest.html","id":"creating-a-bootstrap-sampling-distribution-for-the-trimmed-mean-1","chapter":"6 Interval Estimates","heading":"Creating a bootstrap sampling distribution for the trimmed mean","text":"","code":"\nbs.test.stat<-c()    # placeholder, eventually B long, check after running!\nbs.sd.test.stat<-c() # placeholder, eventually B long, check after running!\n\nB <- 500\nM <- 100\nset.seed(4747)\nfor(b in 1:B){ \n  heroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)  # BS sample\n  bs.test.stat<-c(bs.test.stat, # concatenate each trimmed mean each time go through loop\n                  heroin.bs %>% \n                    summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  \n  bsbs.test.stat <- c() # refresh the vector of double BS test statistics\n  \n  for(m in 1:M){\n    heroin.bsbs<-heroin.bs %>% sample_frac(size=1, replace=TRUE) # BS of the BS!\n    bsbs.test.stat <- c(bsbs.test.stat, # concatenate the trimmed mean of the double BS\n                        heroin.bsbs %>% \n                          summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  }\n  bs.sd.test.stat<-c(bs.sd.test.stat, sd(bsbs.test.stat))\n}"},{"path":"intest.html","id":"what-do-the-data-distributions-look-like-1","chapter":"6 Interval Estimates","heading":"What do the data distributions look like?","text":"","code":"\nggplot(heroin, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"original sample (n=238)\")\nggplot(heroin.bs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"one bootstrap sample (n=238)\")\nggplot(heroin.bsbs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"a bootstrap sample of the one bootstrap sample (n=238)\")"},{"path":"intest.html","id":"what-do-the-sampling-distributions-look-like-1","chapter":"6 Interval Estimates","heading":"What do the sampling distributions look like?","text":"","code":"\nbs.stats <- data.frame(bs.test.stat)\nggplot(bs.stats, aes(x=bs.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of trimmed mean\") +  \n  xlab(paste(\"trimmed.mean=\",round(mean(bs.test.stat),2),\"; SE=\", round(sd(bs.test.stat),2)))"},{"path":"intest.html","id":"what-is-the-distribution-of-the-se-of-the-statistic-1","chapter":"6 Interval Estimates","heading":"What is the distribution of the SE of the statistic?","text":"","code":"\nbs.SE <- data.frame(bs.sd.test.stat)\nggplot(bs.SE, aes(x=bs.sd.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of SE of trimmed means\") +  \n  xlab(paste(\"average SE=\",round(mean(bs.sd.test.stat),2)))"},{"path":"intest.html","id":"what-is-the-distribution-of-the-t-statistics-1","chapter":"6 Interval Estimates","heading":"What is the distribution of the T statistics?","text":"","code":"\nbs.T <- data.frame(T.test.stat = (bs.test.stat - obs.stat) / bs.sd.test.stat)\nggplot(bs.T, aes(x=T.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of T statistics of trimmed means\") +  \n  xlab(paste(\"average T=\",round(mean(bs.T$T.test.stat),2)))"},{"path":"intest.html","id":"normal-ci-with-bs-se-1","chapter":"6 Interval Estimates","heading":"95% normal CI with BS SE","text":"","code":"\nobs.stat + \n  qnorm(c(.025,.975))*\n  sd(bs.test.stat)## [1] 334 423"},{"path":"intest.html","id":"bootstrap-t-ci-1","chapter":"6 Interval Estimates","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample).","code":"\nbs.t.hat<-(bs.test.stat - obs.stat)/bs.sd.test.stat\n\nbs.t.hat.95 = quantile(bs.t.hat, c(.975,.025))\n\nobs.stat - bs.t.hat.95*sd(bs.test.stat)## 97.5%  2.5% \n##   337   427"},{"path":"intest.html","id":"percentile-ci-1","chapter":"6 Interval Estimates","heading":"95% Percentile CI","text":"","code":"\nquantile(bs.test.stat, c(.025, .975))##  2.5% 97.5% \n##   332   422"},{"path":"fisher.html","id":"fisher","chapter":"7 Fisher Information","heading":"7 Fisher Information","text":"Fisher Information method measuring amount information sample data contains unknown parameter. going use Fisher Information approximate variance estimators obtained large samples (specifically, give us info variance MLEs). Note, ’re back thinking like Frequentists (hint Information given MLEs).Note: Fisher information measures curvature likelihood. likelihood flat (respect parameter), estimate maximum likelihood (MLE) can quite . likelihood steep respect parameter (2nd derivative small, asymptotic variance small), estimate maximum (MLE) good.","code":""},{"path":"fisher.html","id":"fisher-information-in-one-observation","chapter":"7 Fisher Information","heading":"7.1 Fisher Information in one observation","text":"","code":""},{"path":"fisher.html","id":"some-intuition-describing-the-second-derivative-of-the-likelihood","chapter":"7 Fisher Information","heading":"Some intuition describing the second derivative of the likelihood","text":"Fisher Information describes curvature likelihood function, , negative expected value second derivative. second derivative? Let’s look examples.Important: plots / derivatives / intuition thinking likelihood function \\(\\theta\\)!! (function data.)Consider simplified example single observation exponential distribution mean \\(\\theta\\).\\[f(x | \\theta) = (1/\\theta) e^{-x/\\theta}\\]Recall maximum likelihood estimator value parameter maximizes likelihood. , plotting likelihood, find value \\(\\theta\\) (x-axis) gives highest likelihood (y-axis).\\[\\mbox{MLE} = \\frac{\\sum_i X_i}{n} \\ \\ \\ \\ \\mbox{} n=1, \\mbox{MLE} = X\\]task today consider certain estimate. \\(X=2\\) likelihood extremely peaked, maximum value appears somewhat obvious. \\(X=10\\) still possible maximize function, process seems somewhat less certain give “best” value \\(\\theta\\).information given expected value second derivative. value X, particular shape second derivative (peaked, less peaked). expected value average values second derivative, giving weights defined pdf (’s expected value ).","code":""},{"path":"fisher.html","id":"mathematical-formulation-of-the-second-derivative-of-the-likelihood","chapter":"7 Fisher Information","heading":"Mathematical formulation of the second derivative of the likelihood","text":"Define following (note, just functions x):\n\\[\\begin{eqnarray*}\n\\lambda(x | \\theta) &=& \\ln f(x | \\theta) \\\\\n\\lambda ' (x | \\theta) &=& \\frac{\\partial}{\\partial \\theta} \\ln f(x | \\theta) \\\\\n\\lambda '' (x | \\theta) &=& \\frac{\\partial^2}{\\partial \\theta^2} \\ln f(x | \\theta) \\\\\n\\end{eqnarray*}\\]Fisher Information:\n\\[\\begin{align}\n(\\theta) = E \\{ [ \\lambda ' (X | \\theta) ] ^2 \\} \\tag{7.1}\n\\end{align}\\]Now math gets little messy…\\[\\begin{eqnarray*}\n(\\theta) &=& \\int [ \\lambda '(x | \\theta) ] ^2 f(x | \\theta) dx\\\\\n\\mbox{} && \\int f(x| \\theta) dx = 1\\\\\n\\mbox{} && \\int f'(x | \\theta) dx = 0\\\\\n\\mbox{} && \\int f''(x | \\theta) dx = 0\\\\\n\\end{eqnarray*}\\]can now construct two alternative forms Fisher Information:\n\\[\\begin{align}\n\\lambda '(x| \\theta) &= \\frac{\\partial}{\\partial \\theta} ln (f(x | \\theta)) = \\frac{f'(x | \\theta)}{f(x| \\theta)} \\nonumber \\\\\nE[ \\lambda '(X | \\theta) ] &= \\int \\frac{f'(x | \\theta)}{f(x|\\theta)} f(x|\\theta) dx \\nonumber\\\\\n&= \\int f'(x | \\theta) dx = 0 \\nonumber \\\\\n(\\theta) &= E \\{ [\\lambda'(X | \\theta) ]^2 \\} = var(\\lambda'(X | \\theta)) \\tag{7.2}\n\\end{align}\\]\n(mean \\(\\lambda'(x | \\theta)\\) zero.)\\[\\begin{align}\n\\lambda''(x|\\theta) &= \\frac{f(x|\\theta) f''(x|\\theta) - [ f'(x|\\theta) ]^2}{ [f(x|\\theta)]^2}  \\mbox{  (chain rule)} \\nonumber\\\\\n&= \\frac{f''(x|\\theta)}{f(x|\\theta)} - [ \\lambda'(x | \\theta) ]^2 \\nonumber\\\\\nE[\\lambda''(X|\\theta)] &= \\int f''(x|\\theta) dx - E \\{ [\\lambda'(x|\\theta)]^2 \\} \\nonumber\\\\\n(\\theta) &= - E [ \\lambda''(X | \\theta) ] \\tag{7.3}\n\\end{align}\\]last equations (7.3) often easiest compute.Example 7.1  Suppose \\(X\\sim\\)Poisson(\\(\\theta\\)). \\(f(x|\\theta) = \\frac{e^{-\\theta} \\theta^x}{x!}\\). Find Fisher Information \\(X\\).\\[\\begin{eqnarray*}\n\\lambda(x | \\theta) &=& \\ln f(x|\\theta) = -\\theta + x \\ln \\theta - \\ln(x!)\\\\\n\\lambda ' (x | \\theta) &=& -1 + x/\\theta\\\\\n\\lambda '' (x | \\theta) &=& -x / \\theta^2\\\\\n\\end{eqnarray*}\\]\n(7.3):\n\\[\\begin{eqnarray*}\n(\\theta) &=& - E [ \\lambda '' (X|\\theta) ] = - E[ -X / \\theta^2]\\\\\n&=& \\theta / \\theta^2 = 1/\\theta\n\\end{eqnarray*}\\]\n(7.2):\n\\[\\begin{eqnarray*}\n(\\theta) &=& var(\\lambda '(X|\\theta)) = var(-1 + X/\\theta)\\\\\n&=& \\frac{1}{\\theta^2} var(X) = \\theta / \\theta^2 = 1/\\theta\n\\end{eqnarray*}\\]\n(7.1):\n\\[\\begin{eqnarray*}\n(\\theta) &=& E \\{ [ \\lambda'(X | \\theta) ]^2 \\}\\\\\n&=& E[ 1 - 2X /\\theta + X^2 / \\theta^2\\\\\n&=& 1 - \\frac{2}{\\theta}E[X] + \\frac{1}{ \\theta^2} E(X^2)\\\\\n&=& 1 - \\frac{2}{\\theta} \\theta + \\frac{1}{ \\theta^2} (var(X) + E[X]^2)\\\\\n&=& 1 - 2 +\\frac{1}{\\theta^2}(\\theta + \\theta^2) = 1/\\theta\n\\end{eqnarray*}\\]Notice information \\(\\theta\\) \\(X\\) depends \\(\\theta\\). isn’t always true.","code":""},{"path":"fisher.html","id":"fisher-information-in-a-random-sample","chapter":"7 Fisher Information","heading":"7.2 Fisher Information in a random sample","text":"seems, though, asking much information parameter joint likelihood (single sample). define Fisher Information random sample :\n\\[\\begin{eqnarray*}\nI_n (\\theta) &=& E \\{ [ \\lambda ' (\\underline{X} | \\theta) ]^2 \\}\\\\\n&=& var [ \\lambda ' (\\underline{X} | \\theta) ] \\\\\n&=& - E [ \\lambda '' (\\underline{X} | \\theta) ] \\\\\n\\end{eqnarray*}\\]find log likelihood joint distribution (based random sample):\n\\[\\begin{eqnarray*}\nf(\\underline{x} | \\theta) &=& \\prod_i f(x_i | \\theta) \\mbox{ (iid)}\\\\\n\\lambda(\\underline{x} | \\theta) &=& \\sum_i \\lambda(x_i | \\theta) \\\\\n\\lambda'(\\underline{x} | \\theta) &=& \\sum_i \\lambda'(x_i | \\theta) \\\\\n\\lambda''(\\underline{x} | \\theta) &=& \\sum_i \\lambda''(x_i | \\theta) \\\\\nI_n (\\theta) &=& - E[\\lambda''(\\underline{X} | \\theta) ]\\\\\n&=& - E[ \\sum \\lambda''(X_i | \\theta) ]\\\\\n&=& \\sum -E[\\lambda''(X_i | \\theta)]\\\\\n&=& n (\\theta) \\ \\ !!!\n\\end{eqnarray*}\\]Example 7.2  Let \\(X_1, X_2, \\ldots, X_n \\sim N(\\theta,\\theta); E[X] = Var[X] = \\theta\\), independently. Find Fisher Information \\(\\theta\\) random sample.\\[\\begin{eqnarray*}\nf(x|\\theta) &=& \\frac{1}{\\sqrt{2 \\pi \\theta}} exp \\{ - (x-\\theta)^2 / 2\\theta \\}\\\\\n\\lambda(x | \\theta) &=& \\frac{-1}{2} \\ln 2\\pi - \\frac{1}{2}\\ln \\theta - \\frac{(x-\\theta)^2}{2\\theta}\\\\\n&=& \\frac{-1}{2} \\ln 2\\pi - \\frac{1}{2}\\ln \\theta - \\frac{x^2}{2\\theta} + \\frac{2x\\theta}{2\\theta} - \\frac{\\theta^2}{2\\theta}\\\\\n\\lambda ' (x | \\theta) &=& \\frac{-1}{2 \\theta} + \\frac{x^2}{2 \\theta^2} + 0 -\\frac{1}{2}\\\\\n\\lambda '' (x| \\theta) &=& \\frac{1}{2 \\theta^2} - \\frac{2x^2}{2\\theta^3}\\\\\n(\\theta) &=& -E[ \\lambda''(X|\\theta) ] \\\\\n&=& \\frac{-1}{2\\theta^2} + \\frac{E[X^2]}{ \\theta^3}\\\\\n&=& \\frac{-1}{2\\theta^2} + \\frac{(\\theta + \\theta^2)}{\\theta^3}\\\\\n&=& \\frac{1+2\\theta}{2 \\theta^2}\\\\\nI_n(\\theta) &=& \\frac{n(1+2\\theta)}{2 \\theta^2}\n\\end{eqnarray*}\\]Note, differentiating log-likelihood (using quadratic formula), can find MLE :\n\\[\\begin{eqnarray*}\n\\hat{\\theta} = \\frac{-1 + \\sqrt{1+4 \\frac{\\sum x_i^2}{n}}}{2}\n\\end{eqnarray*}\\]\nUsing CRLB theorems book, can show MLE approximate normal distribution centered around \\(\\theta\\) variance given Fisher Information.","code":""},{"path":"fisher.html","id":"cramér-rao-lower-bound","chapter":"7 Fisher Information","heading":"7.3 Cramér-Rao Lower Bound","text":"Cramér-Rao Lower Bound using characteristics \\(\\lambda, \\lambda',\\) \\(\\lambda''\\), can show estimator \\(\\theta\\) (see page 519 DeGroot Schervish (2011); note regularity conditions page 514):\n\\[\\begin{eqnarray*}\nT &=& r(X_1, X_2, \\ldots, X_n) = r(\\underline{X}) \\mbox{ }\\\\\nE[T] &=& m(\\theta)\\\\\n\\mbox{: } var(T) &\\geq& \\frac{[m'(\\theta)]^2}{nI(\\theta)}\n\\end{eqnarray*}\\]\nNote, \\(m'(\\theta) = \\partial m(\\theta) / \\partial(\\theta)\\). \\(m(\\theta) = \\theta\\), \\(m'(\\theta) = 1\\) var\\((T) \\geq 1/nI(\\theta)\\).Proof Note: \\(\\lambda'(\\underline{x} | \\theta) = f'(\\underline{x} | \\theta) / f(\\underline{x} | \\theta)\\)Also, recall : \\(Cov(X, Y) = E(XY) - E(X)E(Y) = E[(X - E(X))(Y - E(Y))]\\)\\[\\begin{eqnarray*}\n\\mbox{showed    } \\ \\ \\ \\ 0 = E[\\lambda'(\\underline{X} | \\theta)] &=& \\int_S \\cdots \\int_S f'(\\underline{x} | \\theta) dx_1 \\ldots dx_n.\\\\\n\\mbox{therefore    } \\ \\ \\ \\ \\mbox{Cov}[T, \\lambda'(\\underline{X} | \\theta) &=& E[T \\lambda'(\\underline{X} | \\theta)]\\\\\n&=& \\int_S \\cdots \\int_S r(\\underline{x}) \\lambda'(\\underline{x} | \\theta) f(\\underline{x} | \\theta) dx_1 \\ldots dx_n.\\\\\n&=& \\int_S \\cdots \\int_S r(\\underline{x}) f'(\\underline{x} | \\theta) dx_1 \\ldots dx_n.\\\\\n\\mbox{also know    } \\ \\ \\ \\ m(\\theta) &=& \\int_S \\cdots \\int_S r(\\underline{x}) f(\\underline{x} | \\theta) dx_1 \\ldots dx_n.\\\\\n\\mbox{derivative wrt $\\theta$    } \\ \\ \\ \\  m'(\\theta) &=& \\int_S \\cdots \\int_S r(\\underline{x}) f'(\\underline{x} | \\theta) dx_1 \\ldots dx_n.\\\\\n\\mbox{Therefore    } \\ \\ \\ \\  \\mbox{Cov}[T, \\lambda'(\\underline{X} | \\theta) ]&=& m'(\\theta)\\\\\n\\mbox{Cauchy-Schwarz     } \\ \\ \\ \\  [\\mbox{Cov}[T, \\lambda'(\\underline{X} | \\theta) ]^2 &\\leq& Var(T) Var(\\lambda'(\\underline{X} | \\theta)]\\\\\nVar(T) &\\geq& \\frac{(m'(\\theta))^2}{nI(\\theta)}\n\\end{eqnarray*}\\]equality holds iff: \\(+ b\\lambda'(\\underline{X} | \\theta) = c\\). , T linear combination \\(\\lambda'\\), \\(X\\) \\(\\lambda'\\). [iff part proof Cauchy-Schwarz inequality.] equality term C-S, know minimum variance estimator expected value \\(m(\\theta)\\) \\[ T = u(\\theta) \\lambda'(\\underline{X}|\\theta) + v(\\theta).\\]Example 7.3  Back example, \\(N(\\theta,\\theta)\\). found MLE : \\(r(\\underline{x}) = \\frac{-1 + \\sqrt{1+4 \\frac{\\sum x_i^2}{n}}}{2}\\). \\((\\theta) = \\frac{(1+2\\theta)}{2 \\theta^2}\\).need find \\(m(\\theta)\\), meanwhile, know proof \n\\[ Var(r(\\underline{X})) \\geq \\frac{(m'(\\theta))^2 2 \\theta^2}{n (1+2\\theta)}\\]","code":""},{"path":"fisher.html","id":"efficiency","chapter":"7 Fisher Information","heading":"7.4 Efficiency","text":"var(\\(T) = \\frac{[m'(\\theta)]^2}{nI(\\theta)}\\), say \\(T\\) efficient \\(m(\\theta)\\). \\(T\\) efficient unbiased, say Uniformly Minimum Variance Unbiased Estimator (UMVUE).Example 7.4  Let \\(X_1, X_2, \\ldots, X_n \\sim Poisson(\\theta)\\). Let \\(T=\\overline{X}\\) estimate \\(\\theta\\).\n\\[\\begin{eqnarray*}\nm(\\theta) &=& E[T] = \\theta\\\\\nvar(T) &\\geq& \\frac{1}{nI(\\theta)} = \\frac{1}{n/\\theta} = \\frac{\\theta}{n}\\\\\n\\end{eqnarray*}\\]know var(\\(\\overline{X}) = \\frac{1}{n}\\) var(\\(X_i) = \\frac{\\theta}{n}\\)!. \\(T\\) efficient estimator \\(\\theta\\). \\(T\\) uniformly minimum variance unbiased estimator \\(\\theta\\). Note, \\(T\\) also unbiased, \\(T\\) unbiased efficient? result says \\(T=\\overline{X}\\) smallest variance unbiased estimator \\(\\theta\\). ?","code":""},{"path":"fisher.html","id":"asymptotic-distribution-of-the-mle","chapter":"7 Fisher Information","heading":"7.4.1 Asymptotic distribution of the MLE","text":"Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\) random sample size n. [true every case, \\(T = u(\\theta) \\lambda'(\\underline{X} | \\theta) + v(\\theta)\\), can use CLT \\(\\lambda'(\\underline{x}|\\theta) = \\sum_{=1}^n \\lambda'(x_i | \\theta)\\).] asymptotic distribution :\n\\[\\begin{eqnarray*}\n[nI(\\theta) ]^{1/2} (\\hat{\\theta} - \\theta) &\\mbox{}& N(0,1)\\\\\n\\mbox{say } [nI(\\theta) ]^{1/2} (\\hat{\\theta} - \\theta) &\\stackrel{D}{\\rightarrow}& N(0,1)\n\\end{eqnarray*}\\]Example 7.5  Poisson example, know \\(\\overline{X}\\) MLE \\(\\theta\\). gives:\n\\[\\begin{eqnarray*}\n(n\\frac{1}{\\theta})^{1/2} [ \\overline{X} - \\theta ] &\\stackrel{D}{\\rightarrow}& N(0,1)\\\\\n\\frac{\\overline{X} - \\theta}{\\sqrt{\\theta/n}} &\\stackrel{D}{\\rightarrow}& N(0,1)\n\\end{eqnarray*}\\]\nWell, knew CLT well…","code":""},{"path":"fisher.html","id":"fisher-information-and-bayesian-estimators","chapter":"7 Fisher Information","heading":"7.5 Fisher Information and Bayesian Estimators","text":"Bayesian setting, can shown \\(\\theta | \\underline{X}\\) asymptotically normal mean \\(\\hat{\\theta}\\) = MLE variance \\(\\frac{1}{nI(\\hat{\\theta})}\\). Note \\(\\theta\\) mean variance \\(\\theta\\) now random quantity, \\(\\hat{\\theta}\\) fixed part. large samples, swamp prior information:\n\\[\\begin{eqnarray*}\n[nI(\\hat{\\theta})]^{1/2} (\\theta - \\hat{\\theta}) | \\underline{X} \\stackrel{D}{\\rightarrow} N(0,1)\n\\end{eqnarray*}\\], notice neither \\(\\hat{\\theta}\\) \\((\\hat{\\theta})\\) \\(\\theta\\) values .Example 7.6  Back example, \\(N(\\theta,\\theta)\\). found MLE : \\(\\hat{\\theta} = \\frac{-1 + \\sqrt{1+4 \\frac{\\sum x_i^2}{n}}}{2}\\). \\((\\theta) = \\frac{(1+2\\theta)}{2 \\theta^2}\\).\\[\\Bigg[\\frac{n(1+2\\hat{\\theta})}{2 \\hat{\\theta}^2}\\Bigg]^{1/2} (\\theta - \\hat{\\theta}) | \\underline{X} \\stackrel{D}{\\rightarrow} N(0,1)\\]","code":""},{"path":"fisher.html","id":"giants-in-the-field","chapter":"7 Fisher Information","heading":"7.6 Giants in the Field","text":"came stuff? long ago ideas derived? Answer: really awesome statisticians working recently. Rao still alive just celebrated 102nd birthday.","code":""},{"path":"fisher.html","id":"david-blackwell-the-rao-blackwell-theorem","chapter":"7 Fisher Information","heading":"7.6.1 David Blackwell: The Rao-Blackwell Theorem","text":"beyond scope class, cover Rao-Blackwell theorem due time constraints. Broadly, sufficient statistic one made function data summary function data given likelihood (think: sum \\(X_i\\) sum \\(X_i^2\\) Gaussian likelihood). Rao-Blackwell theorem says conditioning sufficient statistic creates estimator minimum mean squared error. result quite important (want estimators small mean squared error!), also mathematically beautiful. proof theorem given book Section 7.9, Theorem 7.9.1.provide information Rao-Blackwell Theorem class notes highlight one giants statistics: David Blackwell (1919-2010). Blackwell first Black person receive PhD statistics US first Black scholar admitted National Academy Sciences. statistician UC Berkeley 50 years. hired 1954 department almost made offer 1942 (declined one faculty member’s wife said didn’t want Blackwell hired wouldn’t feel comfortable faculty events home Black man). Hear Blackwell tell story words. Also, read Blackwell Mathematicians African Diaspora.\nFigure 1.2: David Blackwell. Image credit: https://math.illinois.edu/david-blackwell\n","code":""},{"path":"fisher.html","id":"c.r.-rao-cramér-rao-lower-bound","chapter":"7 Fisher Information","heading":"7.6.2 C.R. Rao: Cramér-Rao Lower Bound","text":"Cramér-Rao Lower Bound (seen previously notes) Rao-Blackwell Theorem results contributions C.R. Rao Indian-American statistician, originally Karnataka, India. Rao just celebrated \\(102^{nd}\\) birthday (b. Sep 10, 1920), well known among greatest statisticians time. Indeed, much student Fisher, working likelihoods focusing inferential claims. Rao known genial kind. recent article (Brad Efron, creator boostrap) celebrating centennial birthday (Efron et al. 2020) Rao reported written statistical results order…… communicate others great clarity.remember first met Rao person, think visiting colleague Penn State Department Statistics. Rao clearly “heart” department, surrounded many talented individuals, presence created vitality helped ensure stature among US departments statistics.Despite dominant reputation, Rao always seemed extremely modest , moreover, helpful younger colleagues.\nFigure 1.3: C.R. Rao. Image credit: https://pib.gov./PressReleseDetail.aspx?PRID=1653064\n","code":""},{"path":"fisher.html","id":"reflection-questions-6","chapter":"7 Fisher Information","heading":"7.7  Reflection Questions","text":"estimator need unbiased order efficient?estimator unbiased, variance estimator order smallest variance class unbiased estimators?can asymptotic distribution MLE written terms CRLB?can asymptotic distribution Bayes estimator written terms CRLB?main differences MLE Bayes estimator?one use MLE result compared Bayes result?","code":""},{"path":"fisher.html","id":"ethics-considerations-6","chapter":"7 Fisher Information","heading":"7.8  Ethics Considerations","text":"matter mathematics statistics?one definition “best” (terms estimator)?version “best” estimator CRLB speak ?","code":""},{"path":"ht.html","id":"ht","chapter":"8 Hypothesis Testing","heading":"8 Hypothesis Testing","text":"","code":""},{"path":"ht.html","id":"statistical-hypotheses","chapter":"8 Hypothesis Testing","heading":"8.1 Statistical Hypotheses","text":"Instead giving bounds parameter values (interval estimates), going test specific claims parameter (e.g., \\(\\theta\\)). know \\(\\theta \\\\Omega\\). Let \\(\\Omega\\) disjoint partition:\n\\[\\begin{eqnarray*}\n\\Omega_0 \\cup \\Omega_1 &=& \\Omega\\\\\n\\Omega_0 \\cap \\Omega_1 &=& \\emptyset\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\mbox{Claim 1:  } H_0: \\theta \\\\Omega_0\\\\\n\\mbox{Claim 2:  } H_1: \\theta \\\\Omega_1\\\\\n\\end{eqnarray*}\\]want decide one true, \\(H_0\\) (null) \\(H_1\\) (alternative)?\\(\\Omega_0\\) \\(\\Omega_1\\) single value, say simple hypothesisIf \\(\\Omega_0\\) \\(\\Omega_1\\) contain single value, say composite hypothesisExample 8.1  want test whether coin fair. ’ll flip coin 20 times collect data. following two-sided test:\n\\[\\begin{eqnarray*}\nH_0:&& \\theta=0.5 \\mbox{  (simple)}\\\\\nH_1:&& \\theta\\ne0.5 \\mbox{  (composite)}\\\\\n\\end{eqnarray*}\\]Often ’ll write \\(\\theta_0\\) placeholder null value:\n\\[\\begin{eqnarray*}\nH_0:&& \\theta=\\theta_0 \\mbox{  (simple)}\\\\\nH_1:&& \\theta\\ne \\theta_0 \\mbox{  (composite)}\\\\\n\\end{eqnarray*}\\]betting, dealer always bets heads, probably want one-sided test.\n\\[\\begin{eqnarray*}\nH_0: \\theta=0.5 \\mbox{  (simple)}&  \\mbox{ } & H_0: \\theta \\leq 0.5 \\mbox{  (composite)}\\\\\nH_1: \\theta > 0.5 \\mbox{  (composite)}&   &H_1: \\theta > 0.5 \\mbox{  (composite)}\\\\\n\\end{eqnarray*}\\]always put research question alternative hypothesis. Ask : /collecting data?","code":""},{"path":"ht.html","id":"what-is-an-alternative-hypothesis","chapter":"8 Hypothesis Testing","heading":"What is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: https://www.youtube.com/watch?v=b-U_I1DCGEY.video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? see video? relate material class? relationship sampling distributions?","code":""},{"path":"ht.html","id":"critical-regions-and-test-statistics","chapter":"8 Hypothesis Testing","heading":"8.1.1 Critical Regions and Test Statistics","text":"Let X count number heads, think data gave X=20 (n=20) hypotheses :\n\\[\\begin{eqnarray*}\nH_0:&& \\theta=0.5 \\\\\nH_1:&& \\theta\\ne0.5\n\\end{eqnarray*}\\]data help decide hypothesis believe. X=11? X=7? set critical region X region, reject \\(H_0\\). , \\(\\exists \\ \\ c \\ \\ s.t. \\ \\ | x - 10| > c \\ \\Rightarrow\\)   reject   \\(H_0\\).test statistic, \\(T=r(\\underline{X})\\) function data (hopefully) provide enough knowledge make decision hypothesis. , \\(T=X\\). Often \\(T=\\overline{X}\\) \\(T= \\overline{X} - \\mu_0\\) \\(T=(\\overline{X} - \\mu_0) / s/\\sqrt{n}\\). Usually, critical region form like:“\\(T \\geq c\\)” \\(\\Rightarrow\\) reject \\(H_0\\)","code":""},{"path":"ht.html","id":"errors-power-size","chapter":"8 Hypothesis Testing","heading":"8.1.2 Errors: Power & Size","text":"can make two types mistakes (jury example):","code":""},{"path":"ht.html","id":"power","chapter":"8 Hypothesis Testing","heading":"8.1.2.1 Power","text":"rate test considering “power” test. Let \\(\\delta\\) test procedure defines critical region \\(c\\) reject \\(H_0\\) \\(T \\C\\). example:\n\\[\\begin{eqnarray*}\n\\delta &=& \\{ \\mbox{ always reject } H_0 \\}\\\\\n\\delta &=& \\{ \\mbox{ reject } H_0 \\mbox{ } X > 60 \\}\\\\\n\\end{eqnarray*}\\]power function :\n\\[\\begin{eqnarray*}\n\\pi (\\theta | \\delta) = P(T \\C | \\theta)\n\\end{eqnarray*}\\]\\(\\theta \\\\Omega_0\\), \\(\\pi(\\theta | \\delta)\\) probability making type error.\\(\\theta \\\\Omega_1\\), \\(1 - \\pi(\\theta | \\delta)\\) probability making type II error.Note: \\(\\theta \\\\Omega_0\\) \\(\\theta \\\\Omega_1\\), ! , one type error ever possible, never know . choice test procedures, want one :\n\\[\\begin{eqnarray*}\n\\theta \\\\Omega_0, \\pi(\\theta | \\delta) \\mbox{ small}\\\\\n\\theta \\\\Omega_1, \\pi(\\theta | \\delta) \\mbox{ big}\\\\\n\\end{eqnarray*}\\]Example 8.2  Let \\(\\delta =\\)“always reject \\(H_0\\)”, \\(\\delta = \\{ T \\\\!\\!R \\}\\).\n\\[\\begin{eqnarray*}\n\\mbox{} \\theta \\\\Omega_0, \\pi(\\theta | \\delta) = 1, \\mbox{ bad!!}, P(\\mbox{type error}) = 1\\\\\n\\mbox{} \\theta \\\\Omega_1, \\pi(\\theta | \\delta) = 1, \\mbox{ good!!}, P(\\mbox{type II error}) = 0\n\\end{eqnarray*}\\]\nneed balance.Typically, bound \\(\\pi(\\theta | \\delta) \\ \\ \\forall \\ \\ \\theta \\\\Omega_0\\). , \\(\\pi(\\theta | \\delta) \\leq \\alpha_0 \\ \\ \\forall \\ \\ \\theta \\\\Omega_0\\). Note, \\(\\alpha_0\\) called level significance.Among tests satisfy bound \\(\\theta \\\\Omega_0\\), want power function \\(\\theta \\\\Omega_1\\) big possible (probability type II error low possible).","code":""},{"path":"ht.html","id":"size","chapter":"8 Hypothesis Testing","heading":"Size","text":"size test :\n\\[\\begin{eqnarray*}\n\\alpha(\\delta) = \\sup_{\\theta \\\\Omega_0} \\pi (\\theta | \\delta)\n\\end{eqnarray*}\\]\n\\(\\Omega_0 = \\theta_0\\) (simple hypothesis), \\(\\alpha(\\delta) = \\pi(\\theta_0 | \\delta)\\).Note: may wondering \\(\\sup\\) function. Note often \\(\\sup\\) set \\(\\max\\) set! difference? \\(\\max\\) largest value function can take possible values within set (, argument function must contained range evaluating function). \\(\\sup\\) largest value function can take possible values bound range possible values. \\(\\sup\\) may may happen value range. \\(\\max\\) exists, \\(\\sup = \\max\\). hypothesis testing structure set general accommodate different types hypotheses, thus \\(\\sup\\) right choice (\\(\\max\\)) defining \\(\\alpha(\\delta).\\)Example 8.3  Back cheating example, n=100,\n\\[\\begin{eqnarray*}\nH_0: \\theta \\leq 0.5 \\\\\nH_1: \\theta > 0.5\n\\end{eqnarray*}\\]Let \\(\\delta = \\{\\) reject \\(H_0\\) \\(X>55 \\}.\\)\n\\[\\begin{eqnarray*}\n\\pi(\\theta | \\delta) &=& P(X > 55 | \\theta = 0.5)\\\\\n&=& P( Z > 1) \\\\\n&=& 0.1587\n\\end{eqnarray*}\\]\n\\(\\theta < 0.5\\)? \\(\\rightarrow \\pi(\\theta | \\delta) < 0.1587\\) ! size test \\(\\alpha(\\delta) = 0.1587.\\) Can make test better?? previous statement says \\(\\theta=0.5,\\) ’ll reject \\(H_0\\) 15.87% time. (pretty big type error). want \\(\\alpha(\\delta) \\leq 0.05 (= \\alpha_0).\\)\\[\\begin{eqnarray*}\n\\delta &=& \\{ \\mbox{ reject } H_0 \\mbox{ } T \\geq c \\}\\\\\n\\alpha(\\delta) &=& \\sup_{\\theta \\\\Omega_0} P (T \\geq c | \\theta) \\\\\n&=& \\sup_{\\theta \\\\Omega_0} \\pi (\\theta | \\delta)\\\\\n\\pi (\\theta | \\delta) &=& 1 - \\Phi\\Bigg( \\frac{ c - 100 \\theta}{\\sqrt{100\\theta(1-\\theta)}} \\Bigg)\\\\\n\\end{eqnarray*}\\]Note: \\(\\pi (\\theta | \\delta)\\) increasing function \\(\\theta\\). Also, \\(\\pi (\\theta | \\delta)\\) decreasing function c (\\(\\Phi\\) increasing function c).Keeping mind want find c s.t. \\(\\sup_{\\theta \\\\Omega_0} \\pi(\\theta | \\delta) \\leq 0.05\\), want maximize \\(\\pi (\\theta | \\delta) \\ \\ \\forall \\ \\ \\theta \\\\Omega\\) (make c small possible).already know : \\(\\sup_{\\theta \\\\Omega_0} \\pi(\\theta | \\delta) = \\pi(\\theta_0 = 0.5 | \\delta)\\). ,\n\\[\\begin{eqnarray*}\n\\pi (\\theta_0 = 0.5 | \\delta) &\\leq& 0.05\\\\\nc &\\geq& 58.25 \\mbox{ (see WU)}\\\\\n\\mbox{notice: } c &=& \\Phi^{-1} (1 - \\alpha_0) \\sigma_0 + \\mu_0\\\\\n\\mbox{reject : } T &\\geq& c\\\\\n\\\\\n\\mbox{, equivalently, : } \\frac{x - \\mu_0}{\\sigma_0} &\\geq& \\Phi^{-1} (1-\\alpha_0)\\\\\n\\mbox{let: } T &=& \\frac{X - n \\theta_0}{\\sqrt{\\theta_0 (1-\\theta_0) / n}} \\ \\ \\ \\mbox{ test statistic}\\\\\n\\mbox{now, reject : } T &\\geq& \\Phi^{-1} (1-\\alpha_0)\\\\\n\\end{eqnarray*}\\]data discrete, actually set type error rate 0.05. , end \n\\[\\begin{eqnarray*}\n\\alpha_0 &=& 0.05\\\\\n\\alpha(\\delta) &=& P(X \\geq 59 | \\theta = 0.5)\\\\\n&=& 0.04431\n\\end{eqnarray*}\\]Notice can report P(type II error) simple alternative (section 9.2 DeGroot Schervish (2011)). addition reporting decision, like report measure certainty decision. strong evidence unfair coin?p-value probability seeing data extreme null hypothesis true.Let \\(\\hat{T}\\) observed test statisticLet \\(T\\) test statistic random variable\\[\\begin{eqnarray*}\n\\mbox{p-value } &=& \\sup_{\\theta \\\\Omega_0} P(T > \\hat{T})\\\\\n&=& \\sup_{\\theta \\\\Omega_0} \\pi (\\theta | \\delta_{\\hat{T}}) \\mbox{ size test } \\delta_{\\hat{T}}\\\\\n&=& \\alpha(\\delta_{\\hat{T}})\\\\\n\\end{eqnarray*}\\], smallest level significance reject \\(H_0\\) observed data.Example 8.4  cheating dealer example continued…Let x=52\n\\[\\begin{eqnarray*}\n\\mbox{p-value } &=& P(T > \\frac{52-50}{5})\\\\\n&=& P( Z > 0.4)\\\\\n&=& 0.3446\n\\end{eqnarray*}\\]Let x=52\n\\[\\begin{eqnarray*}\n\\mbox{p-value } &=& P(T > \\frac{52-50}{5})\\\\\n&=& P( Z > 0.4)\\\\\n&=& 0.3446\n\\end{eqnarray*}\\]Let x=62\n\\[\\begin{eqnarray*}\n\\mbox{p-value } &=& P(T > \\frac{62-50}{5})\\\\\n&=& P( Z > 2.4)\\\\\n&=& 0.0082\n\\end{eqnarray*}\\]Let x=62\n\\[\\begin{eqnarray*}\n\\mbox{p-value } &=& P(T > \\frac{62-50}{5})\\\\\n&=& P( Z > 2.4)\\\\\n&=& 0.0082\n\\end{eqnarray*}\\]Notice:\n\\[\\begin{eqnarray*}\n0.0082 < 0.05 <  0.3446\\\\\n52 < 58.25 < 62\\\\\n\\end{eqnarray*}\\]\n(equivalence tests rejection regions)Example 8.5  Assume weights cereal 10oz boxes normally distributed, \\(N(\\mu, \\sigma^2),\\) unknown. test whether box label accurate, set hypotheses:\n\\[\\begin{eqnarray*}\nH_0: \\mu = 10\\mbox{oz}\\\\\nH_1: \\mu \\ne 10\\mbox{oz}\\\\\n\\end{eqnarray*}\\]form critical region look like? (Reject less certain amount 10oz, bigger certain amount 10oz, draw number line shaded rejection region.) reject \\(H_0\\) \\(\\overline{X}\\) big small.\\(|\\overline{X} - 10 |\\) big, \\(\\frac{|\\overline{X} - 10 |}{s/\\sqrt{n}}\\) also big.\n\\[\\begin{eqnarray*}\nC &=& \\Bigg\\{ \\frac{|\\overline{X} - 10 |}{s/\\sqrt{n}} > c \\Bigg\\}\\\\\n\\delta: && \\Bigg\\{ \\mbox{reject $H_0$ } T \\C \\mbox{ } T = \\frac{|\\overline{X} - 10|}{s/\\sqrt{n}} \\Bigg\\}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\alpha(\\delta) &=& \\sup_{\\mu \\\\Omega_0} \\pi(\\mu | \\delta)\\\\\n&=& P\\Bigg(\\frac{|\\overline{X} - 10|}{s/\\sqrt{n}} > c \\ \\  \\bigg| \\ \\ \\mu=10 \\Bigg)\\\\\n&=& P \\bigg( -c < \\frac{\\overline{X} - 10}{s/\\sqrt{n}} < c \\ \\  \\bigg| \\ \\  \\mu=10 \\bigg) \\\\\n&=& P( -c < t_{15} < c) = 0.95\\\\\nc &=& 2.131\\\\\n\\end{eqnarray*}\\]\nNote: also reject \\(H_0\\) \\(\\overline{X} \\notin 10 \\pm 0.453\\). (later computing power \\(\\pi(\\mu | \\delta), \\ \\mu \\\\Omega_1.)\\)p-value given data?\n\\[\\begin{eqnarray*}\n\\mbox{p-value} &=& P (T > \\widehat{T})\\\\\n&=& P \\Bigg( \\frac{|\\overline{X} - 10|}{s/\\sqrt{16}} > \\frac{|10.4 - 10|}{0.85/\\sqrt{16}} \\Bigg)\\\\\n&=& 2 P (t_{15} > 1.88)\\\\\n&=& 2 (1 - \\texttt{pt(1.88, 15)})\\\\\n&=& 2 (1 - 0.96) \\\\\n&=& 0.0797\n\\end{eqnarray*}\\]\n\nmoderate evidence (strong) say cereal boxes weigh, average, something 10oz.","code":""},{"path":"ht.html","id":"simple-hypotheses","chapter":"8 Hypothesis Testing","heading":"8.2 Simple Hypotheses","text":"Consider:\n\\[\\begin{eqnarray*}\nH_0: \\theta = \\theta_0\\\\\nH_1: \\theta = \\theta_1\\\\\n\\end{eqnarray*}\\]Let\n\\[\\begin{eqnarray*}\nf_i(\\underline{x}) = f(x_1 | \\theta_i) f(x_2 | \\theta_i) \\cdots f(x_n | \\theta_i)\n\\end{eqnarray*}\\]\n(joint pdf \\(\\theta=\\theta_i, H_i\\), true.)errors now become:\n\\[\\begin{eqnarray*}\n&&\\alpha(\\delta) = P(\\mbox{reject } H_0 | \\theta=\\theta_0) = \\pi(\\theta_0 | \\delta) = \\mbox{size}\\\\\n&&\\beta(\\delta) = P(\\mbox{reject } H_0 | \\theta=\\theta_1) = 1 - \\pi(\\theta_1 | \\delta) = 1 - \\mbox{power}\\\\\n\\end{eqnarray*}\\]find test \\(\\alpha(\\delta)\\) \\(\\beta(\\delta)\\) arbitrarily small. Instead, can minimize linear combination two types errors. , want find \\(\\delta\\) \\(\\alpha(\\delta) + b \\beta(\\delta)\\) minimized. (\\(\\alpha\\) worse? \\(\\beta?)\\)Theorem 8.1  (Theorem 9.2.1, DeGroot Schervish (2011))\nLet \\(> 0\\) \\(b > 0\\). Consider \\(\\delta^*\\), test :\n\\[\\begin{eqnarray*}\n\\mbox{reject } H_0 \\mbox{ } f_0(\\underline{x}) < b f_1(\\underline{x})\\\\\n\\mbox{reject } H_0 \\mbox{ } f_0(\\underline{x}) > b f_1(\\underline{x})\\\\\n\\mbox{either decision }  f_0(\\underline{x}) = b f_1(\\underline{x})\\\\\n\\end{eqnarray*}\\]\n, \\(\\forall \\delta\\)\n\\[\\begin{eqnarray*}\n\\alpha(\\delta^*) + b \\beta(\\delta^*) \\leq \\alpha(\\delta) + b \\beta(\\delta)\n\\end{eqnarray*}\\]Proof:\nLet \\(C\\) critical region, \\(C^c\\) complement critical region. (Note, proof discrete case, continuous case analogous.)\\[\\begin{eqnarray*}\n\\alpha(\\delta) + b \\beta(\\delta) &=& \\sum_{\\underline{x} \\C} f_0(\\underline{x}) + b \\sum_{\\underline{x} \\C^c} f_1(\\underline{x})\\\\\n&=& \\sum_{\\underline{x} \\C} f_0(\\underline{x}) + b [1 - \\sum_{\\underline{x} \\C} f_1(\\underline{x})]\\\\\n&=& b + \\sum_{\\underline{x} \\C} [ af_0(\\underline{x}) - b f_1(\\underline{x})]\\\\\n\\end{eqnarray*}\\]\nwant parenthetical part negative \\(\\forall \\underline{x} \\C\\) positive \\(\\forall \\underline{x} \\C^c\\). , choose \\(C\\) :\\[\\begin{eqnarray*}\nf_0(\\underline{x}) - bf_1(\\underline{x}) &<& 0 \\ \\ \\ \\forall \\underline{x} \\C\\\\\nf_0(\\underline{x}) - bf_1(\\underline{x}) &>& 0 \\ \\ \\ \\forall \\underline{x} \\C^c\\\\\n\\mbox{} f_0(\\underline{x}) - bf_1(\\underline{x}) = 0 && \\mbox{matter } \\underline{x} \\C \\mbox{ } \\C^c\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\Rightarrow \\delta^*: \\{ \\mbox{reject } H_0 \\mbox{ } f_0(\\underline{x}) < b f_1(\\underline{x}) \\} \\\\\n\\mbox{note: } \\delta^*: \\bigg\\{ \\mbox{reject } H_0 \\mbox{ } \\frac{f_1(\\underline{x})}{f_0(\\underline{x})} > \\frac{}{b} \\bigg\\}\n\\end{eqnarray*}\\]\n(likelihood ratio). , \\(\\delta^*\\) minimizes linear combination errors \\(\\alpha(\\delta) + b \\beta(\\delta)\\) tests, \\(\\delta\\)., like , want minimize probability type II error (subject \\(\\alpha(\\delta) \\leq \\alpha_0\\))?Theorem 8.2  (Theorem 9.2.2, DeGroot Schervish (2011), Neyman-Pearson Lemma)Let \\(k > 0\\). Suppose\n\\[\\begin{eqnarray*}\n\\delta^*: && \\{ \\mbox{ reject } H_0 \\mbox{ } k f_0(\\underline{x}) < f_1(\\underline{x})\\\\\n&& \\mbox{ reject } H_0 \\mbox{ } k f_0(\\underline{x}) > f_1(\\underline{x}) \\} \\\\\n\\end{eqnarray*}\\]Let \\(\\delta\\) another test procedure :\n\\[\\begin{eqnarray*}\n&&\\alpha(\\delta) \\leq \\alpha(\\delta^*) \\rightarrow \\mbox{: } \\beta(\\delta^*) \\leq \\beta(\\delta) \\mbox{ (.e., power greater } \\delta^*)\\\\\n&&\\alpha(\\delta) < \\alpha(\\delta^*) \\rightarrow \\mbox{: }  \\beta(\\delta^*) < \\beta(\\delta) \\mbox{ (.e., power strictly greater } \\delta^*)\\\\\n\\end{eqnarray*}\\]\n(find k \\(\\alpha(\\delta^*) \\leq \\alpha_0.)\\)Proof:\\[\\begin{eqnarray*}\nk \\alpha(\\delta^*) +  \\beta(\\delta^*) \\leq k \\alpha(\\delta) +  \\beta(\\delta)\\\\\n\\mbox{ } \\alpha(\\delta^*) \\geq \\alpha(\\delta)\\\\\n\\mbox{ } \\beta(\\delta^*) \\leq \\beta(\\delta)\n\\end{eqnarray*}\\]Example 8.6  Consider situation ’re trying figure someone purely guessing multiple choice questions 5 possible answers. Although wouldn’t typically think type situation simple-vs-simple set hypotheses, ’ll compare random guessing hypothesis \\((\\theta = 0.2)\\) hypothesis able rule one options \\((\\theta = 0.25)\\). collect 25 observations, ’ll use binomial distribution describe probability model.\\(H_0: \\theta = 0.2\\)\\(H_1: \\theta = 0.25\\)calculate likelihoods, ’ll use binomial distribution. Note almost always, joint distribution product \\(n\\) marginal distributions. binomial distribution, likelihood already joint distribution.\\[\\begin{eqnarray*}\nf_0(x) &=& {n \\choose x} (0.2)^x (0.8)^{n-x}\\\\\nf_1(x) &=& {n \\choose x} (0.25)^x (0.75)^{n-x}\\\\\n\\end{eqnarray*}\\]know want find test minimized type plus type II error \\((\\alpha(\\delta) + \\beta(\\delta))\\), find test rejects \\[\\frac{f_1(x)}{f_0(x)} > \\frac{}{b} = \\frac{1}{1} = 1.\\]find actual test, use likelihoods provided :\\[\\begin{eqnarray*}\n\\frac{f_1(x)}{f_0(x)}  = (1.25)^x (0.9375)^{n-x} &>& 1\\\\\nx \\ln(1.25) + (n-x) \\ln(0.9375) &>& 0\\\\\nx( \\ln(1.25) - \\ln(0.9375) ) &>& -n \\ln(0.9375)\\\\\nx &>& \\frac{-n \\ln(0.9375)}{( \\ln(1.25) - \\ln(0.9375) )}\\\\\nx &>& 5.61\\\\\nx &\\geq& 6\\\\\n\\end{eqnarray*}\\]say, ’ll minimize sum two types errors creating following test: \\[\\delta = \\{ \\mbox{reject } H_0 \\mbox{ } X \\geq 6 \\}.\\]leads :\n\\[\\begin{eqnarray*}\n\\alpha(\\delta) = 1 - \\texttt{pbinom}(5,25,0.2) = 0.383\\\\\n\\beta(\\delta) = \\texttt{pbinom}(5,25,0.4) = 0.029\\\\\n\\end{eqnarray*}\\]type II error rate close 3% lovely, find type error rate 38.3% WAY big. fact, need control type error rate order able confidently reject \\(H_0\\) promote scientific claims. , instead using \\(/b\\), ’ll find cutoff keeps type error 0.01. , find \\(k\\) :\\[\\delta_{NP} = \\{ \\mbox{reject } H_0 \\mbox{ } X \\geq k \\}, \\ \\ \\ \\ \\  \\alpha(\\delta) \\leq 0.01.\\]can calculate type error rate directly binomial distribution:\\[\\begin{eqnarray*}\n\\alpha(\\delta_{NP}) = 1 - \\texttt{pbinom}(10, 25, .2) = 0.00555\n\\end{eqnarray*}\\]\nleads test :\\[\\delta_{NP} = \\{ \\mbox{reject } H_0 \\mbox{ } X \\geq 11 \\}.\\]Note type II error rate \\(\\delta_{NP}\\) can calculated: \\[\\beta{\\delta_{N}} = \\texttt{pbinom}(10, 25, .25) = 0.97.\\]aren’t happy large type II error rate, turns difficult distinguish two hypotheses . control type error rate, need pretty conservative, type II error becomes pretty large.Example 8.7  Let’s say 2 batches paint one quick-dry. paint unlabeled, forgot ! paint 5 boards batch 1 record drying time. think batch 1 quick dry. also believe drying times normally distributed standard deviation 5 min.\n\\[\\begin{eqnarray*}\nH_0: \\theta=25 \\mbox{ min}\\\\\nH_1: \\theta=10 \\mbox{ min}\\\\\n\\end{eqnarray*}\\]funzies, let’s find \\(\\delta^*\\) minimizes \\(\\alpha(\\delta^*) + b \\beta(\\delta^*).\\)\\[\\begin{eqnarray*}\nf_0(\\underline{x}) &=& \\frac{1}{\\sqrt{2 \\pi 25}} \\exp \\Bigg( \\frac{-1}{2 \\cdot 25} \\sum(x_i - 25)^2\\Bigg)\\\\\nf_1(\\underline{x}) &=& \\frac{1}{\\sqrt{2 \\pi 25}} \\exp \\Bigg( \\frac{-1}{2 \\cdot 25} \\sum(x_i - 10)^2\\Bigg)\\\\\n\\frac{f_1(\\underline{x})}{f_0(\\underline{x})} &=& \\exp\\Bigg( \\frac{-1}{2 \\cdot 25} \\sum( (x_i - 10)^2 - (x_i - 25)^2 )\\Bigg)\\\\\n&=&  \\exp\\Bigg( \\frac{-1}{2 \\cdot 25} \\sum( x_i^2 -20x_i + 100 - x_i^2 + 50x_i - 625 ) \\Bigg)\\\\\n&=&  \\exp\\Bigg( \\frac{-1}{50} \\sum( 30x_i - 525 ) \\Bigg)\\\\\n&=&  \\exp\\Bigg( \\frac{-30}{50} \\sum( x_i - 17.5 )\\Bigg)\\\\\n&=&  \\exp\\Bigg( \\frac{-3 n}{5} (\\overline{x} - 17.5 ) \\Bigg)\\\\\n&> & \\frac{}{b}\\\\\n\\overline{x} - 17.5 &<& \\frac{-5}{3n} \\ln\\bigg(\\frac{}{b}\\bigg)\\\\\n\\overline{x} &<& 17.5 -\\frac{5}{3n} \\ln\\bigg(\\frac{}{b}\\bigg)\\\\\n\\end{eqnarray*}\\]type error worse \\((\\alpha(\\delta) < < )\\) \\(\\frac{}{b} > 1,\\) rejection rule \\(\\overline{x} < c_1\\) \\(c_1 < 17.5\\) (reject \\(H_0\\) less often).type II error worse \\((\\beta(\\delta) < < )\\) \\(\\frac{}{b} < 1,\\) rejection rule \\(\\overline{x} < c_2\\) \\(c_2 > 17.5\\) (reject \\(H_0\\) often).\\(=k\\) \\(b=1\\) \\(\\alpha(\\delta) = \\alpha_0\\)?\n\\[\\begin{eqnarray*}\n\\delta^*: \\{ \\mbox{reject } H_0 \\mbox{ } \\overline{x} < 17.5 - \\frac{5}{3n} \\ln(k) \\} \\\\\n\\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\nP(\\overline{X} < 17.5 - \\frac{5}{3n} \\ln(k)  | \\theta=25) &=& 0.05\\\\\nP( Z < \\frac{17.5 -5/3n \\ln(k) - 25}{5/\\sqrt{n}} ) &=& 0.05\\\\\n\\frac{17.5 -5/3n \\ln(k) - 25}{5/\\sqrt{n}} &=& -1.68\\\\\n\\ln(k) &=& -11.23\\\\\n\\delta^*: \\{ \\mbox{reject } H_0 \\mbox{ } \\overline{x} < 21.24 \\} \\\\\n\\mbox{note: } P(\\overline{X} > 21.24 | \\theta=10) &=& 0\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"ht.html","id":"uniformly-most-powerful-tests","chapter":"8 Hypothesis Testing","heading":"8.3 Uniformly Most Powerful Tests","text":"previous section talked Uniformly Powerful (UMP) tests without explicitly defining . Let hypotheses general.\n\\[\\begin{eqnarray*}\nH_0: \\theta \\\\Omega_0\\\\\nH_1: \\theta \\\\Omega_1\\\\\n\\end{eqnarray*}\\]Let \\(\\delta^*\\) UMP test level \\(\\alpha_0, \\ \\ \\alpha(\\delta^*) \\leq \\alpha_0\\), \\(\\delta\\) s.t. \\(\\alpha(\\delta) \\leq \\alpha_0\\):\n\\[\\begin{eqnarray*}\n\\pi(\\theta | \\delta) \\leq \\pi(\\theta | \\delta^*) \\ \\ \\forall \\theta \\\\Omega_1\n\\end{eqnarray*}\\]\n, test, \\(\\delta\\), equivalent size, \\(\\delta^*\\) power. (Note, \\(\\theta \\\\Omega_1, \\pi(\\theta | \\delta) = 1 - \\beta(\\delta)\\) \\(\\Omega_1\\) simple.)","code":""},{"path":"ht.html","id":"monotone-likelihood-ratio","chapter":"8 Hypothesis Testing","heading":"8.3.1 Monotone Likelihood Ratio","text":"Let \\(T = r(\\underline{X})\\) statistic. say \\(f(\\underline{x} | \\theta)\\) monotone likelihood ratio statistic \\(T\\) \\(\\forall \\theta_1, \\theta_2 \\\\Omega\\) \\(\\theta_1 < \\theta_2\\) :\n\\[\\begin{eqnarray*}\n\\frac{f(\\underline{x} | \\theta_2)}{f(\\underline{x} | \\theta_1)}\n\\end{eqnarray*}\\]depends \\(\\underline{x}\\) \\(T\\)nondecreasing function \\(T\\) range possible values \\(T\\)Example 8.8  Let \\(X_1, X_2, \\ldots, X_n \\sim Gamma(10, \\theta), E[X] = 10/\\theta\\).\n\\[\\begin{eqnarray*}\nf(x | \\theta) &=& \\frac{\\theta^{10}}{\\Gamma(10)} x^{10 - 1} e ^{-x \\theta} \\ \\ \\ \\ 0 \\leq x \\leq \\infty\n\\mbox{let } \\theta_1 < \\theta_2\\\\\n\\frac{f(\\underline{x} | \\theta_2)}{f(\\underline{x} | \\theta_1)} &=& \\frac{\\theta_2^{10n}}{\\theta_1^{10n}} e^{- \\sum x_i \\theta_2 + \\sum x_i \\theta_1}\\\\\n&=& \\Bigg( \\frac{\\theta_2}{\\theta_1}\\Bigg)^{10n} e^{-\\sum x_i (\\theta_2 - \\theta_1)}\\\\\nT &=& - \\sum X_i\\\\\n\\end{eqnarray*}\\]\n\\(f(\\underline{X} | \\theta)\\) monotone likelihood ratio \\(- \\sum X_i\\).Theorem 8.3  (DeGroot Schervish (2011) 9.3.1)Without loss generality, consider:\n\\[\\begin{eqnarray*}\nH_0: \\theta \\leq \\theta_0\\\\\nH_1: \\theta > \\theta_0\n\\end{eqnarray*}\\]Suppose \\(f(\\underline{x} | \\theta)\\) monotone likelihood ratio \\(T=r(\\underline{X})\\). Let \\(c\\) constant \\(P(T \\geq c | \\theta = \\theta_0) = \\alpha_0\\).\n\\[\\begin{eqnarray*}\n\\delta: \\{\\mbox{reject } H_0 \\mbox{ } T \\geq c \\}\n\\end{eqnarray*}\\]\nUMP test level \\(\\alpha_0\\).Note: \\(H_0: \\theta \\geq \\theta_0 \\rightarrow T \\leq c\\) UMP.Read proof (pages 562-563 DeGroot Schervish (2011)). parameter spaces consist consecutive intervals, likelihood ratio monotone, straightforward see \\(\\alpha\\) \\(\\beta\\) minimized.shouldn’t memorize directionality associated theorem one-sided UMP tests (instead use intuition behind theorem). example, consider opposite hypotheses:\n\\[\\begin{eqnarray*}\nH_0: \\theta \\geq \\theta_0\\\\\nH_1: \\theta < \\theta_0\n\\end{eqnarray*}\\]know \\(T \\uparrow\\) produces \\(\\frac{f(\\underline{X} | \\theta_2)}{f(\\underline{X} | \\theta_1)} \\uparrow\\)can visualize \\(\\theta_2 \\\\Omega_0\\) \\(\\theta_1 \\\\Omega_1\\) (convince makes sense)., small T mean evidence toward alternative. Therefore, reject \\(H_0\\) \\(T \\leq c\\).Example 8.9  Suppose \\(X_1, X_2, \\ldots, X_n \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\), desired test:\n\\[\\begin{eqnarray*}\nH_0: \\sigma^2 \\leq 2\\\\\nH_1: \\sigma^2 > 2\n\\end{eqnarray*}\\]Find UMP test level \\(\\alpha_0\\).Assume \\(n=20, \\alpha_0 = 0.01\\), find power function test part 1.Let \\(\\sigma_1^2 < \\sigma_2^2, \\sigma_1^2, \\sigma_2^2 \\\\Omega\\).\n\\[\\begin{eqnarray*}\n\\frac{f(\\underline{x} | \\sigma_2^2)}{f(\\underline{x} | \\sigma_1^2)} &=& \\Bigg(\\frac{\\sigma_1}{\\sigma_2} \\Bigg)^n \\frac{e^{-\\sum x_i^2 / 2 \\sigma_2^2}}{e^{-\\sum x_i^2 / 2 \\sigma_1^2}}\\\\\n&=& \\Bigg(\\frac{\\sigma_1}{\\sigma_2} \\Bigg)^n e^{\\sum x_i^2 ( 1/2 \\sigma_1^2 - 1/2 \\sigma_2^2)}\\\\\n( 1/2 \\sigma_1^2 - 1/2 \\sigma_2^2) &>& 0 \\\\\nT &=& \\sum X_i^2\\\\\n\\delta: && \\{ \\mbox{reject } H_0 \\mbox{ } \\sum x_i^2 \\geq c \\}\n\\end{eqnarray*}\\]\nUMP \\(\\alpha_0\\).\n\\(n=20, \\alpha_0 = 0.01\\), find \\(c\\).\n\\[\\begin{eqnarray*}\nP(\\sum X_i^2 \\geq c | \\sigma^2 = 2) &=& 0.01\\\\\nP(\\sum X_i^2/2 \\geq c/2 | \\sigma^2 = 2) &=& 0.01\\\\\nc/2 &=& 37.57\\\\\nc&=& 75.14\\\\\n\\delta: && \\{ \\mbox{reject } H_0 \\mbox{ } \\sum x_i^2 \\geq 75.14 \\}\n\\end{eqnarray*}\\]\npower function found calculating probability rejecting \\(H_0\\) various values \\(\\sigma^2\\).\n\\[\\begin{eqnarray*}\n\\pi(\\sigma^2 | \\delta) &=& P( \\mbox{reject } H_0 | \\sigma^2)\\\\\n&=& P (\\sum X_i^2 \\geq 75.14 | \\sigma^2)\\\\\n&=& P (\\sum X_i^2 / \\sigma^2 \\geq 75.14 / \\sigma^2)\\\\\n&=& 1 - \\chi^2_{20}(75.14 / \\sigma^2)\\\\\n\\pi(\\sigma^2 = 6 | \\delta) &=& 1 - \\chi^2_{20} (12.52) \\approx 0.80\\\\\n\\pi(\\sigma^2 = 3 | \\delta) &=& 1 - \\chi^2_{20} (25.05) \\approx 0.20\\\\\n\\pi(\\sigma^2 = 4 | \\delta) &=& 1 - \\chi^2_{20} (18.79) \\approx 0.45\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"ht.html","id":"power-functions-revisited","chapter":"8 Hypothesis Testing","heading":"8.4 Power functions revisited","text":"\\(X_i \\sim N(\\mu, \\sigma^2 (\\mbox{known})), \\delta_1: \\{ \\mbox{reject } H_0 \\mbox{ } \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\geq z_{1 - \\alpha_0} \\}\\)\n\\[\\begin{eqnarray*}\nH_0: \\mu \\leq \\mu_0\\\\\nH_1: \\mu > \\mu_0\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\pi(\\mu | \\delta_1) &=& P\\bigg(\\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\geq z_{1 - \\alpha_0} | \\mu \\bigg)\\\\\n&=& P\\bigg(\\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\geq \\frac{z_{1 - \\alpha_0} \\sigma / \\sqrt{n} + \\mu_0 - \\mu}{\\sigma / \\sqrt{n}} | \\mu \\bigg)\\\\\n&=& 1 - \\Phi \\bigg( \\frac{z_{1 - \\alpha_0} \\sigma / \\sqrt{n} + \\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\bigg)\n\\end{eqnarray*}\\]Alternatively, \\(\\delta_2: \\{ \\mbox{reject } H_0 \\mbox{ } \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\leq z_{\\alpha_0} \\}\\)\n\\[\\begin{eqnarray*}\nH_0: \\mu \\geq \\mu_0\\\\\nH_1: \\mu < \\mu_0\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\pi(\\mu | \\delta_2) &=&  \\Phi \\bigg( \\frac{z_{\\alpha_0} \\sigma / \\sqrt{n} + \\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\bigg)\n\\end{eqnarray*}\\]Last, \\(\\delta_3: \\{ \\mbox{reject } H_0 \\mbox{ } \\frac{|\\overline{X} - \\mu_0|}{\\sigma / \\sqrt{n}} \\geq z_{1-{\\alpha_0}/2} \\}\\)\n\\[\\begin{eqnarray*}\nH_0: \\mu = \\mu_0\\\\\nH_1: \\mu \\ne \\mu_0\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\pi(\\mu | \\delta_3) &=& 1 - \\Phi \\bigg( \\frac{z_{1 - {\\alpha_0}/2} \\sigma / \\sqrt{n} + \\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\bigg)  + \\Phi \\bigg( \\frac{ z_{ {\\alpha_0}/2} \\sigma / \\sqrt{n} + \\mu_0 - \\mu}{\\sigma / \\sqrt{n}} \\bigg)\n\\end{eqnarray*}\\]\nFigure 1.1: Power function mu. Top plot describes delta1 test; middle plot describes delta2 test; bottom plot describes delta3 (two-sided!) test. plot, dotted horizontal line 0.05 (size), dashed horizontal line 1, dotted vertical line 4 (null value mu).\n","code":""},{"path":"ht.html","id":"the-t-test","chapter":"8 Hypothesis Testing","heading":"8.5 The t-test","text":"t-test deserves special attention. Remember, ’s test \\(\\mu\\), variance unknown. work ratio likelihoods, ’ll pretty hard find MLR statistic. get likelihood ratios discussing t-test, first let’s think t-test paradigm.Example 8.10  Let’s say population radon detectors whose accuracy ’d like test. don’t want open packages, randomly select 12 put room 105 picocuries per liter (pCi/l) radon.\n\\[\\begin{eqnarray*}\nH_0: \\mu \\geq 105 \\mbox{ pCi/l}\\\\\nH_1: \\mu < 105 \\mbox{ pCi/l}\\\\\n\\end{eqnarray*}\\]\n\\(\\overline{x} = 104.13, \\sum(x_i - \\overline{x})^2 = 931, s=\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1}} = \\sqrt{931/11} = 9.20\\).reject \\(H_0\\) \\(\\overline{X}\\) small. small \\(\\overline{X}\\) ? Let \\(U = \\frac{\\overline{X} - \\mu_0}{s/\\sqrt{n}}.\\)\n\\[\\begin{eqnarray*}\n\\delta: \\{ \\mbox{reject } H_0 \\mbox{ } U \\leq c \\}\n\\end{eqnarray*}\\]\nseen previous sections, often test boils computing constant \\(c\\) gives test size \\(\\alpha_0\\). particular, need know boundary value give maximum type error \\(\\mu \\\\Omega_0\\). Note can find \\(c\\) \\(P( t_{11} \\leq c) = \\alpha_0\\). \\(\\mu=\\mu_0\\) \\(U \\sim t_{11}\\). null composite: \\(\\mu \\geq \\mu_0\\). distribution \\(U\\) \\(\\mu > \\mu_0?\\)\\[\\begin{eqnarray*}\n\\mbox{let } U^* = \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} &\\mbox{}& W = \\frac{\\mu - \\mu_0}{s/\\sqrt{n}}\\\\\nU = U^* + W & U^* \\sim t_{11} & W > 0\\\\\nP(U \\leq c) = P(U^* + W \\leq c)\\\\\n= P( U^* \\leq c  - W)\\\\\n< P( U^* \\leq c) = \\alpha_0\\\\\n\\end{eqnarray*}\\]\n, \\(\\mu > \\mu_0, P(U \\leq c) < \\alpha_0\\). ,\n\\[\\begin{eqnarray*}\n\\delta: \\{ \\mbox{reject } H_0 \\mbox{ } U \\leq c \\}\n\\end{eqnarray*}\\]\nsize \\(\\alpha_0\\) \\(c = t_{n-1}^{-1}(\\alpha_0)\\).Example 8.11  (Radon continued)\\(\\alpha=0.05, df = 11\\)\\[\\begin{eqnarray*}\n\\delta:&& \\{ \\mbox{reject } H_0 \\mbox{ } U \\leq c = -1.812\\}\\\\\nU &=& \\frac{104.13 - 105}{9.2/\\sqrt{12}} = -0.327 \\rightarrow \\mbox{ reject } H_0\\\\\n\\mbox{p-value} &=& P( t_{11} < -0.327) \\approx 0.35\n\\end{eqnarray*}\\]","code":""},{"path":"ht.html","id":"power-of-the-t-test","chapter":"8 Hypothesis Testing","heading":"8.5.1 Power of the t-test","text":"\\[\\begin{eqnarray*}\nH_0: \\mu \\geq \\mu_0\\\\\nH_1: \\mu < \\mu_0\n\\end{eqnarray*}\\]expect following characteristics power function:\n1. \\(\\pi(\\mu,\\sigma^2 | \\delta) = \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\mu=\\mu_0\\)\n2. \\(\\pi(\\mu,\\sigma^2 | \\delta) < \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\mu>\\mu_0\\)\n3. \\(\\pi(\\mu,\\sigma^2 | \\delta) > \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\mu<\\mu_0\\)\n4. \\(\\pi(\\mu,\\sigma^2 | \\delta) \\rightarrow 0 \\ \\ \\ \\ \\ \\ \\ \\mu \\rightarrow \\infty\\)\n5. \\(\\pi(\\mu,\\sigma^2 | \\delta) \\rightarrow 1 \\ \\ \\ \\ \\ \\ \\ \\mu \\rightarrow -\\infty\\)calculate power \\(\\mu = 103.5\\)?\n\\[\\begin{eqnarray*}\n\\pi(\\mu, \\sigma^2 | \\delta) &=& P( U \\leq -1.812\\bigg)\\\\\n&=& P\\bigg(\\frac{\\overline{X} - 105}{s/\\sqrt{n}} \\leq -1.812\\bigg)\\\\\n&=& P\\bigg( \\overline{X} \\leq 105 - 1.812 \\cdot s/\\sqrt{n}\\bigg)\\\\\n&=& P\\bigg( \\frac{\\overline{X} - 103.5}{s/\\sqrt{n}} \\leq \\frac{1.5 - 1.812 \\cdot s/\\sqrt{n}}{s/\\sqrt{n}}\\bigg)\\\\\n&=& P\\bigg( t_{n-1} \\leq \\frac{1.5 - 1.812 \\cdot s/\\sqrt{n}}{s/\\sqrt{n}}\\bigg)\\\\\n&=& P\\bigg( t_{n-1} \\leq \\mbox{ RANDOM VARIABLE!}\\bigg)\\\\\n&\\approx& P\\bigg( t_{n-1} \\leq \\frac{1.5 - 1.812 \\cdot 9.2/\\sqrt{12}}{9.2/\\sqrt{12}}\\bigg)\\\\\n0.10 & \\approx \\leq& \\pi(\\mu=103.5 | \\delta) \\approx \\leq 0.15\\\\\n0.85 &\\approx \\leq& \\beta \\approx \\leq 0.9\\\\\n\\end{eqnarray*}\\]point order calculate power t-test, end probability statement random variables sides inequality. isn’t worst idea approximate power considering righthand side number. However, true power function based called non-central t-distribution.two-sided test?\n\\[\\begin{eqnarray*}\nH_0: \\mu = \\mu_0\\\\\nH_1: \\mu \\ne \\mu_0\n\\end{eqnarray*}\\]\nnow reject \\(|U| > c\\), want \\(P(|U| > c | \\mu= \\mu_0) = \\alpha_0\\) (\\(P(U > c_1 \\mbox{ } U < c_2 | \\mu=\\mu_0) = \\alpha_0\\).)n=12, \\(U = \\frac{\\overline{X} - \\mu_0}{s/\\sqrt{n}} \\sim t_{11}\\).\n\\[\\begin{eqnarray*}\nP(|U| > c | \\mu= \\mu_0) = \\alpha_0 = 0.10 &\\rightarrow& c=1.363\\\\\nP(U > c_1 | \\mu=\\mu_0)=0.025,  P(U < c_2 | \\mu=\\mu_0) = 0.075 &\\rightarrow& c_1=2.201, c_2 = -1.58\\\\\n\\end{eqnarray*}\\]Example 8.12  (Radon example continued)\n\\[\\begin{eqnarray*}\nH_0: \\mu = 105 \\mbox{ pCi/l}\\\\\nH_1: \\mu \\ne 105 \\mbox{ pCi/l}\\\\\n\\end{eqnarray*}\\]\n\\(\\alpha_0 = 0.05, P(|U| > c | \\mu= \\mu_0) = 0.05 \\rightarrow c=2.2014\\).\n\\[\\begin{eqnarray*}\n\\mbox{p-value} &=& P(U < -0.327 \\mbox{ } U > 0.327)\\\\\n&=& 2 P( U > 0.327)\\\\\n&\\approx& 2 \\cdot 0.35 = 0.7\n\\end{eqnarray*}\\]","code":""},{"path":"ht.html","id":"likelihood-ratio-tests","chapter":"8 Hypothesis Testing","heading":"8.6 Likelihood Ratio Tests","text":"Likelihood ratio tests arguably used test statistics. good reasons !seen, theory suggests tests based likelihoods often quite powerful (typically powerful tests hypotheses).likelihood ratio test gives test statistic can use make decisions.general regularity conditions, asymptotic distribution likelihood ratio known.Suppose \n\\[\\begin{eqnarray*}\nH_0: \\theta \\\\Omega_0\\\\\nH_1: \\theta \\\\Omega_1\n\\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\n\\Lambda(\\underline{x}) = \\frac{\\sup_{\\theta \\\\Omega_1} f(\\underline{x} | \\theta)}{\\sup_{\\theta \\\\Omega_0} f(\\underline{x} | \\theta)}\n\\end{eqnarray*}\\]\nJust like Neyman-Pearson!! reject \\(H_0\\) \\(\\Lambda(\\underline{x}) \\geq k\\) (.e., likelihood big \\(\\Omega_1\\) compared \\(\\Omega_0).\\) , \\(k\\) chosen test given size, \\(\\alpha_0.\\) However, likelihood ratio tests, ideas uniformly powerful. fact, two-sided tests generally UMP (, typically possible find UMP test two-sided setting). one-sided test \\(\\mu < \\mu_0\\) powerful two-sided test one side; one-sided test \\(\\mu > \\mu_0\\) powerful two-sided test side.Example 8.13  Consider \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\) \\(\\mu\\) \\(\\sigma^2\\) unknown.\n\\[\\begin{eqnarray*}\nH_0: \\mu \\geq \\mu_0\\\\\nH_1: \\mu < \\mu_0\n\\end{eqnarray*}\\]\nlikelihood maximized? MLEs!! \\(\\hat{\\mu} = \\overline{X}, \\hat{\\sigma}^2 = (1/n) \\sum(X_i - \\overline{X})^2\\).\n\\[\\begin{eqnarray*}\n\\Lambda(\\underline{x}) &=& \\frac{\\sup_{\\mu < \\mu_0} f(\\underline{x} | \\mu, \\sigma^2)}{\\sup_{\\mu \\geq \\mu_0} f(\\underline{x} | \\mu, \\sigma^2)}\\\\\n&=& \\frac{ f(\\underline{x} | \\hat{\\mu}_1, \\hat{\\sigma}_1^2)}{ f(\\underline{x} | \\hat{\\mu_0}, \\hat{\\sigma}_0^2)}\\\\\n\\mbox{} \\overline{X} < \\mu_0 &\\rightarrow& (\\hat{\\mu}, \\hat{\\sigma}^2) \\\\Omega_1\\\\\n(\\mu, \\sigma^2) &\\& \\Omega_0 \\mbox{ maximized } \\hat{\\mu}_0 = \\mu_0, \\hat{\\sigma}_0^2 = (1/n) \\sum (X_i - \\mu_0)^2\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\Lambda(\\underline{x}) &=& \\frac{(1/\\sqrt{2 \\pi \\hat{\\sigma}_1^2})^n \\exp(\\frac{-1}{2 \\hat{\\sigma}_1^2} \\sum(x_i - \\hat{\\mu}_1)^2)} {(1/\\sqrt{2 \\pi \\hat{\\sigma}_0^2})^n \\exp(\\frac{-1}{2 \\hat{\\sigma}_0^2} \\sum(x_i - \\hat{\\mu}_0)^2)}\\\\\n&=& \\bigg(\\frac{\\hat{\\sigma}_0^2}{\\hat{\\sigma}_1^2} \\bigg)^{n/2} \\frac{\\exp \\frac{-1}{2 (1/n) \\sum(x_i - \\overline{x})^2} \\sum(x_i - \\overline{x})^2}{\\exp \\frac{-1}{2 (1/n) \\sum(x_i - \\mu_0)^2} \\sum(x_i - \\mu_0)^2}\\\\\n&=& \\bigg( \\frac{(1/n) \\sum(x_i - \\mu_0)^2}{(1/n) \\sum(x_i - \\overline{x})^2} \\bigg)^{n/2}\\\\\n\\mbox{} \\Lambda(\\underline{x}) &\\geq& k' \\rightarrow \\mbox{ reject } H_0\n\\end{eqnarray*}\\]note: \\(\\sum(X_i - \\mu_0)^2 = \\sum(X_i - \\overline{X})^2 + n(\\overline{X} - \\mu_0)^2\\)\\[\\begin{eqnarray*}\n\\Lambda(\\underline{x}) &=& \\bigg( \\frac{\\sum(x_i - \\overline{x})^2 + n(\\overline{x} - \\mu_0)^2}{\\sum(x_i - \\overline{x})^2} \\bigg)^{n/2} \\geq k' \\\\\n&=& \\bigg ( 1 + \\frac{(\\overline{x} - \\mu_0)^2}{\\sum(x_i - \\overline{x})^2 / n} \\bigg)^{n/2} \\geq k'\\\\\n&\\Rightarrow& \\frac{\\overline{X} - \\mu_0}{\\sqrt{\\sum(X_i - \\overline{X})^2/(n-1)} / \\sqrt{n}} \\leq k\n\\end{eqnarray*}\\]\n(\\(\\overline{x} < \\mu_0\\)).derived t-test!!!!","code":""},{"path":"ht.html","id":"tests-of-goodness-of-fit","chapter":"8 Hypothesis Testing","heading":"8.7 Tests of Goodness-of-fit","text":"Let’s say want test M&M’s claim distribution colors candy:data distributed? Multinomial. Note claim data now distribution parameter. multinomial distribution one certain situation (think back binomial requirements) spread (like continuous distributions). ’re really testing respective probabilities group, ’re testing many parameters simultaneously. (\\(N_i\\) observed counts cell)\n\\[\\begin{eqnarray*}\nf(\\underline{N}) = \\frac{n!}{N_1! N_2! \\cdots N_m!} p_1^{N_1}p_2^{N_2} \\cdots p_m^{N_m}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nH_0:&& p_i = p_i^0 \\ \\ \\ \\ \\forall \\\\\nH_1:&& p_i \\ne p_i^0 \\ \\ \\ \\ \\mbox{} \\\\\n\\mbox{recall LRT: } \\Lambda(\\underline{x}) &=& \\frac{\\max_{\\Omega_1} f(\\underline{x})}{\\max_{\\Omega_0} f(\\underline{x})}\n\\end{eqnarray*}\\]reject \\(H_0\\) \\(\\Lambda(\\underline{x}) \\geq c\\).\\[\\begin{eqnarray*}\n\\Lambda(\\underline{N}) &=& \\frac{\\frac{n!}{N_1! N_2! \\cdots N_m!} \\hat{p}_1^{N_1} \\hat{p}_2^{N_2} \\cdots \\hat{p}_m^{N_m}}{\\frac{n!}{N_1! N_2! \\cdots N_m!} p_1^{0 N_1} p_2^{0 N_2} \\cdots p_m^{0 N_m}}\\\\\n&=& \\prod_{=1}^m \\Big( \\frac{\\hat{p_i}}{p_i^0} \\Big)^{N_i}\\\\\n\\hat{p}_i &=& \\frac{N_i}{n} \\ \\ \\ \\ \\mbox{(MLE)}\\\\\n\\ln \\Lambda(\\underline{N}) &=& \\sum_{=1}^m N_i \\ln \\Big( \\frac{N_i/n}{p_i^0} \\Big)\\\\\n&=& \\sum_{=1}^m N_i \\ln \\Big( \\frac{N_i}{n p_i^0} \\Big)\\\\\n\\end{eqnarray*}\\]general LRT (reasonable conditions) following holds (proof outside scope work):\n\\[\\begin{eqnarray*}\n2 \\ln \\Lambda(\\underline{X}) \\stackrel{n \\rightarrow \\infty}{\\rightarrow} \\chi^2_\\nu\n\\end{eqnarray*}\\]\n\\(\\nu =\\) dim \\(\\Omega_1\\) - dim \\(\\Omega_0.\\) , dim \\(\\Omega_1 = m-1,\\) dim \\(\\Omega_0 = 0.\\) Note dimension gives dimension parameter space (think Euclidean world live , piece paper (sort ) two dimensional, people three dimensional). parameter space possible values parameter. \\(\\Omega_0,\\) one possible value parameter, point. point dimension. \\(\\Omega_1\\) possible values parameter live \\(m-1\\) dimensions. two groups, \\(p_1\\) anything 0 1. \\(p_2\\) \\(p_1 + p_2 = 1.\\) means space possible values one dimensional.think problem estimating \\(\\mu\\) (t-test)… alternative space two dimensional space \\(\\Omega_1 = (\\mu, \\sigma^2): \\mu \\\\!\\!R, \\sigma^2 \\\\!\\!R+ )\\). null space one dimensional specify value \\(\\mu\\), \\(\\Omega_0 = (\\mu, \\sigma^2): \\mu = \\mu_0, \\sigma^2 \\\\!\\!R+ )\\). gives us degrees freedom \\(\\chi^2\\) test mean: \\(\\nu = dim(\\Omega_1) - dim(\\Omega_0) = 2-1=1.\\)likelihood ratio test:\n\\[\\begin{eqnarray*}\n2 \\ln \\Lambda(\\underline{N}) &=& 2 \\sum_{=1}^m N_i \\ln \\Big( \\frac{N_i}{n p_i^0} \\Big)\\\\\n&\\stackrel{n \\rightarrow \\infty}{\\rightarrow}& \\chi^2_{m-1}\n\\end{eqnarray*}\\]\nNote: \\(H_0\\) true \\(n\\) quite large, \\(\\hat{p}_i \\approx p_i^0\\) \\(N_i \\approx n p_i^0\\). Taylor expansion \\(f(x) = x \\ln (x / x_0) \\mbox{ } x_0\\) gives \\(f(x) = (x - x_0) + \\frac{1}{2}(x-x_0)^2\\frac{1}{x_0} + \\ldots .\\)\n\\[\\begin{eqnarray*}\n2 \\ln \\Lambda(\\underline{N}) &\\approx& 2 \\sum_{=1}^m (N_i - n p_i^0) + 2 \\frac{1}{2} \\sum_{=1}^m \\Big( \\frac{(N_i - n p_i^0)^2}{n p_i^0} \\Big)\\\\\n&\\approx& \\sum_{=1}^m \\Big( \\frac{(N_i - n p_i^0)^2}{n p_i^0} \\Big)\\\\\n&\\approx& \\sum_{\\mbox{cells}} \\frac{(\\mbox{observed} - \\mbox{expected})^2}{\\mbox{expected}}\\\\\n&\\rightarrow& \\chi^2_{\\nu = m-1}\n\\end{eqnarray*}\\]Note: derivation lead us \\(\\chi^2\\) test ’ve seen many times (equivalency likelihood ratio test), also allows us (said equivalent) testing without using \\(\\ln\\) function. might care much computing logs, certainly used big deal (many years ago tests derived).Example 8.14  (M & M’s)Let’s say get following data:\\[\\begin{eqnarray*}\n\\chi^2 &=& \\frac{(13 - .3\\cdot55)^2}{.3 \\cdot 55} + \\frac{(8 - .2\\cdot55)^2}{.2 \\cdot 55} + \\frac{(5 - .1\\cdot55)^2}{.1 \\cdot 55} +\\frac{(10 - .1\\cdot55)^2}{.1 \\cdot 55} \\\\\n&& + \\frac{(5 - .1\\cdot55)^2}{.1 \\cdot 55} +\\frac{(14 - .2\\cdot55)^2}{.2 \\cdot 55}\\\\\n&=& 6.15\\\\\n\\mbox{p-value}&=& P(\\chi^2_5 > 6.15) > 0.25\\\\\n\\end{eqnarray*}\\]\nreject null hypothesis. evidence say M & M’s lying color distribution candy.general, asymptotics associated \\(\\chi^2\\) tests hold least 5 observations cell.","code":""},{"path":"ht.html","id":"goodness-of-fit-for-composite-hypotheses","chapter":"8 Hypothesis Testing","heading":"8.7.1 Goodness-of-Fit for Composite Hypotheses","text":"","code":""},{"path":"ht.html","id":"testing-distributions","chapter":"8 Hypothesis Testing","heading":"8.7.1.1 Testing distributions","text":"Notice can use results test particular distribution data. example,\n\\[\\begin{eqnarray*}\nH_0:&& \\mbox{ pdf uniform / normal / exponential...}\\\\\nH_1: &&\\mbox{} H_0\n\\end{eqnarray*}\\]\n, degrees freedom \\(\\nu =\\) # cells - # parameters - 1. (Note dim \\(\\Omega_0 =\\) # parameters, dim \\(\\Omega_1 = m-1\\).)","code":""},{"path":"ht.html","id":"bayes-test-procedures","chapter":"8 Hypothesis Testing","heading":"8.8 Bayes Test Procedures","text":"Recall using Bayesian procedures, can find posterior distribution parameter. , can create distribution gives us probability associated interval interest parameter. Seemingly, natural Bayesian hypothesis test form:\\[\\begin{eqnarray*}\n\\delta = \\{ \\mbox{reject } H_0 \\mbox{ } P(H_0 \\mbox{ true } | \\underline{X}) \\leq ? \\}\n\\end{eqnarray*}\\]problem structure changed (\\(\\mu\\) now random variable!), can’t talk distribution \\(\\mu\\) assumption \\(\\mu \\\\Omega_0\\) \\(\\mu \\\\Omega_1.\\) makes hard talk type type II errors posterior distribution. might consider test rejects \\(H_0\\) \\(P(H_0 \\mbox{ true } | \\underline{X}) \\leq \\alpha_0\\), haven’t considered whether \\(H_0\\) true false.Instead, general idea choose decision leads smaller posterior expected loss. assume loss making right decision zero (, don’t idea “gain”). Let:Yikes! Now conclusions slightly different. ’s logic different actually measure probability null hypothesis true!loss matrix can written (\\(L(\\theta, d_i)\\) loss units \\(\\omega_0\\) \\(\\omega_1).\\)Assume \\(\\Omega_0\\) \\(\\Omega_1\\) simple hypotheses:\n\\[\\begin{eqnarray*}\nH_0: \\theta = \\theta_0\\\\\nH_1: \\theta = \\theta_1\\\\\n\\end{eqnarray*}\\]Let:\\(\\xi_0\\) prior probability \\(H_0\\) true\\(\\xi_1\\) prior probability \\(H_1\\) trueThe expected loss test procedure based prior information :\n\\[\\begin{eqnarray*}\nr(\\delta) &=& \\xi_0 E[loss | \\theta=\\theta_0] + \\xi_1 E[loss | \\theta=\\theta_1]\\\\\nE[loss | \\theta=\\theta_0] &=& \\omega_0 P(d_1 | \\theta=\\theta_0) = \\omega_0 \\alpha(\\delta)\\\\\nE[loss | \\theta=\\theta_1] &=& \\omega_1 P(d_0 | \\theta=\\theta_1) = \\omega_1 \\beta(\\delta)\\\\\nr(\\delta) &=& \\xi_o \\omega_0 \\alpha(\\delta) + \\xi_1 \\omega_1 \\beta(\\delta)\\\\\n\\end{eqnarray*}\\]procedure minimizes \\(r(\\delta)\\) called Bayes Test Procedure. (Notice \\(r(\\delta)\\) just linear combination \\(\\alpha\\) \\(\\beta\\)!! Bayes test procedure can immediately determined Theorem 8.2.1 DeGroot Schervish (2011):\\[\\begin{eqnarray*}\n\\delta: \\{ \\mbox{reject } H_0 \\mbox{ } \\xi_0 \\omega_0 f_0(\\underline{x}) < \\xi_1 \\omega_1 f_1(\\underline{x})\\}\\\\\n\\end{eqnarray*}\\]However, want follow previous Bayesian ideas, makes sense think posterior distributions minimize posterior expected loss. Considering composite hypotheses, posterior expected loss :\\[\\begin{eqnarray*}\nr(d_i | \\underline{x}) &=& \\int_{\\Omega_i} L(\\theta, d_i) \\xi(\\theta | \\underline{x}) d\\theta\\\\\nr(d_0 | \\underline{x}) &=& \\int_{\\Omega_1} \\omega_1 \\xi(\\theta | \\underline{x}) d\\theta = \\omega_1[1-P(H_0 true | \\underline{x})]\\\\\nr(d_1 | \\underline{x}) &=& \\int_{\\Omega_0} \\omega_0 \\xi(\\theta | \\underline{x}) d\\theta = \\omega_0 P(H_0 true | \\underline{x}) \\\\\n\\end{eqnarray*}\\]Bayes test procedure one chooses decision smaller posterior expected loss:\n\\[\\begin{eqnarray*}\n\\mbox{choose } d_0 \\mbox{ } r(d_0 | \\underline{x}) < r(d_1 | \\underline{x})\\\\\n\\mbox{choose } d_1 \\mbox{ } r(d_0 | \\underline{x}) > r(d_1 | \\underline{x})\\\\\n\\delta: \\{ \\mbox{reject } H_0 \\mbox{ } P(H_0 \\mbox{ true } | \\underline{x}) \\leq \\frac{\\omega_1}{\\omega_0 + \\omega_1} \\}\n\\end{eqnarray*}\\]Bayes test situations given loss table (zeros loss). result holds whether MLR, one vs. two-sided, discrete vs. continuous parameters, etc.","code":""},{"path":"ht.html","id":"two-sided-alternatives","chapter":"8 Hypothesis Testing","heading":"8.8.0.1 Two-sided alternatives","text":"\\[\\begin{eqnarray*}\nH_0: \\theta = \\theta_0\\\\\nH_1: \\theta \\ne \\theta_0\\\\\nP(H_0 \\mbox{ true }| \\underline{x}) = 0\n\\end{eqnarray*}\\]\ndon’t even need look data…\\[\\begin{eqnarray*}\nH_0: |\\theta - \\theta_0| \\leq d\\\\\nH_1: |\\theta - \\theta_0| > d\\\\\n\\end{eqnarray*}\\]\nmight want choose hypotheses meaningful. ’d need choose d. Note, ’re trying balance practical versus statistical significance.","code":""},{"path":"ht.html","id":"improper-priors-1","chapter":"8 Hypothesis Testing","heading":"8.8.0.2 Improper Priors","text":"improper priors, t-tests, F-tests, etc. exactly Frequentist likelihood ratio tests saw previously. (Using \\(\\omega_0\\) \\(\\omega_1\\) set \\(\\alpha\\), particular, \\(\\alpha_0 = \\frac{\\omega_1}{\\omega_0 + \\omega_1}.)\\)Example 8.15  Suppose electronic failure can occur either major minor defect. 80% defects minor, 20% major. failure occurs, \\(n\\) independent tests made system. failure due minor defect, \\(X \\sim\\) Poisson(3). failure major, \\(X \\sim\\) Poisson(7). cost deciding failure major actually minor $400. cost deciding failure minor actually major $2500. decision minimizes cost?\\[\\begin{eqnarray*}\nH_0: \\mbox{minor } \\lambda=3\\\\\nH_1: \\mbox{major } \\lambda=7\\\\\n\\end{eqnarray*}\\]Reject :\n\\[\\begin{eqnarray*}\n0.8 \\cdot 400 f(\\underline{x} | 3) &<& 0.2 \\cdot 2500 f(\\underline{x} | 7)\\\\\n320 \\frac{e^{-3n}3^{\\Sigma x_i}}{\\prod x_i !} &<& 500 \\frac{e^{-7n}7^{\\Sigma x_i}}{\\prod x_i !}\\\\\n0.64 e^{4n} &<& (7/3)^{\\Sigma x_i}\\\\\n\\Sigma x_i &>& \\frac{ \\ln(0.64) + 4n}{\\ln(7/3)}\n\\end{eqnarray*}\\]Example 8.16  Suppose situation Bayes test rejects \\(H_0\\) P(\\(H_0\\) true \\(| \\underline{x}) \\leq \\alpha_0\\) level \\(\\alpha_0\\) test \\(H_0\\) \\(\\alpha_0\\) [“” means sample \\(\\underline{x}\\), tests come conclusion reject \\(H_0\\) don’t reject \\(H_0.]\\) Prove p-value equals posterior probability \\(H_0\\) true.Solution:\nArgue contradiction. Suppose \\(\\underline{x}\\) p-value equal posterior probability \\(H_0\\) true. First, suppose p-value greater. Let \\(\\alpha_0\\) greater posterior probability less p-value. tst rejects \\(H_0\\) \\(P(H_0 \\mbox{ true } \\leq \\alpha_0)\\) reject \\(H_0\\), level \\(\\alpha_0\\) test reject \\(H_0\\) p-value greater \\(\\alpha_0\\). contradicts fact two tests . p-value smaller, argument identical.","code":""},{"path":"ht.html","id":"foundational-ideas","chapter":"8 Hypothesis Testing","heading":"8.9 Foundational Ideas","text":"Section 9.9 (DeGroot Schervish 2011) lots important ideas ’ve talked last weeks. fair game final, probably read section.relationship level significance sample sizestatistically significant resultsExample 8.17  Suppose random sample 10,000 observations taken normal distribution unknown mean \\(\\mu\\) known variance 1, desired test following hypotheses level significance 0.05.\n\\[\\begin{eqnarray*}\nH_0: && \\mu=0\\\\\nH_1: && \\mu \\ne 0\n\\end{eqnarray*}\\]\nSuppose test procedure specifies rejecting \\(H_0\\) \\(|\\overline{X}| \\geq c\\), constant \\(c\\) chosen \\(P(|\\overline{X}| \\geq c | \\mu=0) = 0.05.\\) Find probability test reject \\(H_0\\) () actual value \\(\\mu\\) 0.01, (b) actual value \\(\\mu\\) 0.02.Solution:\\(\\mu=0\\), \\(\\frac{\\overline{X}-0}{1/\\sqrt{10,000}}\\) standard normal distribution. Therefore, \\(P(100 | \\overline{X}| > 1.96 | \\mu=0) = 0.05\\). Therefore, \\(c=1.96/100 = 0.0196\\)\\(\\mu=0.01\\), random variable \\(\\frac{\\overline{X} - 0.01}{1/\\sqrt{10,000}}\\) standard normal distribution.\n\\[\\begin{eqnarray*}\nP(|\\overline{X}| < c) &=& P(-1.96 < 100 \\overline{X} < 1.96 | \\mu=0.01)\\\\\n&=& P(-2.96 < Z < 0.96)\\\\\n&=& 0.8315 - 0.0015 = 0.83\n\\end{eqnarray*}\\]\nTherefore, \\(P(|\\overline{X}| \\geq c | \\mu = 0.01) = 0.17\\)\\(\\mu=0.01\\), random variable \\(\\frac{\\overline{X} - 0.01}{1/\\sqrt{10,000}}\\) standard normal distribution.\n\\[\\begin{eqnarray*}\nP(|\\overline{X}| < c) &=& P(-1.96 < 100 \\overline{X} < 1.96 | \\mu=0.01)\\\\\n&=& P(-2.96 < Z < 0.96)\\\\\n&=& 0.8315 - 0.0015 = 0.83\n\\end{eqnarray*}\\]\nTherefore, \\(P(|\\overline{X}| \\geq c | \\mu = 0.01) = 0.17\\)\\(\\mu=0.02\\), random variable \\(\\frac{\\overline{X} - 0.02}{1/\\sqrt{10,000}}\\) standard normal distribution.\n\\[\\begin{eqnarray*}\nP(|\\overline{X}| < c) &=& P(-1.96 < 100 \\overline{X} < 1.96 | \\mu=0.02)\\\\\n&=& P(-3.96 < Z < -.04)\\\\\n&=& 0.484\n\\end{eqnarray*}\\]\nTherefore, \\(P(|\\overline{X}| \\geq c | \\mu = 0.02) = 0.516.\\)\\(\\mu=0.02\\), random variable \\(\\frac{\\overline{X} - 0.02}{1/\\sqrt{10,000}}\\) standard normal distribution.\n\\[\\begin{eqnarray*}\nP(|\\overline{X}| < c) &=& P(-1.96 < 100 \\overline{X} < 1.96 | \\mu=0.02)\\\\\n&=& P(-3.96 < Z < -.04)\\\\\n&=& 0.484\n\\end{eqnarray*}\\]\nTherefore, \\(P(|\\overline{X}| \\geq c | \\mu = 0.02) = 0.516.\\)","code":""},{"path":"ht.html","id":"reflection-questions-7","chapter":"8 Hypothesis Testing","heading":"8.10  Reflection Questions","text":"One important tools statisticians can bring table understanding variability. , don’t take results test, CI, model truth. Instead, think likely results one setting another setting. use power way differentiating processes (tests, algorithms, statistics) comparison others.happens power sample size changes (increases / decreases)?9What happens power curve two-sided test?10Why (values) one-sided test powerful two-sided hypothesis?11Why can’t find exact power associated t-test? mean probability statement associated two different random variables?12What difference size, level significance, power?13What mean result “significant”?14How can hypothesis testing used assess whether data come particular probability model (.e., population given pdf)?15Let’s say person using test “\\(-\\overline{X} \\leq c\\)” person B using test “\\(\\frac{1}{\\overline{X}} \\leq c'\\)”. mean say two tests (given two people use level significance)?16","code":""},{"path":"ht.html","id":"ethics-considerations-7","chapter":"8 Hypothesis Testing","heading":"8.11  Ethics Considerations","text":"worse, type type II errors?17N-P, MLR, LRT provide tests “best” (close best) way. way? way criteria assessing whether test good?18Is p-value probability null hypothesis true? Explain.19","code":""},{"path":"ht.html","id":"r-code","chapter":"8 Hypothesis Testing","heading":"8.12 R code:","text":"","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
