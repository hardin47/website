[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 152 Pomona College: Computational Statistics. notes based extensively Probability & Statistics (DeGroot Schervish 2011).responsible reading relevant chapters text. text good & readable, use . make sure coming class also re-visiting warm-ups feedback provided.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"","code":""},{"path":"intro.html","id":"what-is-statistics","chapter":"1 Introduction","heading":"1.1.0.1 What is Statistics?","text":"Generally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims population","code":""},{"path":"intro.html","id":"what-is-the-content-of-math-152","chapter":"1 Introduction","heading":"1.1.0.2 What is the content of Math 152?","text":"Statistical Theory introduction statistics students background probability theory, calculus, linear algebra. statistics prerequisite class. course focused theoretical aspects material, though real world applications class homework assignments. idea strong mathematical understanding concepts also understanding concepts applied real world.completion course, students :able derive methods introductory statistics using tools mathematics (.e., calculus probability).able justify use particular method (technical conditions).able weigh advantages disadvantages different estimation techniques (e.g., bias, variability, resistance outliers).know communicate results effectively.","code":""},{"path":"intro.html","id":"who-should-take-math-152","chapter":"1 Introduction","heading":"1.1.0.3 Who should take Math 152?","text":"Linear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.","code":""},{"path":"intro.html","id":"what-are-the-prerequisites-for-math-152","chapter":"1 Introduction","heading":"1.1.0.4 What are the prerequisites for Math 152?","text":"prerequisites Statistical Theory Probability (Math 151 equivalent) completion three semester sequence calculus linear algebra. rely heavily prerequisites, students background probability multivariable calculus find trying catch throughout semester. familiar topics conditional probabilities expectations, Central Limit Theorem, moment generating functions, probability density functions.","code":""},{"path":"intro.html","id":"is-there-overlap-with-other-classes","chapter":"1 Introduction","heading":"1.1.0.5 Is there overlap with other classes?","text":"Statistical Theory covers many topics Introductory Statistics (e.g., Math 58), however, treatment topics completely different.\nStatistical Theory uses mathematical tools (e.g., probability theory calculus) derive topics prove , many ways, optimal.","code":""},{"path":"intro.html","id":"when-should-i-take-math-152","chapter":"1 Introduction","heading":"1.1.0.6 When should I take Math 152?","text":"prerequisite structure, students Math 152 juniors seniors.","code":""},{"path":"intro.html","id":"what-is-the-workload-for-math-152","chapter":"1 Introduction","heading":"1.1.0.7 What is the workload for Math 152?","text":"one homework assignment per week, two -class midterm exams, two take-home midterm exams, final exam. Many students report working 8-10 hours per week outside class.##software use? real world applications? mathematics? CS?majority work Statistical Theory done pencil (/ pen / LaTeX).\nHowever, use R way investigating theorecial concepts computationally.\nR work done using RStudio IDE.\nneed either download R RStudio (free) onto computer use Pomona’s server.\nreal-world applications motivate theory, cours applied focus data analysis.can use R Pomona server: https://rstudio.pomona.edu/ (Pomona students able log immediately.\nNon-Pomona students need get Pomona login information.)Alternatively, feel free download R onto computer.\nR freely available http://www.r-project.org/ already installed college computers.\nAdditionally, required install RStudio turn R assignments using RMarkdown. http://rstudio.org/.\n(can use LaTeX compiler : https://yihui.name/tinytex/)\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Although Statistical Theory statistics prerequisite, students seen many building blocks high school classes Probability Theory.\nreview vocabulary covered previous contexts.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"","code":""},{"path":"intro.html","id":"probability","chapter":"1 Introduction","heading":"Probability","text":"probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).experiment activity situation uncertainty outcome.experiment activity situation uncertainty outcome.sample space list possible outcomes random trial. (Called S.)sample space list possible outcomes random trial. (Called S.)event potential subset sample space.event potential subset sample space.set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).simple event event consisting exactly one outcome.simple event event consisting exactly one outcome.Two events mutually exclusive disjoint occur simultaneously.Two events mutually exclusive disjoint occur simultaneously.Two events independent occurrence one change probability second occur.Two events independent occurrence one change probability second occur.random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]","code":""},{"path":"intro.html","id":"statistics","chapter":"1 Introduction","heading":"Statistics","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]\\(\\Omega\\) parameter space, set values contains possible realizations parameter.many statistical problems, assume distribution generated data completely known except parameters. goal statistics make statements (inference) population using sample (data). course broken two main parts: parameter estimation tests hypotheses.","code":""},{"path":"intro.html","id":"estimation","chapter":"1 Introduction","heading":"1.2.2 Estimation","text":"many statistical problems, assume distribution generated data completely known except parameter(s). first half course, ’re going learn different ways estimate parameters, properties estimators, makes good estimator.","code":""},{"path":"intro.html","id":"example","chapter":"1 Introduction","heading":"example","text":"Consider trying estimate true average weight dozen eggs particular farm. add random 12 values pdf different multiplying one observation 12. Clearly 12*one egg much variable weight dozen eggs.\n\\[\\begin{align*}\n\\mbox{statistic } 1 &= 12 X\\\\\n\\mbox{statistic } 2 &= \\sum_{=1}^{12} X_i\\\\\n\\Omega &= \\{ \\mu: \\mu \\\\!\\!R+\\}\\\\\n\\end{align*}\\]","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3  Reflection Questions","text":"difference sample population?difference discrete random variable continuous random variable?difference pdf cdf?can find expected value discrete random variable? can find expected value continuous random variable?difference R RStudio?","code":""},{"path":"intro.html","id":"ethics-considerations","chapter":"1 Introduction","heading":"1.4  Ethics Considerations","text":"important distinguish sample population?ever okay model discrete random variable using continuous probability model? , ? , ?better understand method’s properties theoretically better use computational tools investigate method’s properties? [question probably won’t able answer end semester.]","code":""},{"path":"intro.html","id":"r-code-reproduciblity","chapter":"1 Introduction","heading":"1.5 R code: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.5.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.5.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.5.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"bayes.html","id":"bayes","chapter":"2 Bayesian Estimation","heading":"2 Bayesian Estimation","text":"","code":""},{"path":"bayes.html","id":"bayes-rule","chapter":"2 Bayesian Estimation","heading":"2.1 Bayes’ Rule","text":"Theorem 2.1  Bayes’ RuleGiven events \\(\\) \\(B\\),\n\\[\\begin{eqnarray*}\nP(|B) &=& \\frac{P(AB)}{P(B)} = \\frac{P(B|) P()}{P(AB) + P(^cB)} \\nonumber \\\\\n&=& \\frac{P(B|)P()}{ \\sum_i P(B|A_i) P(A_i)}\n\\end{eqnarray*}\\]Many following examples may familiar . reading , work understand intuition (denominator changes condition!) well mathematical connection Bayes’ Rule.Example 2.1  Suppose rate infection TB 1 1000 (0.1 percent = 0.001). Suppose TB test used 90% accurate: gives positive result 10 percent people actually TB, reaction skin test. Also, 10% people actually TB fail react test.1What’s chance someone TB test positive?’s chance randomly chosen person tests negative actually TB?another TB test gives fewer false positives, expensive. better use one?prior probability TB?posterior probability TB (given positive test)?Solution:can use table figure probabilities. Consider population 10,000 people:Alternatively, can use probability statements easier work long-run scenarios get complicated. know following:\\[\\begin{eqnarray*}\nP(TB + ) &=& 0.001\\\\\nP(Test + | TB - ) &=& 0.1 \\\\\nP(Test - | TB + ) &=& 0.1\n\\end{eqnarray*}\\]10 10,000 people disease. 9 10 actually test positive TB. However, 999 9990 people false positives. , \\(9/(999+9) = 0.0089 \\mbox{ } \\approx\\) 0.9% people test positive actually TB.\\[\\begin{eqnarray*}\nP( TB + | Test + ) &=& \\frac{P(TB + \\& Test + )}{P(Test + )} = \\frac{P(Test  + | TB + ) P(TB+)}{P(Test + )}\\\\\n&=& \\frac{P(Test  + | TB + ) P(TB+)}{P(Test  + | TB - ) P(TB-) + P(Test  + | TB + ) P(TB+)} \\\\\n&=& \\frac{0.9 \\cdot 0.001}{ 0.1 \\cdot 0.999 + 0.9 \\cdot 0.001}\\\\\n&=& 0.0089\n\\end{eqnarray*}\\]1 10,000 people false negative (0.0001 = 0.01% opposed 9.99% false positives).\\[\\begin{eqnarray*}\nP( TB + | Test - ) &=& \\frac{P(TB + \\& Test - )}{P(Test - )} = \\frac{P(Test  - | TB + ) P(TB+)}{P(Test - )}\\\\\n&=& \\frac{P(Test - | TB + ) P(TB+)}{P(Test  - | TB - ) P(TB-) + P(Test  - | TB + ) P(TB+)} \\\\\n&=& \\frac{0.1 \\cdot 0.001}{ 0.9 \\cdot 0.999 + 0.1 \\cdot 0.001}\\\\\n&=& 0.00011121\n\\end{eqnarray*}\\]necessarily, since ’s much worse false negative false positive. People test positive given another test fewer false positives.0.0010.009Example 2.2  cab involved hit run accident night. Two cab companies, Green Blue, operate city. Suppose told following:85 percent cabs city Green, remaining 15 percent Blue.witness identified cab Blue (dark!) court tested reliability witness circumstances existed night accident, determined witness correctly identified cab color 80% time, made mistake 20% time, regardless actual color cab.’s verdict? .e., probability cab involved hit--run actually Blue?Solution:information given, following probabilities known:\n\\[\\begin{eqnarray*}\nP(said B | B) &=& 0.8\\\\\nP(said G | B) &=& 0.2\\\\\nP(B) &=& 0.15 \\ \\ \\ \\mbox{prior probability!}\n\\end{eqnarray*}\\]probability interest :\n\\[\\begin{eqnarray*}\nP(B | said B) &=& \\frac{P(said B | B) P(B)}{P(said B | G) P(G) + P(said B | B) P(B)}\\\\\n&=& \\frac{0.8*0.15}{0.2*0.85 + 0.8*0.15} = 0.41\n\\end{eqnarray*}\\]Example 2.3  Consider famous Monte Hall problem based game show, Let’s Make Deal. part show, contestant asked pick one three doors. Two doors nothing behind , third door car prize. Monte Hall (host) opens non-prize door contestant hadn’t chosen (always door available open one prize). Monte offers contestant opportunity switch original door remaining door. switch? Stay? doesn’t matter? probability winning situations?Define following:\n\\[\\begin{eqnarray*}\nC_i &:& \\mbox{car behind Door $$, } \\\\{ 1, 2, 3\\}\\\\\nH_{ij} &:& \\mbox{host opens Door $j$ player picked Door $$, } , j \\\\{ 1,2,3\\}\n\\end{eqnarray*}\\]example, \\(C_2\\) situation car behind Door 2, \\(H_{23}\\) denotes situation host opened Door 3 chose Door 2. Note \\(H_{ij}\\) information (’ll condition ). Let’s say start picking Door 2, host opens Door 3.\\[\\begin{eqnarray*}\nP(C_i) &=& \\frac{1}{3}\\\\\nP(C_2) &=& \\frac{1}{3}\\\\\nP(C_2 | H_{23}) &=& \\frac{P(H_{23} | C_2) P(C_2)}{P(H_{23})} \\ \\ \\ \\ \\ \\mbox{(Bayes rule, right !  See } P(H_{23}) \\mbox{ )}\\\\\n&=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{P(H_{23})} \\\\\n\\nonumber \\\\\nP(H_{23}) &=&  P(H_{23} C_1) + P(H_{23} C_2) + P(H_{23} C_3)\\\\\n&=&  P(H_{23} | C_1) P(C_1) + P(H_{23} | C_2) P(C_2) + P(H_{23} | C_3) P(C_3)\\\\\n&=& \\frac{1}{2} \\cdot \\frac{1}{3} + 1 \\frac{1}{3} + 0 \\frac{1}{3} = \\frac{1}{2}\\\\\n\\nonumber\\\\\nP(C_2 | H_{23}) &=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}}\\\\\n&=& \\frac{1}{3}\\\\\n\\end{eqnarray*}\\]Using know rules game (, car behind door 3, ’d never open !), know:\n\\[\\begin{eqnarray*}\nP(C_3 | H_{23}) = 0\n\\end{eqnarray*}\\]car behind one three doors:\n\\[\\begin{eqnarray*}\n1 &=& P(C_1 | H_{23}) + P(C_2 | H_{23}) + P(C_3 | H_{23})\\\\\nP(C_1 | H_{23} ) &=& 1 - P(C_2 | H_{23}) - P(C_3 | H_{23})\\\\\n&=& 1 - \\frac{1}{3} - 0\\\\\n&=& \\frac{2}{3}\n\\end{eqnarray*}\\]…. probability car behind Door 1 2/3 probability ’s behind Door 2 1/3 \\(\\rightarrow\\) switch doors!previous situations based discrete values parameter data (neither typically true). think {} {}, want find value {} \\(P(|B)\\) maximized. going use probability distribution functions (pdfs) instead discrete probabilities, need notation.","code":""},{"path":"bayes.html","id":"prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.2 Prior Distributions","text":"prior distribution distribution parameter (e.g., \\(\\theta\\)) observing data. Note:observations come \\(f(x|\\theta), \\theta \\\\Omega\\)can express likely \\(\\theta\\) various regions \\(\\Omega\\) terms probability distribution \\(\\theta\\).Bayesians believe use prior distributions modeling always know something situation hand.Frequentists believe use data collected experiment sample (prior information).Example 2.4  want predict high temperature given day October.\n\\[\\begin{align*}\n\\Omega = \\{ (\\theta, \\sigma^2) &: \\theta \\\\!\\!R, \\sigma^2 \\\\!\\!R^+\\}\\\\\n\\mbox{} &: -\\infty < \\theta < \\infty, \\sigma^2 > 0 \\}\\\\\n\\mbox{} &: \\theta > 30, 0 < \\sigma^2 < 625 \\}\\\\\n\\end{align*}\\]\nSuppose know variance \\(\\sigma^2 = 12^2\\). mean, \\(\\theta\\) unknown. might specify prior distribution \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\xi (\\theta) &\\rightarrow& \\theta \\sim N(\\mu, \\nu^2)\\\\\n&& \\mu =78^\\circ, \\nu^2 = (2.5^\\circ)^2\\\\\n\\end{eqnarray*}\\]value \\(\\nu\\) measure uncertainty prior beliefs. , estimated \\(\\theta\\) \\(78^\\circ\\), aren’t sure value, add uncertainty belief (\\(\\nu\\)). \\(\\mu\\) \\(\\nu\\) called hyper-parameters.","code":""},{"path":"bayes.html","id":"posterior-distributions","chapter":"2 Bayesian Estimation","heading":"2.3 Posterior Distributions","text":"posterior distribution conditional distribution parameter (e.g., \\(\\theta\\)) given observed data.Aside, little probability review:Example 2.5  Suppose interested rolling two dice. Let \\(X\\) larger value; let \\(Y\\) sum two dice. Find joint marginal distributions \\(X\\) \\(Y\\). solution table probabilities:notation review. Suppose \\(n\\) data points \\(f(x| \\theta)\\). ’ll assume ’s simple random sample (SRS), therefore observations independent.\\[\\begin{eqnarray*}\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(x_1| \\theta) f(x_2| \\theta) \\cdots f(x_n| \\theta)\\\\\n\\mbox{ } \\underline{x} &=& \\{ x_1, x_2, \\ldots, x_n \\}\\\\\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(\\underline{x} | \\theta)\n\\end{eqnarray*}\\]Remember, \\(f(\\underline{x} | \\theta)\\) conditional distribution \\(\\underline{x}\\) given \\(\\theta\\). likelihood function, \\(f(\\underline{x} | \\theta)\\), joint pdf observations (representing: likely data?).Define\n\\[\\begin{eqnarray*}\nf(\\underline{x}, \\theta) &=& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\ng_n(\\underline{x}) &=& \\int_\\Omega f(\\underline{x}, \\theta) d\\theta\\\\\n&=& \\int_\\Omega f(\\underline{x} | \\theta) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]Remember, however, interested probability parameter given data:\n\\[\\begin{eqnarray*}\n\\xi(\\theta| \\underline{x}) = \\frac{f(\\underline{x} | \\theta) \\xi(\\theta)}{g_n(\\underline{x})} \\ \\ \\ \\ \\ \\ \\ \\theta \\\\Omega\n\\end{eqnarray*}\\](Bayes’ Theorem!!!)prior, \\(\\xi(\\theta)\\) relative likelihood \\(\\theta\\) data observed.posterior, \\(\\xi(\\theta | \\underline{x})\\) relative likelihood \\(\\theta\\) \\(\\underline{X} = \\underline{x}\\) observed.know posterior function \\(\\theta\\). ’s important keep mind function , posterior function data.\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi(\\theta)\\\\\n\\mbox{posterior} &\\propto& \\mbox{likelihood} \\cdot \\mbox{prior}\n\\end{eqnarray*}\\], posterior proportional product likelihood prior. Note \\(g_n(\\underline{x})\\) depend \\(\\theta\\) part proportionality constant. , can always find \\(g_n(\\underline{x})\\) know posterior integrates 1. (Sometimes \\(g_n\\) extraordinarily difficult find.)\n\\[\\begin{eqnarray*}\n\\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta = 1\n\\end{eqnarray*}\\]Example 2.6  Suppose true proportion freethrows Steph Curry able make successfully unknown. assume freethrows distributed according Bernoulli process.\\[\\begin{eqnarray*}\nX = \\left\\{ \\begin{array}{ll}\n    1 & \\mbox{Curry makes shot}\\\\\n    0 & \\mbox{Curry misses shot}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]say,\n\\[\\begin{eqnarray*}\nX &\\sim& \\mbox{Bernoulli}(\\theta)\\\\\nf(\\underline{x}|\\theta)&=&  \\theta ^y (1 - \\theta)^{n-y} \\ \\ \\ \\ y = \\sum_{=1}^n x_i\n\\end{eqnarray*}\\]\nNote: \\(\\underline{x} = \\{x_1, x_2, \\ldots, x_n\\}\\) specific ordering 0s 1s.prior information Curry’s abilities, put uniform prior \\(\\theta\\).\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    1 & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) \\propto \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]functional form posterior (likelihood times prior). ’re trying estimate \\(\\theta\\), distribution \\(\\theta\\) given data? Recall Beta distribution (probability review):\\[\\begin{eqnarray*}\nW &\\sim& \\mbox{Beta}(\\alpha,\\beta)\\\\\nf(w) &=& \\frac{1}{B(\\alpha,\\beta)} w^{\\alpha - 1} (1-w)^{\\beta-1}\\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}w^{\\alpha - 1} (1-w)^{\\beta-1} \\ \\ \\ \\ w \\[0,1]\n\\end{eqnarray*}\\]Using likelihood prior, able find full posterior distribution integration:\n\\[\\begin{eqnarray*}\n1 &=& \\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta \\\\\n&=&  k \\cdot \\int_\\Omega \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta) d\\theta\\\\\n&=& k \\cdot \\int_0^1 \\theta^y (1-\\theta)^{n-y} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(y+1 + n-y + 1)} \\cdot \\int_0^1 \\frac{\\Gamma(y+1 + n-y + 1)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^{y+1-1} (1-\\theta)^{n-y+1-1} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)} \\cdot 1\\\\\nk &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\\\\ng(\\underline{x}) &=& \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)}\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]Note, however, didn’t actually need integrate find \\(g(\\underline{x})\\). Instead, needed note \\(\\theta\\) takes place \\(w\\) Beta distribution, automatically know appropriate constant value.information prior distribution \\(\\theta\\)? Suppose believe Beta(,b). (Note: \\(E[\\theta] = \\frac{}{+b}\\), Var\\((\\theta) = \\frac{ab}{(+b)^2 (+b+1)}\\), SD$() = $ ).\\[\\begin{eqnarray*}\n\\xi(\\theta) &\\propto& \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y+-1} (1-\\theta)^{n-y+b-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y+, n-y+b)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n++b)}{\\Gamma(y+)\\Gamma(n-y+b)}\\theta^{y+-1} (1-\\theta)^{n-y+b-1}\n\\end{eqnarray*}\\]Note: didn’t need calculate \\(g(\\underline{x})\\)!!!\\(E[\\theta | \\underline{x}] = \\frac{y+}{n++b}\\)Var \\((\\theta | \\underline{x}) = \\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}\\)SD \\((\\theta | \\underline{x}) = \\sqrt{\\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}}\\)Given prior sample size \\(n\\), best guess Curry’s ability hit freethrows?kind confidence estimate?","code":""},{"path":"bayes.html","id":"conjugate-prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.4 Conjugate Prior Distributions","text":"conjugate prior distribution one prior distribution family posterior distribution. Beta distribution conjugate Binomial distribution (note, \\(U[0,1]\\) distribution Beta(\\(\\alpha\\)=1,\\(\\beta\\)=1)).Example 2.7  Continuing example temperature, \\(X \\sim N(\\theta, \\sigma^2\\) (known)) normal prior, \\(\\theta \\sim N(\\mu, \\nu^2)\\). Remember typically, prior completely specified. example \\(\\mu=78\\) \\(\\nu=2.5\\). :\n\\[\\begin{eqnarray*}\nf(\\underline{x} | \\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\theta)^2 \\bigg]\\\\\n&\\propto& \\exp \\Bigg[ - \\frac{1}{2 \\sigma^2} \\bigg(n (\\theta-\\overline{x})^2 + \\sum_{=1}^n (x_i - \\overline{x})^2 \\bigg) \\Bigg]\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2\\bigg]\\\\\n&&\\\\\n\\xi(\\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n&&\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2 -\\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n\\mbox{note: } && \\frac{n}{\\sigma^2}(\\theta - \\overline{x})^2 + \\frac{1}{\\nu^2}(\\theta - \\mu)^2 = \\frac{1}{\\nu_1^2}(\\theta - \\mu_1)^2 + \\frac{n}{\\sigma^2 + n \\nu^2}(\\overline{x}-\\mu)^2 \\ \\ \\ \\mbox{ pg 399}\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu_1^2} (\\theta - \\mu_1)^2 \\bigg]\\\\\n\\theta | \\underline{x} &\\sim& N (\\mu_1, \\nu_1^2)\\\\\n\\mbox{: } && \\mu_1 = \\frac{\\sigma^2 \\mu + n \\nu^2 \\overline{x}}{\\sigma^2 + n \\nu^2} \\ \\ \\ \\ \\nu_1^2 = \\frac{\\sigma^2 \\nu^2}{\\sigma^2 + n \\nu^2}\n\\end{eqnarray*}\\]Remember, computing posterior \\(\\theta\\), can ignore anything doesn’t depend \\(\\theta\\) (“known” parameters, data, constants,…)conjugate families. Note conjugate, prior distribution family posterior distribution.","code":""},{"path":"bayes.html","id":"improper-priors","chapter":"2 Bayesian Estimation","heading":"2.5 Improper Priors","text":"Improper prior distributions actually probability functions, yet lead posterior distributions probability functions (, integrate 1). Improper priors capture idea data worth prior belief. Often, improper prior lead Frequentist result. example, Beta(0,0) prior Bernoulli likelihood leads :\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\left\\{ \\begin{array}{ll}\n    \\theta^{-1}(1-\\theta)^{-1} & 0 \\leq \\theta \\leq 1\\\\\n    0 & \\mbox{else}\\\\\n    \\end{array} \\right.\n\\end{eqnarray*}\\]\\(\\xi(\\theta)\\) integrate 1, proper pdf. However posterior proper pdf,\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y-1} (1-\\theta)^{n-y-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y, n-y)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)}\\theta^{y-1} (1-\\theta)^{n-y-1}\\\\\n\\mbox{Note: } && E[\\theta | \\underline{X} ] = \\frac{y}{n} \\ \\ \\ \\mbox{ expected frequentist model!}\n\\end{eqnarray*}\\]conjugate priors improper prior limiting case: Beta(0,0), gamma(0,0), Normal(\\(\\mu, \\nu^2 = \\infty\\)). normal improper prior ignores prior constant, becomes:\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\lim_{\\nu^2 \\rightarrow \\infty} exp\\bigg(-\\frac{1}{2\\nu^2}(\\theta - \\mu)^2\\bigg) = 1\n\\end{eqnarray*}\\], improper normal prior flat line reals. Note improper normal prior used normal likelihood, posterior \\(\\theta | \\underline{x} \\sim N (\\underline{x}, \\sigma^2 /n)\\). [, prior indicating knowledge \\(\\theta\\) produces posterior depends data.]","code":""},{"path":"bayes.html","id":"bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.6 Bayes’ Estimators","text":"Prior posterior distributions tell us Bayesians think parameters. next question need address think estimators? estimator function data hope close true value parameter.Note:\n\\[\\begin{eqnarray*}\n\\delta(X_1, X_2, \\ldots, X_n) &=& \\delta(\\underline{X}) \\mbox{  estimator}\\\\\n\\delta(x_1, x_2, \\ldots, x_n) &=& \\delta(\\underline{x}) \\mbox{  estimate}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"loss-functions","chapter":"2 Bayesian Estimation","heading":"2.6.1 Loss functions","text":"(responsible material loss functions. take away message section Bayes estimator use expected value posterior distribution. However, seen , Bayes estimators, example, median posterior distribution used.)want estimator \\(\\theta\\) leads estimate close true value \\(\\theta\\). loss function helps determine far estimator . particular estimate, \\(\\):\n\\[\\begin{eqnarray*}\n\\mbox{squared error loss: } L(\\theta, ) &=& (\\theta - )^2\\\\\n\\mbox{absolute error loss: } L(\\theta, ) &=& |\\theta - |\\\\\n\\end{eqnarray*}\\]\nwant loss small (minimized).","code":""},{"path":"bayes.html","id":"squared-error-loss","chapter":"2 Bayesian Estimation","heading":"Squared Error Loss","text":"Without data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,)] = \\int_\\Omega L(\\theta, ) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,) | \\underline{X}] &=& \\int_\\Omega L(\\theta, ) \\xi(\\theta| \\underline{X}) d\\theta\\\\\n\\end{eqnarray*}\\]\nLet \\(\\delta^*(\\underline{X})\\) value \n\\[\\begin{eqnarray*}\nE[L(\\theta, \\delta^*(\\underline{X})) | \\underline{X} ] &=& \\min_{\\\\Omega} E[ L(\\theta,) | \\underline{X} ]\\\\\n\\delta^*(\\underline{X}) && \\mbox{ Bayes estimator } \\theta\\\\\n\\delta^*(\\underline{x}) && \\mbox{ Bayes estimate } \\theta\\\\\n\\end{eqnarray*}\\]\n\\(\\delta^*\\)?Note: \\(\\delta^*\\) depends loss function prior / posterior.\\[\\begin{eqnarray*}\nE[L(\\theta, ) | \\underline{X} ] &=& E[ (\\theta - )^2 | \\underline{X}]\\\\\n&=& E[ \\theta^2 - 2a\\theta + ^2 | \\underline{X}]\\\\\n&=& E[\\theta^2 | \\underline{X}] - 2a E[\\theta | \\underline{X}] + ^2\\\\\n&&\\\\\n\\frac{\\partial E[ (\\theta - )^2 | \\underline{X}]}{\\partial } &=& 0\\\\\n&&\\\\\n- 2 E[\\theta | \\underline{X}] + 2a &=& 0\\\\\n&=& E[\\theta | \\underline{X}] = \\delta^*(\\underline{X}) !!!\\\\\n&&\\\\\n\\frac{\\partial^2 E[ (\\theta - )^2 | \\underline{X}]}{\\partial ^2} &=& 2 > 0 \\rightarrow \\mbox{ loss minimized}\\\\\n\\end{eqnarray*}\\]Example 2.8  Let \\(\\theta\\) denote average number defects per 100 feet tape. \\(\\theta\\) unknown, prior \\(\\theta\\) gamma distribution \\(E[\\theta] = \\alpha / \\beta = 2/10, \\alpha= 2, \\beta = 10\\). 1200 foot roll tape inspected, exactly 4 defects found.Bayes’ estimate average number defects per 100 feet?\\[\\begin{eqnarray*}\n\\mbox{Prior:     }&&\\\\\n\\xi(\\theta) &=& \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta \\theta} = \\frac{10^2}{\\Gamma(2)} \\theta e^{-10\\theta}\\\\\n\\mbox{Likelihood:     }&&\\\\\nf(\\underline{x} | \\theta) &=& \\prod_{=1}^n \\frac{e^{-\\theta} \\theta^{x_i}}{x_i!} = \\frac{e^{-n\\theta} \\theta^{\\sum x_i}}{\\prod (x_i !)}\\\\\n\\mbox{Posterior:     }&&\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\frac{ \\theta e^{-10\\theta} e^{-n \\theta} \\theta^{\\sum x_i}}{\\Gamma(2) 10^2 \\prod (x_i !)}\\\\\n&\\propto& e^{-\\theta(n+10)} \\theta ^{\\sum x_i + 1} \\\\\n\\\\\n\\theta | \\underline{x} &\\sim& \\mbox{Gamma } (\\sum x_i + 2 = 6, n + 10 = 22)\\\\\n\\\\\n\\delta^*(\\underline{X}) &=& \\frac{ \\sum X_i + 2}{n+10}\\\\\n\\delta^*(\\underline{x}) &=& \\frac{6}{22} = \\frac{3}{11}\n\\end{eqnarray*}\\]\nNote: Gamma distribution parameterized slightly differently DeGroot sheet (exponential). Make sure expected value matches ’ve given problem.","code":""},{"path":"bayes.html","id":"absolute-loss","chapter":"2 Bayesian Estimation","heading":"Absolute Loss","text":"minimize \\(E[ | \\theta - | | \\underline{X}]\\) ? \\(\\rightarrow \\ \\ \\ \\delta^*(\\underline{X}) = \\mbox{median} (\\theta | \\underline{X})\\) (see Theorem 4.5.1 DeGroot Schervish (2011)). However, isn’t always obvious compute median non-symmetric distributions. symmetric distributions median = mean.","code":""},{"path":"bayes.html","id":"evaluating-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.7 Evaluating Bayes Estimators","text":"","code":""},{"path":"bayes.html","id":"mean-squared-error","chapter":"2 Bayesian Estimation","heading":"2.7.1 Mean Squared Error","text":"get MSE, let’s talk Agresti-Coull estimate Binomial parameter probability success. estimator attenuates sample proportion closer 0.5 effect reducing variability estimator. Indeed, evidence “add two successes two failures” approach create precise confidence intervals.2Even though agreement better estimate, ’d like show biased.\\[\\begin{eqnarray*}\n\\tilde{p} &=& \\frac{X+2}{n+4}\\\\\nE[\\tilde{p}] &=& \\frac{n\\theta+2}{n+4}\\\\\nbias(\\tilde{p}) &=& \\frac{n\\theta+2}{n+4} - \\theta\\\\\n&=& \\frac{n\\theta + 2 - n\\theta - 4\\theta}{n+4}\\\\\n&=& \\frac{2-4\\theta}{n+4}\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"mse-in-general-frequentist","chapter":"2 Bayesian Estimation","heading":"MSE in general (“Frequentist”)","text":"Mean Squared Error (MSE) expected squared difference parameter (\\(\\theta\\)) estimate (\\(\\hat{\\theta}\\)), , typically, \\(\\hat{\\theta}\\) function data.frequentist MSE based expected values taken likelihood pdf (\\(X | \\theta\\)).Note MSE can written sum bias squared variance:\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_F(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_F(\\hat{\\theta}) &=& E [ (\\hat{\\theta} - \\theta)^2 ]\\\\\n&=& E [ ( \\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2 ] \\mbox{ RV X!} \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2 + 2(\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2]\\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + 0 + E[ (E(\\hat{\\theta}) - \\theta)^2 ] \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + (E(\\hat{\\theta}) - \\theta)^2  \\\\\n&=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\n\\end{eqnarray*}\\]Note taking expected values, resulting MSE function \\(\\theta\\) (function data).example, consider Bernoulli situation (e.g., Basketball shooter):\n\\[\\begin{eqnarray*}\n\\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\nVar(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\nbias(\\hat{\\theta}) &=& E(\\hat{\\theta}) - \\theta = 0\\\\\nMSE_F(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayes.html","id":"bayesian-mse","chapter":"2 Bayesian Estimation","heading":"Bayesian MSE","text":"Bayesians, MSE expected squared error loss conditional data (, expected value taken posterior, \\(\\theta | \\underline{X}\\).)Bayesian MSE based expected values taken posterior pdf \\((\\theta | \\underline{X}).\\)let \\(\\delta = \\delta(\\underline{X}) = E(\\theta | \\underline{X})\\) estimator, MSE :\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_B(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_B(\\delta(\\underline{X})) &=& E [ (\\delta - \\theta)^2 | \\underline{X}]\\\\\n&=& E [ ( E(\\theta | \\underline{X}) - \\theta)^2 | \\underline{X}] \\mbox{RV } \\theta \\mbox{!!}\\\\\n&=& \\mbox{var}(\\theta | \\underline{X})\\\\\n\\end{eqnarray*}\\]Continuing example, note \\(\\theta \\sim Beta(,b)\\), means \\(\\theta | \\underline{X} \\sim Beta(X + , n - X + b)\\).\\[\\begin{eqnarray*}\n\\delta(X) &=& \\frac{X + }{ n - X + b}\\\\\nMSE_B(\\delta(X)) &=& \\frac{(X+)(n++b)}{(n++b)^2(n++b+1)}\\\\\n\\end{eqnarray*}\\]Note Bayesian MSE posterior variance parameter interest. ’ve used expected value estimate, bias. Note Bayesian MSE function data (\\(\\theta\\)), compare Bayesian MSE Frequentist MSE directly.Example 2.9  Recall tape example, Example 2.8.Prior: Gamma(2, 10) ((2, 1/10) depending parametrize))Data likelihood: Poisson(\\(\\theta\\))Posterior: Gamma (\\(\\sum X_i + 2\\), \\(n\\) + 10)Note calculations done Frequentist MSE conditional data.\\[\\begin{eqnarray*}\n(\\mbox{frequentist estimator}) \\ \\ \\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\n(\\mbox{Bayesian estimator}) \\ \\ \\delta(\\underline{X}) &=& \\frac{\\sum X_i+2}{n+10}\\\\\n& \\\\\nMSE_F(\\hat{\\theta}) &=& var(\\hat{\\theta}) + bias(\\hat{\\theta})^2\\\\\n&=& \\theta/n + 0 = \\theta/n\\\\\nMSE_B(\\delta(\\underline{X})) &= & var(\\theta | \\underline{X})\\\\\n&=& \\frac{\\sum X_i+2}{(n+10)^2}\\\\\n&&\\\\\nMSE_F(\\delta(\\underline{X})) &=& var(\\delta(\\underline{X})) + bias(\\delta(\\underline{X}))^2\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nbias(\\delta(\\underline{X})) &=& E\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg] - \\theta\\\\\n&=& \\frac{n \\theta +2}{n+10} - \\theta = \\frac{n\\theta + 2 - n\\theta -10\\theta}{n+10}\\\\\n&=& \\frac{2-n\\theta}{n+10}\\\\\nvar(\\delta(\\underline{X})) &=& var\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg]\\\\\n&=& \\frac{1}{(n+10)^2}var\\bigg(\\sum X_i\\bigg)\\\\\n&=& \\frac{1}{(n+10)^2} n \\  var(X_i)\\\\\n&=& \\frac{n}{(n+10)^2} \\theta\\\\\n\\end{eqnarray*}\\]\\[\\begin{align*}\nMSE_F(\\delta(\\underline{X})) &= \\frac{n}{(n+10)^2} \\theta + \\frac{(2-n\\theta)^2}{(n+10)^2}\\\\\n&= \\frac{n \\theta+ (2-n\\theta)^2}{(n+10)^2}\n\\end{align*}\\]\nNote couldn’t directly compare \\(MSE_F\\) \\(MSE_B\\) (functions different variables!). ’d come prior think \\(MSE_B(\\hat{\\theta})\\), seems like can’t calculate quantity. Instead, take easier route, find \\(MSE_F(\\delta(\\underline{X}))\\) order reasonable comparison estimators.","code":""},{"path":"bayes.html","id":"sensitivity-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.2 Sensitivity of Estimators","text":"sensitive results different priors?Example 2.10  Continuing tape example, Example 2.8, different values estimate theta depending different priors data values:Note: \\(E[ \\theta | \\underline{x} ] = \\frac{\\sum x_i + \\alpha}{ n + \\beta} = w_1 \\frac{\\sum x_i}{n} + w_2 \\frac{\\alpha}{\\beta}\\), \\(w_1 = \\frac{n}{n+\\beta}, w_2 = \\frac{\\beta}{n+\\beta}\\).\\\n\\(n \\rightarrow \\infty, \\hat{\\theta} \\rightarrow \\frac{\\sum x_i}{n}\\), \\(n \\rightarrow 0, \\hat{\\theta} \\rightarrow \\frac{\\alpha}{\\beta}\\).","code":""},{"path":"bayes.html","id":"consistency-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.3 Consistency of Estimators","text":"consistent estimator \\(\\theta\\) one converges probability \\(\\theta\\). Many Bayes estimators consistent. fact, fairly general regularity conditions, wide class Bayes estimators consistent.Note, estimator \\(Y_n\\) converges \\(\\theta\\) probability :\n\\[\\begin{eqnarray*}\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | < \\epsilon ] &=& 1 \\ \\ \\ \\ \\ \\mbox{pg 233}\\\\\n\\mbox{, equivalently}\\\\\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | \\geq \\epsilon ] &=& 0\\\\\n\\end{eqnarray*}\\](saw idea weak strong laws large numbers: \\(\\overline{X} \\stackrel{P}{\\rightarrow} \\mu\\) \\(n \\rightarrow \\infty\\) Weak Law Large Numbers.) [n.b. case curious, strong law large numbers says \\(\\overline{X} \\stackrel{.s.}{\\rightarrow} \\mu\\) (almost surely). means \\(\\lim_{n \\rightarrow \\infty} P [ \\overline{X} = \\mu ] = 1\\)Example 2.11  Continuing tape example, Example 2.8:\n\\[\\begin{eqnarray*}\n\\delta^*(\\underline{X}) &=& \\frac{\\sum X_i + \\alpha}{n+\\beta}\\\\\n\\overline{X} &\\stackrel{P}{\\rightarrow}& \\theta \\mbox{ (WLLN)}\\\\\n\\mbox{} && \\\\\n\\delta^*(\\underline{X}) - \\overline{X} &=& \\frac{- \\beta}{n+\\beta} \\overline{X} + \\frac{\\beta}{n+\\beta} \\frac{\\alpha}{\\beta} \\stackrel{P}{\\rightarrow} 0 \\mbox{ (Slutsky's theorem)}\\\\\n&& \\\\\n\\delta^*(\\underline{X}) &\\stackrel{P}{\\rightarrow} \\theta\\\\\n\\end{eqnarray*}\\]\\(\\delta^*(\\underline{X})\\) consistent estimator \\(\\theta\\).","code":""},{"path":"bayes.html","id":"benefits-and-limitations-of-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.8 Benefits and Limitations of Bayes’ Estimators","text":"","code":""},{"path":"bayes.html","id":"benefits","chapter":"2 Bayesian Estimation","heading":"2.8.1 Benefits","text":"can incorporate informationthe interpretation intuitive","code":""},{"path":"bayes.html","id":"limitations","chapter":"2 Bayesian Estimation","heading":"2.8.2 Limitations","text":"need prior informationit can difficult produce prior two parameters simultaneously (e.g., normal, gamma)need agree prior information","code":""},{"path":"bayes.html","id":"additional-examples","chapter":"2 Bayesian Estimation","heading":"2.9 Additional Examples","text":"Example 2.12  Suppose Beta(4,4) prior distribution probability \\(\\theta\\) coin yield head spun specified manner. coin independently spun ten times, heads appears fewer 3 times. told many heads seen, number less 3. Calculate exact posterior density \\(\\theta\\).3Example 2.13  Baseball Bayes4You statistician employed Ball Consulting. Veteran major-league baseball scout Rocky Chew seeks advice regarding estimating probability amateur baseball player John Spurrier get base hit major-league pitcher. Rocky arranged Spurrier ten bats major-league pitcher.traditional batting average, \\(\\hat{\\theta}_f = X/n\\) frequentist estimator makes use observed data, ignores prior information might exist.5 assume bats independent Bernoulli trials constant probability getting base hit, \\[\\begin{eqnarray*}\nX \\sim Bin( n=\\mbox{number bat}, \\theta=\\mbox{P(getting base hit)})\n\\end{eqnarray*}\\]\\(\\hat{\\theta}_f\\), good estimator unknown probability (getting base hit), ignores information might baseball. following prior information:John Spurrier appears good great player. one better batters somewhat -average American Legion (high school) baseball team.major-league scouts watched play believe Spurrier’s batting ability professional level.barely adequate major-league hitter batting average 0.200.good major-league batter batting average 0.300.Ty Cobb -time best major-league batting average 0.366.’re going use Beta prior incorporate previous knowledge. prior look like?John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).Example 2.14  Kidney Cancer rates6 example, ’re going use Bayes theory adjust kidney cancer rates less variability. First, ’d like investigate counties highest lowest kidney cancer death rates US (white men, 1980-1989).patterns see figures 2.3 & 2.4? Can give plausible reasons patterns see?county 100 people? Small counties variable. Keep mind rates age-adjusted.\nFigure 1.6: Figure 2.3 Teaching Statistics, bag tricks Gelman Nolan.\nConsider figure 13.4, highest 10% Bayes-estimated kidney cancer death rates US (white men, 1980-1989). Let’s assume number deaths distributed Poisson(\\(n_j \\theta_j\\)) \\(n_j\\) number people county, \\(\\theta_j\\) true kidney cancer death rate county. , assume outside influences kidney cancer (e.g., pollution) county’s cancer rate comes Gamma distribution parameters (\\(\\alpha = 61, \\beta = 47000\\)). ,\\[\\begin{eqnarray*}\n\\mbox{Likelihood:} && y_j \\sim \\mbox{ Poisson}(n_j \\theta_j) \\ \\ \\ n_j = \\mbox{ county population}\\\\\n\\mbox{Prior:} && \\theta_j \\sim \\mbox{ Gamma}(\\alpha=61, \\beta = 47000)\\\\\n&& E[\\theta_j] = \\alpha / \\beta = 1.296 \\times 10^{-3} \\ \\ \\ \\ (\\mbox{10 yr cancer rate})\\\\\n\\end{eqnarray*}\\]\nknow \\(E[\\theta | y] = \\frac{\\alpha + y}{m + \\beta}\\). estimate compare frequentist estimate, \\(\\hat{\\theta} = \\frac{y}{m}\\)?investigate relationship Bayes estimate versus frequentist estimate, ’re going simulate kidney cancer death rates variety counties.Everyone gets county (population). county, ’ll generate true, underlying, kidney cancer rate \\(\\theta_j\\). (Note, \\(\\theta_j\\) sampled (simulated) Gamma(\\(\\alpha=61, \\beta=47,000\\)) distribution.)Using cancer rate (\\(\\theta_j\\)) county’s population (\\(n_j\\)), simulate value number people county died kidney cancer last 10 years (Poisson(\\(n_j \\theta_j\\))).Report frequentist estimate kidney cancer rate county (erase / hide true cancer rate). Leave county name, population, number deaths, estimated rate.public officials left task guessing counties highest cancer rates… think?Calculate Bayes estimate cancer rate. Compare underlying (\\(\\theta_j\\)), observed / frequentist (\\(\\frac{y_j}{n_j}\\)), posterior / Bayesian (\\(\\hat{\\theta}_j | y_j\\)) kidney cancer death rates.\nFigure 2.1: Figure 2.4 Teaching Statistics, bag tricks Gelman Nolan.\n","code":""},{"path":"bayes.html","id":"the-experiment","chapter":"2 Bayesian Estimation","heading":"The Experiment","text":"John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).","code":""},{"path":"bayes.html","id":"reflection-questions-1","chapter":"2 Bayesian Estimation","heading":"2.10  Reflection Questions","text":"prior distribution? random variable described prior?likelihood? random variable described likelihood?posterior distribution? random variable described posterior?conjugate prior? benefit conjugate prior?hope lost prior conjugate? , approach problem coming posterior?","code":""},{"path":"bayes.html","id":"ethics-considerations-1","chapter":"2 Bayesian Estimation","heading":"2.11  Ethics Considerations","text":"make sense incorporate prior information? doesn’t make sense incorporate prior information?legitimate ways calculate prior? illegitimate ways calculate prior?analyst able come prior likelihood?","code":""},{"path":"bayes.html","id":"r-code-bayesian-example","chapter":"2 Bayesian Estimation","heading":"2.12 R code: Bayesian Example","text":"Example 2.15  Recall Baseball Bayes example, Example 2.13.functions allow us label plot parameter information.trying variety different values \\(\\) \\(b\\), prior distributions can visualized.prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\nlibrary(tidyverse)\nlibrary(glue)\n\nex <- function(a,b) {round(a / (a+b), 2)}\nsdx <- function(a,b) {round(sqrt(a*b/((a+b)^2 * (a+b+1))),3)}\n\nbeta_legend <- function(a,b) {\n  glue::glue('a = {a}, ',\n             'b = {b}, ',\n             'EX = {ex(a,b)}, ',\n             'SDX = {sdx(a,b)}')}\n\n# see it in action:\nggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"bayes.html","id":"priors","chapter":"2 Bayesian Estimation","heading":"Priors:","text":"trying variety different values \\(\\) \\(b\\), prior distributions can visualized.","code":"\nlibrary(patchwork)\n\np1 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3,17), n = 100) + \n  ggtitle(beta_legend(3,17)) + ylab(\"y\") + xlab(\"theta\")\np2 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2,15), n = 100) + \n  ggtitle(beta_legend(2,15)) + ylab(\"y\") + xlab(\"theta\") \np3 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1,8), n = 100) + \n  ggtitle(beta_legend(1,8)) + ylab(\"y\") + xlab(\"theta\")\np4 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(12,68), n = 100) + \n  ggtitle(beta_legend(12,68)) + ylab(\"y\") + xlab(\"theta\") \np5 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(1, 17), n = 100) + \n  ggtitle(beta_legend(1, 17)) + ylab(\"y\") + xlab(\"theta\") \np6 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(3, 47), n = 100) + \n  ggtitle(beta_legend(3, 47)) + ylab(\"y\") + xlab(\"theta\") \np7 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(2, 36), n = 100) + \n  ggtitle(beta_legend(2, 36)) + ylab(\"y\") + xlab(\"theta\") \np8 <- ggplot(data = data.frame(x = c(0, 1)), mapping = aes(x = x)) +\n  stat_function(fun = dbeta, args = c(9, 162), n = 100) + \n  ggtitle(beta_legend(9, 162)) + ylab(\"y\") + xlab(\"theta\") \n\n\n(p1 + p2) / (p3 + p4) + \n  plot_annotation(\n    title = \"Possible Prior Distributions I\")\n(p5 + p6) / (p7 + p8) + \n  plot_annotation(\n    title = \"Possible Prior Distributions II\")"},{"path":"bayes.html","id":"mse","chapter":"2 Bayesian Estimation","heading":"MSE:","text":"prior distribution used? answer depends! course, lot information situation, use steep prior contains known information. information weak, use flat prior.Mean Squared Error can used determine prior correct one use, know true value \\(\\theta\\)!! case, ’ll compare MSE Bayesian estimator MSE frequentist estimator various truth conditions. comparing apples oranges (Bayesian vs. frequentist), forced use frequentist formulation MSE (way find expected value variance \\(\\theta\\) random variable frequentist paradigm).Consider \\(X\\) random variable Binomial(n=10, \\(\\theta\\)) distribution. Bayesian setting, \\(\\hat{\\theta} = (x+\\alpha) / (n+\\alpha+\\beta)\\). Deriving \\(MSE\\) (function \\(\\theta\\)) given homework problem.\\[\\begin{eqnarray*}\n\\mbox{MSE}_F(\\hat{\\theta}) &=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\\\\\n&=& \\frac{(\\alpha-\\alpha\\theta-\\beta\\theta)^2+n\\theta(1-\\theta)}{(n+\\alpha+\\beta)^2}\n\\end{eqnarray*}\\]MSE can used assess estimator (may may function prior information). Note value x-axis truth, value y-axis good / bad estimator (measured mean squared error).","code":"\n# frequentist MSE for Bayesian estimator\nmse_b <- function(t,a,b,n) {\n  ( (a - a*t - b*t)^2 + n*t*(1-t) ) / (n + a + b)^2\n}\n\n# frequentist MSE for frequentist estimator\nmse_f <- function(t, n){\n  t*(1-t) / n\n}\nt <- data.frame(theta = seq(0, 1, by = 0.01))\n\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE for frequentist and different beta priors\")\nggplot(t) + \n  geom_line(aes(x = theta, y = mse_f(theta, 10), color = \"frequentist est\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 1, 17, 10), color = \"beta(1,17) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 3, 47, 10), color = \"beta(3,47) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 2, 36, 10), color = \"beta(2,36) prior\")) + \n  geom_line(aes(x = theta, y = mse_b(theta, 9, 162, 10), color = \"beta(9,162) prior\")) +\n  ylab(\"MSE\") + \n  ggtitle(\"MSE zoomed in\") +\n  ylim(c(0, 0.05))"},{"path":"MLE.html","id":"MLE","chapter":"3 Maximum Likelihood Estimation","heading":"3 Maximum Likelihood Estimation","text":"Maximum likelihood estimation method choosing estimators parameters avoids using prior distributions loss functions. MLE chooses \\(\\hat{\\theta}\\) estimate \\(\\theta\\) maximizes likelihood function (easily widely used estimation method statistics).Let’s say \\(X \\sim U[0, \\theta].\\) collect \\(X=47.\\) ever pick \\(\\theta=10?\\) ! \\(10 \\notin \\Omega!\\)Let’s say \\(X \\sim\\) Bin(\\(\\theta,\\)n=4). 4 independent trials, X=1, chose \\(\\theta=0.99?\\) \\(0.99 \\\\Omega???\\) choose \\(\\theta = 0.25.\\) ? maximized likelihood.\\[\\begin{eqnarray*}\nP(X=1 | \\theta = 0.25) &=& 0.422\\\\\nP(X=1 | \\theta = 0.5) &=& 0.25\\\\\nP(X=1 | \\theta = 0.05)  &=& 0.171\\\\\nP(X=1 | \\theta = 0.15) &=& 0.368\\\\\n\\end{eqnarray*}\\]\nmaximized probability seeing data!Example 3.1  Let’s say houses block electricity connected way works house #47 neighbor’s electricity works (sequence). Sometimes house’s electricity fail (w/prob p). many houses can expect provide electricity? best guess p?Data: let’s say \\(n\\) neighborhoods information house lost electricity. [distribution \\(X?\\) \\(X \\sim geometric(p),\\) means \\(E[X] = 1/p.]\\)\\[\\begin{eqnarray*}\nf(x_i | p) &=& p (1-p)^{x_i -1}\\\\\nf(\\underline{x} | p) &=& \\prod_{=1}^n p (1-p)^{x_i -1}\\\\\n&=& p^n (1-p)^{\\sum x_i -n}\\\\\n&& \\mbox{want maximize wrt } p\n\\end{eqnarray*}\\]Often, log-likelihood easier maximize likelihood. define log likelihood \n\\[\\begin{eqnarray*}\nL(\\theta) = \\ln f(\\underline{X} | \\theta).\n\\end{eqnarray*}\\]example,\n\\[\\begin{eqnarray*}\nL(p) &=& \\ln f(\\underline{x} | p) = n \\ln(p) + (\\sum x_i -n) \\ln(1-p)\\\\\n\\frac{\\partial L(p)}{\\partial p} &=& \\frac{n}{p} + \\frac{(\\sum x_i -n)(-1)}{(1-p)} = 0\\\\\n\\hat{p} &=& \\frac{1}{\\overline{X}}\n\\end{eqnarray*}\\]\\(\\overline{x} = 10\\), \\(\\hat{p} = 1/10\\) (1 failure 10 homes). E[X] = 1/p. (let \\(\\theta = E[X] -1\\), good estimate expected (average) number homes can provide electricity \\(1/\\hat{p} -1 = 10 - 1\\). turns MLE invariance property says function MLE MLE function parameter.)Note ’ve found maximum:\n\\[\\begin{eqnarray*}\n\\frac{\\partial^2 L(p)}{\\partial p^2} &=& \\frac{2np -n -p^2\\sum x_i}{p^2 (1-p)^2}\\\\\n&\\leq& \\frac{2p n - n -p^2n}{p^2 (1-p)^2}\\\\\n(\\mbox{} n &\\leq& \\sum x_i)\\\\\n&=& \\frac{n ( -2p + 1 +p^2}{p^2 (1-p)^2}\\\\\n&=& \\frac{-n}{p^2} < 0\\\\\n\\end{eqnarray*}\\]Definition 3.1  Maximum Likelihood Estimator (Def 7.5.2).\npossible observed vector \\(\\underline{x}\\), let \\(\\delta(\\underline{x}) \\\\Omega\\) denote value \\(\\theta \\\\Omega\\) likelihood function, \\(f(\\underline{x}|\\theta)\\) maximum, let \\(\\hat{\\theta} = \\delta(\\underline{X})\\) estimator \\(\\theta\\) defined way. estimator \\(\\hat{\\theta}\\) called maximum likelihood estimator \\(\\theta.\\) \\(\\underline{X} = \\underline{x}\\) observed, value \\(\\delta(\\underline{x})\\) called maximum likelihood estimate \\(\\theta.\\)","code":""},{"path":"MLE.html","id":"sampling-from-a-normal-distribution","chapter":"3 Maximum Likelihood Estimation","heading":"Sampling from a Normal Distribution","text":"\\[\\begin{eqnarray*}\nX_1, X_2, \\ldots X_n &\\stackrel{iid}{\\sim}& N(\\mu, \\sigma^2)  \\ \\ \\ \\mu \\ \\& \\ \\sigma^2 \\mbox{ fixed unknown}\\\\\nf(x_i | \\mu, \\sigma^2) &=& \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp [ \\frac{-1}{2\\sigma^2}(x_i - \\mu)^2 ]\\\\\nf(\\underline{x} | \\mu, \\sigma^2) &=& \\frac{1}{(2 \\pi \\sigma^2)^{n/2}} \\exp [ \\frac{-1}{2\\sigma^2}\\sum_{=1}^n(x_i - \\mu)^2 ]\\\\\nL(\\mu, \\sigma^2) &=& \\frac{-n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum(x_i - \\mu)^2\\\\\n\\end{eqnarray*}\\]want maximize \\(f(\\underline{x}|\\mu, \\sigma^2)\\) respect \\(\\mu;\\) equivalently, can minimize \\(\\sum (x_i - \\mu)^2.\\)\\[\\begin{eqnarray*}\n\\mbox{Let } Q(\\mu) &=& \\sum(x_i - \\mu)^2\\\\\n\\frac{\\partial Q(\\mu)}{\\partial \\mu} &=& -2 \\sum(x_i - \\mu) = 0\\\\\n-2 \\sum x_i &=& -2 n \\mu\\\\\n\\hat{\\mu} &=& \\overline{x}\n\\end{eqnarray*}\\]MLE \\(\\mu\\) doesn’t depend \\(\\sigma^2\\), know joint MLE, \\(\\hat{\\theta} = (\\mu, \\sigma^2)\\), \\(\\hat{\\theta}\\) \\(\\hat{\\sigma}^2\\) maximizes (\\(\\theta' = (\\overline{x}, \\sigma^2) ):\\)\n\\[\\begin{eqnarray*}\nL(\\theta') &=& \\frac{-n}{2} \\ln (2 \\pi) - \\frac{n}{2} \\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum(x_i - \\overline{x})^2\\\\\n\\frac{\\partial L(\\theta')}{\\partial \\sigma^2} &=& \\frac{-n}{2 \\sigma^2} - \\frac{1(-1)}{2 (\\sigma^2)^2} \\sum(x_i - \\overline{x}) = 0\\\\\n\\sum (x_i - \\overline{x})^2 &=& \\frac{n(\\sigma^2)^2}{\\sigma^2}\\\\\n\\hat{\\sigma^2} &=& \\frac{\\sum(x_i - \\overline{x})^2}{n}\\\\\n\\hat{\\theta} &=& (\\overline{x}, \\frac{\\sum(x_i - \\overline{x})^2}{n})\n\\end{eqnarray*}\\]Example 3.2  Non-existence MLESuppose \\(X_1, X_2, \\ldots X_n \\stackrel{iid}{\\sim} f(x | \\theta)\\) :\n\\[\\begin{eqnarray*}\nf(x | \\theta) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x > \\theta\\\\\n    0 & x \\leq \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]\\(\\theta\\) unknown, \\(-\\infty < \\theta < \\infty\\). MLE exist?\n\\[\\begin{eqnarray*}\nf(\\underline{x}|\\theta) &=& e^{n \\theta - \\sum x_i} \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\nL(\\theta) &=& n \\theta - \\sum x_i \\ \\ \\ \\ \\ \\forall x_i > \\theta\\\\\n\\frac{\\partial L(\\theta)}{\\partial \\theta} &=& n = 0 ?!?!?!\\\\\n\\end{eqnarray*}\\]want find largest \\(\\theta\\) \\(\\theta < x_i \\ \\ \\forall x_i\\). \\(\\hat{\\theta} = \\min x_i\\)? , \\(\\theta < \\min x_i!\\) , instead,\n\\[\\begin{eqnarray*}\nf (x | \\theta) = \\left\\{ \\begin{array}{ll}\n    e^{\\theta - x} & x \\geq \\theta\\\\\n    0 & x < \\theta \\\\\n    \\end{array} \\right.\n    \\end{eqnarray*}\\]MLE \\(\\theta\\) \\(\\hat{\\theta} = \\min x_i\\).","code":""},{"path":"MLE.html","id":"qualities-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1 Qualities of the MLE","text":"","code":""},{"path":"MLE.html","id":"invariance-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.1 Invariance of the MLE","text":"Theorem 3.1  (DeGroot Schervish (2011) Theorem 6.6.1) Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\), let \\(g(\\theta)\\) function \\(\\theta\\). MLE \\(g(\\theta)\\) \\(g(\\hat{\\theta})\\). (Proof: see page 427 DeGroot & Schervish)Proof. Let \\(\\hat{\\theta}\\) MLE \\(\\theta\\). know \\[L(\\hat{\\theta}) = \\max_{\\theta \\\\Omega} L(\\theta).\\]let \\(g\\) one--one function \\(\\psi = g(\\theta) \\\\Gamma\\) (image \\(\\Omega\\) g). can denote inverse function \\(g\\) \\[\\theta = h(\\psi).\\]know MLE \\(\\psi\\) \\(\\hat{\\psi}\\) \\[L(h(\\hat{\\psi})) = \\max_{\\psi \\\\Gamma} L(h(\\psi)).\\]know \\(L(\\theta)\\) maximized \\(\\hat{\\theta}\\). \\(L(h(\\psi))\\) must also maximized \\(h(\\psi) = \\hat{\\theta}\\). \\[\\therefore h(\\hat{\\psi}) = \\hat{\\theta} \\ \\ \\ \\mbox{ } \\ \\ \\ \\hat{\\psi} = g(\\hat{\\theta}).\\]\\(g\\) one--one, however, need redefine mean likelihood. \\(g(\\theta)\\) one--one may many values \\(\\psi\\) satisfy \\[g(\\theta) = \\psi.\\] (example, square function. n.b. \\(g\\) still function, can’t map multiple values.)case, correspondence maximum \\(\\psi\\) maximum \\(\\theta\\) breaks . E.g., \\(\\hat{\\theta}\\) MLE \\(\\theta\\), may another value \\(\\theta\\), say \\(\\theta_0\\) \\(g(\\hat{\\theta}) = g(\\theta_0).\\)define induced log likelihood function: \\[L^*(t) = \\max_{\\theta \\G_t} \\ln f(\\underline{x} | \\theta).\\]Let \\(G\\) image \\(\\Omega\\) g, \\[G_t = \\{ \\theta: g(\\theta) = t\\} \\ \\ \\forall t \\G.\\]define MLE \\(g(\\theta)\\) \\(\\hat{t}\\) \\[L^*(\\hat{t}) = \\max_{t \\G} L^* (t).\\]proof follows (page 427). [Note two maximizations. , first maximization created log likelihood function. second maximization found maximum function parameter space.]Example 3.3  Functions MLEs MLEs also!standard deviation normal:\n\\[\\begin{eqnarray*}\n\\sigma &=& \\sqrt{\\sigma^2}\\\\\n\\hat{\\sigma} &=& \\mbox{MLE}(\\sigma) = \\sqrt{\\frac{\\sum (x_i - \\overline{x})^2}{n}}\n\\end{eqnarray*}\\]mean uniform:\n\\[\\begin{eqnarray*}\nX &\\sim& U [\\theta_1, \\theta_2]\\\\\n\\mu &=& \\frac{\\theta_1 + \\theta_2}{2}\\\\\n\\hat{\\mu} &=& \\mbox{MLE}(\\mu) = \\frac{\\max(x_i) + \\min(x_i)}{2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"consistency-of-the-mle","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.2 Consistency of the MLE","text":"certain regularity conditions, MLE \\(\\theta\\) consistent \\(\\theta\\). ,\n\\[\\begin{eqnarray*}\n\\hat{\\theta} \\stackrel{P}{\\rightarrow} \\theta\\\\\n\\stackrel{\\lim}{ n \\rightarrow \\infty} P [ | \\hat{\\theta} - \\theta | > \\epsilon ] = 0\n\\end{eqnarray*}\\]\\(\\hat{\\theta}\\) function \\(X\\).","code":""},{"path":"MLE.html","id":"bias-of-the-mle-section-7.7","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.3 Bias of the MLE (section 7.7)","text":"Previously, discussed briefly concept bias:Definition 3.2  Unbiased (page 428 DeGroot & Schervish): Let \\(\\delta(X_1, X_2, \\ldots, X_n)\\) estimator \\(g(\\theta)\\). say \\(\\delta(X_1, X_2, \\ldots, X_n)\\) unbiased :\n\\[\\begin{eqnarray*}\nE[ \\delta(X_1, X_2, \\ldots, X_n)] = g(\\theta)\n\\end{eqnarray*}\\]Note: bias \\((\\delta(\\underline{X}) ) = E[ \\delta(\\underline{X})] - g(\\theta)\\).Example 3.4  Let \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\). Recall, \\(\\hat{\\sigma^2} = \\frac{\\sum(X_i - \\overline{X})^2}{n}\\) MLE.\n\\[\\begin{eqnarray*}\nE[\\hat{\\sigma^2}] &=& E \\bigg[ \\frac{\\sum(X_i - \\overline{X})^2}{n}\\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu + \\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n} E\\bigg[ \\sum (X_i - \\mu)^2 + 2 \\sum (X_i - \\mu)(\\mu - \\overline{X}) + n(\\mu - \\overline{X})^2 \\bigg]\\\\\n&=& \\frac{1}{n}\\bigg\\{ E\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2 ] \\bigg] \\bigg\\}\\\\\n&=& \\frac{1}{n} \\bigg\\{  \\sum E (X_i - \\mu)^2 -  n E [(\\overline{X} - \\mu)^2 ] \\bigg\\}\\\\\n&=& \\frac{1}{n}\\{ n \\sigma^2 - n \\frac{\\sigma^2}{n}  \\}\\\\\n&=& \\frac{n-1}{n} \\sigma^2 \\Rightarrow \\mbox{Biased!}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"but-consistent-yes.","chapter":"3 Maximum Likelihood Estimation","heading":"But consistent? Yes.","text":"\\[\\begin{eqnarray*}\n\\frac{\\sum(X_i - \\overline{X})^2}{n} &=& \\frac{1}{n}\\bigg[ \\sum (X_i - \\mu)^2 -  n [(\\overline{X} - \\mu)^2] \\bigg] \\\\\n&=& \\frac{\\sum(X_i - \\mu)^2}{n} - \\frac{1}{n} \\frac{\\sum n (\\overline{X} - \\mu)^2}{n}\\\\\n&=& \\stackrel{P}{\\rightarrow} \\sigma^2   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow 0 \\ \\ \\ \\stackrel{P}{\\rightarrow} \\sigma^2\\\\\n&\\stackrel{P}{\\rightarrow}& \\sigma^2\\\\\n\\end{eqnarray*}\\]\n","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-maximum-likelihood-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.1.4 Benefits and Limitations of Maximum Likelihood Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-1","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"functions MLEs MLEs (invariance MLE)certain regularity conditions, MLEs asymptotically distributed normally.","code":""},{"path":"MLE.html","id":"limitations-1","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"MLE always existMLE always uniqueMLE value parameter \\(\\theta\\) maximizes distribution \\(X|\\theta\\) (likely produced observed data). MLE likely parameter given data: \\(E[ \\theta | X]\\) (’s Bayesian estimator!).","code":""},{"path":"MLE.html","id":"method-of-moments","chapter":"3 Maximum Likelihood Estimation","heading":"3.2 Method of Moments","text":"Method Moments (MOM) another parameter estimation technique. find MOM estimate, set expected moment equal sample moment solve parameter value interest.\\[\\begin{eqnarray*}\nE[X^k] &=& k^{th} \\mbox{ expected moment}\\\\\n\\frac{1}{n} X_i^k &=& k^{th} \\mbox{ sample moment}\\\\\n\\end{eqnarray*}\\]Example 3.5  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim N(\\mu, \\sigma^2)\\).\n\\[\\begin{eqnarray*}\n\\tilde{\\mu} &=& \\overline{X}\\\\\n\\tilde{\\sigma^2} &=& \\frac{\\sum X_i^2}{n} - \\overline{X}^2\\\\\n&=& \\frac{1}{n} \\sum(X_i - \\overline{X})^2 \\mbox{   MLE!!}\n\\end{eqnarray*}\\]Example 3.6  Find MOM parameters using random sample, \\(X_1, X_2, \\ldots, X_n \\sim Gamma(\\alpha,\\beta)\\).\n\\[\\begin{eqnarray*}\n\\alpha / \\beta &=& \\overline{X}\\\\\n\\alpha / \\beta^2 &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\alpha &=& \\overline{X} \\beta\\\\\n\\frac{\\overline{X} \\beta}{\\beta^2} &=& \\sum X_i^2 /n - \\overline{X}^2\\\\\n\\tilde{\\beta} &=& \\frac{\\overline{X}}{(\\sum X_i^2 / n - \\overline{X}^2)}\\\\\n&=& \\overline{X} / \\hat{\\sigma^2}\\\\\n\\tilde{\\alpha} &=& \\overline{X}^2 / \\hat{\\sigma^2}\n\\end{eqnarray*}\\]","code":""},{"path":"MLE.html","id":"benefits-and-limitations-of-method-of-moments-estimation","chapter":"3 Maximum Likelihood Estimation","heading":"3.2.1 Benefits and Limitations of Method of Moments Estimation","text":"","code":""},{"path":"MLE.html","id":"benefits-2","chapter":"3 Maximum Likelihood Estimation","heading":"Benefits","text":"Often easier compute MLEs (e.g., MLE \\(\\alpha\\) Gamma distribution intractable).Estimates MOM can used first approximations likelihood.Can easily estimate multiple parameter families.","code":""},{"path":"MLE.html","id":"limitations-2","chapter":"3 Maximum Likelihood Estimation","heading":"Limitations","text":"Can sometimes (usually cases small sample sizes) produce estimates outside parameter space.Doesn’t work moments don’t exist (e.g., Cauchy distribution)MLEs typically closer quantity estimated.Example 3.7  Tank EstimatorsHow can random sample integers 1 N (N unknown researcher) used estimate N?tanks numbered 1 N. Working group, randomly select five tanks, without replacement, bowl. tanks numbered:Think use data estimate N. (Come least 3 estimators.) Come consensus within group done.estimates N :rules formulas estimators N based sample n (case 5) integers :Assuming random variables distributed according discrete uniform:\n\\[\\begin{eqnarray*}\nX_i \\sim P(X=x | N) = \\frac{1}{N} \\ \\ \\ \\ \\ x = 1,2,\\ldots, N \\ \\ \\ \\ =1,2,\\ldots, n\n\\end{eqnarray*}\\]method moments estimator N?maximum likelihood estimator N?estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"mean-squared-error-1","chapter":"3 Maximum Likelihood Estimation","heading":"Mean Squared Error","text":"estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"MLE.html","id":"reflection-questions-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.3  Reflection Questions","text":"function maximized MLE found? Derivative respect ? , explain intuition behind MLE good estimator \\(\\theta.\\)MOM estimator found? Explain MOM good estimator \\(\\theta.\\)asymptotic properties MLE make nice work ?benefits limitations MLE?benefits limitations MOM?","code":""},{"path":"MLE.html","id":"ethics-considerations-2","chapter":"3 Maximum Likelihood Estimation","heading":"3.4  Ethics Considerations","text":"derivative always work find MLE? , ways can find maximum?MLE always best estimator \\(\\theta\\)?MOM always best estimator \\(\\theta\\)?makes good estimator?consulting, given dataset, asked find \\(\\theta\\), single right answer? know give boss?dataset don’t know likelihood?","code":""},{"path":"MLE.html","id":"r-code-mle-example","chapter":"3 Maximum Likelihood Estimation","heading":"3.5 R code: MLE Example","text":"Example 7.6.5 text looks MLE center Cauchy distribution.\nCauchy distribution interesting tails decay rate \\(1/x^2\\), try take expected value, end integrating something looks like \\(1/x\\) real line.\nHence, expected value exist.\nThus, method moments estimators use.\nMLE still useful, though easy find. stated text, likelihood proportional \n\\[\\prod_{=1}^n [1 + (x_i - \\theta)^2]^{-1}\\]Compute first second derivative log likelihood.Consider trying find root function \\(h(x)\\). Suppose current\nguess value \\(x_0\\). might approximate function tangent line (first order Taylor approximation) \\(x_0\\) take next guess root line. Use Taylor expansion find next guess \\(x_1\\) \\[x_1 = x_0 - \\frac{h(x_0)}{h'(x_0)}\\]Continually updating guess via method known Newton’s Method Newton-Raphson Method.Generate 50 observations Cauchy distribution centered \\(\\theta = 10\\). Based 50 observations, use parts () (b) estimate \\(\\theta\\). Remember, ’re trying maximize likelihood, function trying find root derivative log-likelihood. R code might look something like :Cauchy distribution sensitive initial value used Newton’s method, theta_guess close 10, probably MLE estimate infinite.","code":"library(tidyverse)\n\n# Use Newton's method to find the MLE for a Cauchy distribution\ncauchy_mle <- function(guess, data){\n    l1 <- ((compute 1st derivative of log-likelihood evaluated at \"guess\")) # 1st deriv\n    l2 <- ((compute 2nd derivative of log-likelihood evaluated at \"guess\")) # 2nd deriv\n    guess <- guess - l1/l2\n    guess\n}\n\n# set up the initial conditions (including the dataset to use)\nset.seed(54321)  # to get the same answer each time the .Rmd is knit\n\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\ntheta_guess <- ((pick a starting value, you might try different ones)) # just a number"},{"path":"MLE.html","id":"running-cauchy_mle-recursively","chapter":"3 Maximum Likelihood Estimation","heading":"Running cauchy_mle recursively","text":"","code":"theta_guess <- 10\nguess_vec <- numeric()\nreps <- 10  # play around with how many times you loop through. \n# you probably don't need very many times through to see what happens.\n# what happens might seem unsettling!\n\nfor(i in 1:reps){\n  theta_guess <- cauchy_mle(___, ___)  # what changes for each rep?\n  guess_vec[i] <- theta_guess\n}\nguess_vec\n\ndata.frame(guess_vec) %>%  # ggplot needs a data frame instead of a vector\n  ggplot(aes(y = guess_vec, x = 1:reps)) + \n  geom_line()  # line plot connecting the guesses for each rep"},{"path":"MLE.html","id":"reflection","chapter":"3 Maximum Likelihood Estimation","heading":"Reflection","text":"Just kicks, ’ve plotted derivative log-likelihood (red, trying find zero derivative log-likelihood) well two different guess trajectories (green start 10 blue start 3).\ngreen line (hard see, look right theta = 10) indicates get maximum close 10 start. blue line indicates happens initial value far 10 (MLE estimated infinite).’ve left R code, don’t worry understanding specific lines code. Instead focus understanding image . tell function maximum (.e., derivative goes zero!), x-value derivative equals zero difficult find. Using Newton-Raphson method requires initial guess quite close truth, otherwise, estimate go positive negative infinity.","code":"\nset.seed(470)\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\n\n\n# print the derivative of the log likelihood function\n# this is what you're trying to find the zero of\nthetas <-  seq(-100,50,by = .01)\nllfun <-  c()\nfor (k in 1:length(thetas))\n{\nllfun[k] <- 2*sum((observations-thetas[k])/(1+(observations-thetas[k])^2))\n}\n\n\n# draw on top two instances of Newton's method:\n# starting at theta_guess = 10  (RED)\ntheta_guess <- 10\ntheta_10 <- 10\nl1_vals_10 <- c(0)\nfor (i in 1:5)\n    {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_10[i + 1] <- theta_guess\n    l1_vals_10 <- c(l1_vals_10, l1, 0)\n    }\n\n\n# then starting at theta_guess = 3 (PURPLE)\ntheta_guess <- 3\ntheta_3 <- 3\nl1_vals_3 <- c(0)\nfor (i in 1:5)\n  {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_3[i + 1] <- theta_guess\n    l1_vals_3 <- c(l1_vals_3, l1, 0)\n}\n\n\nguesses_10 <- data.frame(thetas = rep(theta_10, each = 2),\n                        guess = c(l1_vals_10,0))\nguesses_3 <- data.frame(thetas = rep(theta_3, each = 2),\n                        guess = c(l1_vals_3,0))\ncauchy_data <- data.frame(thetas, llfun)\n\n\n\ncauchy_data %>%\n  ggplot() + \n  geom_hline(yintercept = 0) +\n  geom_line(aes(x = thetas, y = llfun, color = \"deriv loglik\")) +\n  geom_line(data = guesses_3, aes(x = thetas, y = guess, color = \"guesses, init is 3\")) +\n  geom_line(data = guesses_10, aes(x = thetas, y = guess, color = \"guesses, init is 10\")) +\n  xlim(c(-100, 50))"},{"path":"MLE.html","id":"odd-things-happen-sometimes","chapter":"3 Maximum Likelihood Estimation","heading":"Odd things happen sometimes","text":"’ve run code many many many times years. every time, initial guess super close 10, estimate doesn’t converge. just , set seed particular way, start value 3 actually converged answer! Randomness wild.","code":"\nset.seed(4747)\nn_obs <- 50\nobservations <- rcauchy(n_obs, location = 10)  # 50 random Cauchy, centered at 10\n\n\n# print the derivative of the log likelihood function\n# this is what you're trying to find the zero of\nthetas <-  seq(-100,50,by = .01)\nllfun <-  c()\nfor (k in 1:length(thetas))\n{\nllfun[k] <- 2*sum((observations-thetas[k])/(1+(observations-thetas[k])^2))\n}\n\n\n# draw on top two instances of Newton's method:\n# starting at theta_guess = 10  (RED)\ntheta_guess <- 10\ntheta_10 <- 10\nl1_vals_10 <- c(0)\nfor (i in 1:5)\n    {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_10[i + 1] <- theta_guess\n    l1_vals_10 <- c(l1_vals_10, l1, 0)\n    }\n\n\n# then starting at theta_guess = 3 (PURPLE)\ntheta_guess <- 3\ntheta_3 <- 3\nl1_vals_3 <- c(0)\nfor (i in 1:5)\n  {\n    l1 <- 2*sum((observations-theta_guess)/(1+(observations-theta_guess)^2))\n    l2 <- 2*sum((-1+(observations-theta_guess)^2)/(1+(observations-theta_guess)^2)^2)\n    theta_next <- theta_guess - l1/l2\n    theta_guess <- theta_next\n    theta_3[i + 1] <- theta_guess\n    l1_vals_3 <- c(l1_vals_3, l1, 0)\n}\n\n\nguesses_10 <- data.frame(thetas = rep(theta_10, each = 2),\n                        guess = c(l1_vals_10,0))\nguesses_3 <- data.frame(thetas = rep(theta_3, each = 2),\n                        guess = c(l1_vals_3,0))\ncauchy_data <- data.frame(thetas, llfun)\n\n\n\ncauchy_data %>%\n  ggplot() + \n  geom_hline(yintercept = 0) +\n  geom_line(aes(x = thetas, y = llfun, color = \"deriv loglik\")) +\n  geom_line(data = guesses_3, aes(x = thetas, y = guess, color = \"guesses, init is 3\")) +\n  geom_line(data = guesses_10, aes(x = thetas, y = guess, color = \"guesses, init is 10\")) +\n  xlim(c(-100, 50))"},{"path":"sampdist.html","id":"sampdist","chapter":"4 Sampling Distributions of Estimators","heading":"4 Sampling Distributions of Estimators","text":"statistic function observed random variables. sampling distribution statistic tells us values statistic assumes likely values . statistic function random variables \\(X_1, X_2, \\ldots, X_n\\), know distribution \\(X\\), principle, able derive distribution statistic.Example 4.1  data normal, sampling distribution mean also normal. result may look like Central Limit Theorem, limiting behavior . , sampling distribution \\(\\overline{X}\\) normal regardless sample size. proof come directly result linear combinations normal random variables also normal.\\[\\begin{eqnarray*}\nX_i &\\sim& N(\\mu, \\sigma^2)\\\\\n\\overline{X} &\\sim& N(\\mu, \\sigma^2 / n)\n\\end{eqnarray*}\\]","code":""},{"path":"sampdist.html","id":"the-chi-square-distribution","chapter":"4 Sampling Distributions of Estimators","heading":"4.1 The Chi-Square Distribution","text":"chi-square distribution probability distribution following characteristics.\\[\\begin{eqnarray*}\nX &\\sim& \\chi^2_n\\\\\nf_X(x) &=& \\frac{1}{2^{n/2} \\Gamma(n/2)} x^{n/2 -1} e^{-x/2} \\ \\ \\ \\ \\ \\ x > 0\\\\\nE[X] &=& n\\\\\nVar[X] &=& 2n\\\\\n\\psi_X(t) &=& \\Bigg( \\frac{1}{1-2t} \\Bigg)^{n/2} \\ \\ \\ \\ \\ \\ t < 1/2\\\\\n( &=&  E[e^{tX}] )\\\\\n\\end{eqnarray*}\\]Recall Moment Generating Functions \\((\\psi_X(t))\\), DeGroot Schervish (2011) page 205.Theorem 4.1  DeGroot Schervish (2011) 7.2.1Let \\(X_1, X_2, \\ldots X_k \\sim \\chi^2_{n_i}, \\ \\ =1, \\ldots, k\\), independently. , \\(X_1 + X_2 + \\cdots + X_k = \\sum_{=1}^k X_i \\sim \\chi^2_{n_1+n_2 +\\cdots+n_k}\\)., data independent chi-square random variables, sampling distribution sum also chi-square.Proof. \\[\\begin{eqnarray*}\nY&=& \\sum_{=1}^k X_i\\\\\n\\psi_Y (t) &=& E[e^{Yt} ]\\\\\n&=& E[e^{t \\sum_{=1}^k X_i}]\\\\\n&=& \\prod_{=1}^k E[e^{tX_i}]\\\\\n&=& \\prod_{=1}^k \\psi_{X_i}(t)\\\\\n&=& \\prod_{=1}^k \\bigg( \\frac{1}{1-2t} \\bigg)^{ n_i /2}\\\\\n&=& \\Bigg( \\frac{1}{1-2t} \\Bigg)^{\\sum n_i /2}\n\\end{eqnarray*}\\]\nSee theorem 4.4.3, pg 207.Theorem 4.2  DeGroot Schervish (2011) 7.2.1 1/2If \\(Z \\sim N(0,1), Y=Z^2,\\) \\(Y \\sim \\chi^2_1\\).Note result provide distribution transformation random variable. single data value \\((Z),\\) result provides distribution another single value, \\((Y).\\) value \\(Y\\) typically referred statistic summary observations.said, just , \\(Z\\) statistic (instead single value) \\(Z\\) \\(Y\\) sampling distributions!Proof. Let \\(\\Phi\\) \\(\\phi\\) cdf pdf Z.\nLet F f cdf pdf Y.\\[\\begin{eqnarray*}\nF_Y(y) &=& P(Y \\leq y) = P(Z^2 \\leq y)\\\\\n&=& P(-y^{1/2} \\leq Z \\leq y^{1/2})\\\\\n&=& \\Phi(y^{1/2}) -  \\Phi(- y^{1/2})  \\ \\ \\ \\ \\ y > 0\\\\\n\\end{eqnarray*}\\]\n\\[\\begin{eqnarray*}\nf_Y(y) &=& \\frac{\\partial F_Y(y)}{\\partial y} = \\phi(y^{1/2}) \\cdot \\frac{1}{2} y^{-1/2} - \\phi(-y^{1/2}) \\cdot \\frac{1}{2} -y^{-1/2}\\\\\n&=& \\frac{1}{2} y^{-1/2} ( \\phi(y^{1/2}) + \\phi(-y^{1/2})) \\ \\ \\ \\ \\ \\ y > 0\\\\\n\\mbox{know} && \\phi(y^{1/2}) = \\phi(-y^{1/2}) = \\frac{1}{\\sqrt{2 \\pi}} e^{-y/2}\\\\\n%\\therefore\nf_Y(y) &=& y^{-1/2} \\frac{1}{\\sqrt{2 \\pi}} e^{-y/2} \\ \\ \\ \\ \\ \\ y > 0 \\\\\n&=& \\frac{1}{2^{1/2}\\pi^{1/2}} y^{1/2 - 1} e^{-y/2} \\ \\ \\ \\ \\ \\ y >0\\\\\nY &\\sim& \\chi^2_1\\\\\n\\end{eqnarray*}\\]\nnote: \\(\\Gamma(1/2) = \\sqrt{\\pi}\\).combining Theorems 7.2.1 7.2.1 1/2, get:Theorem 4.3  DeGroot Schervish (2011) 7.2.2If \\(X_1, X_2, \\ldots, X_k \\stackrel{iid}{\\sim} N(0,1)\\),\n\\[\\begin{eqnarray*}\n\\sum_{=1}^k X_i^2 \\sim \\chi^2_k\n\\end{eqnarray*}\\]\nNote: \\(X_1, X_2, \\ldots, X_k \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\),\n\\[\\begin{eqnarray*}\n\\frac{X_i - \\mu}{\\sigma} &\\sim& N(0,1)\\\\\n\\sum_{=1}^k \\frac{(X_i - \\mu)^2}{\\sigma^2} &\\sim& \\chi^2_k\\\\\n\\end{eqnarray*}\\]data \\(iid\\) normal, sum squared values sampling distribution chi-square.","code":""},{"path":"sampdist.html","id":"independence-of-the-mean-and-variance-of-a-normal-random-variable","chapter":"4 Sampling Distributions of Estimators","heading":"4.2 Independence of the Mean and Variance of a Normal Random Variable","text":"Theorem 4.4  DeGroot Schervish (2011) 8.3.1Let \\(X_1, X_2, \\ldots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\).\\(\\overline{X}\\) \\(\\frac{1}{n} \\sum(X_i - \\overline{X})^2\\) independent. (true normal random variables. read proof book.)\\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\). (CLT, ?)\\(\\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\). (Main idea \\(n-1\\) independent things.)","code":""},{"path":"sampdist.html","id":"the-t-distribution","chapter":"4 Sampling Distributions of Estimators","heading":"4.3 The t-distribution","text":"Let \\(Z \\sim N(0,1)\\) \\(Y \\sim \\chi^2_n\\). \\(Z\\) \\(Y\\) independent, :Definition 4.1  \\[\\begin{eqnarray*}\nX = \\frac{Z}{\\sqrt{Y/n}} \\sim t_n \\mbox{  definition}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nf_X(x) &=& \\frac{\\Gamma(\\frac{n+1}{2})}{(n \\pi)^{1/2} \\Gamma(\\frac{n}{2})} (1 + \\frac{x^2}{n})^{-(n+1)/2} \\ \\ \\ \\ n > 2\\\\\nE[X] &=&0\\\\\nVar(X) &=& \\frac{n}{n-2}\n\\end{eqnarray*}\\]Remember:\n\\[\\begin{eqnarray*}\n\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\mbox{ independently } \\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2} \\sim \\chi^2_{n-1}\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\n\\frac{\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{\\frac{\\sum(X_i - \\overline{X})^2}{\\sigma^2}/(n-1)}} &=& \\frac{\\overline{X} - \\mu}{\\sqrt{\\frac{\\sum(X_i - \\overline{X})^2}{n-1}/n}}\\\\\n&=& \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\sim t_{n-1} !\n\\end{eqnarray*}\\]stated , t-distribution defined distribution given standard normal divided square root chi-square random variable divided degrees freedom. may seem obtuse first glance, comes extremely handy standardizing sample mean using standard error (instead standard deviation) mean.Example 4.2  According investors, foreign stocks potential high yield, variability dividends may greater typical American companies. Let’s say take random sample 10 foreign stocks; assume also know population distribution American stocks come (.e., American parameters). believe foreign stock prices distributed similarly (normal mean variance) American stock prices, likely sample 10 foreign stocks produce standard deviation 50% bigger American stocks?\\[\\begin{eqnarray*}\nP(\\hat{\\sigma} / \\sigma > 1.5 ) &=& ?\\\\\n\\frac{\\sum (X_i - \\overline{X})^2}{\\sigma^2} &\\sim& \\chi^2_{n-1} \\ \\ \\ \\ \\mbox{(normality assumption)}\\\\\n\\frac{\\sum (X_i - \\overline{X})^2}{\\sigma^2} &=& n\\frac{\\sum (X_i - \\overline{X})^2/n}{\\sigma^2}\\\\\n&=& \\frac{n \\hat{\\sigma^2}}{\\sigma^2}\\\\\nP(\\hat{\\sigma} / \\sigma > 1.5 ) &=& P(\\hat{\\sigma}^2 / \\sigma^2 > 1.5^2 ) \\\\\n&=& P(n \\hat{\\sigma}^2 / \\sigma^2 > n 1.5^2 )\\\\\n&=& 1 - \\chi^2_{n-1} (n 1.5^2)\\\\\n&=& 1 - \\chi^2_{n-1} (22.5)\\\\\n&=& 1 - pchisq(22.5,9) = 0.00742 \\ \\ \\ \\mbox{ R}\n\\end{eqnarray*}\\]Example 4.3  Suppose take random sample foreign stocks (\\(\\mu\\) \\(\\sigma^2\\) unknown). Find value \\(k\\) sample mean \\(k\\) sample standard deviations \\((s)\\) mean \\(\\mu\\) probability 0.90.\nData: \\(n=10\\), \\(\\hat{\\mu} = \\overline{x}\\), \\(s^2 = \\frac{\\sum(x_i - \\overline{x})^2}{n-1}\\), \\(s = \\sigma'\\).\\[\\begin{eqnarray*}\nP(\\overline{X} < \\mu + k s) &=& 0.9\\\\\nP\\Bigg(\\frac{\\overline{X} - \\mu}{s} < k \\Bigg) = P\\Bigg(\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} < k \\sqrt{n}\\Bigg) &=& 0.9\\\\\n\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} &\\sim& t_9\\\\\n\\sqrt{n} k &=& 1.383\\\\\nk &=& \\frac{1.383}{\\sqrt{10}} = 0.437\\\\\n\\mbox{note, R: } qt(0.9,9) &=& 1.383\n\\end{eqnarray*}\\]problem different known \\(\\sigma\\)? even wanted answer question terms number population standard deviations?","code":""},{"path":"sampdist.html","id":"reflection-questions-3","chapter":"4 Sampling Distributions of Estimators","heading":"4.4  Reflection Questions","text":"mean statistic sampling distribution?difference theoretical MSE empirical MSE (e.g., tank example )?can’t standard normal distribution used statistic interest \\(\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}}?\\)different tools used determine distribution random variable? (Note, chapter, majority random variables interest functions data, also called statistics.)","code":""},{"path":"sampdist.html","id":"ethics-considerations-3","chapter":"4 Sampling Distributions of Estimators","heading":"4.5  Ethics Considerations","text":"know estimator use consulting situation?respond someone tells “isn’t one right answer” previous question?technical conditions running t-test? , conditions data give rise t-distribution? happens technical conditions violated t-test run anyway?","code":""},{"path":"sampdist.html","id":"r-code-tanks-example","chapter":"4 Sampling Distributions of Estimators","heading":"4.6 R code: Tanks Example","text":"can random sample integers 1 \\(N\\) (\\(N\\) unknown researcher) used estimate \\(N\\)? problem known German tank problem derived directly situation Allies used maximum likelihood estimation determine many tanks Axes produced. See .tanks numbered 1 \\(N\\).\nThink use data estimate \\(N\\).possible estimators \\(N\\) :7\\[\\begin{eqnarray*}\n\\hat{N}_1 &=& 2\\cdot\\overline{X} - 1 \\ \\ \\ \\mbox{MOM}\\\\\n\\hat{N}_2 &=& 2\\cdot \\mbox{median}(\\underline{X}) - 1 \\\\\n\\hat{N}_3 &=& \\max(\\underline{X})  \\ \\ \\ \\mbox{MLE}\\\\\n\\hat{N}_4 &=& \\frac{n+1}{n} \\max(\\underline{X})  \\ \\ \\ \\mbox{less biased version MLE}\\\\\n\\hat{N}_5 &=& \\max(\\underline{X}) + \\min(\\underline{X}) \\\\\n\\hat{N}_6 &=& \\frac{n+1}{n-1}[\\max(\\underline{X}) - \\min(\\underline{X})] \\\\\n\\end{eqnarray*}\\]","code":""},{"path":"sampdist.html","id":"theoretical-mean-squared-error","chapter":"4 Sampling Distributions of Estimators","heading":"4.6.1 Theoretical Mean Squared Error","text":"estimators made four basic functions data: mean, median, min, max. Fortunately, know something moments functions:Using information expected value variance, can calculate MSE 4 estimators derived. (Remember MSE = Variance + Bias\\(^2\\).)\\[\\begin{eqnarray}\n\\mbox{MSE } ( 2 \\cdot \\overline{X} - 1) &=& \\frac{4 (N+1) (N-1)}{12n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N+1) (N-1)}{12n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( 2 \\cdot M - 1) &=& \\frac{4 (N-1)^2}{4n} + \\Bigg(2 \\bigg(\\frac{N+1}{2}\\bigg) - 1 - N\\Bigg)^2 \\nonumber \\\\\n&=& \\frac{4 (N-1)^2}{4n} \\\\\n\\nonumber \\\\\n\\mbox{MSE } ( \\max(\\underline{X})) &=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(N - \\frac{(N-1)}{n} - N\\Bigg)^2 \\nonumber\\\\\n&=& \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\bigg(\\frac{N-1}{n} \\bigg)^2  = 2*\\bigg(\\frac{N-1}{n} \\bigg)^2 \\\\\n\\nonumber \\\\\n\\mbox{MSE } \\Bigg( \\bigg( \\frac{n+1}{n} \\bigg) \\max(\\underline{X})\\Bigg) &=& \\bigg(\\frac{n+1}{n}\\bigg)^2 \\bigg(\\frac{N-1}{n}\\bigg)^2 + \\Bigg(\\bigg(\\frac{n+1}{n}\\bigg) \\bigg(N - \\frac{N-1}{n} \\bigg) - N \\Bigg)^2\n\\end{eqnarray}\\]","code":""},{"path":"sampdist.html","id":"empirical-mse","chapter":"4 Sampling Distributions of Estimators","heading":"4.6.2 Empirical MSE","text":"don’t need know theoretical expected value variance functions approximate MSE. can visualize sampling distributions also calculate actual empirical MSE estimator come .changing population size sample size, can assess estimators compare whether one particularly better given setting.","code":"\ncalculate_N <- function(nsamp,npop){\n  mysample =  sample(1:npop,nsamp,replace=F)  # what does this line do?\n  xbar2 <- 2 * mean(mysample) - 1\n  median2 <- 2 * median(mysample) - 1\n  samp.max <- max(mysample)\n  mod.max <- ((nsamp + 1)/nsamp) * max(mysample)\n  sum.min.max <- min(mysample) + max(mysample)\n  diff.min.max <- ((nsamp + 1)/(nsamp - 1)* (max(mysample) - min(mysample)))\n  data.frame(xbar2, median2, samp.max, mod.max, sum.min.max, diff.min.max,nsamp,npop)\n}\n\nreps <- 2\nnsamp_try <- c(10,100, 10, 100)\nnpop_try <- c(147, 147, 447, 447)\nmap_df(1:reps, ~map2(nsamp_try, npop_try, calculate_N))##    xbar2 median2 samp.max mod.max sum.min.max diff.min.max nsamp npop\n## 1 151.60     164      143  157.30         144     173.5556    10  147\n## 2 147.52     148      147  148.47         148     148.9495   100  147\n## 3 505.40     591      371  408.10         414     400.8889    10  447\n## 4 439.02     418      445  449.45         451     447.8687   100  447\n## 5 144.00     147      141  155.10         143     169.8889    10  147\n## 6 145.46     148      145  146.45         146     146.9091   100  147\n## 7 480.40     438      438  481.80         481     482.7778    10  447\n## 8 431.56     420      446  450.46         447     453.9899   100  447\nreps <- 1000\nresults <- map_df(1:reps, ~map2(nsamp_try, npop_try, calculate_N))\n\n# making the results long instead of wide:\nresults_long <- results %>%\n  pivot_longer(cols = xbar2:diff.min.max, names_to = \"estimator\", values_to = \"estimate\")\n\n# how is results different from results_long?  let's look at it:\nresults_long## # A tibble: 24,000 × 4\n##    nsamp  npop estimator    estimate\n##    <dbl> <dbl> <chr>           <dbl>\n##  1    10   147 xbar2            113.\n##  2    10   147 median2           81 \n##  3    10   147 samp.max         147 \n##  4    10   147 mod.max          162.\n##  5    10   147 sum.min.max      148 \n##  6    10   147 diff.min.max     178.\n##  7   100   147 xbar2            150.\n##  8   100   147 median2          150 \n##  9   100   147 samp.max         147 \n## 10   100   147 mod.max          148.\n## # … with 23,990 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nresults_long %>%\n  group_by(nsamp, npop, estimator) %>%\n  summarize(mean = mean(estimate), median = median(estimate), bias = mean(estimate - npop),\n            var = var(estimate), mse = (mean(estimate - npop))^2 + var(estimate))## `summarise()` has grouped output by 'nsamp', 'npop'. You can override using the\n## `.groups` argument.## # A tibble: 24 × 8\n## # Groups:   nsamp, npop [4]\n##    nsamp  npop estimator     mean median    bias    var    mse\n##    <dbl> <dbl> <chr>        <dbl>  <dbl>   <dbl>  <dbl>  <dbl>\n##  1    10   147 diff.min.max  149.   153.   1.73    400.   403.\n##  2    10   147 median2       147.   145   -0.448  1505.  1505.\n##  3    10   147 mod.max       148.   152.   0.694   178.   179.\n##  4    10   147 samp.max      134.   138  -12.7     147.   309.\n##  5    10   147 sum.min.max   147.   148   -0.151   289.   289.\n##  6    10   147 xbar2         146.   146.  -1.09    654.   655.\n##  7    10   447 diff.min.max  451.   461.   4.48   3617.  3637.\n##  8    10   447 median2       445.   444.  -1.57  14417. 14419.\n##  9    10   447 mod.max       449.   460.   2.29   1474.  1479.\n## 10    10   447 samp.max      408.   418  -38.6    1218.  2705.\n## # … with 14 more rows\n## # ℹ Use `print(n = ...)` to see more rows\nresults_long %>%\n  filter(npop == 147) %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = npop), color = \"red\") +\n  facet_grid(nsamp ~ estimator) + \n  ggtitle(\"sampling distributions of estimators of N, pop size = 147\")\nresults_long %>%\n  filter(npop == 447) %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  geom_vline(aes(xintercept = npop), color = \"red\") +\n  facet_grid(nsamp ~ estimator) + \n  ggtitle(\"sampling distributions of estimators of N, pop size = 447\")"},{"path":"bootdist.html","id":"bootdist","chapter":"5 Bootstrap Distributions","heading":"5 Bootstrap Distributions","text":"","code":""},{"path":"bootdist.html","id":"bootstrap-sampling-distributions","chapter":"5 Bootstrap Distributions","heading":"5.1 Bootstrap Sampling Distributions","text":"introducing bootstrap, let’s reflect work ’ve done far. Note one big goals understand variability data (samples) affects estimation. Consider different approaches ’ve taken understanding variability:Bayesian, \\(\\theta | \\underline{X} \\sim\\) according posterior distribution describes variability \\(\\theta\\) conditional data.Frequentist, focus variability statistic instead variability parameter.\n\\(f(x|\\theta)\\) known, can simulate data population use empirical sampling distribution see estimates vary sample sample. example, consider work tanks.\n\\(f(x|\\theta)\\) statistic lovely, can use mathematical theory. Mostly (far) “lovely” meant data normal statistic one produced either \\(\\chi^2\\) \\(t\\) distribution.\n\\(f(x|\\theta)\\) unknown \\(n\\) large statistic interest sample mean (note: proportion sample mean!), can use Central Limit Theorem.\n\\(f(x|\\theta)\\) unknown statistic arbitrary, use Bootstrap.\n\\(f(x|\\theta)\\) known, can simulate data population use empirical sampling distribution see estimates vary sample sample. example, consider work tanks.\\(f(x|\\theta)\\) statistic lovely, can use mathematical theory. Mostly (far) “lovely” meant data normal statistic one produced either \\(\\chi^2\\) \\(t\\) distribution.\\(f(x|\\theta)\\) unknown \\(n\\) large statistic interest sample mean (note: proportion sample mean!), can use Central Limit Theorem.\\(f(x|\\theta)\\) unknown statistic arbitrary, use Bootstrap.","code":""},{"path":"bootdist.html","id":"introduction","chapter":"5 Bootstrap Distributions","heading":"5.1.1 Introduction","text":"Main idea: able estimate variability estimator, using single random sample describe population (hey, ’s always usually one sample use!).’s strange get \\(\\hat{\\theta}\\) SE(\\(\\hat{\\theta}\\)) data (consider \\(\\hat{p}\\) & \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\) \\(\\overline{X}\\) & \\(s/\\sqrt{n}\\)).’ll consider confidence intervals now.Bootstrapping doesn’t help get around small samples.following applets may helpful:Bootstrapping actual datasets: http://lock5stat.com/statkey/index.htmlBootstrapping contrast sampling population: https://www.rossmanchance.com/applets/OneSample.html?population=bootstrapThe logic confidence intervals: http://www.rossmanchance.com/applets/ConfSim.html","code":""},{"path":"bootdist.html","id":"basics-notation","chapter":"5 Bootstrap Distributions","heading":"5.1.2 Basics & Notation","text":"Let \\(\\theta\\) parameter interest, let \\(\\hat{\\theta}\\) estimate \\(\\theta\\). , ’d take lots samples size \\(n\\) population create \\(\\hat{\\theta}\\). Consider taking \\(B\\) random samples \\(F\\):\n\\[\\begin{eqnarray*}\n\\hat{\\theta}(\\cdot) = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}_b\n\\end{eqnarray*}\\]\nbest guess \\(\\theta\\). \\(\\hat{\\theta}\\) different \\(\\theta\\), call .\n\\[\\begin{eqnarray*}\nSE(\\hat{\\theta}) &=& \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B(\\hat{\\theta}_b - \\hat{\\theta}(\\cdot))^2 \\bigg]^{1/2}\\\\\nq_1 &=& [0.25 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_1)} = \\mbox{25}\\% \\mbox{ cutoff}\\\\\nq_3 &=& [0.75 B] \\ \\ \\ \\ \\hat{\\theta}^{(q_3)} = \\mbox{75}\\% \\mbox{ cutoff}\\\\\n\\end{eqnarray*}\\], completely characterize sampling distribution (function \\(\\theta\\)) allow us make inference \\(\\theta\\) \\(\\hat{\\theta}\\).\nFigure 1.1: Image credit: Hesterberg: supplemental chapter Introduction Practice Statistics, 5th Edition Moore McCabe. https://www.timhesterberg.net/bootstrap--resampling\n","code":""},{"path":"bootdist.html","id":"the-plug-in-principle","chapter":"5 Bootstrap Distributions","heading":"The Plug-in Principle","text":"Recall\n\\[\\begin{eqnarray*}\nF(x) &=& P(X \\leq x)\\\\\n\\hat{F}(x) &=& S(x) = \\frac{\\# X_i \\leq x}{n}\n\\end{eqnarray*}\\]\n\\(\\hat{F}(x)\\) sufficient statistic \\(F(x)\\). , information \\(F\\) data contained \\(\\hat{F}(x)\\). Additionally, \\(\\hat{F}(x)\\) MLE \\(F(x)\\). (probabilities, ’s multinomial argument. similar binomial argument, maximization happens additional constraint probabilities sum one, Lagrangian multipliers used.)Note , general, interested parameter, \\(\\theta\\) functional \\(F\\) (functional means function).\n\\[\\begin{eqnarray*}\nt(F) = \\theta \\ \\ \\ \\ \\Bigg[\\mbox{e.g., } \\int x f(x) dx = \\mu \\Bigg]\n\\end{eqnarray*}\\]plug-estimate \\(\\theta\\) :\n\\[\\begin{eqnarray*}\nt(\\hat{F}) = \\hat{\\theta}  \\ \\ \\ \\ \\Bigg[\\mbox{e.g., }  \\sum_x x \\hat{f}(x) = \\sum_{=1}^n x_i \\bigg(\\frac{1}{n}\\bigg) =  \\frac{1}{n} \\sum x_i  = \\overline{x} \\Bigg]\n\\end{eqnarray*}\\]: estimate parameter, use statistic corresponding quantity sample.idea boostrapping (fact, bootstrap samples ), depends double arrow . must random sample: , \\(\\hat{F}\\) must good job estimating \\(F\\) order bootstrap concepts meaningful.\\[\\begin{eqnarray*}\n\\underline{\\mbox{Real World}} && \\underline{\\mbox{Boostrap World}}\\\\\nF \\rightarrow x &\\Rightarrow& \\hat{F} \\rightarrow x^*\\\\\n\\downarrow & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow\\\\\n\\hat{\\theta} & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}^*\n\\end{eqnarray*}\\]Note ’ve seen plug-principle :\n\\[\\begin{eqnarray*}\n\\sqrt{\\frac{p(1-p)}{n}} &\\approx& \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\\\\n\\mbox{Fisher's Information: } (\\theta) &\\approx& (\\hat{\\theta})\n\\end{eqnarray*}\\]Okay, okay, haven’t seen Fisher’s Information yet, ’ll see weeks, plug-principle apply.","code":""},{"path":"bootdist.html","id":"the-bootstrap-idea","chapter":"5 Bootstrap Distributions","heading":"The Bootstrap Idea","text":"can resample sample represent samples actual population! boostrap distribution statistic, based many resamples, represents sampling distribution statistic based many samples. okay?? assuming?\\(n \\rightarrow \\infty\\), \\(\\hat{F}(x) \\rightarrow F(x)\\)\\(B \\rightarrow \\infty\\), \\(\\hat{F}(\\hat{\\theta}^*) \\rightarrow F(\\hat{\\theta})\\) (large \\(n\\)). really, typically see \\(\\hat{F}(\\hat{\\theta}^* / \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} / \\theta)\\) \\(\\hat{F}(\\hat{\\theta}^* - \\hat{\\theta}) \\rightarrow F(\\hat{\\theta} - \\theta)\\)","code":""},{"path":"bootdist.html","id":"bootstrap-procedure","chapter":"5 Bootstrap Distributions","heading":"Bootstrap Procedure","text":"Resample data replacement.Calculate statistic interest resample.Repeat 1. 2. \\(B\\) times.Use bootstrap distribution inference.","code":""},{"path":"bootdist.html","id":"bootstrap-notation","chapter":"5 Bootstrap Distributions","heading":"Bootstrap Notation","text":"Take lots (\\(B\\)) resamples sample size n sample, \\(\\hat{F}(x)\\) (instead population, \\(F(x)\\) ) create bootstrap distribution \\(\\hat{\\theta}^*\\) (instead sampling distribution \\(\\hat{\\theta}\\)).Let \\(\\hat{\\theta}^*_b\\) calculated statistic interest \\(b^{th}\\) bootstrap sample. best guess \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\hat{\\theta}^* = \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^*_b\n\\end{eqnarray*}\\]\n(\\(\\hat{\\theta}^*\\) different \\(\\hat{\\theta}\\), call biased.) estimated value standard error estimate \n\\[\\begin{eqnarray*}\n\\hat{SE}^*_B = \\bigg[ \\frac{1}{B-1} \\sum_{b=1}^B ( \\hat{\\theta}^*_b - \\hat{\\theta}^*)^2 \\bigg]^{1/2}\n\\end{eqnarray*}\\]Just like repeatedly taking samples population, taking resamples sample allows us characterize bootstrap distribution approximates sampling distribution. bootstrap distribution approximates shape, spread, & bias actual sampling distribution. bootstrap sampling distribution estimate center true sampling distribution.\nFigure 1.2: Hesterberg et al., Chapter 16 Introduction Practice Statistics Moore, McCabe, Craig. left image represents mean n=50. center image represents mean n=9. right image represents median n=15.\n","code":""},{"path":"bootdist.html","id":"finding-the-bootstrap-distribution","chapter":"5 Bootstrap Distributions","heading":"5.1.3 Finding the Bootstrap Distribution","text":"Consider using bootstrap estimate distribution \\(\\frac{\\hat{\\theta} - \\theta}{SE(\\hat{\\theta})}\\).\n\\[\\begin{eqnarray*}\nT^*(b) &=& \\frac{\\hat{\\theta}^*_b - \\hat{\\theta}}{\\hat{SE}^*(b)}\n\\end{eqnarray*}\\]\n\\(\\hat{\\theta}^*_b\\) value \\(\\hat{\\theta}\\) \\(b^{th}\\) bootstrap sample, \\(\\hat{SE}^*(b)\\) estimated standard error \\(\\hat{\\theta}^*_b\\) \\(b^{th}\\) bootstrap sample. \\(\\alpha/2^{th}\\) percentile \\(T^*(b)\\) estimated value \\(\\hat{q}^*_{\\alpha/2}\\) \n\\[\\begin{eqnarray*}\n\\frac{\\# \\{T^*(b) \\leq \\hat{q}^*_{\\alpha/2} \\} }{B} = \\alpha/2\n\\end{eqnarray*}\\]\nexample, \\(B=1000\\), estimate 5% point \\(50^{th}\\) smallest value \\(T^*(b)\\)s, estimate 95% point \\(950^{th}\\) smallest value \\(T^*(b)\\)s.find different SE bootstrapped dataset, bootstrap twice. algorithm follows:Generate \\(B_1\\) bootstrap samples, sample \\(\\underline{X}^{*b}\\) compute bootstrap estimate \\(\\hat{\\theta}^*_b\\).Take \\(B_2\\) bootstrap samples \\(\\underline{X}^{*b}\\), estimate standard error, \\(\\hat{SE}^*(b)\\).Find \\(B_1\\) values \\(T^*(b)\\). Calculate \\(\\hat{q}^*_\\alpha\\) \\(\\hat{q}^*_{1-\\alpha}\\) (\\(\\alpha/2\\)).","code":""},{"path":"bootdist.html","id":"reflection-questions-4","chapter":"5 Bootstrap Distributions","heading":"5.2  Reflection Questions","text":"bootstrap? , bootstrapping provide?technical conditions bootstrapping?bootstrap sampling distribution?bootstrap standard error?mean statistic sampling distribution? can conceptualize sampling distribution (always!) one sample? can approximate sampling distribution (always!) one sample?","code":""},{"path":"bootdist.html","id":"ethics-considerations-4","chapter":"5 Bootstrap Distributions","heading":"5.3  Ethics Considerations","text":"friend tells planning bootstrap don’t enough data central limit theorem kick . Explain wrong logic.different methods ’ve used develop sampling distributions? one use setting?SE statistic valuable: skewed symmetric sampling distribution? ?dataset hand missing observations, remove missing data bootstrap bootstrap missing observations? answer “depends”? one missing data anyway????","code":""},{"path":"bootdist.html","id":"r-code-bootstrapping-survival-example","chapter":"5 Bootstrap Distributions","heading":"5.4 R code: Bootstrapping Survival Example","text":"many built functions R (Python, Matlab, Stata, etc. matter) bootstrap dataset create number standard bootstrap intervals. However, order understand bootstrap process, example uses loops repeated resample calculate statistics interest.Example 5.1  heroin survival timeHesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.Hesketh Everitt (2000) report study Caplehorn Bell (1991) investigated times heroin addicts remained clinic methadone maintenance treatment.data include amount time subjects stayed facility treatment terminated (column 4).data include amount time subjects stayed facility treatment terminated (column 4).37% subjects, study ended still clinic (status=0).37% subjects, study ended still clinic (status=0).survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)survival time truncated. reason might want estimate mean survival time, rather measure typical survival time. explore using 25% trimmed mean. (ISCAM Chance & Rossman, Investigation 4.5.3)","code":"\nlibrary(tidyverse)\nheroin <- readr::read_table(\"http://www.rossmanchance.com/iscam2/data/heroin.txt\")\nnames(heroin)## [1] \"id\"     \"clinic\" \"status\" \"times\"  \"prison\" \"dose\"\nhead(heroin)## # A tibble: 6 × 6\n##      id clinic status times prison  dose\n##   <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>\n## 1     1      1      1   428      0    50\n## 2     2      1      1   275      1    55\n## 3     3      1      1   262      0    55\n## 4     4      1      1   183      0    30\n## 5     5      1      1   259      1    65\n## 6     6      1      1   714      0    55\nobs.stat <- heroin %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()\nobs.stat## [1] 378.3"},{"path":"bootdist.html","id":"bootstrapping-the-data","chapter":"5 Bootstrap Distributions","heading":"Bootstrapping the data","text":"","code":"\nset.seed(4747)\nheroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)\n\nheroin.bs %>% \n  summarize(tmeantime = mean(times, trim=0.25)) %>% pull()## [1] 372.2583"},{"path":"bootdist.html","id":"creating-a-bootstrap-sampling-distribution-for-the-trimmed-mean","chapter":"5 Bootstrap Distributions","heading":"Creating a bootstrap sampling distribution for the trimmed mean","text":"","code":"\nbs.test.stat<-c()    # placeholder, eventually B long, check after running!\nbs.sd.test.stat<-c() # placeholder, eventually B long, check after running!\n\nB <- 500\nM <- 100\nset.seed(4747)\nfor(b in 1:B){ \n  heroin.bs<-heroin %>% sample_frac(size=1, replace=TRUE)  # BS sample\n  bs.test.stat<-c(bs.test.stat, # concatenate each trimmed mean each time go through loop\n                  heroin.bs %>% \n                    summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  \n  bsbs.test.stat <- c() # refresh the vector of double BS test statistics\n  \n  for(m in 1:M){\n    heroin.bsbs<-heroin.bs %>% sample_frac(size=1, replace=TRUE) # BS of the BS!\n    bsbs.test.stat <- c(bsbs.test.stat, # concatenate the trimmed mean of the double BS\n                        heroin.bsbs %>% \n                          summarize(tmeantime = mean(times, trim = 0.25)) %>% pull())\n  }\n  bs.sd.test.stat<-c(bs.sd.test.stat, sd(bsbs.test.stat))\n}"},{"path":"bootdist.html","id":"what-do-the-data-distributions-look-like","chapter":"5 Bootstrap Distributions","heading":"What do the data distributions look like?","text":"","code":"\nggplot(heroin, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"original sample (n=238)\")\nggplot(heroin.bs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"one bootstrap sample (n=238)\")\nggplot(heroin.bsbs, aes(x=times)) + \n  geom_histogram(bins=30) + \n  ggtitle(\"a bootstrap sample of the one bootstrap sample (n=238)\")"},{"path":"bootdist.html","id":"what-do-the-sampling-distributions-look-like","chapter":"5 Bootstrap Distributions","heading":"What do the sampling distributions look like?","text":"","code":"\nbs.stats <- data.frame(bs.test.stat)\nggplot(bs.stats, aes(x=bs.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of trimmed mean\") +  \n  xlab(paste(\"trimmed.mean=\",round(mean(bs.test.stat),2),\"; SE=\", round(sd(bs.test.stat),2)))"},{"path":"bootdist.html","id":"what-is-the-distribution-of-the-se-of-the-statistic","chapter":"5 Bootstrap Distributions","heading":"What is the distribution of the SE of the statistic?","text":"","code":"\nbs.SE <- data.frame(bs.sd.test.stat)\nggplot(bs.SE, aes(x=bs.sd.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of SE of trimmed means\") +  \n  xlab(paste(\"average SE=\",round(mean(bs.sd.test.stat),2)))"},{"path":"bootdist.html","id":"what-is-the-distribution-of-the-t-statistics","chapter":"5 Bootstrap Distributions","heading":"What is the distribution of the T statistics?","text":"","code":"\nbs.T <- data.frame(T.test.stat = (bs.test.stat - obs.stat) / bs.sd.test.stat)\nggplot(bs.T, aes(x=T.test.stat)) + \n  geom_histogram(bins=20) + \n  ggtitle(\"dist of T statistics of trimmed means\") +  \n  xlab(paste(\"average T=\",round(mean(bs.T$T.test.stat),2)))"},{"path":"bootdist.html","id":"normal-ci-with-bs-se","chapter":"5 Bootstrap Distributions","heading":"95% normal CI with BS SE","text":"","code":"\nobs.stat + \n  qnorm(c(.025,.975))*\n  sd(bs.test.stat)## [1] 334.0961 422.5039"},{"path":"bootdist.html","id":"bootstrap-t-ci","chapter":"5 Bootstrap Distributions","heading":"95% Bootstrap-t CI","text":"Note t-value needed (requires different SE bootstrap sample).","code":"\nbs.t.hat<-(bs.test.stat - obs.stat)/bs.sd.test.stat\n\nbs.t.hat.95 = quantile(bs.t.hat, c(.975,.025))\n\nobs.stat - bs.t.hat.95*sd(bs.test.stat)##    97.5%     2.5% \n## 336.5108 426.8502"},{"path":"bootdist.html","id":"percentile-ci","chapter":"5 Bootstrap Distributions","heading":"95% Percentile CI","text":"","code":"\nquantile(bs.test.stat, c(.025, .975))##     2.5%    97.5% \n## 332.2373 422.3135"},{"path":"intest.html","id":"intest","chapter":"6 Interval Estimates","heading":"6 Interval Estimates","text":"now, ’ve used \\(\\hat{\\theta}\\) estimate \\(\\theta\\). single numerical value gives us information degree uncertainty estimate. confidence interval set values, (,B), think likely contain \\(\\theta\\) (true parameter). length interval gives us idea closely able estimate \\(\\theta\\).","code":""},{"path":"intest.html","id":"frequentist-confidence-intervals","chapter":"6 Interval Estimates","heading":"6.1 Frequentist Confidence Intervals","text":"frequentist confidence interval (CI) created way interval contains parameter specified percentage time.","code":""},{"path":"intest.html","id":"ci-for-the-mean-mu-in-a-normal-random-sample","chapter":"6 Interval Estimates","heading":"6.1.1 CI for the mean, \\(\\mu\\) in a normal random sample","text":"know,\n\\[\\begin{eqnarray*}\n\\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n-1}\n\\end{eqnarray*}\\]n.b., text uses \\(\\sigma' = s = \\sqrt{\\frac{\\sum_{=1}{n}(X_i - \\overline{X})^2}{n-1}}.\\) whenever see \\(\\sigma'\\), think \\(s.\\)Let \\(c\\) constant \\(\\int_{-c}^c f_{t_{n-1}}(x) dx = \\gamma\\) (e.g., = 0.95). :\\[\\begin{eqnarray*}\nP(-c \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c) &=& 0.95\\\\\nP( \\overline{X} - c s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} + c s/\\sqrt{n} ) &=& 0.95\\\\\n\\end{eqnarray*}\\]random ? \\(\\overline{X}\\) \\(s\\) random! probability getting \\((\\overline{X}\\),\\(s)\\) \\(\\overline{X} - c s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} + c s/\\sqrt{n}\\) 0.95. \\(c?\\)frequentists, don’t interpret interval “probability \\(\\theta\\) interval”\n\\[\\begin{eqnarray*}\nX_1, X_2, \\ldots, X_n &\\rightarrow& \\mbox{random}\\\\\n\\theta &\\rightarrow& \\mbox{fixed}\\\\\n\\end{eqnarray*}\\]95% confident \\(\\mu\\) \\(\\overline{X} - c s/\\sqrt{n}\\) \\(\\overline{X} + c s/\\sqrt{n}\\)Bayesians interpret intervals “probability \\(\\theta\\) interval” Bayesians treat \\(\\theta\\) random variable (need prior, etc…) [See next section Bayesian Intervals.]Example 6.1  sample 25 statistics students reported spend average 110 minutes per week studying statistics, standard deviation 40 minutes. Find one-sided CI 98% confident know lower bound true average studying time population. Let’s assume data reasonably independent normally distributed.need interval \\([ ,\\infty)\\), 98% confidence, \\(\\mu\\) interval. know:\n\\[\\begin{eqnarray*}\nP(c_1 \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c_2) &=& 0.98\\\\\nP( \\overline{X} - c_2 s/\\sqrt{n} \\leq \\mu \\leq \\overline{X} - c_1 s/\\sqrt{n} ) &=& 0.98\\\\\nP(-\\infty \\leq \\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} \\leq c_2) &=& 0.98\\\\\n\\\\\n\\frac{\\overline{X} - \\mu}{s/\\sqrt{n}} &\\sim& t_{24} \\rightarrow c_2 = 2.172\\\\\nP( \\overline{X} - 2.172 s/\\sqrt{n} \\leq \\mu (\\leq \\infty) ) &=& 0.98\\\\\n( \\overline{X} - 2.172 s/\\sqrt{n},\\infty ) && \\mbox{98% CI}\\\\\n\\end{eqnarray*}\\]\n98% confident true average studying time (population) least 92.62 minutes. can’t plug numbers keep probability?2-sided interval: \\(c=2.492=c_2, c_1 = -2.492\\)\n\\[\\begin{eqnarray*}\n( \\overline{X} - 2.492 s/\\sqrt{n},\\overline{X} + 2.492 s/\\sqrt{n} ) && \\mbox{98% CI}\\\\\n(90.06 \\mbox{ min}, 129.94 \\mbox{ min}) &&\\\\\n\\end{eqnarray*}\\]98% confident average number minutes per week spent studying population 90.06 min 129.94 min.","code":""},{"path":"intest.html","id":"bayesian-intervals","chapter":"6 Interval Estimates","heading":"6.2 Bayesian Intervals","text":"Note: book calls Bayesian intervals “posterior intervals”, stick language. However, people Bayesian literature call “credible intervals”.’d like say “probability \\(\\theta\\) interval …” Bayesian can Bayesians think \\(\\theta\\) random, put distribution \\(\\theta | \\underline{x}\\).Bayesian posterior credible interval given posterior distribution. , (\\(1-\\alpha\\))% posterior interval \\(\\theta\\) \\[\\Xi^{-1}_{\\alpha/2} (\\theta | \\underline{X}), \\Xi^{-1}_{1-\\alpha/2} (\\theta | \\underline{X})\\]\n\\(\\Xi(\\theta | \\underline{X})\\) posterior cumulative distribution function \\(\\theta\\) (maybe use better notation). focus \\(\\Xi\\) cdf notation. Instead, keep mind inverse cdf defines tail probabilities associated posterior distribution.","code":""},{"path":"intest.html","id":"joint-posterior-distribution-for-mu-and-sigma-in-a-normal-distribution","chapter":"6 Interval Estimates","heading":"6.2.1 Joint Posterior Distribution for \\(\\mu\\) and \\(\\sigma\\) in a Normal Distribution","text":"Remember, found posterior distribution \\(\\mu | \\underline{x}\\) known \\(\\sigma\\). don’t really ever know \\(\\sigma\\). find joint posterior \\(\\mu, \\sigma | \\underline{x}\\), need two priors.simplify calculations, let \\(\\tau = 1/\\sigma^2\\). \\(\\tau\\) called precision. joint distribution calculated using product marginal normal distributions. Note data assumed random sample (.e., independent identically distributed according \\(N(\\mu, 1/\\tau)\\) distribution).\\[\\begin{eqnarray*}\nf(x | \\mu, \\tau) &=& \\Bigg( \\frac{\\tau}{2 \\pi} \\Bigg) ^{1/2} exp \\bigg[ \\frac{-1}{2} \\tau (x-\\mu)^2 \\bigg]\\\\\nf(\\underline{x} | \\mu, \\tau) &=& \\Bigg( \\frac{\\tau}{2 \\pi} \\Bigg) ^{n/2} exp \\bigg[ \\frac{-1}{2} \\tau \\sum_{=1}^n (x_i-\\mu)^2 \\bigg]\\\\\n\\end{eqnarray*}\\]Theorem 6.1  (DeGroot Schervish (2011) Theorem 7.6.1)\nLet \\(X_1, X_2, \\ldots X_n \\sim N(\\mu, 1/\\tau)\\) suppose priors \\(\\mu|\\tau\\) \\(\\tau\\),\n\\[\\begin{eqnarray*}\n\\mu|\\tau &\\sim& N(\\mu_0, 1/(\\lambda_0 \\tau) )\\\\\n\\tau &\\sim& \\mbox{ Gamma} (\\alpha_0, \\beta_0)\n\\end{eqnarray*}\\], posteriors \\(\\mu|\\tau\\) \\(\\tau\\) ,\n\\[\\begin{eqnarray*}\n\\mu | \\tau, \\underline{x} &\\sim& N(\\mu_1, 1/(\\lambda_1 \\tau) )\\\\\n\\tau \\  | \\ \\underline{x}  &\\sim& \\mbox{ Gamma} (\\alpha_1, \\beta_1)\n\\end{eqnarray*}\\]\n \\(\\mu_1 = \\frac{\\lambda_0 \\mu_0 + n \\overline{x}}{\\lambda_0 + n}, \\ \\ \\ \\ \\lambda_1 = \\lambda_0 + n, \\ \\ \\ \\ \\alpha_1 = \\alpha_0 + \\frac{n}{2}, \\ \\ \\ \\ \\ \\beta_1 = \\beta_0 + \\frac{1}{2} \\sum_{=1}^n (x_i - \\overline{x})^2 + \\frac{n \\lambda_0 (\\overline{x} - \\mu_0)^2}{2(\\lambda_0 +n)}.\\)Note prior joint conjugate family distributions. \\(\\mu\\) \\(\\tau\\) normal-gamma distribution. Note also \\(\\mu\\) \\(\\tau\\) independent.Theorem 6.2  Let \\(X_1, X_2, \\ldots X_n \\sim N(\\mu, 1/\\tau)\\) suppose priors \\(\\mu|\\tau\\) \\(\\tau\\),\n\\[\\begin{eqnarray*}\n\\mu|\\tau &\\sim& N(\\mu_0, 1/(\\lambda_0 \\tau) )\\\\\n\\tau &\\sim& \\mbox{ Gamma} (\\alpha_0, \\beta_0)\n\\end{eqnarray*}\\], marginal posterior distribution \\(\\mu\\) can written :\n\\[\\begin{eqnarray*}\n\\bigg(\\frac{\\lambda_1 \\alpha_1}{\\beta_1} \\bigg)^{1/2} (\\mu-\\mu_1) \\  | \\ \\underline{x} \\sim t_{2\\alpha_1}\n\\end{eqnarray*}\\]\n\\(\\mu_1, \\lambda_1, \\alpha_1,\\) \\(\\beta_1\\) given previous theorem.Note: \\(E[ U | \\underline{x} ] = 0\\) \\(Var(U | \\underline{x}) = \\frac{2 \\alpha_1}{2 \\alpha_1 - 2} = \\frac{ \\alpha_1}{\\alpha_1 -1}\\)\\\n\\(\\rightarrow E[\\mu| \\underline{x}] = \\mu_1\\) \\(Var(\\mu | \\underline{x}) = \\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\frac{\\alpha_1}{\\alpha_1 -1} = \\frac{\\beta_1}{\\lambda_1(\\alpha_1 -1)}\\)","code":""},{"path":"intest.html","id":"posterior-interval-for-the-mean-mu-in-a-normal-random-sample","chapter":"6 Interval Estimates","heading":"6.2.2 Posterior Interval for the mean, \\(\\mu\\) in a normal random sample","text":"Just like frequentist CI:\n\\[\\begin{eqnarray*}\nP( -c \\leq U \\leq c \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\nP( -c \\leq \\Bigg( \\frac{\\lambda_1 \\alpha_1}{\\beta_1} \\Bigg)^{1/2} (\\mu - \\mu_1) \\leq c \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\nP( \\mu_1 - c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2} \\leq \\mu \\leq \\mu_1 + c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2} \\  | \\ \\underline{x} ) &=& 1 - \\alpha\\\\\n\\end{eqnarray*}\\]\\(\\Rightarrow\\)    \\(\\mu_1 \\pm c \\Bigg(\\frac{\\beta_1}{\\lambda_1 \\alpha_1} \\Bigg)^{1/2}\\) \\((1 - \\alpha)100\\%\\) posterior interval \\(\\mu\\).Example 6.2  Let’s say trying estimate total number soft drinks particular vending machine sell typical week. want find 90% posterior interval \\(\\mu\\). prior information (e.g., past weeks) tells us:\n\\[\\begin{eqnarray*}\n\\mu | \\tau &\\sim& N(750, 5 / \\tau = \\frac{1}{(1/5)\\tau} )\\\\\n\\tau &\\sim& gamma(1, 45)\n\\end{eqnarray*}\\]\n\\(\\mu_0 = 750\\), \\(\\lambda_0 = 1/5\\), \\(\\alpha_0 = 1\\), \\(\\beta_0=45\\)\\\nrandom sample 10 weeks gives \\(\\overline{x} = 692\\) \\(s^2 = \\frac{14400}{9} = 1600\\).\\\nposterior parameters :\n\\[\\begin{eqnarray*}\n\\mu_1 &=& \\frac{\\lambda_0 \\mu_0 + n \\overline{x}}{\\lambda_0 + n} = \\frac{(1/5)750 + 6920}{(1/5) + 10} = 693.14\\\\\n\\lambda_1 &=& \\lambda_0 + n = 10.2\\\\\n\\alpha_1 &=& \\alpha_0 + n/2 = 1+ 5 =6\\\\\n\\beta_1 &=& \\beta_0 + \\frac{1}{2} \\sum_{=1}^n (x_i - \\overline{x})^2 + \\frac{n \\lambda_0 (\\overline{x} - \\mu_0)^2}{2(\\lambda_0 +n)} = 45 + 14400/2 + \\frac{10(1/5) (692-750)^2}{2(1/5 +10)} = 7574.8\n\\end{eqnarray*}\\]find 90% PI, find cutoff values \\(P(-c \\leq t_{2\\alpha_1} \\leq c) = 0.9\\). \\(2 \\alpha_1 =12\\), \\(P(t_{12} \\leq 1.782) = 0.95\\).\n\\[\\begin{eqnarray*}\nP(-1.782 \\leq U \\leq 1.782  \\  | \\ \\underline{x} ) &=& 0.9\\\\\nP(\\mu_1 - 1.782 (\\frac{\\beta_1}{\\lambda_1 \\alpha_1})^{1/2} \\leq \\mu \\leq \\mu_1 + 1.782 (\\frac{\\beta_1}{\\lambda_1 \\alpha_1})^{1/2} \\  | \\ \\underline{x} ) &=& 0.9\\\\\nP(673.31 \\leq \\mu \\leq 712.97 \\  | \\ \\underline{x} ) &=& 0.9\n\\end{eqnarray*}\\]Given prior beliefs data, 90% probability average number cans sold per week 673.31 712.97 cans.","code":""},{"path":"intest.html","id":"improper-priors-continued","chapter":"6 Interval Estimates","heading":"Improper priors, continued…","text":"Notice \\(\\mu_0 = \\beta_0 = \\lambda_0 = 0\\) \\(\\alpha_0= -1/2\\), \\(\\mu_1 = \\overline{x}\\), \\(\\lambda_1 = n\\), \\(\\alpha_1 = (n-1)/2\\), \\(\\beta_1 = \\sum(x_i - \\overline{x})^2 / 2\\). interval becomes:\\[\\begin{eqnarray*}\n\\overline{x} &\\pm& t^* \\Bigg( \\frac{(1/2) \\sum(x_i - \\overline{x})^2}{n (n-1)/2} \\Bigg)^{1/2}\\\\\n\\overline{x} &\\pm& t^* s/\\sqrt{n}\n\\end{eqnarray*}\\]improper prior -weights beliefs gives frequentist-like (.e., data ) answer Bayesian-like interpretation.","code":""},{"path":"intest.html","id":"reflection-questions-5","chapter":"6 Interval Estimates","heading":"6.3  Reflection Questions","text":"","code":""},{"path":"intest.html","id":"ethics-considerations-5","chapter":"6 Interval Estimates","heading":"6.4  Ethics Considerations","text":"","code":""},{"path":"intest.html","id":"r-code-creating-interval-estimates","chapter":"6 Interval Estimates","heading":"6.5 R code: Creating Interval Estimates","text":"","code":""},{"path":"intest.html","id":"finding-cutoffs","chapter":"6 Interval Estimates","heading":"6.5.1 Finding cutoffs","text":"Recall q distributional functions (e.g., qnorm(), qbinom(), qunif(), qchisq(), qt()) indicates output quantile.mosiac package adds x front function name allows figure accompany numerical value quantile. highly recommend drawing pictures finding quantiles percentages.One-sided 98% t-interval \\((df = 24)\\) 98% probability left quantile interest. Note t-distribution symmetric.Two-sided 98% t-interval \\((df = 24)\\) 98% area center, 99% area left. Note code can written two different ways provides quantile values. Note also t-distribution symmetric.Two-sided 95% chi-square interval \\((df = 9)\\). Note chi-square distribution symmetric.find 90% prediction inteval cutoff, R code used:","code":"\nmosaic::xqt(.98, 24)## [1] 2.17\nmosaic::xqt(.99, 24)## [1] 2.49\nmosaic::xqt(c(0.01, 0.99), 24)## [1] -2.49  2.49\nmosaic::xqchisq(c(0.025, 0.975), 9)## [1]  2.7 19.0\nmosaic::xqt(0.95, 12)## [1] 1.78"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
