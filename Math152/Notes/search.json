[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 152 Pomona College: Computational Statistics. notes based extensively Probability & Statistics (DeGroot Schervish 2011).responsible reading relevant chapters text. text good & readable, use . make sure coming class also re-visiting warm-ups feedback provided.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"","code":""},{"path":"intro.html","id":"course-logistics","chapter":"1 Introduction","heading":"1.1 Course Logistics","text":"","code":""},{"path":"intro.html","id":"what-is-statistics","chapter":"1 Introduction","heading":"1.1.0.1 What is Statistics?","text":"Generally, statistics academic discipline uses data make claims predictions larger populations interest. science collecting, wrangling, visualizing, analyzing data representation larger whole. worth noting probability represents majority mathematical tools used statistics, probability discipline work data. taken probability class may help mathematics covered course, substitute understanding basics introductory statistics.\nFigure 1.1: Probability vs. Statistics\ndescriptive statistics describe sample hand intent making generalizations.inferential statistics use sample make claims population","code":""},{"path":"intro.html","id":"what-is-the-content-of-math-152","chapter":"1 Introduction","heading":"1.1.0.2 What is the content of Math 152?","text":"Statistical Theory introduction statistics students background probability theory, calculus, linear algebra. statistics prerequisite class. course focused theoretical aspects material, though real world applications class homework assignments. idea strong mathematical understanding concepts also understanding concepts applied real world.completion course, students :able derive methods introductory statistics using tools mathematics (.e., calculus probability).able justify use particular method (technical conditions).able weigh advantages disadvantages different estimation techniques (e.g., bias, variability, resistance outliers).know communicate results effectively.","code":""},{"path":"intro.html","id":"who-should-take-math-152","chapter":"1 Introduction","heading":"1.1.0.3 Who should take Math 152?","text":"Linear Models ubiquitous. used every science social science analyze relationships variables. Anyone planning work field uses statistical arguments make claims based data fundamental knowledge linear models. Additionally, linear models common required applied statistics course someone applying graduate school statistics.","code":""},{"path":"intro.html","id":"what-are-the-prerequisites-for-math-152","chapter":"1 Introduction","heading":"1.1.0.4 What are the prerequisites for Math 152?","text":"prerequisites Statistical Theory Probability (Math 151 equivalent) completion three semester sequence calculus linear algebra. rely heavily prerequisites, students background probability multivariable calculus find trying catch throughout semester. familiar topics conditional probabilities expectations, Central Limit Theorem, moment generating functions, probability density functions.","code":""},{"path":"intro.html","id":"is-there-overlap-with-other-classes","chapter":"1 Introduction","heading":"1.1.0.5 Is there overlap with other classes?","text":"Statistical Theory covers many topics Introductory Statistics (e.g., Math 58), however, treatment topics completely different.\nStatistical Theory uses mathematical tools (e.g., probability theory calculus) derive topics prove , many ways, optimal.","code":""},{"path":"intro.html","id":"when-should-i-take-math-152","chapter":"1 Introduction","heading":"1.1.0.6 When should I take Math 152?","text":"prerequisite structure, students Math 152 juniors seniors.","code":""},{"path":"intro.html","id":"what-is-the-workload-for-math-152","chapter":"1 Introduction","heading":"1.1.0.7 What is the workload for Math 152?","text":"one homework assignment per week, two -class midterm exams, two take-home midterm exams, final exam. Many students report working 8-10 hours per week outside class.##software use? real world applications? mathematics? CS?majority work Statistical Theory done pencil (/ pen / LaTeX).\nHowever, use R way investigating theorecial concepts computationally.\nR work done using RStudio IDE.\nneed either download R RStudio (free) onto computer use Pomona’s server.\nreal-world applications motivate theory, cours applied focus data analysis.can use R Pomona server: https://rstudio.pomona.edu/ (Pomona students able log immediately.\nNon-Pomona students need get Pomona login information.)Alternatively, feel free download R onto computer.\nR freely available http://www.r-project.org/ already installed college computers.\nAdditionally, required install RStudio turn R assignments using RMarkdown. http://rstudio.org/.\n(can use LaTeX compiler : https://yihui.name/tinytex/)\nFigure 1.2: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.3: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"statistics-a-review","chapter":"1 Introduction","heading":"1.2 Statistics: a review","text":"Although Statistical Theory statistics prerequisite, students seen many building blocks high school classes Probability Theory.\nreview vocabulary covered previous contexts.","code":""},{"path":"intro.html","id":"vocabulary","chapter":"1 Introduction","heading":"1.2.1 Vocabulary","text":"","code":""},{"path":"intro.html","id":"probability","chapter":"1 Introduction","heading":"Probability","text":"probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).probability outcome refers often outcome occur long run random process repeated identical conditions (relative frequency interpretation) degree statement supported available evidence (subjective interpretation).experiment activity situation uncertainty outcome.experiment activity situation uncertainty outcome.sample space list possible outcomes random trial. (Called S.)sample space list possible outcomes random trial. (Called S.)event potential subset sample space.event potential subset sample space.set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).set events (.e., set subsets S) called power set \\(S\\), \\(\\mathcal{P}(S)\\).simple event event consisting exactly one outcome.simple event event consisting exactly one outcome.Two events mutually exclusive disjoint occur simultaneously.Two events mutually exclusive disjoint occur simultaneously.Two events independent occurrence one change probability second occur.Two events independent occurrence one change probability second occur.random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable function sample space, \\(S\\), real line: \\[ X: S \\mapsto {\\mathbb R}\\]random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).random variable, \\(X\\), discrete distribution iff \\(X\\) takes discrete set values:\n\\[ f(x_i) = P( X = x_i) \\quad = 1, 2, \\dots k\\]\n\\(f(x) = 0\\) values \\(x\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous random variable \\(X: S \\mapsto {\\mathbb R}\\) can take continuous\nset values (.e., values interval). :\n\\[P ( \\le X \\le b) = \\int_a^b f(x) \\, dx \\]\n\\(f(x)\\) called probability density function, pdf, \\(X\\).continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]continuous discrete random variables, cumulative distribution function (cdf) random variable, \\(X\\), defined :\n\\[ F(x) = P(X \\le x), \\quad -\\infty < x < \\infty\\]","code":""},{"path":"intro.html","id":"statistics","chapter":"1 Introduction","heading":"Statistics","text":"statistic numerical measurement get sample, function data.parameter numerical measurement population. never know true value parameter.estimator function unobserved data tries approximate unknown parameter value.estimate value estimator given set data. [Estimate statistic can used interchangeably.]\\(\\Omega\\) parameter space, set values contains possible realizations parameter.many statistical problems, assume distribution generated data completely known except parameters. goal statistics make statements (inference) population using sample (data). course broken two main parts: parameter estimation tests hypotheses.","code":""},{"path":"intro.html","id":"estimation","chapter":"1 Introduction","heading":"1.2.2 Estimation","text":"many statistical problems, assume distribution generated data completely known except parameter(s). first half course, ’re going learn different ways estimate parameters, properties estimators, makes good estimator.","code":""},{"path":"intro.html","id":"example","chapter":"1 Introduction","heading":"example","text":"Consider trying estimate true average weight dozen eggs particular farm. add random 12 values pdf different multiplying one observation 12. Clearly 12*one egg much variable weight dozen eggs.\n\\[\\begin{align*}\n\\mbox{statistic } 1 &= 12 X\\\\\n\\mbox{statistic } 2 &= \\sum_{=1}^{12} X_i\\\\\n\\Omega &= \\{ \\mu: \\mu \\\\!\\!R+\\}\\\\\n\\end{align*}\\]","code":""},{"path":"intro.html","id":"reflection-questions","chapter":"1 Introduction","heading":"1.3  Reflection Questions","text":"difference sample population?difference discrete random variable continuous random variable?difference pdf cdf?can find expected value discrete random variable? can find expected value continuous random variable?difference R RStudio?","code":""},{"path":"intro.html","id":"ethics-considerations","chapter":"1 Introduction","heading":"1.4  Ethics Considerations","text":"important distinguish sample population?ever okay model discrete random variable using continuous probability model? , ? , ?better understand method’s properties theoretically better use computational tools investigate method’s properties? [question probably won’t able answer end semester.]","code":""},{"path":"intro.html","id":"r-code-reproduciblity","chapter":"1 Introduction","heading":"1.5 R code: reproduciblity","text":"","code":""},{"path":"intro.html","id":"repro","chapter":"1 Introduction","heading":"1.5.1 Reproducibility","text":"Reproducibility long considered important topic consideration research project. However, recently increased press available examples understanding impact non-reproducible science can .Kitzes, Turek, Deniz (2018) provide full textbook structure reproducible research well dozens case studies help hone skills consider different aspects reproducible pipeline. handful examples get us started.","code":""},{"path":"intro.html","id":"need-for-reproducibility","chapter":"1 Introduction","heading":"1.5.1.1 Need for Reproducibility","text":"\nFigure 1.4: slide taken Kellie Ottoboni https://github.com/kellieotto/useR2016\n","code":""},{"path":"intro.html","id":"example-1","chapter":"1 Introduction","heading":"Example 1","text":"Science retracts gay marriage paper without agreement lead author LaCourIn May 2015 Science retracted study canvassers can sway people’s opinions gay marriage published just 5 months prior.Science Editor--Chief Marcia McNutt:\nOriginal survey data made available independent reproduction results.\nSurvey incentives misrepresented.\nSponsorship statement false.\nOriginal survey data made available independent reproduction results.Survey incentives misrepresented.Sponsorship statement false.Two Berkeley grad students attempted replicate study quickly discovered data must faked.Methods ’ll discuss can’t prevent fraud, can make easier discover issues.Source: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent","code":""},{"path":"intro.html","id":"example-2","chapter":"1 Introduction","heading":"Example 2","text":"Seizure study retracted authors realize data got “terribly mixed”authors Low Dose Lidocaine Refractory Seizures Preterm Neonates:article retracted request authors. carefully re-examining data presented article, identified data two different hospitals got terribly mixed. published results reproduced accordance scientific clinical correctness.Source: http://retractionwatch.com/2013/02/01/seizure-study-retracted--authors-realize-data-got-terribly-mixed/","code":""},{"path":"intro.html","id":"example-3","chapter":"1 Introduction","heading":"Example 3","text":"Bad spreadsheet merge kills depression paper, quick fix resurrects itThe authors informed journal merge lab results survey data used paper resulted error regarding identification codes. Results analyses based incorrectly merged data set. analyses established results reported manuscript interpretation data correct.Original conclusion: Lower levels CSF IL-6 associated current depression future depression …Revised conclusion: Higher levels CSF IL-6 IL-8 associated current depression …Source: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-/","code":""},{"path":"intro.html","id":"example-4","chapter":"1 Introduction","heading":"Example 4","text":"PNAS paper retracted due problems figure reproducibility (April 2016):\nhttp://cardiobrief.org/2016/04/06/pnas-paper--prominent-cardiologist--dean-retracted/","code":""},{"path":"intro.html","id":"the-reproducible-data-analysis-process","chapter":"1 Introduction","heading":"1.5.1.2 The reproducible data analysis process","text":"Scriptability \\(\\rightarrow\\) RLiterate programming \\(\\rightarrow\\) R MarkdownVersion control \\(\\rightarrow\\) Git / GitHub","code":""},{"path":"intro.html","id":"scripting-and-literate-programming","chapter":"1 Introduction","heading":"Scripting and literate programming","text":"Donald Knuth “Literate Programming” (1983)Let us change traditional attitude construction programs: Instead imagining main task instruct computer- , let us concentrate rather explaining human beings- want computer .ideas literate programming around many years!tools putting practice also aroundbut never accessible current tools","code":""},{"path":"intro.html","id":"reproducibility-checklist","chapter":"1 Introduction","heading":"Reproducibility checklist","text":"tables figures reproducible code data?code actually think ?addition done, clear done? (e.g., parameter settings chosen?)Can code used data?Can extend code things?","code":""},{"path":"intro.html","id":"tools-r-r-studio","chapter":"1 Introduction","heading":"Tools: R & R Studio","text":"See great video (less 2 min) reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.beYou must use R RStudio software programsR programmingR Studio brings everything togetherYou may use Pomona’s server: https://rstudio.pomona.edu/\nFigure 1.5: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.6: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"bayesian-estimation.html","id":"bayesian-estimation","chapter":"2 Bayesian Estimation","heading":"2 Bayesian Estimation","text":"","code":""},{"path":"bayesian-estimation.html","id":"bayes-rule","chapter":"2 Bayesian Estimation","heading":"2.1 Bayes’ Rule","text":"Theorem 2.1  Bayes’ RuleGiven events \\(\\) \\(B\\),\n\\[\\begin{eqnarray*}\nP(|B) &=& \\frac{P(AB)}{P(B)} = \\frac{P(B|) P()}{P(AB) + P(^cB)} \\nonumber \\\\\n&=& \\frac{P(B|)P()}{ \\sum_i P(B|A_i) P(A_i)}\n\\end{eqnarray*}\\]Many following examples may familiar . reading , work understand intuition (denominator changes condition!) well mathematical connection Bayes’ Rule.Example 2.1  Suppose rate infection TB 1 1000 (0.1 percent = 0.001). Suppose TB test used 90% accurate: gives positive result 10 percent people actually TB, reaction skin test. Also, 10% people actually TB fail react test.1What’s chance someone TB test positive?’s chance randomly chosen person tests negative actually TB?another TB test gives fewer false positives, expensive. better use one?prior probability TB?posterior probability TB (given positive test)?Solution:\n* 10 10,000 people disease. 9 10 actually test positive TB. However, 999 9990 people false positives. , $9/(999+9) = 0.0089 $0.9% people test positive actually TB.\n* 1 10,000 people false negative (0.0001 = 0.01% opposed 9.99% false positives).\n* necessarily, since ’s much worse false negative false positive. People test positive given another test fewer false positives.\n* 0.001\n* 0.009Example 2.2  cab involved hit run accident night. Two cab companies, Green Blue, operate city. Suppose told following:85 percent cabs city Green, remaining 15 percent Blue.witness identified cab Blue (dark!) court tested reliability witness circumstances existed night accident, determined witness correctly identified cab color 80% time, made mistake 20% time, regardless actual color cab.’s verdict? .e., probability cab involved hit--run actually Blue?Solution:information given, following probabilities known:\n\\[\\begin{eqnarray*}\nP(said B | B) &=& 0.8\\\\\nP(said G | B) &=& 0.2\\\\\nP(B) &=& 0.15 \\ \\ \\ \\mbox{prior probability!}\n\\end{eqnarray*}\\]probability interest :\n\\[\\begin{eqnarray*}\nP(B | said B) &=& \\frac{P(said B | B) P(B)}{P(said B | G) P(G) + P(said B | B) P(B)}\\\\\n&=& \\frac{0.8*0.15}{0.2*0.85 + 0.8*0.15} = 0.41\n\\end{eqnarray*}\\]Example 2.3  Consider famous Monte Hall problem based game show, Let’s Make Deal. part show, contestant asked pick one three doors. Two doors nothing behind , third door car prize. Monte Hall (host) opens non-prize door contestant hadn’t chosen (always door available open one prize). Monte offers contestant opportunity switch original door remaining door. switch? Stay? doesn’t matter? probability winning situations?Define following:\n\\[\\begin{eqnarray*}\nC_i &:& \\mbox{car behind Door $$, } \\\\{ 1, 2, 3\\}\\\\\nH_{ij} &:& \\mbox{host opens Door $j$ player picked Door $$, } , j \\\\{ 1,2,3\\}\n\\end{eqnarray*}\\]example, \\(C_2\\) situation car behind Door 2, \\(H_{23}\\) denotes situation host opened Door 3 chose Door 2. Note \\(H_{ij}\\) information (’ll condition ). Let’s say start picking Door 2, host opens Door 3.\\[\\begin{eqnarray*}\nP(C_i) &=& \\frac{1}{3}\\\\\nP(C_2) &=& \\frac{1}{3}\\\\\nP(C_2 | H_{23}) &=& \\frac{P(H_{23} | C_2) P(C_2)}{P(H_{23})} \\ \\ \\ \\ \\ \\mbox{(Bayes rule, right !  See } P(H_{23}) \\mbox{ )}\\\\\n&=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{P(H_{23})} \\\\\n\\nonumber \\\\\nP(H_{23}) &=&  P(H_{23} C_1) + P(H_{23} C_2) + P(H_{23} C_3)\\\\\n&=&  P(H_{23} | C_1) P(C_1) + P(H_{23} | C_2) P(C_2) + P(H_{23} | C_3) P(C_3)\\\\\n&=& \\frac{1}{2} \\cdot \\frac{1}{3} + 1 \\frac{1}{3} + 0 \\frac{1}{3} = \\frac{1}{2}\\\\\n\\nonumber\\\\\nP(C_2 | H_{23}) &=& \\frac{\\frac{1}{2} \\cdot \\frac{1}{3}}{\\frac{1}{2}}\\\\\n&=& \\frac{1}{3}\\\\\n\\end{eqnarray*}\\]Using know rules game (, car behind door 3, ’d never open !), know:\n\\[\\begin{eqnarray*}\nP(C_3 | H_{23}) = 0\n\\end{eqnarray*}\\]car behind one three doors:\n\\[\\begin{eqnarray*}\n1 &=& P(C_1 | H_{23}) + P(C_2 | H_{23}) + P(C_3 | H_{23})\\\\\nP(C_1 | H_{23} ) &=& 1 - P(C_2 | H_{23}) - P(C_3 | H_{23})\\\\\n&=& 1 - \\frac{1}{3} - 0\\\\\n&=& \\frac{2}{3}\n\\end{eqnarray*}\\]…. probability car behind Door 1 2/3 probability ’s behind Door 2 1/3 \\(\\rightarrow\\) switch doors!previous situations based discrete values parameter data (neither typically true). think {} {}, want find value {} \\(P(|B)\\) maximized. going use probability distribution functions (pdfs) instead discrete probabilities, need notation.","code":""},{"path":"bayesian-estimation.html","id":"prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.2 Prior Distributions","text":"prior distribution distribution parameter (e.g., \\(\\theta\\)) observing data. Note:observations come \\(f(x|\\theta), \\theta \\\\Omega\\)can express likely \\(\\theta\\) various regions \\(\\Omega\\) terms probability distribution \\(\\theta\\).Bayesians believe use prior distributions modeling always know something situation hand.Frequentists believe use data collected experiment sample (prior information).Example 2.4  want predict high temperature given day October.\n\\[\\begin{align*}\n\\Omega = \\{ (\\theta, \\sigma^2) &: \\theta \\\\!\\!R, \\sigma^2 \\\\!\\!R^+\\}\\\\\n\\mbox{} &: -\\infty < \\theta < \\infty, \\sigma^2 > 0 \\}\\\\\n\\mbox{} &: \\theta > 30, 0 < \\sigma^2 < 625 \\}\\\\\n\\end{align*}\\]\nSuppose know variance \\(\\sigma^2 = 12^2\\). mean, \\(\\theta\\) unknown. might specify prior distribution \\(\\theta\\) :\n\\[\\begin{eqnarray*}\n\\xi (\\theta) &\\rightarrow& \\theta \\sim N(\\mu, \\nu^2)\\\\\n&& \\mu =78^\\circ, \\nu^2 = (2.5^\\circ)^2\\\\\n\\end{eqnarray*}\\]value \\(\\nu\\) measure uncertainty prior beliefs. , estimated \\(\\theta\\) \\(78^\\circ\\), aren’t sure value, add uncertainty belief (\\(\\nu\\)). \\(\\mu\\) \\(\\nu\\) called hyper-parameters.","code":""},{"path":"bayesian-estimation.html","id":"posterior-distributions","chapter":"2 Bayesian Estimation","heading":"2.3 Posterior Distributions","text":"posterior distribution conditional distribution parameter (e.g., \\(\\theta\\)) given observed data.Aside, little probability review:Example 2.5  Suppose interested rolling two dice. Let \\(X\\) larger value; let \\(Y\\) sum two dice. Find joint marginal distributions \\(X\\) \\(Y\\). solution table probabilities:notation review. Suppose \\(n\\) data points \\(f(x| \\theta)\\). ’ll assume ’s simple random sample (SRS), therefore observations independent.\\[\\begin{eqnarray*}\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(x_1| \\theta) f(x_2| \\theta) \\cdots f(x_n| \\theta)\\\\\n\\mbox{ } \\underline{x} &=& \\{ x_1, x_2, \\ldots, x_n \\}\\\\\nf(x_1, x_2, \\ldots, x_n| \\theta) &=& f(\\underline{x} | \\theta)\n\\end{eqnarray*}\\]Remember, \\(f(\\underline{x} | \\theta)\\) conditional distribution \\(\\underline{x}\\) given \\(\\theta\\). likelihood function, \\(f(\\underline{x} | \\theta)\\), joint pdf observations (representing: likely data?).Define\n\\[\\begin{eqnarray*}\nf(\\underline{x}, \\theta) &=& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\ng_n(\\underline{x}) &=& \\int_\\Omega f(\\underline{x}, \\theta) d\\theta\\\\\n&=& \\int_\\Omega f(\\underline{x} | \\theta) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]Remember, however, interested probability parameter given data:\n\\[\\begin{eqnarray*}\n\\xi(\\theta| \\underline{x}) = \\frac{f(\\underline{x} | \\theta) \\xi(\\theta)}{g_n(\\underline{x})} \\ \\ \\ \\ \\ \\ \\ \\theta \\\\Omega\n\\end{eqnarray*}\\](Bayes’ Theorem!!!)prior, \\(\\xi(\\theta)\\) relative likelihood \\(\\theta\\) data observed.posterior, \\(\\xi(\\theta | \\underline{x})\\) relative likelihood \\(\\theta\\) \\(\\underline{X} = \\underline{x}\\) observed.know posterior function \\(\\theta\\). ’s important keep mind function , posterior function data.\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi(\\theta)\\\\\n\\mbox{posterior} &\\propto& \\mbox{likelihood} \\cdot \\mbox{prior}\n\\end{eqnarray*}\\], posterior proportional product likelihood prior. Note \\(g_n(\\underline{x})\\) depend \\(\\theta\\) part proportionality constant. , can always find \\(g_n(\\underline{x})\\) know posterior integrates 1. (Sometimes \\(g_n\\) extraordinarily difficult find.)\n\\[\\begin{eqnarray*}\n\\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta = 1\n\\end{eqnarray*}\\]Example 2.6  Suppose true proportion freethrows Steph Curry able make successfully unknown. assume freethrows distributed according Bernoulli process.say,\n\\[\\begin{eqnarray*}\nX &\\sim& \\mbox{Bernoulli}(\\theta)\\\\\nf(\\underline{x}|\\theta)&=&  \\theta ^y (1 - \\theta)^{n-y} \\ \\ \\ \\ y = \\sum_{=1}^n x_i\n\\end{eqnarray*}\\]\nNote: \\(\\underline{x} = \\{x_1, x_2, \\ldots, x_n\\}\\) specific ordering 0s 1s.prior information Curry’s abilities, put uniform prior \\(\\theta\\).\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) \\propto \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]functional form posterior (likelihood times prior). ’re trying estimate \\(\\theta\\), distribution \\(\\theta\\) given data? Recall Beta distribution (probability review):\\[\\begin{eqnarray*}\nW &\\sim& \\mbox{Beta}(\\alpha,\\beta)\\\\\nf(w) &=& \\frac{1}{B(\\alpha,\\beta)} w^{\\alpha - 1} (1-w)^{\\beta-1}\\\\\n&=& \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)}w^{\\alpha - 1} (1-w)^{\\beta-1} \\ \\ \\ \\ w \\[0,1]\n\\end{eqnarray*}\\]Using likelihood prior, able find full posterior distribution integration:\n\\[\\begin{eqnarray*}\n1 &=& \\int_\\Omega \\xi(\\theta | \\underline{x}) d\\theta \\\\\n&=&  k \\cdot \\int_\\Omega \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta) d\\theta\\\\\n&=& k \\cdot \\int_0^1 \\theta^y (1-\\theta)^{n-y} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(y+1 + n-y + 1)} \\cdot \\int_0^1 \\frac{\\Gamma(y+1 + n-y + 1)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^{y+1-1} (1-\\theta)^{n-y+1-1} d\\theta\\\\\n&=& k \\cdot \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)} \\cdot 1\\\\\nk &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)}\\\\\ng(\\underline{x}) &=& \\frac{\\Gamma(y+1)\\Gamma(n-y+1)}{\\Gamma(n+2)}\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n+2)}{\\Gamma(y+1)\\Gamma(n-y+1)} \\theta^y (1-\\theta)^{n-y} I_{[0,1]}(\\theta)\n\\end{eqnarray*}\\]Note, however, didn’t actually need integrate find \\(g(\\underline{x})\\). simply needed note \\(\\theta\\) takes place \\(w\\) Beta distribution, automatically know appropriate constant value.information prior distribution \\(\\theta\\)? Suppose believe Beta(,b). (Note: \\(E[\\theta] = \\frac{}{+b}\\), Var\\((\\theta) = \\frac{ab}{(+b)^2 (+b+1)}\\), SD$() = $ ).\\[\\begin{eqnarray*}\n\\xi(\\theta) &\\propto& \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{b-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y+-1} (1-\\theta)^{n-y+b-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y+, n-y+b)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n++b)}{\\Gamma(y+)\\Gamma(n-y+b)}\\theta^{y+-1} (1-\\theta)^{n-y+b-1}\n\\end{eqnarray*}\\]Note: didn’t need calculate \\(g(\\underline{x})\\)!!!\\(E[\\theta | \\underline{x}] = \\frac{y+}{n++b}\\)Var \\((\\theta | \\underline{x}) = \\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}\\)SD \\((\\theta | \\underline{x}) = \\sqrt{\\frac{(y+)(n++b)}{(n++b)^2 (n++b+1)}}\\)Given prior sample size \\(n\\), best guess Curry’s ability hit freethrows?kind confidence estimate?","code":""},{"path":"bayesian-estimation.html","id":"conjugate-prior-distributions","chapter":"2 Bayesian Estimation","heading":"2.4 Conjugate Prior Distributions","text":"conjugate prior distribution one prior distribution family posterior distribution. Beta distribution conjugate Binomial distribution (note, \\(U[0,1]\\) distribution Beta(\\(\\alpha\\)=1,\\(\\beta\\)=1)).Example 2.7  Continuing example temperature, \\(X \\sim N(\\theta, \\sigma^2\\) (known)) normal prior, \\(\\theta \\sim N(\\mu, \\nu^2)\\). Remember typically, prior completely specified. example \\(\\mu=78\\) \\(\\nu=2.5\\). :\\\n\\[\\begin{eqnarray*}\nf(\\underline{x} | \\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\sigma^2} \\sum_{=1}^n (x_i - \\theta)^2 \\bigg]\\\\\n&\\propto& \\exp \\Bigg[ - \\frac{1}{2 \\sigma^2} \\bigg(n (\\theta-\\overline{x})^2 + \\sum_{=1}^n (x_i - \\overline{x})^2 \\bigg) \\Bigg]\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2\\bigg]\\\\\n&&\\\\\n\\xi(\\theta) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n&&\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& f(\\underline{x} | \\theta) \\xi (\\theta)\\\\\n&\\propto& \\exp \\bigg[ - \\frac{n}{2 \\sigma^2} (\\theta - \\overline{x})^2 -\\frac{1}{2 \\nu^2} (\\theta - \\mu)^2 \\bigg]\\\\\n\\mbox{note: } && \\frac{n}{\\sigma^2}(\\theta - \\overline{x})^2 + \\frac{1}{\\nu^2}(\\theta - \\mu)^2 = \\frac{1}{\\nu_1^2}(\\theta - \\mu_1)^2 + \\frac{n}{\\sigma^2 + n \\nu^2}(\\overline{x}-\\mu)^2 \\ \\ \\ \\mbox{ pg 399}\\\\\n\\xi(\\theta|\\underline{x}) &\\propto& \\exp \\bigg[ - \\frac{1}{2 \\nu_1^2} (\\theta - \\mu_1)^2 \\bigg]\\\\\n\\theta | \\underline{x} &\\sim& N (\\mu_1, \\nu_1^2)\\\\\n\\mbox{: } && \\mu_1 = \\frac{\\sigma^2 \\mu + n \\nu^2 \\overline{x}}{\\sigma^2 + n \\nu^2} \\ \\ \\ \\ \\nu_1^2 = \\frac{\\sigma^2 \\nu^2}{\\sigma^2 + n \\nu^2}\n\\end{eqnarray*}\\]Remember, computing posterior \\(\\theta\\), can ignore anything doesn’t depend \\(\\theta\\) (“known” parameters, data, constants,…)","code":""},{"path":"bayesian-estimation.html","id":"improper-priors","chapter":"2 Bayesian Estimation","heading":"2.5 Improper Priors","text":"Improper prior distributions actually probability functions, yet lead posterior distributions probability functions (, integrate 1). Improper priors capture idea data worth prior belief. Often, improper prior lead Frequentist result. example, Beta(0,0) prior Bernoulli likelihood leads :\\(\\xi(\\theta)\\) integrate 1, proper pdf. However posterior proper pdf,\\[\\begin{eqnarray*}\n\\xi(\\theta | \\underline{x}) &\\propto& \\theta^y (1-\\theta)^{n-y} \\theta^{-1} (1-\\theta)^{-1} I_{[0,1]}(\\theta)\\\\\n&\\propto& \\theta^{y-1} (1-\\theta)^{n-y-1} I_{[0,1]}(\\theta)\\\\\n\\theta| \\underline{x} &\\sim& \\mbox{Beta}(y, n-y)\\\\\n\\xi(\\theta | \\underline{x}) &=& \\frac{\\Gamma(n)}{\\Gamma(y)\\Gamma(n-y)}\\theta^{y-1} (1-\\theta)^{n-y-1}\\\\\n\\mbox{Note: } && E[\\theta | \\underline{X} ] = \\frac{y}{n} \\ \\ \\ \\mbox{ expected frequentist model!}\n\\end{eqnarray*}\\]conjugate priors improper prior limiting case: Beta(0,0), gamma(0,0), Normal(\\(\\mu, \\nu^2 = \\infty\\)). normal improper prior ignores prior constant, becomes:\\[\\begin{eqnarray*}\n\\xi(\\theta) = \\lim_{\\nu^2 \\rightarrow \\infty} exp\\bigg(-\\frac{1}{2\\nu^2}(\\theta - \\mu)^2\\bigg) = 1\n\\end{eqnarray*}\\], improper normal prior flat line reals. Note improper normal prior used normal likelihood, posterior \\(\\theta | \\underline{x} \\sim N (\\underline{x}, \\sigma^2 /n)\\). [, prior indicating knowledge \\(\\theta\\) produces posterior depends data.]","code":""},{"path":"bayesian-estimation.html","id":"bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.6 Bayes’ Estimators","text":"Prior posterior distributions tell us Bayesians think parameters. next question need address think estimators? estimator function data hope close true value parameter.Note:\n\\[\\begin{eqnarray*}\n\\delta(X_1, X_2, \\ldots, X_n) &=& \\delta(\\underline{X}) \\mbox{  estimator}\\\\\n\\delta(x_1, x_2, \\ldots, x_n) &=& \\delta(\\underline{x}) \\mbox{  estimate}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayesian-estimation.html","id":"loss-functions","chapter":"2 Bayesian Estimation","heading":"2.6.1 Loss functions","text":"(responsible material loss functions. take away message section Bayes estimator use expected value posterior distribution. However, seen , Bayes estimators, example, median posterior distribution used.)want estimator \\(\\theta\\) leads estimate close true value \\(\\theta\\). loss function helps determine far estimator . particular estimate, \\(\\):\n\\[\\begin{eqnarray*}\n\\mbox{squared error loss: } L(\\theta, ) &=& (\\theta - )^2\\\\\n\\mbox{absolute error loss: } L(\\theta, ) &=& |\\theta - |\\\\\n\\end{eqnarray*}\\]\nwant loss small (minimized).","code":""},{"path":"bayesian-estimation.html","id":"squared-error-loss","chapter":"2 Bayesian Estimation","heading":"2.6.1.1 Squared Error Loss","text":"Without data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,)] = \\int_\\Omega L(\\theta, ) \\xi(\\theta) d\\theta\n\\end{eqnarray*}\\]data, find \\(\\) minimizes:\n\\[\\begin{eqnarray*}\nE[L(\\theta,) | \\underline{X}] &=& \\int_\\Omega L(\\theta, ) \\xi(\\theta| \\underline{X}) d\\theta\\\\\n\\end{eqnarray*}\\]\nLet \\(\\delta^*(\\underline{X})\\) value \n\\[\\begin{eqnarray*}\nE[L(\\theta, \\delta^*(\\underline{X})) | \\underline{X} ] &=& \\min_{\\\\Omega} E[ L(\\theta,) | \\underline{X} ]\\\\\n\\delta^*(\\underline{X}) && \\mbox{ Bayes estimator } \\theta\\\\\n\\delta^*(\\underline{x}) && \\mbox{ Bayes estimate } \\theta\\\\\n\\end{eqnarray*}\\]\n\\(\\delta^*\\)?Note: \\(\\delta^*\\) depends loss function prior / posterior.\\[\\begin{eqnarray*}\nE[L(\\theta, ) | \\underline{X} ] &=& E[ (\\theta - )^2 | \\underline{X}]\\\\\n&=& E[ \\theta^2 - 2a\\theta + ^2 | \\underline{X}]\\\\\n&=& E[\\theta^2 | \\underline{X}] - 2a E[\\theta | \\underline{X}] + ^2\\\\\n&&\\\\\n\\frac{\\partial E[ (\\theta - )^2 | \\underline{X}]}{\\partial } &=& 0\\\\\n&&\\\\\n- 2 E[\\theta | \\underline{X}] + 2a &=& 0\\\\\n&=& E[\\theta | \\underline{X}] = \\delta^*(\\underline{X}) !!!\\\\\n&&\\\\\n\\frac{\\partial^2 E[ (\\theta - )^2 | \\underline{X}]}{\\partial ^2} &=& 2 > 0 \\rightarrow \\mbox{ loss minimized}\\\\\n\\end{eqnarray*}\\]Example 2.8  Let \\(\\theta\\) denote average number defects per 100 feet tape. \\(\\theta\\) unknown, prior \\(\\theta\\) gamma distribution \\(E[\\theta] = \\alpha / \\beta = 2/10, \\alpha= 2, \\beta = 10\\). 1200 foot roll tape inspected, exactly 4 defects found.Bayes’ estimate average number defects per 100 feet?\\[\\begin{eqnarray*}\n\\mbox{Prior:     }&&\\\\\n\\xi(\\theta) &=& \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta \\theta} = \\frac{10^2}{\\Gamma(2)} \\theta e^{-10\\theta}\\\\\n\\mbox{Likelihood:     }&&\\\\\nf(\\underline{x} | \\theta) &=& \\prod_{=1}^n \\frac{e^{-\\theta} \\theta^{x_i}}{x_i!} = \\frac{e^{-n\\theta} \\theta^{\\sum x_i}}{\\prod (x_i !)}\\\\\n\\mbox{Posterior:     }&&\\\\\n\\xi(\\theta | \\underline{x}) &\\propto& \\frac{ \\theta e^{-10\\theta} e^{-n \\theta} \\theta^{\\sum x_i}}{\\Gamma(2) 10^2 \\prod (x_i !)}\\\\\n&\\propto& e^{-\\theta(n+10)} \\theta ^{\\sum x_i + 1} \\\\\n\\\\\n\\theta | \\underline{x} &\\sim& \\mbox{Gamma } (\\sum x_i + 2 = 6, n + 10 = 22)\\\\\n\\\\\n\\delta^*(\\underline{X}) &=& \\frac{ \\sum X_i + 2}{n+10}\\\\\n\\delta^*(\\underline{x}) &=& \\frac{6}{22} = \\frac{3}{11}\n\\end{eqnarray*}\\]\nNote: Gamma distribution parameterized slightly differently DeGroot sheet (exponential). Make sure expected value matches ’ve given problem.","code":""},{"path":"bayesian-estimation.html","id":"absolute-loss","chapter":"2 Bayesian Estimation","heading":"2.6.1.2 Absolute Loss","text":"minimize \\(E[ | \\theta - | | \\underline{X}]\\) ? $    ^*() = (| )$ (see Theorem 4.5.1). However, isn’t always obvious compute median non-symmetric distributions. symmetric distributions median = mean.","code":""},{"path":"bayesian-estimation.html","id":"evaluating-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.7 Evaluating Bayes Estimators","text":"","code":""},{"path":"bayesian-estimation.html","id":"mean-squared-error","chapter":"2 Bayesian Estimation","heading":"2.7.1 Mean Squared Error","text":"get MSE, let’s talk Agresti-Coull estimate Binomial parameter probability success. estimator attenuates sample proportion closer 0.5 effect reducing variability estimator. Indeed, evidence “add two successes two failures” approach create precise confidence intervals.2Even though agreement better estimate, ’d like show biased.\\[\\begin{eqnarray*}\n\\tilde{p} &=& \\frac{X+2}{n+4}\\\\\nE[\\tilde{p}] &=& \\frac{n\\theta+2}{n+4}\\\\\nbias(\\tilde{p}) &=& \\frac{n\\theta+2}{n+4} - \\theta\\\\\n&=& \\frac{n\\theta + 2 - n\\theta - 4\\theta}{n+4}\\\\\n&=& \\frac{2-4\\theta}{n+4}\n\\end{eqnarray*}\\]","code":""},{"path":"bayesian-estimation.html","id":"mse-in-general-frequentist","chapter":"2 Bayesian Estimation","heading":"2.7.1.1 MSE in general (“Frequentist”)","text":"Mean Squared Error (MSE) expected squared difference parameter (\\(\\theta\\)) estimate (\\(\\hat{\\theta}\\)), , typically, \\(\\hat{\\theta}\\) function data.frequentist MSE based expected values taken likelihood pdf (\\(X | \\theta\\)).Note MSE can written sum bias squared variance:\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_F(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_F(\\hat{\\theta}) &=& E [ (\\hat{\\theta} - \\theta)^2 ]\\\\\n&=& E [ ( \\hat{\\theta} - E(\\hat{\\theta}) + E(\\hat{\\theta}) - \\theta)^2 ] \\mbox{ RV X!} \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2 + 2(\\hat{\\theta} - E(\\hat{\\theta}))(E(\\hat{\\theta}) - \\theta) + (E(\\hat{\\theta}) - \\theta)^2]\\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + 0 + E[ (E(\\hat{\\theta}) - \\theta)^2 ] \\\\\n&=& E [ (\\hat{\\theta} -  E(\\hat{\\theta}))^2] + (E(\\hat{\\theta}) - \\theta)^2  \\\\\n&=& \\mbox{var}(\\hat{\\theta}) + (\\mbox{bias}(\\hat{\\theta}))^2\n\\end{eqnarray*}\\]Note taking expected values, resulting MSE function \\(\\theta\\) (function data).example, consider Bernoulli situation (e.g., Basketball shooter):\n\\[\\begin{eqnarray*}\n\\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\nVar(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\nbias(\\hat{\\theta}) &=& E(\\hat{\\theta}) - \\theta = 0\\\\\nMSE_F(\\hat{\\theta}) &=& \\frac{\\theta}{n}\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"bayesian-estimation.html","id":"bayesian-mse","chapter":"2 Bayesian Estimation","heading":"2.7.1.2 Bayesian MSE","text":"Bayesians, MSE simply expected squared error loss conditional data (, expected value taken posterior, \\(\\theta | \\underline{X}\\).)Bayesian MSE based expected values taken posterior pdf (\\(\\theta | \\underline{X}\\)).let \\(\\delta = \\delta(\\underline{X}) = E(\\theta | \\underline{X})\\) estimator, MSE :\n\\[\\begin{eqnarray*}\n(\\mbox{MSE}_B(\\hat{\\theta}(\\underline{X})) =) \\mbox{MSE}_B(\\delta(\\underline{X})) &=& E [ (\\delta - \\theta)^2 | \\underline{X}]\\\\\n&=& E [ ( E(\\theta | \\underline{X}) - \\theta)^2 | \\underline{X}] \\mbox{RV } \\theta \\mbox{!!}\\\\\n&=& \\mbox{var}(\\theta | \\underline{X})\\\\\n\\end{eqnarray*}\\]Continuing example, note \\(\\theta \\sim Beta(,b)\\), means \\(\\theta | \\underline{X} \\sim Beta(X + , n - X + b)\\).\\[\\begin{eqnarray*}\n\\delta(X) &=& \\frac{X + }{n ++b}\\\\\nMSE_B(\\delta(X)) &=& \\frac{(X+)(n++b)}{(n++b)^2(n++b+1)}\\\\\n\\end{eqnarray*}\\]Note Bayesian MSE simply posterior variance parameter interest. ’ve used expected value estimate, bias. Note Bayesian MSE function data (\\(\\theta\\)), compare Bayesian MSE Frequentist MSE directly.Example 2.9  Recall tape example, Example 2.8.Prior: Gamma(2, 10) ((2, 1/10) depending parametrize))Data likelihood: Poisson(\\(\\theta\\))Posterior: Gamma (\\(\\sum X_i + 2\\), \\(n\\) + 10)Note calculations done Frequentist MSE conditional data.\\[\\begin{eqnarray*}\n(\\mbox{frequentist estimator}) \\ \\ \\hat{\\theta} &=& \\frac{\\sum X_i}{n}\\\\\n(\\mbox{Bayesian estimator}) \\ \\ \\delta(\\underline{X}) &=& \\frac{\\sum X_i+2}{n+10}\\\\\n& \\\\\nMSE_F(\\hat{\\theta}) &=& var(\\hat{\\theta}) + bias(\\hat{\\theta})^2\\\\\n&=& \\theta/n + 0 = \\theta/n\\\\\nMSE_B(\\delta(\\underline{X})) &= & var(\\theta | \\underline{X})\\\\\n&=& \\frac{\\sum X_i+2}{(n+10)^2}\\\\\n&&\\\\\nMSE_F(\\delta(\\underline{X})) &=& var(\\delta(\\underline{X})) + bias(\\delta(\\underline{X}))^2\\\\\n\\end{eqnarray*}\\]\\[\\begin{eqnarray*}\nbias(\\delta(\\underline{X})) &=& E\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg] - \\theta\\\\\n&=& \\frac{n \\theta +2}{n+10} - \\theta = \\frac{n\\theta + 2 - n\\theta -10\\theta}{n+10}\\\\\n&=& \\frac{2-n\\theta}{n+10}\\\\\nvar(\\delta(\\underline{X})) &=& var\\bigg[\\frac{\\sum X_i+2}{n+10}\\bigg]\\\\\n&=& \\frac{1}{(n+10)^2}var\\bigg(\\sum X_i\\bigg)\\\\\n&=& \\frac{1}{(n+10)^2} n \\  var(X_i)\\\\\n&=& \\frac{n}{(n+10)^2} \\theta\\\\\n\\end{eqnarray*}\\]\\[\\begin{align*}\nMSE_F(\\delta(\\underline{X})) &= \\frac{n}{(n+10)^2} \\theta + \\frac{(2-n\\theta)^2}{(n+10)^2}\\\\\n&= \\frac{n \\theta+ (2-n\\theta)^2}{(n+10)^2}\n\\end{align*}\\]\nNote couldn’t directly compare \\(MSE_F\\) \\(MSE_B\\) (functions different variables!). ’d come prior think \\(MSE_B(\\hat{\\theta})\\), seems like can’t calculate quantity. Instead, take easier route, find \\(MSE_F(\\delta(\\underline{X}))\\) order reasonable comparison estimators.","code":""},{"path":"bayesian-estimation.html","id":"sensitivity-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.2 Sensitivity of Estimators","text":"sensitive results different priors?Example 2.10  Continuing tape example, Example 2.8, different values estimate theta depending different priors data values:Note: \\(E[ \\theta | \\underline{x} ] = \\frac{\\sum x_i + \\alpha}{ n + \\beta} = w_1 \\frac{\\sum x_i}{n} + w_2 \\frac{\\alpha}{\\beta}\\), \\(w_1 = \\frac{n}{n+\\beta}, w_2 = \\frac{\\beta}{n+\\beta}\\).\\\n\\(n \\rightarrow \\infty, \\hat{\\theta} \\rightarrow \\frac{\\sum x_i}{n}\\), \\(n \\rightarrow 0, \\hat{\\theta} \\rightarrow \\frac{\\alpha}{\\beta}\\).","code":""},{"path":"bayesian-estimation.html","id":"consistency-of-estimators","chapter":"2 Bayesian Estimation","heading":"2.7.3 Consistency of Estimators","text":"consistent estimator \\(\\theta\\) one converges probability \\(\\theta\\). Many Bayes estimators consistent. fact, fairly general regularity conditions, wide class Bayes estimators consistent.Note, estimator \\(Y_n\\) converges \\(\\theta\\) probability :\n\\[\\begin{eqnarray*}\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | < \\epsilon ] &=& 1 \\ \\ \\ \\ \\ \\mbox{pg 233}\\\\\n\\mbox{, equivalently}\\\\\n\\lim_{n \\rightarrow \\infty} P [ | Y_n - \\theta | \\geq \\epsilon ] &=& 0\\\\\n\\end{eqnarray*}\\](saw idea weak strong laws large numbers: \\(\\overline{X} \\stackrel{P}{\\rightarrow} \\mu\\) \\(n \\rightarrow \\infty\\) Weak Law Large Numbers.) [n.b. case curious, strong law large numbers says \\(\\overline{X} \\stackrel{.s.}{\\rightarrow} \\mu\\) (almost surely). means \\(\\lim_{n \\rightarrow \\infty} P [ \\overline{X} = \\mu ] = 1\\)Example 2.11  Continuing tape example, Example 2.8:\n\\[\\begin{eqnarray*}\n\\delta^*(\\underline{X}) &=& \\frac{\\sum X_i + \\alpha}{n+\\beta}\\\\\n\\overline{X} &\\stackrel{P}{\\rightarrow}& \\theta \\mbox{ (WLLN)}\\\\\n\\mbox{} && \\\\\n\\delta^*(\\underline{X}) - \\overline{X} &=& \\frac{- \\beta}{n+\\beta} \\overline{X} + \\frac{\\beta}{n+\\beta} \\frac{\\alpha}{\\beta} \\stackrel{P}{\\rightarrow} 0 \\mbox{ (Slutsky's theorem)}\\\\\n&& \\\\\n\\delta^*(\\underline{X}) &\\stackrel{P}{\\rightarrow} \\theta\\\\\n\\end{eqnarray*}\\]\\(\\delta^*(\\underline{X})\\) consistent estimator \\(\\theta\\).","code":""},{"path":"bayesian-estimation.html","id":"benefits-and-limitations-of-bayes-estimators","chapter":"2 Bayesian Estimation","heading":"2.8 Benefits and Limitations of Bayes’ Estimators","text":"","code":""},{"path":"bayesian-estimation.html","id":"benefits","chapter":"2 Bayesian Estimation","heading":"2.8.1 Benefits","text":"can incorporate informationthe interpretation intuitive","code":""},{"path":"bayesian-estimation.html","id":"limitations","chapter":"2 Bayesian Estimation","heading":"2.8.2 Limitations","text":"need prior informationit can difficult produce prior two parameters simultaneously (e.g., normal, gamma)need agree prior information","code":""},{"path":"bayesian-estimation.html","id":"additional-examples","chapter":"2 Bayesian Estimation","heading":"2.9 Additional Examples","text":"Example 2.12  Suppose Beta(4,4) prior distribution probability \\(\\theta\\) coin yield head spun specified manner. coin independently spun ten times, heads appears fewer 3 times. told many heads seen, number less 3. Calculate exact posterior density \\(\\theta\\).3Example 2.13  Baseball Bayes4You statistician employed Ball Consulting. Veteran major-league baseball scout Rocky Chew seeks advice regarding estimating probability amateur baseball player John Spurrier get base hit major-league pitcher. Rocky arranged Spurrier ten bats major-league pitcher.traditional batting average, \\(\\hat{\\theta}_f = X/n\\) frequentist estimator makes use observed data, ignores prior information might exist.5 assume bats independent Bernoulli trials constant probability getting base hit, \\[\\begin{eqnarray*}\nX \\sim Bin( n=\\mbox{number bat}, \\theta=\\mbox{P(getting base hit)})\n\\end{eqnarray*}\\]\\(\\hat{\\theta}_f\\), good estimator unknown probability (getting base hit), ignores information might baseball. following prior information:John Spurrier appears good great player. one better batters somewhat -average American Legion (high school) baseball team.major-league scouts watched play believe Spurrier’s batting ability professional level.barely adequate major-league hitter batting average 0.200.good major-league batter batting average 0.300.Ty Cobb -time best major-league batting average 0.366.’re going use Beta prior incorporate previous knowledge. prior look like?John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).Example 2.14  Kidney Cancer rates6 example, ’re going use Bayes theory adjust kidney cancer rates less variability. First, ’d like investigate counties highest lowest kidney cancer death rates US (white men, 1980-1989).patterns see figures 2.3 & 2.4? Can give plausible reasons patterns see?county 100 people? Small counties variable. Keep mind rates age-adjusted.\nFigure 2.1: Figure 2.3 Teaching Statistics, bag tricks Gelman Nolan.\nConsider figure 13.4, highest 10% Bayes-estimated kidney cancer death rates US (white men, 1980-1989). Let’s assume number deaths distributed Poisson(\\(n_j \\theta_j\\)) \\(n_j\\) number people county, \\(\\theta_j\\) true kidney cancer death rate county. , assume outside influences kidney cancer (e.g., pollution) county’s cancer rate comes Gamma distribution parameters (\\(\\alpha = 61, \\beta = 47000\\)). ,\\[\\begin{eqnarray*}\n\\mbox{Likelihood:} && y_j \\sim \\mbox{ Poisson}(n_j \\theta_j) \\ \\ \\ n_j = \\mbox{ county population}\\\\\n\\mbox{Prior:} && \\theta_j \\sim \\mbox{ Gamma}(\\alpha=61, \\beta = 47000)\\\\\n&& E[\\theta_j] = \\alpha / \\beta = 1.296 \\times 10^{-3} \\ \\ \\ \\ (\\mbox{10 yr cancer rate})\\\\\n\\end{eqnarray*}\\]\nknow \\(E[\\theta | y] = \\frac{\\alpha + y}{m + \\beta}\\). estimate compare frequentist estimate, \\(\\hat{\\theta} = \\frac{y}{m}\\)?investigate relationship Bayes estimate versus frequentist estimate, ’re going simulate kidney cancer death rates variety counties.Everyone gets county (population). county, ’ll generate true, underlying, kidney cancer rate \\(\\theta_j\\). (Note, \\(\\theta_j\\) sampled (simulated) Gamma(\\(\\alpha=61, \\beta=47,000\\)) distribution.)Using cancer rate (\\(\\theta_j\\)) county’s population (\\(n_j\\)), simulate value number people county died kidney cancer last 10 years (Poisson(\\(n_j \\theta_j\\))).Report frequentist estimate kidney cancer rate county (erase / hide true cancer rate). Leave county name, population, number deaths, estimated rate.public officials left task guessing counties highest cancer rates… think?Calculate Bayes estimate cancer rate. Compare underlying (\\(\\theta_j\\)), observed / frequentist (\\(\\frac{y_j}{n_j}\\)), posterior / Bayesian (\\(\\hat{\\theta}_j | y_j\\)) kidney cancer death rates.\nFigure 2.2: Figure 2.4 Teaching Statistics, bag tricks Gelman Nolan.\n","code":""},{"path":"bayesian-estimation.html","id":"the-experiment","chapter":"2 Bayesian Estimation","heading":"The Experiment","text":"John Spurrier n=10 bats. random variable, \\(X\\), number base hits gets.Determining prior probability: class find \\(\\alpha\\) \\(\\beta\\) consistent prior information.Collecting data: let’s calculate estimates possible realizations random variable.Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Comparison estimators:\n\\[ \\ \\ \\ \\ \\hat{\\theta}_f = \\frac{x}{n} \\ \\ \\ \\ \\ \\ \\ \\hat{\\theta}_b = \\frac{x + \\alpha}{ n + \\alpha + \\beta}\\]\nevaluate two estimators, might use Mean Squared Error (MSE) frequentist sense (, \\(X\\) random variable, \\(\\theta\\) longer random) compare estimators (apples apples):\n\\[\\begin{eqnarray*}\nMSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + bias^2(\\hat{\\theta}) = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2\n\\end{eqnarray*}\\]Problems:Problems:choices \\(\\alpha\\) \\(\\beta\\)? features plot prior density function made think good choices?Use properties expectation X, find bias (=\\(E[\\hat{\\theta}] - \\theta\\)) variance (=Var(\\(\\hat{\\theta}\\))) \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\).recommend using \\(\\hat{\\theta}_f\\) \\(\\hat{\\theta}_b\\)? Explain.John Spurrier gets three hits ten bats, estimate \\(\\theta\\)?Show \\(\\hat{\\theta}_b\\) weighted average \\(\\hat{\\theta}_f\\) prior mean, \\(\\frac{\\alpha}{\\alpha + \\beta}\\).","code":""},{"path":"bayesian-estimation.html","id":"reflection-questions-1","chapter":"2 Bayesian Estimation","heading":"2.10  Reflection Questions","text":"","code":""},{"path":"bayesian-estimation.html","id":"ethics-considerations-1","chapter":"2 Bayesian Estimation","heading":"2.11  Ethics Considerations","text":"","code":""},{"path":"bayesian-estimation.html","id":"r-code-bayesian-examples","chapter":"2 Bayesian Estimation","heading":"2.12 R code: Bayesian Examples","text":"Example 2.15  Recall Baseball Bayes example, Example 2.13.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
