<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Fisher Information | Statistical Theory</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="Fisher Information is a method for measuring the amount of information a sample of data contains about an unknown parameter. We are going to use Fisher Information to approximate the variance of...">
<meta name="generator" content="bookdown 0.26 with bs4_book()">
<meta property="og:title" content="Chapter 7 Fisher Information | Statistical Theory">
<meta property="og:type" content="book">
<meta property="og:description" content="Fisher Information is a method for measuring the amount of information a sample of data contains about an unknown parameter. We are going to use Fisher Information to approximate the variance of...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 7 Fisher Information | Statistical Theory">
<meta name="twitter:description" content="Fisher Information is a method for measuring the amount of information a sample of data contains about an unknown parameter. We are going to use Fisher Information to approximate the variance of...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.0/transition.js"></script><script src="libs/bs3compat-0.4.0/tabs.js"></script><script src="libs/bs3compat-0.4.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Statistical Theory</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="bayes.html"><span class="header-section-number">2</span> Bayesian Estimation</a></li>
<li><a class="" href="MLE.html"><span class="header-section-number">3</span> Maximum Likelihood Estimation</a></li>
<li><a class="" href="sampdist.html"><span class="header-section-number">4</span> Sampling Distributions of Estimators</a></li>
<li><a class="" href="bootdist.html"><span class="header-section-number">5</span> Bootstrap Distributions</a></li>
<li><a class="" href="intest.html"><span class="header-section-number">6</span> Interval Estimates</a></li>
<li><a class="active" href="fisher.html"><span class="header-section-number">7</span> Fisher Information</a></li>
<li><a class="" href="ht.html"><span class="header-section-number">8</span> Hypothesis Testing</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="fisher" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Fisher Information<a class="anchor" aria-label="anchor" href="#fisher"><i class="fas fa-link"></i></a>
</h1>
<p>Fisher Information is a method for measuring the amount of information a sample of data contains about an unknown parameter. We are going to use Fisher Information to approximate the variance of estimators obtained from large samples (specifically, it will give us info about the variance of MLEs). Note, we’re back to thinking like Frequentists (the hint is that Information is given about the MLEs).</p>
<p>Note: Fisher information measures the curvature of the likelihood. If the likelihood is flat (with respect to the parameter), the estimate of the maximum of the likelihood (MLE) can be quite off. If the likelihood is steep with respect to the parameter (2nd derivative small, asymptotic variance small), the estimate of the maximum (MLE) will be good.</p>
<div id="fisher-information-in-one-observation" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Fisher Information in one observation<a class="anchor" aria-label="anchor" href="#fisher-information-in-one-observation"><i class="fas fa-link"></i></a>
</h2>
<div id="some-intuition-describing-the-second-derivative-of-the-likelihood" class="section level3 unnumbered">
<h3>Some intuition describing the second derivative of the likelihood<a class="anchor" aria-label="anchor" href="#some-intuition-describing-the-second-derivative-of-the-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>Fisher Information describes the curvature of a likelihood function, that is, the negative of the expected value of the second derivative. Why the second derivative? Let’s look at some examples.</p>
<p><strong>Important:</strong> all the plots below / derivatives / intuition is in thinking about the likelihood as a function of <span class="math inline">\(\theta\)</span>!! (Not a function of the data.)</p>
<p>Consider a simplified example with only a single observation from an exponential distribution with mean <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[f(x | \theta) = (1/\theta) e^{-x/\theta}\]</span></p>
<p>Recall that the maximum likelihood estimator is the value of the parameter the maximizes the likelihood. That is, in plotting the likelihood, find the value of <span class="math inline">\(\theta\)</span> (on the x-axis) that gives the highest likelihood (on the y-axis).</p>
<p><span class="math display">\[\mbox{MLE} = \frac{\sum_i X_i}{n} \ \ \ \ \mbox{if } n=1, \mbox{MLE} = X\]</span></p>
<p>The task for today is to consider how certain we are about the estimate. When <span class="math inline">\(X=2\)</span> the likelihood is extremely peaked, and the maximum value appears somewhat obvious. When <span class="math inline">\(X=10\)</span> it is still possible to maximize the function, but the process seems somewhat less certain to give the “best” value of <span class="math inline">\(\theta\)</span>.</p>
<p><img src="07-fisherinfo_files/figure-html/unnamed-chunk-1-1.png" width="672"><img src="07-fisherinfo_files/figure-html/unnamed-chunk-1-2.png" width="672"><img src="07-fisherinfo_files/figure-html/unnamed-chunk-1-3.png" width="672"></p>
<p>The information itself is given by the expected value of the second derivative. For each value of X, there is a particular shape to the second derivative (some more peaked, some less peaked). The expected value will average over the values of the second derivative, giving weights as defined by the pdf (because that’s what expected value does).</p>
</div>
<div id="mathematical-formulation-of-the-second-derivative-of-the-likelihood" class="section level3 unnumbered">
<h3>Mathematical formulation of the second derivative of the likelihood<a class="anchor" aria-label="anchor" href="#mathematical-formulation-of-the-second-derivative-of-the-likelihood"><i class="fas fa-link"></i></a>
</h3>
<p>Define the following (note, they are all just functions of x):
<span class="math display">\[\begin{eqnarray*}
\lambda(x | \theta) &amp;=&amp; \ln f(x | \theta) \\
\lambda ' (x | \theta) &amp;=&amp; \frac{\partial}{\partial \theta} \ln f(x | \theta) \\
\lambda '' (x | \theta) &amp;=&amp; \frac{\partial^2}{\partial \theta^2} \ln f(x | \theta) \\
\end{eqnarray*}\]</span></p>
<p><strong>Fisher Information:</strong>
<span class="math display" id="eq:1FI">\[\begin{align}
I (\theta) = E \{ [ \lambda ' (X | \theta) ] ^2 \} \tag{7.1}
\end{align}\]</span></p>
<p>Now the math gets a little messy…</p>
<p><span class="math display">\[\begin{eqnarray*}
I(\theta) &amp;=&amp; \int [ \lambda '(x | \theta) ] ^2 f(x | \theta) dx\\
\mbox{because} &amp;&amp; \int f(x| \theta) dx = 1\\
\mbox{then} &amp;&amp; \int f'(x | \theta) dx = 0\\
\mbox{and} &amp;&amp; \int f''(x | \theta) dx = 0\\
\end{eqnarray*}\]</span></p>
<p>We can now construct two alternative forms for Fisher Information:
<span class="math display" id="eq:2FI">\[\begin{align}
\lambda '(x| \theta) &amp;= \frac{\partial}{\partial \theta} ln (f(x | \theta)) = \frac{f'(x | \theta)}{f(x| \theta)} \nonumber \\
E[ \lambda '(X | \theta) ] &amp;= \int \frac{f'(x | \theta)}{f(x|\theta)} f(x|\theta) dx \nonumber\\
&amp;= \int f'(x | \theta) dx = 0 \nonumber \\
I(\theta) &amp;= E \{ [\lambda'(X | \theta) ]^2 \} = var(\lambda'(X | \theta)) \tag{7.2}
\end{align}\]</span>
(Because the mean of <span class="math inline">\(\lambda'(x | \theta)\)</span> is zero.)</p>
<p><span class="math display" id="eq:3FI">\[\begin{align}
\lambda''(x|\theta) &amp;= \frac{f(x|\theta) f''(x|\theta) - [ f'(x|\theta) ]^2}{ [f(x|\theta)]^2}  \mbox{  (chain rule)} \nonumber\\
&amp;= \frac{f''(x|\theta)}{f(x|\theta)} - [ \lambda'(x | \theta) ]^2 \nonumber\\
E[\lambda''(X|\theta)] &amp;= \int f''(x|\theta) dx - E \{ [\lambda'(x|\theta)]^2 \} \nonumber\\
I(\theta) &amp;= - E [ \lambda''(X | \theta) ] \tag{7.3}
\end{align}\]</span></p>
<p>The last of the equations <a href="fisher.html#eq:3FI">(7.3)</a> is often the easiest to compute.</p>
<div class="example">
<p><span id="exm:unlabeled-div-39" class="example"><strong>Example 7.1  </strong></span>Suppose <span class="math inline">\(X\sim\)</span>Poisson(<span class="math inline">\(\theta\)</span>). <span class="math inline">\(f(x|\theta) = \frac{e^{-\theta} \theta^x}{x!}\)</span>. Find the Fisher Information in <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[\begin{eqnarray*}
\lambda(x | \theta) &amp;=&amp; \ln f(x|\theta) = -\theta + x \ln \theta - \ln(x!)\\
\lambda ' (x | \theta) &amp;=&amp; -1 + x/\theta\\
\lambda '' (x | \theta) &amp;=&amp; -x / \theta^2\\
\end{eqnarray*}\]</span></p>
<p>
By <a href="fisher.html#eq:3FI">(7.3)</a>:
<span class="math display">\[\begin{eqnarray*}
I(\theta) &amp;=&amp; - E [ \lambda '' (X|\theta) ] = - E[ -X / \theta^2]\\
&amp;=&amp; \theta / \theta^2 = 1/\theta
\end{eqnarray*}\]</span></p>
<p>
By <a href="fisher.html#eq:2FI">(7.2)</a>:
<span class="math display">\[\begin{eqnarray*}
I(\theta) &amp;=&amp; var(\lambda '(X|\theta)) = var(-1 + X/\theta)\\
&amp;=&amp; \frac{1}{\theta^2} var(X) = \theta / \theta^2 = 1/\theta
\end{eqnarray*}\]</span></p>
<p>
By <a href="fisher.html#eq:1FI">(7.1)</a>:
<span class="math display">\[\begin{eqnarray*}
I(\theta) &amp;=&amp; E \{ [ \lambda'(X | \theta) ]^2 \}\\
&amp;=&amp; E[ 1 - 2X /\theta + X^2 / \theta^2\\
&amp;=&amp; 1 - \frac{2}{\theta}E[X] + \frac{1}{ \theta^2} E(X^2)\\
&amp;=&amp; 1 - \frac{2}{\theta} \theta + \frac{1}{ \theta^2} (var(X) + E[X]^2)\\
&amp;=&amp; 1 - 2 +\frac{1}{\theta^2}(\theta + \theta^2) = 1/\theta
\end{eqnarray*}\]</span></p>
</div>
<p>Notice that the information about <span class="math inline">\(\theta\)</span> in <span class="math inline">\(X\)</span> depends on <span class="math inline">\(\theta\)</span>. This isn’t always true.</p>
</div>
</div>
<div id="fisher-information-in-a-random-sample" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Fisher Information in a random sample<a class="anchor" aria-label="anchor" href="#fisher-information-in-a-random-sample"><i class="fas fa-link"></i></a>
</h2>
<p>It seems, as though, we should be asking ourselves how much information there is about a parameter in a joint likelihood (not in a single sample). We define the Fisher Information on a random sample as:
<span class="math display">\[\begin{eqnarray*}
I_n (\theta) &amp;=&amp; E \{ [ \lambda ' (\underline{X} | \theta) ]^2 \}\\
&amp;=&amp; var [ \lambda ' (\underline{X} | \theta) ] \\
&amp;=&amp; - E [ \lambda '' (\underline{X} | \theta) ] \\
\end{eqnarray*}\]</span></p>
<p>To find the log likelihood of a joint distribution (based on a random sample):
<span class="math display">\[\begin{eqnarray*}
f(\underline{x} | \theta) &amp;=&amp; \prod_i f(x_i | \theta) \mbox{ (iid)}\\
\lambda(\underline{x} | \theta) &amp;=&amp; \sum_i \lambda(x_i | \theta) \\
\lambda'(\underline{x} | \theta) &amp;=&amp; \sum_i \lambda'(x_i | \theta) \\
\lambda''(\underline{x} | \theta) &amp;=&amp; \sum_i \lambda''(x_i | \theta) \\
I_n (\theta) &amp;=&amp; - E[\lambda''(\underline{X} | \theta) ]\\
&amp;=&amp; - E[ \sum \lambda''(X_i | \theta) ]\\
&amp;=&amp; \sum -E[\lambda''(X_i | \theta)]\\
&amp;=&amp; n I(\theta) \ \ !!!
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-40" class="example"><strong>Example 7.2  </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \sim N(\theta,\theta); E[X] = Var[X] = \theta\)</span>, independently. Find the Fisher Information for <span class="math inline">\(\theta\)</span> in the random sample.</p>
<!-- This problem takes a while.  They get pretty stuck with the derivatives.  Maybe a hint?  Or doing some of the work for them????-->
<p><span class="math display">\[\begin{eqnarray*}
f(x|\theta) &amp;=&amp; \frac{1}{\sqrt{2 \pi \theta}} exp \{ - (x-\theta)^2 / 2\theta \}\\
\lambda(x | \theta) &amp;=&amp; \frac{-1}{2} \ln 2\pi - \frac{1}{2}\ln \theta - \frac{(x-\theta)^2}{2\theta}\\
&amp;=&amp; \frac{-1}{2} \ln 2\pi - \frac{1}{2}\ln \theta - \frac{x^2}{2\theta} + \frac{2x\theta}{2\theta} - \frac{\theta^2}{2\theta}\\
\lambda ' (x | \theta) &amp;=&amp; \frac{-1}{2 \theta} + \frac{x^2}{2 \theta^2} + 0 -\frac{1}{2}\\
\lambda '' (x| \theta) &amp;=&amp; \frac{1}{2 \theta^2} - \frac{2x^2}{2\theta^3}\\
I(\theta) &amp;=&amp; -E[ \lambda''(X|\theta) ] \\
&amp;=&amp; \frac{-1}{2\theta^2} + \frac{E[X^2]}{ \theta^3}\\
&amp;=&amp; \frac{-1}{2\theta^2} + \frac{(\theta + \theta^2)}{\theta^3}\\
&amp;=&amp; \frac{1+2\theta}{2 \theta^2}\\
I_n(\theta) &amp;=&amp; \frac{n(1+2\theta)}{2 \theta^2}
\end{eqnarray*}\]</span></p>
<p>Note, by differentiating the log-likelihood (and using the quadratic formula), you can find that the MLE is:
<span class="math display">\[\begin{eqnarray*}
\hat{\theta} = \frac{-1 + \sqrt{1+4 \frac{\sum x_i^2}{n}}}{2}
\end{eqnarray*}\]</span>
Using the CRLB and the theorems in the book, we can show that the MLE has an approximate normal distribution centered around <span class="math inline">\(\theta\)</span> with variance given above by the Fisher Information.</p>
</div>
</div>
<div id="cramér-rao-lower-bound" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Cramér-Rao Lower Bound<a class="anchor" aria-label="anchor" href="#cram%C3%A9r-rao-lower-bound"><i class="fas fa-link"></i></a>
</h2>
<p><strong>Cramér-Rao Lower Bound</strong> By using the characteristics of <span class="math inline">\(\lambda, \lambda',\)</span> and <span class="math inline">\(\lambda''\)</span>, we can show that for any estimator of <span class="math inline">\(\theta\)</span> (see page 519 in <span class="citation">DeGroot and Schervish (<a href="references.html#ref-degroot" role="doc-biblioref">2011</a>)</span>; note regularity conditions on page 514):
<span class="math display">\[\begin{eqnarray*}
T &amp;=&amp; r(X_1, X_2, \ldots, X_n) = r(\underline{X}) \mbox{ such that}\\
E[T] &amp;=&amp; m(\theta)\\
\mbox{then: } var(T) &amp;\geq&amp; \frac{[m'(\theta)]^2}{nI(\theta)}
\end{eqnarray*}\]</span>
Note, <span class="math inline">\(m'(\theta) = \partial m(\theta) / \partial(\theta)\)</span>. If <span class="math inline">\(m(\theta) = \theta\)</span>, then <span class="math inline">\(m'(\theta) = 1\)</span> and var<span class="math inline">\((T) \geq 1/nI(\theta)\)</span>.</p>
<p><strong>Proof</strong> Note: <span class="math inline">\(\lambda'(\underline{x} | \theta) = f'(\underline{x} | \theta) / f(\underline{x} | \theta)\)</span></p>
<p>Also, recall that: <span class="math inline">\(Cov(X, Y) = E(XY) - E(X)E(Y) = E[(X - E(X))(Y - E(Y))]\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{we showed    } \ \ \ \ 0 = E[\lambda'(\underline{X} | \theta)] &amp;=&amp; \int_S \cdots \int_S f'(\underline{x} | \theta) dx_1 \ldots dx_n.\\
\mbox{therefore    } \ \ \ \ \mbox{Cov}[T, \lambda'(\underline{X} | \theta) &amp;=&amp; E[T \lambda'(\underline{X} | \theta)]\\
&amp;=&amp; \int_S \cdots \int_S r(\underline{x}) \lambda'(\underline{x} | \theta) f(\underline{x} | \theta) dx_1 \ldots dx_n.\\
&amp;=&amp; \int_S \cdots \int_S r(\underline{x}) f'(\underline{x} | \theta) dx_1 \ldots dx_n.\\
\mbox{we also know    } \ \ \ \ m(\theta) &amp;=&amp; \int_S \cdots \int_S r(\underline{x}) f(\underline{x} | \theta) dx_1 \ldots dx_n.\\
\mbox{derivative wrt $\theta$    } \ \ \ \  m'(\theta) &amp;=&amp; \int_S \cdots \int_S r(\underline{x}) f'(\underline{x} | \theta) dx_1 \ldots dx_n.\\
\mbox{Therefore    } \ \ \ \  \mbox{Cov}[T, \lambda'(\underline{X} | \theta) ]&amp;=&amp; m'(\theta)\\
\mbox{Cauchy-Schwarz     } \ \ \ \  [\mbox{Cov}[T, \lambda'(\underline{X} | \theta) ]^2 &amp;\leq&amp; Var(T) Var(\lambda'(\underline{X} | \theta)]\\
Var(T) &amp;\geq&amp; \frac{(m'(\theta))^2}{nI(\theta)}
\end{eqnarray*}\]</span></p>
<p>The equality holds iff: <span class="math inline">\(aT + b\lambda'(\underline{X} | \theta) = c\)</span>. That is, if T is a linear combination of <span class="math inline">\(\lambda'\)</span>, where the only <span class="math inline">\(X\)</span> is in the <span class="math inline">\(\lambda'\)</span>. [The iff is a part of the proof of the Cauchy-Schwarz inequality.] Because of the equality term in the C-S, we know that the minimum variance estimator with expected value <span class="math inline">\(m(\theta)\)</span> is <span class="math display">\[ T = u(\theta) \lambda'(\underline{X}|\theta) + v(\theta).\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-41" class="example"><strong>Example 7.3  </strong></span>Back to the example, <span class="math inline">\(N(\theta,\theta)\)</span>. We found that the MLE is: <span class="math inline">\(r(\underline{x}) = \frac{-1 + \sqrt{1+4 \frac{\sum x_i^2}{n}}}{2}\)</span>. And <span class="math inline">\(I(\theta) = \frac{(1+2\theta)}{2 \theta^2}\)</span>.</p>
<p>We need to find <span class="math inline">\(m(\theta)\)</span>, but meanwhile, we know from the proof above that
<span class="math display">\[ Var(r(\underline{X})) \geq \frac{(m'(\theta))^2 2 \theta^2}{n (1+2\theta)}\]</span></p>
</div>
</div>
<div id="efficiency" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Efficiency<a class="anchor" aria-label="anchor" href="#efficiency"><i class="fas fa-link"></i></a>
</h2>
<p>If var(<span class="math inline">\(T) = \frac{[m'(\theta)]^2}{nI(\theta)}\)</span>, we say that <span class="math inline">\(T\)</span> is efficient for <span class="math inline">\(m(\theta)\)</span>. If <span class="math inline">\(T\)</span> is both efficient and unbiased, we say it is the Uniformly Minimum Variance Unbiased Estimator (UMVUE).</p>
<div class="example">
<p><span id="exm:unlabeled-div-42" class="example"><strong>Example 7.4  </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_n \sim Poisson(\theta)\)</span>. Let <span class="math inline">\(T=\overline{X}\)</span> estimate <span class="math inline">\(\theta\)</span>.
<span class="math display">\[\begin{eqnarray*}
m(\theta) &amp;=&amp; E[T] = \theta\\
var(T) &amp;\geq&amp; \frac{1}{nI(\theta)} = \frac{1}{n/\theta} = \frac{\theta}{n}\\
\end{eqnarray*}\]</span></p>
<p>But we know that var(<span class="math inline">\(\overline{X}) = \frac{1}{n}\)</span> var(<span class="math inline">\(X_i) = \frac{\theta}{n}\)</span>!. <span class="math inline">\(T\)</span> is an efficient estimator of <span class="math inline">\(\theta\)</span>. <span class="math inline">\(T\)</span> is the uniformly minimum variance unbiased estimator of <span class="math inline">\(\theta\)</span>. Note, <span class="math inline">\(T\)</span> is also unbiased, does <span class="math inline">\(T\)</span> have to be unbiased to be efficient? Our result above says that <span class="math inline">\(T=\overline{X}\)</span> has the smallest variance of <strong>any</strong> unbiased estimator of <span class="math inline">\(\theta\)</span>. Why?</p>
</div>
<div id="asymptotic-distribution-of-the-mle" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> Asymptotic distribution of the MLE<a class="anchor" aria-label="anchor" href="#asymptotic-distribution-of-the-mle"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be the MLE for <span class="math inline">\(\theta\)</span> in a random sample of size n. [Not true for every case, but if <span class="math inline">\(T = u(\theta) \lambda'(\underline{X} | \theta) + v(\theta)\)</span>, we can use the CLT because <span class="math inline">\(\lambda'(\underline{x}|\theta) = \sum_{i=1}^n \lambda'(x_i | \theta)\)</span>.] The asymptotic distribution of:
<span class="math display">\[\begin{eqnarray*}
[nI(\theta) ]^{1/2} (\hat{\theta} - \theta) &amp;\mbox{is}&amp; N(0,1)\\
\mbox{we say } [nI(\theta) ]^{1/2} (\hat{\theta} - \theta) &amp;\stackrel{D}{\rightarrow}&amp; N(0,1)
\end{eqnarray*}\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-43" class="example"><strong>Example 7.5  </strong></span>In the Poisson example, we know <span class="math inline">\(\overline{X}\)</span> is the MLE for <span class="math inline">\(\theta\)</span>. Which gives:
<span class="math display">\[\begin{eqnarray*}
(n\frac{1}{\theta})^{1/2} [ \overline{X} - \theta ] &amp;\stackrel{D}{\rightarrow}&amp; N(0,1)\\
\frac{\overline{X} - \theta}{\sqrt{\theta/n}} &amp;\stackrel{D}{\rightarrow}&amp; N(0,1)
\end{eqnarray*}\]</span></p>
<p>
Well, we knew that through the CLT as well…</p>
</div>
</div>
</div>
<div id="fisher-information-and-bayesian-estimators" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Fisher Information and Bayesian Estimators<a class="anchor" aria-label="anchor" href="#fisher-information-and-bayesian-estimators"><i class="fas fa-link"></i></a>
</h2>
<p>In the Bayesian setting, it can be shown that <span class="math inline">\(\theta | \underline{X}\)</span> is asymptotically normal with mean <span class="math inline">\(\hat{\theta}\)</span> = MLE and variance <span class="math inline">\(\frac{1}{nI(\hat{\theta})}\)</span>. Note that there is no <span class="math inline">\(\theta\)</span> in the mean or variance because <span class="math inline">\(\theta\)</span> is now our random quantity, and <span class="math inline">\(\hat{\theta}\)</span> is our fixed part. In large samples, we swamp the prior information:
<span class="math display">\[\begin{eqnarray*}
[nI(\hat{\theta})]^{1/2} (\theta - \hat{\theta}) | \underline{X} \stackrel{D}{\rightarrow} N(0,1)
\end{eqnarray*}\]</span></p>
<p>Again, notice that neither <span class="math inline">\(\hat{\theta}\)</span> nor <span class="math inline">\(I(\hat{\theta})\)</span> have any <span class="math inline">\(\theta\)</span> values in them.</p>
<div class="example">
<p><span id="exm:unlabeled-div-44" class="example"><strong>Example 7.6  </strong></span>Back to the example, <span class="math inline">\(N(\theta,\theta)\)</span>. We found that the MLE is: <span class="math inline">\(\hat{\theta} = \frac{-1 + \sqrt{1+4 \frac{\sum x_i^2}{n}}}{2}\)</span>. And <span class="math inline">\(I(\theta) = \frac{(1+2\theta)}{2 \theta^2}\)</span>.</p>
<p><span class="math display">\[\Bigg[\frac{n(1+2\hat{\theta})}{2 \hat{\theta}^2}\Bigg]^{1/2} (\theta - \hat{\theta}) | \underline{X} \stackrel{D}{\rightarrow} N(0,1)\]</span></p>
</div>
<!--
### Jefferys' Prior

(We won't cover Jefferys' Prior, but it's pretty cool, and it's fun to think about how Bayesian and Frequentist methods come together.)


Jefferys' prior gives an additional uninformative prior when approaching a problem as a Bayesian.  Consider the information gotten from the mean ($\mu$) and variance ($\sigma^2$) in a $N(\mu, \sigma^2)$ problem.

\begin{eqnarray*}
f(\underline{x} | \mu, \sigma^2) &=& \bigg( \frac{1}{\sqrt{2 \pi \sigma^2}} \bigg)^2 e^{\frac{-\sum(x_i - \mu)^2}{2\sigma^2}} \\
I(\mu) &=& \frac{n}{\sigma^2}\\
I(\sigma^2) &=& \frac{n}{2 \sigma^4}
\end{eqnarray*}

Notice that the information in $\mu$ does not depend on $\mu$, but the information in $\sigma^2$ does depend on $\sigma^2$.  In many cases, it is desirable for the prior to be constant with respect to the parameter of interest.  That is, we can specify a flat (i.e., non-informative) prior on $\mu$ by using the Information (because the information does not depend on $\mu$).  Indeed, we use $$\xi_J(\mu) \propto \sqrt{I(\mu)} \propto k$$ to be Jefferys' prior.

However, as above, the information in $\sigma^2$ is not constant with respect to $\sigma^2$.  That means for different values of $\sigma^2$, the information in the likelihood varies.  Recall, information has to do with the second derivative of the likelihood and how peaked it is for estimating the parameter of interest.  We can find a transformation on $\sigma^2$ ($=g(\sigma^2)$ )such that the information contained in the likelihood about $g(\sigma^2)$ IS constant with respect to $g(\sigma^2)$.  In fact, $$I(\ln(\sigma^2)) \propto k$$ where k is a constant that does not depend on $\sigma^2$.

Which is to say, given the likelihood, we believe that there should be a flat prior on $\ln(\sigma^2)$!  It turns out that if we put a flat prior on $\ln(\sigma^2)$, we need a prior on $\sigma^2$ to be $$\xi_J(\sigma^2) \propto \sqrt{I(\sigma^2)} \propto \frac{1}{\sigma^2}.$$

The Jefferys prior is an uninformative prior that uses only the likelihood.
\begin{eqnarray*}
X &\sim& f(x| \theta)\\
\xi_J(\theta) &\propto& I(\theta)^{1/2}
\end{eqnarray*}

Let $\phi = g(\theta)$, so that $\theta = g^{-1} (\phi)$.

\begin{eqnarray*}
\xi_J(\phi) &=& \xi_J(\theta) \cdot \bigg| \frac{\partial \theta}{\partial \phi} \bigg|\\
&=& \sqrt{I(\theta) \bigg(\frac{\partial \theta}{\partial \phi} \bigg)^2}\\
&=& \sqrt{E \bigg[ \bigg(\frac{\partial \lambda(X | \theta)}{\partial \theta} \bigg)^2 \bigg] \bigg(\frac{\partial \theta}{\partial \phi} \bigg)^2}\\
&=& \sqrt{E \bigg(\frac{\partial \lambda(X | \theta)}{\partial \theta} \frac{\partial \theta}{\partial \phi} \bigg)^2}\\
&=& \sqrt{E \bigg(\frac{\partial \lambda(X | \theta)}{\partial \phi}  \bigg)^2}\\
&=& \sqrt{E \bigg(\frac{\partial \lambda(X | \phi)}{\partial \phi}  \bigg)^2}\\
&=& \sqrt{I(\phi)}
\end{eqnarray*}

Jefferys prior is invariant to a change in variable.

%\fi

%\iffalse
Historically, flat uninformative priors don't stay uninformative under transformations.  Consider the following example:

\begin{eqnarray*}
X \sim Bin(n, \theta) \ \ \ \ \ \ \xi(\theta) &=& 1  \ \ \ \ \ \  \rho = \ln(\frac{\theta}{1-\theta})\\
F_\Theta(\theta) &=& P(\Theta \leq \theta) = \theta\\
F_\rho(\rho) &=& P(\rho \leq \rho) = P(\ln (\frac{\Theta}{1-\Theta}) \leq \rho)\\
&=&  P(\frac{\Theta}{1-\Theta} \leq e^\rho)= P(\Theta \leq \frac{e^\rho}{1+e^\rho})\\
&=& \frac{e^\rho}{1+e^\rho}\\
\xi(\rho) &=& \frac{e^\rho}{1+e^\rho} \frac{\partial}{\partial \rho} \frac{e^\rho}{1+e^\rho}\\
&=& \frac{e^{2\rho}}{(1+e^\rho)^3} \ \ \ \ \ \ -\infty < \rho < \infty
\end{eqnarray*}
The prior is uniform in one parametrization and very informative in another parametrization.  The problem gets worse in higher dimension (e.g., multivariate Gaussian estimation of the $p$-variate mean vector $\underline{\mu}$).\\
-->
</div>
<div id="giants-in-the-field" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Giants in the Field<a class="anchor" aria-label="anchor" href="#giants-in-the-field"><i class="fas fa-link"></i></a>
</h2>
<p>Who came up with all this stuff? And how long ago were the ideas derived? Answer: some really awesome statisticians who were working on it very recently. Rao is still alive and just celebrated his 102nd birthday.</p>
<div id="david-blackwell-the-rao-blackwell-theorem" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">7.6.1</span> David Blackwell: The Rao-Blackwell Theorem<a class="anchor" aria-label="anchor" href="#david-blackwell-the-rao-blackwell-theorem"><i class="fas fa-link"></i></a>
</h3>
<p>While not beyond the scope of this class, we will not cover the <a href="https://en.wikipedia.org/wiki/Rao-Blackwell_theorem" target="_blank">Rao-Blackwell theorem</a> due to time constraints. Broadly, a <em>sufficient</em> statistic is one that is made up of the same function of the data as the summary function of the data given in the likelihood (think: sum of <span class="math inline">\(X_i\)</span> or sum of <span class="math inline">\(X_i^2\)</span> in the Gaussian likelihood). The Rao-Blackwell theorem says that conditioning on the sufficient statistic creates an estimator with minimum mean squared error. The result is not only quite important (we all want estimators with small mean squared error!), but it is also mathematically beautiful. The proof of the theorem is given in your book in Section 7.9, Theorem 7.9.1.</p>
<p>I provide information on the Rao-Blackwell Theorem in the class notes to highlight one of the giants in statistics: <a href="https://en.wikipedia.org/wiki/David_Blackwell" target="_blank">David Blackwell</a> (1919-2010). Blackwell was the first Black person to receive a PhD in statistics in the US and the first Black scholar to be admitted to the National Academy of Sciences. He was a statistician at UC Berkeley for more than 50 years. He was hired in 1954 after the department almost made him an offer in 1942 (but declined to do so when one faculty member’s wife said she didn’t want Blackwell hired because she wouldn’t feel comfortable having faculty events in her home with a Black man). Hear Blackwell <a href="https://www.youtube.com/watch?v=Mqpf9tw44Xw" target="_blank">tell the story in his own words</a>. Also, read more about Blackwell and other <a href="https://www.mathad.com/profile?bidg2518292537111213526biii=" target="_blank">Mathematicians of the African Diaspora.</a></p>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="figs/blackwell.png" alt="David Blackwell. Image credit: https://math.illinois.edu/david-blackwell" width="80%"><p class="caption">
Figure 1.2: David Blackwell. Image credit: <a href="https://math.illinois.edu/david-blackwell" class="uri">https://math.illinois.edu/david-blackwell</a>
</p>
</div>
</div>
<div id="c.r.-rao-cramér-rao-lower-bound" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">7.6.2</span> C.R. Rao: Cramér-Rao Lower Bound<a class="anchor" aria-label="anchor" href="#c.r.-rao-cram%C3%A9r-rao-lower-bound"><i class="fas fa-link"></i></a>
</h3>
<p>The Cramér-Rao Lower Bound (seen previously in these notes) and the Rao-Blackwell Theorem were both results with contributions from C.R. Rao who is an Indian-American statistician, originally from Karnataka, India. Rao just celebrated his <span class="math inline">\(102^{nd}\)</span> birthday (b. Sep 10, 1920), and he is well known to be among the greatest statisticians of all time. Indeed, he was very much a student of Fisher, working with likelihoods and focusing on inferential claims. Rao was known to be genial and kind. In a recent article (by Brad Efron, the creator of the boostrap) celebrating his centennial birthday <span class="citation">(<a href="references.html#ref-rao" role="doc-biblioref">Efron et al. 2020</a>)</span> Rao is reported to have written his statistical results in order…</p>
<blockquote>
<p>… to communicate to others with great clarity.</p>
</blockquote>
<blockquote>
<p>I do not remember when I first met Rao in person, but I think it was when I was visiting a colleague at the Penn State Department of Statistics. Rao was clearly the “heart” of that department, surrounded by many talented individuals, but his presence created its vitality and helped to ensure its stature among US departments of statistics.</p>
</blockquote>
<blockquote>
<p>Despite his dominant reputation, Rao always seemed to be extremely modest and, moreover, helpful to younger colleagues.</p>
</blockquote>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="figs/rao.jpeg" alt="C.R. Rao. Image credit: https://pib.gov.in/PressReleseDetail.aspx?PRID=1653064" width="60%"><p class="caption">
Figure 1.3: C.R. Rao. Image credit: <a href="https://pib.gov.in/PressReleseDetail.aspx?PRID=1653064" class="uri">https://pib.gov.in/PressReleseDetail.aspx?PRID=1653064</a>
</p>
</div>
</div>
</div>
<div id="reflection-questions-6" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> <i class="fas fa-lightbulb" target="_blank"></i> Reflection Questions<a class="anchor" aria-label="anchor" href="#reflection-questions-6"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Does an estimator need to be unbiased in order to be efficient?</li>
<li>If the estimator is unbiased, what variance would that estimator have in order to have the smallest variance of the class of all unbiased estimators?</li>
<li>How can the asymptotic distribution of the MLE be written down in terms of the CRLB?</li>
<li>How can the asymptotic distribution of the Bayes estimator be written down in terms of the CRLB?</li>
<li>What are the main differences between the MLE and the Bayes estimator?</li>
<li>Why would one use the MLE result as compared to the Bayes result?</li>
</ol>
</div>
<div id="ethics-considerations-6" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> <i class="fas fa-balance-scale"></i> Ethics Considerations<a class="anchor" aria-label="anchor" href="#ethics-considerations-6"><i class="fas fa-link"></i></a>
</h2>
<ol style="list-style-type: decimal">
<li>Why does it matter <strong>who</strong> does mathematics and statistics?</li>
<li>Is there only one definition of “best” (in terms of an estimator)?</li>
<li>What version of “best” estimator does the CRLB speak to?</li>
</ol>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intest.html"><span class="header-section-number">6</span> Interval Estimates</a></div>
<div class="next"><a href="ht.html"><span class="header-section-number">8</span> Hypothesis Testing</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#fisher"><span class="header-section-number">7</span> Fisher Information</a></li>
<li>
<a class="nav-link" href="#fisher-information-in-one-observation"><span class="header-section-number">7.1</span> Fisher Information in one observation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#some-intuition-describing-the-second-derivative-of-the-likelihood">Some intuition describing the second derivative of the likelihood</a></li>
<li><a class="nav-link" href="#mathematical-formulation-of-the-second-derivative-of-the-likelihood">Mathematical formulation of the second derivative of the likelihood</a></li>
</ul>
</li>
<li><a class="nav-link" href="#fisher-information-in-a-random-sample"><span class="header-section-number">7.2</span> Fisher Information in a random sample</a></li>
<li><a class="nav-link" href="#cram%C3%A9r-rao-lower-bound"><span class="header-section-number">7.3</span> Cramér-Rao Lower Bound</a></li>
<li>
<a class="nav-link" href="#efficiency"><span class="header-section-number">7.4</span> Efficiency</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#asymptotic-distribution-of-the-mle"><span class="header-section-number">7.4.1</span> Asymptotic distribution of the MLE</a></li></ul>
</li>
<li><a class="nav-link" href="#fisher-information-and-bayesian-estimators"><span class="header-section-number">7.5</span> Fisher Information and Bayesian Estimators</a></li>
<li>
<a class="nav-link" href="#giants-in-the-field"><span class="header-section-number">7.6</span> Giants in the Field</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#david-blackwell-the-rao-blackwell-theorem"><span class="header-section-number">7.6.1</span> David Blackwell: The Rao-Blackwell Theorem</a></li>
<li><a class="nav-link" href="#c.r.-rao-cram%C3%A9r-rao-lower-bound"><span class="header-section-number">7.6.2</span> C.R. Rao: Cramér-Rao Lower Bound</a></li>
</ul>
</li>
<li><a class="nav-link" href="#reflection-questions-6"><span class="header-section-number">7.7</span>  Reflection Questions</a></li>
<li><a class="nav-link" href="#ethics-considerations-6"><span class="header-section-number">7.8</span>  Ethics Considerations</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/07-fisherinfo.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/07-fisherinfo.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Statistical Theory</strong>" was written by Jo Hardin. It was last built on 2022-11-10.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
