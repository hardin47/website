# Simple Linear Regression {#SLR}

##### Agenda {-}
1. sampling distributions
2. transformations
3. confidence / prediction intervals 


Though we've discussed the relationship between tests of means and simple linear regression, we will really consider simple linear regression in a much broader context (one where both the explanatory and response variables are quantitative).

The data below represents 10 different variables on health of a country measured on 143 countries.  Data taken from [@Lock5], originally from the Happy Planet Index Project [http://www.happyplanetindex.org/].  Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries.  We are going to investigate happiness and life expectancy.  

## Transformations

### Model assumptions {-}

\begin{itemize}
\item
The average value for the response variable is a linear function of the explanatory variable.
\item
The error terms follow a normal distribution around the linear model.
\item
The error terms have a mean of zero.
\item
The error terms have a constant variance of $\sigma^2$.
\item
The error terms are independent (and identically distributed).
\item
[http://www.rossmanchance.com/applets/RegSim/RegCoeff.html]
\end{itemize}

How do we tell whether the assumptions are met?  We can't always.  But it's good to look at plots: scatter plot, residual plot, histograms of residuals.  We denote the residuals for this model as:

\begin{eqnarray}
r_i = \hat{e}_i = y_i - \hat{y}_i
\end{eqnarray}


![Figs 3.13 and 3.15 taken from @kutner](transfor.jpg)


**important note!!**  The idea behind transformations is to make the model as appropriate as possible for the data at hand.  We want to find the correct **linear** model; we want our assumptions to hold.  We are not trying to find the most *significant* model or big $R^2$.


See section 2.9 in your text.  No normal probability plots (qq-plots); use histograms or boxplots to assess the symmetry and normality of the residuals.


## Fitting the regression line
How do we fit a regression line?  Find $b_0$ and $b_1$ that minimize the sum of squared distance of the points to the line (called ordinary least squares):

\begin{eqnarray}
\min \sum (y_i \hat{y}_i)^2 &=& \min RSS \mbox{ residual sum of squares}\\
RSS &=& \sum (y_i - b_0 - b_1 x_i)^2\\
\frac{\partial RSS}{\partial b_0} = 0\\
\frac{\partial RSS}{\partial b_1} = 0\\
b_0 &=& \overline{y} - b_1 \overline{x}\\
b_1 &=& r(x,y) \frac{s_x}{s_y}\\
\end{eqnarray}
\begin{itemize}
\item
Is that the only way to find values for $b_0$ and $b_1$? (absolute distances, maximum likelihood,...)
\item
Resistance to outliers?
\item
What is $\hat{y}$ at $\overline{x}$?
\begin{eqnarray}
\hat{y} &=& b_0 + b_1 \overline{x}\\
&=& \overline{y} - b_1 \overline{x} + b_1 \overline{x}\\
&=& \overline{y}
\end{eqnarray}
The regression line will always pass through the point $(\overline{x}, \overline{y})$.
\end{itemize}

\begin{defn}
An estimate is *unbiased* if, over many repeated samples drawn from the population, the average value of the estimates based on the different samples would equal the population value of the parameter being estimated.  That is, a statistic is unbiased if the mean of its sampling distribution is the population parameter.
\end{defn}


## Correlation

Consider a scatterplot, you'll have variability in both directions:  $(x_i - \overline{x}) \& (y_i - \overline{y})$.

\begin{eqnarray}
\mbox{sample covariance}&&\\
cov(x,y) &=& \frac{1}{n-1}\sum (x_i - \overline{x}) (y_i - \overline{y})\\
\mbox{sample correlation}&&\\
r(x,y) &=& \frac{cov(x,y)}{s_x s_y}\\
&=& \frac{\frac{1}{n-1} \sum (x_i - \overline{x}) (y_i - \overline{y})}{\sqrt{\frac{\sum(x_i - \overline{x})^2}{n-1} \frac{\sum(y_i - \overline{y})^2}{n-1}}}\\
\mbox{pop cov} &=& \sigma_{xy}\\
\mbox{pop cor} &=& \rho = \frac{\sigma_{xy}}{\sigma_x \sigma_y}\\
\end{eqnarray}

\begin{itemize}
\item
$-1 \leq r \leq 1 \& -1 \leq \rho \leq 1$.
\item
No Spearman's rank correlation or Kendall's $\tau$.
\item
$b_1 = r \frac{s_y}{s_x}$
\begin{itemize}
\item
if $r=0, b_1=0$
\item
if $r=1, b_1 > 0$ but can be anything!
\item
$r < 0 \leftrightarrow b < 0, r > 0 \leftrightarrow b > 0$
\item
Recall that $R^2$ is the proportion of variability explained by the line (see below).\end{itemize}
\end{itemize}


## Errors
Recall, $\epsilon_i \sim N(0, \sigma^2)$.  How do we estimate $\sigma^2$?

\begin{eqnarray}
RSS &=& \sum (y_i - \hat{y}_i)^2 \ \ \ \mbox{ residual sum of squares}\\
MSS &=& \sum (\hat{y}_i - \overline{y})^2 \ \ \ \mbox{ model sum of squares}\\
TSS &=& \sum (y_i - \overline{y})^2 \ \ \ \mbox{ total sum of squares}\\
s_{y|x}^2 &=& \hat{\sigma^2} = \frac{1}{n-2} RSS\\
s_x^2 &=& \frac{1}{n-1} \sum (x_i - \overline{x})^2\\
s_y^2 &=& \frac{1}{n-1} \sum (y_i - \overline{y})^2\\
var(\epsilon) &=& s_{y|x}^2 = \frac{RSS}{n-2} = \frac{\sum(y_i - \hat{y}_i)^2}{n-2} = SE(\epsilon)\\
var(b_1) &=& \frac{s_{y|x}^2}{(n-1) s_x^2}\\
SE(b_1) &=& \frac{s_{y|x}}{\sqrt{(n-1)} s_x}\\
&=& \frac{\hat{\sigma}}{\sqrt{\sum(x_i - \overline{x})^2}} = \frac{\sqrt{\sum(y_i - \hat{y}_i)^2/(n-2)}}{\sqrt{\sum(x_i - \overline{x})^2}}\\
\end{eqnarray}
\begin{itemize}
\item
$SE(b_1) \downarrow$ as $\sigma \downarrow$
\item
$SE(b_1) \downarrow$ as $n \uparrow$
\item
$SE(b_1) \downarrow$ as $s_x \uparrow$
\item
WHY?
\item
What do we mean by $SE(b_1)$?
\end{itemize}

As we saw above, the correlation and the slope estimates are intimately related.  They are also both related to the *coefficient of determination*.
\begin{eqnarray}
R^2 = r^2 = \frac{MSS}{TSS}
\end{eqnarray}
$R^2$ is the proportion of total variability explained by the regression line (the linear relationship between the explanatory and response variables).
\begin{itemize}
\item
If $x$ and $y$ are not at all correlated, $\hat{y}_i \approx \overline{y}$, MSS = 0, $R^2=0$.
\item
If $x$ and $y$ are perfectly correlated, $\hat{y}_i = y_i$, MSS=TSS, $R^2 = 1$.
\end{itemize}


### Testing $\beta_1$
If $H_0: \beta=0$ is true, then
\begin{eqnarray}
\frac{b_1 - 0}{SE(b_1)} \sim t_{n-2}
\end{eqnarray}
Note that the degrees of freedom are now $n-2$ because we are estimating two parameters ($\beta_0$ and $\beta_1$).  We can also find a $(1-\alpha)100\%$ confidence interval for $\beta_1$:
\begin{eqnarray}
b_1 \pm t_{\alpha/2, n-2} SE(b_1)
\end{eqnarray}

## Intervals

As with anything that has some type of standard error, we can create intervals that give us some confidence in the statements we are making.

### Confidence Intervals

In general, confidence intervals are of the form:

```
point estimate +/- multiplier * SE(point estimate)
```

### Slope

We can create a CI for the slope parameter, $\beta_1$:
\begin{eqnarray}
b_1 &\pm& t_{\alpha/2,n-2} SE(b_1)\\
b_1 &\pm& t_{\alpha/2, n-2} \frac{s_{y|x}}{\sqrt{(n-1)}s_x}\\
6.693 &\pm& t_{.025, 141} 0.375\\
t_{.025,141} &=& qt(0.025, 141) = -1.977\\
\mbox{CI} && (5.95 \mbox{ years/unit of happy}, 7.43 \mbox{ years/unit of happy})
\end{eqnarray}
How can we interpret the CI?  Does it make sense to talk about a unit of happiness?

### Mean Response
We can also create a CI for the mean response, $E[Y|x^*] = \beta_0 + \beta_1 x^*$.  Note that the standard error of the point estimate ($\hat{y}=b_0 + b_1 x^*$) now depends on the variability associated with two things ($b_0, b_1$).
\begin{eqnarray}
SE(\hat{y(x^*)}) &=& \sqrt{ \frac{s^2_{y|x}}{n} + (x^* - \overline{x})^2 SE(b_1)^2}\\
SE(\hat{y}(\overline{x})) &=& s_{y|x}/\sqrt{n}\\
SE(\hat{y}(x)) &\geq& s_{y|x}/\sqrt{n} \ \ \ \forall x
\end{eqnarray}
How would you interpret the associated interval?


### Prediction of an Individual Response

As should be obvious, predicting an individual is more variable than predicting a mean.

\begin{eqnarray}
SE(y(x^*)) &=& \sqrt{ \frac{s^2_{y|x}}{n} + (x^* - \overline{x})^2 SE(b_1)^2 + s^2_{y|x}}\\
SE(y(x^*)) &=& \sqrt{ SE(\hat{y}(x^*))^2 + s^2_{y|x}}\\
\end{eqnarray}
How would you interpret the associated interval?


### Outlying, High Leverage, and Influential Points

**We are skipping the rest of this section in the notes.  You are not responsible for it.**

Read section 4.7 (no loess, ignore the multiple predictors part, )


\begin{defn}
*High leverage points* are x-outliers with the potential to exert undue influence on regression coefficient estimates.  *Influential points* are points that have exerted undue influence on the regression coefficient estimates.
\end{defn}


Note: typically we think of more data as better; more values will tend to decrease the sampling variability of our statistic.  But if I give you a lot more data and put it all at $\overline{x}$, $SE(b_1)$ stays exactly the same.  Why??

Recall
\begin{eqnarray}
y_{i} &=& \beta_0 + \beta_1 x_i \ \ \ \epsilon_i \sim N(0,\sigma^2)\\
e_i &=& y_i - \hat{y}_i
\end{eqnarray}

We plot $e_i$ versus $\hat{y}_i$.  (Why?  Typically, we want the $e_i$ to be constant at each value of $x_i$.  Note that $\hat{y}_i$ is a simple linear transformation of $x_i$, so the plot is identical.)  We want to see if the distributions of the residuals is different across the fitted line (we look for patterns).\\


**Not all residuals have an equal effect on the regression line!!**

#### leverage
\begin{eqnarray}
h_i = \frac{1}{n} +\frac{(x_i - \overline{x})^2}{\sum_{j=1}^n (x_j - \overline{x})^2}\\
\frac{1}{n} \leq h_i \leq 1\\
\end{eqnarray}
Leverage represents the effect of point $x_i$ on the line.  We need large leverage for a particular value to have a large effect.

Note:
\begin{eqnarray}
SE(\hat{y}(x_i)) &=& s_{y|x} \sqrt{h_i}\\
SE(y(x_i)) &=& s_{y|x} \sqrt{(h_i + 1)}\\
SE(e_i) &=& s_{y|x} \sqrt{(1-h_i)}\\
\hat{y}(x^*) &\pm& t_{n-2, .025} (s_{y|x} \sqrt{h(x^*)+1})\\
\end{eqnarray}
is a 95\% prediction interval at $x^*$.  High leverage reduces the variability because the line gets pulled toward the point.

#### standardized residuals

\begin{eqnarray}
\frac{e_i}{s_{y|x} \sqrt{1-h_i}} \sim t_{n-2}\\
\end{eqnarray}

#### studentized residuals

\begin{eqnarray}
\frac{e_i}{s_{y|x, (i)} \sqrt{1-h_i}} &\sim& t_{n-3}\\
s_{y|x, (i)} &=& \frac{1}{n-3} \sum_{j \ne i} (y_j - \hat{y}_{j(i)})^2
\end{eqnarray}

Where do we predict 90\% of residuals?  $\pm t_{n-2,3 , .05}$.  About $\pm 2$.

#### DFBETAs

DFBETAs represent the change in the parameter estimate due to one observation.

\begin{eqnarray}
DFBETAS_i &=& \frac{b_1 - b_{1(i)}}{\frac{s_{y|x, (i)}}{\sqrt{(n-1)} s_x}}\\
\end{eqnarray}




## R Example (SLR): Happy Planet  
  

The data below represents 10 different variables on health of a country measured on 143 countries.  Data taken from [@Lock5], originally from the Happy Planet Index Project [http://www.happyplanetindex.org/].  Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries.  We are going to investigate happiness and life expectancy.  

```{r warning=FALSE, message=FALSE, include=FALSE}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

### Reading the data into R
```{r warning=FALSE, message=FALSE}
happy <- read_delim("~/Dropbox/teaching/math150/spring17/happyPlanet.txt", delim="\t")
glimpse(happy)  
```


### Running the linear model (lm)

```{r}
happy.lm = lm(LifeExpectancy ~ Happiness, data=happy) 

happy.lm %>% tidy()
```

### Ouptut

Some analyses will need the residuals, fitted values, or coefficients individually.
```{r}
happy.lm %>% augment()
```


We can plot the main relationship, or we can plot the residuals (to check that technical conditions hold):
```
ggplot(happy, aes(x=Happiness, y=LifeExpectancy)) + geom_point() + 
         geom_smooth(method="lm", se=FALSE) 
happy.lm %>% augment %>% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + 
         geom_hline(yintercept=0)
```

```{r fig.height=3, echo=FALSE}
p1 <- ggplot(happy, aes(x=Happiness, y=LifeExpectancy)) + geom_point() + 
         geom_smooth(method="lm", se=FALSE) 
p2 <- happy.lm %>% augment %>% ggplot(aes(x = .fitted, y = .resid)) + geom_point() +
         geom_hline(yintercept=0)
multiplot(p1, p2, cols=2)
```


Intervals of interest: mean response, individual response, and parameter(s).
```{r}
predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c("conf"), level=.95)
predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c("pred"), level=.95)

happy.lm %>% tidy(conf.int = TRUE)
```
  
  
#### Residuals in R

We skipped the residuals section, so you are not responsible for finding residuals in R, but the R code is here for completion in case you are interested:
  

```{r}
happy.lm %>% augment()
```









