[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 150 Pomona College: Methods Biostatistics. notes based primarily text Practicing Statistics, (Kuiper Sklar 2013).responsible reading text. text good & readable, use . text , however, overly technical. make sure coming class asking lots questions.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"\nFigure 1.1: reject null hypothesis based ‘hot damn, check chart’ test. https://xkcd.com/2400/\n","code":""},{"path":"intro.html","id":"course-goals","chapter":"1 Introduction","heading":"1.1 Course Goals","text":"goals course :better evaluate quantitative information regards clinical biological data. ’ll sure keep mind:\nCareful presentation data\nConsideration variability\nMeaningful comparisons\nCareful presentation dataConsideration variabilityMeaningful comparisonsto able critically evaluate medical literature respect design, analysis, interpretation results.understand role inherent variability keep perspective inferring results population.critically evaluate medical results given mainstream media.read published studies skepticism. people (fields!) wrongly believe studies published peer review publication must 100% accurate /well designed studies. course, learn tools recognize, interpret, critique statistical results medical literature.\nFigure 1.2: Probability vs. Statistics\n","code":""},{"path":"intro.html","id":"using-r","chapter":"1 Introduction","heading":"1.2 Using R","text":"Much work done R using RStudio front end. need either download R RStudio (free) onto computer use Pomona’s server.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.http://swirlstats.com/ great way walk learning basics R.http://swirlstats.com/ great way walk learning basics R.computing assignments turned using R Markdown compiled pdf.computing assignments turned using R Markdown compiled pdf.\nFigure 1.3: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.4: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"experimental-design","chapter":"1 Introduction","heading":"Experimental Design","text":"class ’ll talk techniques used analyze data medical studies. Along computational methods, however, ’ll continue think issues experimental design interpretation.Descriptive statistics describe sample hand intent making generalizations.Inferential statistics use sample make claims populationSimple Random Sample unbiased sample. Sample selected way every possible sample size \\(n\\) equally likely.Blind / double blind patient /doctor know patient receiving treatment.Placebo mock treatmentSample size reduces variability (large samples make small effects easier discern)Experiment vs. Observational Study whether treatment assigned researchers; randomized experiments make concluding causation possibleFunding study goals, bias","code":""},{"path":"t-tests-vs-slr.html","id":"t-tests-vs-slr","chapter":"2 t-tests vs SLR","heading":"2 t-tests vs SLR","text":"going build basic model following form:planned variability experimental conditions, hopefully represented interesting deterministic modelrandom error natural variability due individuals.systematic error error contained within model. can happen poor sampling poor experimental conditions.","code":"data = deterministic model + random error"},{"path":"t-tests-vs-slr.html","id":"surgery-timing","chapter":"2 t-tests vs SLR","heading":"Surgery Timing","text":"study, “Operation Timing 30-Day Mortality Elective General Surgery,” tested hypotheses risk 30-day mortality associated elective general surgery: 1) increases morning evening throughout routine workday; 2) increases Monday Friday workweek; 3) frequent July August months year. presumed negative control, investigators also evaluated mortality function phase moon. Secondarily, evaluated hypotheses pertain composite -hospital morbidity endpoint.related data set contains 32,001 elective general surgical patients. Age, gender, race, BMI, several comorbidities, several surgical risk indices, surgical timing predictors (hour, day week, month,moon phase) outcomes (30-day mortality -hospital complication) provided. dataset cleaned complete (missing data except BMI). outliers data problems. data (Sessler et al. 2011)Note example, mortality rates compared patients electing surgery July vs August. ’d like compare average age participants July group August group. Even mortality difference significant, can’t conclude causation observational study. However, similar groups based clinical variables, likely differences mortality due timing. different groups based clinical variables?\nTable 2.1: Varibles associated surgery data.\n","code":"\nsurgeryurl <- url(\"https://www.causeweb.org/tshs/datasets/surgery_timing.Rdata\")\nload(surgeryurl)\nsurgery <- stata_data\nhead(surgery)  %>%\n  select(age, gender, race, hour, dow, month, complication, bmi, everything(), -ahrq_ccs) %>%\n  kable(caption = \"Varibles associated with the surgery data.\") %>%\n kable_styling()\nsurgery %>%\n dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n dplyr::group_by(month) %>%\n dplyr::summarize(agemean = mean(age, na.rm=TRUE), agesd = sd(age, na.rm=TRUE), agen = sum(!is.na(age)))\n#> # A tibble: 2 x 4\n#>   month agemean agesd  agen\n#>   <chr>   <dbl> <dbl> <int>\n#> 1 Aug      58.1  15.2  3176\n#> 2 Jul      57.6  15.5  2325"},{"path":"t-tests-vs-slr.html","id":"ttest","chapter":"2 t-tests vs SLR","heading":"2.1 t-test","text":"(Section 2.1 Kuiper Sklar (2013).)t-test test means. surgery timing data, groups ideally similar age distributions. ? advantages disadvantages running retrospective cohort study?two-sample t-test starts assumption population means two groups equal, \\(H_0: \\mu_1 = \\mu_2\\). sample means \\(\\overline{y}_1\\) \\(\\overline{y}_2\\) always different. different must \\(\\overline{y}\\) values order reject null hypothesis?","code":""},{"path":"t-tests-vs-slr.html","id":"model-1","chapter":"2 t-tests vs SLR","heading":"Model 1:","text":"\\[\\begin{align}\ny_{1j} &= \\mu_{1} + \\epsilon_{1j} \\ \\ \\ \\ j=1, 2, \\ldots, n_1\\\\\ny_{2j} &= \\mu_{2} + \\epsilon_{2j} \\ \\ \\ \\ j=1, 2, \\ldots, n_2\\\\\n\\epsilon_{ij} &\\sim N(0,\\sigma^2)\\\\\nE[Y_i] &= \\mu_i\n\\end{align}\\], assuming group true population average fixed individual randomly selected amount random error away true population mean. Note assumed variances two groups equal. also assumed independence within groups.Note: assume population variances equal neither sample variance twice big .Example 2.1  mean ages July vs August patients statistically different? (two sided?)\\[\\begin{align}\nH_0: \\mu_1 = \\mu_2\\\\\nH_1: \\mu_1 \\ne \\mu_2\n\\end{align}\\]\\[\\begin{align}\nt &= \\frac{(\\overline{y}_1 - \\overline{y}_2) - 0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\\\\ns_p &= \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 -2}}\\\\\ndf &= n_1 + n_2 -2\\\\\n&\\\\\nt &= \\frac{(58.05 - 57.57) - 0}{15.34 \\sqrt{\\frac{1}{3176} + \\frac{1}{2325}}}\\\\\n&= 1.15\\\\  \ns_p &= \\sqrt{ \\frac{(3176-1)15.22^2 + (2325-1) 15.5^2}{3176 + 2325 -2}}\\\\\n&= 15.34\\\\\ndf &= n_1 + n_2 -2\\\\\n&= 5499\\\\\n\\mbox{p-value} &= 2 \\cdot (1-pt(1.15,5499)) = 0.25\\\\\n\\end{align}\\]analysis can done R (without tidying output):Look SD SEMWhat statistic? sampling distribution statistic?use t-distribution?big p-value important? (’s good thing!) interpret p-value?can conclude?applet (Chance Rossman 2018): [http://www.rossmanchance.com/applets/2021/sampling/OneSample.html]model assumptions? (basically assumptions given original linear model: independence & within groups, random sample, pop values don’t change, additive error, \\(\\epsilon_{,j} \\ \\sim \\ iid \\  N(0, \\sigma^2))\\)Considerations running t-test:one-sample vs two-sample t-testone-sided vs. two-sided hypothesest-test unequal variance (less powerful, conservative)\\[\\begin{align}\nt &= \\frac{(\\overline{y}_1 - \\overline{y}_2) - (\\mu_1 - \\mu_2)}{ \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\\\\ndf &= \\min(n_1-1, n_2-1)\\\\\n\\end{align}\\]two dependent (paired) samples – one sample t-test!Example 2.2  Assume two small samples: \\((y_{11}=3, y_{12} = 9, y_{21} = 5, y_{22}=1, y_{23}=9).\\) Find \\(\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\epsilon}_{11}, \\hat{\\epsilon}_{12}, \\hat{\\epsilon}_{21}, \\hat{\\epsilon}_{22}, \\hat{\\epsilon}_{23}, n_1, n_2\\).","code":"\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  age by month\n#> t = 1, df = 4954, p-value = 0.2\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.337  1.309\n#> sample estimates:\n#> mean in group Aug mean in group Jul \n#>              58.1              57.6\n\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 1 x 10\n#>   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n#>      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n#> 1    0.486      58.1      57.6      1.16   0.247     4954.   -0.337      1.31\n#> # … with 2 more variables: method <chr>, alternative <chr>"},{"path":"t-tests-vs-slr.html","id":"what-is-an-alternative-hypothesis","chapter":"2 t-tests vs SLR","heading":"2.1.1 What is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: [https://www.youtube.com/watch?v=b-U_I1DCGEY]video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? watch video? relate material class? relationship sampling distributions? [Thanks Ben Baumer Smith College pointer specific video.]","code":""},{"path":"t-tests-vs-slr.html","id":"anova","chapter":"2 t-tests vs SLR","heading":"ANOVA","text":"Skip ANOVA text (2.4 part 2.9 Kuiper Sklar (2013)).","code":""},{"path":"t-tests-vs-slr.html","id":"tslr","chapter":"2 t-tests vs SLR","heading":"2.2 Simple Linear Regression","text":"(Section 2.3 Kuiper Sklar (2013).)Simple Linear Regression model (hopefully discussed introductory statistics) used describing {linear} relationship two variables. typically form :\\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i  \\ \\ \\ \\ = 1, 2, \\ldots, n\\\\\n\\epsilon_i &\\sim N(0, \\sigma^2)\\\\\nE(Y|x) &= \\beta_0 + \\beta_1 x\n\\end{align}\\]model, deterministic component (\\(\\beta_0 + \\beta_1 x\\)) linear function two parameters, \\(\\beta_0\\) \\(\\beta_1\\), explanatory variable \\(x\\). random error terms, \\(\\epsilon_i\\), assumed independent follow normal distribution mean 0 variance \\(\\sigma^2\\).can use model describe two sample means case discussed esophageal data? Consider \\(x\\) dummy variable takes value 0 observation control 1 observation case. Assume \\(n_1\\) controls \\(n_2\\) cases. turns , coded way, regression model two-sample t-test model mathematically equivalent!(color game text, natural way code 1 color distracter 0 standard game. ? \\(\\beta_0\\) represent? \\(\\beta_1\\) represent?)\\[\\begin{align}\n\\mu_1 &= \\beta_0 + \\beta_1 (0) = \\beta_0 \\\\\n\\mu_2 &= \\beta_0 +  \\beta_1 (1) = \\beta_0 + \\beta_1\\\\\n\\mu_2 - \\mu_1 &= \\beta_1\n\\end{align}\\]","code":""},{"path":"t-tests-vs-slr.html","id":"why-are-they-the-same","chapter":"2 t-tests vs SLR","heading":"Why are they the same?","text":"\\[\\begin{align}\nb_1= \\hat{\\beta}_1 &= \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - (\\sum x_i )^2}\\\\\n&= \\frac{n \\sum_2 y_i - n_2 \\sum y_i}{(n n_2-n_2^2)}\\\\\n&= \\frac{ n \\sum_2 y_i - n_2 (\\sum_1 y_i + \\sum_2 y_i)}{n_2(n-n_2)}\\\\\n&= \\frac{(n_1 + n_2) \\sum_2 y_i - n_2 \\sum_1 y_i - n_2 \\sum_2 y_i}{n_1 n_2}\\\\\n&= \\frac{n_1 \\sum_2 y_i - n_2 \\sum_1 y_i}{n_1 n_2}\\\\\n&= \\frac{n_1 n_2 \\overline{y}_2 - n_2 n_1 \\overline{y}_1}{n_1 n_2}\\\\\n&= \\overline{y}_2 - \\overline{y}_1\\\\\nb_0 = \\hat{\\beta}_0 &= \\frac{\\sum y_i - b_1 \\sum x_i}{n}\\\\\n&= \\frac{\\sum_1 y_i + \\sum_2 y_i - b_1 n_2}{n}\\\\\n&= \\frac{n_1 \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\\n&= \\frac{n \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\\n&= \\frac{n \\overline{y}_1}{n} = \\overline{y}_1\n\\end{align}\\]","code":""},{"path":"t-tests-vs-slr.html","id":"model-2","chapter":"2 t-tests vs SLR","heading":"Model 2:","text":"\\[\\begin{align}\ny_{} &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ =1, 2, \\ldots, n\\\\\n\\epsilon_{} &\\sim N(0,\\sigma^2)\\\\\nE[Y_i] &= \\beta_0 + \\beta_1 x_i\\\\\n\\hat{y}_i &= b_0 + b_1 x_i\n\\end{align}\\], assuming observation true population average fixed individual randomly selected amount random error away true population mean value explanatory variable, \\(x_i\\). Note assumed variance constant across level explanatory variable. also assumed independence across individuals. [Note: assumptions distribution explanatory variable, \\(X\\)].Note similarity running t.test() linear model (lm()):similarities t-test vs. SLR models?\npredicting average\nassuming independent, constant errors\nerrors follow normal distribution zero mean variance \\(\\sigma^2\\)\npredicting averageassuming independent, constant errorserrors follow normal distribution zero mean variance \\(\\sigma^2\\)differences two models?\none subscript versus two (similarly, two models t-test)\ntwo samples t-test (two variables regression… similarity??)\nvariables quantitative SLR\none subscript versus two (similarly, two models t-test)two samples t-test (two variables regression… similarity??)variables quantitative SLR","code":"\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 1 x 10\n#>   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n#>      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n#> 1    0.486      58.1      57.6      1.16   0.247     4954.   -0.337      1.31\n#> # … with 2 more variables: method <chr>, alternative <chr>\n\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  lm(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   58.1       0.272    213.     0    \n#> 2 monthJul      -0.486     0.419     -1.16   0.245"},{"path":"t-tests-vs-slr.html","id":"confidence-intervals","chapter":"2 t-tests vs SLR","heading":"2.3 Confidence Intervals","text":"(Section 2.11 Kuiper Sklar (2013).)fantastic applet visualizing means 95% confidence: [http://www.rossmanchance.com/applets/2021/confsim/ConfSim.html]general, format confidence interval give … interpretation? Remember, interval given parameter “coverage” happens alternative universes repeated sampling. ’re 95% confident interval captures parameter.Age data:\n\\[\\begin{align}\n90\\% \\mbox{ CI } \\mu_1: & \\overline{y}_1 \\pm t^*_{3176-1} \\times \\hat{\\sigma}_{\\overline{y}_1}\\\\\n& 58.05 \\pm 1.645 \\times 15.22/\\sqrt{3176}\\\\\n& (57.61, 58.49)\\\\\n95\\% \\mbox{ CI }\\mu_1 - \\mu_2: & \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{5499} s_p \\sqrt{1/n_1 + 1/n_2}\\\\\n& 0.48 \\pm 1.96 \\times 0.42\\\\\n& (-0.34, 1.30)\n\\end{align}\\]Note CI pgs 54/55, typo. correct interval \\(\\mu_1 - \\mu_2\\) games data :\\[\\begin{align}\n95\\% \\mbox{ CI } \\mu_1 - \\mu_2: & \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} \\hat{\\sigma}_{\\overline{y}_1 - \\overline{y}_2}\\\\\n& \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} s_p \\sqrt{1/n_1 + 1/n_2}\\\\\n& 38.1 - 35.55 \\pm 2.02 \\times \\sqrt{\\frac{(19)3.65^2 + (19)3.39^2}{20+20-2}} \\sqrt{\\frac{1}{20} + \\frac{1}{20}}\\\\\n& (0.29 s, 4.81 s)\n\\end{align}\\]","code":"estimate +/- critical value x standard error of the estimate"},{"path":"t-tests-vs-slr.html","id":"random-sample-vs.-random-allocation","chapter":"2 t-tests vs SLR","heading":"2.4 Random Sample vs. Random allocation","text":"Recall ’ve learned good random samples lead inference population. hand, order make causal conclusion, need randomized experiment random allocation treatments (impossible happen many settings). Random sampling random allocation DIFFERENT ideas clear mind.\nFigure 2.1: Figure taken (Chance Rossman 2018)\nNote: ANOVA (section 2.4 Kuiper Sklar (2013)) normal probability plots (section 2.8 Kuiper Sklar (2013)).","code":""},{"path":"SLR.html","id":"SLR","chapter":"3 Simple Linear Regression","heading":"3 Simple Linear Regression","text":"Though ’ve discussed relationship tests means simple linear regression, really consider simple linear regression much broader context (one explanatory response variables quantitative).data represents 10 different variables health country measured 143 countries. Data taken (Lock et al. 2016), originally Happy Planet Index Project [http://www.happyplanetindex.org/]. Region world coded 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. going investigate happiness life expectancy.","code":""},{"path":"SLR.html","id":"inference-on-the-linear-model","chapter":"3 Simple Linear Regression","heading":"3.1 Inference on the Linear Model","text":"order make inferential claims linear regression model (e.g., p-values hypotheses coefficients, confidence intervals coefficients, confidence interval line, prediction interval points, …), set technical conditions provide mathematical structure leading t-procedures (e.g., t-test). course focused linear regression spend time discussing robust model various deviations following technical conditions. now, say sometimes transformations either explanatory response variables can effective way mitigate deviations model.measurement data / population, regression models built either statistics (Roman letters describe sample) parameters (Greek letters describe population). linear regression, one additional differentiation due whether observed values (\\(y_i\\)) average values (\\(\\hat{y}_i\\) \\(E[Y_i]\\)) modeled.\\[\\begin{eqnarray*}\nE[Y_i|x_i] &=& \\beta_0 + \\beta_1 x_i \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n&& \\epsilon_i = y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\n&& e_i = y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"SLR.html","id":"technical-conditions","chapter":"3 Simple Linear Regression","heading":"3.1.1 Technical Conditions","text":"average value response variable linear function explanatory variable.error terms follow normal distribution around linear model.error terms mean zero.error terms constant variance \\(\\sigma^2\\).error terms independent (identically distributed).[http://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm]tell whether assumptions met? can’t always. ’s good look plots: scatter plot, residual plot, histograms residuals. denote residuals model :\\[\\begin{align}\nr_i = \\hat{e}_i = y_i - \\hat{y}_i\n\\end{align}\\]\nFigure 1.2: Figs 3.13 3.15 taken Kutner et al. (2004).\nimportant note!! idea behind transformations make model appropriate possible data hand. want find correct linear model; want assumptions hold. trying find significant model big \\(R^2\\).See section 2.9 Kuiper Sklar (2013). normal probability plots (qq-plots); use histograms boxplots assess symmetry normality residuals.","code":""},{"path":"SLR.html","id":"fitting-the-regression-line","chapter":"3 Simple Linear Regression","heading":"3.2 Fitting the regression line","text":"fit regression line? Find \\(b_0\\) \\(b_1\\) minimize sum squared distance points line (called ordinary least squares):\\[\\begin{align}\n\\min \\sum (y_i \\hat{y}_i)^2 &= \\min RSS \\mbox{ residual sum squares}\\\\\nRSS &= \\sum (y_i - b_0 - b_1 x_i)^2\\\\\n\\frac{\\partial RSS}{\\partial b_0} &= 0\\\\\n\\frac{\\partial RSS}{\\partial b_1} &= 0\\\\\nb_0 &= \\overline{y} - b_1 \\overline{x}\\\\\nb_1 &= r(x,y) \\frac{s_x}{s_y}\\\\\n\\end{align}\\]way find values \\(b_0\\) \\(b_1\\)? (absolute distances, maximum likelihood,…)Resistance outliers?\\(\\hat{y}\\) \\(\\overline{x}\\)?\\[\\begin{align}\n\\hat{y} &= b_0 + b_1 \\overline{x}\\\\\n&= \\overline{y} - b_1 \\overline{x} + b_1 \\overline{x}\\\\\n&= \\overline{y}\n\\end{align}\\]regression line always pass point \\((\\overline{x}, \\overline{y})\\).Definition 3.1  estimate unbiased , many repeated samples drawn population, average value estimates based different samples equal population value parameter estimated. , statistic unbiased mean sampling distribution population parameter.","code":""},{"path":"SLR.html","id":"correlation","chapter":"3 Simple Linear Regression","heading":"3.3 Correlation","text":"Consider scatterplot, ’ll variability directions: \\((x_i - \\overline{x}) \\& (y_i - \\overline{y})\\).\\[\\begin{align}\n\\mbox{sample covariance}&\\\\\ncov(x,y) &= \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\\n\\mbox{sample correlation}&\\\\\nr(x,y) &= \\frac{cov(x,y)}{s_x s_y}\\\\\n&= \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\\n\\mbox{pop cov} &= \\sigma_{xy}\\\\\n\\mbox{pop cor} &= \\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\\\\n\\end{align}\\]\\(-1 \\leq r \\leq 1 \\ \\ \\ \\ \\ \\& \\ \\ \\ -1 \\leq \\rho \\leq 1\\).Spearman’s rank correlation Kendall’s \\(\\tau\\).\\(b_1 = r \\frac{s_y}{s_x}\\)\n\\(r=0, b_1=0\\)\n\\(r=1, b_1 > 0\\) can anything!\n\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)\n\\(r=0, b_1=0\\)\\(r=1, b_1 > 0\\) can anything!\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)Recall \\(R^2\\) proportion variability explained line.","code":""},{"path":"SLR.html","id":"errors-residuals","chapter":"3 Simple Linear Regression","heading":"3.4 Errors / Residuals","text":"Recall, part technical conditions required \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). estimate \\(\\sigma^2\\)?\\[\\begin{align}\nRSS &= \\sum (y_i - \\hat{y}_i)^2 \\ \\ \\ \\mbox{ residual sum squares}\\\\\nMSS &= \\sum (\\hat{y}_i - \\overline{y})^2 \\ \\ \\ \\mbox{ model sum squares}\\\\\nTSS &= \\sum (y_i - \\overline{y})^2 \\ \\ \\ \\mbox{ total sum squares}\\\\\ns_{y|x}^2 &= \\hat{\\sigma^2} = \\frac{1}{n-2} RSS\\\\\ns_x^2 &= \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2\\\\\ns_y^2 &= \\frac{1}{n-1} \\sum (y_i - \\overline{y})^2\\\\\nvar(\\epsilon) &= s_{y|x}^2 = \\frac{RSS}{n-2} = \\frac{\\sum(y_i - \\hat{y}_i)^2}{n-2} = SE(\\epsilon)\\\\\nvar(b_1) &= \\frac{s_{y|x}^2}{(n-1) s_x^2}\\\\\nSE(b_1) &= \\frac{s_{y|x}}{\\sqrt{(n-1)} s_x}\\\\\n&= \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(x_i - \\overline{x})^2}} = \\frac{\\sqrt{\\sum(y_i - \\hat{y}_i)^2/(n-2)}}{\\sqrt{\\sum(x_i - \\overline{x})^2}}\\\\\n\\end{align}\\]\\(SE(b_1) \\downarrow\\) \\(\\sigma \\downarrow\\)\\(SE(b_1) \\downarrow\\) \\(n \\uparrow\\)\\(SE(b_1) \\downarrow\\) \\(s_x \\uparrow\\)?mean \\(SE(b_1)\\)?saw , correlation slope estimates intimately related. also related coefficient determination.\n\\[\\begin{align}\nR^2 = r^2 = \\frac{MSS}{TSS}\n\\end{align}\\]\\(R^2\\) proportion total variability explained regression line (linear relationship explanatory response variables).\\(x\\) \\(y\\) correlated, \\(\\hat{y}_i \\approx \\overline{y}\\), MSS = 0, \\(R^2=0\\).\\(x\\) \\(y\\) perfectly correlated, \\(\\hat{y}_i = y_i\\), MSS=TSS, \\(R^2 = 1\\).","code":""},{"path":"SLR.html","id":"testing-beta_1","chapter":"3 Simple Linear Regression","heading":"3.4.1 Testing \\(\\beta_1\\)","text":"technical conditions hold, mathematics describing sampling distribution \\(b_1\\) well defined. :\\(H_0: \\beta=0\\) true, \n\\[\\begin{align}\n\\frac{b_1 - 0}{SE(b_1)} \\sim t_{n-2}\n\\end{align}\\]\nNote degrees freedom now \\(n-2\\) estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). can also find \\((1-\\alpha)100\\%\\) confidence interval \\(\\beta_1\\):\n\\[\\begin{align}\nb_1 \\pm t_{\\alpha/2, n-2} SE(b_1)\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"intervals","chapter":"3 Simple Linear Regression","heading":"3.5 Intervals","text":"anything type standard error, can create intervals give us confidence statements making.","code":""},{"path":"SLR.html","id":"confidence-intervals-1","chapter":"3 Simple Linear Regression","heading":"3.5.1 Confidence Intervals","text":"general, confidence intervals form:","code":"point estimate +/- multiplier * SE(point estimate)"},{"path":"SLR.html","id":"slope","chapter":"3 Simple Linear Regression","heading":"3.5.2 Slope","text":"can create CI slope parameter, \\(\\beta_1\\):\n\\[\\begin{align}\nb_1 &\\pm t_{\\alpha/2,n-2} SE(b_1)\\\\\nb_1 &\\pm t_{\\alpha/2, n-2} \\frac{s_{y|x}}{\\sqrt{(n-1)}s_x}\\\\\n6.693 &\\pm t_{.025, 141} 0.375\\\\\nt_{.025,141} &= qt(0.025, 141) = -1.977\\\\\n\\mbox{CI} & (5.95 \\mbox{ years/unit happy}, 7.43 \\mbox{ years/unit happy})\n\\end{align}\\]\ncan interpret CI? make sense talk unit happiness?","code":""},{"path":"SLR.html","id":"mean-response","chapter":"3 Simple Linear Regression","heading":"3.5.3 Mean Response","text":"can also create CI mean response, \\(E[Y|x^*] = \\beta_0 + \\beta_1 x^*\\). Note standard error point estimate (\\(\\hat{y}=b_0 + b_1 x^*\\)) now depends variability associated two things (\\(b_0, b_1\\)).\n\\[\\begin{align}\nSE(\\hat{y(x^*)}) &= \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2}\\\\\nSE(\\hat{y}(\\overline{x})) &= s_{y|x}/\\sqrt{n}\\\\\nSE(\\hat{y}(x)) &\\geq s_{y|x}/\\sqrt{n} \\ \\ \\ \\forall x\n\\end{align}\\]\ninterpret associated interval?","code":""},{"path":"SLR.html","id":"prediction-of-an-individual-response","chapter":"3 Simple Linear Regression","heading":"3.5.4 Prediction of an Individual Response","text":"obvious, predicting individual variable predicting mean.\\[\\begin{align}\nSE(y(x^*)) &= \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2 + s^2_{y|x}}\\\\\nSE(y(x^*)) &= \\sqrt{ SE(\\hat{y}(x^*))^2 + s^2_{y|x}}\\\\\n\\end{align}\\]\ninterpret associated interval?","code":""},{"path":"SLR.html","id":"infl","chapter":"3 Simple Linear Regression","heading":"3.6 Influential Points","text":"skipping Section 3.6; responsible .Theorem 3.1  High leverage points x-outliers potential exert undue influence regression coefficient estimates. Influential points points exerted undue influence regression coefficient estimates.Note: typically think data better; values tend decrease sampling variability statistic. give lot data put \\(\\overline{x}\\), \\(SE(b_1)\\) stays exactly . ??Recall\n\\[\\begin{align}\ny_{} &= \\beta_0 + \\beta_1 x_i \\ \\ \\ \\epsilon_i \\sim N(0,\\sigma^2)\\\\\ne_i &= y_i - \\hat{y}_i\n\\end{align}\\]plot \\(e_i\\) versus \\(\\hat{y}_i\\). (? Typically, want \\(e_i\\) constant value \\(x_i\\). Note \\(\\hat{y}_i\\) simple linear transformation \\(x_i\\), plot identical.) want see distributions residuals different across fitted line (look patterns).residuals equal effect regression line!!","code":""},{"path":"SLR.html","id":"leverage","chapter":"3 Simple Linear Regression","heading":"3.6.1 leverage","text":"\\[\\begin{align}\nh_i = \\frac{1}{n} +\\frac{(x_i - \\overline{x})^2}{\\sum_{j=1}^n (x_j - \\overline{x})^2}\\\\\n\\frac{1}{n} \\leq h_i \\leq 1\\\\\n\\end{align}\\]\nLeverage represents effect point \\(x_i\\) line. need large leverage particular value large effect.Note:\n\\[\\begin{align}\nSE(\\hat{y}(x_i)) &= s_{y|x} \\sqrt{h_i}\\\\\nSE(y(x_i)) &= s_{y|x} \\sqrt{(h_i + 1)}\\\\\nSE(e_i) &= s_{y|x} \\sqrt{(1-h_i)}\\\\\n\\hat{y}(x^*) &\\pm t_{n-2, .025} (s_{y|x} \\sqrt{h(x^*)+1})\\\\\n\\end{align}\\]\n95% prediction interval \\(x^*\\). High leverage reduces variability line gets pulled toward point.","code":""},{"path":"SLR.html","id":"standardized-residuals","chapter":"3 Simple Linear Regression","heading":"3.6.2 standardized residuals","text":"\\[\\begin{align}\n\\frac{e_i}{s_{y|x} \\sqrt{1-h_i}} \\sim t_{n-2}\\\\\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"studentized-residuals","chapter":"3 Simple Linear Regression","heading":"3.6.3 studentized residuals","text":"\\[\\begin{align}\n\\frac{e_i}{s_{y|x, ()} \\sqrt{1-h_i}} &\\sim t_{n-3}\\\\\ns_{y|x, ()} &= \\frac{1}{n-3} \\sum_{j \\ne } (y_j - \\hat{y}_{j()})^2\n\\end{align}\\]predict 90% residuals? \\(\\pm t_{n-2,3 , .05}\\). \\(\\pm 2\\).","code":""},{"path":"SLR.html","id":"dfbetas","chapter":"3 Simple Linear Regression","heading":"3.6.4 DFBETAs","text":"DFBETAs represent change parameter estimate due one observation.\\[\\begin{align}\nDFBETAS_i &= \\frac{b_1 - b_{1()}}{\\frac{s_{y|x, ()}}{\\sqrt{(n-1)} s_x}}\\\\\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"r-example-slr-happy-planet","chapter":"3 Simple Linear Regression","heading":"3.7 R Example (SLR): Happy Planet","text":"data represents 10 different variables health country measured 143 countries. Data taken (Lock et al. 2016), originally Happy Planet Index Project [http://www.happyplanetindex.org/]. Region world coded 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. going investigate happiness life expectancy.","code":""},{"path":"SLR.html","id":"reading-the-data-into-r","chapter":"3 Simple Linear Regression","heading":"3.7.1 Reading the data into R","text":"","code":"\nhappy <- read_delim(\"~/Dropbox/teaching/MA150/spring17/happyPlanet.txt\", delim=\"\\t\")\nglimpse(happy)  \n#> Rows: 143\n#> Columns: 11\n#> $ Country        <chr> \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", \"Armenia\",…\n#> $ Region         <dbl> 7, 3, 4, 1, 7, 2, 2, 7, 5, 7, 2, 1, 4, 5, 1, 7, 4, 1, 7…\n#> $ Happiness      <dbl> 5.5, 5.6, 4.3, 7.1, 5.0, 7.9, 7.8, 5.3, 5.3, 5.8, 7.6, …\n#> $ LifeExpectancy <dbl> 76.2, 71.7, 41.7, 74.8, 71.7, 80.9, 79.4, 67.1, 63.1, 6…\n#> $ Footprint      <dbl> 2.2, 1.7, 0.9, 2.5, 1.4, 7.8, 5.0, 2.2, 0.6, 3.9, 5.1, …\n#> $ HLY            <dbl> 41.7, 40.1, 17.8, 53.4, 36.1, 63.7, 61.9, 35.4, 33.1, 4…\n#> $ HPI            <dbl> 47.9, 51.2, 26.8, 59.0, 48.3, 36.6, 47.7, 41.2, 54.1, 3…\n#> $ HPIRank        <dbl> 54, 40, 130, 15, 48, 102, 57, 85, 31, 104, 64, 27, 134,…\n#> $ GDPperCapita   <dbl> 5316, 7062, 2335, 14280, 4945, 31794, 33700, 5016, 2053…\n#> $ HDI            <dbl> 0.801, 0.733, 0.446, 0.869, 0.775, 0.962, 0.948, 0.746,…\n#> $ Population     <dbl> 3.15, 32.85, 16.10, 38.75, 3.02, 20.40, 8.23, 8.39, 153…"},{"path":"SLR.html","id":"running-the-linear-model-lm","chapter":"3 Simple Linear Regression","heading":"3.7.2 Running the linear model (lm)","text":"","code":"\nhappy.lm = lm(LifeExpectancy ~ Happiness, data=happy) \n\nhappy.lm %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)    28.2      2.28       12.4 2.76e-24\n#> 2 Happiness       6.69     0.375      17.8 5.78e-38"},{"path":"SLR.html","id":"ouptut","chapter":"3 Simple Linear Regression","heading":"3.7.3 Ouptut","text":"analyses need residuals, fitted values, coefficients individually.can plot main relationship, can plot residuals (check technical conditions hold):Intervals interest: mean response, individual response, parameter(s).","code":"\nhappy.lm %>% augment()\n#> # A tibble: 143 x 8\n#>   LifeExpectancy Happiness .fitted  .resid    .hat .sigma   .cooksd .std.resid\n#>            <dbl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n#> 1           76.2       5.5    65.0  11.2   0.00765   6.09 0.0128        1.83  \n#> 2           71.7       5.6    65.7   6.00  0.00737   6.14 0.00357       0.980 \n#> 3           41.7       4.3    57.0 -15.3   0.0168    6.02 0.0539       -2.51  \n#> 4           74.8       7.1    75.7  -0.944 0.0122    6.16 0.000148     -0.155 \n#> 5           71.7       5      61.7  10.0   0.0101    6.10 0.0138        1.64  \n#> 6           80.9       7.9    81.1  -0.198 0.0216    6.16 0.0000118    -0.0326\n#> # … with 137 more rowshappy %>%\n         ggplot(aes(x=Happiness, y=LifeExpectancy)) + \n         geom_point() + \n         geom_smooth(method=\"lm\", se=FALSE) \n\nhappy.lm %>% \n         augment() %>% \n         ggplot(aes(x = .fitted, y = .resid)) + \n         geom_point() + \n         geom_hline(yintercept=0)\npredict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(\"conf\"), level=.95)\n#>    fit  lwr  upr\n#> 1 55.0 53.2 56.7\n#> 2 75.1 73.8 76.4\npredict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(\"pred\"), level=.95)\n#>    fit  lwr  upr\n#> 1 55.0 42.7 67.3\n#> 2 75.1 62.9 87.3\n\nhappy.lm %>% tidy(conf.int = TRUE)\n#> # A tibble: 2 x 7\n#>   term        estimate std.error statistic  p.value conf.low conf.high\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)    28.2      2.28       12.4 2.76e-24    23.7      32.7 \n#> 2 Happiness       6.69     0.375      17.8 5.78e-38     5.95      7.43"},{"path":"SLR.html","id":"residuals-in-r","chapter":"3 Simple Linear Regression","heading":"3.7.3.1 Residuals in R","text":"skipped residuals section, responsible finding residuals R, R code completion case interested:","code":"\nhappy.lm %>% augment()\n#> # A tibble: 143 x 8\n#>   LifeExpectancy Happiness .fitted  .resid    .hat .sigma   .cooksd .std.resid\n#>            <dbl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n#> 1           76.2       5.5    65.0  11.2   0.00765   6.09 0.0128        1.83  \n#> 2           71.7       5.6    65.7   6.00  0.00737   6.14 0.00357       0.980 \n#> 3           41.7       4.3    57.0 -15.3   0.0168    6.02 0.0539       -2.51  \n#> 4           74.8       7.1    75.7  -0.944 0.0122    6.16 0.000148     -0.155 \n#> 5           71.7       5      61.7  10.0   0.0101    6.10 0.0138        1.64  \n#> 6           80.9       7.9    81.1  -0.198 0.0216    6.16 0.0000118    -0.0326\n#> # … with 137 more rows"},{"path":"analysis-of-categorical-data.html","id":"analysis-of-categorical-data","chapter":"4 Analysis of Categorical Data","heading":"4 Analysis of Categorical Data","text":"(Section 6.3 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"cat","chapter":"4 Analysis of Categorical Data","heading":"4.1 Categorical Inference","text":"either observational study randomized experiment, often interested assessing statistical significance differences see: observed difference big reasonably occurred just due chance? answer question, usesimulationmathematical probability models.Example 4.1  Back Pain & Botox, Chance Rossman (2018), Foster et al. (2001)\nrandomized clinical trial examined whether drug botulinum toxin (Botox) helpful reducing pain among patients suffer chronic low back pain. 31 subjects participated study randomly assigned one two treatment groups: 16 received placebo normal saline 15 received drug . subjects’ pain levels evaluated beginning study eight weeks. researchers found 2 16 subjects received saline experienced substantial reduction pain, compared 9 15 subjects received actual drug.experiment observational study?Explain importance using “placebo” treatment saline.Create two-way table summarizing data, putting explanatory variable rows response variable columnsCalculate conditional proportions pain reduction two groups. Display results segmented bar graph. Comment preliminary analysis.\\[\\begin{eqnarray*}\n\\mbox{risk}_{\\mbox{placebo}} = \\frac{2}{16} &=& 0.125\\\\\n\\mbox{risk}_{\\mbox{Botox}} = \\frac{9}{15} &=& 0.6\\\\\nRR &=& 4.8\n\\end{eqnarray*}\\]Note sometimes makes sense y-axis count sometimes makes sense y-axis percent. Probably doesn’t matter much , choose bar plot seems informative .association treatment back pain relief, many 11 “successes” expect see group? researchers observe successes saline group expected (drug effect) fewer successes expected? direction conjectured researchers?association treatment back pain relief, many 11 “successes” expect see group? researchers observe successes saline group expected (drug effect) fewer successes expected? direction conjectured researchers?possible drug absolutely effect back pain? differences simply due chance random variability? likely ?possible drug absolutely effect back pain? differences simply due chance random variability? likely ?","code":"\nbackpain <- data.frame(treatment = c(rep(\"placebo\", 16), rep(\"Botox\", 15)),\n                     outcome = c(rep(\"reduction\", 2), rep(\"no_reduction\", 14), \n                                 rep(\"reduction\", 9), rep(\"no_reduction\", 6)))\nbackpain %>%\n  table()\n#>          outcome\n#> treatment no_reduction reduction\n#>   Botox              6         9\n#>   placebo           14         2\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome), position = \"fill\") +\n  ylab(\"percentage\")\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome))"},{"path":"analysis-of-categorical-data.html","id":"simulation","chapter":"4 Analysis of Categorical Data","heading":"Simulation","text":"11 red “success” cards (pain reduction); 20 black “failure” cards (pain reduction)11 red “success” cards (pain reduction); 20 black “failure” cards (pain reduction)randomly deal (.e. shuffle) 15 cards treatment group 16 cards placebo group.randomly deal (.e. shuffle) 15 cards treatment group 16 cards placebo group.count many people treatment group successes? Repeat 5 times.count many people treatment group successes? Repeat 5 times.process\ncards represent?\nshuffling cards represent?\nimplicit assumption two groups shuffling cards represent?\nobservational units represented dots dotplot?\ncount number repetitions 9 “successes?”\nprocesswhat cards represent?shuffling cards represent?implicit assumption two groups shuffling cards represent?observational units represented dots dotplot?count number repetitions 9 “successes?”Repeat simulation using two-way table applet:\n[http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm]Repeat simulation using two-way table applet:\n[http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm]summary\nmany reps?\nmany extreme true data?\nproportion least extreme true data?\ndata support researchers conjecture?\nactual data 7 successes treatment group (4 placebo group)?\nsummaryHow many reps?many extreme true data?proportion least extreme true data?data support researchers conjecture?actual data 7 successes treatment group (4 placebo group)?Definition 4.1  p-value p-value probability seeing results extreme nothing interesting going data. (definition p-value always use class research.)Notice regardless whether drug effect, data different time (think: new 31 people). small p-value allows us draw cause--effect conclusions, doesn’t necessarily allow us infer larger population. ?","code":""},{"path":"analysis-of-categorical-data.html","id":"inferFET","chapter":"4 Analysis of Categorical Data","heading":"4.1.1 Simulation using R","text":"simulation applet can recreated using infer package R. Note different pieces simulation using functions like specify(), hypothesize(), generate(), calculate(). Also notice particular function works best using difference proportions (discussed class equivalent recording single count Botox patients reduced back pain).Step 1. Calculate observed difference proportion patients reduced back pain. Note linear regression continue use syntax: responsevariable ~ explanatoryvariable.Step 2. Go simulation steps, just like applet.specify() variableshypothesize() null claimgenerate() many permutions datacalculate() statistic interest different permutationsStep 3. Plot histogram representing differences proportions many permuted tables. plot represents distribution differences proportion null hypothesis.Step 4. Calculate p-value sampling distribution generated Step 3.","code":"\nlibrary(infer)\n\n# Step 1.\ndiff_props <- backpain %>%\n  specify(outcome ~ treatment, success = \"reduction\") %>%\n  calculate(stat = \"diff in props\")\n\ndiff_props  # print to screen to see the observed difference\n#> # A tibble: 1 x 1\n#>    stat\n#>   <dbl>\n#> 1 0.475\n\n# Step 2.\nnulldist <- backpain %>%\n  specify(outcome ~ treatment, success = \"reduction\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"diff in props\")\n\n# Step 3.\nvisualize(nulldist) + \n  shade_p_value(obs_stat = diff_props, direction = \"greater\")\n\n# Step 4.\nnulldist %>%\n  get_p_value(obs_stat = diff_props, direction = \"greater\")\n#> # A tibble: 1 x 1\n#>   p_value\n#>     <dbl>\n#> 1   0.007"},{"path":"analysis-of-categorical-data.html","id":"fisher","chapter":"4 Analysis of Categorical Data","heading":"4.2 Fisher’s Exact Test","text":"(Section 6.4 Kuiper Sklar (2013), great detailed explanation!)fixed sample, use hypergeometric distribution enumerate possible ways choosing data extreme given fixed row column totals.make simpler, let’s say 5 items (N=5), want choose 3 (n=3). many ways can ?SSSNN, SSNSN, SSNNS, SNSSN, SNSNS, SNNSS, NSSSN, NSSNS, NSNSS, NNSSS [5!/ 3! 2!]\n(S = select, N = selected), many different ways can select 11 people (31) “pain reduction” group? total number different groups size 11 31. really, want groups certain breakdown. need 2 (16) gotten placebo 9 (15) gotten Botox treatment.Definition 4.2  Hypergeometric Probability \\(2 \\times 2\\) table N observations M total successes , probability observing x successes sample size n :\\[\\begin{eqnarray*}\nP(X=x) = \\frac{\\# \\mbox{ ways select x successes n-x failures}}{\\# \\mbox{ ways select n subjects}} = \\frac{ { M \\choose x} {N-M \\choose n-x}}{{N \\choose n}}\\\\\n\\end{eqnarray*}\\]Find P(X=2)can now find EXACT probabilities associated following hypotheses.\n\\[\\begin{eqnarray*}\n&&H_0: p_{pl} = p_{btx}\\\\\n&&H_a: p_{pl} < p_{btx}\\\\\n&&p = \\mbox{true probability pain}\\\\\n\\end{eqnarray*}\\]one- two-sided test? ? [Note: conditions include row column totals fixed – conditional test independence. However, research project back chapter 6 extends permutation test demonstrated probabilities hold even alternative technical conditions.Note also get exact probability conditions needed sample size big enough (can use Fisher’s Exact Test even true probabilities close 0 1.]","code":""},{"path":"analysis-of-categorical-data.html","id":"chisq","chapter":"4 Analysis of Categorical Data","heading":"4.3 Testing independence of two categorical variables","text":"(Sections 6.5, 6.6, 6.7 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"chi2-tests","chapter":"4 Analysis of Categorical Data","heading":"4.3.1 \\(\\chi^2\\) tests","text":"(Section 6.6 Kuiper Sklar (2013).)2x2… also rxc (\\(p_a = p_b = p_c\\))can also use \\(\\chi^2\\) tests evaluate \\(r \\times c\\) contingency tables. main question now whether association two categorical variables interest. Note now generalizing Botox back pain example. two variables independent? two variables independent, state one variable related probability different outcomes variable.data sampled way random samples explanatory response variables (e.g., cross classification study), typically test association:\\[\\begin{eqnarray*}\nH_0: && \\mbox{ two variables independent}\\\\\nH_a: && \\mbox{ two variables independent}\n\\end{eqnarray*}\\]data sampled way response measured across specified populations (example ), typically test homogeneity proportions. example,\\[\\begin{eqnarray*}\nH_0: && p_1 = p_2 = p_3\\\\\nH_a: && \\mbox{} H_0\n\\end{eqnarray*}\\]\n\\(p=P(\\mbox{success})\\) groups 1,2,3.get expected frequencies? mathematics hold regardless type test (.e., sampling mechanism used collect data). , fact,variables independent, able multiply probabilities. probabilities , expect overall proportion response variable proportion response variable explanatory group. math example follows directly.Example 4.2  table show observed distributions ABO blood type three random samples African Americans living different locations. three datasets, collected 1950s three different investigators, reproduced (Mourant, Kopec, Domaniewsa-Sobczak 1976).","code":""},{"path":"analysis-of-categorical-data.html","id":"test-of-homogeneity-of-proportions-equivalent-mathematically-to-independence","chapter":"4 Analysis of Categorical Data","heading":"4.3.1.1 Test of Homogeneity of Proportions (equivalent mathematically to independence)","text":"difference blood type proportions across groups, :\\[\\begin{eqnarray*}\nP(AB | FL) = P(AB | IA) = P(AB | MO) = P(AB)\n\\end{eqnarray*}\\]use \\(\\hat{P}(AB) = \\frac{368}{8619}\\) baseline expectation (\\(H_0\\)) groups. , expect,\\[\\begin{eqnarray*}\n\\# \\mbox{expected AB blood Iowa} &=&  \\frac{368}{8619} \\cdot 6722\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"test-of-independence-equivalent-mathematically-to-homogeneity-of-proporitions","chapter":"4 Analysis of Categorical Data","heading":"4.3.1.2 Test of Independence (equivalent mathematically to homogeneity of proporitions)","text":"\\[\\begin{eqnarray*}\nP(cond1 \\mbox{ & } cond2 ) &=& P(cond1) P(cond2)  \\mbox{ variables 1 2 independent}\\\\\nP(AB \\mbox{ blood & Iowa}) &=& P(AB \\mbox{ blood}) P(\\mbox{Iowa}) \\\\\n&=& \\bigg( \\frac{368}{8619}\\bigg) \\bigg( \\frac{6722}{8619} \\bigg)\\\\\n&=& 0.0333\\\\\n\\# \\mbox{expected AB blood Iowa} &=& 0.033 \\cdot 8619\\\\\n&=& \\frac{368 \\cdot 6722}{8619}\\\\\nE_{,j} &=& \\frac{(\\mbox{ row total})(j \\mbox{ col total})}{\\mbox{grand total}}\\\\\n\\end{eqnarray*}\\]expected values null hypothesis…\\[\\begin{eqnarray*}\nX^2 &=& \\sum_{cells} \\frac{( O - E)^2}{E}\\\\\n&=& 5.65\\\\\n\\mbox{p-value} &=& P(\\chi^2_6 \\geq 5.65) \\\\\n&=& 1 - pchisq(5.65, 6)\\\\\n&=& 0.464\n\\end{eqnarray*}\\]reject null hypothesis. , evidence null hypothesis blood types independently distributed various regions.know test statistic big number ? Well, turns test statistic (\\(X^2\\)) approximate \\(\\chi^2\\) distribution degrees freedom = \\((r- 1)\\cdot (c-1)\\). long :random sample population.expect least 1 observation every cell (\\(E_i \\geq 1 \\forall \\))expect least 5 observations 80% cells (\\(E_i \\geq 5\\) 80% \\(\\))two populations, \\(\\chi^2\\) procedure equivalent two-sided z-test proportions. chi-squared test statistic square z-test statistic. , chi-squared test exactly two-sided alternative z-test.use chi-square multiple populationsuse z-test want one-sided tests confidence intervals.","code":""},{"path":"analysis-of-categorical-data.html","id":"catest","chapter":"4 Analysis of Categorical Data","heading":"4.4 Parameter Estimation","text":"(Section 6.8 Kuiper Sklar (2013).)Definition 4.3  Data Types Data often classified asCategorical - unit assigned categoryQuantitative - observational unit assigned numerical value(Binary - special case categorical 2 categories, e.g. male/female)Table 6.6 page 193 Kuiper Sklar (2013) excellent worth looking .Example 4.3  Popcorn & Lung Disease Chance Rossman (2018)can tell popcorn production related lung disease? Consider High / Low exposure:21 lot people? Can compare 6 vs. 15? look ? proportions (always number 0 1). Look data (graphically numerically). Segmented bar graph (mosaic plot):difference two groups? Look difference proportions risk:\\[\\begin{eqnarray*}\n6/58 = 0.103 & 15/58=0.2586 & \\Delta = 0.156\\\\\np_1 = 0.65 & p_2 = 0.494 & \\Delta = 0.156\\\\\np_1 = 0.001 & p_2 = 0.157 & \\Delta = 0.156\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"differences-in-proportions","chapter":"4 Analysis of Categorical Data","heading":"4.4.1 Differences in Proportions","text":"turns sampling distribution difference sample proportions (success) across two independent groups can modeled normal distribution reasonably large sample sizes (CLT).ensure accuracy test, check whether np n(1-p) bigger 5 samples usually adequate. precise check \\(n_s \\hat{p}_c\\) \\(n_s(1-\\hat{p}_c)\\) greater 5; \\(n_s\\) smaller two sample sizes \\(\\hat{p}_c\\)sample proportion two samples combined one.Note:\n\\[\\begin{eqnarray*}\n\\hat{p}_1 - \\hat{p}_2 \\sim N\\Bigg(p_1 - p_2, \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\Bigg)\n\\end{eqnarray*}\\]testing independence, assume \\(p_1=p_2\\), use pooled estimate proportion calculate SE:\n\\[\\begin{eqnarray*}\nSE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) \\bigg(\\frac{1}{n_1} + \\frac{1}{n_2}\\bigg)}\n\\end{eqnarray*}\\], testing, appropriate test statistic :\n\\[\\begin{eqnarray*}\n Z = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{ \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) (\\frac{1}{n_1} + \\frac{1}{n_2})}}\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"ci-for-differences-in-proportions","chapter":"4 Analysis of Categorical Data","heading":"4.4.2 CI for differences in proportions","text":"can’t pool estimate SE, everything else stays …\\[\\begin{eqnarray*}\nSE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\end{eqnarray*}\\]main idea determine whether two categorical variables independent. , knowledge value one variable tell something probability variable (gender pregnancy). ’re going talk two different ways approach problem.","code":""},{"path":"analysis-of-categorical-data.html","id":"relative-risk","chapter":"4 Analysis of Categorical Data","heading":"4.4.3 Relative Risk","text":"Definition 4.4  Relative Risk relative risk (RR) ratio risks group. say, “risk success RR times higher group 1 compared group 2.”\\[\\begin{eqnarray*}\n\\mbox{relative risk} &=& \\frac{\\mbox{risk group 1}}{\\mbox{risk group 2}}\\\\\n&=&  \\frac{\\mbox{proportion successes group 1}}{\\mbox{proportion successes group 2}}\\\\\n\\mbox{RR} &=& \\frac{p_1}{p_2} = \\frac{p_1}{p_2}\\\\\n\\hat{\\mbox{RR}} &=& \\frac{\\hat{p}_1}{\\hat{p}_2}\n\\end{eqnarray*}\\]\\(\\hat{RR}\\) popcorn example \\(\\frac{15/58}{6/58} = 2.5\\). say, “risk airway obstruction 2.5 times higher high exposure group compared low exposure group.” aboutsample size?baseline risk?create confidence intervals relative risk, use fact :\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{RR})) &\\approx& \\sqrt{\\frac{(1 - \\hat{p}_1)}{n_1 \\hat{p}_1} + \\frac{(1-\\hat{p}_2)}{n_2 \\hat{p}_2}}\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"odds-ratios","chapter":"4 Analysis of Categorical Data","heading":"4.4.4 Odds Ratios","text":"related concept risk odds. often used horse racing, “success” typically defined losing. , odds 3 1 expect lose 3/4 time.Definition 4.5  Odds Ratio related concept risk odds. often used horse racing, “success” typically defined losing. , odds 3 1 expect lose 3/4 time. odds ratio () ratio odds group. say, “odds success times higher group 1 compared group 2.”\\[\\begin{eqnarray*}\n\\mbox{odds} &=& \\frac{\\mbox{proportion successes}}{\\mbox{proportion failures}}\\\\\n&=& \\frac{\\mbox{number successes}}{\\mbox{number failures}} = \\theta\\\\\n\\hat{\\mbox{odds}} &=& \\hat{\\theta}\\\\\n\\mbox{odds ratio} &=& \\frac{\\mbox{odds group 1}}{\\mbox{odds group 2}} \\\\\n\\mbox{} &=& \\frac{\\theta_1}{\\theta_2} = \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}= \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\\\\n\\hat{\\mbox{}} &=& \\frac{\\hat{\\theta}_1}{\\hat{\\theta}_2} = \\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\\\\n\\end{eqnarray*}\\]\\(\\hat{}\\) popcorn example \\(\\frac{15/43}{6/52} = 3.02\\). say, “odds airway obstruction 3 times higher high exposure group compared low exposure group.”","code":""},{"path":"analysis-of-categorical-data.html","id":"or-is-more-extreme-than-rr","chapter":"4 Analysis of Categorical Data","heading":"4.4.4.1 OR is more extreme than RR","text":"Without loss generality, assume true \\(RR > 1\\), implying \\(p_1 / p_2 > 1\\) \\(p_1 > p_2\\).Note following sequence consequences:\\[\\begin{eqnarray*}\nRR = \\frac{p_1}{p_2} &>& 1\\\\\n\\frac{1 - p_1}{1 - p_2} &<& 1\\\\\n\\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& 1\\\\\n\\frac{p_1}{p_2} \\cdot \\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& \\frac{p_1}{p_2}\\\\\n&>& RR\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"other-considerations","chapter":"4 Analysis of Categorical Data","heading":"4.4.4.2 Other considerations:","text":"Observational study (worked place?)Cross sectional (one point time)Healthy worker effect (stayed home sick?)Explanatory variable one potential explanation changes (exposure level).Response variable measured outcome interest (airway obstruction).Example 4.4  Smoking & Lung Cancer Chance Rossman (2018)World War II, evidence began mounting link cigarette smoking pulmonary carcinoma (lung cancer). 1950s, two now classic articles published subject. One studies conducted United States Wynder Graham (1950). found records large number (684) patients proven bronchiogenic carcinoma (specific form lung cancer) hospitals California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, Utah. personally interviewed 634 subjects identify smoking habits, occupation, exposure dust fumes, alcohol intake, education, cause death parents siblings. Thirty-three subjects completed mailed questionnaires, information 17 obtained family members close acquaintances. study, researchers focused 605 male patients form lung cancer. Another 1332 hospital patients similar age economic distribution (including 780 males) without lung cancer interviewed researchers St. Louis researchers Boston, Cleveland, Hines, Illinois.following two-way table replicates counts 605 male patients form cancer “control-group” 780 males.Given results study, think can generalize sample population? Explain make clear know difference sample population.Causation?Case-control study (605 lung cancer, 780 without… baseline rate?)lung cancer considered success smoking baseline:\\[\\begin{eqnarray*}\n\\hat{RR} &=& \\frac{122/182}{8/122} = 10.22\\\\\n\\hat{} &=& \\frac{122/60}{8/114} = 28.9\\\\\n\\end{eqnarray*}\\]\nrisk lung cancer 10.22 times higher smoke don’t smoke.odds lung cancer 28.9 times higher smoke don’t smoke.chain smoking considered success healthy baseline:\\[\\begin{eqnarray*}\n\\hat{RR} &=& \\frac{122/130}{60/174} = 2.7\\\\\n\\hat{} &=& \\frac{122/8}{60/114} = 28.9\\\\\n\\end{eqnarray*}\\]\nrisk smoking 2.7 times higher lung cancer don’t lung cancer.odds smoking 28.9 times higher lung cancer don’t lung cancer.\nknow risk light smoker lung cancer know risk lung cancer light smoker. Let’s say population 1,000,000 people:\\[\\begin{eqnarray*}\nP(\\mbox{light} | \\mbox{lung cancer}) &=& \\frac{49,000}{50,000} = 0.98\\\\\nP(\\mbox{lung cancer} | \\mbox{light}) &=& \\frac{49,000}{100,000} = 0.49\\\\\n\\end{eqnarray*}\\]explanatory variable?response variable?relative risk?odds ratio?lung cancer considered success smoking baseline:\\[\\begin{eqnarray*}\nRR &=& \\frac{49/100}{1/900} = 441\\\\\n&=& \\frac{49/51}{1/899} = 863.75\\\\\n\\end{eqnarray*}\\]light smoking considered success healthy baseline:\\[\\begin{eqnarray*}\nRR &=& \\frac{49/50}{51/950} = 18.25\\\\\n&=& \\frac{49/1}{51/899} = 863.75\\\\\n\\end{eqnarray*}\\]matter variable choose explanatory versus response! Though, general, still prefer know baseline odds baseline risk (can’t know case-control study).Example 4.5  Smoking & Lung Cancer, Chance Rossman (2018)\nNow cohort prospective study. (Previously case-control retrospective study). Now baseline risk estimates? Yes! careful, can’t conclude causation, study still observational.","code":""},{"path":"analysis-of-categorical-data.html","id":"ciOR","chapter":"4 Analysis of Categorical Data","heading":"4.4.5 Confidence Interval for OR","text":"Due theory won’t cover:\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{})) &\\approx& \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\n\\end{eqnarray*}\\]Note book introduces \\(SE(\\ln(\\hat{}))\\) context hypothesis testing null, \\(H_0: p_1 = p_2\\), assumed true. null true, ’d prefer estimate proportion success based entire sample:\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{})) &\\approx& \\sqrt{\\frac{1}{n_1 \\hat{p} (1-\\hat{p})} + \\frac{1}{n_2 \\hat{p}(1-\\hat{p})}}\n\\end{eqnarray*}\\], \\((1-\\alpha)100\\%\\) CI \\(\\ln()\\) :\n\\[\\begin{eqnarray*}\n\\ln(\\hat{}) \\pm z_{1-\\alpha/2} SE(\\ln(\\hat{}))\n\\end{eqnarray*}\\]gives \\((1-\\alpha)100\\%\\) CI \\(\\):\n\\[\\begin{eqnarray*}\n(e^{\\ln(\\hat{}) - z_{1-\\alpha/2} SE(\\ln(\\hat{}))}, e^{\\ln(\\hat{}) + z_{1-\\alpha/2} SE(\\ln(\\hat{}))})\n\\end{eqnarray*}\\]Back example… \\(= 28.9.\\)\n\\[\\begin{eqnarray*}\nSE(\\ln(\\hat{})) &=& \\sqrt{\\frac{1}{182*0.67*(1-0.67)} + \\frac{1}{122*0.0656*(1-0.0656)}}\\\\\n&=& 0.398\\\\\n90\\% \\mbox{ CI } \\ln() && \\ln(28.9) \\pm 1.645 \\cdot 0.398\\\\\n&& 3.366 \\pm 1.645 \\cdot 0.398\\\\\n&& (2.71, 4.02)\\\\\n90\\% \\mbox{ CI } && (e^{2.71}, e^{4.02})\\\\\n&& (15.04, 55.47)\\\\\n\\end{eqnarray*}\\]90% confident true \\(\\ln()\\) 2.71 4.02. 90% confident true \\(\\) 15.04 55.47. , true odds getting lung cancer smoke somewhere 15.04 55.47 times higher don’t smoke, 90% confidence.Note 1: use theory allows us understand sampling distribution \\(\\ln(\\hat{}).\\) use process creating CIs transform back \\(\\).Note 2: use t-distribution estimating population standard deviation.Note 3: good general guidelines checking whether sample sizes large enough normal approximation. authorities agree one can get away smaller sample sizes differences two proportions. sample sizes pass rough check discussed \\(\\chi^2\\), large enough support inferences based approximate normality log estimated odds ratio, . (Ramsey Schafer 2012, 541)normal approximation hold, need expected counts cell least 5. (Pagano Gauvreau 2000, 355)Note 4: cells zero, many people add 0.5 cell’s observed value.Note 5: always extreme RR (one reason careful…)\\[\\begin{eqnarray*}\n\\mbox{assume } && \\frac{X_1 / n_1}{X_2 / n_2} = RR > 1\\\\\n& & \\\\\n\\frac{X_1}{n_1} &=& RR \\ \\ \\frac{X_2}{n_2}\\\\\n\\frac{X_1}{n_1 - X_1} &=& RR \\ \\ \\bigg( \\frac{n_1}{n_2}  \\frac{n_2 - X_2}{n_1 - X_1} \\bigg) \\frac{X_2}{n_2-X_2}\\\\\n&=& RR \\ \\ \\bigg(\\frac{n_1}{n_2} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\\n &=& RR \\ \\ \\bigg(\\frac{1/n_2}{1/n_1} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\\n &=& RR  \\ \\ \\frac{1 - X_2/n_2}{1 - X_1/n_1}\\\\\n & > & RR\n\\end{eqnarray*}\\]\n[\\(1 - \\frac{X_2}{n_2} > 1 - \\frac{X_1}{n_1} \\rightarrow \\frac{1 - \\frac{X_2}{n_2}}{1 - \\frac{X_1}{n_1}} > 1\\)]Note 6: \\(RR \\approx \\) risk small (denominator similar denominator RR).","code":""},{"path":"analysis-of-categorical-data.html","id":"studies","chapter":"4 Analysis of Categorical Data","heading":"4.5 Types of Studies","text":"(Section 6.9 Kuiper Sklar (2013).)Definition 4.6  Explanatory variable one potential explanation changes response variable.Definition 4.7  Response variable measured outcome interest.Definition 4.8  Case-control study: identify observational units responseDefinition 4.9  Cohort study: identify observational units explanatory variableDefinition 4.10  Cross-classification study: identify observational units regardless levels variable.","code":""},{"path":"analysis-of-categorical-data.html","id":"retrospective-versus-prospective-studies","chapter":"4 Analysis of Categorical Data","heading":"4.5.1 Retrospective versus Prospective Studies","text":"much research (asking many people agree!), finally came across definition retrospective like. Note, however, many many books define retrospective synonymous case-control. , define retrospective study one observational units chosen based status response variable. disagree definition. see , retrospective studies defined based variables measured. ’ve also given quote Kuiper text retrospective defined study historic data collected (like definition less).Definition 4.11  prospective study watches outcomes, development disease, study period. explanatory variables measured response variable occurs.Definition 4.12  retrospective study looks backwards examines exposures suspected risk protection factors relation outcome established start study. explanatory variables measured response happened.Studies can classified either prospective retrospective. define prospective study one exposure covariate measurements made cases illness occur. retrospective study measurements made cases already occurred… Early writers referred cohort studies prospective studies case-control studies retrospective studies cohort studies usually begin identification exposure status measure disease occurrence, whereas case-control studies usually begin identifying cases controls measure exposure status. terms prospective retrospective, however, usefully employed describe timing disease occurrence respect exposure measurement. example, case-control studies can either prospective retrospective. prospective case-control study uses exposure measurements taken disease, whereas retrospective case-control study uses measurements taken disease. (Rothman Greenland 1998, 74)Retrospective cohort studies also exist. designs past (medical) records often used collect data. prospective cohort studies, objective still first establish groups based explanatory variable. However since past records response variable can collected time. (Kuiper Sklar 2013, chap. 6, page 24)Understanding study retrospective prospective leads sense biases within study.retrospective aspect may introduce selection bias misclassification information bias. retrospective studies, temporal relationship frequently difficult assess.","code":""},{"path":"analysis-of-categorical-data.html","id":"disadvantages-of-prospective-cohort-studies","chapter":"4 Analysis of Categorical Data","heading":"Disadvantages of Prospective Cohort Studies","text":"may follow large numbers subjects long time.can expensive time consuming.good rare diseases.good diseases long latency.Differential loss follow can introduce bias.","code":""},{"path":"analysis-of-categorical-data.html","id":"disadvantages-of-retrospective-cohort-studies","chapter":"4 Analysis of Categorical Data","heading":"Disadvantages of Retrospective Cohort Studies","text":"prospective cohort studies, good rare diseases.one uses records designed study, available data may poor quality.frequently absence data potential confounding factors data recorded past.may difficult identify appropriate exposed cohort appropriate comparison group.Differential losses follow can also bias retrospective cohort studies.Disadvantages : http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_CohortStudies/EP713_CohortStudies5.html","code":""},{"path":"analysis-of-categorical-data.html","id":"examples-of-studies","chapter":"4 Analysis of Categorical Data","heading":"Examples of studies:","text":"cross-classification, prospective: NHANEScross-classification, retrospective: death records (exposure measured post-hoc)case-control, prospective: investigator still enrolls based outcome status, investigator must wait cases occurcase-control, retrospective: start study, cases already occurred investigator goes back measure exposure (explanatory) variablecohort, prospective: follows selected participants assess proportion develop disease interestcohort, retrospective: exposure outcomes already happened (.e., death records)","code":""},{"path":"analysis-of-categorical-data.html","id":"which-test","chapter":"4 Analysis of Categorical Data","heading":"Which test?","text":"(Section 6.1 Kuiper Sklar (2013).)turns tests (independence, homogeneity proportions, homogeneity odds) typically equivalent respect conclusions. However, particular conditions related testing, can generally use hypotheses interest. However, need careful interpretations!(goodness fit, section 6.11 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"r-example-categorical-data-botox-and-back-pain","chapter":"4 Analysis of Categorical Data","heading":"4.6 R Example (categorical data): Botox and back pain","text":"","code":""},{"path":"analysis-of-categorical-data.html","id":"entering-and-visualizing-the-data","chapter":"4 Analysis of Categorical Data","heading":"4.6.1 Entering and visualizing the data","text":"","code":"\nbackpain <- data.frame(treatment = c(rep(\"placebo\", 16), rep(\"Botox\", 15)),\n                     outcome = c(rep(\"reduction\", 2), rep(\"no reduction\", 14), \n                                 rep(\"reduction\", 9), rep(\"no reduction\", 6)))\nbackpain %>%\n  table()\n#>          outcome\n#> treatment no reduction reduction\n#>   Botox              6         9\n#>   placebo           14         2\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome), position = \"fill\") +\n  ylab(\"percentage\")\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome))"},{"path":"analysis-of-categorical-data.html","id":"fishers-exact-test","chapter":"4 Analysis of Categorical Data","heading":"4.6.2 Fisher’s Exact Test","text":"","code":"\nbackpain %>%\n  table() %>%\n  fisher.test()\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  .\n#> p-value = 0.009\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  0.00848 0.71071\n#> sample estimates:\n#> odds ratio \n#>      0.104\n\n# their CI is an inversion of the HT\n# an approximate SE for the ln(OR) is given by:\nse.lnOR <- sqrt(1/(16*(2/16)*(14/16)) + 1/(15*(9/15)*(6/15)))\nse.lnOR\n#> [1] 0.922"},{"path":"analysis-of-categorical-data.html","id":"chi-squared-analysis","chapter":"4 Analysis of Categorical Data","heading":"4.6.3 Chi-squared Analysis","text":"","code":"\nbackpain %>%\n  table() %>%\n  chisq.test()\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  .\n#> X-squared = 6, df = 1, p-value = 0.02"},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"5 Logistic Regression","heading":"5 Logistic Regression","text":"","code":""},{"path":"logistic-regression.html","id":"logmodel","chapter":"5 Logistic Regression","heading":"5.1 Motivation for Logistic Regression","text":"investigation US space shuttle Challenger disaster, learned project managers judged probability mission failure 0.00001, whereas engineers working project estimated failure probability 0.005. difference two probabilities, 0.00499 discounted small worry . different picture provided considering odds? interpreted?logistic regression model generalized linear model. , linear model function expected value response variable. can now model binary response variables.\n\\[\\begin{align}\nGLM: g(E[Y | X]) = \\beta_0 + \\beta_1 X\n\\end{align}\\]\n\\(g(\\cdot)\\) link function. logistic regression, use logit link function:\n\\[\\begin{align}\n\\mbox{logit} (p) = \\ln \\bigg( \\frac{p}{1-p} \\bigg)\n\\end{align}\\]","code":""},{"path":"logistic-regression.html","id":"ex:burnexamp","chapter":"5 Logistic Regression","heading":"5.1.0.1 Surviving Third-degree Burns","text":"data refer 435 adults treated third-degree burns University Southern California General Hospital Burn Center. patients grouped according area third-degree burns body (measured square cm). table recorded, midpoint groupings log(area +1), number patients corresponding group survived, number died burns. (Fan, Heckman, Wand 1995)can see logit transformation linearizes relationship.first idea might model relationship probability success (patient survives) explanatory variable log(area +1) simple linear regression model. However, scatterplot proportions patients surviving third-degree burn explanatory variable shows distinct curved relationship two variables, rather linear one. seems transformation data place.functional form relating x probability success looks like S shape. ’d work figure form S looks like. ’ve given different relationships x probability success using \\(\\beta_0\\) \\(\\beta_1\\) values yet defined. Regardless, can see tuning functional relationship S curve, can get good fit data.S-curves ( y = exp(linear) / (1+exp(linear)) ) variety different parameter settings. Note x-axis continuous variable x y-axis probability success value x. move model.doesn’t linear regression work ?response isn’t normalThe response isn’t linear (transform)predicted values go outside bounds (0,1)Note: work think values inside (0,1) probabilities","code":""},{"path":"logistic-regression.html","id":"the-logistic-model","chapter":"5 Logistic Regression","heading":"5.1.1 The logistic model","text":"Instead trying model using linear regression, let’s say consider relationship variable \\(x\\) probability success given following generalized linear model. (logistic model just one model, isn’t anything magical . good reasons defined , doesn’t mean aren’t good ways model relationship.)\\[\\begin{align}\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\n\\(p(x)\\) probability success (surviving burn). \\(\\beta_1\\) still determines direction slope line. \\(\\beta_0\\) now determines location (median survival).Note 1 probability success patient covariate \\(x = -\\beta_0 / \\beta_1\\)?\\[\\begin{align}\nx &= - \\beta_0 / \\beta_1\\\\\n\\beta_0 + \\beta_1 x &= 0\\\\\ne^{0} &= 1\\\\\np(-\\beta_0 / \\beta_1) &= p(x) = 0.5\n\\end{align}\\]\n(given \\(\\beta_1\\), \\(\\beta_0\\) determines median survival value)Note 1 probability success patient covariate \\(x = -\\beta_0 / \\beta_1\\)?\\[\\begin{align}\nx &= - \\beta_0 / \\beta_1\\\\\n\\beta_0 + \\beta_1 x &= 0\\\\\ne^{0} &= 1\\\\\np(-\\beta_0 / \\beta_1) &= p(x) = 0.5\n\\end{align}\\]\n(given \\(\\beta_1\\), \\(\\beta_0\\) determines median survival value)Note 2 \\(x=0\\),\n\\[\\begin{align}\np(0) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\n\\end{align}\\]\n\\(x=0\\) can often thought baseline condition, probability \\(x=0\\) takes place thinking intercept linear regression.Note 2 \\(x=0\\),\n\\[\\begin{align}\np(0) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\n\\end{align}\\]\n\\(x=0\\) can often thought baseline condition, probability \\(x=0\\) takes place thinking intercept linear regression.Note 3\\[\\begin{align}\n1 - p(x) = \\frac{1}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\ngives probability failure.\n\\[\\begin{align}\n\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x}\n\\end{align}\\]\ngives odds success.\n\\[\\begin{align}\n\\ln \\bigg( \\frac{p(x)}{1-p(x)} \\bigg) = \\beta_0 + \\beta_1 x\n\\end{align}\\]\ngives \\(\\ln\\) odds success .Note 3\\[\\begin{align}\n1 - p(x) = \\frac{1}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\ngives probability failure.\n\\[\\begin{align}\n\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x}\n\\end{align}\\]\ngives odds success.\n\\[\\begin{align}\n\\ln \\bigg( \\frac{p(x)}{1-p(x)} \\bigg) = \\beta_0 + \\beta_1 x\n\\end{align}\\]\ngives \\(\\ln\\) odds success .Note 4 Every type generalized linear model link function. called logit. link relationship response variable linear function x.\n\\[\\begin{align}\n\\mbox{logit}(\\star) = \\ln \\bigg( \\frac{\\star}{1-\\star} \\bigg) \\ \\ \\ \\ 0 < \\star < 1\n\\end{align}\\]Note 4 Every type generalized linear model link function. called logit. link relationship response variable linear function x.\n\\[\\begin{align}\n\\mbox{logit}(\\star) = \\ln \\bigg( \\frac{\\star}{1-\\star} \\bigg) \\ \\ \\ \\ 0 < \\star < 1\n\\end{align}\\]","code":""},{"path":"logistic-regression.html","id":"model-assumptions","chapter":"5 Logistic Regression","heading":"5.1.1.1 model assumptions","text":"Just like linear regression, Y response random component.\\[\\begin{align}\ny &= \\begin{cases}\n1 & \\mbox{ died}\\\\\n0 & \\mbox{ survived}\n\\end{cases}\n\\end{align}\\]\\[\\begin{align}\nY &\\sim \\mbox{Bernoulli}(p)\\\\\nP(Y=y) &= p^y(1-p)^{1-y}\n\\end{align}\\]person risk different covariate (.e., explanatory variable), end different probability success.\n\\[\\begin{align}\nY_i \\sim \\mbox{Bernoulli} \\bigg( p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+ e^{\\beta_0 + \\beta_1 x_i}}\\bigg)\n\\end{align}\\]independent trialssuccess / failureprobability success constant particular \\(X\\).\\(E[Y|x] = p(x)\\) given logistic function","code":""},{"path":"logistic-regression.html","id":"interpreting-coefficients","chapter":"5 Logistic Regression","heading":"5.1.1.2 interpreting coefficients","text":"Let’s say log odds survival given observed (log) burn areas \\(x\\) \\(x+1\\) :\n\\[\\begin{align}\n\\mbox{logit}(p(x)) &= \\beta_0 + \\beta_1 x\\\\\n\\mbox{logit}(p(x+1)) &= \\beta_0 + \\beta_1 (x+1)\\\\\n\\beta_1 &= \\mbox{logit}(p(x+1)) - \\mbox{logit}(p(x))\\\\\n&= \\ln \\bigg(\\frac{p(x+1)}{1-p(x+1)} \\bigg) -  \\ln \\bigg(\\frac{p(x)}{1-p(x)} \\bigg)\\\\\n&= \\ln \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\\ne^{\\beta_1} &= \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\\n\\end{align}\\]\\(e^{\\beta_1}\\) odds ratio dying associated one unit increase x. [\\(\\beta_1\\) change log-odds associated one unit increase x.\\[\\begin{align}\n\\mbox{logit} (\\hat{p}) = 22.708 - 10.662 \\cdot \\ln(\\mbox{ area }+1).\n\\end{align}\\](Suppose interested comparing odds surviving third-degree burns patients burns corresponding log(area +1)= 1.90, patients burns corresponding\nlog(area +1)= 2.00. odds ratio \\(\\hat{}_{1.90, 2.00}\\) given \n\\[\\begin{align}\n\\hat{}_{1.90, 2.00} = e^{-10.662} (1.90-2.00) = e^{1.0662} = 2.904\n\\end{align}\\]\n, odds survival patient log(area+1)= 1.90 2.9 times higher odds survival patient log(area+1)= 2.0.)RR (relative risk) difference risks? won’t constant given \\(X\\), must calculated function \\(X\\).","code":""},{"path":"logistic-regression.html","id":"constant-or-varying-rr","chapter":"5 Logistic Regression","heading":"5.1.2 constant OR, varying RR","text":"previous model specifies constant value \\(X\\) true RR. Using burn data, convince RR isn’t constant. Try computing RR 1.5 versus 2.5, 1 versus 2.\n\\[\\begin{align}\n\\mbox{logit} (\\hat{p}) &= 22.708 - 10.662 \\cdot \\ln(\\mbox{ area }+1)\\\\\n\\hat{p}(x) &= \\frac{e^{22.708 - 10.662 x}}{1+e^{22.708 - 10.662 x}}\\\\\n\\end{align}\\]\\[\\begin{align}\n\\hat{p}(1) &= 0.9999941\\\\\n\\hat{p}(1.5) &= 0.9987889\\\\\n\\hat{p}(2) &= 0.7996326\\\\\n\\hat{p}(2.5) &= 0.01894664\\\\\n\\hat{RR}_{1, 2} &= 1.250567\\\\\n\\hat{RR}_{1.5, 2.5} &= 52.71587\\\\\n\\end{align}\\]\\[\\begin{align}\n\\hat{RR} &= \\frac{\\frac{e^{b_0 + b_1 x}}{1+e^{b_0 + b_1 x}}}{\\frac{e^{b_0 + b_1 (x+1)}}{1+e^{b_0 + b_1 (x+1)}}}\\\\\n&= \\frac{\\frac{e^{b_0}e^{b_1 x}}{1+e^{b_0}e^{b_1 x}}}{\\frac{e^{b_0} e^{b_1 x} e^{b_1}}{1+e^{b_0}e^{b_1 x} e^{b_1}}}\\\\\n&= \\frac{1+e^{b_0}e^{b_1 x}e^{b_1}}{e^{b_1}(1+e^{b_0}e^{b_1 x})}\\\\\n\\end{align}\\]\n(see log-linear model , 5.1.2.1 )","code":""},{"path":"logistic-regression.html","id":"altmodels","chapter":"5 Logistic Regression","heading":"5.1.2.1 Alternative strategies for binary outcomes","text":"quite common binary outcomes (response variable) medical literature. However, logit link (logistic regression) one variety models can use. see logistic model imposes constant value \\(X\\) (constant RR).complementary log-log\ncomplementary log-log model used rate , example, infection, model instances contact (based Poisson model).\n\\[\\begin{align}\np(k) &= 1-(1-\\lambda)^k\\\\\n\\ln[ - \\ln (1-p(k))] &= \\ln[-\\ln(1-\\lambda)] + \\ln(k)\\\\\n\\ln[ - \\ln (1-p(k))] &= \\beta_0 + 1 \\cdot \\ln(k)\\\\\n\\ln[ - \\ln (1-p(k))] &= \\beta_0 + \\beta_1 x\\\\\np(x) &= 1 - \\exp [ -\\exp(\\beta_0 + \\beta_1 x) ]\n\\end{align}\\]linear\nexcess (additive) risk model can modeled using simple linear regression:\n\\[\\begin{align}\np(x) &= \\beta_0 + \\beta_1 x\n\\end{align}\\]\nalready seen problematic variety reasons. However, unit increase \\(x\\) gives \\(\\beta_1\\) increase risk (values \\(x\\)).log-linear\nlong case-control study, can model risk using log-linear model.\n\\[\\begin{align}\n\\ln (p(x)) = \\beta_0 + \\beta_1 x\n\\end{align}\\]\nregression coefficient, \\(\\beta_1\\), interpretation logarithm relative risk associated unit increase \\(x\\). Although many software programs fit model, may present numerical difficulties constraint sum terms right-hand side must greater zero results make sense (due constraint outcome probability p(x) must interval [0,1]). result, convergence standard fitting algorithms may unreliable cases.","code":""},{"path":"logistic-regression.html","id":"logMLE","chapter":"5 Logistic Regression","heading":"5.2 Estimating coefficients in logistic regression","text":"","code":""},{"path":"logistic-regression.html","id":"maximum-likelihood-estimation","chapter":"5 Logistic Regression","heading":"5.2.1 Maximum Likelihood Estimation","text":"Recall estimated coefficients linear regression. values \\(b_0\\) \\(b_1\\) minimize residual sum squares:\n\\[\\begin{align}\nRSS &= \\sum_i (Y_i - \\hat{Y}_i)^2\\\\\n &= \\sum_i (Y_i - (\\beta_0 + \\beta_1 X_i))^2\n\\end{align}\\]\n, take derivatives respect \\(\\beta_0\\) \\(\\beta_1\\), set equal zero (take second derivatives ensure minimums), solve get \\(b_0\\) \\(b_1\\). turns ’ve also maximized normal likelihood.\n\\[\\begin{align}\nL(\\underline{y} | \\beta_0, \\beta_1, \\underline{x}) &= \\prod_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{(y_i - \\beta_0 - \\beta_1 x_i)^2 / 2 \\sigma}\\\\\n&= \\bigg( \\frac{1}{2 \\pi \\sigma^2} \\bigg)^{n/2} e^{\\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 / 2 \\sigma}\\\\\n\\end{align}\\]even mean? Likelihood? Maximizing likelihood? ??? likelihood probability distribution data given specific values unknown parameters.Consider toy example take sample size 4 binary population (e.g., flipping coin probability heads \\(p\\)) get: failure, success, failure, failure (FSFF).guess \\(p=0.49\\)?? , guess \\(p=0.25\\)… maximized likelihood seeing data.\n\\[\\begin{align}\nP(FSFF| p) &=  p^1 (1-p)^{4-1}\\\\\nP(FSFF | p = 0.90) &= 0.0009 \\\\\nP(FSFF | p = 0.75) &= 0.0117 \\\\\nP(FSFF | p = 0.50) &= 0.0625\\\\\nP(FSFF | p = 0.25) &= 0.105\\\\\nP(FSFF | p = 0.15) &= 0.092\\\\\nP(FSFF | p = 0.05) &= 0.043\\\\\n\\end{align}\\]Think example set independent binary responses, \\(Y_1, Y_2, \\ldots Y_n\\). Since observed response independent follows Bernoulli distribution, probability particular outcome can found :\n\\[\\begin{align}\nP(Y_1=y_1, Y_2=y_2, \\ldots, Y_n=y_n) &= P(Y_1=y_1) P(Y_2 = y_2) \\cdots P(Y_n = y_n)\\\\\n&= p^{y_1}(1-p)^{1-y_1} p^{y_2}(1-p)^{1-y_2} \\cdots p^{y_n}(1-p)^{1-y_n}\\\\\n&= p^{\\sum_i y_i} (1-p)^{\\sum_i (1-y_i)}\\\\\n\\end{align}\\]\n\\(y_1, y_2, \\ldots, y_n\\) represents particular observed series 0 1 outcomes \\(p\\) probability \\(0 \\leq p \\leq 1\\). \\(y_1, y_2, \\ldots, y_n\\) observed, fixed values. Maximum likelihood estimates functions sample data derived finding value \\(p\\) maximizes likelihood functions.maximize likelihood, use natural log likelihood (know ’ll get answer):\n\\[\\begin{align}\n\\ln L(p) &= \\ln \\Bigg(p^{\\sum_i y_i} (1-p)^{\\sum_i (1-y_i)} \\Bigg)\\\\\n&= \\sum_i y_i \\ln(p) + (n- \\sum_i y_i) \\ln (1-p)\\\\\n\\frac{ \\partial \\ln L(p)}{\\partial p} &= \\sum_i y_i \\frac{1}{p} + (n - \\sum_i y_i) \\frac{-1}{(1-p)} = 0\\\\\n0 &= (1-p) \\sum_i y_i + p (n-\\sum_i y_i) \\\\\n\\hat{p} &= \\frac{ \\sum_i y_i}{n}\n\\end{align}\\]Using logistic regression model makes likelihood substantially complicated probability success changes individual. Recall:\n\\[\\begin{align}\np_i = p(x_i) &= \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}}\n\\end{align}\\]\ngives likelihood :\n\\[\\begin{align}\nL(\\beta_0,\\beta_1) &= \\prod_i \\Bigg( \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\Bigg)^{y_i} \\Bigg(1-\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\Bigg)^{(1- y_i)} \\\\\n\\mbox{log likelihood }: &\\\\\n\\ln L(\\beta_0, \\beta_1) &= \\sum_i y_i \\ln\\Bigg( \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\Bigg) + (1-  y_i) \\ln \\Bigg(1-\\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+e^{\\beta_0 + \\beta_1 x_i}} \\Bigg)\\\\\n\\end{align}\\]find \\(b_0\\) \\(b_1\\) actually use numerical optimization techniques maximize \\(L(\\beta_0,\\beta_1)\\) (…using calculus won’t provide closed form solutions. Try taking derivatives setting equal zero. happens?)use maximum likelihood estimates?Estimates essentially unbiased.can estimate SE (Wald estimates via Fisher Information).estimates low variability.estimates approximately normal sampling distribution large sample sizes maximum likelihood estimates.Though important realize find estimates closed form.","code":""},{"path":"logistic-regression.html","id":"loginf","chapter":"5 Logistic Regression","heading":"5.3 Formal Inference","text":"","code":""},{"path":"logistic-regression.html","id":"wald-tests-intervals","chapter":"5 Logistic Regression","heading":"5.3.1 Wald Tests & Intervals","text":"use maximum likelihood parameter estimates, can also use large sample theory find SEs consider estimates normal distributions (large sample sizes). However, (Menard 1995) warns large coefficients, standard error inflated, lowering Wald statistic (chi-square) value. (Agresti 1996) states likelihood-ratio test reliable small sample sizes Wald test.\\[\\begin{align}\nz = \\frac{b_1 - \\beta_1}{SE(b_1)}\n\\end{align}\\]Note: Although model Bernoulli model really binomial model (although Bernoulli binomial \\(n=1\\)), way fit logistic regression model R use family = \"binomial\". just .","code":"\nburnglm %>%\n  glm(burnresp~burnexpl, data = ., family=\"binomial\") %>% \n  tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     22.7      2.27     10.0  1.23e-23\n#> 2 burnexpl       -10.7      1.08     -9.85 6.95e-23"},{"path":"logistic-regression.html","id":"likelihood-ratio-tests","chapter":"5 Logistic Regression","heading":"5.3.2 Likelihood Ratio Tests","text":"\\(\\frac{L(p_0)}{L(\\hat{p})}\\) gives us sense whether null value observed value produces higher likelihood. Recall:\n\\[\\begin{align}\nL(\\hat{\\underline{p}}) > L(p_0)\n\\end{align}\\]\nalways. [\\(\\hat{\\underline{p}}\\) maximum likelihood estimate probability success (vector probabilities, based MLE estimates linear parameters). ] inequality holds \\(\\hat{\\underline{p}}\\) maximizes likelihood.can show \\(H_0\\) true,\n\\[\\begin{align}\n-2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg) \\sim \\chi^2_1\n\\end{align}\\]\ntesting one parameter value. generally,\n\\[\\begin{align}\n-2 \\ln \\bigg( \\frac{\\max L_0}{\\max L} \\bigg) \\sim \\chi^2_\\nu\n\\end{align}\\]\n\\(\\nu\\) number extra parameters estimate using unconstrained likelihood (compared constrained null likelihood).Example 5.1  Consider data set 147 people. 49 got cancer 98 didn’t. Let’s test whether true proportion people get cancer \\(p=0.25\\).\n\\[\\begin{align}\nH_0:& p=0.25\\\\\nH_1:& p \\ne 0.25\\\\\n\\hat{p} &= \\frac{49}{147}\\\\\n-2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg) &= -2 [ \\ln (L(p_0)) - \\ln(L(\\hat{p}))]\\\\\n&= -2 \\Bigg[ \\ln \\bigg( (0.25)^{y} (0.75)^{n-y} \\bigg) - \\ln \\Bigg( \\bigg( \\frac{y}{n} \\bigg)^{y} \\bigg( \\frac{(n-y)}{n} \\bigg)^{n-y} \\Bigg) \\Bigg]\\\\\n&= -2 \\Bigg[ \\ln \\bigg( (0.25)^{49} (0.75)^{98} \\bigg) - \\ln \\Bigg( \\bigg( \\frac{1}{3} \\bigg)^{49} \\bigg( \\frac{2}{3} \\bigg)^{98} \\Bigg) \\Bigg]\\\\\n&= -2 [ \\ln(0.0054) - \\ln(0.0697) ] = 5.11\\\\\nP( \\chi^2_1 \\geq 5.11) &= 0.0238\n\\end{align}\\]really, usually likelihood ratio tests interesting. fact, usually, use test whether coefficients zero:\\[\\begin{align}\nH_0: & \\beta_1 =0\\\\\nH_1: & \\beta_1 \\ne 0\\\\\np_0 &= \\frac{e^{\\hat{b}_0}}{1 + e^{\\hat{b}_0}}\n\\end{align}\\]\n\\(\\hat{b}_0\\) MLE logistic regression model contain explanatory variable, \\(x\\).Important note:\n\\[\\begin{align}\n\\mbox{deviance} = \\mbox{constant} - 2 \\ln(\\mbox{likelihood})\n\\end{align}\\]\n, difference log likelihoods opposite difference deviances:\n\\[\\begin{align}\n\\mbox{test stat} &= \\chi^2\\\\\n&= -2 \\ln \\bigg( \\frac{L(\\mbox{null value(s)})}{L(MLEs)} \\bigg)\\\\\n&= -2 [ \\ln(L(\\mbox{null value(s)}) - \\ln(L(MLEs)) ]\\\\\n&= \\mbox{deviance}_0 - \\mbox{deviance}_{model}\\\\\n&= \\mbox{deviance}_{null} - \\mbox{deviance}_{residual}\\\\\n&= \\mbox{deviance}_{reduced} - \\mbox{deviance}_{full}\\\\\n\\end{align}\\]\\[\\begin{align}\n\\mbox{test stat} &= G\\\\\n&= -2 \\ln \\bigg( \\frac{L(\\mbox{null value(s)})}{L(MLEs)} \\bigg)\\\\\n&= -2 [ \\ln(L(\\mbox{null value(s)}) - \\ln(L(MLEs)) ]\\\\\n&= \\mbox{deviance}_0 - \\mbox{deviance}_{model}\\\\\n&= \\mbox{deviance}_{null} - \\mbox{deviance}_{residual}\\\\\n&= \\mbox{deviance}_{reduced} - \\mbox{deviance}_{full}\\\\\n\\end{align}\\], LRT (see columns null deviance deviance):\n\\[\\begin{align}\nG &= 525.39 - 335.23 = 190.16\\\\\np-value &= P(\\chi^2_1 \\geq 190.16) = 0\n\\end{align}\\]","code":"\nburnglm %>% \n  glm(burnresp~burnexpl, data = ., family=\"binomial\") %>% \n  glance() \n#> # A tibble: 1 x 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1          525.     434  -168.  339.  347.     335.         433   435"},{"path":"logistic-regression.html","id":"modeling-categorical-predictors-with-multiple-levels","chapter":"5 Logistic Regression","heading":"5.3.2.1 modeling categorical predictors with multiple levels","text":"Example 5.2  Snoring study undertaken investigate whether snoring related heart disease. survey, 2484 people classified according proneness snoring (never, occasionally, often, always) whether heart disease.Source: (Norton Dunn 1985)\\[\\begin{align}\nX_1 = \\begin{cases}\n  1 & \\text{occasionally} \\\\\n  0 & \\text{otherwise} \\\\\n\\end{cases}\nX_2 = \\begin{cases}\n  1 & \\text{often} \\\\\n  0 & \\text{otherwise} \\\\\n\\end{cases}\nX_3 = \\begin{cases}\n  1 & \\text{always} \\\\\n  0 & \\text{otherwise} \\\\\n\\end{cases}\n\\end{align}\\]new model becomes:\n\\[\\begin{align}\n\\mbox{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3\n\\end{align}\\]can use drop--deviance test test effect parameters (now four) model.See birdnest example, 5.8","code":""},{"path":"logistic-regression.html","id":"multlog","chapter":"5 Logistic Regression","heading":"5.4 Multiple Logistic Regression","text":"","code":""},{"path":"logistic-regression.html","id":"interaction","chapter":"5 Logistic Regression","heading":"5.4.1 Interaction","text":"Another worry building models multiple explanatory variables variables interacting. , one level variable, relationship main predictor response different.Example 5.3  Consider simple linear regression model number hours studied exam grade. add class year model. probably different slope class year order model two variables effectively. simplicity, consider first year students seniors.\\[\\begin{align}\nE[\\mbox{grade seniors}| \\mbox{hours studied}] &= \\beta_{0s} + \\beta_{1s} \\mbox{hrs}\\\\\nE[\\mbox{grade first years}| \\mbox{hours studied}] &= \\beta_{0f} + \\beta_{1f} \\mbox{hrs}\\\\\nE[\\mbox{grade}| \\mbox{hours studied}] &= \\beta_{0} + \\beta_{1} \\mbox{hrs} + \\beta_2 (\\mbox{year=senior}) + \\beta_{3} \\mbox{hrs} (\\mbox{year = senior})\\\\\n\\beta_{0f} &= \\beta_{0}\\\\\n\\beta_{0s} &= \\beta_0 + \\beta_2\\\\\n\\beta_{1f} &= \\beta_1\\\\\n\\beta_{1s} &= \\beta_1 + \\beta_3\n\\end{align}\\]need \\((\\mbox{year=seniors})\\) variable?Definition 5.1  Interaction means effect explanatory variable outcome differs according level another explanatory variable. (case age smoking lung cancer . smoking example, age significant variable, interact lung cancer.)Example 5.4  Heart Estrogen/progestin Replacement Study () randomized, double-blind, placebo-controlled trial designed test efficacy safety estrogen plus progestin therapy prevention recurrent coronary heart disease (CHD) events women. participants postmenopausal women uterus CHD. woman randomly assigned receive one tablet containing 0.625 mg conjugated estrogens plus 2.5 mg medroxyprogesterone acetate daily identical placebo. results first large randomized clinical trial examine effect hormone replacement therapy (HRT) women heart disease appeared JAMA 1998 (Hulley et al. 1998).Heart Estrogen/Progestin Replacement Study () found use estrogen plus progestin postmenopausal women heart disease prevent heart attacks death coronary heart disease (CHD). occurred despite positive effect treatment lipoproteins: LDL (bad) cholesterol reduced 11 percent HDL (good) cholesterol increased 10 percent.hormone replacement regimen also increased risk clots veins (deep vein thrombosis) lungs (pulmonary embolism). results surprising light previous observational studies, found lower rates CHD women take postmenopausal estrogen.Data available : http://www.biostat.ucsf.edu/vgsm/data/excel/hersdata.xls now, try predict whether individuals pre-existing medical condition (CHD, self reported), medcond. use variables age, weight, diabetes drinkany.Write models hand, significance change respect interaction? interpretation change interaction? last model, might want remove age information. Age seems less important drinking status. decide? model?","code":"\nglm(medcond ~ age, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -1.60     0.401       -4.00 0.0000624\n#> 2 age           0.0162   0.00597      2.71 0.00664\nglm(medcond ~ age + weight, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept) -2.17      0.496       -4.37 0.0000124\n#> 2 age          0.0189    0.00613      3.09 0.00203  \n#> 3 weight       0.00528   0.00274      1.93 0.0542\nglm(medcond ~ age+diabetes, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic      p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n#> 1 (Intercept)  -1.89     0.408       -4.64 0.00000349  \n#> 2 age           0.0185   0.00603      3.07 0.00217     \n#> 3 diabetes      0.487    0.0882       5.52 0.0000000330\nglm(medcond ~ age*diabetes, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic     p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>       <dbl>\n#> 1 (Intercept)   -2.52     0.478       -5.26 0.000000141\n#> 2 age            0.0278   0.00707      3.93 0.0000844  \n#> 3 diabetes       2.83     0.914        3.10 0.00192    \n#> 4 age:diabetes  -0.0354   0.0137      -2.58 0.00986\nglm(medcond ~ age*drinkany, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)  -0.991     0.511       -1.94  0.0526\n#> 2 age           0.00885   0.00759      1.17  0.244 \n#> 3 drinkany     -1.44      0.831       -1.73  0.0833\n#> 4 age:drinkany  0.0168    0.0124       1.36  0.175"},{"path":"logistic-regression.html","id":"simpsons-paradox","chapter":"5 Logistic Regression","heading":"5.4.2 Simpson’s Paradox","text":"Simpson’s paradox association two variables opposite partial association two variables controlling one variables.Example 5.5  Back linear regression consider Simpson’s Paradox wild. Consider data SAT scores across different states information educational expenditure. correlation SAT score average teacher salary negative combined data. However, SAT score average teacher salary positive controlling fraction students take exam. fewer students take exam, higher SAT score. ’s states whose public universities encourage ACT SAT-takers leaving state college (higher SAT scores).Example 5.6  Consider example smoking 20-year mortality (case) section 3.4 Regression Methods Biostatistics, pg 52-53. study represents women participating health survey Whickham, England 1972-1972 follow-20 years later (Vanderpump et al. 1995). Mortality recorded response variable self-reported smoking age (given original study) explanatory variables.see vast majority controls young, high rate smoking. good chunk cases older, rate smoking substantially lower oldest group. However, within group, cases likely smoke controls.adjusting age, smoking longer significant. importantly, age variable reverses effect smoking cancer - Simpson’s Paradox. effect due observational nature study, important adjust possible influential variables regardless study hand.mean adjust age context? means include model:Using additive model :\n\\[\\begin{align}\n\\mbox{logit} (p(x_1, x_2) ) &= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\\\\n&= \\mbox{odds dying } (x_1, x_2) / \\mbox{odds dying } (x_1^*, x_2^*) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}{e^{\\beta_0 + \\beta_1 x_1^* + \\beta_2 x_2^*}}\\\\\nx_1 &= \\begin{cases}\n0 & \\mbox{ smoke}\\\\\n1 & \\mbox{ smoke}\\\\\n\\end{cases}\\\\\nx_2 &= \\begin{cases}\n\\mbox{young} & \\mbox{18-44 years old}\\\\\n\\mbox{middle} & \\mbox{45-64 years old}\\\\\n\\mbox{old} & \\mbox{65+ years old}\\\\\n\\end{cases}\n\\end{align}\\]\nmodeling probability 20-year mortality using smoking status age group.Note 1: can see coefficients variable significantly different zero. , variables important predicting odds survival.Note 2: can see smoking becomes less significant add age model. age smoking status highly associated (think coin example).Note 3: can estimate (dying smoke vs smoke) given coefficients:\\[\\begin{align}\n\\mbox{simple model} &\\\\\n\\mbox{overall } &= e^{-0.37858 } = 0.6848332\\\\\n& \\\\\n\\mbox{additive model} &\\\\\n\\mbox{young, middle, old } &= e^{ 0.3122} = 1.3664\\\\\n& \\\\\n\\mbox{interaction model} &\\\\\n\\mbox{young } &= e^{0.2689 + 0.2177} = 1.626776\\\\\n\\mbox{middle } &= e^{0.2689} = 1.308524\\\\\n\\mbox{old } &= e^{0.2689 + -0.2505} = 1.018570\\\\\n\\end{align}\\]\nmean interaction terms significant last model?","code":"\nhead(SAT)\n#>        state expend ratio salary frac verbal math  sat         fracgrp\n#> 1    Alabama   4.41  17.2   31.1    8    491  538 1029    low fraction\n#> 2     Alaska   8.96  17.6   48.0   47    445  489  934 medium fraction\n#> 3    Arizona   4.78  19.3   32.2   27    448  496  944 medium fraction\n#> 4   Arkansas   4.46  17.1   28.9    6    482  523 1005    low fraction\n#> 5 California   4.99  24.0   41.1   45    417  485  902 medium fraction\n#> 6   Colorado   5.44  18.4   34.6   29    462  518  980 medium fraction\nlm(sat ~ salary, data=SAT) %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)  1159.       57.7      20.1  5.13e-25\n#> 2 salary         -5.54      1.63     -3.39 1.39e- 3\nlm(sat ~ salary + frac, data=SAT) %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   988.      31.9       31.0  6.20e-33\n#> 2 salary          2.18     1.03       2.12 3.94e- 2\n#> 3 frac           -2.78     0.228    -12.2  4.00e-16\nlm(sat ~ salary * frac, data=SAT) %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic  p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept) 1082.       54.4       19.9   3.00e-24\n#> 2 salary        -0.720     1.70      -0.424 6.73e- 1\n#> 3 frac          -5.03      1.09      -4.62  3.15e- 5\n#> 4 salary:frac    0.0648    0.0308     2.11  4.05e- 2\nlm(sat ~ salary + fracgrp, data=SAT) %>% tidy()\n#> # A tibble: 4 x 5\n#>   term                   estimate std.error statistic  p.value\n#>   <chr>                     <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)             1002.      31.8       31.5  8.55e-33\n#> 2 salary                     1.09     0.988      1.10 2.76e- 1\n#> 3 fracgrpmedium fraction  -112.      14.3       -7.82 5.46e-10\n#> 4 fracgrphigh fraction    -150.      12.8      -11.7  2.09e-15\nlm(sat ~ salary * fracgrp, data=SAT) %>% tidy()\n#> # A tibble: 6 x 5\n#>   term                           estimate std.error statistic  p.value\n#>   <chr>                             <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)                   1012.         55.8    18.1    4.85e-22\n#> 2 salary                           0.768       1.77    0.435  6.65e- 1\n#> 3 fracgrpmedium fraction        -107.        103.     -1.04   3.03e- 1\n#> 4 fracgrphigh fraction          -175.         79.2    -2.21   3.27e- 2\n#> 5 salary:fracgrpmedium fraction   -0.0918      2.93   -0.0313 9.75e- 1\n#> 6 salary:fracgrphigh fraction      0.692       2.28    0.303  7.63e- 1\nglm( death ~ smoke, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   -0.781    0.0796     -9.80 1.10e-22\n#> 2 smoke         -0.379    0.126      -3.01 2.59e- 3\n\nglm( death ~ as.factor(age), family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term                estimate std.error statistic  p.value\n#>   <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)           -0.571     0.125     -4.56 5.01e- 6\n#> 2 as.factor(age)old      1.45      0.187      7.75 9.00e-15\n#> 3 as.factor(age)young   -1.44      0.167     -8.63 6.02e-18\n\nglm( death ~ smoke + as.factor(age), family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term                estimate std.error statistic  p.value\n#>   <chr>                  <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)           -0.668     0.135     -4.96 7.03e- 7\n#> 2 smoke                  0.312     0.154      2.03 4.25e- 2\n#> 3 as.factor(age)old      1.47      0.188      7.84 4.59e-15\n#> 4 as.factor(age)young   -1.52      0.173     -8.81 1.26e-18\n\nglm( death ~ smoke * as.factor(age), family=\"binomial\") %>% tidy()\n#> # A tibble: 6 x 5\n#>   term                      estimate std.error statistic  p.value\n#>   <chr>                        <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)                 -0.655     0.152    -4.31  1.61e- 5\n#> 2 smoke                        0.269     0.269     0.999 3.18e- 1\n#> 3 as.factor(age)old            1.53      0.221     6.93  4.29e-12\n#> 4 as.factor(age)young         -1.65      0.240    -6.88  6.00e-12\n#> 5 smoke:as.factor(age)old     -0.251     0.420    -0.596 5.51e- 1\n#> 6 smoke:as.factor(age)young    0.218     0.355     0.614 5.40e- 1"},{"path":"logistic-regression.html","id":"multicol","chapter":"5 Logistic Regression","heading":"5.5 Multicolinearity","text":"Example 5.7  Consider following data set collected church offering plates 62 consecutive Sundays. Also noted whether enough change buy candy bar $1.25.Notice directionality low coins changes included model already contains number coins total. Lesson story: careful interpreting coefficients multiple explanatory variables.","code":"\nglm(Candy ~ Coins, data = Offering, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)   -4.14     0.996      -4.16 0.0000321\n#> 2 Coins          0.286    0.0772      3.70 0.000213\n\nglm(Candy ~ Small, data = Offering, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)   -2.33     0.585      -3.98 0.0000693\n#> 2 Small          0.184    0.0576      3.19 0.00142\n\nglm(Candy ~ Coins + Small, data = Offering, family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   -17.0       7.80     -2.18  0.0296\n#> 2 Coins           3.49      1.75      1.99  0.0461\n#> 3 Small          -3.04      1.57     -1.93  0.0531"},{"path":"logistic-regression.html","id":"logstep","chapter":"5 Logistic Regression","heading":"5.6 Model Building","text":"Example 5.8  Suppose take exam covers 100 different topics, know . rules, however, state can bring two classmates consultants. Suppose also know topics classmates familiar . bring one consultant, easy figure bring: one knows topics (variable associated answer). Let’s say Sage knows 85 topics. two consultants might choose Sage first, second option, seems reasonable choose second knowledgeable classmate (second highly associated variable), example Bruno, knows 75 topics.problem strategy may 75 subjects Bruno knows already included 85 Sage knows, therefore, Bruno provide knowledge beyond Sage.better strategy select second considering know regarding entire agenda, looking person knows topics first know (variable best explains residual equation variables entered). may even happen best pair consultants knowledgeable, may two complement perfectly way one knows 55 topics knows remaining 45, knowledgeable complement anybody.forward selection, two chosen?1With backward selection, two chosen?2","code":""},{"path":"logistic-regression.html","id":"formal-model-building","chapter":"5 Logistic Regression","heading":"5.6.1 Formal Model Building","text":"going discuss add (subtract) variables model. , can define two criteria used suggesting optimal model.AIC: Akaike’s Information Criteria = \\(-2 \\ln\\) likelihood + \\(2p\\)\nBIC: Bayesian Information Criteria = \\(-2 \\ln\\) likelihood \\(+p \\ln(n)\\)techniques suggest choosing model smallest AIC BIC value; adjust number parameters model likely select models fewer variables drop--deviance test.","code":""},{"path":"logistic-regression.html","id":"stepwise-regression","chapter":"5 Logistic Regression","heading":"5.6.1.1 Stepwise Regression","text":"done previously, can add remove variables based deviance. Recall, comparing two nested models, differences deviances can modeled \\(\\chi^2_\\nu\\) variable \\(\\nu = \\Delta p\\).Consider data described book (page 30); variable description also given book website http://www.epibiostat.ucsf.edu/biostat/vgsm/data/hersdata.codebook.txtFor now, try predict whether individuals medical condition, medcond (defined pre-existing self-reported medical condition). use variables age, weight, diabetes drinkany.","code":"\nglm(medcond ~ age, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -1.60     0.401       -4.00 0.0000624\n#> 2 age           0.0162   0.00597      2.71 0.00664\nglm(medcond ~ age + weight, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept) -2.17      0.496       -4.37 0.0000124\n#> 2 age          0.0189    0.00613      3.09 0.00203  \n#> 3 weight       0.00528   0.00274      1.93 0.0542\nglm(medcond ~ age+diabetes, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 3 x 5\n#>   term        estimate std.error statistic      p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n#> 1 (Intercept)  -1.89     0.408       -4.64 0.00000349  \n#> 2 age           0.0185   0.00603      3.07 0.00217     \n#> 3 diabetes      0.487    0.0882       5.52 0.0000000330\nglm(medcond ~ age*diabetes, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic     p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>       <dbl>\n#> 1 (Intercept)   -2.52     0.478       -5.26 0.000000141\n#> 2 age            0.0278   0.00707      3.93 0.0000844  \n#> 3 diabetes       2.83     0.914        3.10 0.00192    \n#> 4 age:diabetes  -0.0354   0.0137      -2.58 0.00986\nglm(medcond ~ age*drinkany, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)  -0.991     0.511       -1.94  0.0526\n#> 2 age           0.00885   0.00759      1.17  0.244 \n#> 3 drinkany     -1.44      0.831       -1.73  0.0833\n#> 4 age:drinkany  0.0168    0.0124       1.36  0.175\n\n\nglm(medcond ~ age + weight + diabetes + drinkany, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 5 x 5\n#>   term        estimate std.error statistic    p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n#> 1 (Intercept) -1.87      0.505      -3.72  0.000203  \n#> 2 age          0.0184    0.00620     2.96  0.00304   \n#> 3 weight       0.00143   0.00285     0.500 0.617     \n#> 4 diabetes     0.432     0.0924      4.68  0.00000288\n#> 5 drinkany    -0.253     0.0835     -3.03  0.00248\nglm(medcond ~ age , data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 (Intercept)  -1.60     0.401       -4.00 0.0000624\n#> 2 age           0.0162   0.00597      2.71 0.00664\nglm(medcond ~ weight , data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept) -0.769     0.198       -3.88 0.000106\n#> 2 weight       0.00339   0.00267      1.27 0.204\nglm(medcond ~ diabetes , data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   -0.652    0.0467    -13.9  3.18e-44\n#> 2 diabetes       0.468    0.0878      5.34 9.55e- 8\nglm(medcond ~ drinkany, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)   -0.398    0.0498     -8.00 1.26e-15\n#> 2 drinkany      -0.330    0.0818     -4.04 5.46e- 5"},{"path":"logistic-regression.html","id":"forward-selection","chapter":"5 Logistic Regression","heading":"Forward Selection","text":"One idea start empty model adding best available variable iteration, checking needs transformations. also look interactions might suspect. However, looking possible interactions (2-way interactions, also consider 3-way interactions etc.), things can get hand quickly.start response variable versus variables find best predictor. many, might just look correlation matrix. However, may miss variables good predictors aren’t linearly related. Therefore, possible, scatter plot matrix best.locate best variable, regress response variable .variable seems useful, keep move looking second., stop.","code":""},{"path":"logistic-regression.html","id":"forward-stepwise-selection","chapter":"5 Logistic Regression","heading":"Forward Stepwise Selection","text":"method follows way Forward Regression, new variable enters model, check see variables already model can now removed. done specifying two values, \\(\\alpha_e\\) \\(\\alpha\\) level needed enter model, \\(\\alpha_l\\) \\(\\alpha\\) level needed leave model. require \\(\\alpha_e<\\alpha_l\\), otherwise, algorithm cycle, add variable, immediately decide delete , continuing ad infinitum. bad.start empty model, add best predictor, assuming p-value associated smaller \\(\\alpha_e\\).Now, find best remaining variables, add p-value smaller \\(\\alpha_e\\). add , also check see first variable can dropped, calculating p-value associated (different first time, now two variables model). p-value greater \\(\\alpha_l\\), remove variable.continue process variables meet either requirements. many situations, help us stopping less desirable model.choose \\(\\alpha\\) values? set \\(\\alpha_e\\) small, might walk away variables model, least many. set large, wander around , good thing, explore models, may end variables model aren’t necessary.","code":""},{"path":"logistic-regression.html","id":"backward-selection","chapter":"5 Logistic Regression","heading":"Backward Selection","text":"Start full model including every term (possibly every interaction, etc.).Remove variable least significant (biggest p-value) model.Continue removing variables variables significant chosen \\(\\alpha\\) level.big model (interaction terms) deviance 3585.7; additive model deviance 3594.8.\\[\\begin{align}\nG &= 3594.8 - 3585.7= 9.1\\\\\np-value &= P(\\chi^2_6 \\geq 9.1)= 1 - pchisq(9.1, 6) = 0.1680318\n\\end{align}\\]\nreject null hypothesis, know don’t need 6 interaction terms. Next check whether need weight.additive model deviance 3594.8; model without weight 3597.3.\\[\\begin{align}\nG &= 3597.3 - 3594.8 =2.5\\\\\np-value &= P(\\chi^2_1 \\geq 2.5)= 1 - pchisq(2.5, 1) = 0.1138463\n\\end{align}\\]\nreject null hypothesis, know don’t need weight model either.","code":"\nglm(medcond ~ (age + diabetes + weight + drinkany)^2, data = HERS, family=\"binomial\") %>% glance()\n#> # A tibble: 1 x 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3643.    2758 -1793. 3608. 3673.    3586.        2748  2759\nglm(medcond ~ age + diabetes + weight + drinkany, data = HERS, family=\"binomial\") %>% glance()\n#> # A tibble: 1 x 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3643.    2758 -1797. 3605. 3634.    3595.        2754  2759\nglm(medcond ~ age + diabetes + drinkany, data = HERS, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term        estimate std.error statistic     p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>       <dbl>\n#> 1 (Intercept)  -1.72     0.413       -4.17 0.0000300  \n#> 2 age           0.0176   0.00605      2.90 0.00369    \n#> 3 diabetes      0.442    0.0895       4.94 0.000000786\n#> 4 drinkany     -0.252    0.0834      -3.01 0.00257"},{"path":"logistic-regression.html","id":"getting-the-model-right","chapter":"5 Logistic Regression","heading":"5.6.2 Getting the Model Right","text":"terms selecting variables model particular response, four things can happen:logistic regression model correct!logistic regression model underspecified.logistic regression model contains extraneous variables.logistic regression model overspecified.","code":""},{"path":"logistic-regression.html","id":"underspecified","chapter":"5 Logistic Regression","heading":"Underspecified","text":"regression model underspecified missing one important predictor variables. underspecified worst case scenario model ends biased predictions wrong virtually every observation. (Think Simpson’s Paradox need interaction.)","code":""},{"path":"logistic-regression.html","id":"extraneous","chapter":"5 Logistic Regression","heading":"Extraneous","text":"third type variable situation comes extra variables included model variables neither related response correlated explanatory variables. Generally, extraneous variables problematic produce models unbiased coefficient estimators, unbiased predictions, unbiased variance estimates. worst thing happens error degrees freedom lowered makes confidence intervals wider p-values bigger (lower power). Also problematic model becomes unnecessarily complicated harder interpret.","code":""},{"path":"logistic-regression.html","id":"overspecified","chapter":"5 Logistic Regression","heading":"Overspecified","text":"model overspecified, one redundant variables. , variables contain information variables (.e., correlated!). ’ve seen, correlated variables cause trouble inflate variance coefficient estimates. correlated variables still possible get unbiased prediction estimates, coefficients variable interpreted (can inference easily performed).Generally: idea use model building strategy criteria (\\(\\chi^2\\)-tests, AIC, BIC, ROC, AUC) find middle ground underspecified model overspecified model.","code":""},{"path":"logistic-regression.html","id":"one-model-building-strategy","chapter":"5 Logistic Regression","heading":"5.6.2.1 One Model Building Strategy","text":"Taken https://onlinecourses.science.psu.edu/stat501/node/332.Model building definitely “art.” Unsurprisingly, many approaches model building, one strategy, consisting seven steps, commonly used building regression model.","code":""},{"path":"logistic-regression.html","id":"the-first-step","chapter":"5 Logistic Regression","heading":"The first step","text":"Decide type model needed order achieve goals study. general, five reasons one might want build regression model. :predictive reasons - , model used predict response variable chosen set predictors.theoretical reasons - , researcher wants estimate model based known theoretical relationship response predictors. example, may known physical relationship (parameters esimate) differential equation models state changes.control purposes - , model used control response variable manipulating values predictor variables.inferential reasons - , model used explore strength relationships response predictors.data summary reasons - , model used merely way summarize large set data single equation.","code":""},{"path":"logistic-regression.html","id":"the-second-step","chapter":"5 Logistic Regression","heading":"The second step","text":"Decide explanatory variables response variable collect data. Collect data.","code":""},{"path":"logistic-regression.html","id":"the-third-step","chapter":"5 Logistic Regression","heading":"The third step","text":"Explore data. :univariate basis, check outliers, gross data errors, missing values.Study bivariate relationships reveal outliers, suggest possible transformations, identify possible multicollinearities.can’t possibly -emphasize data exploration step. ’s data analyst hasn’t made mistake skipping step later regretting data point found error, thereby nullifying hours work.","code":""},{"path":"logistic-regression.html","id":"the-fourth-step","chapter":"5 Logistic Regression","heading":"The fourth step","text":"(fourth step good modeling practice. gives sense whether ’ve overfit model building process.) Randomly divide data training set validation set:training set, least 15-20 error degrees freedom, used estimate model.validation set used cross-validation fitted model.","code":""},{"path":"logistic-regression.html","id":"the-fifth-step","chapter":"5 Logistic Regression","heading":"The fifth step","text":"Using training set, identify several candidate models:Use best subsets regression.Use stepwise regression, course yields one model unless different alpha--remove alpha--enter values specified.","code":""},{"path":"logistic-regression.html","id":"the-sixth-step","chapter":"5 Logistic Regression","heading":"The sixth step","text":"Select evaluate “good” models:Select models based criteria learned, well number nature predictors.Evaluate selected models violation model conditions.none models provide satisfactory fit, try something else, collecting data, identifying different predictors, formulating different type model.","code":""},{"path":"logistic-regression.html","id":"the-seventh-and-final-step","chapter":"5 Logistic Regression","heading":"The seventh and final step","text":"Select final model:large cross-validation AUC validation data indicative good predictive model (population interest).Consider false positive rate, false negative rate, outliers, parsimony, relevance, ease measurement predictors., , don’t forget necessarily one good model given set data. might equally satisfactory models.","code":""},{"path":"logistic-regression.html","id":"another-model-building-strategy","chapter":"5 Logistic Regression","heading":"5.6.2.2 Another Model Building Strategy","text":"\nFigure 5.1: Another strategy model building. Figure taken (Ramsey Schafer 2012).\n","code":""},{"path":"logistic-regression.html","id":"model-assessment","chapter":"5 Logistic Regression","heading":"5.7 Model Assessment","text":"","code":""},{"path":"logistic-regression.html","id":"measures-of-association","chapter":"5 Logistic Regression","heading":"5.7.1 Measures of Association","text":"logistic regression, don’t residuals, don’t value like \\(R^2\\). can, however, measure whether estimated model consistent data. , model able discriminate successes failures.’d like choose model produces high probabilities success observations success recorded low probabilities success observation failure recorded. words, want mostly concordant pairs.Given particular pair observations one success failure, observation corresponding success higher probability success observation corresponding failure, call pair concordant. observation corresponding success lower probability success observation corresponding failure, call pair discordant. Tied pairs occur observed success estimated probability observed failure.","code":""},{"path":"logistic-regression.html","id":"back-to-the-burn-data-refexburnexamp","chapter":"5 Logistic Regression","heading":"5.7.1.1 Back to the burn data 5.1.0.1:","text":"Consider looking pairs successes failures burn data, 308 survivors 127 deaths = 39,116 pairs individuals.One pair individuals burn areas 1.75 2.35.\n\\[\\begin{align}\np(x=1.75) &= \\frac{e^{22.7083-10.6624\\cdot 1.75}}{1+e^{22.7083 -10.6624\\cdot 1.75}} = 0.983\\\\\np(x=2.35) &= \\frac{e^{22.7083-10.6624\\cdot 2.35}}{1+e^{22.7083 -10.6624\\cdot 2.35}} = 0.087\n\\end{align}\\]\npairs concordant first individual survived second didn’t. pairs discordant first individual died second survived.Ideally model chosen large number concordant pairs. following metrics quantify concordance across entire model respect observed data:\\(D_{xy}\\): Somers’ D number concordant pairs minus number discordant pairs divided total number pairs.gamma: Goodman-Kruskal gamma number concordant pairs minus number discordant pairs divided total number pairs excluding ties.tau-: Kendall’s tau-number concordant pairs minus number discordant pairs divided total number pairs people (including pairs survived died).summary contains following elements:number observations used fit, maximum absolute value first derivative log likelihood, model likelihood ratio chi2, d.f., P-value, \\(c\\) index (area ROC curve), Somers’ Dxy, Goodman-Kruskal gamma, Kendall’s tau-rank correlations predicted probabilities observed response, Nagelkerke \\(R^2\\) index, Brier score computed respect Y \\(>\\) lowest level, \\(g\\)-index, \\(gr\\) (\\(g\\)-index odds ratio scale), \\(gp\\) (\\(g\\)-index probability scale using cutoff used Brier score).","code":"\n# install.packages(c(\"Hmisc\", \"rms\"))\n\nlibrary(rms)   # you need this line!!\nburn.glm <- lrm(burnresp~burnexpl, data = burnglm)\nprint(burn.glm)\n#> Logistic Regression Model\n#>  \n#>  lrm(formula = burnresp ~ burnexpl, data = burnglm)\n#>  \n#>                         Model Likelihood    Discrimination    Rank Discrim.    \n#>                               Ratio Test           Indexes          Indexes    \n#>  Obs           435    LR chi2     190.15    R2       0.505    C       0.877    \n#>   0            127    d.f.             1    g        2.576    Dxy     0.753    \n#>   1            308    Pr(> chi2) <0.0001    gr      13.146    gamma   0.824    \n#>  max |deriv| 8e-11                          gp       0.313    tau-a   0.312    \n#>                                             Brier    0.121                     \n#>  \n#>            Coef     S.E.   Wald Z Pr(>|Z|)\n#>  Intercept  22.7083 2.2661 10.02  <0.0001 \n#>  burnexpl  -10.6624 1.0826 -9.85  <0.0001 \n#> "},{"path":"logistic-regression.html","id":"roc","chapter":"5 Logistic Regression","heading":"5.7.2 Receiver Operating Characteristic Curves","text":"Recall logistic regression can used predict outcome binary event (response variable). Receiver Operating Characteristic (ROC) Curve graphical representation relationship betweentype error = FPtype II error = FNsensitivity = power = true positive rate (TPR) = TP / P = TP / (TP+FN)false positive rate (FPR) = FP / N = FP / (FP + TN)specificity = 1 - FPR = TN / (FP + TN)accuracy (acc) = (TP+TN) / (P+N)positive predictive value (PPV) = precision = TP / (TP + FP)negative predictive value (NPV) = TN / (TN + FN)false discovery rate = 1 - PPV = FP / (FP + TP)Example 5.9  example: consider pair individuals burn areas 1.75 2.35.\n\\[\\begin{align}\np(x=1.75) &= \\frac{e^{22.7083-10.6624\\cdot 1.75}}{1+e^{22.7083 -10.6624\\cdot 1.75}} = 0.983\\\\\np(x=2.35) &= \\frac{e^{22.7083-10.6624\\cdot 2.35}}{1+e^{22.7083 -10.6624\\cdot 2.35}} = 0.087\\\\\nx &= \\mbox{log area burned}\n\\end{align}\\]\nvalue assign 1.75 2.35 15 log(area) burned? changing cutoff, can fit entire curve. want curve far upper left corner possible (sensitivity = 1, specificity = 1). Notice color band represents probability cutoff predicting ``success.\"\nFigure 5.2: ROC curve. Color indicates probability cutoff used determine predictions.\n: Let’s say use prob=0.25 cutoff:\\[\\begin{align}\n\\mbox{sensitivity} &= TPR = 300/308 = 0.974\\\\\n\\mbox{specificity} &= 61 / 127 = 0.480, \\mbox{1 - specificity} =  FPR = 0.520\\\\\n\\end{align}\\]B: Let’s say use prob=0.7 cutoff:\\[\\begin{align}\n\\mbox{sensitivity} &= TPR = 265/308 = 0.860\\\\\n\\mbox{specificity} &= 92/127 = 0.724, \\mbox{1 - specificity} = FPR = 0.276\\\\\n\\end{align}\\]C: Let’s say use prob=0.9 cutoff:\\[\\begin{align}\n\\mbox{sensitivity} &= TPR = 144/308 = 0.467\\\\\n\\mbox{specificity} &= 120/127 = 0.945, \\mbox{1 - specificity} = FPR = 0.055\\\\\n\\end{align}\\]D: models go (0,0) \\(\\rightarrow\\) predict everything negative, prob=1 cutoffE: models go (1,1) \\(\\rightarrow\\) predict everything positive, prob=0 cutoffF: model gives perfect sensitivity (FN!) specificity (FP)G: random guessing. classifier randomly guess, get half positives correct half negatives correct. guesses 90% positives correctly, also guess 90% negatives positive.H: worse random guessing. Note opposite classifier (H) might quite good!","code":""},{"path":"logistic-regression.html","id":"cv","chapter":"5 Logistic Regression","heading":"5.7.3 Cross Validation","text":"reading notes , look following visualization. Don’t worry building model (classification trees topic class), check end talk predicting test training data.http://www.r2d3.us/visual-intro--machine-learning-part-1/","code":""},{"path":"logistic-regression.html","id":"overfitting","chapter":"5 Logistic Regression","heading":"5.7.3.1 Overfitting","text":"Imagine preparing statistics exam. Helpfully, Professor Hardin made previous exam papers worked answers available online. begin trying answer questions previous papers comparing answers model answers provided. Unfortunately, get carried away spend time memorizing model answers past questions.Now, upcoming exam completely consists past questions, certain well. new exam asks different questions material, ill-prepared get much lower mark traditional preparation. case, one say overfitting past exam papers knowledge gained didn’t generalize future exam questions.","code":""},{"path":"logistic-regression.html","id":"cv-model-assessment","chapter":"5 Logistic Regression","heading":"5.7.3.2 CV Model Assessment","text":"Cross validation commonly used perform two different tasks:\n1. assess model’s accuracy (model assessment).\n2. build model (model selection).focus model assessment.Suppose build classifier (logistic regression model) given data set. ’d like know well model classifies observations, test samples hand, error rate much lower model’s inherent accuracy rate. Instead, ’d like predict new observations used create model. various ways creating test validation sets data:one training set, one test set [two drawbacks: estimate error highly variable depends points go training set; training data set smaller full data set, error rate biased way overestimates actual error rate modeling technique.]leave one cross validation (LOOCV) [LOOCV special case \\(k\\)-fold CV \\(k=n\\)]remove one observationbuild model using remaining n-1 pointspredict class membership observation removedrepeat removing observation one time (time consuming keep building models)\\(k\\)-fold cross validation (\\(k\\)-fold CV)\nlike LOOCV except algorithm run \\(k\\) times group (approximately equal size) partition data set.\nadvantage \\(k\\)-fold computational\n\\(k\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher. \\(k\\)-fold, prediction \\(n\\) values \\(k\\) models much less correlated. effect average predicted values way less variability data set data set.\nlike LOOCV except algorithm run \\(k\\) times group (approximately equal size) partition data set.advantage \\(k\\)-fold computational\\(k\\)-fold often better bias-variance trade-[bias lower LOOCV. however, LOOCV predicts \\(n\\) observations \\(n\\) models basically , variability higher. \\(k\\)-fold, prediction \\(n\\) values \\(k\\) models much less correlated. effect average predicted values way less variability data set data set.\nFigure 5.3: 4-fold CV depicted. Notice holdout group never used part coefficient estimation process.\n","code":""},{"path":"logistic-regression.html","id":"birdexamp","chapter":"5 Logistic Regression","heading":"5.8 R: Birdnest Example","text":"Length Bird Nest example problem E1 text includes 99 species N. American passerine birds. Recall response variable binary represents whether small opening (closed=1) large opening (closed=0) nest. explanatory variable interest length bird.","code":""},{"path":"logistic-regression.html","id":"drop-in-deviance-likelihood-ratio-test-lrt","chapter":"5 Logistic Regression","heading":"5.8.1 Drop-in-deviance (Likelihood Ratio Test, LRT)","text":"\\(\\chi^2\\): Likelihood ratio test also tests whether response explained explanatory variable. can output deviance ( = K - 2 * log-likelihood) full (maximum likelihood!) reduced (null) models.\n\\[\\begin{align}\nG &= 2 \\cdot \\ln(L(MLE)) - 2 \\cdot \\ln(L(null))\\\\\n&= \\mbox{null (restricted) deviance - residual (full model) deviance}\\\\\nG &\\sim \\chi^2_{\\nu} \\ \\ \\ \\mbox{null hypothesis true}\n\\end{align}\\]\n\\(\\nu\\) represents difference number parameters needed estimate full model versus null model.","code":"\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   0.457     0.753      0.607   0.544\n#> 2 Length       -0.0677    0.0425    -1.59    0.112\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% glance() %>% \n  print.data.frame(digits=6)\n#>   null.deviance df.null   logLik    AIC     BIC deviance df.residual nobs\n#> 1       119.992      94 -58.4399 120.88 125.987   116.88          93   95"},{"path":"logistic-regression.html","id":"difference-between-tidy-and-augment-and-glance","chapter":"5 Logistic Regression","heading":"5.8.2 Difference between tidy and augment and glance","text":"Note tidy contains number rows number coefficients. augment contains number rows number observations. glance always one row (containing overall model information).","code":"\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   0.457     0.753      0.607   0.544\n#> 2 Length       -0.0677    0.0425    -1.59    0.112\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% augment()\n#> # A tibble: 95 x 9\n#>   .rownames `Closed?` Length .fitted .resid .std.resid   .hat .sigma .cooksd\n#>   <chr>         <dbl>  <dbl>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>   <dbl>\n#> 1 1                 0   20    -0.896 -0.827     -0.833 0.0137   1.12 0.00288\n#> 2 2                 1   20    -0.896  1.57       1.58  0.0137   1.11 0.0173 \n#> 3 4                 1   20    -0.896  1.57       1.58  0.0137   1.11 0.0173 \n#> 4 5                 1   22.5  -1.07   1.65       1.67  0.0202   1.11 0.0305 \n#> 5 6                 0   18.5  -0.795 -0.863     -0.868 0.0116   1.12 0.00267\n#> 6 7                 1   17    -0.693  1.48       1.49  0.0110   1.12 0.0112 \n#> # … with 89 more rows\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% glance() %>% \n  print.data.frame(digits=6)\n#>   null.deviance df.null   logLik    AIC     BIC deviance df.residual nobs\n#> 1       119.992      94 -58.4399 120.88 125.987   116.88          93   95"},{"path":"logistic-regression.html","id":"looking-at-variables-in-a-few-different-ways.","chapter":"5 Logistic Regression","heading":"5.8.3 Looking at variables in a few different ways.","text":"Length continuous explanatory variable:Length categorical explanatory variables:Length plus explanatory variables:","code":"\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   0.457     0.753      0.607   0.544\n#> 2 Length       -0.0677    0.0425    -1.59    0.112\nglm(`Closed?` ~ Length, data = nests, family=\"binomial\") %>% glance() %>%\n  print.data.frame(digits=6)\n#>   null.deviance df.null   logLik    AIC     BIC deviance df.residual nobs\n#> 1       119.992      94 -58.4399 120.88 125.987   116.88          93   95\nglm(`Closed?` ~ as.factor(Length), data = nests, family=\"binomial\") %>% tidy()\n#> # A tibble: 34 x 5\n#>   term                       estimate std.error statistic p.value\n#>   <chr>                         <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)            19.6            10754.  1.82e- 3   0.999\n#> 2 as.factor(Length)10     0.000000432    13171.  3.28e-11   1.00 \n#> 3 as.factor(Length)10.5   0.000000430    15208.  2.82e-11   1.00 \n#> 4 as.factor(Length)11   -18.9            10754. -1.75e- 3   0.999\n#> 5 as.factor(Length)12   -21.2            10754. -1.97e- 3   0.998\n#> 6 as.factor(Length)12.5   0.000000431    15208.  2.83e-11   1.00 \n#> # … with 28 more rows\nglm(`Closed?` ~ as.factor(Length), data = nests, family=\"binomial\") %>% glance() %>%\n  print.data.frame(digits=6)\n#>   null.deviance df.null   logLik     AIC     BIC deviance df.residual nobs\n#> 1       119.992      94 -36.8776 141.755 228.587  73.7552          61   95\nglm(`Closed?` ~ Length + Incubate +  Color, data = nests, family=\"binomial\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   -2.64     2.06      -1.28   0.201 \n#> 2 Length        -0.114    0.0527    -2.17   0.0302\n#> 3 Incubate       0.314    0.172      1.82   0.0684\n#> 4 Color         -0.420    0.609     -0.690  0.490\nglm(`Closed?` ~ Length + Incubate +  Color, data = nests, family=\"binomial\") %>% glance() %>% \n  print.data.frame(digits=6)\n#>   null.deviance df.null   logLik     AIC     BIC deviance df.residual nobs\n#> 1       110.086      87 -51.6633 111.327 121.236  103.327          84   88"},{"path":"logistic-regression.html","id":"predicting-response","chapter":"5 Logistic Regression","heading":"5.8.4 Predicting Response","text":"","code":"\nbird_glm <- glm(`Closed?` ~ Length, data = nests, family=\"binomial\")\nbird_glm %>% tidy()\n#> # A tibble: 2 x 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   0.457     0.753      0.607   0.544\n#> 2 Length       -0.0677    0.0425    -1.59    0.112\n\n# predicting the linear part:\n# reasonable to use the SE to create CIs\npredict(bird_glm, newdata = list(Length = 47), se.fit = TRUE, type = \"link\")\n#> $fit\n#>     1 \n#> -2.72 \n#> \n#> $se.fit\n#> [1] 1.3\n#> \n#> $residual.scale\n#> [1] 1\n\n# predicting the probability of success (on the `scale` of the response variable):\n# do NOT use the SE to create a CI for the predicted value\n# instead, use the SE from `type=\"link\" ` and transform the interval\npredict(bird_glm, newdata = list(Length = 47), se.fit = TRUE, type = \"response\")\n#> $fit\n#>      1 \n#> 0.0616 \n#> \n#> $se.fit\n#>      1 \n#> 0.0751 \n#> \n#> $residual.scale\n#> [1] 1"},{"path":"logistic-regression.html","id":"measues-of-association","chapter":"5 Logistic Regression","heading":"5.8.5 Measues of association","text":"","code":"\n# install.packages(c(\"Hmisc\", \"rms\"))\n\nlibrary(rms)   # you need this line!!\nbird_lrm <- lrm(`Closed?` ~ Length, data = nests)\nprint(bird_lrm)\n#> Frequencies of Missing Values Due to Each Variable\n#> Closed?  Length \n#>       0       4 \n#> \n#> Logistic Regression Model\n#>  \n#>  lrm(formula = `Closed?` ~ Length, data = nests)\n#>  \n#>  \n#>                        Model Likelihood    Discrimination    Rank Discrim.    \n#>                              Ratio Test           Indexes          Indexes    \n#>  Obs            95    LR chi2      3.11    R2       0.045    C       0.638    \n#>   0             64    d.f.            1    g        0.455    Dxy     0.276    \n#>   1             31    Pr(> chi2) 0.0777    gr       1.576    gamma   0.288    \n#>  max |deriv| 2e-07                         gp       0.088    tau-a   0.123    \n#>                                            Brier    0.210                     \n#>  \n#>            Coef    S.E.   Wald Z Pr(>|Z|)\n#>  Intercept  0.4571 0.7530  0.61  0.5438  \n#>  Length    -0.0677 0.0425 -1.59  0.1117  \n#> "},{"path":"logistic-regression.html","id":"roc-curves","chapter":"5 Logistic Regression","heading":"5.8.6 ROC curves","text":"","code":"\nlibrary(plotROC)\n\nnests <- nests %>%\n  mutate(Closed = as.factor(ifelse(`Closed?` == 0, \"no\", \n                                   ifelse(`Closed?` == 1, \"yes\", `Closed?`))))\n\nbird_glm <- glm(Closed ~ Length, data = nests, family=\"binomial\")\n\nbird_indiv <- bird_glm %>% \n  augment(type.predict = \"response\") \n\nhead(bird_indiv)\n#> # A tibble: 6 x 9\n#>   .rownames Closed Length .fitted .resid .std.resid   .hat .sigma .cooksd\n#>   <chr>     <fct>   <dbl>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>   <dbl>\n#> 1 1         no       20     0.290 -0.827     -0.833 0.0137   1.12 0.00288\n#> 2 2         yes      20     0.290  1.57       1.58  0.0137   1.11 0.0173 \n#> 3 4         yes      20     0.290  1.57       1.58  0.0137   1.11 0.0173 \n#> 4 5         yes      22.5   0.256  1.65       1.67  0.0202   1.11 0.0305 \n#> 5 6         no       18.5   0.311 -0.863     -0.868 0.0116   1.12 0.00267\n#> 6 7         yes      17     0.333  1.48       1.49  0.0110   1.12 0.0112\n\nbird_cv_plot <- bird_indiv %>%\n  ggplot() + \n  geom_roc(aes(d = Closed, m = .fitted)) +\n  geom_abline(intercept = 0, slope = 1)\n\nbird_cv_plot\n\ncalc_auc(bird_cv_plot)\n#>   PANEL group   AUC\n#> 1     1    -1 0.638"},{"path":"logistic-regression.html","id":"cross-validation-on-nest-data","chapter":"5 Logistic Regression","heading":"5.8.7 Cross Validation on nest data","text":"Note syntax slightly different ’ve seen previously. caret plotROC packages:train() partition data 4 groups run logistic regression separatelygeom_roc() plot ROC curve partitionWe might try whole thing using model one variable. CV values accuracy get better?","code":"\nlibrary(caret)\nlibrary(plotROC)\n\nbird_cv <- train(Closed ~ Length + Incubate + Nestling,\n                 data = nests,\n                 na.action = na.omit,\n                 method = \"glm\", \n                 family = \"binomial\",\n                 trControl = trainControl(method = \"cv\", number = 4,\n                                          classProbs = TRUE,\n                                          savePredictions = TRUE))\n\nbird_cv\n#> Generalized Linear Model \n#> \n#> 99 samples\n#>  3 predictor\n#>  2 classes: 'no', 'yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (4 fold) \n#> Summary of sample sizes: 64, 65, 65, 64 \n#> Resampling results:\n#> \n#>   Accuracy  Kappa\n#>   0.744     0.39\nbird_cv$pred %>% head()\n#>   pred obs     no    yes rowIndex parameter Resample\n#> 1   no yes 0.7817 0.2183        2      none    Fold1\n#> 2   no  no 0.6644 0.3356        8      none    Fold1\n#> 3   no  no 0.9808 0.0192        9      none    Fold1\n#> 4  yes yes 0.0338 0.9662       12      none    Fold1\n#> 5  yes  no 0.1455 0.8545       14      none    Fold1\n#> 6   no  no 0.9729 0.0271       15      none    Fold1\n\nbird_cv_plot <- bird_cv$pred %>%\n  ggplot() + \n  geom_roc(aes(m = yes, d = obs, color = Resample))\n\n\nbird_cv_plot +  \n  geom_abline(slope = 1, intercept = 0) +\n  geom_roc(aes(m = yes, d = obs), color = \"black\")\n\ncalc_auc(bird_cv_plot)\n#>   PANEL group   AUC\n#> 1     1     1 0.686\n#> 2     1     2 0.847\n#> 3     1     3 0.806\n#> 4     1     4 0.876\nbird_cv_length <- train(Closed ~ Length,\n                 data = nests,\n                 na.action = na.omit,\n                 method = \"glm\", \n                 family = \"binomial\",\n                 trControl = trainControl(method = \"cv\", number = 4,\n                                          classProbs = TRUE,\n                                          savePredictions = TRUE))\n\nbird_cv_length\n#> Generalized Linear Model \n#> \n#> 99 samples\n#>  1 predictor\n#>  2 classes: 'no', 'yes' \n#> \n#> No pre-processing\n#> Resampling: Cross-Validated (4 fold) \n#> Summary of sample sizes: 72, 71, 71, 71 \n#> Resampling results:\n#> \n#>   Accuracy  Kappa\n#>   0.684     0.04\nbird_cv_length$pred %>% head()\n#>   pred obs    no   yes rowIndex parameter Resample\n#> 1   no  no 0.706 0.294        1      none    Fold1\n#> 2   no yes 0.706 0.294        3      none    Fold1\n#> 3   no  no 0.620 0.380        8      none    Fold1\n#> 4   no yes 0.638 0.362       11      none    Fold1\n#> 5   no yes 0.706 0.294       14      none    Fold1\n#> 6   no  no 0.800 0.200       19      none    Fold1\n\nbird_cv_length_plot <- bird_cv_length$pred %>%\n  ggplot() + \n  geom_roc(aes(m = yes, d = obs, color = Resample))\n\n\nbird_cv_length_plot +  \n  geom_abline(slope = 1, intercept = 0) +\n  geom_roc(aes(m = yes, d = obs), color = \"black\")\n\ncalc_auc(bird_cv_length_plot)\n#>   PANEL group   AUC\n#> 1     1     1 0.509\n#> 2     1     2 0.750\n#> 3     1     3 0.781\n#> 4     1     4 0.574"},{"path":"logistic-regression.html","id":"drawing-interactions","chapter":"5 Logistic Regression","heading":"5.8.8 Drawing interactions","text":"https://interactions.jacob-long.com/index.html","code":""},{"path":"survival-analysis.html","id":"survival-analysis","chapter":"6 Survival Analysis","heading":"6 Survival Analysis","text":"motivate technical details vital understanding survival analysis, consider following example (Gerds 2016).Example 6.1  class – experience Titanic going .Titanic sinking. long can hold breath?Every person sinking also time keeper (number seconds sinker can hold breath).Titanic sinking slowly, participants go water asynchronously (.e., different times).Accrual period last minute.say “stop” (another minute), everyone end time clocks (end follow-period).participant recorded time well indicator whether died.Based data, like calculate:\n1. probability surviving 40 seconds?\n2. median survival time?\n3. average survival time?next example given text (Kuiper Sklar 2013 (chapter 9)) part motivation survival analysis.Example 6.2  Class chocolate melting activityEach student randomly assigned white milk chocolate chip (flip coin)instructor gives approval, students place either white milk chocolate chip mouths record time completely melts.Treat study done specified period time (may require experimenting, 60 seconds worked well). actual time less 60 seconds, actual time complete. student submit data (chip color, actual time, censoring status); 1=observed, 0=censored. chips “swallowed” prior 60 seconds regarded censored.Survival analysis typically done prospective study (see censoring ) cohort patients (observational experimental) determine clinical outcome. also usually measure covariates interest: treatment, clinical variables measured recruitment, etc.Key Point: outcome interest (death, recurrence, etc.) may occurPossible responses:Outcome interest occursOutcome interest occursStudy endsStudy endsThe individual can longer measured (moves away, etc.)individual can longer measured (moves away, etc.)response fate length follow-upresponse fate length follow-updata suppose following n patients\\[\n\\begin{align*}\nt_i &= \\mbox{time } ^{th} \\mbox{ event interest}\\\\\nm(t) &= \\mbox{number patients } t_i > t \\mbox{ (die\nlater)}\\\\\nd(t) &= \\mbox{number patients } t_i \\leq t \\mbox{\n(die sooner)}\\\\\n\\end{align*}\n\\]data suppose following n patients\\[\n\\begin{align*}\nt_i &= \\mbox{time } ^{th} \\mbox{ event interest}\\\\\nm(t) &= \\mbox{number patients } t_i > t \\mbox{ (die\nlater)}\\\\\nd(t) &= \\mbox{number patients } t_i \\leq t \\mbox{\n(die sooner)}\\\\\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"timedata","chapter":"6 Survival Analysis","heading":"6.1 Time-to-event data","text":"","code":""},{"path":"survival-analysis.html","id":"survival-function","chapter":"6 Survival Analysis","heading":"6.1.1 Survival Function","text":"two main quantities interest (.e., parameters) :\\[\n\\begin{align*}\nS(t) &= P(T > t) = \\mbox{ probability surviving time } t\\\\\nD(t) &= P(T \\leq t) = \\mbox{ probability dying time } t \\ (= F(t))\\\\\n\\end{align*}\n\\] \\(T\\) random variable representing time event; t number.\\(t_i\\) known (censoring), can estimate \\(S(t)\\) \\(D(t)\\) \\[\n\\begin{align*}\n\\hat{S}(t)_E &= m(t)/n = \\mbox{ proportion alive time } t\\\\\n\\hat{D}(t)_E &= d(t)/n = \\mbox{ proportion died time } t\\\\\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"censoring","chapter":"6 Survival Analysis","heading":"6.1.1.1 censoring","text":"right censoring: observation individual begins defined starting time ends outcome interest happens (censoring model)left censoring: outcome interest known occurred study begins (infection disease, learning count). Note event interest happened, unlike right censoring event interest happened.interval censoring; event interest known occurred two time points, precise time known.Important Assumption: survival time must independent mechanism causes censoring (called non-informative censoring). Censoring random: person censored probability dying non-censored people given explanatory variables \\(\\underline{X}\\).Said differently: within subgroup interest, subjects censored time \\(t\\) representative subjects subgroup remained risk time \\(t\\) respect survival experience.independent\nSubjects drop extremely ill\nSubjects drop adverse effects treatment regimen\nindependentSubjects drop extremely illSubjects drop adverse effects treatment regimenIndependent\nSubjects drop study ends\nSubjects drop move away\nIndependentSubjects drop study endsSubjects drop move awayExample 6.3  Suppose following melting times (seconds) milk chocolate chips 7 students maximum time allowed experiment 60 seconds:find estimated proportion chocolate chips melted 45 seconds use empirical survival function, \\(\\hat{S}(45)_E\\). \\[\n\\begin{align*}\n\\hat{S}(45)_E &= \\frac{\\mbox{number chips melted\n45 seconds}}{\\mbox{total number chips sample}}\\\\\n&= 2/7 = 0.286\\\\\n\\hat{S}(t)_E &= \\frac{\\mbox{number individuals yet experience\nevent time } t}{\\mbox{number individuals study}}\\\\\n&= \\frac{\\mbox{number event times greater } t}{\\mbox{number\nindividuals study}}\\\\\n\\end{align*}\n\\]observations incomplete (.e., censored)?One way deal censored observations remove study. consider sample observations complete information.\\[\n\\begin{align*}\n\\hat{S}(45)_E &= \\frac{\\mbox{number chips melted 45 seconds}}{\\mbox{total number chips sample}}\\\\\n&= 1/4 = 0.25\\\\\n\\end{align*}\n\\]treating censored observations complete, assume event times shorter actually (thereby underestimating true probability survival). removing censored observations data, lose information. treating censored observations complete bias estimate based remaining times; ignoring censored observations, reduce power inferential model.","code":""},{"path":"survival-analysis.html","id":"KM","chapter":"6 Survival Analysis","heading":"6.2 Kaplan-Meier Curves","text":"data set contains incomplete observations, best estimator survival function Kaplan-Meier estimator, \\(\\hat{S}(t)_{KM}\\). ’ll create curves hand, ’ll let R make us.\\[\n\\begin{align*}\nt_1 < t_2 < \\cdots < t_n & \\ \\mbox{ ordered completed times}\\\\\nn_i &= \\mbox{number patients known risk time (day) } t_i, \\mbox{ just } [t_i, t_{+1})\\\\\nd_i &= \\mbox{number patients die time (day) } t_i, \\mbox{ } [t_i, t_{+1})\\\\\n\\end{align*}\n\\]patients alive beginning \\(t_i^{th}\\) time, probability surviving time \\[p_i = \\frac{n_i - d_i}{n_i}\\] probability patient survives time 2 given survived time 1 \\[p_2 = \\frac{n_2 - d_2}{n_2}\\] probability (outset) patient survives time 2 : \\[\n\\begin{align*}\nP(T > t_2) &= P(T > t_2 | T > t_1) P(T > t_1)\\\\\n&= \\frac{n_1 - d_1}{n_1} \\frac{n_2 - d_2}{n_2}\n\\end{align*}\n\\] probability surviving first t days : \\[\n\\begin{align*}\n\\hat{S}(t)_{KM} &= \\prod_{:t_i < t} \\frac{n_i - d_i}{n_i}\\\\\n&= \\prod_{:t_i < t} p_i\\\\\n\\hat{D}(t)_{KM} &= 1 - \\hat{S}(t)_{KM}\\\\\n\\end{align*}\n\\] deaths time \\(t_i\\), \\((n_i -d_i) / n_i = 1\\).censoring time \\(t_i\\), \\(n_i - d_i = n_{+1}\\). Kaplan-Meier survival curve equivalent empirical survival curve:\\[\n\\begin{align*}\n\\hat{S}(t)_{KM} &= \\prod_{:t_i < t} \\frac{n_i - d_i}{n_i}\\\\\n&= \\frac{n_1 - d_1}{n_1} \\frac{n_2 - d_2}{n_2} \\cdots \\frac{n_k - d_k}{n_k}\\\\\n&= \\frac{m(t)}{n}\n\\end{align*}\n\\] ::: {.example} milk chocolate times hand.:::","code":""},{"path":"survival-analysis.html","id":"KMCI","chapter":"6 Survival Analysis","heading":"6.2.1 CI for KM curve","text":"Recall: \\(\\hat{S}(t)_{KM} = \\prod_{k:t_k < t} [ (n_k - d_k) / n_k]\\).\\(\\hat{S}(t)_{KM}\\) just product counts, can “easily” estimate SE: \\[\n\\begin{align*}\ns^2_{\\hat{S}(t)_{KM}} = \\hat{S}(t)_{KM}^2 \\sum_{k:t_k < t}\n\\frac{d_k}{n_k(n_k - d_k)}\n\\end{align*}\n\\]book uses SE normal theory work, particularly large sample sizes. However, smaller sample sizes ends survival curve (close zero one), sampling distribution estimated KM curve normal. Additionally try apply normal theory, ’ll get something meaningless CI go outside bounds 0 1. Fortunately, someone figured transformation us!\\(l l \\hat{S}(t) = \\ln (-\\ln \\hat{S}(t)_{KM})\\) much normal sampling distribution sampling distribution untransformed estimator \\(\\hat{S}(t)_{KM}\\).\\[\n\\begin{align*}\n\\hat{\\sigma}^2(t) &= SE( l l \\hat{S}(t))\\\\\n&= \\frac{\\sum_{k:t_k < t} \\frac{d_k}{n_k(n_k - d_k)}}{\\bigg[\\sum_{k:t_k < t} \\ln ( (n_k - d_k) / n_k) \\bigg]^2}\n\\end{align*}\n\\] 95 % CI \\(\\ln ( -\\ln S(t))\\) \\[ll\\hat{S}(t) \\pm 1.96 \\hat{\\sigma}(t) \\ \\ \\ \\mbox{(function \nt!!!)}\\]95 % CI \\(S(t)\\) \\[\\bigg(\\hat{S}(t)_{KM}^{\\exp(1.96\\hat{\\sigma}(t))},\n\\hat{S}(t)_{KM}^{\\exp(-1.96\\hat{\\sigma}(t))} \\bigg)\\]can also find CI \\(D(t) = 1- S(t)\\) using:\\[\n\\begin{align*}\n\\bigg(1 - \\hat{S}(t)_{KM}^{\\exp(-1.96\\hat{\\sigma}(t))}, 1 -\n\\hat{S}(t)_{KM}^{\\exp(1.96\\hat{\\sigma}(t))} \\bigg)\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"derivation-of-the-above-confidence-intervals.","chapter":"6 Survival Analysis","heading":"Derivation of the above confidence intervals.","text":"Remember \\(\\exp(\\cdot b) = (\\exp())^b\\), ’ll need . CI interval values captures true parameter 95 samples. Let’s say complementary log-log interval happens catch true value \\(S(t)\\). ,\\[\n\\begin{align*}\nll\\hat{S}(t) - 1.96 \\hat{\\sigma}(t) \\leq  & llS(t)  \\leq ll\\hat{S}(t) + 1.96 \\hat{\\sigma}(t) \\\\\n\\exp(ll\\hat{S}(t) - 1.96 \\hat{\\sigma}(t)) \\leq  & -lS(t)  \\leq \\exp(ll\\hat{S}(t) + 1.96 \\hat{\\sigma}(t)) \\\\\n-l\\hat{S}(t)\\exp(- 1.96 \\hat{\\sigma}(t))\\leq  &-lS(t)  \\leq -l\\hat{S}(t)\\exp(1.96 \\hat{\\sigma}(t)) \\\\\nl\\hat{S}(t)\\exp(- 1.96 \\hat{\\sigma}(t))\\geq  & lS(t)  \\geq l\\hat{S}(t)\\exp(1.96 \\hat{\\sigma}(t)) \\\\\nl\\hat{S}(t)\\exp(1.96 \\hat{\\sigma}(t))\\leq  & lS(t)  \\leq l\\hat{S}(t)\\exp(-1.96 \\hat{\\sigma}(t)) \\\\\n\\exp(l\\hat{S}(t)\\exp(1.96 \\hat{\\sigma}(t))) \\leq  & S(t)  \\leq \\exp(l\\hat{S}(t)\\exp(-1.96 \\hat{\\sigma}(t))) \\\\\n\\exp(l\\hat{S}(t))^{\\exp(1.96 \\hat{\\sigma}(t))} \\leq  & S(t)  \\leq \\exp(l\\hat{S}(t))^{\\exp(-1.96 \\hat{\\sigma}(t))} \\\\\n(\\hat{S}(t))^{\\exp(1.96 \\hat{\\sigma}(t))} \\leq  & S(t)  \\leq (\\hat{S}(t))^{\\exp(-1.96 \\hat{\\sigma}(t))}\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"mean-and-median-values","chapter":"6 Survival Analysis","heading":"6.2.1.1 Mean and Median values","text":"MeanMean survival time estimated area survival curve. estimator based upon entire range data. software uses data last observed event (Hosmer, Lemeshow, May 2008) point biases estimate mean downwards, recommend entire range data used. large sample method used estimate variance mean survival time thus construct confidence interval (Andersen et al. 1996). (StatsDirect Limited 2016)ways, easier conceptualize mean average curve thinking calculating average horizontal bars instead vertical bars. jumps along y-axis approximately 1/n, horizontal bar represents one individual deaths. See example : http://blog.data-miners.com/2010/06/--area--survival-curve-equal.html\\[\n\\begin{align*}\n\\hat{\\mu} = \\left\\{\n    \\begin{array}{ll}\n    \\sum_{=0}^{m-1} \\hat{S}(t_i)_{KM} (t_{+1} - t_i) & \\mbox{} t_n = t_m  \\mbox{ (last obs complete)}\\\\\n    \\sum_{=0}^{m-1} \\hat{S}(t_i)_{KM} (t_{+1} - t_i) + \\hat{S}(t_m)_{KM}(t_n-t_m) & \\mbox{last obs censored}\n    \\end{array}\n\\right.\n\\end{align*}\n\\]entry can rearranged thought proportion people die interval times width interval: \\([\\hat{S}(t_{-1})_{KM} - \\hat{S}(t_{})_{KM}]t_i\\). difference, \\([\\hat{S}(t_{-1})_{KM} - \\hat{S}(t_{})_{KM}]\\), gives proportion individuals die time interval. Therefore, estimate weighted average time points \\(t_i\\) weight \\([\\hat{S}(t_{-1})_{KM} - \\hat{S}(t_{})_{KM}]\\).\\[\n\\begin{align*}\n\\hat{\\mu} &= \\sum_{=0}^{m-1} \\hat{S}(t_i)_{KM}(t_{+1} - t_i)\\\\\n&= \\hat{S}(t_0)_{KM}(t_1 - t_0) + \\hat{S}(t_1)_{KM}(t_2 - t_1) + \\hat{S}(t_2)_{KM} (t_3 - t_2) + \\cdots + \\hat{S}(t_{m-1})_{KM} (t_m - t_{m-1}) \\\\\n&= -t_0  \\hat{S}(t_0)_{KM} + t_1 ( \\hat{S}(t_0)_{KM} -  \\hat{S}(t_1)_{KM}) + \\cdots + t_{m-1}( \\hat{S}(t_{m-2})_{KM} -  \\hat{S}(t_{m-1})_{KM}) + t_m  \\hat{S}(t_{m-1})_{KM}\\\\\n&= 0 + t_1 ( \\hat{S}(t_0)_{KM} -  \\hat{S}(t_1)_{KM}) + \\cdots + t_{m-1}( \\hat{S}(t_{m-2})_{KM} -  \\hat{S}(t_{m-1})_{KM}) + t_m ( \\hat{S}(t_{m-1})_{KM} - 0)\\\\\n&= \\mbox{weighted average times}\n\\end{align*}\n\\]Median\nSince distribution survival times tend positively skewed, median often preferred summary measure location distribution. survivor function estimated, straightforward obtain estimate median survival time. , time beyond 50 % individuals population study expected survive, given value \\(t_{(50)}\\) \\(S(t_{(50)}) = 0.5\\).\\[\n\\begin{align*}\n\\hat{t}_{(p)} = \\mbox{smallest complete event time, } t_i, \\mbox{ sample } \\hat{S}(t_i)_{KM} \\leq 1-p/100\n\\end{align*}\n\\]\\(\\hat{S}(t_i)_{KM}\\) gives time least p % events occurred. also ways compute confidence intervals percentiles. won’t cover .","code":""},{"path":"survival-analysis.html","id":"logrank","chapter":"6 Survival Analysis","heading":"6.2.2 Log-rank Test","text":", ’d like know two treatments produce probability survival (logistic regression? significance \\(\\beta\\) coefficient). : \\[\n\\begin{align*}\nH_0: & S_1(t) = S_2(t) \\ \\ \\ \\ \\ \\forall t \\mbox{\nparameters!}\\\\\nH_1: & S_1(t) \\ne S_2(t) \\ \\ \\ \\ \\ \\mbox{ } t\\\\\n\\end{align*}\n\\]want test whether curves \\(t\\) (different \\(t\\)). Let’s consider particular time, \\(j^{th}\\) event time (don’t look censored times, consider curve changes / steps):’s \\(2x2\\) table fixed margins, need consider one group. survival similar two groups, ’d expect:\\[\n\\begin{align*}\nd_{1j} &\\approx \\frac{d_j}{n_j}n_{1j}\\\\\nd_{1j} &= \\mbox{observed}\\\\\n\\frac{d_j}{n_j} n_{1j} &= E(d_{1j} | d_j) = E_{1j} \\\\\nvar(d_{1j}) &= \\frac{n_{1j} n_{2j} d_j (n_j - d_j)}{n_j^2(n_j-1)}\n= V_{1j}\\\\\n\\end{align*}\n\\] variance derived hypergeometric distribution.estimate overall difference observed expected death counts: \\[\n\\begin{align*}\n\\sum_j d_{1j} - \\sum_j E(d_{1j} | d_j) = \\sum_j d_{1j} - \\sum_j\n\\frac{d_j}{n_j}n\\_{1j}\n\\end{align*}\n\\] overall variability: \\[\\sum_j var(d_{1j}|d_j)\\] test statistic use compare observed expected frequency deaths. (Sum death (event) times.) \\[\n\\begin{align*}\nT = \\frac{(|\\sum_j d_{1j} - \\sum_j E(d_{1j} | d_j)| -\n0.5)^2}{\\sum_j var(d_{1j}|d_j)} \\sim \\chi^2_1\n\\end{align*}\n\\]\\(g\\) groups (instead 2)? \\(g-1\\) “death” values. Consider test statistic , call \\(T_1\\). \\(g\\) groups, need sum \\(T_i, =1,2, \\ldots, g-1\\). However, variances \\(T_i\\) correlated (values must sum \\(D_k\\))… test statistic slightly complicated. (Collett 2015(section 2.6))Wilcoxon test similar log-rank test, uses different derivation variance, therefore different denominator test statistic.log-rank test powerful Wilcoxon, log-rank test requires proportional hazards assumption.Wilcoxon test require proportional hazards assumption, require one curve always bigger .Neither work curves cross.technical conditions sample size “big enough.” sample size dependent number deaths. specific criteria, results asymptotic, p-values converge actual probabilities number deaths gets larger.","code":""},{"path":"survival-analysis.html","id":"hazfunc","chapter":"6 Survival Analysis","heading":"6.3 Hazard Functions","text":"Another important idea survival analysis hazard function instantaneous death rate. Let T random variable representing death time. [text implicitly redefines \\(S(t) = P(T \\geq t)\\) inconsistent fundamentally problematic.]\\[\n\\begin{align*}\nh(t)&= \\lim_{\\Delta t \\rightarrow 0} \\frac{P(\\mbox{patient dies time } t + \\Delta t | \\mbox{alive } t)}{\\Delta t}\\\\\n &= \\lim_{\\Delta t \\rightarrow 0} \\frac{P(T < t + \\Delta t | T \\geq t)}{\\Delta t}\\\\\n &= \\lim_{\\Delta t \\rightarrow 0} \\frac{P(t \\leq T < t + \\Delta t | T \\geq t)}{\\Delta t}\\\\\n &= \\lim_{\\Delta t \\rightarrow 0} \\frac{P(t \\leq T < t + \\Delta t )}{P(T \\geq t) \\Delta t}\\\\\n &= \\lim_{\\Delta t \\rightarrow 0} \\frac{S(t) - S(t + \\Delta t)}{S(t) \\Delta t}\\\\\n &= \\lim_{\\Delta t \\rightarrow 0} -\\frac{S(t + \\Delta t) - S(t)}{\\Delta t}\\frac{1}{S(t)}\\\\\n &= -S'(t) \\frac{1}{S(t)}\\\\\n &= -\\frac{d}{dt} \\ln(S(t))\\\\\nS(t) &= \\exp \\{ - \\int^t_0 h(x) dx  \\}\\\\\n\\end{align*}\n\\]different functions \\(h\\) affect survival curve, \\(S\\)?\\[\n\\begin{align*}\nh(t) = 0 &&\\Rightarrow \\mbox{ risk death time } t\\\\\n&& \\Rightarrow S(t) \\mbox{ flat } t\\\\\nh(t) \\> \\>&& \\Rightarrow S(t) \\mbox{ rapidly declining } t\\\\\nh(t) =k && \\Rightarrow S(t) = e^{-kt}\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"hazard-function-as-a-constant","chapter":"6 Survival Analysis","heading":"6.3.0.1 hazard function as a constant","text":"\\(h(t) =k \\rightarrow S(t) = e^{-kt}\\).Plots different hazard functions corresponding survival functions.","code":""},{"path":"survival-analysis.html","id":"estimating-ht-ala-kaplan-meier","chapter":"6 Survival Analysis","heading":"6.3.1 Estimating \\(h(t)\\) ala Kaplan-Meier","text":"\\(h(t)\\) rate individuals population experience event next instant time, conditional surviving time \\(t\\). estimated hazard curve can extend last complete event time, otherwise denominator infinitely large. Also, keep mind \\(h(t)\\) hard estimate, \\(\\hat{h}_{KM}(t)\\) can erratic.\\[\n\\begin{align*}\n\\hat{P}(t_i \\leq T < t_{+1} | T \\geq t_i) &= \\frac{d_i}{n_i} = \\hat{p}_i\\\\\n\\hat{h}_{KM}(t) &= \\frac{\\hat{p}_i}{t_{+1} - t_i}  \\ \\ \\ \\ \\ \\ t_{+1} \\leq t < t_i\\\\\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"proportional-hazards","chapter":"6 Survival Analysis","heading":"6.3.2 Proportional Hazards","text":"Suppose \\(h_0(t)\\) \\(h_1(t)\\) hazard functions patients control experimental treatments. two groups {proportional hazards} \\(h_1(t) = R h_0(t)\\) constant R: \\[\\frac{h_1(t)}{h_0(t)} = R \\ \\ \\ \\ \\ \\forall t\\] R called hazard ratio. \\(h_0(t)\\) can anything long \\(h_1(t)\\) proportional. Note pictures one dies 3 7 days, survival curves flat interval.Consider notion risk death: \\[\n\\begin{align*}\n\\mbox{risk death } t + \\Delta t \\mbox{ given alive } t=\n\\left\\{\n\\begin{array}{ll}\nh_0(t) \\Delta t & \\mbox{control}\\\\\nh_1(t) \\Delta t & \\mbox{experiment}\n\\end{array}\n\\right.\n\\end{align*}\n\\]\\[\n\\begin{align*}\n\\frac{\\mbox{risk dying experim}}{\\mbox{risk dying control}} = \\frac{h_1(t) \\Delta t}{h_0(t) \\Delta t} = \\frac{h_1(t) }{h_0(t) } = R\n\\end{align*}\n\\]Ratio hazard functions can thought instantaneous relative risk (.e., time \\(t\\)).","code":""},{"path":"survival-analysis.html","id":"coxph","chapter":"6 Survival Analysis","heading":"6.3.3 Cox PH Regression Analysis","text":"","code":""},{"path":"survival-analysis.html","id":"simple-proportional-hazards-model","chapter":"6 Survival Analysis","heading":"6.3.3.1 Simple proportional hazards model","text":"ASSUMING proportional hazards! Let \\(h_0(t)\\) hazard function control group. \\(x_i\\) =1 \\(^{th}\\) patient receives treatment; 0 \\(^{th}\\) patient receives control. big value \\(\\beta\\) means event likely happen.\\[\n\\begin{align*}\nh_i(t) &= h_0(t) e^{\\beta x_i} = \\left\\{\n\\begin{array}{ll}\nh_0(t) e^\\beta & \\mbox{experim}\\\\\nh_0(t) & \\mbox{control}\n\\end{array}\n\\right.\\\\\n\\mbox{inst. RR} &= \\frac{h_0(t) e^\\beta}{h_0(t)} = e^\\beta\\\\\nS_i(t) &= (S_0(t))^{e^\\beta}\\\\\n\\end{align*}\n\\] don’t yet know run model estimate parameters, still, can probably . Right? run Cox proportional hazards model, get \\(b\\), estimate HR using: \\(\\widehat{HR} = e^{b}\\)! turns large samples (logistic regression, CI tests called Wald CI tests),\\[\n\\begin{align*}\nb \\sim N: & \\\\\n& 95\\% \\mbox{ CI } \\beta: b \\pm 1.96 SE(b)\\\\\n& 95\\% \\mbox{ CI } HR: (e^{b - 1.96 SE(b)}, e^{b + 1.96 SE(b)})\\\\\n\\end{align*}\n\\] can test HR using \\(\\beta\\): \\[\n\\begin{align*}\nH_0: \\ & \\beta=0 \\iff HR = 1\\\\\nZ &= \\frac{b - 0 }{SE(b)}\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"estimating-the-proportional-hazards-coefficients","chapter":"6 Survival Analysis","heading":"6.3.3.2 Estimating the proportional hazards coefficients","text":"main point proportional hazards proportional hazards, don’t actually need know hazard function order estimate coefficients affect survival function. [divide hazard function.]Intervals death times convey information relationship hazard explanatory variables (including treatment). also assuming ties death times. [Importantly, assume proportional hazards assumption holds \\(t\\).]\\[\n\\begin{align*}\nP(^{th} \\mbox{ indiv, w/}x_i, \\mbox{ dies } t_i | \\mbox{one death } t_i) &= \\frac{P(^{th} \\mbox{ indiv w/}x_i \\mbox{ dies } t_i )}{P(\\mbox{least one death } t_i)}\\\\\n&= \\frac{\\mbox{hazard } t_i}{\\mbox{sum patients risk time } t_i}\\\\\n&= \\frac{h_i(t_i)}{\\sum_{k:t_k \\geq t_i} h_k (t_i)} \\\\\n&= \\frac{e^{\\beta x_i}}{\\sum_{k:t_k \\geq t_i} e^{\\beta x_k}}\n\\end{align*}\n\\] logistic regression (linear regression!) ’ll use maximum likelihood estimate parameter(s). (product patients death times recorded, censored times).\\[\n\\begin{align*}\nL(\\beta) &= \\prod_{=1}^r \\frac{e^{\\beta x_i}}{\\sum_{k:t_k\n\\geq t_i} e^{\\beta x_k}}\\\\\n\\delta_i &= \\left\\{\\begin{array}{ll}\n1 & \\mbox{death}\\\\\n0 & \\mbox{censored}\n\\end{array}\n\\right.\\\\\nL(\\beta) &= \\prod_{=1}^n \\Bigg( \\frac{e^{\\beta\nx_i}}{\\sum_{k:t_k \\geq t_i} e^{\\beta x_k}} \\Bigg)^{\\delta_i}\\\\\n\\ln L(\\beta) &= \\sum_{=1}^n \\delta_i \\bigg[ \\beta x_i - \\ln\n(\\sum_{k:t_k \\geq t_i} e^{\\beta x_k}) \\bigg]\n\\end{align*}\n\\] \\(b\\) found using numerical methods (logistic regression).Example 6.4  Consider following data prostate cancer study. study performed randomized clinical trail compare treatments prostatic cancer, begun 1967 Veteran’s Administration Cooperative Urological Research Group. trial double blind two treatments used placebo 1.0 mg diethylstilbestrol (DES). time origin study date patient randomized treatment, end-point death patient prostate cancer. full data set given Andrews Herzberg (1985), data used example patients presenting Stage III cancer given Collett (2015) (page 8).Note 1: intercept linear component model (.e., \\(\\beta_0\\) \\(b_0).\\) baseline estimate (usually role “intercept”) contained within \\(h_0(t)\\) parameter.Note 1: intercept linear component model (.e., \\(\\beta_0\\) \\(b_0).\\) baseline estimate (usually role “intercept”) contained within \\(h_0(t)\\) parameter.Note 2: Nowhere made assumptions form \\(h_0(t)\\).Note 2: Nowhere made assumptions form \\(h_0(t)\\).Note 3: estimate \\(h_0(t)\\), use pointwise values (control group get \\(h_0),\\) just like Kaplan-Meier plots \\((\\tau_j = t_{(j+1)} - t_j):\\) \\[h_0(t_j) = \\frac{d_j}{n_j \\tau_j}\\] covariates, estimation gets much complicated.Note 3: estimate \\(h_0(t)\\), use pointwise values (control group get \\(h_0),\\) just like Kaplan-Meier plots \\((\\tau_j = t_{(j+1)} - t_j):\\) \\[h_0(t_j) = \\frac{d_j}{n_j \\tau_j}\\] covariates, estimation gets much complicated.Note 4: logrank statistic can derived score test Cox proportional hazards model comparing two groups. therefore approximately equivalent likelihood ratio test statistics model (Collett 2015 (section 3.9, page 102-106)). Additionally, log-rank test powerful alternative hazard death given time individual one group proportional hazard time similar individual group (.e., proportional hazards assumption). (Collett 2015 (section 2.5.4, pg 44-45)) \\[\n\\begin{align*}\nH_0:\\ &h_1(t) = h_2(t)\\\\\nH_1:\\ &h_1(t) = R h_2(t)\n\\end{align*}\n\\]Note 4: logrank statistic can derived score test Cox proportional hazards model comparing two groups. therefore approximately equivalent likelihood ratio test statistics model (Collett 2015 (section 3.9, page 102-106)). Additionally, log-rank test powerful alternative hazard death given time individual one group proportional hazard time similar individual group (.e., proportional hazards assumption). (Collett 2015 (section 2.5.4, pg 44-45)) \\[\n\\begin{align*}\nH_0:\\ &h_1(t) = h_2(t)\\\\\nH_1:\\ &h_1(t) = R h_2(t)\n\\end{align*}\n\\]Note 5: need proportional hazards inference (otherwise p-values meaningless!). technical assumptions Cox Proportional Hazards model :\nObservations independent (almost always important assumption statistical inference)\nProportional hazards: hazard ratios dependent time.\nIndependent censoring: censored observations survival prospects non-censored participants.\nNote 5: need proportional hazards inference (otherwise p-values meaningless!). technical assumptions Cox Proportional Hazards model :Observations independent (almost always important assumption statistical inference)Proportional hazards: hazard ratios dependent time.Independent censoring: censored observations survival prospects non-censored participants.","code":"\nlibrary(survival)\nprostate <- readr::read_csv(\"PROSTATE.csv\")\nhead(prostate)\n#> # A tibble: 6 x 7\n#>   Treatment  Time Status   Age  Haem  Size Gleason\n#>       <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>   <dbl>\n#> 1         1    65      0    67  13.4    34       8\n#> 2         2    61      0    60  14.6     4      10\n#> 3         2    60      0    77  15.6     3       8\n#> 4         1    58      0    64  16.2     6       9\n#> 5         2    51      0    65  14.1    21       9\n#> 6         1    51      0    61  13.5     8       8\n\ncoxph(Surv(Time,Status) ~ Treatment, data = prostate)\n#> Call:\n#> coxph(formula = Surv(Time, Status) ~ Treatment, data = prostate)\n#> \n#>           coef exp(coef) se(coef)  z    p\n#> Treatment -2.0       0.1      1.1 -2 0.07\n#> \n#> Likelihood ratio test=5  on 1 df, p=0.03\n#> n= 38, number of events= 6\n\ncoxph(Surv(Time,Status) ~ Treatment, data = prostate) %>% tidy()\n#> # A tibble: 1 x 5\n#>   term      estimate std.error statistic p.value\n#>   <chr>        <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 Treatment    -1.98      1.10     -1.80  0.0717\ncoxph(Surv(Time,Status) ~ Treatment, data = prostate) %>% glance()\n#> # A tibble: 1 x 18\n#>       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n#>   <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>          <dbl>\n#> 1    38      6          4.55      0.0329         4.42     0.0355           3.24\n#> # … with 11 more variables: p.value.wald <dbl>, statistic.robust <dbl>,\n#> #   p.value.robust <dbl>, r.squared <dbl>, r.squared.max <dbl>,\n#> #   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, nobs <int>"},{"path":"survival-analysis.html","id":"multcoxph","chapter":"6 Survival Analysis","heading":"6.3.3.3 Cox PH Multiple Regression Analysis","text":"Extending simple proportional hazards model include multiple covariates.Let \\(x_{i1}, x_{i2}, \\ldots, x_{iq}\\) \\(q\\) covariates person \\(\\). define baseline hazard : \\[\n\\begin{align*}\nh_0(t) &= \\mbox{hazard patients covariates }\nx_{i1}=x_{i2}=\\cdots=x_{iq} = 0\\\\\nh_i(t) &= h_0(t) e^{\\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots +\n\\beta_q x_{iq}}\\\\\n\\end{align*}\n\\] hazard function \\(^{th}\\) patient., can consider nested models compare likelihoods.\\[\n\\begin{align*}\n-2 \\ln \\frac{L_{\\mbox{reduced model}}}{L_{\\mbox{full model}}}\n&\\sim \\chi^2_{\\Delta p}\\\\\n& \\\\\n2 \\bigg( \\ln L_{\\mbox{full model}} - \\ln L_{\\mbox{reduced model}} \\bigg) &=\n\\mbox{test stat}\\\\\n\\end{align*}\n\\]Example 6.5  Framingham Heart Study (example Dupont 2009 (section 3.10)) Long-term follow-cardiovascular risk factor data almost 5000 residents town Framingham, MA. Recruitment started 1948 (went 40+ years). data 4699 patients free coronary heart disease baseline exam (Mahmood et al. 2014):look K-M survival curves broken diastolic blood pressure. logrank statistic comparing 7 groups highly significant (\\(p < 10^{-52}\\)), pairwise logrank tests adjacent pairs risk groups also significant (though careful multiple comparisons!).\\[\n\\begin{align*}\ndbp_{ij} &= \\left\\{\n\\begin{array}{ll}\n1 & \\mbox{} ^{th} \\mbox{ patient DBP group } j\\\\\n0 & \\mbox{otherwise}\n\\end{array}\n\\right.\\\\\nmale_i &= \\left\\{\n\\begin{array}{ll}\n1 & \\mbox{} ^{th} \\mbox{ patient male}\\\\\n0 & \\mbox{} ^{th} \\mbox{ patient female}\\\\\n\\end{array}\n\\right.\\\\\n\\end{align*}\n\\]Table 7.1\nUsing proportional hazards model estimating hazard ratio associated blood pressure group, get: \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\beta_2 dbp_{i2} + \\beta_3 dbp_{i3} + \\beta_4 dbp_{i4} + \\beta_5 dbp_{i5} + \\beta_6 dbp_{i6} + \\beta_7 dbp_{i7} \\bigg\\}\\\\ \nh_i(t) &= h0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j dbp_{ij} \\bigg\\} \n\\end{align*}\n\\]can use model dbp categorical check whether dbp used continuous variable. Indeed, seems model linear (ln(HR)) respect dbp. see (linearity) hazard ratio increase 10 dbp constant regardless actual value dbp. Indeed, seems like hazard ratio 10 unit increase dbp roughly \\(e^{0.3}\\) regardless value dbp. model run dbp continuous variable (instead groups) also see hazard ratio associated 10 unit increase dbp \\(e^{0.3}\\).One reason, however, keep variable broken groups way results nicely laid group.\nTable 6.1: coefficients Cox PH dpb categorical.\n\nTable 6.1: coefficients Cox PH dpb numeric\n\nTable 6.1: glance Cox PH dpb categorical.\n\nFigure 6.1: Table 7.1 Dupont (2009).\nTable 7.2\nnext step, can consider adding gender multiplicative effect model. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j\ndbp_{ij} + \\gamma male_i \\bigg\\}\n\\end{align*}\n\\] say effects multiplicative adding exponent, effect due gender multiplied effect due dbp. tell - results - interaction wasn’t modeled? Look hazard ratios, consistent comparing one variable keeping one constant? , check see HR two different levels dbp regardless whether men women considered.\nchange deviance 133 (\\(H_0: \\gamma =0\\)), one degree freedom, p-value small. think \\(\\gamma=0\\), need gender model.\nTable 7.2\nnext step, can consider adding gender multiplicative effect model. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j\ndbp_{ij} + \\gamma male_i \\bigg\\}\n\\end{align*}\n\\] say effects multiplicative adding exponent, effect due gender multiplied effect due dbp. tell - results - interaction wasn’t modeled? Look hazard ratios, consistent comparing one variable keeping one constant? , check see HR two different levels dbp regardless whether men women considered.change deviance 133 (\\(H_0: \\gamma =0\\)), one degree freedom, p-value small. think \\(\\gamma=0\\), need gender model.\nTable 6.2: coefficients Cox PH sex dpb categorical.\n\nTable 6.2: glance Cox PH sex dpb categorical.\n\nFigure 6.2: Table 7.2 Dupont (2009).\nTable 7.3 Considering gender dbp interacting. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j\ndbp_{ij} + \\gamma male_i + \\sum_{j=2}^7 \\delta_j dbp_{ij}\nmale_i \\bigg\\}\n\\end{align*}\n\\]\nchange deviance 21.23 (\\(H_0: \\delta_j =0\\)), six degrees freedom, p-value 0.002. evidence interaction statistically significant.\nNote marked differences estimates table 7.2 7.3. interactive model indicates effect gender risk CHD greatest people low moderate blood pressure diminishes blood pressure rises. Gender appears effect CHD people DBP 110 mm Hg.\nTable 7.3 Considering gender dbp interacting. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j\ndbp_{ij} + \\gamma male_i + \\sum_{j=2}^7 \\delta_j dbp_{ij}\nmale_i \\bigg\\}\n\\end{align*}\n\\]change deviance 21.23 (\\(H_0: \\delta_j =0\\)), six degrees freedom, p-value 0.002. evidence interaction statistically significant.Note marked differences estimates table 7.2 7.3. interactive model indicates effect gender risk CHD greatest people low moderate blood pressure diminishes blood pressure rises. Gender appears effect CHD people DBP 110 mm Hg.\nTable 6.3: coefficients Cox PH sex dpb categorical interacting.\n\nTable 6.3: glance Cox PH sex dpb categorical interacting.\n\nFigure 6.3: Table 7.3 Dupont (2009).\nTable 7.4\nAdjusting confounding variables. particular interest age baseline exam. know age varied widely among study subjects, DBP risk CHD increase age. also consider effect body mass index serum cholesterol. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j dbp_{ij} + \\gamma male_i + \\sum_{j=2}^7 \\delta_j dbp_{ij} male_i \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\  \\theta_1 age_i + \\theta_2 bmi_i + \\theta_3 scl_i \\bigg\\}\n\\end{align*}\n\\]\nneed \\(\\theta_1, \\theta_2, \\theta_3\\), statistically significant.\nHR Table 7.4 substantially smaller Table 7.3. important realize HR Table 7.4 compare people age, body mass index, serum cholesterol, Table 7.3 compare people without regard variables. surprising unadjusted HR inflated due confounding variables.\nGoal: predict CHD \\(\\rightarrow\\) ’s easier measure blood pressure cholesterol, may just prefer use unadjusted model.\nGoal: establish causal link \\(\\rightarrow\\) must adjust possible confounding variables.\nTable 7.4\nAdjusting confounding variables. particular interest age baseline exam. know age varied widely among study subjects, DBP risk CHD increase age. also consider effect body mass index serum cholesterol. \\[\n\\begin{align*}\nh_i(t) &= h_0(t) \\exp \\bigg\\{ \\sum_{j=2}^7 \\beta_j dbp_{ij} + \\gamma male_i + \\sum_{j=2}^7 \\delta_j dbp_{ij} male_i \\\\\n& \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\  \\theta_1 age_i + \\theta_2 bmi_i + \\theta_3 scl_i \\bigg\\}\n\\end{align*}\n\\]need \\(\\theta_1, \\theta_2, \\theta_3\\), statistically significant.HR Table 7.4 substantially smaller Table 7.3. important realize HR Table 7.4 compare people age, body mass index, serum cholesterol, Table 7.3 compare people without regard variables. surprising unadjusted HR inflated due confounding variables.Goal: predict CHD \\(\\rightarrow\\) ’s easier measure blood pressure cholesterol, may just prefer use unadjusted model.Goal: establish causal link \\(\\rightarrow\\) must adjust possible confounding variables.transform continuous variable factor variable?continuous believe relationship linear log(HR)factor relationship linear. Although keep mind can lots coefficients estimate make factor variables, lose df.\nFigure 6.4: Table 7.4 Dupont (2009).\n","code":"\nheart <- readr::read_csv(\"framingham.csv\")\n\nheart <- heart %>% \n  mutate(dbpf = case_when(\n    dbp <= 60 ~ \"under60\",\n    dbp <=70 ~ \"60-70\",\n    dbp <= 80 ~ \"70-80\",\n    dbp <=90 ~ \"80-90\",\n    dbp <=100 ~ \"90-100\",\n    dbp <=110 ~ \"100-110\",\n    TRUE ~ \"over110\")) %>%\n  mutate(dbpf = factor(dbpf,\n                       levels = c(\"under60\", \"60-70\", \"70-80\",\"80-90\", \"90-100\",\"100-110\",\"over110\")),\n         sex = case_when(\n           sex == 1 ~ \"male\", \n           TRUE ~ \"female\"))\ncoxph(Surv(followup,chdfate) ~ dbpf, data = heart) %>% tidy() %>%\n  kable(caption = \"coefficients for Cox PH with dpb categorical.\",\n        digits = 3) %>%\n  kable_styling()\n\ncoxph(Surv(followup,chdfate) ~ dbp, data = heart) %>% tidy() %>%\n  kable(caption = \"coefficients for Cox PH with dpb numeric\",\n        digits = 3) %>%\n  kable_styling()\n\ncoxph(Surv(followup,chdfate) ~ dbpf, data = heart) %>% glance() %>%\n  kable(caption = \"glance for Cox PH with dpb categorical.\",\n        digits = 2) %>%\n  kable_styling()\ncoxph(Surv(followup,chdfate) ~ sex + dbpf, data = heart) %>% \n  tidy() %>%\n  kable(caption = \"coefficients for Cox PH with sex and dpb categorical.\",\n        digits = 3) %>%\n  kable_styling()\n  \ncoxph(Surv(followup,chdfate) ~ sex + dbpf, data = heart) %>% \n  glance() %>%\n  kable(caption = \"glance for Cox PH with sex and dpb categorical.\",\n        digits = 2) %>%\n  kable_styling()\ncoxph(Surv(followup,chdfate) ~ sex*dbpf, data = heart) %>% \n  tidy() %>%\n  kable(caption = \"coefficients for Cox PH with sex and dpb categorical interacting.\",\n        digits = 3) %>%\n  kable_styling()\n  \ncoxph(Surv(followup,chdfate) ~ sex*dbpf, data = heart) %>% \n  glance() %>%\n  kable(caption = \"glance for Cox PH with sex and dpb categorical interacting.\",\n        digits = 2) %>%\n  kable_styling()\ncoxph(Surv(followup,chdfate) ~ dbpf * sex + age + bmi + scl, data = heart) %>% tidy()\n#> # A tibble: 16 x 5\n#>   term        estimate std.error statistic   p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 dbpf60-70      0.415     0.352      1.18 0.238    \n#> 2 dbpf70-80      0.503     0.343      1.47 0.142    \n#> 3 dbpf80-90      0.648     0.344      1.89 0.0592   \n#> 4 dbpf90-100     0.661     0.351      1.88 0.0599   \n#> 5 dbpf100-110    1.13      0.363      3.12 0.00183  \n#> 6 dbpfover110    1.66      0.377      4.40 0.0000107\n#> # … with 10 more rows\ncoxph(Surv(followup,chdfate) ~ dbpf * sex + age + bmi + scl, data = heart) %>% glance()\n#> # A tibble: 1 x 18\n#>       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n#>   <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>          <dbl>\n#> 1  4658   1465          754.   5.12e-150         774.  2.21e-154           707.\n#> # … with 11 more variables: p.value.wald <dbl>, statistic.robust <dbl>,\n#> #   p.value.robust <dbl>, r.squared <dbl>, r.squared.max <dbl>,\n#> #   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, nobs <int>"},{"path":"survival-analysis.html","id":"testingph","chapter":"6 Survival Analysis","heading":"6.3.4 Testing Proportional Hazards","text":"mentioned earlier, proportional hazards means ratio hazard functions function time. Therefore violation proportional hazards means interaction covariate (e.g., treatment) time determining response (time event). Non-proportional hazards means work interpreting model understanding coefficients. Generally, violation proportional hazards also means lower power.","code":""},{"path":"survival-analysis.html","id":"proportional-hazards-via-survival-curves","chapter":"6 Survival Analysis","heading":"6.3.4.1 Proportional Hazards via survival curves:","text":"’ve discussed proportional hazards means hazard one group constant multiple hazard another group. mean terms survival function?\\[\n\\begin{align*}\nh_1 (t) &= h_0 (t) e^\\beta\\\\\nS(t) &= e^{-\\int_0^t h(x) dx}\\\\\nS_0(t) &= e^{-\\int_0^t h_0(x) dx}\\\\\nS_1(t) &= e^{-\\int_0^t h_0(x)e^\\beta dx}\\\\\nS_1(t) &= e^{- e^\\beta \\int_0^t h_0(x) dx}\\\\\nS_1(t) &= \\bigg[ e^{- \\int_0^t h_0(x) dx} \\bigg]^{e^\\beta}\\\\\n\\ln(S_1(t)) &= e^\\beta \\ln [e^{- \\int_0^t h_0(x) dx}]\\\\\n-\\ln(S_1(t)) &= e^\\beta  [-\\ln(S_0(t))]\\\\\n\\ln(-\\ln(S_1(t))) &= \\beta + \\ln [-\\ln(S_0(t))]\\\\\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"test-1-for-ph","chapter":"6 Survival Analysis","heading":"Test 1 for PH","text":"Therefore, \\(\\ln (- \\ln\\) survival curves) parallel differ y-intercept constant \\(\\beta\\).Note \\(h_0(t)\\) constant (.e., \\(h_i(t) = e^\\beta\\)), \\[\n\\begin{align*}\nS(t) &= e^{-\\int_0^t h(x) dx}\\\\\n&= e^{- e^\\beta t}\\\\\n\\ln S(t) &= - e^\\beta t\\\\\n- \\ln S(t) &= e^\\beta t\\\\\n\\ln(- \\ln S(t)) &= \\beta + \\ln(t)\\\\\n\\end{align*}\n\\]say \\(ln(- ln S(t))\\) linear \\(\\ln(t)\\), \\(h_0(t)\\) PH also constant \\(t\\).","code":""},{"path":"survival-analysis.html","id":"proportional-hazards-via-a-test-for-time-dependent-covariates","chapter":"6 Survival Analysis","heading":"6.3.4.2 Proportional Hazards via a test for time dependent covariates","text":"PH assumption violated, :\\[\\frac{h_i(t)}{h_0(t)} = g(t)\\] function \\(t\\). Consider following model (function time completely specified, just one possibility dependence relationship):\\[\n\\begin{align*}\nx_{i1}  &= \\left\\{\n\\begin{array}{ll}\n1 & treatment\\\\\n0 & control\n\\end{array}\n\\right.\\\\\nx_{i2} &= \\left\\{\n\\begin{array}{ll}\nt & treatment\\\\\n0 & control\\\\\n\\end{array}\n\\right.\\\\\n\\end{align*}\n\\]relative hazard becomes:\n\\[\n\\begin{align*}\nh_i(t) &= e^{\\beta_1 x_{i1} + \\beta_2 x_{i2}} h_0(t)\\\\\n\\frac{h_1(t)}{h_0(t)} &= e^{\\beta_1 +\\beta_2 t}\\\\\n\\end{align*}\n\\]\\(\\beta_2 < 0\\), relative hazard decreases time. \\(\\beta_2 > 0\\), relative hazard increases time.","code":""},{"path":"survival-analysis.html","id":"test-2-for-ph","chapter":"6 Survival Analysis","heading":"Test 2 for PH","text":"PH test interest : \\[H_0: \\beta_2 = 0\\] isn’t Wald test, general idea (R: cox.zph). [variable denominator likelihood (calculating MLE) different values different times.] alternative form test call: \\(x_{i2} = t \\ \\ \\forall t\\) let \\[h_i(t) = e^{\\beta_1 x_{i1} + \\beta_2 x_{i1} x_{i2}} h_0(t).\\]time dependency model, coxph analysis won’t accurate.","code":""},{"path":"survival-analysis.html","id":"proportional-hazards-via-schoenfeld-residuals","chapter":"6 Survival Analysis","heading":"6.3.4.3 Proportional Hazards via Schoenfeld Residuals","text":"Recall: Cox model, probability particular member \\(\\) fails time \\(t_j\\) :\\[\n\\begin{align*}\nP(^{th} \\mbox{ indiv w/}x_i \\mbox{ dies } t_j | \\mbox{least\none death } t_j) &= \\frac{P(^{th} \\mbox{ indiv w/}x_i \\mbox{\ndies } t_j )}{P(\\mbox{least one death } t_j)}\\\\\n&= \\frac{\\mbox{hazard } t_i}{\\mbox{sum patients risk\ntime } t_j}\\\\\n&= \\frac{h_i(t_j)}{\\sum_{k:t_k \\geq t_j} h_k (t_j)} \\\\\n&= \\frac{e^{\\beta x_i}}{\\sum_{k:t_k \\geq t_j} e^{\\beta\nx_k}}\\\\\nw_j(\\beta, t_j) &= \\frac{e^{\\beta x_i}}{\\sum_{k:t_k \\geq t_j}\ne^{\\beta x_k}}\n\\end{align*}\n\\]Using weights , can calculate average value \\(l^{th}\\) covariate (.e., explanatory variable): \\[\\bar{x}_l (\\beta,t_j) = \\sum_{k: t_k \\geq t_j} x_{kl}\nw_j(\\beta, t_j)\\]Schoenfeld Residual \\(x_l\\) subject \\(\\) still alive time \\(t_j\\), difference covariate \\(x_{il}\\) subject weighted average covariates risk set: \\[\\mbox{Schoenfeld resid }_i = x_{il} - \\bar{x}_l(\\beta, t_i)\\] Note calculation \\(^{th}\\) subject means death time \\(t_i\\).","code":""},{"path":"survival-analysis.html","id":"test-3-for-ph","chapter":"6 Survival Analysis","heading":"Test 3 for PH","text":"idea residual plot (Schoenfeld residual wrt paticular covariate y-axis, time x-axis)flat. strong linear trend residuals? say time dependency? Imagine scatterplot residual positively linearly associated time. \\(t_i > >\\) \\(x_i\\) much bigger expected; \\(t_i < <\\) \\(x_i\\) much smaller expected. , covariate interest changes time effect risk survival , .","code":""},{"path":"survival-analysis.html","id":"solutions-to-violations-of-ph","chapter":"6 Survival Analysis","heading":"6.3.4.4 Solutions to violations of PH","text":"crossing hazard functions (survivor functions!) cross time, PH assumption violated.help ?Don’t coxph, just fit K-M curves separately perform log-rank test.Start \\(t^*\\), crossing point.Fit different models \\(t^*\\) \\(t^*\\).Fit model time dependent covariate.Creative analysis: instead tumor size, use % change set period time.examples PH violations\ntime, treatment effect lessens (short term benefits)\ntumor size endpoint predictive tumor size entrance.\nclinical variables change time (lung capacity, weight, white blood cell count)\nexposure variables (pollution, UV rays, etc.)\nage, temperature\nexamples PH violationsover time, treatment effect lessens (short term benefits)tumor size endpoint predictive tumor size entrance.clinical variables change time (lung capacity, weight, white blood cell count)exposure variables (pollution, UV rays, etc.)age, temperature","code":""},{"path":"survival-analysis.html","id":"log-linearity","chapter":"6 Survival Analysis","heading":"6.3.4.5 Log linearity","text":"’ve also assumed log hazard ratio linear covariates. one unit change covariate effect log HR (level covariate). Just like logistic regression, assumption hard check. However, can see modeling performs checking see quadratic (higher order terms) significant model. Another possibility categorize continuous predictor check PH assumptions .","code":""},{"path":"survival-analysis.html","id":"othersurv","chapter":"6 Survival Analysis","heading":"6.4 Other stuff","text":"","code":""},{"path":"survival-analysis.html","id":"sample-size-calculation","chapter":"6 Survival Analysis","heading":"6.4.1 Sample Size Calculation","text":"","code":""},{"path":"survival-analysis.html","id":"sample-size-for-two-independent-sample-means-inference","chapter":"6 Survival Analysis","heading":"6.4.1.1 Sample Size for Two Independent Sample Means Inference","text":"Taking step back consider slightly simpler inference (two sample mean), let’s say want size \\(\\alpha=0.05\\) power \\(1-\\beta=0.8\\). mean? relate type type II errors? Draw pictures. ’re talking thing can control: sample size. actually control truth hypotheses, extent difference \\(H_0\\) versus \\(H_A\\), variability underlying population. can control sample size. larger sample size, easier identify true alternative hypothesis (, higher power).order estimate needed sample size, need know often willing make type errors, often willing make type II errors, big difference \\(H_0\\) \\(H_A\\) like determine. Without three components, impossible determine sample size. read sample size calculation done, must report associated size, power, difference (calculation meaningless).\\[\n\\begin{align*}\nH_0: \\ &\\mu_1 - \\mu_2 = 0\\\\\nH_A: \\ &\\mu_1 - \\mu_2 \\ne 0\n\\end{align*}\n\\]make assumptions. assume size 0.05, power 0.8, alternative hypothesis \\(D = \\mu_1 - \\mu_2\\). case, let’s also assume \\(\\sigma_1=\\sigma_2=\\sigma\\) \\(n_1=n_2=n\\). Without loss generality, let’s say \\(\\overline{Y}_1 - \\overline{Y}_2 > 0\\).\\[\n\\begin{align*}\nP\\bigg(\\frac{\\overline{Y}_1 - \\overline{Y}_2 - 0}{\\sigma\n\\sqrt{1/n + 1/n}} > 1.96 \\bigg| \\mu_1 - \\mu_2 = 0 \\bigg) &=\n0.025\\\\\nP(\\overline{Y}_1 - \\overline{Y}_2 > 1.96 \\sigma \\sqrt{1/n + 1/n}\n\\bigg| \\mu_1 - \\mu_2 = 0) &= 0.025\\\\\nP(\\overline{Y}_1 - \\overline{Y}_2 > 1.96 \\sigma \\sqrt{1/n + 1/n}\n\\bigg| \\mu_1 - \\mu_2 = D) &\\geq 0.8\\\\\nP\\bigg(\\frac{\\overline{Y}_1 - \\overline{Y}_2 - D}{\\sigma\n\\sqrt{1/n + 1/n}} > \\frac{1.96 \\sigma \\sqrt{1/n + 1/n}-D}{\\sigma\n\\sqrt{1/n + 1/n}} \\bigg| \\mu_1 - \\mu_2 = D \\bigg) &\\geq 0.8\\\\\nP\\bigg(Z > \\frac{1.96 \\sigma \\sqrt{1/n + 1/n}-D}{\\sigma \\sqrt{1/n\n+ 1/n}} \\bigg) &\\geq 0.8\\\\\n\\frac{1.96 \\sigma \\sqrt{1/n + 1/n}-D}{\\sigma \\sqrt{1/n + 1/n}}\n&\\leq Z\\_{0.8}\\\\\n1.96 \\sigma \\sqrt{2/n} - D &\\leq Z\\_{0.8} \\sigma \\sqrt{2/n}\\\\\n2.845 \\sigma \\sqrt{2/n} &\\leq D\\\\\nn &\\geq \\frac{16 \\sigma^2}{D^2}\n\\end{align*}\n\\], want able find 2 point difference unrestricted versus deprived mean visual acuity scores, (\\(\\sigma=14\\), equal sample sizes):\\[\n\\begin{align*}\nn &\\geq \\frac{16 \\cdot 14^2}{4}\\\\\n&\\geq 784\n\\end{align*}\n\\]","code":""},{"path":"survival-analysis.html","id":"sample-size-for-survival-model","chapter":"6 Survival Analysis","heading":"6.4.1.2 Sample Size for Survival model","text":"sample size calculations derived inference done log-rank statistics comparing two groups. total number deaths required detect difference groups size \\(\\alpha\\) power \\(1-\\beta\\) \\[d = \\frac{(z_{\\alpha/2} + z_\\beta)^2}{\\pi_1 \\pi_2 \\theta^2_R}\\] \\(\\pi_1\\) proportion sample group 1, \\(\\pi_2\\) proportion sample group 2, \\(\\theta_R\\) log hazard ratio detect.(Derivation Collett (2015) pg 256.)Consider following sample size calculator. variables included slightly different, give general structure calculation. example, equation calculates number deaths. applet calculates number samples (\\(d = p_E \\cdot n\\)). http://powerandsamplesize.com/Calculators/Test-Time--Event-Data/Cox-PH-Equivalence","code":""},{"path":"survival-analysis.html","id":"significance-level-vs.-power-vs.-sample-size","chapter":"6 Survival Analysis","heading":"6.4.1.3 Significance level vs. Power vs. Sample Size","text":"Sig level \\(\\alpha\\): set type error ratepower \\(1-\\beta\\): probability making type II error. set power, need know sample size, significance level, clinically important difference wish detect (effect size).sample size n: set sample size, need \\(\\alpha\\), \\(\\beta\\), effect size“trial 90% power” meaningless“500 subjects per group, trial 90% power detect decrease 10mmHg blood pressure due new treatment 0.05 significance level.”","code":""},{"path":"survival-analysis.html","id":"study-design","chapter":"6 Survival Analysis","heading":"6.4.2 Study Design","text":"Randomized, controlled trialRandomized, controlled trialFactorial design\nRandom allocation (within factorial design) allows estimation interaction treatments\nPhysicians’ health study factorial aspirin (measure myocardial infarction) beta carotene (cancer)\nCan include 3 treatments (need large sample sizes measure anything)\nFactorial designRandom allocation (within factorial design) allows estimation interaction treatmentsPhysicians’ health study factorial aspirin (measure myocardial infarction) beta carotene (cancer)Can include 3 treatments (need large sample sizes measure anything)Cross-design: participants receive treatments (e.g., surgery chemotherapy)\npatient serves control (need fewer samples)\nOften cross-effect initial treatment\nCross-design: participants receive treatments (e.g., surgery chemotherapy)patient serves control (need fewer samples)Often cross-effect initial treatmentNoncompliance\nPatients switch ! analyze data?\nAnalyze according assigned treatment (intent--treat). Crossover happen randomly well known associated outcome.\nCan perform secondary analysis patients stayed treatment.\nExample: surgery radiation\nNoncompliancePatients switch ! analyze data?Analyze according assigned treatment (intent--treat). Crossover happen randomly well known associated outcome.Can perform secondary analysis patients stayed treatment.Example: surgery radiation","code":""},{"path":"survival-analysis.html","id":"subgroup-analysis","chapter":"6 Survival Analysis","heading":"6.4.2.1 subgroup analysis","text":"Refers analyzing subset patients trial separately (e.g., gender, race, family background, etc.)Refers analyzing subset patients trial separately (e.g., gender, race, family background, etc.)Important : clinical decision making, regulatory requirements, hypothesis generationImportant : clinical decision making, regulatory requirements, hypothesis generationProblems:\ninsufficient power. powered overall treatment effect, underpowered detect similar effects subgroups.\nmultiple comparisons. torture data long enough, eventually confess anything.\nwhenever possible, subgroup analysis defined priori protocol\nProblems:insufficient power. powered overall treatment effect, underpowered detect similar effects subgroups.multiple comparisons. torture data long enough, eventually confess anything.whenever possible, subgroup analysis defined priori protocol","code":""},{"path":"survival-analysis.html","id":"some-topics-worth-investigating","chapter":"6 Survival Analysis","heading":"6.4.2.2 Some topics worth investigating","text":"Meta Analysis (Green, Benedetti, Crowley 1997)General Thoughts Clinical Trials cancer (Green, Benedetti, Crowley 1997)Stratified Cox Model (Vittinghoff et al. 2012)Interim analyses / stopping rulesIntent--treat (missing data)Investigating time-varying effects (Bellera et al. 2010)","code":""},{"path":"survival-analysis.html","id":"simulating-survival-data","chapter":"6 Survival Analysis","heading":"6.4.3 Simulating survival data","text":"package designed simulate survival data, simsurv. allows complex models, difficulty scenarios events binary explanatory variables. Sam Brilleman spoke simsurv package useR! 2018: https://www.youtube.com/watch?v=fJTYsncvpvI.","code":""},{"path":"survival-analysis.html","id":"Rsurv","chapter":"6 Survival Analysis","heading":"6.5 R example: ProPublica Analysis","text":"","code":""},{"path":"survival-analysis.html","id":"recidivism-in-florida","chapter":"6 Survival Analysis","heading":"6.5.1 Recidivism in Florida","text":"[ProPublica] analysis Northpointe’s tool, called COMPAS (stands Correctional Offender Management Profiling Alternative Sanctions), found black defendants far likely white defendants incorrectly judged higher risk recidivism, white defendants likely black defendants incorrectly flagged low risk.[ProPublica] looked 10,000 criminal defendants Broward County, Florida, compared predicted recidivism rates rate actually occurred two-year period. defendants booked jail, respond COMPAS questionnaire. answers fed COMPAS software generate several scores including predictions “Risk Recidivism” “Risk Violent Recidivism.” (Larson et al. {May 23, 2016})original article : https://www.propublica.org/article/machine-bias-risk-assessments--criminal-sentencingThe data analysis : https://www.propublica.org/article/--analyzed--compas-recidivism-algorithmThe GitHub repo data code : https://github.com/propublica/compas-analysis","code":"\nlibrary(survival)\nrecid <- readr::read_csv(\"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\")\n\nrecid <- dplyr::select(recid, age, c_charge_degree, race, age_cat, score_text, sex, priors_count, \n                    days_b_screening_arrest, decile_score, is_recid, two_year_recid, \n                    c_jail_in, c_jail_out) %>% \n        filter(days_b_screening_arrest <= 30) %>%\n        filter(days_b_screening_arrest >= -30) %>%\n        filter(is_recid != -1) %>%\n        filter(c_charge_degree != \"O\") %>%\n        filter(score_text != 'N/A')\n        \nrecid <- recid %>% mutate(length_of_stay = as.numeric(as.Date(c_jail_out) - as.Date(c_jail_in))) %>%\n      mutate(crime_factor = factor(c_charge_degree)) %>%\n      mutate(age_factor = as.factor(age_cat)) %>%\n      within(age_factor <- relevel(age_factor, ref = 1)) %>%\n      mutate(race_factor = factor(race,\n                                  labels = c(\"African-American\", \n                                             \"Asian\",\n                                             \"Caucasian\", \n                                             \"Hispanic\", \n                                             \"Native American\",\n                                             \"Other\"))) %>%\n      within(race_factor <- relevel(race_factor, ref = 3)) %>%\n      mutate(gender_factor = factor(sex, labels= c(\"Female\",\"Male\"))) %>%\n      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%\n      mutate(score_factor = factor(score_text != \"Low\", labels = c(\"LowScore\",\"HighScore\")))\n      \nrecidKM <- filter(filter(read_csv(\"https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-parsed.csv\"), score_text != \"N/A\"), end > start) %>%\n        mutate(race_factor = factor(race,\n                                  labels = c(\"African-American\", \n                                             \"Asian\",\n                                             \"Caucasian\", \n                                             \"Hispanic\", \n                                             \"Native American\",\n                                             \"Other\"))) %>%\n        within(race_factor <- relevel(race_factor, ref = 3)) %>%\n        mutate(score_factor = factor(score_text)) %>%\n        within(score_factor <- relevel(score_factor, ref=2)) %>%\n        mutate(timefollow = end - start) %>%\n        filter(race %in% c(\"African-American\", \"Caucasian\"))\n\nrecidKMV <- filter(filter(read_csv(\"https://raw.githubusercontent.com/propublica/compas-analysis/master/cox-violent-parsed.csv\"), score_text != \"N/A\"), end > start) %>%\n        mutate(race_factor = factor(race,\n                                  labels = c(\"African-American\", \n                                             \"Asian\",\n                                             \"Caucasian\", \n                                             \"Hispanic\", \n                                             \"Native American\",\n                                             \"Other\"))) %>%\n        within(race_factor <- relevel(race_factor, ref = 3)) %>%\n        mutate(score_factor = factor(score_text)) %>%\n        within(score_factor <- relevel(score_factor, ref=2)) %>%\n        mutate(timefollow = end - start) %>%\n        filter(race %in% c(\"African-American\", \"Caucasian\"))"},{"path":"survival-analysis.html","id":"kaplan-meier-survival-curve","chapter":"6 Survival Analysis","heading":"6.5.2 Kaplan-Meier survival curve","text":"different options CI","code":"\nrecid.surv <- survfit(Surv(timefollow,event) ~ score_factor, data=recidKM)\nplot(recid.surv, lty=2:4, xlab=\"time\", ylab=\"survival function\")\nlegend(10,.4, c(\"low\", \"high\", \"medium\"),lty=2:4)\n\nsurvminer::ggsurvplot(recid.surv, conf.int=TRUE, censor=F) + ggtitle(\"Overall\")\nggsurvplot(recid.surv[1], conf.int=TRUE, censor=FALSE) + ggtitle(\"Low Only\")\n\nggsurvplot(recid.surv, conf.int=TRUE, censor=FALSE, risk.table = TRUE)\nset.seed(4747)\nrecidKM2 <- recidKM %>% sample_n(200)  # CI on a smaller random sample just to see the different CIs\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2), \n           censor=F, conf.int=F) + ggtitle(\"No CI\")\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2,\n                   conf.type=\"log\"), censor=F, conf.int=T) + ggtitle(\"Log CI\")\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2,\n                   conf.type=\"log-log\"), censor=F, conf.int=T) + ggtitle(\"Log-Log CI\")\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2,\n                   conf.type=\"plain\"), censor=F, conf.int=T) + ggtitle(\"Plain CI\")\n\nggsurvplot_facet(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2), \n                 data=recidKM2, facet.by = \"race\")"},{"path":"survival-analysis.html","id":"log-rank-test-rho0-and-the-wilcoxon-test-rho1","chapter":"6 Survival Analysis","heading":"6.5.3 Log-rank test [rho=0] and the Wilcoxon test [rho=1]","text":"General recidivismViolent recidivism","code":"\nsurvdiff(Surv(timefollow,event) ~ score_factor, data=recidKM2, rho=0)\n#> Call:\n#> survdiff(formula = Surv(timefollow, event) ~ score_factor, data = recidKM2, \n#>     rho = 0)\n#> \n#>                      N Observed Expected (O-E)^2/E (O-E)^2/V\n#> score_factor=Low    91       16     29.3      6.04     13.13\n#> score_factor=High   52       20     11.6      6.11      7.80\n#> score_factor=Medium 57       19     14.1      1.70      2.29\n#> \n#>  Chisq= 14.1  on 2 degrees of freedom, p= 9e-04\nsurvdiff(Surv(timefollow,event) ~ score_factor, data=recidKM2, rho=1)\n#> Call:\n#> survdiff(formula = Surv(timefollow, event) ~ score_factor, data = recidKM2, \n#>     rho = 1)\n#> \n#>                      N Observed Expected (O-E)^2/E (O-E)^2/V\n#> score_factor=Low    91     13.8    24.54      4.68     11.69\n#> score_factor=High   52     17.0     9.99      4.88      7.21\n#> score_factor=Medium 57     15.8    12.04      1.16      1.82\n#> \n#>  Chisq= 12.6  on 2 degrees of freedom, p= 0.002\n\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM2), \n           censor=F, conf.int=F, pval=TRUE) + ggtitle(\"No CI\")\nset.seed(4747)\nrecidKMV2 <- recidKMV %>%\n  sample_n(500)\n\nrecidKMV2 %>% filter(race == \"Caucasian\") %>%\n  survdiff(Surv(timefollow,event) ~ score_factor, data=., rho=0)\n#> Call:\n#> survdiff(formula = Surv(timefollow, event) ~ score_factor, data = ., \n#>     rho = 0)\n#> \n#>                       N Observed Expected (O-E)^2/E (O-E)^2/V\n#> score_factor=Low    112        6    6.087   0.00126   0.00392\n#> score_factor=High    25        2    0.966   1.10625   1.24725\n#> score_factor=Medium  46        1    1.946   0.46016   0.58870\n#> \n#>  Chisq= 1.6  on 2 degrees of freedom, p= 0.5\n\nrecidKMV2 %>% filter(race == \"African-American\") %>%\n  survdiff(Surv(timefollow,event) ~ score_factor, data=., rho=0)\n#> Call:\n#> survdiff(formula = Surv(timefollow, event) ~ score_factor, data = ., \n#>     rho = 0)\n#> \n#>                       N Observed Expected (O-E)^2/E (O-E)^2/V\n#> score_factor=Low     94        3     4.11     0.301     0.488\n#> score_factor=High   131        6     3.77     1.322     2.037\n#> score_factor=Medium  92        2     3.12     0.401     0.560\n#> \n#>  Chisq= 2  on 2 degrees of freedom, p= 0.4\n\nsurvdiff(Surv(timefollow,event) ~ score_factor, data=recidKMV2, rho=1)\n#> Call:\n#> survdiff(formula = Surv(timefollow, event) ~ score_factor, data = recidKMV2, \n#>     rho = 1)\n#> \n#>                       N Observed Expected (O-E)^2/E (O-E)^2/V\n#> score_factor=Low    206     8.77     9.63    0.0771     0.158\n#> score_factor=High   156     7.78     4.87    1.7317     2.389\n#> score_factor=Medium 138     2.96     5.00    0.8343     1.151\n#> \n#>  Chisq= 2.7  on 2 degrees of freedom, p= 0.3\n\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKMV2), \n           censor=F, conf.int=T, pval=TRUE) + ggtitle(\"Violent Recidivism\")\n\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKMV2), \n                 data=recidKMV, censor = FALSE, conf.int = TRUE, facet.by = \"race\") + \n  ggtitle(\"Violent Recidivism\")\n\nas.data.frame(recidKMV2) %>%  # must be a data.frame see \".\" below:\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data= .), \n                 data = ., censor = FALSE, conf.int = TRUE, pval=TRUE, facet.by = \"race\") + \n  ggtitle(\"Violent Recidivism\")"},{"path":"survival-analysis.html","id":"cox-proportional-hazards-models","chapter":"6 Survival Analysis","heading":"6.5.4 Cox Proportional Hazards models","text":"Using rms package, can plot CIs relevant HRs model hand:","code":"\n# Just score_factor\ncoxph(Surv(timefollow,event) ~ score_factor, data=recidKM) %>% tidy()\n#> # A tibble: 2 x 5\n#>   term               estimate std.error statistic   p.value\n#>   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 score_factorHigh      1.08     0.0446      24.1 7.67e-129\n#> 2 score_factorMedium    0.704    0.0439      16.0 9.78e- 58\ncoxph(Surv(timefollow,event) ~ score_factor, data=recidKM) %>% glance()\n#> # A tibble: 1 x 18\n#>       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n#>   <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>          <dbl>\n#> 1 11426   3058          617.   9.39e-135         654.  1.15e-142           609.\n#> # … with 11 more variables: p.value.wald <dbl>, statistic.robust <dbl>,\n#> #   p.value.robust <dbl>, r.squared <dbl>, r.squared.max <dbl>,\n#> #   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, nobs <int>\n\n# score_factor and race\ncoxph(Surv(timefollow,event) ~ score_factor + race, data=recidKM) %>% tidy()\n#> # A tibble: 3 x 5\n#>   term               estimate std.error statistic   p.value\n#>   <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\n#> 1 score_factorHigh      1.03     0.0460     22.3  3.96e-110\n#> 2 score_factorMedium    0.674    0.0445     15.2  7.45e- 52\n#> 3 raceCaucasian        -0.170    0.0396     -4.29 1.78e-  5\ncoxph(Surv(timefollow,event) ~ score_factor + race, data=recidKM) %>% glance()\n#> # A tibble: 1 x 18\n#>       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n#>   <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>          <dbl>\n#> 1 11426   3058          636.   1.65e-137         671.  3.72e-145           626.\n#> # … with 11 more variables: p.value.wald <dbl>, statistic.robust <dbl>,\n#> #   p.value.robust <dbl>, r.squared <dbl>, r.squared.max <dbl>,\n#> #   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, nobs <int>\n\n# score_factor, race, age, sex\ncoxph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recidKM) %>% tidy()\n#> # A tibble: 5 x 5\n#>   term               estimate std.error statistic  p.value\n#>   <chr>                 <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 score_factorHigh     0.926    0.0471      19.6  7.63e-86\n#> 2 score_factorMedium   0.611    0.0452      13.5  1.54e-41\n#> 3 raceCaucasian       -0.120    0.0398      -3.01 2.63e- 3\n#> 4 age                 -0.0137   0.00175     -7.82 5.38e-15\n#> 5 sexMale              0.411    0.0502       8.19 2.53e-16\ncoxph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recidKM) %>% glance()\n#> # A tibble: 1 x 18\n#>       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n#>   <int>  <dbl>         <dbl>       <dbl>        <dbl>      <dbl>          <dbl>\n#> 1 11426   3058          768.   8.91e-164         787.  7.45e-168           739.\n#> # … with 11 more variables: p.value.wald <dbl>, statistic.robust <dbl>,\n#> #   p.value.robust <dbl>, r.squared <dbl>, r.squared.max <dbl>,\n#> #   concordance <dbl>, std.error.concordance <dbl>, logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, nobs <int>\nrecid.data <- recidKM %>%\n  select(timefollow, event, score_factor, race, age, sex)\nrecid.final <- rms::cph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recid.data)\nddist <- rms::datadist(recid.data)\noptions(datadist = 'ddist')\nplot(summary(recid.final), log = TRUE)"},{"path":"survival-analysis.html","id":"checking-proportional-hazards-with-the-plot-of-ln-lnst","chapter":"6 Survival Analysis","heading":"6.5.5 Checking proportional hazards with the plot of \\(\\ln(-\\ln(S(t)))\\)","text":"cox.zph function test proportionality predictors model creating interactions time using transformation time specified transform option. example testing proportionality looking interactions log(time). column rho Pearson product-moment correlation scaled Schoenfeld residuals log(time) covariate. last row contains global test interactions tested . p-value less 0.05 indicates violation proportionality assumption.","code":"\nggsurvplot(survfit(Surv(timefollow,event) ~ score_factor, data=recidKM), \n           censor=F, conf.int=T, fun=\"cloglog\") + ggtitle(\"Complementary Log-Log\")"},{"path":"survival-analysis.html","id":"checking-proportional-hazards-with-cox.zph","chapter":"6 Survival Analysis","heading":"6.5.6 Checking proportional hazards with cox.zph","text":"Note big p-values. reject null hypothesis, conclude evidence non-proportional hazards. example, model seemed non-proportional time proportional log(time), might consider transforming time variable (.e., taking natural log) original model.function cox.zph creates cox.zph object contains list scaled Schoenfeld residuals. ordering residuals list order predictors entered cox model. , first element list corresponds scaled Schoenfeld residuals married, second element corresponds scaled Schoenfeld residuals person, forth. cox.zph object can used plot function. specifying particular element list possible generate plots residuals individual predictors. Leaving list number results plots predictors generated one time. plots non-zero slope evidence proportionality. horizontal line y=0 added reference.","code":"\ncox.zph(coxph(Surv(timefollow,event) ~ score_factor, data=recidKM))\n#>              chisq df   p\n#> score_factor 0.457  2 0.8\n#> GLOBAL       0.457  2 0.8\ncox.zph(coxph(Surv(timefollow,event) ~ score_factor, data=recidKM), transform=\"log\")\n#>              chisq df    p\n#> score_factor  3.04  2 0.22\n#> GLOBAL        3.04  2 0.22\ncox.zph(coxph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recidKM))\n#>                chisq df      p\n#> score_factor  0.5254  2 0.7690\n#> race          7.6193  1 0.0058\n#> age           6.7511  1 0.0094\n#> sex           0.0387  1 0.8440\n#> GLOBAL       13.6196  5 0.0182\nggcoxzph(cox.zph(coxph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recidKM))) "},{"path":"survival-analysis.html","id":"coxph-diagnostics-look-into-all-the-different-arguments-of-the-function","chapter":"6 Survival Analysis","heading":"6.5.7 Coxph diagnostics … look into all the different arguments of the function!","text":"","code":"\nggcoxdiagnostics(coxph(Surv(timefollow,event) ~ score_factor + race + age + sex, data=recidKM))"},{"path":"multiple-comparisons.html","id":"multiple-comparisons","chapter":"7 Multiple Comparisons","heading":"7 Multiple Comparisons","text":"p-value?p-value probability collecting observed data (data showing great greater difference null hypothesis) null hypothesis true. p-value number calculated dataset.George Cobb (2014) put p-value perspective:Q: many colleges grad schools teach p = .05?\n: ’s still scientific community journal editors use.Q: many people still use p = 0.05?\n: ’s taught college grad school.2014, journal Basic Applied Social Psychology banned use null hypothesis significance testing procedures (NHSTP). implications authors?Question 3. inferential statistical procedures required?\nAnswer Question 3. , state art remains uncertain. However, BASP require strong descriptive statistics, including effect sizes. also encourage presentation frequency distributional data feasible. Finally, encourage use larger sample sizes typical much psychology research, sample size increases, descriptive statistics become increasingly stable sampling error less problem. However, stop short requiring particular sample sizes, possible imagine circumstances typical sample sizes might justifiable.American Statistical Association put statement p-valuesP-values can indicate incompatible data specified statistical model.P-values measure probability studied hypothesis true, probability data produced random chance alone.Scientific conclusions business policy decisions based whether p- value passes specific threshold.Proper inference requires full reporting transparency.p-value, statistical significance, measure size effect importance result., p-value provide good measure evidence regarding model hypothesis.interested parties weigh :\nStatisticians issue warning misuse P values (Nature, March 7, 2016)\nvalue p (Richard Morey, blog entry, Jan 3, 2021)\nStatisticians issue warning misuse P values (Nature, March 7, 2016)value p (Richard Morey, blog entry, Jan 3, 2021)","code":""},{"path":"multiple-comparisons.html","id":"Ioannidis","chapter":"7 Multiple Comparisons","heading":"7.1 Why Most Published Research Findings are False","text":"Ioannidis article (Ioannidis 2005), related discussion, focuses multiple testing. ’d like understand effect testing three different contexts:looking many possible significant findings possible (publish perish)bias exists workWhen (many) researchers study effect","code":""},{"path":"multiple-comparisons.html","id":"definitions","chapter":"7 Multiple Comparisons","heading":"7.1.0.1 Definitions","text":"R\n\\[\\begin{eqnarray*}\nR = \\frac{ \\mbox{# true relationships}}{\\mbox{# null relationships}} \\ \\ \\ \\mbox{ population}\n\\end{eqnarray*}\\]TRUE\n\\[\\begin{eqnarray*}\nP(\\mbox{study true}) &=& \\frac{ \\mbox{# true relationships}}{\\mbox{# total}}\\\\\n&=& \\frac{ \\mbox{# true relationships}}{\\mbox{# true + # null}}\\\\\n&=& \\frac{ \\mbox{R(# null relationships)}}{\\mbox{R (# null) + # null}}\\\\\n&=& \\frac{R}{R+1}\n\\end{eqnarray*}\\]size\n\\[\\begin{eqnarray*}\n\\alpha &=& P(\\mbox{type error})\\\\\n&=& P(\\mbox{reject } H_0 | H_0 \\mbox{ true})\\\\\n\\end{eqnarray*}\\]power\n\\[\\begin{eqnarray*}\n1 - \\beta &=& P(\\mbox{reject } H_0 | H_0 \\mbox{ false})\\\\\n\\beta &=& P(\\mbox{type II error})\\\\\n&=& P(\\mbox{reject } H_0 | H_0 \\mbox{ false})\\\\\n\\end{eqnarray*}\\]tests\n\\[\\begin{eqnarray*}\nc &=& \\mbox{# tests run}\n\\end{eqnarray*}\\]Baseball power simulation applet (Chance Rossman 2018) http://www.rossmanchance.com/applets/power.html","code":""},{"path":"multiple-comparisons.html","id":"positive-predictive-value-ppv","chapter":"7 Multiple Comparisons","heading":"7.1.1 Positive Predictive Value (PPV)","text":"’ll focus Positive Predictive Value (PPV). , probability positive result found actually true?\\[\\begin{eqnarray*}\nPPV &=& \\frac{c(1-\\beta)R / (R+1)}{c(1-\\beta)R / (R+1) + c \\alpha / (R+1)}\\\\\n&=& \\frac{c(1-\\beta)R}{c(1-\\beta)R + c \\alpha}\\\\\n&=& \\frac{(1-\\beta)R}{(1-\\beta)R +  \\alpha}\\\\\n&=& \\frac{1}{1 + \\alpha / (1-\\beta) R}\\\\\n&& \\\\\nPPV &>& 0.5 \\mbox{ likely true}\\\\\n\\mbox{iff   } (1-\\beta)R &>& (R-\\beta R + \\alpha) 0.5\\\\\n(1-\\beta) R 0.5 &>& \\alpha 0.5\\\\\n(1-\\beta) R &>& \\alpha \\\\\n&& \\\\\nPPV &<& 0.5  \\mbox{ likely false}\\\\\n\\mbox{iff   } (1-\\beta) R &<& \\alpha \\\\\n\\end{eqnarray*}\\]","code":""},{"path":"multiple-comparisons.html","id":"bias","chapter":"7 Multiple Comparisons","heading":"7.1.2 Bias","text":"bias\n\\[\\begin{eqnarray*}\nu &=& \\mbox{proportion results research findings ended }\\\\\n && \\mbox{reported bias}\n\\end{eqnarray*}\\]Note: bias simply moves \\(u\\)% findings row Yes row.\\[\\begin{eqnarray*}\nPPV &=& \\frac{[c(1-\\beta)R +uc \\beta R] / (R+1)}{c[R+\\alpha - \\beta R + u(1-\\alpha + \\beta R)]/(R+1)}\\\\\n &=& \\frac{[(1-\\beta)R +u \\beta R]}{[R+\\alpha - \\beta R + u(1-\\alpha + \\beta R)]}\\\\\n&=& \\frac{1}{1 + \\frac{\\alpha + u(1-\\alpha)}{(1-\\beta)R + u\\beta R}}\n\\end{eqnarray*}\\]Note \\(PPV \\uparrow\\) \\(u \\uparrow\\) long \\((1-\\beta) \\leq \\alpha\\). really, probably easier think \\((1-\\beta) > \\alpha\\), \\(PPV \\uparrow\\) \\(u \\downarrow\\). second sentence realistic, e.g., \\(\\beta < .95\\) means true results list significant findings bias goes . [understand direction relationships, find \\(\\partial PPV / \\partial u < 0\\) \\((1-\\beta) > \\alpha\\) (decreasing u).]","code":""},{"path":"multiple-comparisons.html","id":"multiple-studies","chapter":"7 Multiple Comparisons","heading":"7.1.3 Multiple Studies","text":"\\(\\alpha\\)\nstudy null, probability seeing null \\((1-\\alpha)\\)\n3 us test thing, probability see null \\((1-\\alpha)^3\\)\nprobability least one use see significance goes \\(\\alpha\\) \\(1 - (1-\\alpha)^3\\)\n\\(n \\uparrow\\) someone definitely see significance (bad!)\nstudy null, probability seeing null \\((1-\\alpha)\\)3 us test thing, probability see null \\((1-\\alpha)^3\\)probability least one use see significance goes \\(\\alpha\\) \\(1 - (1-\\alpha)^3\\)\\(n \\uparrow\\) someone definitely see significance (bad!)\\(\\beta\\)\nstudy significant, probability seeing null \\(\\beta\\)\n3 us test thing, probability ’ll see null \\(\\beta^3\\)\nprobability least one us see significance goes \\((1-\\beta)\\) \\((1-\\beta^3)\\)\n\\(n \\uparrow\\), someone definitely see significance (good!)\nstudy significant, probability seeing null \\(\\beta\\)3 us test thing, probability ’ll see null \\(\\beta^3\\)probability least one us see significance goes \\((1-\\beta)\\) \\((1-\\beta^3)\\)\\(n \\uparrow\\), someone definitely see significance (good!)\\[\\begin{eqnarray*}\nPPV &=& \\frac{(1-\\beta^n)R}{(R+1-[1-\\alpha]^n - \\beta^n R)}\n\\end{eqnarray*}\\]\\(PPV \\downarrow\\) \\(n \\uparrow\\) unless \\((1-\\beta) < \\alpha\\) , \\(\\beta > 0.95\\) !!","code":""},{"path":"multiple-comparisons.html","id":"corollaries","chapter":"7 Multiple Comparisons","heading":"7.1.4 Corollaries","text":"Corollary 1 smaller studies conducted scientific field, less likely research findings true. (low power)Corollary 2 smaller effect sizes scientific field, less likely research findings true. (also low power)Corollary 3 greater number lesser selection tested relationships scientific field, less likely research findings true. (pre-study odds: R; phase III trials better odds microarray studies)Corollary 4 greater flexibility designs, definitions, outcomes, analytical modes scientific field, less likely research findings true. (increased bias)Corollary 5 greater financial interests prejudices scientific field, less likely research findings true. (increased bias)Corollary 6 hotter scientific field (scientific teams involved), less likely research findings true. (increased \\(n\\))","code":""},{"path":"multiple-comparisons.html","id":"multcomp","chapter":"7 Multiple Comparisons","heading":"7.2 Multiple Comparisons","text":"might expect, 10 groups, come population, might wrongly conclude means significantly different. try pairwise comparisons 10 groups, ’ll \\({10 \\choose 2} = 45\\) comparisons. 45 CI, ’d expect 2.25 contain true difference (zero); equivalently, ’d expect 2.25 tests reject true \\(H_0\\). overall test comparing 10 groups simultaneously, can’t use size performance measure anymore.","code":""},{"path":"multiple-comparisons.html","id":"familywise-error-rate","chapter":"7 Multiple Comparisons","heading":"7.2.1 Familywise Error Rate","text":"FWER = \\(P(V\\geq1)\\)Familywise Error Rate (FWER) probability making one type errors series multiple tests. example (10 comparisons), almost always make least one type error used size \\(\\alpha = 0.05\\). , FWER close 1. Methods exist controlling FWER instead size (\\(\\alpha\\)).","code":""},{"path":"multiple-comparisons.html","id":"bonferroni","chapter":"7 Multiple Comparisons","heading":"Bonferroni","text":"Bonferroni method adjusting multiple comparisons used control FWER.Assume tests null\n\\[\\begin{eqnarray*}\nA_1 &=& \\mbox{event reject test 1}\\\\\nP(A_1) &=& \\alpha^*\\\\\nA_2 &=& \\mbox{event reject test 2}\\\\\nP(A_2) &=& \\alpha^*\\\\\n\\end{eqnarray*}\\]want bound probability tests don’t commit type 1 error (, none reject).\n\\[\\begin{eqnarray*}\nP( A_1 \\mbox{ } A_2) &=& P(A_1) + P(A_2) - P(A_1 \\mbox{ } A_2)\\\\\n&=& \\alpha^* + \\alpha^* - ???\\\\\n& \\leq& 2 \\alpha^* \\\\\nFWER = P(\\mbox{least one rejects}) &\\leq& 2 \\alpha^*\\\\\n\\mbox{let} && P(A_1) = P(A_2) = \\alpha^* = \\frac{\\alpha}{2}\\\\\nFWER &\\leq& \\alpha\n\\end{eqnarray*}\\], \\(m\\) tests, rejecting test whose p-value less equal \\(\\alpha/m\\) control FWER \\(\\alpha\\). Alternatively, rejecting adjusted p-values less \\(\\alpha\\) also control FWER \\(\\alpha\\). adjusted p-values defined :\\[\\begin{eqnarray*}\n\\tilde{p}_j = \\min(m p_j, 1)\n\\end{eqnarray*}\\]\nReject hypothesis \\(\\tilde{p}_j \\leq \\alpha\\).","code":""},{"path":"multiple-comparisons.html","id":"holm","chapter":"7 Multiple Comparisons","heading":"Holm","text":"Holm less conservative (Bonferroni) method controlling FWER. p-value considered sequentially, Holm adjustment referred “step-.” intuition first step, bound assumes \\(m\\) null hypotheses. second step, \\(m-1\\) null hypotheses (assuming first p-value rejected).control FWER \\(\\alpha\\), first order p-values \\(p_1 \\leq p_2 \\leq \\cdots \\leq p_m\\) define adjusted p-values :\\[\\begin{eqnarray*}\n\\tilde{p}_j = \\max_{\\leq j} [ \\min((m-+1) p_i, 1) ]\n\\end{eqnarray*}\\]\nReject hypothesis \\(\\tilde{p}_j \\leq \\alpha\\).Note interpretations depend heavily number tests (true multiple comparison adjustments).Bonferroni extremely conservative, therefore high type II error rates (low power).aren’t really interested situation “true null tests don’t get rejected.”","code":""},{"path":"multiple-comparisons.html","id":"false-discovery-rate","chapter":"7 Multiple Comparisons","heading":"7.3 False Discovery Rate","text":"FDR = \\(E[V/R]\\)FDR (false discovery rate) expected proportion false discoveries discoveries.PPV = \\(E[S/R]\\)Consider \\(m\\) hypotheses:","code":""},{"path":"multiple-comparisons.html","id":"benjamini-hochberg","chapter":"7 Multiple Comparisons","heading":"Benjamini-Hochberg","text":"running experiments many many tests significance, often desirable worry number false discoveries opposed probability getting one false discovery. , typically comfortable false positives (.e., HIGH FWER) long rate false positives low.Define estimated false discovery proportion cutoff t (\\(\\hat{FDR}(t)\\)) number false discoveries given cutoff. , control FDR \\(\\alpha\\), first order p-values \\(p_1 \\leq p_2 \\leq \\cdots \\leq p_m\\).\\[\\begin{eqnarray*}\n\\hat{FDR}(t)&=& \\frac{\\# \\{ p_j \\leq t \\mbox{ null tests } \\}}{ \\# \\{p_j \\leq t\\} + (1 \\mbox{ } p_j > t)}\n\\end{eqnarray*}\\]\nNotice p-values null hypotheses distributed uniformly zero one. means good estimate numerator \\(t\\cdot\\) # null tests.\n\\[\\begin{eqnarray*}\n\\hat{FDR}(p_j) &=& \\frac{p_j \\cdot m \\cdot \\pi_0}{j} < \\frac{p_j m}{j}\n\\end{eqnarray*}\\]\n\\(\\pi_0\\) proportion tests truly null (\\(m_0/m\\)). Consider adjusted p-values,\\[\\begin{eqnarray*}\n\\tilde{p}_j = \\min [ (m/j) p_j, \\tilde{p}_{j+1} ]\n\\end{eqnarray*}\\]\nReject hypothesis \\(\\tilde{p}_j \\leq \\delta\\) control FDR \\(\\delta\\).Intuition: let \\(m=1000\\), suppose \\(\\tilde{p}_{250} < 0.4\\). Show FDR \\(< 0.4\\).\\(\\tilde{p}_{250} < 0.4\\) implies two different things:\\(p_{250} \\leq \\frac{0.4\\cdot 250}{1000} = 0.1\\)\\(\\approx 750\\) null tests p-values 0.1 1.750 null tests p-values 0.1 1, \\(m_0 \\cdot 0.9 = 750 \\rightarrow m_0 = 833.33\\). Therefore, number null hypotheses rejected 833.33 - 750 = 83.33.\\[\\begin{eqnarray*}\nFDR = \\frac{83.33}{250} = 0.33 < 0.4\n\\end{eqnarray*}\\]generally \\(\\tilde{p}_k = \\frac{p_k \\cdot m}{k} \\leq \\delta\\):\n\\[\\begin{eqnarray*}\np_k &<& \\frac{k}{m} \\delta\\\\\n\\# \\mbox{ null rejected} &=& \\mbox{length null interval }\\bigg[\\frac{k}{m}\\delta,1\\bigg] \\cdot \\mbox{ total } \\# \\mbox{ null}\\\\\n(m-k) &=& \\bigg(1-\\frac{k}{m} \\delta\\bigg) m_0\\\\\nm_0 &=& \\frac{(m-k) m}{(m-k \\delta)}\\\\\n& &\\\\\nFDR &=& \\frac{\\# \\mbox{ null rejected}}{\\# \\mbox{ rejected}}\\\\\n&=& \\frac{\\frac{m(m-k)}{(m-k\\delta)} - (m-k)}{k}\\\\\n&=& \\frac{m^2 - mk - (m-k) (m-k \\delta)}{(m-k \\delta)k}\\\\\n&=& \\frac{k(m \\delta - k \\delta)}{k(m-k \\delta)} = \\delta\\bigg(\\frac{m-k}{m/\\delta - k}\\bigg)\\\\\nFDR &<& \\delta\n\\end{eqnarray*}\\]\n\\(m/\\delta > m\\) \\(\\frac{m-k}{m/\\delta -k} < 1\\).Consider one sample t-test. population normal centered 47 \\(\\sigma=3\\); samples size 20 taken population. following hypotheses tested:\n\\[\\begin{eqnarray*}\nH_0: \\mu = 47\\\\\nH_{0_1}: \\mu = 40\\\\\nH_{0_2}: \\mu = 48\\\\\nH_{0_3}: \\mu = 50\\\\\n\\end{eqnarray*}\\]\nnull setting, p-values uniformly distributed 0 1. data consistent null, p-values close zero (even closer zero data become distinct null).","code":""},{"path":"multiple-comparisons.html","id":"distribution-of-p-values-under-different-amounts-of-divergence-from-the-null-hypothesis.","chapter":"7 Multiple Comparisons","heading":"Distribution of p-values under different amounts of divergence from the null hypothesis.","text":"previous example considered situation p-values came distribution. reality, p-values come many different distributions. simplification, consider situation g100% tests come null distribution, (1-g)100% tests come distribution p-values much closer one skewed right.","code":""},{"path":"multiple-comparisons.html","id":"distribution-of-p-values-under-differing-proportions-of-null-versus-significant-tests.","chapter":"7 Multiple Comparisons","heading":"Distribution of p-values under differing proportions of null versus significant tests.","text":"","code":""},{"path":"multiple-comparisons.html","id":"qvals","chapter":"7 Multiple Comparisons","heading":"7.3.1 Storey & q-values","text":"previous methods allowed larger family tests control either FWER FDR (global measures accuracy). However, given test, measure quantify FDR given test.p-value Recall p-value smallest level significance (P(type error)) possible reject \\(H_0\\).q-value Akin p-value, q-value minimum FDR score can attained calling test significant.Storey defines q-value FDR associated given test significance. example, say q-value = 0.013 test X. 1.3% tests p-values least small test X false positives. particular, let\n\\[\\begin{eqnarray*}\n\\hat{\\pi}_0 = \\frac{\\# \\{ p_j > \\lambda \\} }{m(1-\\lambda)} \\ \\ \\  \\mbox{ } \\lambda\n\\end{eqnarray*}\\]\n(though many ways estimate \\(\\pi_0\\).)step-algorithm, q-value defined using p-value hand next {} p-value. Additionally, \\(\\pi_0\\) implemented seen original intuition behind FDR control.step 1 let\n\\[\\begin{eqnarray*}\n\\hat{q}(p_{m}) = \\hat{\\pi}_0 p_{m}\n\\end{eqnarray*}\\]\nNote \\(\\hat{q}(p_{m})\\) biggest q-value $ p_{m}$ biggest p-value.step 2\n\\[\\begin{eqnarray*}\n\\hat{q}(p_{m-1}) = \\min( \\hat{\\pi}_0 p_{m-1} \\frac{m}{m-1}, \\hat{q}( p_{m}))\n\\end{eqnarray*}\\]\n\\(\\hat{q}(p_{m}) = 0.7\\) \\(\\hat{\\pi}_0 p_{m-1} \\frac{m}{m-1} = 0.8\\), next smallest q-value 0.7 FDR can low 0.7 (see definition FDR).step 3 generally,\n\\[\\begin{eqnarray*}\n\\hat{q}(p_{j}) = \\min( \\hat{\\pi}_0 p_{j} \\frac{m}{j}, \\hat{q}( p_{j+1}))\n\\end{eqnarray*}\\]Can q-value less p-value? Sure! number null hypotheses small test powerful. example, consider testing 1000 hypotheses 20% null tests (\\(\\pi_0=0.2\\)). Assume 500 p-values less 0.05 (powerful!). 200 null expect 10 less 0.05. , FDR 10/500 = 0.02 (smaller p-value 0.05 cutoff \\(500^{th}\\) test).","code":""},{"path":"multiple-comparisons.html","id":"how-are-fwer-and-fdr-related","chapter":"7 Multiple Comparisons","heading":"How are FWER and FDR related?","text":"First note FDR FWER, procedure control errors compute estimate errors (except case q-value).Recall \\(FDR = E[V/R]\\) defined equal zero R=0\\[\\begin{eqnarray*}\nFDR &=& E\\bigg[\\frac{V}{R} \\bigg]\\\\\n&=& E\\bigg[\\frac{V}{R} | R > 0 \\bigg] P(R >0) + E\\bigg[\\frac{V}{R} | R=0 \\bigg] P(R=0)\\\\\n&=& E\\bigg[\\frac{V}{R} | R > 0 \\bigg] P(R >0)\n\\end{eqnarray*}\\]case 1 first case, consider \\(V=R\\) significant hypotheses null.\n\\[\\begin{eqnarray*}\nFDR &=& E\\bigg[\\frac{V}{R} | R > 0 \\bigg] P(R >0)\\\\\n&=& 1 P(R>0) = 1 P(V \\geq 1)\\\\\n&=& FWER\n\\end{eqnarray*}\\]case 2 second case, consider \\(V < R\\) significant hypotheses null . (\\(V/R < 1\\))\n\\[\\begin{eqnarray*}\nFDR &=& E\\bigg[\\frac{V}{R} | R > 0 \\bigg] P(R >0)\\\\\n&=& E\\bigg[\\frac{V}{R} | R > 0, V\\geq 1 \\bigg] P(R >0, V \\geq 1) + E\\bigg[\\frac{V}{R} | R > 0, V=0 \\bigg] P(R >0, V =0)\\\\ \n&& \\mbox{ (note: } V/R \\equiv 0 \\mbox{ } V = 0)\\\\\n&=& E\\bigg[\\frac{V}{R} | R > 0, V\\geq 1 \\bigg] P(R >0, V \\geq 1) \\\\\n&<& P(V \\geq 1) = FWER \\ \\ \\mbox{ } V/R < 1\n\\end{eqnarray*}\\]proof shows FWER controls FDR significant tests null. significant tests null, FWER=FDR.","code":""},{"path":"multiple-comparisons.html","id":"interim","chapter":"7 Multiple Comparisons","heading":"7.4 Interim Analyses","text":"important application multiple comparisons issues comes deciding whether stop clinical trial early due either positive negative results. Looking data often create false positives can quite problematic. (Schulz Grimes 2005)Consider following case studies:HIV – Indinavir stopped early due positive results (Scott M. Hammer et al. 1997)HVTN 505 stopped early due negative results (Yunda Huang et al. 2015)Truvada & Tenofovir also stopped early (Dyer 2004)happens research performs \\(k\\) interim analyses research scenario truly null?\n\\[\\begin{eqnarray*}\nP( test1 < \\alpha \\mbox{ } test2 < \\alpha \\mbox{ } \\ldots \\mbox{ }  testk < \\alpha ) > \\alpha\n\\end{eqnarray*}\\]researcher two options:Let \\(\\alpha^* < < < \\alpha\\)Change \\(\\alpha\\) step along way total probability type error \\(\\alpha\\)","code":""},{"path":"multiple-comparisons.html","id":"different-p-value-cutoff-choices","chapter":"7 Multiple Comparisons","heading":"7.4.1 Different p-value cutoff choices","text":"Different \\(\\alpha^*\\) values three different stopping criteria. Note Peto control test overall \\(\\alpha=0.05\\), although close.Sometimes interim criteria referred “alpha spending” total type error rate (, rejecting original claim interim analysis steps \\(H_0\\) true) \\(\\alpha\\), \\(\\alpha\\) probability type error spread interim tests.","code":""},{"path":"multiple-comparisons.html","id":"pocock","chapter":"7 Multiple Comparisons","heading":"Pocock","text":"Pocock boundaryAdvantages:simpleaggressive respect stopping early. , small expected sample size (effect size large)Disadvantage:low power therefore large maximum sample size (effect size small)","code":""},{"path":"multiple-comparisons.html","id":"obrien-fleming","chapter":"7 Multiple Comparisons","heading":"O’Brien-Fleming","text":"Advantages:final \\(\\alpha\\) close desired \\(\\alpha\\)power Pocock, smaller max sample sizeDisadvantage:less likely stop early, larger expected sample size","code":""},{"path":"multiple-comparisons.html","id":"simulation-1","chapter":"7 Multiple Comparisons","heading":"7.4.2 Simulation","text":"simulation based pair blogs Ed Berry ().Every simulation done null hypothesis. , effects find. worried situation null hypothesis rejected (time points).following done 1000 times (1000 lines created, plots 100 lines can see lines clearly).following done 1000 times (1000 lines created, plots 100 lines can see lines clearly).week 1, generate 500 people (250 group) measure whether reject \\(H_0\\). Note shouldn’t rejecting treatment effect!week 1, generate 500 people (250 group) measure whether reject \\(H_0\\). Note shouldn’t rejecting treatment effect!week 2, 500 people generated. Also weeks 3 4.week 2, 500 people generated. Also weeks 3 4.line colored purple study rejected point. Note actual studies, study rejected (typically) continued. continue weeks past rejection happened (simulation graphing simplicity).line colored purple study rejected point. Note actual studies, study rejected (typically) continued. continue weeks past rejection happened (simulation graphing simplicity).simulated type 1 error rate number studies rejected weeks (1000).","code":""},{"path":"multiple-comparisons.html","id":"raw-p-values","chapter":"7 Multiple Comparisons","heading":"Raw p-values","text":"Note horizontal black line 0.05. simulated type 1 error rate (proportion purple lines 1000 lines) :","code":"#> # A tibble: 1 x 1\n#>   type1\n#>   <dbl>\n#> 1 0.131"},{"path":"multiple-comparisons.html","id":"bonferroni-1","chapter":"7 Multiple Comparisons","heading":"Bonferroni","text":"Note horizontal black line 0.05/4 = 0.0125; gray horizontal line 0.05. simulated type 1 error rate (proportion purple lines 1000 lines) :","code":"#> # A tibble: 1 x 1\n#>   type1\n#>   <dbl>\n#> 1 0.027"},{"path":"multiple-comparisons.html","id":"pocock-1","chapter":"7 Multiple Comparisons","heading":"Pocock","text":"Note horizontal black line 0.018; gray horizontal line 0.05. simulated type 1 error rate (proportion purple lines 1000 lines) :","code":"#> # A tibble: 1 x 1\n#>   type1\n#>   <dbl>\n#> 1 0.048"},{"path":"multiple-comparisons.html","id":"peto","chapter":"7 Multiple Comparisons","heading":"Peto","text":"Note horizontal black line 0.001 first three weeks 0.05 fourth week. gray horizontal line 0.05. simulated type 1 error rate (proportion purple lines 1000 lines) (note expect type 1 error rate 5%):","code":"#> # A tibble: 1 x 1\n#>   type1\n#>   <dbl>\n#> 1 0.054"},{"path":"multiple-comparisons.html","id":"obrien-fleming-1","chapter":"7 Multiple Comparisons","heading":"O’Brien-Fleming","text":"Note horizontal black lines 0.0001, 0.004, 0.019, 0.043. gray horizontal line 0.05. simulated type 1 error rate (proportion purple lines 1000 lines) :","code":"#> # A tibble: 1 x 1\n#>   type1\n#>   <dbl>\n#> 1 0.049"},{"path":"multiple-comparisons.html","id":"some-parting-thoughts","chapter":"7 Multiple Comparisons","heading":"7.4.3 Some parting thoughts","text":"\\(H_A\\) true, effect size likely overestimated (true general, just interim analyses).Symmetry: important, harm good?Can split alpha two halves apply one method (e.g., Pocock) stopping early positive reasons apply another method (e.g., O’Brien-Fleming) stopping early negative reasons.","code":""},{"path":"multiple-comparisons.html","id":"advice-1","chapter":"7 Multiple Comparisons","heading":"7.4.3.1 Advice 1","text":"following quote general advice statistical researchers clinical oncology. (Green, Benedetti, Crowley 1997)Even specifics basic task data monitoring committee, evaluation interim results evidence benefit harm, necessarily obvious. Questions (personal answers) include:often data monitoring committee review interim data? (answer depend fast additional information becomes available trial. generally recommend monitoring advanced disease studies, study rapidly accumulating events, every 6 months. Yearly monitoring may sufficient adjuvant slowly accruing studies.)primary outcome data reviewed time reviewed times planned interim analyses? (data, including primary outcome data, reviewed time, since unexpected occur.)treatment arms blinded data monitoring committee ? (Definitely . looks better B, decision continue well different control arm instead experimental arm.)data monitoring committee decision evidence sufficient close trail final, advisory ? (say advisory, rarely overturned.)advisory, advisory - funding agency? executive group? investigators? (Reports go individuals ultimate responsibility integrity trial.)data monitoring committee able make major design changes trial? (, data monitoring committee may offer suggestions design responsibility principal investigators. hand, major design changes initiated principal investigators approved committee.)data monitoring committee duties study accrual complete, data monitoring committee also decide results reported? (also decide results reported - additional follow-generates additional data still need monitored.)much weight accorded outside information versus current information study monitored? (Definitive outside information ignored - begs question definitive. single trial moderate size probably definitive; two large trials probably ; meta-analysis probably .)much results secondary endpoints influence decision continue ? (much unless toxic death considered secondary.)scary results stop time planned interim analysis? (scary, purpose interim analyses defeated.)accrual problems justify early closure? (results won’t available longer interest.)confidential information ever provided data monitoring committees planning groups? (Sometimes. study conduct compromised limited release information, might reasonable let investigators planning new trials know potential problems benefits treatment arms considering. Risk ongoing trial includes leaked information intelligent guesses current status; risk new trial includes choosing inappropriate arm based early results don’t hold .)Every monitoring committee functions differently two people ethical, scientific, practical perspectives. means different committees might well come different answers monitoring issues. ensure balance opinions, best variety knowledgeable people members committee.","code":""},{"path":"multiple-comparisons.html","id":"advice-2","chapter":"7 Multiple Comparisons","heading":"7.4.3.2 Advice 2","text":"Duncan Temple-Lang leader area combining computer science research concepts within context statistics science generally. Recently, invited participate workshop, Training Students Extract Value Big Data. workshop subsequently summarized manuscript name provided free charge. http://www.nap.edu/catalog.php?record_id=18981Principles Data Science Process, Duncan Temple Lang, University California, Davis (National Research Council: Training Students Extract Value Big Data 2014)Duncan Temple Lang began listing core concepts data science - items need taught: statistics machine learning, computing technologies, domain knowledge problem. stressed importance interpretation reasoning - methods - addressing data. Students work data science broad set skills - including knowledge randomness uncertainty, statistical methods, programming, technology - practical experience . Students tend computing statistics classes entering graduate school domain science.Temple Lang described data analysis pipeline, outlining steps one example \ndata analysis exploration process:Asking general question.Asking general question.Refining question, identifying data, understanding data metadata. Temple Lang\nnoted data used usually collected specific question hand, original experiment data set understood.Refining question, identifying data, understanding data metadata. Temple Lang\nnoted data used usually collected specific question hand, original experiment data set understood.Access data. unrelated science require computational skill.Access data. unrelated science require computational skill.Transforming data structures.Transforming data structures.Exploratory data analyses understand data determine whether results scale.\ncritical step; Temple Lang noted 80 percent data scientist’s time can spent cleaning preparing data.Exploratory data analyses understand data determine whether results scale.\ncritical step; Temple Lang noted 80 percent data scientist’s time can spent cleaning preparing data.Dimension reduction. Temple Lang stressed can difficult impossible automate\nstep.Dimension reduction. Temple Lang stressed can difficult impossible automate\nstep.Modeling estimation. Temple Lang noted computer machine learning scientists\ntend focus predictive models modeling physical behavior characteristics.Modeling estimation. Temple Lang noted computer machine learning scientists\ntend focus predictive models modeling physical behavior characteristics.Diagnostics. helps understand well model fits data identifies\nanomalies aspects study. step similarities exploratory data analysis.Diagnostics. helps understand well model fits data identifies\nanomalies aspects study. step similarities exploratory data analysis.Quantifying uncertainty. Temple Lang indicated quantifying uncertainty statistical\ntechniques important understanding interpreting models results.Quantifying uncertainty. Temple Lang indicated quantifying uncertainty statistical\ntechniques important understanding interpreting models results.Conveying results.Conveying results.Temple Lang stressed data analysis process highly interactive iterative requires presence human loop. next step data processing often clear results current step clear, often something unexpected uncovered. also emphasized importance abstract skills concepts said people need exposed authentic data analyses, methods used. Data scientists also need statistical understanding, Temple Lang described statistical concepts taught student:Mapping general question statistical framework.Understanding scope inference, sampling, biases, limitations.Exploratory data analyses, including missing values, data quality, cleaning, matching, \nfusing.Understanding randomness, variability, uncertainty. Temple Lang noted many\nstudents understand sampling variability.Conditional dependence heterogeneity.Dimension reduction, variable selection, sparsity.Spurious relationships multiple testing.Parameter estimation versus “black box” prediction classification.Diagnostics, residuals, comparing models.Quantifying uncertainty model.Sampling structure dependence data reduction. Temple Lang noted modeling \ndata becomes complicated variables independent, identically distributed.Statistical accuracy versus computational complexity efficiency.Temple Lang briefly discussed practical aspects computing, including \nfollowing:Accessing data.Manipulating raw data.Data structures storage, including correlated data.Visualization stages (particularly exploratory data analyses conveying \nresults).Parallel computing, can challenging new student.Translating high-level descriptions optimal programs.discussion, Temple Lang proposed computing statistics visualizations examine\ndata rigorously statistical automated way. explained “scagnostics” (scatter plot diagnostics) data analysis technique graphically exploring relationships among variables. small set statistical measures can characterize scatter plots, exploratory data analysis can conducted residuals.(information scagnostics can found Wilkinson Wills (2007).)workshop participant noted difference data error data blunder. blunder large, easily noticeable mistake. participant gave example shipboard observations cloud cover; blunders, case, occur location ship observation given land rather sea. Another blunder case ship’s changing location quickly. participant speculated blunders generalized detect problematic observations, although tools need scalable applied large data sets.","code":""},{"path":"poisson-regression.html","id":"poisson-regression","chapter":"8 Poisson Regression","heading":"8 Poisson Regression","text":"","code":""},{"path":"poisson-regression.html","id":"regPois","chapter":"8 Poisson Regression","heading":"8.1 Regression Models","text":"Consider following example Poole (1989) (described Ramsey Schafer (2012)) age mating success (number successful matings) male African Elephants.seem like increasing trend two variables, concerns directly applying linear regression. Indeed, example acts proxy many regression examples response variable count particular properties:response variable count (particular, negative).response variable count (particular, negative).model may may linear X.model may may linear X.model appears increasing variance function X.model appears increasing variance function X.setting , often preferable use Poisson regression instead normal errors linear regression.","code":"\nelephants <- readr::read_csv(\"elephants.csv\")\n\nggplot(elephants, aes(x=jitter(AGE), y=MATINGS)) + geom_point()"},{"path":"poisson-regression.html","id":"the-poisson-regression-model","chapter":"8 Poisson Regression","heading":"8.1.1 The Poisson Regression Model","text":"Poisson distribution given probability function form:\\[P(Y = y) = \\frac{e^{-\\mu} \\mu^y}{y!} \\ \\ \\ \\ \\ y=0, 1, 2, \\ldots\\]gives: \\(E(Y) = \\mu\\) \\(Var(Y) = \\mu\\). , Poisson model characterized mean variance given value.Recall linear regression, \\(E(Y_i) = \\beta_0 + \\beta_1 X_i\\) might reasonable idea apply count data; however, seen , mean distribution modeled strictly linear function \\(X\\), line potential predict negative counts variability function \\(X\\) normal errors regression used.alternative normal errors regression use \\(\\ln\\) transformation describe relationship predicted value response explanatory variables interest:\\[\\ln(E(Y_i)) = \\ln(\\mu_i) = \\beta_0 + \\beta_1 X_i.\\]\nobserved counts come Poisson model: \\(Y_i \\sim Pois(\\mu_i)\\) Poisson parameter given function explanatory variable(s). [Note Poisson regression contains error term like linear regression Poisson distribution inherent variability determined mean equals variance.]","code":""},{"path":"poisson-regression.html","id":"technical-conditions-1","chapter":"8 Poisson Regression","heading":"8.1.1.1 Technical Conditions","text":"Like every model, technical conditions associated Poisson Regression. closer data (population) conform conditions, useful model describing context hand. Remember,\n> models wrong, models useful.\" -George Box., assessing whether technical conditions reasonable help determining whether analysis performed particular model good thing .Line: log mean linear function \\(X\\): \\(\\ln(\\mu_i) = \\beta_0 + \\beta_1 X_i.\\)Independence: observations independent (often characterized simple random sample something approximating simple random sample).Poisson: response variable count.Error: mean response variables equal variance response variable combination explanatory variables model.Visualizing Normal vs Poisson Error Regression Legler Roback (2019)","code":""},{"path":"poisson-regression.html","id":"comparison-to-linear-regression","chapter":"8 Poisson Regression","heading":"8.1.2 Comparison to Linear Regression","text":"One question might come mind whether difference Poisson log-linear model normal errors regression model log transformation response??? short answer yes, difference.Poisson Log-Linear: \\(\\ln(E(Y_i)) = \\beta_0 + \\beta_1 X_i, Y_i \\sim Pois(e^{\\beta_0 + \\beta_1 X_i})\\)Poisson Log-Linear: \\(\\ln(E(Y_i)) = \\beta_0 + \\beta_1 X_i, Y_i \\sim Pois(e^{\\beta_0 + \\beta_1 X_i})\\)Normal w Log Transformation: \\(E(\\ln(Y_i)) = \\beta_0 + \\beta_1 X_i, \\ln(Y_i) \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)Normal w Log Transformation: \\(E(\\ln(Y_i)) = \\beta_0 + \\beta_1 X_i, \\ln(Y_i) \\sim N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\\)two main differences models.first remember average (.e., expected value) logs log averages. Poisson model, linear function measures log average, normal model, linear function measures average logs.second difference note across two regression set-ups variability modeled differently. Indeed, likelihood functions quite different produce different maximum likelihood estimates parameter values.Differences Normal vs Poisson Error Regression Legler Roback (2019)","code":"\nset.seed(47)\nexample <- abs(rcauchy(10))\nexample\n#>  [1]  0.0725  2.3911  0.9302  0.6237  4.2508  1.4575  2.7521 10.2177  7.3042\n#> [10]  0.2404\n\nlog(mean(example))\n#> [1] 1.11\nmean(log(example))\n#> [1] 0.343"},{"path":"poisson-regression.html","id":"interpreting-poisson-regression-coefficients","chapter":"8 Poisson Regression","heading":"8.1.3 Interpreting Poisson Regression Coefficients","text":"’ve done generalized linear models (linear regression, logistic regression, even survival analysis!), order understand model completely, look impact response variable one unit change \\(X\\).Consider \\(X\\) \\(X+1\\).\\[\\frac{E(Y|X+1)}{E(Y|X)} = \\frac{e^{\\beta_0 + \\beta_1 (X+1)}}{e^{\\beta_0 + \\beta_1 (X)}} = e^{\\beta_1}\\], \\(e^{\\beta_1}\\) represents ratio means one unit increase \\(X\\). elephant example, every additional year life, expect elephant’s mating success, average, change factor \\(e^{\\beta_1}\\). [savvy consumer, might note additional contrast normal error regression log transformed Y required interpret multiplicative change median. Poisson, log taken average, taking inverse log gives mean directly. Previously, necessary use identity: \\(median(\\ln(Y)) = \\ln(median(Y))\\).]","code":""},{"path":"poisson-regression.html","id":"assessing-model-validity","chapter":"8 Poisson Regression","heading":"8.1.4 Assessing Model Validity","text":"Just linear regression used scatterplots give sense whether linear regression appropriate, can use exploratory data analysis (including scatterplot!) give sense whether Poisson regression appropriate model. (EDA steps based excellent descriptions Poisson model building Legler Roback 2019.)Technical Condition 3, Poisson: Let’s first look response variable. histogram number successful matings shows right skew typically acceptable normal errors regression (although, remember, value response variable dependent value explanatory variable!)\nLikely, makes sense look distribution response variable value explanatory variables. sample size limited, group age variable 5 year increments. Looking plot , seems though, even conditioning explanatory variable, response right skewed variance dependent mean. might also good find sample mean per group plot Poisson probabilities onto bar.Technical Condition 4, Error: check whether mean variance similar, can calculate values per group (data probably groups explanatory variable, following analysis done scatterplot means x-axis variance y-axis). Note mean variance reasonably similar! “mean=variance” condition violated, almost always violated variance even bigger expected Poisson model. Large variance called overdispersion, methods measuring accounting overdispersion given following section 8.2.1.2.Technical Condition 1, Line: Poisson model implies log mean linear function explanatory variable:\\[\\ln(\\mu_i) = \\beta_0 + \\beta_1 X_i,\\]\nmeans ’d really like plot \\(\\mu_i\\) function explanatory variable. Unfortunately, \\(\\mu_i\\) unknown, plotted. can, however, plot log average value response group x values close one another. plot , ’ve grouped observations based age elephants within 3 years years . actually quite linear! points don’t follow linear relationship based age groups observations.","code":"#> # A tibble: 3 x 5\n#>   AGEgroups  mean variance stdev     n\n#>   <chr>     <dbl>    <dbl> <dbl> <int>\n#> 1 (20,30]    1.08    0.992 0.996    12\n#> 2 (30,40]    2.71    2.85  1.69     17\n#> 3 (40,55]    4.25    7.84  2.80     12\nmatelogmean <- elephants %>%\n  dplyr::mutate(agecut = cut(AGE, breaks=seq(26.5,53.5,by=3))) %>%\n  group_by(agecut) %>%\n  summarize(meanmate = mean(MATINGS), logmate = log(mean(MATINGS)), n = n() )\n\nelephantsGRP <- cbind(matelogmean, age = c(seq(26.5,52.5,by=3)+1.5 ))\n\nelephantsGRP\n#>        agecut meanmate logmate  n age\n#> 1 (26.5,29.5]     1.09   0.087 11  28\n#> 2 (29.5,32.5]     1.50   0.405  2  31\n#> 3 (32.5,35.5]     2.44   0.894  9  34\n#> 4 (35.5,38.5]     3.50   1.253  6  37\n#> 5 (38.5,41.5]     2.00   0.693  2  40\n#> 6 (41.5,44.5]     3.57   1.273  7  43\n#> 7 (44.5,47.5]     6.00   1.792  2  46\n#> 8 (47.5,50.5]     2.00   0.693  1  49\n#> 9 (50.5,53.5]     9.00   2.197  1  52\n\nelephantsGRP %>% ggplot(aes(x=age, y=logmate)) + geom_point()"},{"path":"poisson-regression.html","id":"inferPois","chapter":"8 Poisson Regression","heading":"8.2 Inference in Poisson Regression","text":"Recall Poisson regression, \\(E(Y_i) = \\mu_i = e^{\\beta_0 + \\beta_1 X_i}\\). Therefore, probability function \\(^{th}\\) observation :\\[f(y_i) = \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!} = \\frac{e^{-e^{\\beta_0 + \\beta_1 X_i}} \\bigg(e^{\\beta_0 + \\beta_1 X_i}\\bigg)^{y_i}}{y_i!}.\\]\ngives resulting likelihood :\\[L(\\beta_0, \\beta_1) = \\prod_{=1}^n f(y_i) = \\prod_{=1}^n \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!} = \\prod_{=1}^n \\frac{e^{-e^{\\beta_0 + \\beta_1 X_i}} \\bigg(e^{\\beta_0 + \\beta_1 X_i}\\bigg)^{y_i}}{y_i!}.\\]log likelihood becomes:\\[\\begin{eqnarray*}\nl(\\beta_0, \\beta_1) &=& \\ln L(\\beta_0, \\beta_1) = \\sum_{=1}^n \\bigg(-\\mu_i + y_i \\ln(\\mu_i) - \\ln(y_i!) \\bigg) \\\\\n&=& \\sum_{=1}^n \\bigg(-e^{\\beta_0 + \\beta_1 X_i} + y_i (\\beta_0 + \\beta_1 X_i) - \\ln(y_i!) \\bigg).\n\\end{eqnarray*}\\]","code":""},{"path":"poisson-regression.html","id":"maximum-likelihood","chapter":"8 Poisson Regression","heading":"8.2.1 Maximum Likelihood","text":"probabilistic models ’ve encountered, joint likelihood entire sample represents product individual likelihoods data value. likelihood (typically, \\(\\ln\\)-likelihood) maximized find estimates \\(\\beta_0\\) \\(\\beta_1\\).parameter estimates maximize \\(\\ln\\)-likelihood, SE estimates given Fisher Information likelihood (roughly second derivative).Given \\(b_0\\) \\(b_1\\) estimates parameter values, \\(\\beta_0\\) \\(\\beta_1\\), can estimate average count value:\\[\\hat{\\mu}_i = e^{b_0 + b_1 X_i}\\]","code":""},{"path":"poisson-regression.html","id":"residuals","chapter":"8 Poisson Regression","heading":"8.2.1.1 Residuals","text":"Note expect residuals Poisson model larger variability larger values prediction. makes interpreting residuals slightly different previously linear regression model.Pearson residual:\\[res_{pi} = \\frac{y_i - \\hat{\\mu}_i}{\\hat{\\mu}_i}\\]Deviance residual:\\[\\begin{eqnarray*}\nres_{di} &=& [-\\hat{\\mu}_i + y_i \\ln(\\hat{\\mu}_i) - \\ln(y_i!) ] - [-y_i + y_i \\ln(y_i) - \\ln(y_i!) ] \\\\\n&=& y_i-\\hat{\\mu}_i + y_i \\ln\\bigg(\\frac{\\hat{\\mu}_i}{y_i}\\bigg)\n\\end{eqnarray*}\\]Along giving information individual residual values, sum Deviance residuals also way test goodness--fit model. Given null hypothesis Poisson appropriate model, test goodness--fit Poisson model given :\\[\\sum_{=1}^n res_{di} \\stackrel{H_0}{\\sim} \\chi_{n-p}^2.\\]","code":""},{"path":"poisson-regression.html","id":"overdis","chapter":"8 Poisson Regression","heading":"8.2.1.2 Dealing with Overdispersion","text":"probability structure given variance larger mean, may end :\\[\\begin{eqnarray*}\nE(Y_i) &=& \\mu_i\\\\\nVar(Y_i) &=& \\phi \\mu_i\\\\\n\\end{eqnarray*}\\]can estimate \\(\\phi\\), appropriate SE use adjusted \\(\\phi\\): \\(SE_Q(\\hat{\\beta}) = \\sqrt{\\phi} SE(\\hat{\\beta})\\). (\\(Q\\) stands “quasiPoisson.”)\\(Y\\) comes setting sample size large, can use normal (sums standard normal random variables distributed according \\(\\chi^2\\) distribution) theory assess residuals:\\[\\sum_{=1}^n res_{pi}^2 = \\sum_{=1}^n \\frac{(Y_i - \\mu_i)^2}{Var(Y_i)} = \\sum_{=1}^n \\frac{(Y_i - \\mu_i)^2}{\\phi \\mu_i} \\sim \\chi^2_{n-p}\\]expected value \\(\\chi^2_{n-p}\\) \\(n-p\\), gives estimator \\(\\phi\\) :\\[\\hat{\\phi} =  \\sum_{=1}^n \\frac{(Y_i - \\mu_i)^2}{\\mu_i} /(n-p).\\]Note text uses Deviance residual instead Pearson residual estimate \\(\\phi\\). reasonable things .","code":""},{"path":"poisson-regression.html","id":"wald-tests","chapter":"8 Poisson Regression","heading":"8.2.2 Wald Tests","text":", Wald tests given standardizing coefficients finding p-values using normal theory:\\[\\frac{b_i - 0}{SE(b_i)} \\stackrel{H_0}{\\sim} N(0,1).\\]overdispersion expected, SE adjusted t-distribution used calculating p-values:\\[\\frac{b_i - 0}{SE_Q(b_i)} =  \\frac{b_i - 0}{\\sqrt{\\hat{\\phi}} SE(b_i)} \\stackrel{H_0}{\\sim} t_{n-p}.\\]","code":""},{"path":"poisson-regression.html","id":"drop-in-deviance-tests","chapter":"8 Poisson Regression","heading":"8.2.3 Drop-in-Deviance Tests","text":"deviance Poisson reasonably straightforward comes directly likelihood, twice sum deviance residuals:\\[D = 2 \\sum_{=1}^n [Y_i \\ln(Y_i / \\hat{\\mu_i}) - (Y_i - \\hat{\\mu_i})].\\]two nested models (, smaller one obtained forcing coefficients larger model zero), drop--deviance test can calculated using:\\[D_{reduced} - D_{full} \\stackrel{H_0}{\\sim} \\chi^2_{d}\\]\\(d\\) difference number parameters estimated two models.drop--deviance test can also adjusted overdispersion: \\(F_Q = (D_{reduced} - D_{full}) / \\hat{\\phi} \\sim F_{d, n-p}\\) \\(d\\) difference number parameters estimated two models, \\(p\\) total number parameters estimated full model.","code":""},{"path":"poisson-regression.html","id":"r-poisson-example","chapter":"8 Poisson Regression","heading":"8.3 R Poisson Example","text":"R example taken data given textbook. scientific question relates predicting total number observed species Galapagos archipelago related island area (km\\(^2\\)), elevation (m), distance (km) nearest neighbor largest island (km\\(^2\\)) archipelago Santa Cruz (scruz), area adjacent island (km\\(^2\\)). also consider additional response variable island endemic species count.","code":"\ngalap <- readr::read_csv(\"Galapagos.csv\")\nhead(galap)\n#> # A tibble: 6 x 8\n#>   island      species endemics  area elevation nearest scruz adjacent\n#>   <chr>         <dbl>    <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>\n#> 1 Baltra           58       23 25.1        346     0.6   0.6     1.84\n#> 2 Bartolome        31       21  1.24       109     0.6  26.3   572.  \n#> 3 Caldwell          3        3  0.21       114     2.8  58.7     0.78\n#> 4 Champion         25        9  0.1         46     1.9  47.4     0.18\n#> 5 Coamano           2        1  0.05        77     1.9   1.9   904.  \n#> 6 DaphneMajor      18       11  0.34       119     8     8       1.84\nggplot(galap, aes(y=species, x = log(area), color = adjacent)) + geom_point()"},{"path":"poisson-regression.html","id":"glm","chapter":"8 Poisson Regression","heading":"8.3.1 glm","text":"","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% tidy()\n#> # A tibble: 6 x 5\n#>   term            estimate std.error statistic  p.value\n#>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     3.02     0.303         9.96  2.28e-23\n#> 2 log(area)       0.315    0.0185       17.1   2.20e-65\n#> 3 log(elevation)  0.0977   0.0604        1.62  1.06e- 1\n#> 4 nearest        -0.00106  0.00169      -0.626 5.32e- 1\n#> 5 scruz          -0.00314  0.000597     -5.26  1.40e- 7\n#> 6 adjacent       -0.000243 0.0000281    -8.65  5.31e-18"},{"path":"poisson-regression.html","id":"drop-in-deviance","chapter":"8 Poisson Regression","heading":"8.3.2 drop in deviance","text":"seems like might need either log(elevation) nearest. drop deviance test help:relatively large p-value suggests need either variables log(elevation) nearest","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% glance()\n#> # A tibble: 1 x 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3511.      29  -294.  600.  609.     427.          24    30\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% glance()\n#> # A tibble: 1 x 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3511.      29  -296.  600.  606.     431.          26    30\n\n431.11 - 427.48\n#> [1] 3.63\n1 - pchisq(3.63, 2)\n#> [1] 0.163"},{"path":"poisson-regression.html","id":"residuals-1","chapter":"8 Poisson Regression","heading":"8.3.3 residuals","text":"Keep mind expectation Poisson model residuals variable larger predicted values. broom package provides .residuals observed value minus fitted value. think .std.resid Pearson residual.","code":"\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>% head()\n#> # A tibble: 6 x 10\n#>   species `log(area)` scruz adjacent .fitted .resid .std.resid   .hat .sigma\n#>     <dbl>       <dbl> <dbl>    <dbl>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>\n#> 1      58       3.22    0.6     1.84    4.61 -4.57      -4.83  0.105    4.04\n#> 2      31       0.215  26.3   572.      3.36  0.414      0.427 0.0605   4.15\n#> 3       3      -1.56   58.7     0.78    2.76 -3.96      -4.05  0.0417   4.07\n#> 4      25      -2.30   47.4     0.18    2.55  3.01       3.08  0.0413   4.11\n#> 5       2      -3.00    1.9   904.      2.27 -3.01      -3.10  0.0559   4.11\n#> 6      18      -1.08    8       1.84    3.11 -0.953     -0.986 0.0661   4.15\n#> # … with 1 more variable: .cooksd <dbl>\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.resid)) + geom_point()\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.std.resid)) + geom_point()"},{"path":"poisson-regression.html","id":"quasipoisson","chapter":"8 Poisson Regression","heading":"8.3.4 quasiPoisson","text":"Note analyses can done using overdispersed quasiPoisson model.","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% tidy()\n#> # A tibble: 6 x 5\n#>   term            estimate std.error statistic  p.value\n#>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     3.02      1.29         2.34  0.0278  \n#> 2 log(area)       0.315     0.0786       4.02  0.000507\n#> 3 log(elevation)  0.0977    0.257        0.381 0.707   \n#> 4 nearest        -0.00106   0.00720     -0.147 0.884   \n#> 5 scruz          -0.00314   0.00254     -1.24  0.228   \n#> 6 adjacent       -0.000243  0.000120    -2.03  0.0532\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% tidy()\n#> # A tibble: 4 x 5\n#>   term         estimate std.error statistic  p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)  3.50      0.203        17.3  8.76e-16\n#> 2 log(area)    0.342     0.0304       11.3  1.63e-11\n#> 3 scruz       -0.00354   0.00197      -1.80 8.36e- 2\n#> 4 adjacent    -0.000221  0.000104     -2.12 4.35e- 2\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>% head()\n#> # A tibble: 6 x 10\n#>   species `log(area)` scruz adjacent .fitted .resid .std.resid   .hat .sigma\n#>     <dbl>       <dbl> <dbl>    <dbl>   <dbl>  <dbl>      <dbl>  <dbl>  <dbl>\n#> 1      58       3.22    0.6     1.84    4.61 -4.57      -1.18  0.105    4.04\n#> 2      31       0.215  26.3   572.      3.36  0.414      0.104 0.0605   4.15\n#> 3       3      -1.56   58.7     0.78    2.76 -3.96      -0.989 0.0417   4.07\n#> 4      25      -2.30   47.4     0.18    2.55  3.01       0.752 0.0413   4.11\n#> 5       2      -3.00    1.9   904.      2.27 -3.01      -0.758 0.0559   4.11\n#> 6      18      -1.08    8       1.84    3.11 -0.953     -0.241 0.0661   4.15\n#> # … with 1 more variable: .cooksd <dbl>\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.resid)) + geom_point()\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.std.resid)) + geom_point()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
