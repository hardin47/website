[{"path":"index.html","id":"class-information","chapter":"Class Information","heading":"Class Information","text":"Class notes Math 150 Pomona College: Methods Biostatistics. notes based primarily text Practicing Statistics, (Kuiper Sklar 2013).responsible reading text. text good & readable, use . text , however, overly technical. make sure coming class asking lots questions.","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"\nFigure 1.1: reject null hypothesis based ‘hot damn, check chart’ test. https://xkcd.com/2400/\n","code":""},{"path":"intro.html","id":"course-goals","chapter":"1 Introduction","heading":"1.1 Course Goals","text":"goals course :better evaluate quantitative information regards clinical biological data. ’ll sure keep mind:\nCareful presentation data\nConsideration variability\nMeaningful comparisons\nCareful presentation dataConsideration variabilityMeaningful comparisonsto able critically evaluate medical literature respect design, analysis, interpretation results.understand role inherent variability keep perspective inferring results population.critically evaluate medical results given mainstream media.read published studies skepticism. people (fields!) wrongly believe studies published peer review publication must 100% accurate /well designed studies. course, learn tools recognize, interpret, critique statistical results medical literature.\nFigure 1.2: Probability vs. Statistics\n","code":""},{"path":"intro.html","id":"using-r","chapter":"1 Introduction","heading":"1.2 Using R","text":"Much work done R using RStudio front end. need either download R RStudio (free) onto computer use Pomona’s server.may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)may use R Pomona server: https://rstudio.campus.pomona.edu/ (Pomona students able log immediately. Non-Pomona students need go Pomona get Pomona login information.)want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.want use R machine, may. Please make sure components updated:\nR freely available http://www.r-project.org/ already installed college computers. Additionally, installing R Studio required http://rstudio.org/.http://swirlstats.com/ great way walk learning basics R.http://swirlstats.com/ great way walk learning basics R.computing assignments turned using R Markdown compiled pdf.computing assignments turned using R Markdown compiled pdf.\nFigure 1.3: Taken Modern Drive: introduction statistical data sciences via R, Ismay Kim\n\nFigure 1.4: Jessica Ward, PhD student Newcastle University\n","code":""},{"path":"intro.html","id":"experimental-design","chapter":"1 Introduction","heading":"Experimental Design","text":"class ’ll talk techniques used analyze data medical studies. Along computational methods, however, ’ll continue think issues experimental design interpretation.Descriptive statistics describe sample hand intent making generalizations.Inferential statistics use sample make claims populationSimple Random Sample unbiased sample. Sample selected way every possible sample size \\(n\\) equally likely.Blind / double blind patient /doctor know patient receiving treatment.Placebo mock treatmentSample size reduces variability (large samples make small effects easier discern)Experiment vs. Observational Study whether treatment assigned researchers; randomized experiments make concluding causation possibleFunding study goals, bias","code":""},{"path":"t-tests-vs-slr.html","id":"t-tests-vs-slr","chapter":"2 t-tests vs SLR","heading":"2 t-tests vs SLR","text":"going build basic model following form:planned variability experimental conditions, hopefully represented interesting deterministic modelrandom error natural variability due individuals.systematic error error contained within model. can happen poor sampling poor experimental conditions.","code":"data = deterministic model + random error"},{"path":"t-tests-vs-slr.html","id":"surgery-timing","chapter":"2 t-tests vs SLR","heading":"Surgery Timing","text":"study, “Operation Timing 30-Day Mortality Elective General Surgery”, tested hypotheses risk 30-day mortality associated elective general surgery: 1) increases morning evening throughout routine workday; 2) increases Monday Friday workweek; 3) frequent July August months year. presumed negative control, investigators also evaluated mortality function phase moon. Secondarily, evaluated hypotheses pertain composite -hospital morbidity endpoint.related data set contains 32,001 elective general surgical patients. Age, gender, race, BMI, several comorbidities, several surgical risk indices, surgical timing predictors (hour, day week, month,moon phase) outcomes (30-day mortality -hospital complication) provided. dataset cleaned complete (missing data except BMI). outliers data problems. data (Sessler et al. 2011)Note example, mortality rates compared patients electing surgery July vs August. ’d like compare average age participants July group August group. Even mortality difference significant, can’t conclude causation observational study. However, similar groups based clinical variables, likely differences mortality due timing. different groups based clinical variables?\nTable 2.1: Varibles associated surgery data.\n","code":"\nsurgeryurl <- url(\"https://www.causeweb.org/tshs/datasets/surgery_timing.Rdata\")\nload(surgeryurl)\nsurgery <- stata_data\nhead(surgery)  %>%\n  select(age, gender, race, hour, dow, month, complication, bmi, everything(), -ahrq_ccs) %>%\n  kable(caption = \"Varibles associated with the surgery data.\") %>%\n kable_styling()\nsurgery %>%\n dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n dplyr::group_by(month) %>%\n dplyr::summarize(agemean = mean(age, na.rm=TRUE), agesd = sd(age, na.rm=TRUE), agen = sum(!is.na(age)))\n#> # A tibble: 2 × 4\n#>   month agemean agesd  agen\n#>   <chr>   <dbl> <dbl> <int>\n#> 1 Aug      58.1  15.2  3176\n#> 2 Jul      57.6  15.5  2325"},{"path":"t-tests-vs-slr.html","id":"ttest","chapter":"2 t-tests vs SLR","heading":"2.1 t-test","text":"(Section 2.1 Kuiper Sklar (2013).)t-test test means. surgery timing data, groups ideally similar age distributions. ? advantages disadvantages running retrospective cohort study?two-sample t-test starts assumption population means two groups equal, \\(H_0: \\mu_1 = \\mu_2\\). sample means \\(\\overline{y}_1\\) \\(\\overline{y}_2\\) always different. different must \\(\\overline{y}\\) values order reject null hypothesis?","code":""},{"path":"t-tests-vs-slr.html","id":"model-1","chapter":"2 t-tests vs SLR","heading":"Model 1:","text":"\\[\\begin{align}\ny_{1j} &= \\mu_{1} + \\epsilon_{1j} \\ \\ \\ \\ j=1, 2, \\ldots, n_1\\\\\ny_{2j} &= \\mu_{2} + \\epsilon_{2j} \\ \\ \\ \\ j=1, 2, \\ldots, n_2\\\\\n\\epsilon_{ij} &\\sim N(0,\\sigma^2)\\\\\nE[Y_i] &= \\mu_i\n\\end{align}\\], assuming group true population average fixed individual randomly selected amount random error away true population mean. Note assumed variances two groups equal. also assumed independence within groups.Note: assume population variances equal neither sample variance twice big .Example 2.1  mean ages July vs August patients statistically different? (two sided?)\\[\\begin{align}\nH_0: \\mu_1 = \\mu_2\\\\\nH_1: \\mu_1 \\ne \\mu_2\n\\end{align}\\]\\[\\begin{align}\nt &= \\frac{(\\overline{y}_1 - \\overline{y}_2) - 0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\\\\ns_p &= \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 -2}}\\\\\ndf &= n_1 + n_2 -2\\\\\n&\\\\\nt &= \\frac{(58.05 - 57.57) - 0}{15.34 \\sqrt{\\frac{1}{3176} + \\frac{1}{2325}}}\\\\\n&= 1.15\\\\  \ns_p &= \\sqrt{ \\frac{(3176-1)15.22^2 + (2325-1) 15.5^2}{3176 + 2325 -2}}\\\\\n&= 15.34\\\\\ndf &= n_1 + n_2 -2\\\\\n&= 5499\\\\\n\\mbox{p-value} &= 2 \\cdot (1-pt(1.15,5499)) = 0.25\\\\\n\\end{align}\\]analysis can done R (without tidying output):Look SD SEMWhat statistic? sampling distribution statistic?use t-distribution?big p-value important? (’s good thing!) interpret p-value?can conclude?applet (Chance Rossman 2018): [http://www.rossmanchance.com/applets/2021/sampling/OneSample.html]model assumptions? (basically assumptions given original linear model: independence & within groups, random sample, pop values don’t change, additive error, \\(\\epsilon_{,j} \\ \\sim \\ iid \\  N(0, \\sigma^2))\\)Considerations running t-test:one-sample vs two-sample t-testone-sided vs. two-sided hypothesest-test unequal variance (less powerful, conservative)\\[\\begin{align}\nt &= \\frac{(\\overline{y}_1 - \\overline{y}_2) - (\\mu_1 - \\mu_2)}{ \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\\\\ndf &= \\min(n_1-1, n_2-1)\\\\\n\\end{align}\\]two dependent (paired) samples – one sample t-test!Example 2.2  Assume two small samples: \\((y_{11}=3, y_{12} = 9, y_{21} = 5, y_{22}=1, y_{23}=9).\\) Find \\(\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\epsilon}_{11}, \\hat{\\epsilon}_{12}, \\hat{\\epsilon}_{21}, \\hat{\\epsilon}_{22}, \\hat{\\epsilon}_{23}, n_1, n_2\\).","code":"\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  age by month\n#> t = 1, df = 4954, p-value = 0.2\n#> alternative hypothesis: true difference in means between group Aug and group Jul is not equal to 0\n#> 95 percent confidence interval:\n#>  -0.337  1.309\n#> sample estimates:\n#> mean in group Aug mean in group Jul \n#>              58.1              57.6\n\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 1 × 10\n#>   estim…¹ estim…² estim…³ stati…⁴ p.value param…⁵ conf.…⁶ conf.…⁷ method alter…⁸\n#>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  <chr>  \n#> 1   0.486    58.1    57.6    1.16   0.247   4954.  -0.337    1.31 Welch… two.si…\n#> # … with abbreviated variable names ¹​estimate, ²​estimate1, ³​estimate2,\n#> #   ⁴​statistic, ⁵​parameter, ⁶​conf.low, ⁷​conf.high, ⁸​alternative"},{"path":"t-tests-vs-slr.html","id":"what-is-an-alternative-hypothesis","chapter":"2 t-tests vs SLR","heading":"2.1.1 What is an Alternative Hypothesis?","text":"Consider brief video movie Slacker, early movie Richard Linklater (director Boyhood, School Rock, Sunrise, etc.). can view video starting 2:22 ending 4:30: [https://www.youtube.com/watch?v=b-U_I1DCGEY]video, rider back taxi (played Linklater ) muses alternate realities happened arrived Austin bus. instead taking taxi, found ride woman bus station? take different road different alternate reality, reality current reality alternate reality. .point? watch video? relate material class? relationship sampling distributions? [Thanks Ben Baumer Smith College pointer specific video.]","code":""},{"path":"t-tests-vs-slr.html","id":"anova","chapter":"2 t-tests vs SLR","heading":"ANOVA","text":"Skip ANOVA text (2.4 part 2.9 Kuiper Sklar (2013)).","code":""},{"path":"t-tests-vs-slr.html","id":"tslr","chapter":"2 t-tests vs SLR","heading":"2.2 Simple Linear Regression","text":"(Section 2.3 Kuiper Sklar (2013).)Simple Linear Regression model (hopefully discussed introductory statistics) used describing {linear} relationship two variables. typically form :\\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i  \\ \\ \\ \\ = 1, 2, \\ldots, n\\\\\n\\epsilon_i &\\sim N(0, \\sigma^2)\\\\\nE(Y|x) &= \\beta_0 + \\beta_1 x\n\\end{align}\\]model, deterministic component (\\(\\beta_0 + \\beta_1 x\\)) linear function two parameters, \\(\\beta_0\\) \\(\\beta_1\\), explanatory variable \\(x\\). random error terms, \\(\\epsilon_i\\), assumed independent follow normal distribution mean 0 variance \\(\\sigma^2\\).can use model describe two sample means case discussed esophageal data? Consider \\(x\\) dummy variable takes value 0 observation control 1 observation case. Assume \\(n_1\\) controls \\(n_2\\) cases. turns , coded way, regression model two-sample t-test model mathematically equivalent!(color game text, natural way code 1 color distracter 0 standard game. ? \\(\\beta_0\\) represent? \\(\\beta_1\\) represent?)\\[\\begin{align}\n\\mu_1 &= \\beta_0 + \\beta_1 (0) = \\beta_0 \\\\\n\\mu_2 &= \\beta_0 +  \\beta_1 (1) = \\beta_0 + \\beta_1\\\\\n\\mu_2 - \\mu_1 &= \\beta_1\n\\end{align}\\]","code":""},{"path":"t-tests-vs-slr.html","id":"why-are-they-the-same","chapter":"2 t-tests vs SLR","heading":"Why are they the same?","text":"\\[\\begin{align}\nb_1= \\hat{\\beta}_1 &= \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - (\\sum x_i )^2}\\\\\n&= \\frac{n \\sum_2 y_i - n_2 \\sum y_i}{(n n_2-n_2^2)}\\\\\n&= \\frac{ n \\sum_2 y_i - n_2 (\\sum_1 y_i + \\sum_2 y_i)}{n_2(n-n_2)}\\\\\n&= \\frac{(n_1 + n_2) \\sum_2 y_i - n_2 \\sum_1 y_i - n_2 \\sum_2 y_i}{n_1 n_2}\\\\\n&= \\frac{n_1 \\sum_2 y_i - n_2 \\sum_1 y_i}{n_1 n_2}\\\\\n&= \\frac{n_1 n_2 \\overline{y}_2 - n_2 n_1 \\overline{y}_1}{n_1 n_2}\\\\\n&= \\overline{y}_2 - \\overline{y}_1\\\\\nb_0 = \\hat{\\beta}_0 &= \\frac{\\sum y_i - b_1 \\sum x_i}{n}\\\\\n&= \\frac{\\sum_1 y_i + \\sum_2 y_i - b_1 n_2}{n}\\\\\n&= \\frac{n_1 \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\\n&= \\frac{n \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\\n&= \\frac{n \\overline{y}_1}{n} = \\overline{y}_1\n\\end{align}\\]","code":""},{"path":"t-tests-vs-slr.html","id":"model-2","chapter":"2 t-tests vs SLR","heading":"Model 2:","text":"\\[\\begin{align}\ny_{} &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ =1, 2, \\ldots, n\\\\\n\\epsilon_{} &\\sim N(0,\\sigma^2)\\\\\nE[Y_i] &= \\beta_0 + \\beta_1 x_i\\\\\n\\hat{y}_i &= b_0 + b_1 x_i\n\\end{align}\\], assuming observation true population average fixed individual randomly selected amount random error away true population mean value explanatory variable, \\(x_i\\). Note assumed variance constant across level explanatory variable. also assumed independence across individuals. [Note: assumptions distribution explanatory variable, \\(X\\)].Note similarity running t.test() linear model (lm()):similarities t-test vs. SLR models?\npredicting average\nassuming independent, constant errors\nerrors follow normal distribution zero mean variance \\(\\sigma^2\\)\npredicting averageassuming independent, constant errorserrors follow normal distribution zero mean variance \\(\\sigma^2\\)differences two models?\none subscript versus two (similarly, two models t-test)\ntwo samples t-test (two variables regression… similarity??)\nvariables quantitative SLR\none subscript versus two (similarly, two models t-test)two samples t-test (two variables regression… similarity??)variables quantitative SLR","code":"\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  t.test(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 1 × 10\n#>   estim…¹ estim…² estim…³ stati…⁴ p.value param…⁵ conf.…⁶ conf.…⁷ method alter…⁸\n#>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  <chr>  \n#> 1   0.486    58.1    57.6    1.16   0.247   4954.  -0.337    1.31 Welch… two.si…\n#> # … with abbreviated variable names ¹​estimate, ²​estimate1, ³​estimate2,\n#> #   ⁴​statistic, ⁵​parameter, ⁶​conf.low, ⁷​conf.high, ⁸​alternative\n\nsurgery %>%\n  dplyr::filter(month %in% c(\"Jul\", \"Aug\")) %>%\n  lm(age ~ month, data = .) %>%\n  tidy()\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   58.1       0.272    213.     0    \n#> 2 monthJul      -0.486     0.419     -1.16   0.245"},{"path":"t-tests-vs-slr.html","id":"confidence-intervals","chapter":"2 t-tests vs SLR","heading":"2.3 Confidence Intervals","text":"(Section 2.11 Kuiper Sklar (2013).)fantastic applet visualizing means 95% confidence: [http://www.rossmanchance.com/applets/2021/confsim/ConfSim.html]general, format confidence interval give … interpretation? Remember, interval given parameter “coverage” happens alternative universes repeated sampling. ’re 95% confident interval captures parameter.Age data:\n\\[\\begin{align}\n90\\% \\mbox{ CI } \\mu_1: & \\overline{y}_1 \\pm t^*_{3176-1} \\times \\hat{\\sigma}_{\\overline{y}_1}\\\\\n& 58.05 \\pm 1.645 \\times 15.22/\\sqrt{3176}\\\\\n& (57.61, 58.49)\\\\\n95\\% \\mbox{ CI }\\mu_1 - \\mu_2: & \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{5499} s_p \\sqrt{1/n_1 + 1/n_2}\\\\\n& 0.48 \\pm 1.96 \\times 0.42\\\\\n& (-0.34, 1.30)\n\\end{align}\\]Note CI pgs 54/55, typo. correct interval \\(\\mu_1 - \\mu_2\\) games data :\\[\\begin{align}\n95\\% \\mbox{ CI } \\mu_1 - \\mu_2: & \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} \\hat{\\sigma}_{\\overline{y}_1 - \\overline{y}_2}\\\\\n& \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} s_p \\sqrt{1/n_1 + 1/n_2}\\\\\n& 38.1 - 35.55 \\pm 2.02 \\times \\sqrt{\\frac{(19)3.65^2 + (19)3.39^2}{20+20-2}} \\sqrt{\\frac{1}{20} + \\frac{1}{20}}\\\\\n& (0.29 s, 4.81 s)\n\\end{align}\\]","code":"estimate +/- critical value x standard error of the estimate"},{"path":"t-tests-vs-slr.html","id":"random-sample-vs.-random-allocation","chapter":"2 t-tests vs SLR","heading":"2.4 Random Sample vs. Random allocation","text":"Recall ’ve learned good random samples lead inference population. hand, order make causal conclusion, need randomized experiment random allocation treatments (impossible happen many settings). Random sampling random allocation DIFFERENT ideas clear mind.\nFigure 2.1: Figure taken (Chance Rossman 2018)\nNote: ANOVA (section 2.4 Kuiper Sklar (2013)) normal probability plots (section 2.8 Kuiper Sklar (2013)).","code":""},{"path":"SLR.html","id":"SLR","chapter":"3 Simple Linear Regression","heading":"3 Simple Linear Regression","text":"Though ’ve discussed relationship tests means simple linear regression, really consider simple linear regression much broader context (one explanatory response variables quantitative).data represents 10 different variables health country measured 143 countries. Data taken (Lock et al. 2016), originally Happy Planet Index Project [http://www.happyplanetindex.org/]. Region world coded 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. going investigate happiness life expectancy.","code":""},{"path":"SLR.html","id":"inference-on-the-linear-model","chapter":"3 Simple Linear Regression","heading":"3.1 Inference on the Linear Model","text":"order make inferential claims linear regression model (e.g., p-values hypotheses coefficients, confidence intervals coefficients, confidence interval line, prediction interval points, …), set technical conditions provide mathematical structure leading t-procedures (e.g., t-test). course focused linear regression spend time discussing robust model various deviations following technical conditions. now, say sometimes transformations either explanatory response variables can effective way mitigate deviations model.measurement data / population, regression models built either statistics (Roman letters describe sample) parameters (Greek letters describe population). linear regression, one additional differentiation due whether observed values (\\(y_i\\)) average values (\\(\\hat{y}_i\\) \\(E[Y_i]\\)) modeled.\\[\\begin{eqnarray*}\nE[Y_i|x_i] &=& \\beta_0 + \\beta_1 x_i \\\\\ny_i &=& \\beta_0 + \\beta_1 x_i + \\epsilon_i\\\\\n&& \\epsilon_i = y_i -  (\\beta_0 + \\beta_1 x_i)\\\\\n\\hat{y}_i &=& b_0 + b_1 x_i\\\\\ny_i &=& b_0 + b_1 x_i + e_i\\\\\n&& e_i = y_i - \\hat{y}_i = y_i -  (b_0 + b_1 x_i)\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"SLR.html","id":"technical-conditions","chapter":"3 Simple Linear Regression","heading":"3.1.1 Technical Conditions","text":"average value response variable linear function explanatory variable.error terms follow normal distribution around linear model.error terms mean zero.error terms constant variance \\(\\sigma^2\\).error terms independent (identically distributed).[http://www.rossmanchance.com/applets/2021/regshuffle/regshuffle.htm]tell whether assumptions met? can’t always. ’s good look plots: scatter plot, residual plot, histograms residuals. denote residuals model :\\[\\begin{align}\nr_i = \\hat{e}_i = y_i - \\hat{y}_i\n\\end{align}\\]\nFigure 1.2: Figs 3.13 3.15 taken Kutner et al. (2004).\nimportant note!! idea behind transformations make model appropriate possible data hand. want find correct linear model; want assumptions hold. trying find significant model big \\(R^2\\).See section 2.9 Kuiper Sklar (2013). normal probability plots (qq-plots); use histograms boxplots assess symmetry normality residuals.","code":""},{"path":"SLR.html","id":"fitting-the-regression-line","chapter":"3 Simple Linear Regression","heading":"3.2 Fitting the regression line","text":"fit regression line? Find \\(b_0\\) \\(b_1\\) minimize sum squared distance points line (called ordinary least squares):\\[\\begin{align}\n\\min \\sum (y_i \\hat{y}_i)^2 &= \\min RSS \\mbox{ residual sum squares}\\\\\nRSS &= \\sum (y_i - b_0 - b_1 x_i)^2\\\\\n\\frac{\\partial RSS}{\\partial b_0} &= 0\\\\\n\\frac{\\partial RSS}{\\partial b_1} &= 0\\\\\nb_0 &= \\overline{y} - b_1 \\overline{x}\\\\\nb_1 &= r(x,y) \\frac{s_x}{s_y}\\\\\n\\end{align}\\]way find values \\(b_0\\) \\(b_1\\)? (absolute distances, maximum likelihood,…)Resistance outliers?\\(\\hat{y}\\) \\(\\overline{x}\\)?\\[\\begin{align}\n\\hat{y} &= b_0 + b_1 \\overline{x}\\\\\n&= \\overline{y} - b_1 \\overline{x} + b_1 \\overline{x}\\\\\n&= \\overline{y}\n\\end{align}\\]regression line always pass point \\((\\overline{x}, \\overline{y})\\).Definition 3.1  estimate unbiased , many repeated samples drawn population, average value estimates based different samples equal population value parameter estimated. , statistic unbiased mean sampling distribution population parameter.","code":""},{"path":"SLR.html","id":"correlation","chapter":"3 Simple Linear Regression","heading":"3.3 Correlation","text":"Consider scatterplot, ’ll variability directions: \\((x_i - \\overline{x}) \\& (y_i - \\overline{y})\\).\\[\\begin{align}\n\\mbox{sample covariance}&\\\\\ncov(x,y) &= \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\\n\\mbox{sample correlation}&\\\\\nr(x,y) &= \\frac{cov(x,y)}{s_x s_y}\\\\\n&= \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\\n\\mbox{pop cov} &= \\sigma_{xy}\\\\\n\\mbox{pop cor} &= \\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\\\\n\\end{align}\\]\\(-1 \\leq r \\leq 1 \\ \\ \\ \\ \\ \\& \\ \\ \\ -1 \\leq \\rho \\leq 1\\).Spearman’s rank correlation Kendall’s \\(\\tau\\).\\(b_1 = r \\frac{s_y}{s_x}\\)\n\\(r=0, b_1=0\\)\n\\(r=1, b_1 > 0\\) can anything!\n\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)\n\\(r=0, b_1=0\\)\\(r=1, b_1 > 0\\) can anything!\\(r < 0 \\leftrightarrow b_1 < 0, r > 0 \\leftrightarrow b_1 > 0\\)Recall \\(R^2\\) proportion variability explained line.","code":""},{"path":"SLR.html","id":"errors-residuals","chapter":"3 Simple Linear Regression","heading":"3.4 Errors / Residuals","text":"Recall, part technical conditions required \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). estimate \\(\\sigma^2\\)?\\[\\begin{align}\nRSS &= \\sum (y_i - \\hat{y}_i)^2 \\ \\ \\ \\mbox{ residual sum squares}\\\\\nMSS &= \\sum (\\hat{y}_i - \\overline{y})^2 \\ \\ \\ \\mbox{ model sum squares}\\\\\nTSS &= \\sum (y_i - \\overline{y})^2 \\ \\ \\ \\mbox{ total sum squares}\\\\\ns_{y|x}^2 &= \\hat{\\sigma^2} = \\frac{1}{n-2} RSS\\\\\ns_x^2 &= \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2\\\\\ns_y^2 &= \\frac{1}{n-1} \\sum (y_i - \\overline{y})^2\\\\\nvar(\\epsilon) &= s_{y|x}^2 = \\frac{RSS}{n-2} = \\frac{\\sum(y_i - \\hat{y}_i)^2}{n-2} = SE(\\epsilon)\\\\\nvar(b_1) &= \\frac{s_{y|x}^2}{(n-1) s_x^2}\\\\\nSE(b_1) &= \\frac{s_{y|x}}{\\sqrt{(n-1)} s_x}\\\\\n&= \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(x_i - \\overline{x})^2}} = \\frac{\\sqrt{\\sum(y_i - \\hat{y}_i)^2/(n-2)}}{\\sqrt{\\sum(x_i - \\overline{x})^2}}\\\\\n\\end{align}\\]\\(SE(b_1) \\downarrow\\) \\(\\sigma \\downarrow\\)\\(SE(b_1) \\downarrow\\) \\(n \\uparrow\\)\\(SE(b_1) \\downarrow\\) \\(s_x \\uparrow\\)?mean \\(SE(b_1)\\)?saw , correlation slope estimates intimately related. also related coefficient determination.\n\\[\\begin{align}\nR^2 = r^2 = \\frac{MSS}{TSS}\n\\end{align}\\]\\(R^2\\) proportion total variability explained regression line (linear relationship explanatory response variables).\\(x\\) \\(y\\) correlated, \\(\\hat{y}_i \\approx \\overline{y}\\), MSS = 0, \\(R^2=0\\).\\(x\\) \\(y\\) perfectly correlated, \\(\\hat{y}_i = y_i\\), MSS=TSS, \\(R^2 = 1\\).","code":""},{"path":"SLR.html","id":"testing-beta_1","chapter":"3 Simple Linear Regression","heading":"3.4.1 Testing \\(\\beta_1\\)","text":"technical conditions hold, mathematics describing sampling distribution \\(b_1\\) well defined. :\\(H_0: \\beta=0\\) true, \n\\[\\begin{align}\n\\frac{b_1 - 0}{SE(b_1)} \\sim t_{n-2}\n\\end{align}\\]\nNote degrees freedom now \\(n-2\\) estimating two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). can also find \\((1-\\alpha)100\\%\\) confidence interval \\(\\beta_1\\):\n\\[\\begin{align}\nb_1 \\pm t_{\\alpha/2, n-2} SE(b_1)\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"intervals","chapter":"3 Simple Linear Regression","heading":"3.5 Intervals","text":"anything type standard error, can create intervals give us confidence statements making.","code":""},{"path":"SLR.html","id":"confidence-intervals-1","chapter":"3 Simple Linear Regression","heading":"3.5.1 Confidence Intervals","text":"general, confidence intervals form:","code":"point estimate +/- multiplier * SE(point estimate)"},{"path":"SLR.html","id":"slope","chapter":"3 Simple Linear Regression","heading":"3.5.2 Slope","text":"can create CI slope parameter, \\(\\beta_1\\):\n\\[\\begin{align}\nb_1 &\\pm t_{\\alpha/2,n-2} SE(b_1)\\\\\nb_1 &\\pm t_{\\alpha/2, n-2} \\frac{s_{y|x}}{\\sqrt{(n-1)}s_x}\\\\\n6.693 &\\pm t_{.025, 141} 0.375\\\\\nt_{.025,141} &= qt(0.025, 141) = -1.977\\\\\n\\mbox{CI} & (5.95 \\mbox{ years/unit happy}, 7.43 \\mbox{ years/unit happy})\n\\end{align}\\]\ncan interpret CI? make sense talk unit happiness?","code":""},{"path":"SLR.html","id":"mean-response","chapter":"3 Simple Linear Regression","heading":"3.5.3 Mean Response","text":"can also create CI mean response, \\(E[Y|x^*] = \\beta_0 + \\beta_1 x^*\\). Note standard error point estimate (\\(\\hat{y}=b_0 + b_1 x^*\\)) now depends variability associated two things (\\(b_0, b_1\\)).\n\\[\\begin{align}\nSE(\\hat{y(x^*)}) &= \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2}\\\\\nSE(\\hat{y}(\\overline{x})) &= s_{y|x}/\\sqrt{n}\\\\\nSE(\\hat{y}(x)) &\\geq s_{y|x}/\\sqrt{n} \\ \\ \\ \\forall x\n\\end{align}\\]\ninterpret associated interval?","code":""},{"path":"SLR.html","id":"prediction-of-an-individual-response","chapter":"3 Simple Linear Regression","heading":"3.5.4 Prediction of an Individual Response","text":"obvious, predicting individual variable predicting mean.\\[\\begin{align}\nSE(y(x^*)) &= \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2 + s^2_{y|x}}\\\\\nSE(y(x^*)) &= \\sqrt{ SE(\\hat{y}(x^*))^2 + s^2_{y|x}}\\\\\n\\end{align}\\]\ninterpret associated interval?","code":""},{"path":"SLR.html","id":"infl","chapter":"3 Simple Linear Regression","heading":"3.6 Influential Points","text":"skipping Section 3.6; responsible .Theorem 3.1  High leverage points x-outliers potential exert undue influence regression coefficient estimates. Influential points points exerted undue influence regression coefficient estimates.Note: typically think data better; values tend decrease sampling variability statistic. give lot data put \\(\\overline{x}\\), \\(SE(b_1)\\) stays exactly . ??Recall\n\\[\\begin{align}\ny_{} &= \\beta_0 + \\beta_1 x_i \\ \\ \\ \\epsilon_i \\sim N(0,\\sigma^2)\\\\\ne_i &= y_i - \\hat{y}_i\n\\end{align}\\]plot \\(e_i\\) versus \\(\\hat{y}_i\\). (? Typically, want \\(e_i\\) constant value \\(x_i\\). Note \\(\\hat{y}_i\\) simple linear transformation \\(x_i\\), plot identical.) want see distributions residuals different across fitted line (look patterns).residuals equal effect regression line!!","code":""},{"path":"SLR.html","id":"leverage","chapter":"3 Simple Linear Regression","heading":"3.6.1 leverage","text":"\\[\\begin{align}\nh_i = \\frac{1}{n} +\\frac{(x_i - \\overline{x})^2}{\\sum_{j=1}^n (x_j - \\overline{x})^2}\\\\\n\\frac{1}{n} \\leq h_i \\leq 1\\\\\n\\end{align}\\]\nLeverage represents effect point \\(x_i\\) line. need large leverage particular value large effect.Note:\n\\[\\begin{align}\nSE(\\hat{y}(x_i)) &= s_{y|x} \\sqrt{h_i}\\\\\nSE(y(x_i)) &= s_{y|x} \\sqrt{(h_i + 1)}\\\\\nSE(e_i) &= s_{y|x} \\sqrt{(1-h_i)}\\\\\n\\hat{y}(x^*) &\\pm t_{n-2, .025} (s_{y|x} \\sqrt{h(x^*)+1})\\\\\n\\end{align}\\]\n95% prediction interval \\(x^*\\). High leverage reduces variability line gets pulled toward point.","code":""},{"path":"SLR.html","id":"standardized-residuals","chapter":"3 Simple Linear Regression","heading":"3.6.2 standardized residuals","text":"\\[\\begin{align}\n\\frac{e_i}{s_{y|x} \\sqrt{1-h_i}} \\sim t_{n-2}\\\\\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"studentized-residuals","chapter":"3 Simple Linear Regression","heading":"3.6.3 studentized residuals","text":"\\[\\begin{align}\n\\frac{e_i}{s_{y|x, ()} \\sqrt{1-h_i}} &\\sim t_{n-3}\\\\\ns_{y|x, ()} &= \\frac{1}{n-3} \\sum_{j \\ne } (y_j - \\hat{y}_{j()})^2\n\\end{align}\\]predict 90% residuals? \\(\\pm t_{n-2,3 , .05}\\). \\(\\pm 2\\).","code":""},{"path":"SLR.html","id":"dfbetas","chapter":"3 Simple Linear Regression","heading":"3.6.4 DFBETAs","text":"DFBETAs represent change parameter estimate due one observation.\\[\\begin{align}\nDFBETAS_i &= \\frac{b_1 - b_{1()}}{\\frac{s_{y|x, ()}}{\\sqrt{(n-1)} s_x}}\\\\\n\\end{align}\\]","code":""},{"path":"SLR.html","id":"r-example-slr-happy-planet","chapter":"3 Simple Linear Regression","heading":"3.7 R Example (SLR): Happy Planet","text":"data represents 10 different variables health country measured 143 countries. Data taken (Lock et al. 2016), originally Happy Planet Index Project [http://www.happyplanetindex.org/]. Region world coded 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. going investigate happiness life expectancy.","code":""},{"path":"SLR.html","id":"reading-the-data-into-r","chapter":"3 Simple Linear Regression","heading":"3.7.1 Reading the data into R","text":"","code":"\nhappy <- read_delim(\"~/Dropbox/teaching/MA150/spring17/happyPlanet.txt\", delim=\"\\t\")\nglimpse(happy)  \n#> Rows: 143\n#> Columns: 11\n#> $ Country        <chr> \"Albania\", \"Algeria\", \"Angola\", \"Argentina\", \"Armenia\",…\n#> $ Region         <dbl> 7, 3, 4, 1, 7, 2, 2, 7, 5, 7, 2, 1, 4, 5, 1, 7, 4, 1, 7…\n#> $ Happiness      <dbl> 5.5, 5.6, 4.3, 7.1, 5.0, 7.9, 7.8, 5.3, 5.3, 5.8, 7.6, …\n#> $ LifeExpectancy <dbl> 76.2, 71.7, 41.7, 74.8, 71.7, 80.9, 79.4, 67.1, 63.1, 6…\n#> $ Footprint      <dbl> 2.2, 1.7, 0.9, 2.5, 1.4, 7.8, 5.0, 2.2, 0.6, 3.9, 5.1, …\n#> $ HLY            <dbl> 41.7, 40.1, 17.8, 53.4, 36.1, 63.7, 61.9, 35.4, 33.1, 4…\n#> $ HPI            <dbl> 47.9, 51.2, 26.8, 59.0, 48.3, 36.6, 47.7, 41.2, 54.1, 3…\n#> $ HPIRank        <dbl> 54, 40, 130, 15, 48, 102, 57, 85, 31, 104, 64, 27, 134,…\n#> $ GDPperCapita   <dbl> 5316, 7062, 2335, 14280, 4945, 31794, 33700, 5016, 2053…\n#> $ HDI            <dbl> 0.801, 0.733, 0.446, 0.869, 0.775, 0.962, 0.948, 0.746,…\n#> $ Population     <dbl> 3.15, 32.85, 16.10, 38.75, 3.02, 20.40, 8.23, 8.39, 153…"},{"path":"SLR.html","id":"running-the-linear-model-lm","chapter":"3 Simple Linear Regression","heading":"3.7.2 Running the linear model (lm)","text":"","code":"\nhappy.lm = lm(LifeExpectancy ~ Happiness, data=happy) \n\nhappy.lm %>% tidy()\n#> # A tibble: 2 × 5\n#>   term        estimate std.error statistic  p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)    28.2      2.28       12.4 2.76e-24\n#> 2 Happiness       6.69     0.375      17.8 5.78e-38"},{"path":"SLR.html","id":"ouptut","chapter":"3 Simple Linear Regression","heading":"3.7.3 Ouptut","text":"analyses need residuals, fitted values, coefficients individually.can plot main relationship, can plot residuals (check technical conditions hold):Intervals interest: mean response, individual response, parameter(s).","code":"\nhappy.lm %>% augment()\n#> # A tibble: 143 × 8\n#>   LifeExpectancy Happiness .fitted  .resid    .hat .sigma   .cooksd .std.resid\n#>            <dbl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n#> 1           76.2       5.5    65.0  11.2   0.00765   6.09 0.0128        1.83  \n#> 2           71.7       5.6    65.7   6.00  0.00737   6.14 0.00357       0.980 \n#> 3           41.7       4.3    57.0 -15.3   0.0168    6.02 0.0539       -2.51  \n#> 4           74.8       7.1    75.7  -0.944 0.0122    6.16 0.000148     -0.155 \n#> 5           71.7       5      61.7  10.0   0.0101    6.10 0.0138        1.64  \n#> 6           80.9       7.9    81.1  -0.198 0.0216    6.16 0.0000118    -0.0326\n#> # … with 137 more rows\n#> # ℹ Use `print(n = ...)` to see more rowshappy %>%\n         ggplot(aes(x=Happiness, y=LifeExpectancy)) + \n         geom_point() + \n         geom_smooth(method=\"lm\", se=FALSE) \n\nhappy.lm %>% \n         augment() %>% \n         ggplot(aes(x = .fitted, y = .resid)) + \n         geom_point() + \n         geom_hline(yintercept=0)\npredict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(\"conf\"), level=.95)\n#>    fit  lwr  upr\n#> 1 55.0 53.2 56.7\n#> 2 75.1 73.8 76.4\npredict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(\"pred\"), level=.95)\n#>    fit  lwr  upr\n#> 1 55.0 42.7 67.3\n#> 2 75.1 62.9 87.3\n\nhappy.lm %>% tidy(conf.int = TRUE)\n#> # A tibble: 2 × 7\n#>   term        estimate std.error statistic  p.value conf.low conf.high\n#>   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)    28.2      2.28       12.4 2.76e-24    23.7      32.7 \n#> 2 Happiness       6.69     0.375      17.8 5.78e-38     5.95      7.43"},{"path":"SLR.html","id":"residuals-in-r","chapter":"3 Simple Linear Regression","heading":"3.7.3.1 Residuals in R","text":"skipped residuals section, responsible finding residuals R, R code completion case interested:","code":"\nhappy.lm %>% augment()\n#> # A tibble: 143 × 8\n#>   LifeExpectancy Happiness .fitted  .resid    .hat .sigma   .cooksd .std.resid\n#>            <dbl>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>     <dbl>      <dbl>\n#> 1           76.2       5.5    65.0  11.2   0.00765   6.09 0.0128        1.83  \n#> 2           71.7       5.6    65.7   6.00  0.00737   6.14 0.00357       0.980 \n#> 3           41.7       4.3    57.0 -15.3   0.0168    6.02 0.0539       -2.51  \n#> 4           74.8       7.1    75.7  -0.944 0.0122    6.16 0.000148     -0.155 \n#> 5           71.7       5      61.7  10.0   0.0101    6.10 0.0138        1.64  \n#> 6           80.9       7.9    81.1  -0.198 0.0216    6.16 0.0000118    -0.0326\n#> # … with 137 more rows\n#> # ℹ Use `print(n = ...)` to see more rows"},{"path":"analysis-of-categorical-data.html","id":"analysis-of-categorical-data","chapter":"4 Analysis of Categorical Data","heading":"4 Analysis of Categorical Data","text":"(Section 6.3 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"cat","chapter":"4 Analysis of Categorical Data","heading":"4.1 Categorical Inference","text":"either observational study randomized experiment, often interested assessing statistical significance differences see: observed difference big reasonably occurred just due chance? answer question, usesimulationmathematical probability models.Example 4.1  Back Pain & Botox, Chance Rossman (2018), Foster et al. (2001)\nrandomized clinical trial examined whether drug botulinum toxin (Botox) helpful reducing pain among patients suffer chronic low back pain. 31 subjects participated study randomly assigned one two treatment groups: 16 received placebo normal saline 15 received drug . subjects’ pain levels evaluated beginning study eight weeks. researchers found 2 16 subjects received saline experienced substantial reduction pain, compared 9 15 subjects received actual drug.experiment observational study?Explain importance using “placebo” treatment saline.Create two-way table summarizing data, putting explanatory variable rows response variable columnsCalculate conditional proportions pain reduction two groups. Display results segmented bar graph. Comment preliminary analysis.\\[\\begin{eqnarray*}\n\\mbox{risk}_{\\mbox{placebo}} = \\frac{2}{16} &=& 0.125\\\\\n\\mbox{risk}_{\\mbox{Botox}} = \\frac{9}{15} &=& 0.6\\\\\nRR &=& 4.8\n\\end{eqnarray*}\\]Note sometimes makes sense y-axis count sometimes makes sense y-axis percent. Probably doesn’t matter much , choose bar plot seems informative .association treatment back pain relief, many 11 “successes” expect see group? researchers observe successes saline group expected (drug effect) fewer successes expected? direction conjectured researchers?association treatment back pain relief, many 11 “successes” expect see group? researchers observe successes saline group expected (drug effect) fewer successes expected? direction conjectured researchers?possible drug absolutely effect back pain? differences simply due chance random variability? likely ?possible drug absolutely effect back pain? differences simply due chance random variability? likely ?","code":"\nbackpain <- data.frame(treatment = c(rep(\"placebo\", 16), rep(\"Botox\", 15)),\n                     outcome = c(rep(\"reduction\", 2), rep(\"no_reduction\", 14), \n                                 rep(\"reduction\", 9), rep(\"no_reduction\", 6)))\nbackpain %>%\n  table()\n#>          outcome\n#> treatment no_reduction reduction\n#>   Botox              6         9\n#>   placebo           14         2\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome), position = \"fill\") +\n  ylab(\"percentage\")\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome))"},{"path":"analysis-of-categorical-data.html","id":"simulation","chapter":"4 Analysis of Categorical Data","heading":"Simulation","text":"11 red “success” cards (pain reduction); 20 black “failure” cards (pain reduction)11 red “success” cards (pain reduction); 20 black “failure” cards (pain reduction)randomly deal (.e. shuffle) 15 cards treatment group 16 cards placebo group.randomly deal (.e. shuffle) 15 cards treatment group 16 cards placebo group.count many people treatment group successes? Repeat 5 times.count many people treatment group successes? Repeat 5 times.process\ncards represent?\nshuffling cards represent?\nimplicit assumption two groups shuffling cards represent?\nobservational units represented dots dotplot?\ncount number repetitions 9 “successes”?\nprocesswhat cards represent?shuffling cards represent?implicit assumption two groups shuffling cards represent?observational units represented dots dotplot?count number repetitions 9 “successes”?Repeat simulation using two-way table applet:\n[http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm]Repeat simulation using two-way table applet:\n[http://www.rossmanchance.com/applets/2021/chisqshuffle/ChiSqShuffle.htm]summary\nmany reps?\nmany extreme true data?\nproportion least extreme true data?\ndata support researchers conjecture?\nactual data 7 successes treatment group (4 placebo group)?\nsummaryHow many reps?many extreme true data?proportion least extreme true data?data support researchers conjecture?actual data 7 successes treatment group (4 placebo group)?Definition 4.1  p-value p-value probability seeing results extreme nothing interesting going data. (definition p-value always use class research.)Notice regardless whether drug effect, data different time (think: new 31 people). small p-value allows us draw cause--effect conclusions, doesn’t necessarily allow us infer larger population. ?","code":""},{"path":"analysis-of-categorical-data.html","id":"inferFET","chapter":"4 Analysis of Categorical Data","heading":"4.1.1 Simulation using R","text":"simulation applet can recreated using infer package R. Note different pieces simulation using functions like specify(), hypothesize(), generate(), calculate(). Also notice particular function works best using difference proportions (discussed class equivalent recording single count Botox patients reduced back pain).Step 1. Calculate observed difference proportion patients reduced back pain. Note linear regression continue use syntax: responsevariable ~ explanatoryvariable.Step 2. Go simulation steps, just like applet.specify() variableshypothesize() null claimgenerate() many permutions datacalculate() statistic interest different permutationsStep 3. Plot histogram representing differences proportions many permuted tables. plot represents distribution differences proportion null hypothesis.Step 4. Calculate p-value sampling distribution generated Step 3.","code":"\nlibrary(infer)\n\n# Step 1.\ndiff_props <- backpain %>%\n  specify(outcome ~ treatment, success = \"reduction\") %>%\n  calculate(stat = \"diff in props\")\n\ndiff_props  # print to screen to see the observed difference\n#> Response: outcome (factor)\n#> Explanatory: treatment (factor)\n#> # A tibble: 1 × 1\n#>    stat\n#>   <dbl>\n#> 1 0.475\n\n# Step 2.\nnulldist <- backpain %>%\n  specify(outcome ~ treatment, success = \"reduction\") %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"diff in props\")\n\n# Step 3.\nvisualize(nulldist) + \n  shade_p_value(obs_stat = diff_props, direction = \"greater\")\n\n# Step 4.\nnulldist %>%\n  get_p_value(obs_stat = diff_props, direction = \"greater\")\n#> # A tibble: 1 × 1\n#>   p_value\n#>     <dbl>\n#> 1   0.007"},{"path":"analysis-of-categorical-data.html","id":"fisher","chapter":"4 Analysis of Categorical Data","heading":"4.2 Fisher’s Exact Test","text":"(Section 6.4 Kuiper Sklar (2013), great detailed explanation!)fixed sample, use hypergeometric distribution enumerate possible ways choosing data extreme given fixed row column totals.make simpler, let’s say 5 items (N=5), want choose 3 (n=3). many ways can ?SSSNN, SSNSN, SSNNS, SNSSN, SNSNS, SNNSS, NSSSN, NSSNS, NSNSS, NNSSS [5!/ 3! 2!]\n(S = select, N = selected), many different ways can select 11 people (31) “pain reduction” group? total number different groups size 11 31. really, want groups certain breakdown. need 2 (16) gotten placebo 9 (15) gotten Botox treatment.Definition 4.2  Hypergeometric Probability \\(2 \\times 2\\) table N observations M total successes , probability observing x successes sample size n :\\[\\begin{eqnarray*}\nP(X=x) = \\frac{\\# \\mbox{ ways select x successes n-x failures}}{\\# \\mbox{ ways select n subjects}} = \\frac{ { M \\choose x} {N-M \\choose n-x}}{{N \\choose n}}\\\\\n\\end{eqnarray*}\\]Find P(X=2)can now find EXACT probabilities associated following hypotheses.\n\\[\\begin{eqnarray*}\n&&H_0: p_{pl} = p_{btx}\\\\\n&&H_a: p_{pl} < p_{btx}\\\\\n&&p = \\mbox{true probability pain}\\\\\n\\end{eqnarray*}\\]one- two-sided test? ? [Note: conditions include row column totals fixed – conditional test independence. However, research project back chapter 6 extends permutation test demonstrated probabilities hold even alternative technical conditions.]","code":""},{"path":"analysis-of-categorical-data.html","id":"chisq","chapter":"4 Analysis of Categorical Data","heading":"4.3 Testing independence of two categorical variables","text":"(Sections 6.5, 6.6, 6.7 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"chi2-tests","chapter":"4 Analysis of Categorical Data","heading":"4.3.1 \\(\\chi^2\\) tests","text":"(Section 6.6 Kuiper Sklar (2013).)2x2… also rxc (\\(p_a = p_b = p_c\\))can also use \\(\\chi^2\\) tests evaluate \\(r \\times c\\) contingency tables. main question now whether association two categorical variables interest. Note now generalizing Botox back pain example. two variables independent? two variables independent, state one variable related probability different outcomes variable.data sampled way random samples explanatory response variables (e.g., cross classification study), typically test association:\\[\\begin{eqnarray*}\nH_0: && \\mbox{ two variables independent}\\\\\nH_a: && \\mbox{ two variables independent}\n\\end{eqnarray*}\\]data sampled way response measured across specified populations (example ), typically test homogeneity proportions. example,\\[\\begin{eqnarray*}\nH_0: && p_1 = p_2 = p_3\\\\\nH_a: && \\mbox{} H_0\n\\end{eqnarray*}\\]\n\\(p=P(\\mbox{success})\\) groups 1,2,3.get expected frequencies? mathematics hold regardless type test (.e., sampling mechanism used collect data). , fact,variables independent, able multiply probabilities. probabilities , expect overall proportion response variable proportion response variable explanatory group. math example follows directly.Example 4.2  table show observed distributions ABO blood type three random samples African Americans living different locations. three datasets, collected 1950s three different investigators, reproduced (Mourant, Kopec, Domaniewsa-Sobczak 1976).","code":""},{"path":"analysis-of-categorical-data.html","id":"test-of-homogeneity-of-proportions-equivalent-mathematically-to-independence","chapter":"4 Analysis of Categorical Data","heading":"4.3.1.1 Test of Homogeneity of Proportions (equivalent mathematically to independence)","text":"difference blood type proportions across groups, :\\[\\begin{eqnarray*}\nP(AB | FL) = P(AB | IA) = P(AB | MO) = P(AB)\n\\end{eqnarray*}\\]use \\(\\hat{P}(AB) = \\frac{368}{8619}\\) baseline expectation (\\(H_0\\)) groups. , expect,\\[\\begin{eqnarray*}\n\\# \\mbox{expected AB blood Iowa} &=&  \\frac{368}{8619} \\cdot 6722\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"test-of-independence-equivalent-mathematically-to-homogeneity-of-proporitions","chapter":"4 Analysis of Categorical Data","heading":"4.3.1.2 Test of Independence (equivalent mathematically to homogeneity of proporitions)","text":"\\[\\begin{eqnarray*}\nP(cond1 \\mbox{ & } cond2 ) &=& P(cond1) P(cond2)  \\mbox{ variables 1 2 independent}\\\\\nP(AB \\mbox{ blood & Iowa}) &=& P(AB \\mbox{ blood}) P(\\mbox{Iowa}) \\\\\n&=& \\bigg( \\frac{368}{8619}\\bigg) \\bigg( \\frac{6722}{8619} \\bigg)\\\\\n&=& 0.0333\\\\\n\\# \\mbox{expected AB blood Iowa} &=& 0.033 \\cdot 8619\\\\\n&=& \\frac{368 \\cdot 6722}{8619}\\\\\nE_{,j} &=& \\frac{(\\mbox{ row total})(j \\mbox{ col total})}{\\mbox{grand total}}\\\\\n\\end{eqnarray*}\\]expected values null hypothesis…\\[\\begin{eqnarray*}\nX^2 &=& \\sum_{cells} \\frac{( O - E)^2}{E}\\\\\n&=& 5.65\\\\\n\\mbox{p-value} &=& P(\\chi^2_6 \\geq 5.65) \\\\\n&=& 1 - pchisq(5.65, 6)\\\\\n&=& 0.464\n\\end{eqnarray*}\\]reject null hypothesis. , evidence null hypothesis blood types independently distributed various regions.know test statistic big number ? Well, turns test statistic (\\(X^2\\)) approximate \\(\\chi^2\\) distribution degrees freedom = \\((r- 1)\\cdot (c-1)\\). long :random sample population.expect least 1 observation every cell (\\(E_i \\geq 1 \\forall \\))expect least 5 observations 80% cells (\\(E_i \\geq 5\\) 80% \\(\\))two populations, \\(\\chi^2\\) procedure equivalent two-sided z-test proportions. chi-squared test statistic square z-test statistic. , chi-squared test exactly two-sided alternative z-test.use chi-square multiple populationsuse z-test want one-sided tests confidence intervals.","code":""},{"path":"analysis-of-categorical-data.html","id":"catest","chapter":"4 Analysis of Categorical Data","heading":"4.4 Parameter Estimation","text":"(Section 6.8 Kuiper Sklar (2013).)Definition 4.3  Data Types Data often classified asCategorical - unit assigned categoryQuantitative - observational unit assigned numerical value(Binary - special case categorical 2 categories, e.g. male/female)Table 6.6 page 193 Kuiper Sklar (2013) excellent worth looking .Example 4.3  Popcorn & Lung Disease Chance Rossman (2018)can tell popcorn production related lung disease? Consider High / Low exposure:21 lot people? Can compare 6 vs. 15? look ? proportions (always number 0 1). Look data (graphically numerically). Segmented bar graph (mosaic plot):difference two groups? Look difference proportions risk:\\[\\begin{eqnarray*}\n6/58 = 0.103 & 15/58=0.2586 & \\Delta = 0.156\\\\\np_1 = 0.65 & p_2 = 0.494 & \\Delta = 0.156\\\\\np_1 = 0.001 & p_2 = 0.157 & \\Delta = 0.156\\\\\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"differences-in-proportions","chapter":"4 Analysis of Categorical Data","heading":"4.4.1 Differences in Proportions","text":"turns sampling distribution difference sample proportions (success) across two independent groups can modeled normal distribution reasonably large sample sizes (CLT).ensure accuracy test, check whether np n(1-p) bigger 5 samples usually adequate. precise check \\(n_s \\hat{p}_c\\) \\(n_s(1-\\hat{p}_c)\\) greater 5; \\(n_s\\) smaller two sample sizes \\(\\hat{p}_c\\)sample proportion two samples combined one.Note:\n\\[\\begin{eqnarray*}\n\\hat{p}_1 - \\hat{p}_2 \\sim N\\Bigg(p_1 - p_2, \\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\\Bigg)\n\\end{eqnarray*}\\]testing independence, assume \\(p_1=p_2\\), use pooled estimate proportion calculate SE:\n\\[\\begin{eqnarray*}\nSE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) \\bigg(\\frac{1}{n_1} + \\frac{1}{n_2}\\bigg)}\n\\end{eqnarray*}\\], testing, appropriate test statistic :\n\\[\\begin{eqnarray*}\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{ \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) (\\frac{1}{n_1} + \\frac{1}{n_2})}}\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"ci-for-differences-in-proportions","chapter":"4 Analysis of Categorical Data","heading":"4.4.2 CI for differences in proportions","text":"can’t pool estimate SE, everything else stays …\\[\\begin{eqnarray*}\nSE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}}\n\\end{eqnarray*}\\]main idea determine whether two categorical variables independent. , knowledge value one variable tell something probability variable (gender pregnancy). ’re going talk two different ways approach problem.","code":""},{"path":"analysis-of-categorical-data.html","id":"relative-risk","chapter":"4 Analysis of Categorical Data","heading":"4.4.3 Relative Risk","text":"Definition 4.4  Relative Risk relative risk (RR) ratio risks group. say, “risk success RR times higher group 1 compared group 2.”\\[\\begin{eqnarray*}\n\\mbox{relative risk} &=& \\frac{\\mbox{risk group 1}}{\\mbox{risk group 2}}\\\\\n&=&  \\frac{\\mbox{proportion successes group 1}}{\\mbox{proportion successes group 2}}\\\\\n\\mbox{RR} &=& \\frac{p_1}{p_2} = \\frac{p_1}{p_2}\\\\\n\\hat{\\mbox{RR}} &=& \\frac{\\hat{p}_1}{\\hat{p}_2}\n\\end{eqnarray*}\\]\\(\\hat{RR}\\) popcorn example \\(\\frac{15/58}{6/58} = 2.5\\). say, “risk airway obstruction 2.5 times higher high exposure group compared low exposure group.” aboutsample size?baseline risk?create confidence intervals relative risk, use fact :\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{RR})) &\\approx& \\sqrt{\\frac{(1 - \\hat{p}_1)}{n_1 \\hat{p}_1} + \\frac{(1-\\hat{p}_2)}{n_2 \\hat{p}_2}}\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"odds-ratios","chapter":"4 Analysis of Categorical Data","heading":"4.4.4 Odds Ratios","text":"related concept risk odds. often used horse racing, “success” typically defined losing. , odds 3 1 expect lose 3/4 time.Definition 4.5  Odds Ratio related concept risk odds. often used horse racing, “success” typically defined losing. , odds 3 1 expect lose 3/4 time. odds ratio () ratio odds group. say, “odds success times higher group 1 compared group 2.”\\[\\begin{eqnarray*}\n\\mbox{odds} &=& \\frac{\\mbox{proportion successes}}{\\mbox{proportion failures}}\\\\\n&=& \\frac{\\mbox{number successes}}{\\mbox{number failures}} = \\theta\\\\\n\\hat{\\mbox{odds}} &=& \\hat{\\theta}\\\\\n\\mbox{odds ratio} &=& \\frac{\\mbox{odds group 1}}{\\mbox{odds group 2}} \\\\\n\\mbox{} &=& \\frac{\\theta_1}{\\theta_2} = \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}= \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}\\\\\n\\hat{\\mbox{}} &=& \\frac{\\hat{\\theta}_1}{\\hat{\\theta}_2} = \\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\\\\n\\end{eqnarray*}\\]\\(\\hat{}\\) popcorn example \\(\\frac{15/43}{6/52} = 3.02\\). say, “odds airway obstruction 3 times higher high exposure group compared low exposure group.”","code":""},{"path":"analysis-of-categorical-data.html","id":"or-is-more-extreme-than-rr","chapter":"4 Analysis of Categorical Data","heading":"4.4.4.1 OR is more extreme than RR","text":"Without loss generality, assume true \\(RR > 1\\), implying \\(p_1 / p_2 > 1\\) \\(p_1 > p_2\\).Note following sequence consequences:\\[\\begin{eqnarray*}\nRR = \\frac{p_1}{p_2} &>& 1\\\\\n\\frac{1 - p_1}{1 - p_2} &<& 1\\\\\n\\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& 1\\\\\n\\frac{p_1}{p_2} \\cdot \\frac{ 1 / (1 - p_1)}{1 / (1 - p_2)} &>& \\frac{p_1}{p_2}\\\\\n&>& RR\n\\end{eqnarray*}\\]","code":""},{"path":"analysis-of-categorical-data.html","id":"other-considerations","chapter":"4 Analysis of Categorical Data","heading":"4.4.4.2 Other considerations:","text":"Observational study (worked place?)Cross sectional (one point time)Healthy worker effect (stayed home sick?)Explanatory variable one potential explanation changes (exposure level).Response variable measured outcome interest (airway obstruction).Example 4.4  Smoking & Lung Cancer Chance Rossman (2018)World War II, evidence began mounting link cigarette smoking pulmonary carcinoma (lung cancer). 1950s, two now classic articles published subject. One studies conducted United States Wynder Graham (1950). found records large number (684) patients proven bronchiogenic carcinoma (specific form lung cancer) hospitals California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, Utah. personally interviewed 634 subjects identify smoking habits, occupation, exposure dust fumes, alcohol intake, education, cause death parents siblings. Thirty-three subjects completed mailed questionnaires, information 17 obtained family members close acquaintances. study, researchers focused 605 male patients form lung cancer. Another 1332 hospital patients similar age economic distribution (including 780 males) without lung cancer interviewed researchers St. Louis researchers Boston, Cleveland, Hines, Illinois.following two-way table replicates counts 605 male patients form cancer “control-group” 780 males.Given results study, think can generalize sample population? Explain make clear know difference sample population.Causation?Case-control study (605 lung cancer, 780 without… baseline rate?)lung cancer considered success smoking baseline:\\[\\begin{eqnarray*}\n\\hat{RR} &=& \\frac{122/182}{8/122} = 10.22\\\\\n\\hat{} &=& \\frac{122/60}{8/114} = 28.9\\\\\n\\end{eqnarray*}\\]\nrisk lung cancer 10.22 times higher smoke don’t smoke.odds lung cancer 28.9 times higher smoke don’t smoke.chain smoking considered success healthy baseline:\\[\\begin{eqnarray*}\n\\hat{RR} &=& \\frac{122/130}{60/174} = 2.7\\\\\n\\hat{} &=& \\frac{122/8}{60/114} = 28.9\\\\\n\\end{eqnarray*}\\]\nrisk smoking 2.7 times higher lung cancer don’t lung cancer.odds smoking 28.9 times higher lung cancer don’t lung cancer.\nknow risk light smoker lung cancer know risk lung cancer light smoker. Let’s say population 1,000,000 people:\\[\\begin{eqnarray*}\nP(\\mbox{light} | \\mbox{lung cancer}) &=& \\frac{49,000}{50,000} = 0.98\\\\\nP(\\mbox{lung cancer} | \\mbox{light}) &=& \\frac{49,000}{100,000} = 0.49\\\\\n\\end{eqnarray*}\\]explanatory variable?response variable?relative risk?odds ratio?lung cancer considered success smoking baseline:\\[\\begin{eqnarray*}\nRR &=& \\frac{49/100}{1/900} = 441\\\\\n&=& \\frac{49/51}{1/899} = 863.75\\\\\n\\end{eqnarray*}\\]light smoking considered success healthy baseline:\\[\\begin{eqnarray*}\nRR &=& \\frac{49/50}{51/950} = 18.25\\\\\n&=& \\frac{49/1}{51/899} = 863.75\\\\\n\\end{eqnarray*}\\]matter variable choose explanatory versus response! Though, general, still prefer know baseline odds baseline risk (can’t know case-control study).Example 4.5  Smoking & Lung Cancer, Chance Rossman (2018)\nNow cohort prospective study. (Previously case-control retrospective study). Now baseline risk estimates? Yes! careful, can’t conclude causation, study still observational.","code":""},{"path":"analysis-of-categorical-data.html","id":"ciOR","chapter":"4 Analysis of Categorical Data","heading":"4.4.5 Confidence Interval for OR","text":"Due theory won’t cover:\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{})) &\\approx& \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}}\n\\end{eqnarray*}\\]Note book introduces \\(SE(\\ln(\\hat{}))\\) context hypothesis testing null, \\(H_0: p_1 = p_2\\), assumed true. null true, ’d prefer estimate proportion success based entire sample:\\[\\begin{eqnarray*}\nSE(\\ln (\\hat{})) &\\approx& \\sqrt{\\frac{1}{n_1 \\hat{p} (1-\\hat{p})} + \\frac{1}{n_2 \\hat{p}(1-\\hat{p})}}\n\\end{eqnarray*}\\], \\((1-\\alpha)100\\%\\) CI \\(\\ln()\\) :\n\\[\\begin{eqnarray*}\n\\ln(\\hat{}) \\pm z_{1-\\alpha/2} SE(\\ln(\\hat{}))\n\\end{eqnarray*}\\]gives \\((1-\\alpha)100\\%\\) CI \\(\\):\n\\[\\begin{eqnarray*}\n(e^{\\ln(\\hat{}) - z_{1-\\alpha/2} SE(\\ln(\\hat{}))}, e^{\\ln(\\hat{}) + z_{1-\\alpha/2} SE(\\ln(\\hat{}))})\n\\end{eqnarray*}\\]Back example… \\(= 28.9.\\)\n\\[\\begin{eqnarray*}\nSE(\\ln(\\hat{})) &=& \\sqrt{\\frac{1}{182*0.67*(1-0.67)} + \\frac{1}{122*0.0656*(1-0.0656)}}\\\\\n&=& 0.398\\\\\n90\\% \\mbox{ CI } \\ln() && \\ln(28.9) \\pm 1.645 \\cdot 0.398\\\\\n&& 3.366 \\pm 1.645 \\cdot 0.398\\\\\n&& (2.71, 4.02)\\\\\n90\\% \\mbox{ CI } && (e^{2.71}, e^{4.02})\\\\\n&& (15.04, 55.47)\\\\\n\\end{eqnarray*}\\]90% confident true \\(\\ln()\\) 2.71 4.02. 90% confident true \\(\\) 15.04 55.47. , true odds getting lung cancer smoke somewhere 15.04 55.47 times higher don’t smoke, 90% confidence.Note 1: use theory allows us understand sampling distribution \\(\\ln(\\hat{}).\\) use process creating CIs transform back \\(\\).Note 2: use t-distribution estimating population standard deviation.Note 3: good general guidelines checking whether sample sizes large enough normal approximation. authorities agree one can get away smaller sample sizes differences two proportions. sample sizes pass rough check discussed \\(\\chi^2\\), large enough support inferences based approximate normality log estimated odds ratio, . (Ramsey Schafer 2012, 541)normal approximation hold, need expected counts cell least 5. (Pagano Gauvreau 2000, 355)Note 4: cells zero, many people add 0.5 cell’s observed value.Note 5: always extreme RR (one reason careful…)\\[\\begin{eqnarray*}\n\\mbox{assume } && \\frac{X_1 / n_1}{X_2 / n_2} = RR > 1\\\\\n& & \\\\\n\\frac{X_1}{n_1} &=& RR \\ \\ \\frac{X_2}{n_2}\\\\\n\\frac{X_1}{n_1 - X_1} &=& RR \\ \\ \\bigg( \\frac{n_1}{n_2}  \\frac{n_2 - X_2}{n_1 - X_1} \\bigg) \\frac{X_2}{n_2-X_2}\\\\\n&=& RR \\ \\ \\bigg(\\frac{n_1}{n_2} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\\n&=& RR \\ \\ \\bigg(\\frac{1/n_2}{1/n_1} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\\n&=& RR  \\ \\ \\frac{1 - X_2/n_2}{1 - X_1/n_1}\\\\\n& > & RR\n\\end{eqnarray*}\\]\n[\\(1 - \\frac{X_2}{n_2} > 1 - \\frac{X_1}{n_1} \\rightarrow \\frac{1 - \\frac{X_2}{n_2}}{1 - \\frac{X_1}{n_1}} > 1\\)]Note 6: \\(RR \\approx \\) risk small (denominator similar denominator RR).","code":""},{"path":"analysis-of-categorical-data.html","id":"studies","chapter":"4 Analysis of Categorical Data","heading":"4.5 Types of Studies","text":"(Section 6.9 Kuiper Sklar (2013).)Definition 4.6  Explanatory variable one potential explanation changes response variable.Definition 4.7  Response variable measured outcome interest.Definition 4.8  Case-control study: identify observational units responseDefinition 4.9  Cohort study: identify observational units explanatory variableDefinition 4.10  Cross-classification study: identify observational units regardless levels variable.","code":""},{"path":"analysis-of-categorical-data.html","id":"retrospective-versus-prospective-studies","chapter":"4 Analysis of Categorical Data","heading":"4.5.1 Retrospective versus Prospective Studies","text":"much research (asking many people agree!), finally came across definition retrospective like. Note, however, many many books define retrospective synonymous case-control. , define retrospective study one observational units chosen based status response variable. disagree definition. see , retrospective studies defined based variables measured. ’ve also given quote Kuiper text retrospective defined study historic data collected (like definition less).Definition 4.11  prospective study watches outcomes, development disease, study period. explanatory variables measured response variable occurs.Definition 4.12  retrospective study looks backwards examines exposures suspected risk protection factors relation outcome established start study. explanatory variables measured response happened.Studies can classified either prospective retrospective. define prospective study one exposure covariate measurements made cases illness occur. retrospective study measurements made cases already occurred… Early writers referred cohort studies prospective studies case-control studies retrospective studies cohort studies usually begin identification exposure status measure disease occurrence, whereas case-control studies usually begin identifying cases controls measure exposure status. terms prospective retrospective, however, usefully employed describe timing disease occurrence respect exposure measurement. example, case-control studies can either prospective retrospective. prospective case-control study uses exposure measurements taken disease, whereas retrospective case-control study uses measurements taken disease. (Rothman Greenland 1998, 74)Retrospective cohort studies also exist. designs past (medical) records often used collect data. prospective cohort studies, objective still first establish groups based explanatory variable. However since past records response variable can collected time. (Kuiper Sklar 2013, chap. 6, page 24)Understanding study retrospective prospective leads sense biases within study.retrospective aspect may introduce selection bias misclassification information bias. retrospective studies, temporal relationship frequently difficult assess.","code":""},{"path":"analysis-of-categorical-data.html","id":"disadvantages-of-prospective-cohort-studies","chapter":"4 Analysis of Categorical Data","heading":"Disadvantages of Prospective Cohort Studies","text":"may follow large numbers subjects long time.can expensive time consuming.good rare diseases.good diseases long latency.Differential loss follow can introduce bias.","code":""},{"path":"analysis-of-categorical-data.html","id":"disadvantages-of-retrospective-cohort-studies","chapter":"4 Analysis of Categorical Data","heading":"Disadvantages of Retrospective Cohort Studies","text":"prospective cohort studies, good rare diseases.one uses records designed study, available data may poor quality.frequently absence data potential confounding factors data recorded past.may difficult identify appropriate exposed cohort appropriate comparison group.Differential losses follow can also bias retrospective cohort studies.Disadvantages : http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_CohortStudies/EP713_CohortStudies5.html","code":""},{"path":"analysis-of-categorical-data.html","id":"examples-of-studies","chapter":"4 Analysis of Categorical Data","heading":"Examples of studies:","text":"cross-classification, prospective: NHANEScross-classification, retrospective: death records (exposure measured post-hoc)case-control, prospective: investigator still enrolls based outcome status, investigator must wait cases occurcase-control, retrospective: start study, cases already occurred investigator goes back measure exposure (explanatory) variablecohort, prospective: follows selected participants assess proportion develop disease interestcohort, retrospective: exposure outcomes already happened (.e., death records)","code":""},{"path":"analysis-of-categorical-data.html","id":"which-test","chapter":"4 Analysis of Categorical Data","heading":"Which test?","text":"(Section 6.1 Kuiper Sklar (2013).)turns tests (independence, homogeneity proportions, homogeneity odds) typically equivalent respect conclusions. However, particular conditions related testing, can generally use hypotheses interest. However, need careful interpretations!(goodness fit, section 6.11 Kuiper Sklar (2013).)","code":""},{"path":"analysis-of-categorical-data.html","id":"r-example-categorical-data-botox-and-back-pain","chapter":"4 Analysis of Categorical Data","heading":"4.6 R Example (categorical data): Botox and back pain","text":"","code":""},{"path":"analysis-of-categorical-data.html","id":"entering-and-visualizing-the-data","chapter":"4 Analysis of Categorical Data","heading":"4.6.1 Entering and visualizing the data","text":"","code":"\nbackpain <- data.frame(treatment = c(rep(\"placebo\", 16), rep(\"Botox\", 15)),\n                     outcome = c(rep(\"reduction\", 2), rep(\"no reduction\", 14), \n                                 rep(\"reduction\", 9), rep(\"no reduction\", 6)))\nbackpain %>%\n  table()\n#>          outcome\n#> treatment no reduction reduction\n#>   Botox              6         9\n#>   placebo           14         2\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome), position = \"fill\") +\n  ylab(\"percentage\")\n\nbackpain %>%\n  ggplot(aes(x = treatment)) + \n  geom_bar(aes(fill = outcome))"},{"path":"analysis-of-categorical-data.html","id":"fishers-exact-test","chapter":"4 Analysis of Categorical Data","heading":"4.6.2 Fisher’s Exact Test","text":"","code":"\nbackpain %>%\n  table() %>%\n  fisher.test()\n#> \n#>  Fisher's Exact Test for Count Data\n#> \n#> data:  .\n#> p-value = 0.009\n#> alternative hypothesis: true odds ratio is not equal to 1\n#> 95 percent confidence interval:\n#>  0.00848 0.71071\n#> sample estimates:\n#> odds ratio \n#>      0.104\n\n# their CI is an inversion of the HT\n# an approximate SE for the ln(OR) is given by:\nse.lnOR <- sqrt(1/(16*(2/16)*(14/16)) + 1/(15*(9/15)*(6/15)))\nse.lnOR\n#> [1] 0.922"},{"path":"analysis-of-categorical-data.html","id":"chi-squared-analysis","chapter":"4 Analysis of Categorical Data","heading":"4.6.3 Chi-squared Analysis","text":"","code":"\nbackpain %>%\n  table() %>%\n  chisq.test()\n#> \n#>  Pearson's Chi-squared test with Yates' continuity correction\n#> \n#> data:  .\n#> X-squared = 6, df = 1, p-value = 0.02"},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"5 Logistic Regression","heading":"5 Logistic Regression","text":"","code":""},{"path":"logistic-regression.html","id":"logmodel","chapter":"5 Logistic Regression","heading":"5.1 Motivation for Logistic Regression","text":"investigation US space shuttle Challenger disaster, learned project managers judged probability mission failure 0.00001, whereas engineers working project estimated failure probability 0.005. difference two probabilities, 0.00499 discounted small worry . different picture provided considering odds? interpreted?logistic regression model generalized linear model. , linear model function expected value response variable. can now model binary response variables.\n\\[\\begin{align}\nGLM: g(E[Y | X]) = \\beta_0 + \\beta_1 X\n\\end{align}\\]\n\\(g(\\cdot)\\) link function. logistic regression, use logit link function:\n\\[\\begin{align}\n\\mbox{logit} (p) = \\ln \\bigg( \\frac{p}{1-p} \\bigg)\n\\end{align}\\]","code":""},{"path":"logistic-regression.html","id":"ex:burnexamp","chapter":"5 Logistic Regression","heading":"5.1.0.1 Surviving Third-degree Burns","text":"data refer 435 adults treated third-degree burns University Southern California General Hospital Burn Center. patients grouped according area third-degree burns body (measured square cm). table recorded, midpoint groupings log(area +1), number patients corresponding group survived, number died burns. (Fan, Heckman, Wand 1995)can see logit transformation linearizes relationship.first idea might model relationship probability success (patient survives) explanatory variable log(area +1) simple linear regression model. However, scatterplot proportions patients surviving third-degree burn explanatory variable shows distinct curved relationship two variables, rather linear one. seems transformation data place.functional form relating x probability success looks like S shape. ’d work figure form S looks like. ’ve given different relationships x probability success using \\(\\beta_0\\) \\(\\beta_1\\) values yet defined. Regardless, can see tuning functional relationship S curve, can get good fit data.S-curves ( y = exp(linear) / (1+exp(linear)) ) variety different parameter settings. Note x-axis continuous variable x y-axis probability success value x. move model.doesn’t linear regression work ?response isn’t normalThe response isn’t linear (transform)predicted values go outside bounds (0,1)Note: work think values inside (0,1) probabilities","code":""},{"path":"logistic-regression.html","id":"the-logistic-model","chapter":"5 Logistic Regression","heading":"5.1.1 The logistic model","text":"Instead trying model using linear regression, let’s say consider relationship variable \\(x\\) probability success given following generalized linear model. (logistic model just one model, isn’t anything magical . good reasons defined , doesn’t mean aren’t good ways model relationship.)\\[\\begin{align}\np(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\n\\(p(x)\\) probability success (surviving burn). \\(\\beta_1\\) still determines direction slope line. \\(\\beta_0\\) now determines location (median survival).Note 1 probability success patient covariate \\(x = -\\beta_0 / \\beta_1\\)?\\[\\begin{align}\nx &= - \\beta_0 / \\beta_1\\\\\n\\beta_0 + \\beta_1 x &= 0\\\\\ne^{0} &= 1\\\\\np(-\\beta_0 / \\beta_1) &= p(x) = 0.5\n\\end{align}\\]\n(given \\(\\beta_1\\), \\(\\beta_0\\) determines median survival value)Note 1 probability success patient covariate \\(x = -\\beta_0 / \\beta_1\\)?\\[\\begin{align}\nx &= - \\beta_0 / \\beta_1\\\\\n\\beta_0 + \\beta_1 x &= 0\\\\\ne^{0} &= 1\\\\\np(-\\beta_0 / \\beta_1) &= p(x) = 0.5\n\\end{align}\\]\n(given \\(\\beta_1\\), \\(\\beta_0\\) determines median survival value)Note 2 \\(x=0\\),\n\\[\\begin{align}\np(0) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\n\\end{align}\\]\n\\(x=0\\) can often thought baseline condition, probability \\(x=0\\) takes place thinking intercept linear regression.Note 2 \\(x=0\\),\n\\[\\begin{align}\np(0) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}}\n\\end{align}\\]\n\\(x=0\\) can often thought baseline condition, probability \\(x=0\\) takes place thinking intercept linear regression.Note 3\\[\\begin{align}\n1 - p(x) = \\frac{1}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\ngives probability failure.\n\\[\\begin{align}\n\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x}\n\\end{align}\\]\ngives odds success.\n\\[\\begin{align}\n\\ln \\bigg( \\frac{p(x)}{1-p(x)} \\bigg) = \\beta_0 + \\beta_1 x\n\\end{align}\\]\ngives \\(\\ln\\) odds success .Note 3\\[\\begin{align}\n1 - p(x) = \\frac{1}{1+e^{\\beta_0 + \\beta_1 x}}\n\\end{align}\\]\ngives probability failure.\n\\[\\begin{align}\n\\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x}\n\\end{align}\\]\ngives odds success.\n\\[\\begin{align}\n\\ln \\bigg( \\frac{p(x)}{1-p(x)} \\bigg) = \\beta_0 + \\beta_1 x\n\\end{align}\\]\ngives \\(\\ln\\) odds success .Note 4 Every type generalized linear model link function. called logit. link relationship response variable linear function x.\n\\[\\begin{align}\n\\mbox{logit}(\\star) = \\ln \\bigg( \\frac{\\star}{1-\\star} \\bigg) \\ \\ \\ \\ 0 < \\star < 1\n\\end{align}\\]Note 4 Every type generalized linear model link function. called logit. link relationship response variable linear function x.\n\\[\\begin{align}\n\\mbox{logit}(\\star) = \\ln \\bigg( \\frac{\\star}{1-\\star} \\bigg) \\ \\ \\ \\ 0 < \\star < 1\n\\end{align}\\]","code":""},{"path":"logistic-regression.html","id":"model-assumptions","chapter":"5 Logistic Regression","heading":"5.1.1.1 model assumptions","text":"Just like linear regression, Y response random component.\\[\\begin{align}\ny &= \\begin{cases}\n1 & \\mbox{ died}\\\\\n0 & \\mbox{ survived}\n\\end{cases}\n\\end{align}\\]\\[\\begin{align}\nY &\\sim \\mbox{Bernoulli}(p)\\\\\nP(Y=y) &= p^y(1-p)^{1-y}\n\\end{align}\\]person risk different covariate (.e., explanatory variable), end different probability success.\n\\[\\begin{align}\nY_i \\sim \\mbox{Bernoulli} \\bigg( p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+ e^{\\beta_0 + \\beta_1 x_i}}\\bigg)\n\\end{align}\\]independent trialssuccess / failureprobability success constant particular \\(X\\).\\(E[Y|x] = p(x)\\) given logistic function","code":""},{"path":"logistic-regression.html","id":"interpreting-coefficients","chapter":"5 Logistic Regression","heading":"5.1.1.2 interpreting coefficients","text":"Let’s say log odds survival given observed (log) burn areas \\(x\\) \\(x+1\\) :\n\\[\\begin{align}\n\\mbox{logit}(p(x)) &= \\beta_0 + \\beta_1 x\\\\\n\\mbox{logit}(p(x+1)) &= \\beta_0 + \\beta_1 (x+1)\\\\\n\\beta_1 &= \\mbox{logit}(p(x+1)) - \\mbox{logit}(p(x))\\\\\n&= \\ln \\bigg(\\frac{p(x+1)}{1-p(x+1)} \\bigg) -  \\ln \\bigg(\\frac{p(x)}{1-p(x)} \\bigg)\\\\\n&= \\ln \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\\ne^{\\beta_1} &= \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\\n\\end{align}\\]\\(e^{\\beta_1}\\) odds ratio dying associated one unit increase x. [\\(\\beta_1\\) change log-odds associated one unit increase x.] 1.50 0.405 2 31\n#> 3 (32.5,35.5] 2.44 0.894 9 34\n#> 4 (35.5,38.5] 3.50 1.253 6 37\n#> 5 (38.5,41.5] 2.00 0.693 2 40\n#> 6 (41.5,44.5] 3.57 1.273 7 43\n#> 7 (44.5,47.5] 6.00 1.792 2 46\n#> 8 (47.5,50.5] 2.00 0.693 1 49\n#> 9 (50.5,53.5] 9.00 2.197 1 52elephantsGRP %>% ggplot(aes(x=age, y=logmate)) + geom_point()","code":"\n<img src=\"08-PoisReg_files/figure-html/unnamed-chunk-9-1.png\" width=\"80%\" style=\"display: block; margin: auto;\" />\n\n\n\n## Inference in Poisson Regression {#inferPois}\n\nRecall that in Poisson regression, $E(Y_i) = \\mu_i = e^{\\beta_0 + \\beta_1 X_i}$.  Therefore, the probability function for the $i^{th}$ observation is:\n\n$$f(y_i) = \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!} = \\frac{e^{-e^{\\beta_0 + \\beta_1 X_i}} \\bigg(e^{\\beta_0 + \\beta_1 X_i}\\bigg)^{y_i}}{y_i!}.$$\nWhich gives the resulting likelihood of:\n\n\n$$L(\\beta_0, \\beta_1) = \\prod_{i=1}^n f(y_i) = \\prod_{i=1}^n \\frac{e^{-\\mu_i} \\mu_i^{y_i}}{y_i!} = \\prod_{i=1}^n \\frac{e^{-e^{\\beta_0 + \\beta_1 X_i}} \\bigg(e^{\\beta_0 + \\beta_1 X_i}\\bigg)^{y_i}}{y_i!}.$$\n\nAnd the log likelihood becomes:\n\n\\begin{eqnarray*}\nl(\\beta_0, \\beta_1) &=& \\ln L(\\beta_0, \\beta_1) = \\sum_{i=1}^n \\bigg(-\\mu_i + y_i \\ln(\\mu_i) - \\ln(y_i!) \\bigg) \\\\\n&=& \\sum_{i=1}^n \\bigg(-e^{\\beta_0 + \\beta_1 X_i} + y_i (\\beta_0 + \\beta_1 X_i) - \\ln(y_i!) \\bigg).\n\\end{eqnarray*}\n\n### Maximum Likelihood \n\nAs with other probabilistic models we've encountered, the joint likelihood of the entire sample represents the product of the individual likelihoods for each data value.  The likelihood (or more typically, the $\\ln$-likelihood) is maximized to find estimates for $\\beta_0$ and $\\beta_1$.\n\nThe parameter estimates maximize the $\\ln$-likelihood, and the SE of the estimates are given by the Fisher Information from the likelihood (roughly the second derivative).\n\nGiven $b_0$ and $b_1$ as estimates of the parameter values, $\\beta_0$ and $\\beta_1$, we can estimate the average count value:\n\n$$\\hat{\\mu}_i = e^{b_0 + b_1 X_i}$$\n\n#### Residuals\n\nNote that we *expect* the residuals of a Poisson model to have larger variability for larger values of the prediction.  That makes interpreting the residuals slightly different that previously for a linear regression model.\n\nPearson residual:\n\n$$res_{pi} = \\frac{y_i - \\hat{\\mu}_i}{\\hat{\\mu}_i}$$\n\nDeviance residual:\n\n\\begin{eqnarray*}\nres_{di} &=& [-\\hat{\\mu}_i + y_i \\ln(\\hat{\\mu}_i) - \\ln(y_i!) ] - [-y_i + y_i \\ln(y_i) - \\ln(y_i!) ] \\\\\n&=& y_i-\\hat{\\mu}_i + y_i \\ln\\bigg(\\frac{\\hat{\\mu}_i}{y_i}\\bigg)\n\\end{eqnarray*}\n\n\nAlong with giving information about individual residual values, the sum of the Deviance residuals is also a way to test for goodness-of-fit of the model.  Given the null hypothesis that the Poisson is the appropriate model, the test of goodness-of-fit for the Poisson model is given by:\n\n$$\\sum_{i=1}^n res_{di} \\stackrel{H_0}{\\sim} \\chi_{n-p}^2.$$\n\n\n#### Dealing with Overdispersion {#overdis}\n\nA probability structure that is given by a variance which is larger than the mean, may end up with:\n\n\\begin{eqnarray*}\nE(Y_i) &=& \\mu_i\\\\\nVar(Y_i) &=& \\phi \\mu_i\\\\\n\\end{eqnarray*}\n\nIf we can estimate $\\phi$, then a more appropriate SE to use is adjusted by $\\phi$:  $SE_Q(\\hat{\\beta}) = \\sqrt{\\phi} SE(\\hat{\\beta})$.  ($Q$ stands for \"quasiPoisson\".)\n\nWhen $Y$ comes from a setting where the sample size is large, then we can use normal (sums of standard normal random variables are distributed according to a $\\chi^2$ distribution) theory to assess the residuals:\n\n$$\\sum_{i=1}^n res_{pi}^2 = \\sum_{i=1}^n \\frac{(Y_i - \\mu_i)^2}{Var(Y_i)} = \\sum_{i=1}^n \\frac{(Y_i - \\mu_i)^2}{\\phi \\mu_i} \\sim \\chi^2_{n-p}$$\n\nThe expected value of $\\chi^2_{n-p}$ is $n-p$, which gives an estimator of $\\phi$ to be:\n\n$$\\hat{\\phi} =  \\sum_{i=1}^n \\frac{(Y_i - \\mu_i)^2}{\\mu_i} /(n-p).$$\n\n\nNote that your text uses the Deviance residual instead of the Pearson residual to estimate $\\phi$.  Both are reasonable things to do.\n\n### Wald Tests\n\nAs before, Wald tests are given by standardizing the coefficients and finding p-values using normal theory:\n\n$$\\frac{b_i - 0}{SE(b_i)} \\stackrel{H_0}{\\sim} N(0,1).$$\n\nIf overdispersion is expected, then the SE is adjusted and the t-distribution is used for calculating p-values:\n\n$$\\frac{b_i - 0}{SE_Q(b_i)} =  \\frac{b_i - 0}{\\sqrt{\\hat{\\phi}} SE(b_i)} \\stackrel{H_0}{\\sim} t_{n-p}.$$\n\n\n### Drop-in-Deviance Tests\n\n\nThe deviance for a Poisson is reasonably straightforward and comes directly from the likelihood, it is twice the sum of the deviance residuals:\n\n$$D = 2 \\sum_{i=1}^n [Y_i \\ln(Y_i / \\hat{\\mu_i}) - (Y_i - \\hat{\\mu_i})].$$\n\n\nFor two nested models (that is, the smaller one is obtained by forcing some of the coefficients in the larger model to be zero), a drop-in-deviance test can be calculated using:\n\n$$D_{reduced} - D_{full} \\stackrel{H_0}{\\sim} \\chi^2_{d}$$\n\nwhere $d$ is the difference in the number of parameters estimated in the two models.\n\nThe drop-in-deviance test can also be adjusted for overdispersion:  $F_Q =  (D_{reduced} - D_{full}) / \\hat{\\phi} \\sim F_{d, n-p}$ where $d$ is the difference in the number of parameters estimated in the two models, and $p$ is the total number of parameters estimated in the full model.\n\n## R Poisson Example\n\nThe R example is taken from data given in the textbook.  The scientific question relates to predicting the total number of observed `species` on the Galapagos archipelago related to island `area` (km$^2$), `elevation` (m), distance (km) to the `nearest` neighbor and to the largest island (km$^2$) in the archipelago Santa Cruz (`scruz`), and the area of the `adjacent` island (km$^2$).  We could also consider an additional response variable which is the island `endemic` species count.\n\n\n```r\ngalap <- readr::read_csv(\"Galapagos.csv\")\nhead(galap)\n#> # A tibble: 6 × 8\n#>   island      species endemics  area elevation nearest scruz adjacent\n#>   <chr>         <dbl>    <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>\n#> 1 Baltra           58       23 25.1        346     0.6   0.6     1.84\n#> 2 Bartolome        31       21  1.24       109     0.6  26.3   572.  \n#> 3 Caldwell          3        3  0.21       114     2.8  58.7     0.78\n#> 4 Champion         25        9  0.1         46     1.9  47.4     0.18\n#> 5 Coamano           2        1  0.05        77     1.9   1.9   904.  \n#> 6 DaphneMajor      18       11  0.34       119     8     8       1.84\nggplot(galap, aes(y=species, x = log(area), color = adjacent)) + geom_point()"},{"path":"logistic-regression.html","id":"glm","chapter":"5 Logistic Regression","heading":"5.1.2 glm","text":"","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% tidy()\n#> # A tibble: 6 × 5\n#>   term            estimate std.error statistic  p.value\n#>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     3.02     0.303         9.96  2.28e-23\n#> 2 log(area)       0.315    0.0185       17.1   2.20e-65\n#> 3 log(elevation)  0.0977   0.0604        1.62  1.06e- 1\n#> 4 nearest        -0.00106  0.00169      -0.626 5.32e- 1\n#> 5 scruz          -0.00314  0.000597     -5.26  1.40e- 7\n#> 6 adjacent       -0.000243 0.0000281    -8.65  5.31e-18"},{"path":"logistic-regression.html","id":"drop-in-deviance","chapter":"5 Logistic Regression","heading":"5.1.3 drop in deviance","text":"seems like might need either log(elevation) nearest. drop deviance test help:relatively large p-value suggests need either variables log(elevation) nearest","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% glance()\n#> # A tibble: 1 × 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3511.      29  -294.  600.  609.     427.          24    30\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% glance()\n#> # A tibble: 1 × 8\n#>   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n#>           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n#> 1         3511.      29  -296.  600.  606.     431.          26    30\n\n431.11 - 427.48\n#> [1] 3.63\n1 - pchisq(3.63, 2)\n#> [1] 0.163"},{"path":"logistic-regression.html","id":"residuals","chapter":"5 Logistic Regression","heading":"5.1.4 residuals","text":"Keep mind expectation Poisson model residuals variable larger predicted values. broom package provides .residuals observed value minus fitted value. think .std.resid Pearson residual.","code":"\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>% head()\n#> # A tibble: 6 × 10\n#>   species `log(area)` scruz adjac…¹ .fitted .resid .std.…²   .hat .sigma .cooksd\n#>     <dbl>       <dbl> <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n#> 1      58       3.22    0.6    1.84    4.61 -4.57   -4.83  0.105    4.04 0.581  \n#> 2      31       0.215  26.3  572.      3.36  0.414   0.427 0.0605   4.15 0.00301\n#> 3       3      -1.56   58.7    0.78    2.76 -3.96   -4.05  0.0417   4.07 0.118  \n#> 4      25      -2.30   47.4    0.18    2.55  3.01    3.08  0.0413   4.11 0.131  \n#> 5       2      -3.00    1.9  904.      2.27 -3.01   -3.10  0.0559   4.11 0.0959 \n#> 6      18      -1.08    8      1.84    3.11 -0.953  -0.986 0.0661   4.15 0.0161 \n#> # … with abbreviated variable names ¹​adjacent, ²​.std.resid\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.resid)) + geom_point()\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"poisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.std.resid)) + geom_point()"},{"path":"logistic-regression.html","id":"quasipoisson","chapter":"5 Logistic Regression","heading":"5.1.5 quasiPoisson","text":"Note analyses can done using overdispersed quasiPoisson model.","code":"\nglm(species ~ log(area) + log(elevation) + nearest + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% tidy()\n#> # A tibble: 6 × 5\n#>   term            estimate std.error statistic  p.value\n#>   <chr>              <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)     3.02      1.29         2.34  0.0278  \n#> 2 log(area)       0.315     0.0786       4.02  0.000507\n#> 3 log(elevation)  0.0977    0.257        0.381 0.707   \n#> 4 nearest        -0.00106   0.00720     -0.147 0.884   \n#> 5 scruz          -0.00314   0.00254     -1.24  0.228   \n#> 6 adjacent       -0.000243  0.000120    -2.03  0.0532\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% tidy()\n#> # A tibble: 4 × 5\n#>   term         estimate std.error statistic  p.value\n#>   <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n#> 1 (Intercept)  3.50      0.203        17.3  8.76e-16\n#> 2 log(area)    0.342     0.0304       11.3  1.63e-11\n#> 3 scruz       -0.00354   0.00197      -1.80 8.36e- 2\n#> 4 adjacent    -0.000221  0.000104     -2.12 4.35e- 2\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>% head()\n#> # A tibble: 6 × 10\n#>   species `log(area)` scruz adjac…¹ .fitted .resid .std.…²   .hat .sigma .cooksd\n#>     <dbl>       <dbl> <dbl>   <dbl>   <dbl>  <dbl>   <dbl>  <dbl>  <dbl>   <dbl>\n#> 1      58       3.22    0.6    1.84    4.61 -4.57   -1.18  0.105    4.04 3.47e-2\n#> 2      31       0.215  26.3  572.      3.36  0.414   0.104 0.0605   4.15 1.80e-4\n#> 3       3      -1.56   58.7    0.78    2.76 -3.96   -0.989 0.0417   4.07 7.05e-3\n#> 4      25      -2.30   47.4    0.18    2.55  3.01    0.752 0.0413   4.11 7.81e-3\n#> 5       2      -3.00    1.9  904.      2.27 -3.01   -0.758 0.0559   4.11 5.72e-3\n#> 6      18      -1.08    8      1.84    3.11 -0.953  -0.241 0.0661   4.15 9.58e-4\n#> # … with abbreviated variable names ¹​adjacent, ²​.std.resid\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.resid)) + geom_point()\n\nglm(species ~ log(area) + scruz + adjacent, \n    data= galap, family=\"quasipoisson\") %>% augment() %>%\n  ggplot(aes(x=.fitted, y=.std.resid)) + geom_point()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
