<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 4 Analysis of Categorical Data (section 6.3) | Methods in Biostatistics</title>
  <meta name="description" content="Class notes for Math 150 at Pomona College: Methods in Biostatistics. The notes are based primarily on the text Practicing Statistics, Kuiper and Sklar">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 4 Analysis of Categorical Data (section 6.3) | Methods in Biostatistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Class notes for Math 150 at Pomona College: Methods in Biostatistics. The notes are based primarily on the text Practicing Statistics, Kuiper and Sklar" />
  <meta name="github-repo" content="hardin47/website/Math150/" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Analysis of Categorical Data (section 6.3) | Methods in Biostatistics" />
  
  <meta name="twitter:description" content="Class notes for Math 150 at Pomona College: Methods in Biostatistics. The notes are based primarily on the text Practicing Statistics, Kuiper and Sklar" />
  

<meta name="author" content="Jo Hardin">


<meta name="date" content="2019-03-20">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="SLR.html">
<link rel="next" href="logistic-regression.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Methods in Biostatistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Class Information</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#course-goals"><i class="fa fa-check"></i><b>1.1</b> Course Goals</a><ul>
<li class="chapter" data-level="" data-path="intro.html"><a href="intro.html#experimental-design"><i class="fa fa-check"></i>Experimental Design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html"><i class="fa fa-check"></i><b>2</b> t-tests vs. SLR</a><ul>
<li class="chapter" data-level="" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#surgery-timing"><i class="fa fa-check"></i>Surgery Timing</a></li>
<li class="chapter" data-level="2.1" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#ttest"><i class="fa fa-check"></i><b>2.1</b> t-test (book: 2.1)</a><ul>
<li class="chapter" data-level="2.1.1" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#what-is-an-alternative-hypothesis"><i class="fa fa-check"></i><b>2.1.1</b> What is an Alternative Hypothesis?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#anova"><i class="fa fa-check"></i>ANOVA</a></li>
<li class="chapter" data-level="2.2" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#tslr"><i class="fa fa-check"></i><b>2.2</b> Simple Linear Regression (book: 2.3)</a><ul>
<li class="chapter" data-level="" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#why-are-they-the-same"><i class="fa fa-check"></i>Why are they the same?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#confidence-intervals-section-2.11"><i class="fa fa-check"></i><b>2.3</b> Confidence Intervals (section 2.11)</a></li>
<li class="chapter" data-level="2.4" data-path="t-tests-vs-slr.html"><a href="t-tests-vs-slr.html#random-sample-vs.random-allocation"><i class="fa fa-check"></i><b>2.4</b> Random Sample vs. Random allocation</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="SLR.html"><a href="SLR.html"><i class="fa fa-check"></i><b>3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="SLR.html"><a href="SLR.html#transformations"><i class="fa fa-check"></i><b>3.1</b> Transformations</a><ul>
<li class="chapter" data-level="" data-path="SLR.html"><a href="SLR.html#model-assumptions"><i class="fa fa-check"></i>Model assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="SLR.html"><a href="SLR.html#fitting-the-regression-line"><i class="fa fa-check"></i><b>3.2</b> Fitting the regression line</a></li>
<li class="chapter" data-level="3.3" data-path="SLR.html"><a href="SLR.html#correlation"><i class="fa fa-check"></i><b>3.3</b> Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="SLR.html"><a href="SLR.html#errors"><i class="fa fa-check"></i><b>3.4</b> Errors</a><ul>
<li class="chapter" data-level="3.4.1" data-path="SLR.html"><a href="SLR.html#testing-beta_1"><i class="fa fa-check"></i><b>3.4.1</b> Testing <span class="math inline">\(\beta_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="SLR.html"><a href="SLR.html#intervals"><i class="fa fa-check"></i><b>3.5</b> Intervals</a><ul>
<li class="chapter" data-level="3.5.1" data-path="SLR.html"><a href="SLR.html#confidence-intervals"><i class="fa fa-check"></i><b>3.5.1</b> Confidence Intervals</a></li>
<li class="chapter" data-level="3.5.2" data-path="SLR.html"><a href="SLR.html#slope"><i class="fa fa-check"></i><b>3.5.2</b> Slope</a></li>
<li class="chapter" data-level="3.5.3" data-path="SLR.html"><a href="SLR.html#mean-response"><i class="fa fa-check"></i><b>3.5.3</b> Mean Response</a></li>
<li class="chapter" data-level="3.5.4" data-path="SLR.html"><a href="SLR.html#prediction-of-an-individual-response"><i class="fa fa-check"></i><b>3.5.4</b> Prediction of an Individual Response</a></li>
<li class="chapter" data-level="3.5.5" data-path="SLR.html"><a href="SLR.html#outlying-high-leverage-and-influential-points"><i class="fa fa-check"></i><b>3.5.5</b> Outlying, High Leverage, and Influential Points</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="SLR.html"><a href="SLR.html#r-example-slr-happy-planet"><i class="fa fa-check"></i><b>3.6</b> R Example (SLR): Happy Planet</a><ul>
<li class="chapter" data-level="3.6.1" data-path="SLR.html"><a href="SLR.html#reading-the-data-into-r"><i class="fa fa-check"></i><b>3.6.1</b> Reading the data into R</a></li>
<li class="chapter" data-level="3.6.2" data-path="SLR.html"><a href="SLR.html#running-the-linear-model-lm"><i class="fa fa-check"></i><b>3.6.2</b> Running the linear model (lm)</a></li>
<li class="chapter" data-level="3.6.3" data-path="SLR.html"><a href="SLR.html#ouptut"><i class="fa fa-check"></i><b>3.6.3</b> Ouptut</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html"><i class="fa fa-check"></i><b>4</b> Analysis of Categorical Data (section 6.3)</a><ul>
<li class="chapter" data-level="4.1" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#cat"><i class="fa fa-check"></i><b>4.1</b> Categorical Inference</a></li>
<li class="chapter" data-level="4.2" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#fisher"><i class="fa fa-check"></i><b>4.2</b> Fisher’s Exact Test (section 6.4)</a></li>
<li class="chapter" data-level="4.3" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#chisq"><i class="fa fa-check"></i><b>4.3</b> Testing independence of two categorical variables (sections 6.5, 6.6, 6.7)</a><ul>
<li class="chapter" data-level="4.3.1" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#chi2-tests-section-6.6"><i class="fa fa-check"></i><b>4.3.1</b> <span class="math inline">\(\chi^2\)</span> tests (section 6.6)</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#catest"><i class="fa fa-check"></i><b>4.4</b> Parameter Estimation (section 6.8)</a><ul>
<li class="chapter" data-level="4.4.1" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#ci-for-differences-in-proportions"><i class="fa fa-check"></i><b>4.4.1</b> CI for differences in proportions</a></li>
<li class="chapter" data-level="4.4.2" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#relative-risk"><i class="fa fa-check"></i><b>4.4.2</b> Relative Risk</a></li>
<li class="chapter" data-level="4.4.3" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#odds-ratios"><i class="fa fa-check"></i><b>4.4.3</b> Odds Ratios</a></li>
<li class="chapter" data-level="4.4.4" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#confidence-interval-for-or"><i class="fa fa-check"></i><b>4.4.4</b> Confidence Interval for OR</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#studies"><i class="fa fa-check"></i><b>4.5</b> Types of Studies (section 6.9)</a><ul>
<li class="chapter" data-level="4.5.1" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#retrospective-versus-prospective-studies"><i class="fa fa-check"></i><b>4.5.1</b> Retrospective versus Prospective Studies</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#r-example-categorical-data-botox-and-back-pain"><i class="fa fa-check"></i><b>4.6</b> R Example (categorical data): Botox and back pain</a><ul>
<li class="chapter" data-level="4.6.1" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#entering-and-visualizing-the-data"><i class="fa fa-check"></i><b>4.6.1</b> Entering and visualizing the data</a></li>
<li class="chapter" data-level="4.6.2" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#fishers-exact-test"><i class="fa fa-check"></i><b>4.6.2</b> Fisher’s Exact Test</a></li>
<li class="chapter" data-level="4.6.3" data-path="analysis-of-categorical-data-section-6-3.html"><a href="analysis-of-categorical-data-section-6-3.html#chi-squared-analysis"><i class="fa fa-check"></i><b>4.6.3</b> Chi-squared Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>5</b> Logistic Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logmodel"><i class="fa fa-check"></i><b>5.1</b> Motivation for Logistic Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#the-logistic-model"><i class="fa fa-check"></i><b>5.1.1</b> The logistic model</a></li>
<li class="chapter" data-level="5.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#constant-or-varying-rr"><i class="fa fa-check"></i><b>5.1.2</b> constant OR, varying RR</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="logistic-regression.html"><a href="logistic-regression.html#logMLE"><i class="fa fa-check"></i><b>5.2</b> Estimating coefficients in logistic regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>5.2.1</b> Maximum Likelihood Estimation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="logistic-regression.html"><a href="logistic-regression.html#loginf"><i class="fa fa-check"></i><b>5.3</b> Formal Inference</a><ul>
<li class="chapter" data-level="5.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#wald-tests-intervals"><i class="fa fa-check"></i><b>5.3.1</b> Wald Tests &amp; Intervals</a></li>
<li class="chapter" data-level="5.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#likelihood-ratio-tests"><i class="fa fa-check"></i><b>5.3.2</b> Likelihood Ratio Tests</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="logistic-regression.html"><a href="logistic-regression.html#multlog"><i class="fa fa-check"></i><b>5.4</b> Multiple Logistic Regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#interaction"><i class="fa fa-check"></i><b>5.4.1</b> Interaction</a></li>
<li class="chapter" data-level="5.4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#simpsons-paradox"><i class="fa fa-check"></i><b>5.4.2</b> Simpson’s Paradox</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="logistic-regression.html"><a href="logistic-regression.html#multicol"><i class="fa fa-check"></i><b>5.5</b> Multicolinearity</a></li>
<li class="chapter" data-level="5.6" data-path="logistic-regression.html"><a href="logistic-regression.html#logstep"><i class="fa fa-check"></i><b>5.6</b> Model Building</a><ul>
<li class="chapter" data-level="5.6.1" data-path="logistic-regression.html"><a href="logistic-regression.html#formal-model-building"><i class="fa fa-check"></i><b>5.6.1</b> Formal Model Building</a></li>
<li class="chapter" data-level="5.6.2" data-path="logistic-regression.html"><a href="logistic-regression.html#getting-the-model-right"><i class="fa fa-check"></i><b>5.6.2</b> Getting the Model Right</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="logistic-regression.html"><a href="logistic-regression.html#model-assessment"><i class="fa fa-check"></i><b>5.7</b> Model Assessment</a><ul>
<li class="chapter" data-level="5.7.1" data-path="logistic-regression.html"><a href="logistic-regression.html#measures-of-association"><i class="fa fa-check"></i><b>5.7.1</b> Measures of Association</a></li>
<li class="chapter" data-level="5.7.2" data-path="logistic-regression.html"><a href="logistic-regression.html#roc"><i class="fa fa-check"></i><b>5.7.2</b> Receiver Operating Characteristic Curves</a></li>
<li class="chapter" data-level="5.7.3" data-path="logistic-regression.html"><a href="logistic-regression.html#cv"><i class="fa fa-check"></i><b>5.7.3</b> Cross Validation</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="logistic-regression.html"><a href="logistic-regression.html#birdexamp"><i class="fa fa-check"></i><b>5.8</b> R: Birdnest Example</a><ul>
<li class="chapter" data-level="5.8.1" data-path="logistic-regression.html"><a href="logistic-regression.html#drop-in-deviance-likelihood-ratio-test-lrt"><i class="fa fa-check"></i><b>5.8.1</b> Drop-in-deviance (Likelihood Ratio Test, LRT)</a></li>
<li class="chapter" data-level="5.8.2" data-path="logistic-regression.html"><a href="logistic-regression.html#difference-between-tidy-and-augment-and-glance"><i class="fa fa-check"></i><b>5.8.2</b> Difference between <code>tidy</code> and <code>augment</code> and <code>glance</code></a></li>
<li class="chapter" data-level="5.8.3" data-path="logistic-regression.html"><a href="logistic-regression.html#looking-at-variables-in-a-few-different-ways."><i class="fa fa-check"></i><b>5.8.3</b> Looking at variables in a few different ways.</a></li>
<li class="chapter" data-level="5.8.4" data-path="logistic-regression.html"><a href="logistic-regression.html#predicting-response"><i class="fa fa-check"></i><b>5.8.4</b> Predicting Response</a></li>
<li class="chapter" data-level="5.8.5" data-path="logistic-regression.html"><a href="logistic-regression.html#measues-of-association"><i class="fa fa-check"></i><b>5.8.5</b> Measues of association</a></li>
<li class="chapter" data-level="5.8.6" data-path="logistic-regression.html"><a href="logistic-regression.html#roc-curves"><i class="fa fa-check"></i><b>5.8.6</b> ROC curves</a></li>
<li class="chapter" data-level="5.8.7" data-path="logistic-regression.html"><a href="logistic-regression.html#drawing-interactions"><i class="fa fa-check"></i><b>5.8.7</b> Drawing interactions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="survival-analysis.html"><a href="survival-analysis.html"><i class="fa fa-check"></i><b>6</b> Survival Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="survival-analysis.html"><a href="survival-analysis.html#timedata"><i class="fa fa-check"></i><b>6.1</b> Time-to-event data</a><ul>
<li class="chapter" data-level="6.1.1" data-path="survival-analysis.html"><a href="survival-analysis.html#survival-function"><i class="fa fa-check"></i><b>6.1.1</b> Survival Function</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="survival-analysis.html"><a href="survival-analysis.html#KM"><i class="fa fa-check"></i><b>6.2</b> Kaplan-Meier Curves</a><ul>
<li class="chapter" data-level="6.2.1" data-path="survival-analysis.html"><a href="survival-analysis.html#KMCI"><i class="fa fa-check"></i><b>6.2.1</b> CI for KM curve</a></li>
<li class="chapter" data-level="6.2.2" data-path="survival-analysis.html"><a href="survival-analysis.html#logrank"><i class="fa fa-check"></i><b>6.2.2</b> Log-rank Test</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="survival-analysis.html"><a href="survival-analysis.html#hazfunc"><i class="fa fa-check"></i><b>6.3</b> Hazard Functions</a><ul>
<li class="chapter" data-level="6.3.1" data-path="survival-analysis.html"><a href="survival-analysis.html#estimating-ht-ala-kaplan-meier"><i class="fa fa-check"></i><b>6.3.1</b> Estimating <span class="math inline">\(h(t)\)</span> ala Kaplan-Meier</a></li>
<li class="chapter" data-level="6.3.2" data-path="survival-analysis.html"><a href="survival-analysis.html#proportional-hazards"><i class="fa fa-check"></i><b>6.3.2</b> Proportional Hazards</a></li>
<li class="chapter" data-level="6.3.3" data-path="survival-analysis.html"><a href="survival-analysis.html#coxph"><i class="fa fa-check"></i><b>6.3.3</b> Cox PH Regression Analysis</a></li>
<li class="chapter" data-level="6.3.4" data-path="survival-analysis.html"><a href="survival-analysis.html#testingph"><i class="fa fa-check"></i><b>6.3.4</b> Testing Proportional Hazards</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="survival-analysis.html"><a href="survival-analysis.html#othersurv"><i class="fa fa-check"></i><b>6.4</b> Other stuff</a><ul>
<li class="chapter" data-level="6.4.1" data-path="survival-analysis.html"><a href="survival-analysis.html#sample-size-calculation"><i class="fa fa-check"></i><b>6.4.1</b> Sample Size Calculation</a></li>
<li class="chapter" data-level="6.4.2" data-path="survival-analysis.html"><a href="survival-analysis.html#study-design"><i class="fa fa-check"></i><b>6.4.2</b> Study Design</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="multiple-comparisons.html"><a href="multiple-comparisons.html"><i class="fa fa-check"></i><b>7</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="8" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>8</b> Poisson Regression</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://st47s.com/Math150" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Methods in Biostatistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis-of-categorical-data-section-6.3" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Analysis of Categorical Data (section 6.3)</h1>
<div id="cat" class="section level2">
<h2><span class="header-section-number">4.1</span> Categorical Inference</h2>
<p>In either an observational study or a randomized experiment, we are often interested in assessing the statistical significance of the differences we see: Is the observed difference too big to have reasonably occurred just due to chance? To answer the question, we will use</p>
<ul>
<li>simulation</li>
<li>mathematical probability models.</li>
</ul>

<div class="example">
<span id="exm:unnamed-chunk-2" class="example"><strong>Example 4.1  </strong></span><strong>Back Pain &amp; Botox</strong>, <span class="citation">Chance and Rossman (<a href="#ref-iscam">2018</a>)</span>, <span class="citation">Foster et al. (<a href="#ref-botox">2001</a>)</span> The randomized clinical trial examined whether the drug botulinum toxin A (Botox) is helpful for reducing pain among patients who suffer from chronic low back pain. The 31 subjects who participated in the study were randomly assigned to one of two treatment groups: 16 received a placebo of normal saline and the other 15 received the drug itself. The subjects’ pain levels were evaluated at the beginning of the study and again after eight weeks. The researchers found that 2 of the 16 subjects who received the saline experienced a substantial reduction in pain, compared to 9 of the 15 subjects who received the actual drug.
</div>

<ol style="list-style-type: decimal">
<li>Is this an experiment or an observational study?<br />
</li>
<li>Explain the importance of using the “placebo” treatment of saline.<br />
</li>
<li>Create the two-way table for summarizing the data, putting the explanatory variable as the columns and the response as rows.<br />
</li>
<li>Calculate the conditional proportions of pain reduction in the two groups. Display the results as a segmented bar graph. Comment on the preliminary analysis.</li>
</ol>
<table>
<thead>
<tr class="header">
<th></th>
<th>placebo</th>
<th>Botox</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pain reduction</td>
<td>2</td>
<td>9</td>
<td>11</td>
</tr>
<tr class="even">
<td>no pain reduction</td>
<td>14</td>
<td>6</td>
<td>20</td>
</tr>
<tr class="odd">
<td></td>
<td>16</td>
<td>15</td>
<td>31</td>
</tr>
</tbody>
</table>
<span class="math display">\[\begin{eqnarray*}
\mbox{risk}_{\mbox{placebo}} = \frac{2}{16} &amp;=&amp; 0.125\\
\mbox{risk}_{\mbox{Botox}} = \frac{9}{15} &amp;=&amp; 0.6\\
RR &amp;=&amp; 4.8
\end{eqnarray*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">backpain &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">14</span>,<span class="dv">9</span>,<span class="dv">6</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>F)
backpain</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    2    9
## [2,]   14    6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">backpain.bp &lt;-<span class="st"> </span><span class="kw">barplot</span>(backpain, <span class="dt">plot=</span><span class="ot">FALSE</span>)
<span class="kw">barplot</span>(backpain, <span class="dt">names.arg=</span><span class="kw">c</span>(<span class="st">&quot;placebo&quot;</span>,<span class="st">&quot;botulinum toxin A&quot;</span>))
<span class="kw">text</span>(backpain.bp, <span class="kw">c</span>(backpain[<span class="dv">1</span>,]<span class="op">-</span><span class="dv">1</span>,backpain[<span class="dv">2</span>,]<span class="op">+</span>backpain[<span class="dv">1</span>,]<span class="op">-</span><span class="dv">1</span>),<span class="kw">t</span>(backpain))</code></pre></div>
<p><img src="04-cat_files/figure-html/unnamed-chunk-3-1.png" width="480" style="display: block; margin: auto;" /></p>
<ol start="5" style="list-style-type: decimal">
<li><p>If there was no association between the treatment and the back pain relief, about how many of the 11 “successes” would you expect to see in each group? Did the researchers observe more successes in the saline group than expected (if the drug had no effect) or fewer successes than expected? Is this in the direction conjectured by the researchers?</p></li>
<li><p>Is is <em>possible</em> that the drug has absolutely no effect on back pain? That the differences were simply due to chance or random variability? How likely is that?</p></li>
</ol>
<div id="simulation" class="section level4 unnumbered">
<h4>Simulation</h4>
<ul>
<li>11 red “success” cards (pain reduction); 20 black “failure” cards (no pain reduction)</li>
<li>randomly deal out (i.e. shuffle) 15 cards to the treatment group and 16 cards to the placebo group.</li>
<li><p>count how many people in the treatment group were successes? Repeat 5 times.</p></li>
<li>process
<ul>
<li>what do the cards represent?</li>
<li>what does shuffling the cards represent?</li>
<li>what implicit assumption about the two groups did the shuffling of cards represent?</li>
<li>what observational units would be represented by the dots on the dotplot?</li>
<li>why would we count the number of repetitions with 9 or more “successes”?</li>
</ul></li>
<li><p>Repeat simulation using the two-way table applet: <a href="http://www.rossmanchance.com/applets/TwowaySim/TwowaySim.html" class="uri">http://www.rossmanchance.com/applets/TwowaySim/TwowaySim.html</a></p></li>
<li>summary
<ul>
<li>How many reps?</li>
<li>How many as extreme as the true data?</li>
<li>What proportion are at least as extreme as the true data?</li>
<li>Do our data support the researchers conjecture?</li>
<li>What if the actual data had been 7 successes in the treatment group (and 4 in the placebo group)?</li>
</ul></li>
</ul>

<div class="definition">
<span id="def:unnamed-chunk-4" class="definition"><strong>Definition 2.1  </strong></span><strong>p-value</strong> The p-value is the probability of seeing our results or more extreme if there is nothing interesting going on with the data. (This is the same definition of p-value that you will always use in this class and in your own research.)
</div>

<p>Notice that regardless of whether or not the drug has an effect, the data will be different each time (think: new 31 people). The small p-value allows us to draw cause-and-effect conclusions, but doesn’t necessarily allow us to infer to a larger population. Why not?</p>
<table>
<thead>
<tr class="header">
<th align="center">low cutoff</th>
<th align="center">p-value</th>
<th align="center">high cutoff</th>
<th align="left">evidence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"></td>
<td align="center">p-value <span class="math inline">\(\leq\)</span></td>
<td align="center">0.001</td>
<td align="left">very strong evidence</td>
</tr>
<tr class="even">
<td align="center">0.001</td>
<td align="center"><span class="math inline">\(&lt;\)</span> p-value <span class="math inline">\(\leq\)</span></td>
<td align="center">0.01</td>
<td align="left">strong evidence</td>
</tr>
<tr class="odd">
<td align="center">0.01</td>
<td align="center"><span class="math inline">\(&lt;\)</span> p-value <span class="math inline">\(\leq\)</span></td>
<td align="center">0.05</td>
<td align="left">moderate evidence</td>
</tr>
<tr class="even">
<td align="center">0.05</td>
<td align="center"><span class="math inline">\(&lt;\)</span> p-value <span class="math inline">\(\leq\)</span></td>
<td align="center">0.10</td>
<td align="left">weak but suggestive evidence</td>
</tr>
<tr class="odd">
<td align="center">0.10</td>
<td align="center"><span class="math inline">\(&lt;\)</span> p-value</td>
<td align="center"></td>
<td align="left">little or no evidence</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="fisher" class="section level2">
<h2><span class="header-section-number">4.2</span> Fisher’s Exact Test (section 6.4)</h2>
<p>Because we have a fixed sample, we can’t use the Binomial distribution to figure out associated probabilities. Instead, we use the hypergeometric distribution to enumerate the possible ways of choosing our data or more extreme given fixed row and column totals.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">placebo</th>
<th align="center">Botox</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>pain reduction</td>
<td align="center">2 = x</td>
<td align="center">9</td>
<td align="center">11 = n</td>
</tr>
<tr class="even">
<td>no pain reduction</td>
<td align="center">14</td>
<td align="center">6</td>
<td align="center">20</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">16 = M</td>
<td align="center">15 = N - M</td>
<td align="center">31 = N</td>
</tr>
</tbody>
</table>
<p>To make it simpler, let’s say I have 5 items (N=5), and I want to choose 3 of them (n=3). How many ways can I do that?</p>
<pre><code>SSSNN, SSNSN, SSNNS, SNSSN, SNSNS, SNNSS, NSSSN, NSSNS, NSNSS, NNSSS  [5!/ 3! 2!]
(S = select, N = not selected)</code></pre>
<p>So, how many different ways can I select 11 people (out of 31) to be my “pain reduction” group? That is the total number of different groups of size 11 from 31. But really, we want our groups to be of a certain breakdown. We need 2 (of 16) to have gotten the placebo and 9 (of 15) to have gotten the Botox treatment.</p>

<div class="definition">
<span id="def:unnamed-chunk-5" class="definition"><strong>Definition 4.1  </strong></span><strong>Hyp Geom Prob</strong> For any <span class="math inline">\(2 \times 2\)</span> table when there are N observations with M total successes , the probability of observing x successes in a sample of size n is:
</div>

<span class="math display">\[\begin{eqnarray*}
P(X=x) = \frac{\# \mbox{ of ways to select x successes and n-x failures}}{\# \mbox{ of ways to select n subjects}} = \frac{ { M \choose x} {N-M \choose n-x}}{{N \choose n}}\\
\end{eqnarray*}\]</span>
<p>Find the P(X=2)</p>
We can now find EXACT probabilities associated with the following hypotheses.
<span class="math display">\[\begin{eqnarray*}
&amp;&amp;H_0: \pi_{pl} = \pi_{Btx}\\
&amp;&amp;H_a: \pi_{pl} &lt; \pi_{Btx}\\
&amp;&amp;\pi = \mbox{true probability of no pain}\\
\end{eqnarray*}\]</span>
<p>Is this a one- or two-sided test? Why? [Note: the assumptions here are that the row and column totals are fixed – a <strong>conditional test of independence</strong>. However, the research project in the back of chapter 6 extends the permutation test to demonstrated that the probabilities hold even under alternative technical conditions.</p>
<p>Note also that we get an exact probability with no assumptions about sample size (we can use Fisher’s Exact Test even when true probabilities are close to 0 or 1.]</p>
</div>
<div id="chisq" class="section level2">
<h2><span class="header-section-number">4.3</span> Testing independence of two categorical variables (sections 6.5, 6.6, 6.7)</h2>
<div id="chi2-tests-section-6.6" class="section level3">
<h3><span class="header-section-number">4.3.1</span> <span class="math inline">\(\chi^2\)</span> tests (section 6.6)</h3>
<p>2x2… but also rxc (<span class="math inline">\(\pi_a = \pi_b = \pi_c\)</span>)</p>
<p>We can also use <span class="math inline">\(\chi^2\)</span> tests to evaluate <span class="math inline">\(r \times c\)</span> contingency tables. Our main question now will be whether there is an association between two categorical variables of interest. Note that we are now generalizing what we did with the Botox and back pain example. Are the two variables independent? If the two variables are independent, then the state of one variable is not related to the probability of the different outcomes of the other variable.</p>
<p>If the data were sampled in such a way that we have random samples of both the explanatory and response variables (e.g., cross classification study), then we typically do a test of association:</p>
<span class="math display">\[\begin{eqnarray*}
H_0: &amp;&amp; \mbox{ the two variables are independent}\\
H_a: &amp;&amp; \mbox{ the two variables are not independent}
\end{eqnarray*}\]</span>
<p>If the data are sampled in such a way that the response is measured across specified populations (as in the example below), we typically do a test of homogeneity of proportions. For example,</p>
<span class="math display">\[\begin{eqnarray*}
H_0: &amp;&amp; \pi_1 = \pi_2 = \pi_3\\
H_a: &amp;&amp; \mbox{not } H_0
\end{eqnarray*}\]</span>
<p>where <span class="math inline">\(\pi=P(\mbox{success})\)</span> for each of groups 1,2,3.</p>
<p>How do we get expected frequencies? The same mathematics hold regardless of the type of test (i.e., sampling mechanism used to collect the data). If, in fact,the variables are independent, then we should be able to multiply their probabilities. If the probabilities are the same, we expect the overall proportion of each response variable to be the same as the proportion of the response variable in each explanatory group. And the math in the example below follows directly.</p>

<div class="example">
<span id="exm:unnamed-chunk-6" class="example"><strong>Example 4.2  </strong></span>The table below show the observed distributions of ABO blood type in three random samples of African Americans living in different locations. The three datasets, collected in the 1950s by three different investigators, are reproduced in <span class="citation">(Mourant, Kopec, and Domaniewsa-Sobczak <a href="#ref-bloodtype">1976</a>)</span>.
</div>

<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th align="center">Location</th>
<th align="center"></th>
<th align="center"></th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td align="center">(Florida)</td>
<td align="center">(Iowa)</td>
<td align="center">(Missouri)</td>
<td align="right">Total</td>
</tr>
<tr class="even">
<td></td>
<td>A</td>
<td align="center">122</td>
<td align="center">1781</td>
<td align="center">353</td>
<td align="right">2256</td>
</tr>
<tr class="odd">
<td>Blood</td>
<td>B</td>
<td align="center">117</td>
<td align="center">1351</td>
<td align="center">269</td>
<td align="right">1737</td>
</tr>
<tr class="even">
<td>Type</td>
<td>AB</td>
<td align="center">19</td>
<td align="center">289</td>
<td align="center">60</td>
<td align="right">368</td>
</tr>
<tr class="odd">
<td></td>
<td>O</td>
<td align="center">244</td>
<td align="center">3301</td>
<td align="center">713</td>
<td align="right">4258</td>
</tr>
<tr class="even">
<td></td>
<td>Total</td>
<td align="center">502</td>
<td align="center">6722</td>
<td align="center">1395</td>
<td align="right">8619</td>
</tr>
</tbody>
</table>
<div id="test-of-homogeneity-of-proportions-equivalent-mathematically-to-independence" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> Test of Homogeneity of Proportions (equivalent mathematically to independence)</h4>
<p>If there is no difference in blood type proportions across the groups, then:</p>
<span class="math display">\[\begin{eqnarray*}
P(AB | FL) = P(AB | IA) = P(AB | MO) = P(AB)
\end{eqnarray*}\]</span>
<p>We will use <span class="math inline">\(\hat{P}(AB) = \frac{368}{8619}\)</span> as baseline for expectation (under <span class="math inline">\(H_0\)</span>) for all the groups. That is, we would expect,</p>
<span class="math display">\[\begin{eqnarray*}
\# \mbox{expected for AB blood and Iowa} &amp;=&amp;  \frac{368}{8619} \cdot 6722\\
\end{eqnarray*}\]</span>
</div>
<div id="test-of-independence-equivalent-mathematically-to-homogeneity-of-proporitions" class="section level4">
<h4><span class="header-section-number">4.3.1.2</span> Test of Independence (equivalent mathematically to homogeneity of proporitions)</h4>
<span class="math display">\[\begin{eqnarray*}
P(cond1 \mbox{ &amp; } cond2 ) &amp;=&amp; P(cond1) P(cond2)  \mbox{ if variables 1 and 2 are independent}\\
P(AB \mbox{ blood &amp; Iowa}) &amp;=&amp; P(AB \mbox{ blood}) P(\mbox{Iowa}) \\
&amp;=&amp; \bigg( \frac{368}{8619}\bigg) \bigg( \frac{6722}{8619} \bigg)\\
&amp;=&amp; 0.0333\\
\# \mbox{expected for AB blood and Iowa} &amp;=&amp; 0.033 \cdot 8619\\
&amp;=&amp; \frac{368 \cdot 6722}{8619}\\
E_{i,j} &amp;=&amp; \frac{(i \mbox{ row total})(j \mbox{ col total})}{\mbox{grand total}}\\
\end{eqnarray*}\]</span>
<p>And the expected values under the null hypothesis…</p>
<table>
<thead>
<tr class="header">
<th></th>
<th></th>
<th align="center">Location</th>
<th align="center"></th>
<th align="center"></th>
<th align="right"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td align="center">(Florida)</td>
<td align="center">(Iowa)</td>
<td align="center">(Missouri)</td>
<td align="right">Total</td>
</tr>
<tr class="even">
<td></td>
<td>A</td>
<td align="center">131.4</td>
<td align="center">1759.47</td>
<td align="center">365.14</td>
<td align="right">2256</td>
</tr>
<tr class="odd">
<td>Blood</td>
<td>B</td>
<td align="center">101.17</td>
<td align="center">1354.69</td>
<td align="center">281.14</td>
<td align="right">1737</td>
</tr>
<tr class="even">
<td>Type</td>
<td>AB</td>
<td align="center">21.43</td>
<td align="center">287.00</td>
<td align="center">59.56</td>
<td align="right">368</td>
</tr>
<tr class="odd">
<td></td>
<td>O</td>
<td align="center">248.00</td>
<td align="center">3320.83</td>
<td align="center">689.16</td>
<td align="right">4258</td>
</tr>
<tr class="even">
<td></td>
<td>Total</td>
<td align="center">502</td>
<td align="center">6722</td>
<td align="center">1395</td>
<td align="right">8619</td>
</tr>
</tbody>
</table>
<span class="math display">\[\begin{eqnarray*}
X^2 &amp;=&amp; \sum_{all cells} \frac{( O - E)^2}{E}\\
&amp;=&amp; 5.65\\
\mbox{p-value} &amp;=&amp; P(\chi^2_6 \geq 5.65) \\
&amp;=&amp; 1 - pchisq(5.65, 6)\\
&amp;=&amp; 0.464
\end{eqnarray*}\]</span>
<p>We cannot reject the null hypothesis. Again, we have no evidence against the null hypothesis that blood types are independently distributed in the various regions.</p>
<p>How do we know if our test statistic is a big number or not? Well, it turns out that the test statistic (<span class="math inline">\(X^2\)</span>) will have an approximate <span class="math inline">\(\chi^2\)</span> distribution with degrees of freedom = <span class="math inline">\((r- 1)\cdot (c-1)\)</span>. As long as:</p>
<ul>
<li>We have a random sample from the population.<br />
</li>
<li>We expect at least 1 observation in every cell (<span class="math inline">\(E_i \geq 1 \forall i\)</span>)<br />
</li>
<li>We expect at least 5 observations in 80% of the cells (<span class="math inline">\(E_i \geq 5\)</span> for 80% of <span class="math inline">\(i\)</span>)</li>
</ul>
<p>When there are only two populations, the <span class="math inline">\(\chi^2\)</span> procedure is equivalent to the two-sided z-test for proportions. The chi-squared test statistic is the square of the z-test statistic. That is, the chi-squared test is exactly the same as the two-sided alternative for the z-test.</p>
<p>use chi-square if you have multiple populations</p>
<p>use z-test if you want one-sided tests or confidence intervals.</p>
</div>
</div>
</div>
<div id="catest" class="section level2">
<h2><span class="header-section-number">4.4</span> Parameter Estimation (section 6.8)</h2>

<div class="definition">
<span id="def:unnamed-chunk-7" class="definition"><strong>Definition 4.2  </strong></span><strong>Data Types</strong> Data are often classified as
</div>

<ul>
<li>Categorical - each unit is assigned to a category<br />
</li>
<li>Quantitative - each observational unit is assigned a numerical value<br />
</li>
<li>(Binary - a special case of categorical with 2 categories, e.g. male/female)</li>
</ul>
<pre><code>Table 6.6 on page 193 of the textbook is excellent and worth looking at.</code></pre>

<div class="example">
<p><span id="exm:unnamed-chunk-8" class="example"><strong>Example 4.3  </strong></span><strong>Popcorn &amp; Lung Disease</strong> <span class="citation">Chance and Rossman (<a href="#ref-iscam">2018</a>)</span></p>
How can we tell if popcorn production is related to lung disease? Consider High / Low exposure:
</div>

<table>
<thead>
<tr class="header">
<th></th>
<th align="center">low exposure</th>
<th align="center">high exposure</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Airway obstructed</td>
<td align="center">6</td>
<td align="center">15</td>
<td align="center">21</td>
</tr>
<tr class="even">
<td>Airway not obstructed</td>
<td align="center">52</td>
<td align="center">43</td>
<td align="center">95</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">58</td>
<td align="center">58</td>
<td align="center">116</td>
</tr>
</tbody>
</table>
<p>Is 21 a lot of people? Can we compare 6 vs. 15? What should we look at? <em>proportions</em> (always a number between 0 and 1). Look at your data (graphically and numerically). Segmented bar graph (mosaic plot):</p>

<p>Is there a difference in the two groups? Look at the difference in proportions or risk:</p>
<span class="math display">\[\begin{eqnarray*}
6/58 = 0.103 &amp; 15/58=0.2586 &amp; \Delta = 0.156\\
p_1 = 0.65 &amp; p_2 = 0.494 &amp; \Delta = 0.156\\
p_1 = 0.001 &amp; p_2 = 0.157 &amp; \Delta = 0.156\\
\end{eqnarray*}\]</span>

<p>It turns out that the sampling distribution of the difference in the sample proportions (of success) across two <em>independent</em> groups can be modeled by the normal distribution if we have reasonably large sample sizes (CLT).</p>
<p>To ensure the accuracy of the test, check whether np and n(1-p) is bigger than 5 in both samples is usually adequate. A more precise check is <span class="math inline">\(n_s \hat{p}_c\)</span> and <span class="math inline">\(n_s(1-\hat{p}_c)\)</span> are both greater than 5; <span class="math inline">\(n_s\)</span> is the smaller of the two sample sizes and <span class="math inline">\(\hat{p}_c\)</span>is the sample proportion when the two samples are combined into one.</p>
Note:
<span class="math display">\[\begin{eqnarray*}
\hat{p}_1 - \hat{p}_2 \sim N\Bigg(\pi_1 - \pi_2, \sqrt{\frac{\pi_1(1-\pi_1)}{n_1} + \frac{\pi_2(1-\pi_2)}{n_2}}\Bigg)
\end{eqnarray*}\]</span>
When testing independence, we assume that <span class="math inline">\(\pi_1=\pi_2\)</span>, so we use the pooled estimate of the proportion to calculate the SE:
<span class="math display">\[\begin{eqnarray*}
SE(\hat{p}_1 - \hat{p}_2) = \sqrt{ \hat{p}_c(1-\hat{p}_c) \bigg(\frac{1}{n_1} + \frac{1}{n_2}\bigg)}
\end{eqnarray*}\]</span>
So, when testing, the appropriate test statistic is:
<span class="math display">\[\begin{eqnarray*}
 Z = \frac{\hat{p}_1 - \hat{p}_2 - 0}{ \sqrt{ \hat{p}_c(1-\hat{p}_c) (\frac{1}{n_1} + \frac{1}{n_2})}}
\end{eqnarray*}\]</span>
<div id="ci-for-differences-in-proportions" class="section level3">
<h3><span class="header-section-number">4.4.1</span> CI for differences in proportions</h3>
<p>We can’t pool our estimate for the SE, but everything else stays the same…</p>
<span class="math display">\[\begin{eqnarray*}
SE(\hat{p}_1 - \hat{p}_2) = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}
\end{eqnarray*}\]</span>
<p>The main idea here is to determine whether two categorical variables are independent. That is, does knowledge of the value of one variable tell me something about the probability of the other variable (gender and pregnancy). We’re going to talk about two different ways to approach this problem.</p>
</div>
<div id="relative-risk" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Relative Risk</h3>

<div class="definition">
<span id="def:unnamed-chunk-9" class="definition"><strong>Definition 4.3  </strong></span><strong>Relative Risk</strong> The relative risk (RR) is the ratio of risks for each group. We say, “The risk of success is <strong>RR</strong> times higher for those in group 1 compared to those in group 2.”
</div>

<span class="math display">\[\begin{eqnarray*}
\mbox{relative risk} &amp;=&amp; \frac{\mbox{risk group 1}}{\mbox{risk group 2}}\\
&amp;=&amp;  \frac{\mbox{proportion of successes in group 1}}{\mbox{proportion of successes in group 2}}\\
\mbox{RR} &amp;=&amp; \frac{p_1}{p_2} = \frac{\pi_1}{\pi_2}\\
\hat{\mbox{RR}} &amp;=&amp; \frac{\hat{p}_1}{\hat{p}_2}
\end{eqnarray*}\]</span>
<p><span class="math inline">\(\hat{RR}\)</span> in the popcorn example is <span class="math inline">\(\frac{15/58}{6/58} = 2.5\)</span>. We say, “The risk of airway obstruction is 2.5 times higher for those in high exposure group compared to those in the low exposure group.” What about</p>
<ul>
<li>sample size?<br />
</li>
<li>baseline risk?</li>
</ul>
<p>To create confidence intervals for relative risk, we use the fact that:</p>
<span class="math display">\[\begin{eqnarray*}
SE(\ln (\hat{RR})) &amp;\approx&amp; \sqrt{\frac{(1 - \hat{p}_1)}{n_1 \hat{p}_1} + \frac{(1-\hat{p}_2)}{n_2 \hat{p}_2}}
\end{eqnarray*}\]</span>
</div>
<div id="odds-ratios" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Odds Ratios</h3>
<p>A related concept to risk is odds. It is often used in horse racing, where “success” is typically defined as losing. So, if the odds are 3 to 1 we would expect to lose 3/4 of the time.</p>

<div class="definition">
<span id="def:unnamed-chunk-10" class="definition"><strong>Definition 4.4  </strong></span><strong>Odds Ratio</strong> A related concept to risk is odds. It is often used in horse racing, where “success” is typically defined as losing. So, if the odds are 3 to 1 we would expect to lose 3/4 of the time. The odds ratio (OR) is the ratio of odds for each group. We say, “The odds of success is <strong>OR</strong> times higher for those in group 1 compared to those group 2.”
</div>

<span class="math display">\[\begin{eqnarray*}
\mbox{odds} &amp;=&amp; \frac{\mbox{proportion of successes}}{\mbox{proportion of failures}}\\
&amp;=&amp; \frac{\mbox{number of successes}}{\mbox{number of failures}} = \theta\\
\hat{\mbox{odds}} &amp;=&amp; \hat{\theta}\\
\mbox{odds ratio} &amp;=&amp; \frac{\mbox{odds group 1}}{\mbox{odds group 2}} \\
\mbox{OR} &amp;=&amp; \frac{\theta_1}{\theta_2} = \frac{p_1/(1-p_1)}{p_2/(1-p_2)}= \frac{\pi_1/(1-\pi_1)}{\pi_2/(1-\pi_2)}\\
\hat{\mbox{OR}} &amp;=&amp; \frac{\hat{\theta}_1}{\hat{\theta}_2} = \frac{\hat{p}_1/(1-\hat{p}_1)}{\hat{p}_2/(1-\hat{p}_2)}\\
\end{eqnarray*}\]</span>
<p><span class="math inline">\(\hat{OR}\)</span> in the popcorn example is <span class="math inline">\(\frac{15/43}{6/52} = 3.02\)</span>. We say, “The odds of airway obstruction are 3 times higher for those in the high exposure group compared to those in the low exposure group.”</p>
<div id="or-is-more-extreme-than-rr" class="section level4">
<h4><span class="header-section-number">4.4.3.1</span> OR is more extreme than RR</h4>
<p>Without loss of generality, assume the true <span class="math inline">\(RR &gt; 1\)</span>, implying <span class="math inline">\(\pi_1 / \pi_2 &gt; 1\)</span> and <span class="math inline">\(\pi_1 &gt; \pi_2\)</span>.</p>
<p>Note the following sequence of consequences:</p>
<span class="math display">\[\begin{eqnarray*}
RR = \frac{\pi_1}{\pi_2} &amp;&gt;&amp; 1\\
\frac{1 - \pi_1}{1 - \pi_2} &amp;&lt;&amp; 1\\
\frac{ 1 / (1 - \pi_1)}{1 / (1 - \pi_2)} &amp;&gt;&amp; 1\\
\frac{\pi_1}{\pi_2} \cdot \frac{ 1 / (1 - \pi_1)}{1 / (1 - \pi_2)} &amp;&gt;&amp; \frac{\pi_1}{\pi_2}\\
OR &amp;&gt;&amp; RR
\end{eqnarray*}\]</span>
</div>
<div id="other-considerations" class="section level4">
<h4><span class="header-section-number">4.4.3.2</span> Other considerations:</h4>
<ul>
<li>Observational study (who worked in each place?)<br />
</li>
<li>Cross sectional (only one point in time)<br />
</li>
<li>Healthy worker effect (who stayed home sick?)<br />
</li>
<li><strong>Explanatory variable</strong> is one that is a potential explanation for any changes (here exposure level).<br />
</li>
<li><strong>Response variable</strong> is the measured outcome of interest (here airway obstruction).</li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-11" class="example"><strong>Example 4.4  </strong></span><strong>Smoking &amp; Lung Cancer</strong> <span class="citation">Chance and Rossman (<a href="#ref-iscam">2018</a>)</span></p>
<p>After World War II, evidence began mounting that there was a link between cigarette smoking and pulmonary carcinoma (lung cancer). In the 1950s, two now classic articles were published on the subject. One of these studies was conducted in the United States by <span class="citation">Wynder and Graham (<a href="#ref-wynder">1950</a>)</span>. They found records from a large number (684) of patients with proven bronchiogenic carcinoma (a specific form of lung cancer) in hospitals in California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, and Utah. They personally interviewed 634 of the subjects to identify their smoking habits, occupation, exposure to dust and fumes, alcohol intake, education, and cause of death of parents and siblings. Thirty-three subjects completed mailed questionnaires, and information for the other 17 was obtained from family members or close acquaintances. Of those in the study, the researchers focused on 605 male patients with the same form of lung cancer. Another 1332 hospital patients with similar age and economic distribution (including 780 males) without lung cancer were interviewed by these researchers in St. Louis and by other researchers in Boston, Cleveland, and Hines, Illinois.</p>
The following two-way table replicates the counts for the 605 male patients with the same form of cancer and for the “control-group” of 780 males.
</div>

<table>
<thead>
<tr class="header">
<th></th>
<th align="center">none</th>
<th align="center">light</th>
<th align="center">mod heavy</th>
<th align="center">heavy</th>
<th align="center">excessive</th>
<th align="center">chain</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td align="center"><span class="math inline">\(&lt;\)</span> 1/day</td>
<td align="center">1-9/day</td>
<td align="center">10-15/day</td>
<td align="center">16-20/day</td>
<td align="center">21-34/day</td>
<td align="center">35<span class="math inline">\(+\)</span>/day</td>
</tr>
<tr class="even">
<td>patients</td>
<td align="center">8</td>
<td align="center">14</td>
<td align="center">61</td>
<td align="center">213</td>
<td align="center">187</td>
<td align="center">122</td>
</tr>
<tr class="odd">
<td>controls</td>
<td align="center">114</td>
<td align="center">90</td>
<td align="center">148</td>
<td align="center">278</td>
<td align="center">90</td>
<td align="center">60</td>
</tr>
</tbody>
</table>
<p>Given the results of the study, do you think we can generalize from the sample to the population? Explain and make it clear that you know the difference between a sample and a population.</p>

<ul>
<li>Causation?<br />
</li>
<li>Case-control study (605 with lung cancer, 780 without… baseline rate?)</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Group A</th>
<th>Group B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>expl = smoking status</td>
<td>expl = lung cancer</td>
</tr>
<tr class="even">
<td>resp = lung cancer</td>
<td>resp = smoking status</td>
</tr>
</tbody>
</table>
<ul>
<li>If lung cancer is considered a success and no smoking is baseline:</li>
</ul>
<span class="math display">\[\begin{eqnarray*}
\hat{RR} &amp;=&amp; \frac{122/182}{8/122} = 10.22\\
\hat{OR} &amp;=&amp; \frac{122/60}{8/114} = 28.9\\
\end{eqnarray*}\]</span>
<p>The risk of lung cancer is 10.22 times higher for those who smoke than for those who don’t smoke.</p>
<p>The odds of lung cancer is 28.9 times higher for those who smoke than for those who don’t smoke.</p>
<ul>
<li>If chain smoking is considered a success and healthy is baseline:</li>
</ul>
<span class="math display">\[\begin{eqnarray*}
\hat{RR} &amp;=&amp; \frac{122/130}{60/174} = 2.7\\
\hat{OR} &amp;=&amp; \frac{122/8}{60/114} = 28.9\\
\end{eqnarray*}\]</span>
<p>The risk of smoking is 2.7 times higher for those who have lung cancer than for those who don’t have lung cancer.</p>
<p>The odds of smoking is 28.9 times higher for those who have lung cancer than for those who don’t have lung cancer.</p>
<p>We know the risk of being a light smoker if you have lung cancer but we do <em>not</em> know the risk of lung cancer if you are a light smoker. Let’s say we have a <em>population</em> of 1,000,000 people:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">light smoking</th>
<th align="center">no smoking</th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cancer</td>
<td align="center">49,000</td>
<td align="center">1,000</td>
<td align="center">50,000</td>
</tr>
<tr class="even">
<td>healthy</td>
<td align="center">51,000</td>
<td align="center">899,000</td>
<td align="center">950,000</td>
</tr>
<tr class="odd">
<td></td>
<td align="center">100,000</td>
<td align="center">900,000</td>
<td align="center">1,000,000</td>
</tr>
</tbody>
</table>
<span class="math display">\[\begin{eqnarray*}
P(\mbox{light} | \mbox{lung cancer}) &amp;=&amp; \frac{49,000}{50,000} = 0.98\\
P(\mbox{lung cancer} | \mbox{light}) &amp;=&amp; \frac{49,000}{100,000} = 0.49\\
\end{eqnarray*}\]</span>
<ul>
<li>What is the explanatory variable?</li>
<li>What is the response variable?</li>
<li>relative risk?</li>
<li>odds ratio?</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Group A</th>
<th>Group B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>expl = smoking status</td>
<td>expl = lung cancer</td>
</tr>
<tr class="even">
<td>resp = lung cancer</td>
<td>resp = smoking status</td>
</tr>
</tbody>
</table>
<ul>
<li>If lung cancer is considered a success and no smoking is baseline:</li>
</ul>
<span class="math display">\[\begin{eqnarray*}
RR &amp;=&amp; \frac{49/100}{1/900} = 441\\
OR &amp;=&amp; \frac{49/51}{1/899} = 863.75\\
\end{eqnarray*}\]</span>
<ul>
<li>If light smoking is considered a success and healthy is baseline:</li>
</ul>
<span class="math display">\[\begin{eqnarray*}
RR &amp;=&amp; \frac{49/50}{51/950} = 18.25\\
OR &amp;=&amp; \frac{49/1}{51/899} = 863.75\\
\end{eqnarray*}\]</span>
<p>OR is the same no matter which variable you choose as explanatory versus response! Though, in general, we still prefer to know baseline odds or baseline risk (which we can’t know with a case-control study).</p>

<div class="example">
<span id="exm:unnamed-chunk-12" class="example"><strong>Example 4.5  </strong></span><strong>More on Smoking &amp; Lung Cancer</strong>, <span class="citation">Chance and Rossman (<a href="#ref-iscam">2018</a>)</span> Now we have a cohort prospective study. (Previously we had a case-control retrospective study). Now do we have baseline risk estimates? Yes! But be careful, we can’t conclude causation, because the study is still observational.
</div>

</div>
</div>
<div id="confidence-interval-for-or" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Confidence Interval for OR</h3>
<p>Due to some theory that we won’t cover:</p>
<span class="math display">\[\begin{eqnarray*}
SE(\ln (\hat{OR})) &amp;\approx&amp; \sqrt{\frac{1}{n_1 \hat{p}_1 (1-\hat{p}_1)} + \frac{1}{n_2 \hat{p}_2 (1-\hat{p}_2)}}
\end{eqnarray*}\]</span>
<p>Note that your book introduces <span class="math inline">\(SE(\ln(\hat{OR}))\)</span> in the context of hypothesis testing where the null, <span class="math inline">\(H_0: \pi_1 = \pi_2\)</span>, is assumed to be true. If the null is true, you’d prefer an estimate for the proportion of success to be based on the entire sample:</p>
<span class="math display">\[\begin{eqnarray*}
SE(\ln (\hat{OR})) &amp;\approx&amp; \sqrt{\frac{1}{n_1 \hat{p} (1-\hat{p})} + \frac{1}{n_2 \hat{p}(1-\hat{p})}}
\end{eqnarray*}\]</span>
So, a <span class="math inline">\((1-\alpha)100\%\)</span> CI for the <span class="math inline">\(\ln(OR)\)</span> is:
<span class="math display">\[\begin{eqnarray*}
\ln(\hat{OR}) \pm z_{1-\alpha/2} SE(\ln(\hat{OR}))
\end{eqnarray*}\]</span>
Which gives a <span class="math inline">\((1-\alpha)100\%\)</span> CI for the <span class="math inline">\(OR\)</span>:
<span class="math display">\[\begin{eqnarray*}
(e^{\ln(OR) - z_{1-\alpha/2} SE(\ln(OR))}, e^{\ln(OR) + z_{1-\alpha/2} SE(\ln(OR))})
\end{eqnarray*}\]</span>
Back to the example… OR = 28.9.
<span class="math display">\[\begin{eqnarray*}
SE(\ln(\hat{OR})) &amp;=&amp; \sqrt{\frac{1}{182*0.67*(1-0.67)} + \frac{1}{122*0.0656*(1-0.0656)}}\\
&amp;=&amp; 0.398\\
90\% \mbox{ CI for } \ln(OR) &amp;&amp; \ln(28.9) \pm 1.645 \cdot 0.398\\
&amp;&amp; 3.366 \pm 1.645 \cdot 0.398\\
&amp;&amp; (2.71, 4.02)\\
90\% \mbox{ CI for } OR &amp;&amp; (e^{2.71}, e^{4.02})\\
&amp;&amp; (15.04, 55.47)\\
\end{eqnarray*}\]</span>
<p>We are 90% confident that the true <span class="math inline">\(\ln(OR)\)</span> is between 2.71 and 4.02. We are 90% confident that the true <span class="math inline">\(OR\)</span> is between 15.04 and 55.47. That is, the true odds of getting lung cancer if you smoke are somewhere between 15.04 and 55.47 times higher than if you don’t smoke, with 90% confidence.</p>
<p>Note 1: we use the theory which allows us to understand the sampling distribution for the <span class="math inline">\(\ln(\hat{OR}).\)</span> We use the <em>process</em> for creating CIs to transform back to <span class="math inline">\(OR\)</span>.</p>
<p>Note 2: We do not use the t-distribution here because we are not estimating the population standard deviation.</p>
<p>Note 3: There are not good general guidelines for checking whether the sample sizes are large enough for the normal approximation. Most authorities agree that one can get away with smaller sample sizes here than for the differences of two proportions. If the sample sizes pass the rough check discussed for <span class="math inline">\(\chi^2\)</span>, they should be large enough to support inferences based on the approximate normality of the log of the estimated odds ratio, too. <span class="citation">(Ramsey and Schafer <a href="#ref-sleuth">2012</a>, 541)</span></p>
<p>For the normal approximation to hold, we need the expected counts in each cell to be at least 5. <span class="citation">(Pagano and Gauvreau <a href="#ref-pagano">2000</a>, 355)</span></p>
<p>Note 4: If any of the cells are zero, many people will add 0.5 to that cell’s observed value.</p>
<p>Note 5: The OR will always be more extreme than the RR (one more reason to be careful…)</p>
<span class="math display">\[\begin{eqnarray*}
\mbox{assume } &amp;&amp; \frac{X_1 / n_1}{X_2 / n_2} = RR &gt; 1\\
&amp; &amp; \\
\frac{X_1}{n_1} &amp;=&amp; RR \ \ \frac{X_2}{n_2}\\
\frac{X_1}{n_1 - X_1} &amp;=&amp; RR \ \ \bigg( \frac{n_1}{n_2}  \frac{n_2 - X_2}{n_1 - X_1} \bigg) \frac{X_2}{n_2-X_2}\\
OR &amp;=&amp; RR \ \ \bigg(\frac{n_1}{n_2} \bigg) \frac{n_2 - X_2}{n_1 - X_1}\\
 &amp;=&amp; RR \ \ \bigg(\frac{1/n_2}{1/n_1} \bigg) \frac{n_2 - X_2}{n_1 - X_1}\\
 &amp;=&amp; RR  \ \ \frac{1 - X_2/n_2}{1 - X_1/n_1}\\
 &amp; &gt; &amp; RR
\end{eqnarray*}\]</span>
<p>[<span class="math inline">\(1 - \frac{X_2}{n_2} &gt; 1 - \frac{X_1}{n_1} \rightarrow \frac{1 - \frac{X_2}{n_2}}{1 - \frac{X_1}{n_1}} &gt; 1\)</span>]</p>
<p>Note 6: <span class="math inline">\(RR \approx OR\)</span> if RR is very small (the denominator of the OR will be very similar to the denominator of the RR).</p>
</div>
</div>
<div id="studies" class="section level2">
<h2><span class="header-section-number">4.5</span> Types of Studies (section 6.9)</h2>

<div class="definition">
<span id="def:unnamed-chunk-13" class="definition"><strong>Definition 4.5  </strong></span><strong>Explanatory variable</strong> is one that is a potential explanation for any changes in the response variable.
</div>


<div class="definition">
<span id="def:unnamed-chunk-14" class="definition"><strong>Definition 4.6  </strong></span><strong>Response variable</strong> is the measured outcome of interest.
</div>


<div class="definition">
<span id="def:unnamed-chunk-15" class="definition"><strong>Definition 4.7  </strong></span><strong>Case-control study:</strong> identify observational units by response
</div>


<div class="definition">
<span id="def:unnamed-chunk-16" class="definition"><strong>Definition 4.8  </strong></span><strong>Cohort study:</strong> identify observational units by explanatory variable
</div>


<div class="definition">
<span id="def:unnamed-chunk-17" class="definition"><strong>Definition 4.9  </strong></span><strong>Cross-classification study:</strong> identify observational units regardless of levels of the variable.
</div>

<div id="retrospective-versus-prospective-studies" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Retrospective versus Prospective Studies</h3>
<p>After much research (and asking many people who do not all agree!), I finally came across a definition of retrospective that I like. Note, however, that many many books <em>define</em> retrospective as synonymous with case-control. That is, they define a retrospective study to be one in which the observational units were chosen based on their status of the response variable. I disagree with that definition. As you see below, retrospective studies are defined based on the when the variables were <em>measured</em>. I’ve also given a quote from the Kuiper text where retrospective is defined as any study where historic data are collected (I like this definition less).</p>

<div class="definition">
<span id="def:unnamed-chunk-18" class="definition"><strong>Definition 4.10  </strong></span>A <strong>prospective</strong> study watches for outcomes, such as the development of a disease, during the study period. The explanatory variables are <em>measured</em> before the response variable occurs.
</div>


<div class="definition">
<span id="def:unnamed-chunk-19" class="definition"><strong>Definition 4.11  </strong></span>A <strong>retrospective</strong> study looks backwards and examines exposures to suspected risk or protection factors in relation to an outcome that is established at the start of the study. The explanatory variables are <em>measured</em> after the response has happened.
</div>

<blockquote>
<p>Studies can be classified further as either prospective or retrospective. We define a prospective study as one in which exposure and covariate measurements are made before the cases of illness occur. In a retrospective study these measurements are made after the cases have already occurred… Early writers referred to cohort studies as prospective studies and to case-control studies as retrospective studies because cohort studies usually begin with identification of the exposure status and then measure disease occurrence, whereas case-control studies usually begin by identifying cases and controls and then measure exposure status. The terms prospective and retrospective, however, are more usefully employed to describe the <strong>timing of disease occurrence with respect to exposure measurement</strong>. For example, case-control studies can be either prospective or retrospective. A prospective case-control study uses exposure measurements taken before disease, whereas a retrospective case-control study uses measurements taken after disease. <span class="citation">(Rothman and Greenland <a href="#ref-modepi">1998</a>, 74)</span></p>
</blockquote>
<blockquote>
<p>Retrospective cohort studies also exist. In these designs past (medical) records are often used to collect data. As with prospective cohort studies, the objective is still to first establish groups based on an explanatory variable. However since these are past records the response variable can be collected at the same time. <span class="citation">(Kuiper and Sklar <a href="#ref-KuiperSklar">2013</a>, chap. 6, page 24)</span></p>
</blockquote>
<p>Understanding if a study is retrospective or prospective leads to having a sense of the biases within a study.</p>
<ul>
<li>The retrospective aspect may introduce selection bias and misclassification or information bias. With retrospective studies, the temporal relationship is frequently difficult to assess.</li>
</ul>
<div id="disadvantages-of-prospective-cohort-studies" class="section level4 unnumbered">
<h4>Disadvantages of Prospective Cohort Studies</h4>
<ul>
<li>You may have to follow large numbers of subjects for a long time.<br />
</li>
<li>They can be very expensive and time consuming.<br />
</li>
<li>They are not good for rare diseases.<br />
</li>
<li>They are not good for diseases with a long latency.<br />
</li>
<li>Differential loss to follow up can introduce bias.</li>
</ul>
</div>
<div id="disadvantages-of-retrospective-cohort-studies" class="section level4 unnumbered">
<h4>Disadvantages of Retrospective Cohort Studies</h4>
<ul>
<li>As with prospective cohort studies, they are not good for very rare diseases.</li>
<li>If one uses records that were not designed for the study, the available data may be of poor quality.</li>
<li>There is frequently an absence of data on potential confounding factors if the data was recorded in the past.</li>
<li>It may be difficult to identify an appropriate exposed cohort and an appropriate comparison group.</li>
<li>Differential losses to follow up can also bias retrospective cohort studies.</li>
</ul>
<p>Disadvantages from: <a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_CohortStudies/EP713_CohortStudies5.html" class="uri">http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_CohortStudies/EP713_CohortStudies5.html</a></p>
</div>
<div id="examples-of-studies" class="section level4 unnumbered">
<h4>Examples of studies:</h4>
<ul>
<li>cross-classification, prospective: NHANES<br />
</li>
<li>cross-classification, retrospective: death records (if exposure is measured post-hoc)<br />
</li>
<li>case-control, prospective: the investigator still <em>enrolls</em> based on outcome status, but the investigator must wait to the cases to occur<br />
</li>
<li>case-control, retrospective: at the start of the study, all cases have already occurred and the investigator goes back to measure the exposure (explanatory) variable<br />
</li>
<li>cohort, retrospective: the exposure and outcomes have already happened (i.e., death records)<br />
</li>
<li>cohort, prospective: follows the selected participants to assess the proportion who develop the disease of interest</li>
</ul>
</div>
<div id="which-test-section-6.1" class="section level4 unnumbered">
<h4>Which test? (section 6.1)</h4>
<p>It turns out that the tests above (independence, homogeneity of proportions, homogeneity of odds) are typically equivalent with respect to their conclusions. However, they each have particular assumptions about what they are testing, but that we can generally use any of them for our hypotheses of interest. However, we need to be very careful about our <strong>interpretations</strong>!</p>
<p>(no goodness of fit, section 6.11)</p>
</div>
</div>
</div>
<div id="r-example-categorical-data-botox-and-back-pain" class="section level2">
<h2><span class="header-section-number">4.6</span> R Example (categorical data): Botox and back pain</h2>
<div id="entering-and-visualizing-the-data" class="section level3">
<h3><span class="header-section-number">4.6.1</span> Entering and visualizing the data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">backpain &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">14</span>,<span class="dv">9</span>,<span class="dv">6</span>),<span class="dt">ncol=</span><span class="dv">2</span>,<span class="dt">byrow=</span>F)
backpain</code></pre></div>
<pre><code>##      [,1] [,2]
## [1,]    2    9
## [2,]   14    6</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">backpain.bp &lt;-<span class="st"> </span><span class="kw">barplot</span>(backpain, <span class="dt">plot=</span><span class="ot">FALSE</span>)
<span class="kw">barplot</span>(backpain, <span class="dt">names.arg=</span><span class="kw">c</span>(<span class="st">&quot;placebo&quot;</span>,<span class="st">&quot;botulinum toxin A&quot;</span>))
<span class="kw">text</span>(backpain.bp, <span class="kw">c</span>(backpain[<span class="dv">1</span>,]<span class="op">-</span><span class="dv">1</span>,backpain[<span class="dv">2</span>,]<span class="op">+</span>backpain[<span class="dv">1</span>,]<span class="op">-</span><span class="dv">1</span>),<span class="kw">t</span>(backpain))</code></pre></div>
<p><img src="04-cat_files/figure-html/unnamed-chunk-20-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="fishers-exact-test" class="section level3">
<h3><span class="header-section-number">4.6.2</span> Fisher’s Exact Test</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fisher.test</span>(backpain)</code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  backpain
## p-value = 0.009147
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.008475855 0.710710371
## sample estimates:
## odds ratio 
##  0.1040127</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># their CI is an inversion of the HT</span>
<span class="co"># an approximate SE for the ln(OR) is given by:</span>
se.lnOR &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span>(<span class="dv">16</span><span class="op">*</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">16</span>)<span class="op">*</span>(<span class="dv">14</span><span class="op">/</span><span class="dv">16</span>)) <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">15</span><span class="op">*</span>(<span class="dv">9</span><span class="op">/</span><span class="dv">15</span>)<span class="op">*</span>(<span class="dv">6</span><span class="op">/</span><span class="dv">15</span>)))
se.lnOR</code></pre></div>
<pre><code>## [1] 0.9215239</code></pre>
</div>
<div id="chi-squared-analysis" class="section level3">
<h3><span class="header-section-number">4.6.3</span> Chi-squared Analysis</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(backpain)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  backpain
## X-squared = 5.6964, df = 1, p-value = 0.017</code></pre>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-iscam">
<p>Chance, Beth, and Allan Rossman. 2018. <em>Investigating Statistics, Concepts, Applications, and Methods</em>. 3rd ed. <a href="http://www.rossmanchance.com/iscam3/" class="uri">http://www.rossmanchance.com/iscam3/</a>.</p>
</div>
<div id="ref-botox">
<p>Foster, L., L. Clapp, M. Erickson, and B. Jabbari. 2001. “Botulinum Toxin A and Chronic Low Back Pain: A Randomized, Double-Blind Study.” <em>Neurology</em> 56: 1290–3.</p>
</div>
<div id="ref-bloodtype">
<p>Mourant, A.E., A.c. Kopec, and K. Domaniewsa-Sobczak. 1976. <em>The Distribution of the Human Blood Groups and Other Polymorphisms</em>. Oxford University Press.</p>
</div>
<div id="ref-wynder">
<p>Wynder, E., and E. Graham. 1950. “Tobacco Smoking as a Possible Etiologic Factor in Bronchiogenic Carcinoma.” <em>The Journal of the American Medical Association</em> 143: 329–36.</p>
</div>
<div id="ref-sleuth">
<p>Ramsey, F., and D. Schafer. 2012. <em>The Statistical Sleuth</em>. 3rd ed. Cengage Learning.</p>
</div>
<div id="ref-pagano">
<p>Pagano, M., and K. Gauvreau. 2000. <em>Principles of Biostatistics</em>. 2nd ed. Duxbury Press.</p>
</div>
<div id="ref-modepi">
<p>Rothman, K., and S. Greenland. 1998. <em>Modern Epidemiology</em>. 2nd ed. LWW.</p>
</div>
<div id="ref-KuiperSklar">
<p>Kuiper, Shonda, and Jeff Sklar. 2013. <em>Practicing Statistics</em>. Pearson. <a href="http://web.grinnell.edu/individuals/kuipers/stat2labs/" class="uri">http://web.grinnell.edu/individuals/kuipers/stat2labs/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="SLR.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logistic-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-cat.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Math-150-Notes.pdf", "Math-150-Notes.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
