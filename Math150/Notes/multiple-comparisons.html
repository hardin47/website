<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 8 Multiple Comparisons | Methods in Biostatistics</title>
<meta name="author" content="Jo Hardin">
<meta name="description" content="What is a p-value? The p-value is the probability of collecting the observed data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. The...">
<meta name="generator" content="bookdown 0.31 with bs4_book()">
<meta property="og:title" content="Chapter 8 Multiple Comparisons | Methods in Biostatistics">
<meta property="og:type" content="book">
<meta property="og:description" content="What is a p-value? The p-value is the probability of collecting the observed data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. The...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 8 Multiple Comparisons | Methods in Biostatistics">
<meta name="twitter:description" content="What is a p-value? The p-value is the probability of collecting the observed data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. The...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet">
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><link rel="shortcut icon" href="figs/favicon.ico">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Methods in Biostatistics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Class Information</a></li>
<li><a class="" href="rfunc.html"><span class="header-section-number">1</span> R functions</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">2</span> Introduction</a></li>
<li><a class="" href="t-tests-vs-slr.html"><span class="header-section-number">3</span> t-tests vs SLR</a></li>
<li><a class="" href="SLR.html"><span class="header-section-number">4</span> Simple Linear Regression</a></li>
<li><a class="" href="analysis-of-categorical-data.html"><span class="header-section-number">5</span> Analysis of Categorical Data</a></li>
<li><a class="" href="logistic-regression.html"><span class="header-section-number">6</span> Logistic Regression</a></li>
<li><a class="" href="survival-analysis.html"><span class="header-section-number">7</span> Survival Analysis</a></li>
<li><a class="active" href="multiple-comparisons.html"><span class="header-section-number">8</span> Multiple Comparisons</a></li>
<li><a class="" href="poisson-regression.html"><span class="header-section-number">9</span> Poisson Regression</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/hardin47/website">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multiple-comparisons" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Multiple Comparisons<a class="anchor" aria-label="anchor" href="#multiple-comparisons"><i class="fas fa-link"></i></a>
</h1>
<ul>
<li>What is a p-value?</li>
</ul>
<blockquote>
<p>The <strong>p-value</strong> is the probability of collecting the observed data (or data showing as great or greater difference from the null hypothesis) if the null hypothesis is true. <em>The p-value is a number calculated from the dataset.</em></p>
</blockquote>
<ul>
<li>George Cobb (2014) put the p-value into perspective:</li>
</ul>
<blockquote>
<p>Q: Why do so many colleges and grad schools teach p = .05?<br>
A: Because that’s still what the scientific community and journal editors use.</p>
</blockquote>
<blockquote>
<p>Q: Why do so many people still use p = 0.05?<br>
A: Because that’s what they were taught in college or grad school.</p>
</blockquote>
<ul>
<li>In 2014, the journal <em>Basic and Applied Social Psychology</em> banned the use of all null hypothesis significance testing procedures (NHSTP). What are the <a href="https://www.tandfonline.com/doi/full/10.1080/01973533.2015.1012991">implications for authors</a>?</li>
</ul>
<blockquote>
<p>Question 3. Are any inferential statistical procedures required?<br>
Answer to Question 3. No, because the state of the art remains uncertain. However, BASP will require strong descriptive statistics, including effect sizes. We also encourage the presentation of frequency or distributional data when this is feasible. Finally, we encourage the use of larger sample sizes than is typical in much psychology research, because as the sample size increases, descriptive statistics become increasingly stable and sampling error is less of a problem. However, we will stop short of requiring particular sample sizes, because it is possible to imagine circumstances where more typical sample sizes might be justifiable.</p>
</blockquote>
<ul>
<li>The American Statistical Association put out a <a href="https://amstat.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108">statement on p-values</a>
</li>
</ul>
<ol style="list-style-type: decimal">
<li>P-values can indicate how incompatible the data are with a specified statistical model.<br>
</li>
<li>P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.<br>
</li>
<li>Scientific conclusions and business or policy decisions should not be based only on whether a p- value passes a specific threshold.<br>
</li>
<li>Proper inference requires full reporting and transparency.<br>
</li>
<li>A p-value, or statistical significance, does not measure the size of an effect or the importance of a result.<br>
</li>
<li>By itself, a p-value does not provide a good measure of evidence regarding a model or hypothesis.</li>
</ol>
<ul>
<li>Other interested parties weigh in:
<ul>
<li>
<a href="http://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503">Statisticians issue warning over misuse of P values</a> (Nature, March 7, 2016)</li>
<li>
<a href="https://richarddmorey.medium.com/the-value-of-p-212bcfb1ed66">The value of p</a> (Richard Morey, blog entry, Jan 3, 2021)</li>
</ul>
</li>
</ul>
<div id="Ioannidis" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> Why Most Published Research Findings are False<a class="anchor" aria-label="anchor" href="#Ioannidis"><i class="fas fa-link"></i></a>
</h2>
<p>The Ioannidis article <span class="citation">(<a href="references.html#ref-Ioannidis" role="doc-biblioref">Ioannidis 2005</a>)</span>, and our related discussion, focuses on multiple testing. We’d like to understand the effect of testing in three different contexts:</p>
<ol style="list-style-type: decimal">
<li>When looking for as many possible significant findings as possible (publish or perish)</li>
<li>When bias exists in our work</li>
<li>When (many) researchers study the same effect</li>
</ol>
<div id="definitions" class="section level4" number="8.1.0.1">
<h4>
<span class="header-section-number">8.1.0.1</span> Definitions<a class="anchor" aria-label="anchor" href="#definitions"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>
<strong>R</strong>
<span class="math display">\[\begin{eqnarray*}
R = \frac{ \mbox{# true relationships}}{\mbox{# null relationships}} \ \ \ \mbox{ in the population}
\end{eqnarray*}\]</span>
</li>
<li>
<strong>TRUE</strong>
<span class="math display">\[\begin{eqnarray*}
P(\mbox{study is true}) &amp;=&amp; \frac{ \mbox{# true relationships}}{\mbox{# total}}\\
&amp;=&amp; \frac{ \mbox{# true relationships}}{\mbox{# true + # null}}\\
&amp;=&amp; \frac{ \mbox{R(# null relationships)}}{\mbox{R (# null) + # null}}\\
&amp;=&amp; \frac{R}{R+1}
\end{eqnarray*}\]</span>
</li>
<li>
<strong>size</strong>
<span class="math display">\[\begin{eqnarray*}
\alpha &amp;=&amp; P(\mbox{type I error})\\
&amp;=&amp; P(\mbox{reject } H_0 | H_0 \mbox{ true})\\
\end{eqnarray*}\]</span>
</li>
<li>
<strong>power</strong>
<span class="math display">\[\begin{eqnarray*}
1 - \beta &amp;=&amp; P(\mbox{reject } H_0 | H_0 \mbox{ false})\\
\beta &amp;=&amp; P(\mbox{type II error})\\
&amp;=&amp; P(\mbox{not reject } H_0 | H_0 \mbox{ false})\\
\end{eqnarray*}\]</span>
</li>
<li>
<strong>tests</strong>
<span class="math display">\[\begin{eqnarray*}
c &amp;=&amp; \mbox{# of tests run}
\end{eqnarray*}\]</span>
</li>
</ul>
<p>Baseball power simulation applet <span class="citation">(<a href="references.html#ref-iscam" role="doc-biblioref">Chance and Rossman 2018</a>)</span> <a href="http://www.rossmanchance.com/applets/power.html" class="uri">http://www.rossmanchance.com/applets/power.html</a></p>
</div>
<div id="positive-predictive-value-ppv" class="section level3" number="8.1.1">
<h3>
<span class="header-section-number">8.1.1</span> Positive Predictive Value (PPV)<a class="anchor" aria-label="anchor" href="#positive-predictive-value-ppv"><i class="fas fa-link"></i></a>
</h3>
<p>We’ll focus here on the Positive Predictive Value (PPV). That is, what is the probability that the positive result you found is actually true?</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="19%">
<col width="24%">
<col width="22%">
<col width="33%">
</colgroup>
<thead><tr class="header">
<th>Research Finding</th>
<th>Yes</th>
<th>No</th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Yes</td>
<td><span class="math inline">\(c(1-\beta)R / (R+1)\)</span></td>
<td><span class="math inline">\(c \alpha / (R+1)\)</span></td>
<td><span class="math inline">\(c(R+\alpha - \beta R)/(R+1)\)</span></td>
</tr>
<tr class="even">
<td>No</td>
<td><span class="math inline">\(c \beta R / (R+1)\)</span></td>
<td><span class="math inline">\(c(1-\alpha)/(R+1)\)</span></td>
<td><span class="math inline">\(c(1-\alpha + \beta R)/(R+1)\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(cR/(R+1)\)</span></td>
<td><span class="math inline">\(c/(R+1)\)</span></td>
<td><span class="math inline">\(c\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math display">\[\begin{eqnarray*}
PPV &amp;=&amp; \frac{c(1-\beta)R / (R+1)}{c(1-\beta)R / (R+1) + c \alpha / (R+1)}\\
&amp;=&amp; \frac{c(1-\beta)R}{c(1-\beta)R + c \alpha}\\
&amp;=&amp; \frac{(1-\beta)R}{(1-\beta)R +  \alpha}\\
&amp;=&amp; \frac{1}{1 + \alpha / (1-\beta) R}\\
&amp;&amp; \\
PPV &amp;&gt;&amp; 0.5 \mbox{ more likely true}\\
\mbox{iff   } (1-\beta)R &amp;&gt;&amp; (R-\beta R + \alpha) 0.5\\
(1-\beta) R 0.5 &amp;&gt;&amp; \alpha 0.5\\
(1-\beta) R &amp;&gt;&amp; \alpha \\
&amp;&amp; \\
PPV &amp;&lt;&amp; 0.5  \mbox{ more likely false}\\
\mbox{iff   } (1-\beta) R &amp;&lt;&amp; \alpha \\
\end{eqnarray*}\]</span></p>
<div id="how-do-ppv-and-fdr-relate-to-type-1-and-type-2-errors" class="section level4 unnumbered">
<h4>How do PPV and FDR relate to type 1 and type 2 errors?<a class="anchor" aria-label="anchor" href="#how-do-ppv-and-fdr-relate-to-type-1-and-type-2-errors"><i class="fas fa-link"></i></a>
</h4>
<p>First, remember that conditional probabilities cannot be inverted. Here is an illustrative example:</p>
<p><span class="math display">\[P(\mbox{play in the NBA} \ | \ \mbox{taller than 6'}) \ne P(\mbox{taller than 6'} \ | \ \mbox{play in the NBA})\]</span></p>
<p>If you are taller than 6’, it is still <strong>extremely unlikely</strong> that you play basketball in the NBA. If you are in the NBA however, is is <strong>extremely likely</strong> that you are taller than 6’. As mentioned above, conditional probabilities cannot be reversed because they have fundamentally different meanings.</p>
<p>Notice that FDR (defined in more detail below) can be thought of as reaching into the sea of all significant studies and pulling out a null study. PPV can be though of as reaching into the sea of all significant studies and pulling out a true study.</p>
<p><span class="math display">\[
\begin{align*}
PPV &amp;= P(\mbox{true} \ | \ \mbox{sig}) \ &amp;  \ \mbox{power} &amp;= P(\mbox{sig} \ | \ \mbox{true})\\
FDR &amp;= P(\mbox{null} \ | \ \mbox{sig}) \ &amp; \ P(\mbox{type 1 error}) &amp;= P(\mbox{sig} \ | \ \mbox{null})
\end{align*}
\]</span></p>
</div>
</div>
<div id="bias" class="section level3" number="8.1.2">
<h3>
<span class="header-section-number">8.1.2</span> Bias<a class="anchor" aria-label="anchor" href="#bias"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>bias</strong>
<span class="math display">\[\begin{eqnarray*}
u &amp;=&amp; \mbox{proportion of results that would not have been research findings but ended up}\\
&amp;&amp; \mbox{reported as such because of bias}
\end{eqnarray*}\]</span>
</li>
</ul>
<p>Note: bias simply moves <span class="math inline">\(u\)</span>% of the findings from the <code>No</code> row to the <code>Yes</code> row.</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="12%">
<col width="25%">
<col width="25%">
<col width="37%">
</colgroup>
<thead><tr class="header">
<th>Research Finding</th>
<th>Yes</th>
<th>No</th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Yes</td>
<td><span class="math inline">\([c(1-\beta)R +uc \beta R] / (R+1)\)</span></td>
<td><span class="math inline">\([c \alpha + u c(1-\alpha)] / (R+1)\)</span></td>
<td><span class="math inline">\(c[R+\alpha - \beta R + u(1-\alpha + \beta R)]/(R+1)\)</span></td>
</tr>
<tr class="even">
<td>No</td>
<td><span class="math inline">\((1-u)c \beta R / (R+1)\)</span></td>
<td><span class="math inline">\((1-u)c(1-\alpha)/(R+1)\)</span></td>
<td><span class="math inline">\(c(1-u)(1-\alpha + \beta R)/(R+1)\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(cR/(R+1)\)</span></td>
<td><span class="math inline">\(c/(R+1)\)</span></td>
<td><span class="math inline">\(c\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math display">\[\begin{eqnarray*}
PPV &amp;=&amp; \frac{[c(1-\beta)R +uc \beta R] / (R+1)}{c[R+\alpha - \beta R + u(1-\alpha + \beta R)]/(R+1)}\\
&amp;=&amp; \frac{[(1-\beta)R +u \beta R]}{[R+\alpha - \beta R + u(1-\alpha + \beta R)]}\\
&amp;=&amp; \frac{1}{1 + \frac{\alpha + u(1-\alpha)}{(1-\beta)R + u\beta R}}
\end{eqnarray*}\]</span></p>
<p>Note that <span class="math inline">\(PPV \uparrow\)</span> as <span class="math inline">\(u \uparrow\)</span> as long as <span class="math inline">\((1-\beta) \leq \alpha.\)</span> Or really, it is probably easier to think about if <span class="math inline">\((1-\beta) &gt; \alpha,\)</span> then <span class="math inline">\(PPV \uparrow\)</span> as <span class="math inline">\(u \downarrow.\)</span> The second sentence is more realistic, e.g., <span class="math inline">\(\beta &lt; .95\)</span> means that we have more true results in our list of significant findings as the bias goes down. [To understand the direction of the relationships, find <span class="math inline">\(\partial PPV / \partial u &lt; 0\)</span> if <span class="math inline">\((1-\beta) &gt; \alpha\)</span> (decreasing with u).]</p>
</div>
<div id="multiple-studies" class="section level3" number="8.1.3">
<h3>
<span class="header-section-number">8.1.3</span> Multiple Studies<a class="anchor" aria-label="anchor" href="#multiple-studies"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(\alpha\)</span>
<ul>
<li>If a study is null, the probability of seeing null is <span class="math inline">\((1-\alpha)\)</span>
</li>
<li>If 3 of us test the same thing, the probability that we will all see null is <span class="math inline">\((1-\alpha)^3\)</span>
</li>
<li>
<em>and</em> the probability that at least one of use will see significance goes from <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(1 - (1-\alpha)^3\)</span>
</li>
<li>As <span class="math inline">\(n \uparrow\)</span> someone will definitely see significance (bad!)</li>
</ul>
</li>
<li>
<span class="math inline">\(\beta\)</span>
<ul>
<li>If a study is significant, the probability of seeing null is <span class="math inline">\(\beta\)</span>
</li>
<li>If 3 of us test the same thing, the probability that we’ll all see null is <span class="math inline">\(\beta^3\)</span>
</li>
<li>
<em>and</em> the probability that at least one of us will see significance goes from <span class="math inline">\((1-\beta)\)</span> to <span class="math inline">\((1-\beta^3)\)</span>
</li>
<li>As <span class="math inline">\(n \uparrow,\)</span> someone will definitely see significance (good!)</li>
</ul>
</li>
</ul>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="15%">
<col width="21%">
<col width="26%">
<col width="35%">
</colgroup>
<thead><tr class="header">
<th>Research Finding</th>
<th>Yes</th>
<th>No</th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Yes</td>
<td><span class="math inline">\(c(1-\beta^n)R / (R+1)\)</span></td>
<td><span class="math inline">\(c(1-[1- \alpha]^n) / (R+1)\)</span></td>
<td><span class="math inline">\(c(R+1-[1-\alpha]^n - \beta^n R)/(R+1)\)</span></td>
</tr>
<tr class="even">
<td>No</td>
<td><span class="math inline">\(c \beta^n R / (R+1)\)</span></td>
<td><span class="math inline">\(c(1-\alpha)^n/(R+1)\)</span></td>
<td><span class="math inline">\(c([1-\alpha]^n + \beta^n R)/(R+1)\)</span></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(cR/(R+1)\)</span></td>
<td><span class="math inline">\(c/(R+1)\)</span></td>
<td><span class="math inline">\(c\)</span></td>
</tr>
</tbody>
</table></div>
<p><span class="math display">\[\begin{eqnarray*}
PPV &amp;=&amp; \frac{(1-\beta^n)R}{(R+1-[1-\alpha]^n - \beta^n R)}
\end{eqnarray*}\]</span></p>
<p><span class="math inline">\(PPV \downarrow\)</span> as <span class="math inline">\(n \uparrow\)</span> unless <span class="math inline">\((1-\beta) &lt; \alpha\)</span> that is, <span class="math inline">\(\beta &gt; 0.95\)</span> !! Again, the result showing how PPV changes as a function of <span class="math inline">\(n\)</span> is given by <span class="math inline">\(\partial PPV / \partial n &lt; 0\)</span> if <span class="math inline">\((1-\beta) &gt; \alpha.\)</span></p>
</div>
<div id="corollaries" class="section level3" number="8.1.4">
<h3>
<span class="header-section-number">8.1.4</span> Corollaries<a class="anchor" aria-label="anchor" href="#corollaries"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>Corollary 1</strong> The smaller the studies conducted in a scientific field, the less likely the research findings are to be true. (low power)<br>
</li>
<li>
<strong>Corollary 2</strong> The smaller the effect sizes in a scientific field, the less likely the research findings are to be true. (also low power)<br>
</li>
<li>
<strong>Corollary 3</strong> The greater the number and the lesser the selection of tested relationships in a scientific field, the less likely the research findings are to be true. (pre-study odds: R; phase III trials have better odds than microarray studies)<br>
</li>
<li>
<strong>Corollary 4</strong> The greater the flexibility in designs, definitions, outcomes, and analytical modes in a scientific field, the less likely the research findings are to be true. (increased bias)<br>
</li>
<li>
<strong>Corollary 5</strong> The greater the financial and other interests and prejudices in a scientific field, the less likely the research findings are to be true. (increased bias)</li>
<li>
<strong>Corollary 6</strong> The hotter a scientific field (with more scientific teams involved), the less likely the research findings are to be true. (increased <span class="math inline">\(n)\)</span>
</li>
</ul>
</div>
</div>
<div id="multcomp" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Multiple Comparisons<a class="anchor" aria-label="anchor" href="#multcomp"><i class="fas fa-link"></i></a>
</h2>
<p>As you might expect, if you have 10 groups, <strong>all of which come from the same population</strong>, you might wrongly conclude that some of the means are <em>significantly</em> different. If you try pairwise comparisons on all 10 groups, you’ll have <span class="math inline">\({10 \choose 2} = 45\)</span> comparisons. Out of the 45 CI, you’d expect 2.25 of them to not contain the true difference (of zero); equivalently, you’d expect 2.25 tests to reject the true <span class="math inline">\(H_0.\)</span> In an overall test of comparing all 10 groups simultaneously, you can’t use size as a performance measure anymore.</p>
<div id="familywise-error-rate" class="section level3" number="8.2.1">
<h3>
<span class="header-section-number">8.2.1</span> Familywise Error Rate<a class="anchor" aria-label="anchor" href="#familywise-error-rate"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>FWER</strong> = <span class="math inline">\(P(V\geq1)\)</span>
</li>
</ul>
<p>The Familywise Error Rate (FWER) is the probability of making one or more type I errors in a series of multiple tests. In the example above (with 10 comparisons), you would almost always make at least one type I error if you used a size of <span class="math inline">\(\alpha = 0.05.\)</span> So, your FWER would be close to 1. Methods exist for controlling the FWER instead of the size <span class="math inline">\((\alpha).\)</span></p>
<div id="bonf" class="section level4" number="8.2.1.1">
<h4>
<span class="header-section-number">8.2.1.1</span> Bonferroni<a class="anchor" aria-label="anchor" href="#bonf"><i class="fas fa-link"></i></a>
</h4>
<p>The Bonferroni method of adjusting for multiple comparisons is used to control the FWER.</p>
<p>Assume all our tests are null
<span class="math display">\[\begin{eqnarray*}
A_1 &amp;=&amp; \mbox{event reject test 1}\\
P(A_1) &amp;=&amp; \alpha^*\\
A_2 &amp;=&amp; \mbox{event reject test 2}\\
P(A_2) &amp;=&amp; \alpha^*\\
\end{eqnarray*}\]</span></p>
<p>We want to bound the probability that all our tests don’t commit a type 1 error (that is, none of them reject).
<span class="math display">\[\begin{eqnarray*}
P( A_1 \mbox{ or } A_2) &amp;=&amp; P(A_1) + P(A_2) - P(A_1 \mbox{ and } A_2)\\
&amp;=&amp; \alpha^* + \alpha^* - ???\\
&amp; \leq&amp; 2 \alpha^* \\
FWER = P(\mbox{at least one rejects}) &amp;\leq&amp; 2 \alpha^*\\
\mbox{let} &amp;&amp; P(A_1) = P(A_2) = \alpha^* = \frac{\alpha}{2}\\
FWER &amp;\leq&amp; \alpha
\end{eqnarray*}\]</span></p>
<p>That is, with <span class="math inline">\(m\)</span> tests, rejecting any test whose p-value is less than or equal to <span class="math inline">\(\alpha/m\)</span> will control the FWER at <span class="math inline">\(\alpha.\)</span> Alternatively, rejecting adjusted p-values less than <span class="math inline">\(\alpha\)</span> will also control the FWER at <span class="math inline">\(\alpha.\)</span> Where the adjusted p-values are defined as:</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{p}_j = \min(m p_j, 1)
\end{eqnarray*}\]</span>
Reject any hypothesis such that <span class="math inline">\(\tilde{p}_j \leq \alpha.\)</span></p>
</div>
<div id="holm" class="section level4" number="8.2.1.2">
<h4>
<span class="header-section-number">8.2.1.2</span> Holm<a class="anchor" aria-label="anchor" href="#holm"><i class="fas fa-link"></i></a>
</h4>
<p>Holm is a less conservative (than Bonferroni) method for controlling the FWER. Because each p-value is considered sequentially, the Holm adjustment is referred to as “step-up”. The intuition is that on the first step, the bound assumes up to <span class="math inline">\(m\)</span> null hypotheses. But on the second step, there are only <span class="math inline">\(m-1\)</span> null hypotheses (assuming the first p-value is rejected).</p>
<p>To control the FWER at <span class="math inline">\(\alpha,\)</span> first order the p-values as <span class="math inline">\(p_1 \leq p_2 \leq \cdots \leq p_m\)</span> and define the adjusted p-values to be:</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{p}_j = \max_{i \leq j} [ \min((m-i+1) p_i, 1) ]
\end{eqnarray*}\]</span>
Reject any hypothesis such that <span class="math inline">\(\tilde{p}_j \leq \alpha.\)</span></p>
<p>Why do we need the <span class="math inline">\(\max\)</span> in the p-value adjustment? Let’s say we have 7 p-values and the top 3 are 0.001, 0.0011, and 0.0012 (in order). To adjust them with only the multiplicative factor, we’d get: 0.007, 0.0066, and 0.006. The p-values were close enough to each other (to start with) that the descending multiplicative factor <strong>reversed</strong> the order of significance! And we’re trying to control how often we make Type 1 errors in the entire report, we aren’t trying to re-consider which test is more or less significant. The original information (about order of significance) is the order we should stick to. With the three values above, after applying the <span class="math inline">\(\max\)</span> function, the three adjusted p-values will all be 0.007.</p>
<ul>
<li>Note that interpretations depend heavily on the number of tests (which is true about most multiple comparison adjustments).<br>
</li>
<li>Bonferroni is extremely conservative, and therefore it has very high type II error rates (low power).<br>
</li>
<li>We aren’t really interested in the situation where “all true null tests don’t get rejected.”</li>
</ul>
</div>
</div>
<div id="FDR" class="section level3" number="8.2.2">
<h3>
<span class="header-section-number">8.2.2</span> False Discovery Rate<a class="anchor" aria-label="anchor" href="#FDR"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<strong>FDR</strong> = <span class="math inline">\(E[V/R]\)</span>
</li>
</ul>
<p>The FDR (false discovery rate) is the expected proportion of false discoveries out of all the discoveries.</p>
<ul>
<li>
<strong>PPV</strong> = <span class="math inline">\(E[S/R]\)</span>
</li>
</ul>
<p>Consider <span class="math inline">\(m\)</span> hypotheses:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th></th>
<th align="center">Null True</th>
<th align="center">Alt True</th>
<th align="center"></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Research</td>
<td>not significant</td>
<td align="center"><span class="math inline">\(U\)</span></td>
<td align="center"><span class="math inline">\(T\)</span></td>
<td align="center"><span class="math inline">\(m-R\)</span></td>
</tr>
<tr class="even">
<td>Finding</td>
<td>significant</td>
<td align="center"><span class="math inline">\(V\)</span></td>
<td align="center"><span class="math inline">\(S\)</span></td>
<td align="center"><span class="math inline">\(R\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td align="center"><span class="math inline">\(m_0\)</span></td>
<td align="center"><span class="math inline">\(m-m_0\)</span></td>
<td align="center"><span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table></div>
<div id="BH" class="section level4" number="8.2.2.1">
<h4>
<span class="header-section-number">8.2.2.1</span> Benjamini-Hochberg<a class="anchor" aria-label="anchor" href="#BH"><i class="fas fa-link"></i></a>
</h4>
<p>When running experiments that have many many tests of significance, it is often more desirable to worry about the <em>number</em> of false discoveries as opposed to the probability of getting <em>one</em> false discovery. That is, we are typically comfortable with a few false positives (i.e., a very HIGH FWER) as long as the rate of false positives is low.</p>
<p>Define the estimated false discovery proportion at a cutoff t <span class="math inline">\((\hat{FDR}(t))\)</span> to be the number of false discoveries at a given cutoff. Again, to control the FDR at <span class="math inline">\(\alpha,\)</span> first order the p-values as <span class="math inline">\(p_1 \leq p_2 \leq \cdots \leq p_m.\)</span></p>
<p><span class="math display">\[\begin{eqnarray*}
\hat{FDR}(t)&amp;=&amp; \frac{\# \{ p_j \leq t \mbox{ out of the null tests } \}}{ \# \{p_j \leq t\} + (1 \mbox{ if all } p_j &gt; t)}
\end{eqnarray*}\]</span>
Notice that p-values from null hypotheses will be distributed uniformly from zero to one. That means that a good estimate of the numerator is <span class="math inline">\(t\cdot\)</span> # of null tests.
<span class="math display">\[\begin{eqnarray*}
\hat{FDR}(p_j) &amp;=&amp; \frac{p_j \cdot m \cdot \pi_0}{j} &lt; \frac{p_j m}{j}
\end{eqnarray*}\]</span>
where <span class="math inline">\(\pi_0\)</span> is the proportion of tests which are truly null (<span class="math inline">\(m_0/m).\)</span> Consider the adjusted p-values,</p>
<p><span class="math display">\[\begin{eqnarray*}
\tilde{p}_j = \min [ (m/j) p_j, \tilde{p}_{j+1} ]
\end{eqnarray*}\]</span>
Reject any hypothesis such that <span class="math inline">\(\tilde{p}_j \leq \delta\)</span> to control the FDR at <span class="math inline">\(\delta.\)</span></p>
<p>Intuition: let <span class="math inline">\(m=1000,\)</span> and suppose the <span class="math inline">\(\tilde{p}_{250} &lt; 0.4.\)</span> Show that FDR <span class="math inline">\(&lt; 0.4.\)</span></p>
<p><span class="math inline">\(\tilde{p}_{250} &lt; 0.4\)</span> implies two different things:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_{250} \leq \frac{0.4\cdot 250}{1000} = 0.1\)</span></li>
<li>and <span class="math inline">\(\approx 750\)</span> null tests have p-values between 0.1 and 1.</li>
</ol>
<p>If 750 null tests have p-values between 0.1 and 1, then <span class="math inline">\(m_0 \cdot 0.9 = 750 \rightarrow m_0 = 833.33.\)</span> Therefore, the number of null hypotheses which were rejected is 833.33 - 750 = 83.33.</p>
<p><span class="math display">\[\begin{eqnarray*}
FDR = \frac{83.33}{250} = 0.33 &lt; 0.4
\end{eqnarray*}\]</span></p>
<p>More generally if <span class="math inline">\(\tilde{p}_k = \frac{p_k \cdot m}{k} \leq \delta\)</span>:
<span class="math display">\[\begin{eqnarray*}
p_k &amp;&lt;&amp; \frac{k}{m} \delta\\
\# \mbox{ null not rejected} &amp;=&amp; \mbox{length of null interval }\bigg[\frac{k}{m}\delta,1\bigg] \cdot \mbox{ total } \# \mbox{ of null}\\
(m-k) &amp;=&amp; \bigg(1-\frac{k}{m} \delta\bigg) m_0\\
m_0 &amp;=&amp; \frac{(m-k) m}{(m-k \delta)}\\
&amp; &amp;\\
FDR &amp;=&amp; \frac{\# \mbox{ null rejected}}{\# \mbox{ rejected}}\\
&amp;=&amp; \frac{\frac{m(m-k)}{(m-k\delta)} - (m-k)}{k}\\
&amp;=&amp; \frac{m^2 - mk - (m-k) (m-k \delta)}{(m-k \delta)k}\\
&amp;=&amp; \frac{k(m \delta - k \delta)}{k(m-k \delta)} = \delta\bigg(\frac{m-k}{m/\delta - k}\bigg)\\
FDR &amp;&lt;&amp; \delta
\end{eqnarray*}\]</span>
because <span class="math inline">\(m/\delta &gt; m\)</span> so <span class="math inline">\(\frac{m-k}{m/\delta -k} &lt; 1.\)</span></p>
<p>Consider a one sample t-test. The population is normal centered at 47 with <span class="math inline">\(\sigma=3\)</span>; samples of size 20 are taken from the population. The following hypotheses are tested:
<span class="math display">\[\begin{eqnarray*}
H_0: \mu = 47\\
H_{0_1}: \mu = 40\\
H_{0_2}: \mu = 48\\
H_{0_3}: \mu = 50\\
\end{eqnarray*}\]</span>
In the null setting, the p-values are uniformly distributed from 0 to 1. When the data are not consistent with the null, there are more p-values close to zero (and even closer to zero as the data become more and more distinct from the null).</p>
</div>
<div id="distribution-of-p-values-under-different-amounts-of-divergence-from-the-null-hypothesis." class="section level4 unnumbered">
<h4>Distribution of p-values under different amounts of divergence from the null hypothesis.<a class="anchor" aria-label="anchor" href="#distribution-of-p-values-under-different-amounts-of-divergence-from-the-null-hypothesis."><i class="fas fa-link"></i></a>
</h4>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="07-MC_files/figure-html/unnamed-chunk-2-1.png" alt="All simulations come from the same population.  In the first histogram, the null and alternative hypotheses are the same (i.e., the null hypothesis is true).  In the other three histograms, the null varies (i.e., the null hypothesis is not true).  Each simulation is run with the same true alternative, HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-2-2.png" alt="All simulations come from the same population.  In the first histogram, the null and alternative hypotheses are the same (i.e., the null hypothesis is true).  In the other three histograms, the null varies (i.e., the null hypothesis is not true).  Each simulation is run with the same true alternative, HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-2-3.png" alt="All simulations come from the same population.  In the first histogram, the null and alternative hypotheses are the same (i.e., the null hypothesis is true).  In the other three histograms, the null varies (i.e., the null hypothesis is not true).  Each simulation is run with the same true alternative, HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-2-4.png" alt="All simulations come from the same population.  In the first histogram, the null and alternative hypotheses are the same (i.e., the null hypothesis is true).  In the other three histograms, the null varies (i.e., the null hypothesis is not true).  Each simulation is run with the same true alternative, HA: mu = 47." width="47%"><p class="caption">
Figure 1.2: All simulations come from the same population. In the first histogram, the null and alternative hypotheses are the same (i.e., the null hypothesis is true). In the other three histograms, the null varies (i.e., the null hypothesis is not true). Each simulation is run with the same true alternative, HA: mu = 47.
</p>
</div>
<p>The previous example considered the situation where all the p-values came from the distribution. In reality, p-values will come from many different distributions. For simplification, consider the situation where g100% of the tests come from the null distribution, and (1-g)100% of the tests come from a distribution where the p-values are much closer to one and skewed right.</p>
</div>
<div id="distribution-of-p-values-under-differing-proportions-of-null-versus-significant-tests." class="section level4 unnumbered">
<h4>Distribution of p-values under differing proportions of null versus significant tests.<a class="anchor" aria-label="anchor" href="#distribution-of-p-values-under-differing-proportions-of-null-versus-significant-tests."><i class="fas fa-link"></i></a>
</h4>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="07-MC_files/figure-html/unnamed-chunk-3-1.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-2.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-3.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-4.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-5.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-6.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-7.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-8.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-9.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-10.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-11.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><img src="07-MC_files/figure-html/unnamed-chunk-3-12.png" alt="Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47." width="47%"><p class="caption">
Figure 2.1: Varying the proportion of null genes and the value of the parameter being tested in the null hypothesis. The null hypothesis varies, with the alternative set at HA: mu = 47.
</p>
</div>
</div>
<div id="qvals" class="section level4" number="8.2.2.2">
<h4>
<span class="header-section-number">8.2.2.2</span> Storey &amp; q-values<a class="anchor" aria-label="anchor" href="#qvals"><i class="fas fa-link"></i></a>
</h4>
<p>The previous methods allowed the larger family of tests to control either the FWER or the FDR (global measures of accuracy). However, for a given test, there was no measure to quantify FDR for a given test.</p>
<ul>
<li>
<strong>p-value</strong> Recall that the p-value is the smallest level of significance (P(type I error)) possible to reject <span class="math inline">\(H_0.\)</span>
</li>
<li>
<strong>q-value</strong> Akin to the p-value, the q-value is the minimum FDR at that score which can be attained when calling the test significant.</li>
</ul>
<p>Storey defines the q-value to be the FDR associated with a given test of significance. For example, say a q-value = 0.013 for test X. Then at most 1.3% of tests with p-values at least as small as test X are false positives. In particular, let
<span class="math display">\[\begin{eqnarray*}
\hat{\pi}_0 = \frac{\# \{ p_j &gt; \lambda \} }{m(1-\lambda)} \ \ \  \mbox{ for some } \lambda
\end{eqnarray*}\]</span>
(though there are many ways to estimate <span class="math inline">\(\pi_0.)\)</span></p>
<p>In a step-down algorithm, the q-value is defined using the p-value at hand and the next <em>largest</em> p-value. Additionally, the <span class="math inline">\(\pi_0\)</span> is implemented as seen in the original intuition behind FDR control.</p>
<ul>
<li>
<strong>step 1</strong> let
<span class="math display">\[\begin{eqnarray*}
\hat{q}(p_{m}) = \hat{\pi}_0 p_{m}
\end{eqnarray*}\]</span>
Note that <span class="math inline">\(\hat{q}(p_{m})\)</span> is the biggest q-value and <span class="math inline">\(p_{m}\)</span> is the biggest p-value.</li>
<li>
<strong>step 2</strong>
<span class="math display">\[\begin{eqnarray*}
\hat{q}(p_{m-1}) = \min( \hat{\pi}_0 p_{m-1} \frac{m}{m-1}, \hat{q}( p_{m}))
\end{eqnarray*}\]</span>
If <span class="math inline">\(\hat{q}(p_{m}) = 0.7\)</span> and <span class="math inline">\(\hat{\pi}_0 \cdot p_{m-1} \cdot \frac{m}{m-1} = 0.8,\)</span> then the next smallest q-value would be 0.7 because the FDR can be as low as 0.7 (see the definition of FDR).</li>
<li>
<strong>step 3</strong> more generally,
<span class="math display">\[\begin{eqnarray*}
\hat{q}(p_{j}) = \min( \hat{\pi}_0 p_{j} \frac{m}{j}, \hat{q}( p_{j+1}))
\end{eqnarray*}\]</span>
</li>
</ul>
<p>Can a q-value be less than a p-value? Sure! If the number of null hypotheses is small and the test is powerful. For example, consider testing 1000 hypotheses with 20% null tests <span class="math inline">\((\pi_0=0.2).\)</span> Assume 500 of the p-values are less than 0.05 (very powerful!). With 200 null we would expect 10 to be less than 0.05. So, the FDR is 10/500 = 0.02 (which is smaller than the p-value of 0.05 at the cutoff for the <span class="math inline">\(500^{th}\)</span> test).</p>
</div>
<div id="how-are-fwer-and-fdr-related" class="section level4 unnumbered">
<h4>How are FWER and FDR related?<a class="anchor" aria-label="anchor" href="#how-are-fwer-and-fdr-related"><i class="fas fa-link"></i></a>
</h4>
<p>First note that for both FDR and FWER, the procedure is to <em>control</em> the errors not to compute or estimate the errors (except in the case of the q-value).</p>
<p>Recall that <span class="math inline">\(FDR = E[V/R]\)</span> and is defined to equal zero if R=0</p>
<p><span class="math display">\[\begin{eqnarray*}
FDR &amp;=&amp; E\bigg[\frac{V}{R} \bigg]\\
&amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0 \bigg] P(R &gt;0) + E\bigg[\frac{V}{R} | R=0 \bigg] P(R=0)\\
&amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0 \bigg] P(R &gt;0)
\end{eqnarray*}\]</span></p>
<ul>
<li>
<strong>case 1</strong> In the first case, consider <span class="math inline">\(V=R\)</span> such that all significant hypotheses are null.
<span class="math display">\[\begin{eqnarray*}
FDR &amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0 \bigg] P(R &gt;0)\\
&amp;=&amp; 1 P(R&gt;0) = 1 P(V \geq 1)\\
&amp;=&amp; FWER
\end{eqnarray*}\]</span>
</li>
<li>
<strong>case 2</strong> In the second case, consider <span class="math inline">\(V &lt; R\)</span> such that some of the significant hypotheses are null and some are not. <span class="math inline">\((V/R &lt; 1)\)</span>
<span class="math display">\[\begin{eqnarray*}
FDR &amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0 \bigg] P(R &gt;0)\\
&amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0, V\geq 1 \bigg] P(R &gt;0, V \geq 1) + E\bigg[\frac{V}{R} | R &gt; 0, V=0 \bigg] P(R &gt;0, V =0)\\
&amp;&amp; \mbox{ (note: } V/R \equiv 0 \mbox{ if } V = 0)\\
&amp;=&amp; E\bigg[\frac{V}{R} | R &gt; 0, V\geq 1 \bigg] P(R &gt;0, V \geq 1) \\
&amp;&lt;&amp; P(V \geq 1) = FWER \ \ \mbox{ because } V/R &lt; 1
\end{eqnarray*}\]</span>
</li>
</ul>
<p>The proof above shows that FWER controls the FDR when not all significant tests are null. When all significant tests are null, FWER=FDR.</p>
</div>
</div>
</div>
<div id="interim" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> Interim Analyses<a class="anchor" aria-label="anchor" href="#interim"><i class="fas fa-link"></i></a>
</h2>
<p>An important application of multiple comparisons issues comes when deciding whether or not to stop a clinical trial early due to either positive or negative results. Looking at the data too often will create false positives which can be quite problematic. <span class="citation">(<a href="references.html#ref-subgroup" role="doc-biblioref">Schulz and Grimes 2005</a>)</span></p>
<p>Consider the following case studies:</p>
<ul>
<li>HIV – Indinavir was stopped early due to positive results <span class="citation">(<a href="references.html#ref-hiv" role="doc-biblioref">Scott M. Hammer et al. 1997</a>)</span>
</li>
<li>HVTN 505 was stopped early due to negative results <span class="citation">(<a href="references.html#ref-hvtn" role="doc-biblioref">Yunda Huang et al. 2015</a>)</span>
</li>
<li>Truvada &amp; Tenofovir were also stopped early <span class="citation">(<a href="references.html#ref-hrt" role="doc-biblioref">Dyer 2004</a>)</span>
</li>
</ul>
<p>What happens when the research performs <span class="math inline">\(k\)</span> interim analyses and the research scenario is truly null?
<span class="math display">\[\begin{eqnarray*}
P( test1 &lt; \alpha \mbox{ or } test2 &lt; \alpha \mbox{ or } \ldots \mbox{ or }  testk &lt; \alpha ) &gt; \alpha
\end{eqnarray*}\]</span></p>
<p>The researcher has two options:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\alpha^* &lt; &lt; &lt; \alpha\)</span>
</li>
<li>Change <span class="math inline">\(\alpha\)</span> at each step along the way so that the total probability of a type I error is <span class="math inline">\(\alpha\)</span>
</li>
</ol>
<div id="different-p-value-cutoff-choices" class="section level3" number="8.3.1">
<h3>
<span class="header-section-number">8.3.1</span> Different p-value cutoff choices<a class="anchor" aria-label="anchor" href="#different-p-value-cutoff-choices"><i class="fas fa-link"></i></a>
</h3>
<div class="figure">
<img src="alphaInterim.png" alt=""><p class="caption">Different <span class="math inline">\(\alpha^*\)</span> values for three different stopping criteria. Note that Peto does not control the test at an overall <span class="math inline">\(\alpha=0.05,\)</span> although it is close.</p>
</div>
<p>Sometimes the interim criteria are referred to as “alpha spending” because the total type I error rate (that is, rejecting the original claim at any of the interim analysis steps if <span class="math inline">\(H_0\)</span> is true) should be <span class="math inline">\(\alpha,\)</span> but the <span class="math inline">\(\alpha\)</span> probability of a type I error should be spread over the interim tests.</p>
<div id="pocock" class="section level4 unnumbered">
<h4>Pocock<a class="anchor" aria-label="anchor" href="#pocock"><i class="fas fa-link"></i></a>
</h4>
<p>The Pocock boundary</p>
<p>Advantages:</p>
<ul>
<li>simple</li>
<li>aggressive with respect to stopping early. that is, there is a small expected sample size (when the effect size is large)</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>low power and therefore large maximum sample size (when the effect size is small)</li>
</ul>
</div>
<div id="obrien-fleming" class="section level4 unnumbered">
<h4>O’Brien-Fleming<a class="anchor" aria-label="anchor" href="#obrien-fleming"><i class="fas fa-link"></i></a>
</h4>
<p>Advantages:</p>
<ul>
<li>final <span class="math inline">\(\alpha\)</span> is close to desired <span class="math inline">\(\alpha\)</span>
</li>
<li>more power than Pocock, so smaller max sample size</li>
</ul>
<p>Disadvantage:</p>
<ul>
<li>less likely to stop early, so larger expected sample size</li>
</ul>
</div>
</div>
<div id="simulation-1" class="section level3" number="8.3.2">
<h3>
<span class="header-section-number">8.3.2</span> Simulation<a class="anchor" aria-label="anchor" href="#simulation-1"><i class="fas fa-link"></i></a>
</h3>
<p>The simulation below is based on a pair of blogs by Ed Berry (<a href="https://www.eddjberry.com/post/simulating-a-b-tests-with-data-table/">here</a> and <a href="https://www.eddjberry.com/post/intro-group-sequential-designs/">here</a>).</p>
<p>Every simulation is done under the null hypothesis. That is, there are no effects to find. We are worried about the situation when the null hypothesis is rejected (at any of the time points).</p>
<ul>
<li><p>Each of the following is done 1000 times (there are 1000 lines created, each of the plots only has 100 lines so that we can see the lines more clearly).</p></li>
<li><p>For week 1, generate 500 people (250 in each group) and measure whether or not to reject <span class="math inline">\(H_0.\)</span> Note that we shouldn’t be rejecting because there is no treatment effect!</p></li>
<li><p>For week 2, 500 more people are generated. Also for weeks 3 and 4.</p></li>
<li><p>The line is colored purple if the study is rejected at any point. Note that in actual studies, if the study is rejected it would not (typically) be continued. I did continue for weeks past where the rejection would have happened (only because of simulation and graphing simplicity).</p></li>
</ul>
<p>The simulated type 1 error rate is the number of studies that rejected at any of the weeks (out of 1000).</p>
<div id="raw-p-values" class="section level4 unnumbered">
<h4>Raw p-values<a class="anchor" aria-label="anchor" href="#raw-p-values"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the horizontal black line is at 0.05. The simulated type 1 error rate (proportion of purple lines out of 1000 lines) is:</p>
<pre><code>#&gt; # A tibble: 1 × 1
#&gt;   type1
#&gt;   &lt;dbl&gt;
#&gt; 1 0.131</code></pre>
<div class="inline-figure"><img src="07-MC_files/figure-html/unnamed-chunk-6-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="bonferroni" class="section level4 unnumbered">
<h4>Bonferroni<a class="anchor" aria-label="anchor" href="#bonferroni"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the horizontal black line is at 0.05/4 = 0.0125; there is a gray horizontal line at 0.05. The simulated type 1 error rate (proportion of purple lines out of 1000 lines) is:</p>
<pre><code>#&gt; # A tibble: 1 × 1
#&gt;   type1
#&gt;   &lt;dbl&gt;
#&gt; 1 0.027</code></pre>
<div class="inline-figure"><img src="07-MC_files/figure-html/unnamed-chunk-7-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="pocock-1" class="section level4 unnumbered">
<h4>Pocock<a class="anchor" aria-label="anchor" href="#pocock-1"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the horizontal black line is at 0.018; there is a gray horizontal line at 0.05. The simulated type 1 error rate (proportion of purple lines out of 1000 lines) is:</p>
<pre><code>#&gt; # A tibble: 1 × 1
#&gt;   type1
#&gt;   &lt;dbl&gt;
#&gt; 1 0.048</code></pre>
<div class="inline-figure"><img src="07-MC_files/figure-html/unnamed-chunk-8-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="peto" class="section level4 unnumbered">
<h4>Peto<a class="anchor" aria-label="anchor" href="#peto"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the horizontal black line is at 0.001 for the first three weeks and 0.05 for the fourth week. There is a gray horizontal line at 0.05. The simulated type 1 error rate (proportion of purple lines out of 1000 lines) is (note that we expect the type 1 error rate to be above 5%):</p>
<pre><code>#&gt; # A tibble: 1 × 1
#&gt;   type1
#&gt;   &lt;dbl&gt;
#&gt; 1 0.054</code></pre>
<div class="inline-figure"><img src="07-MC_files/figure-html/unnamed-chunk-9-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="obrien-fleming-1" class="section level4 unnumbered">
<h4>O’Brien-Fleming<a class="anchor" aria-label="anchor" href="#obrien-fleming-1"><i class="fas fa-link"></i></a>
</h4>
<p>Note that the horizontal black lines are at 0.0001, 0.004, 0.019, and 0.043. There is a gray horizontal line at 0.05. The simulated type 1 error rate (proportion of purple lines out of 1000 lines) is:</p>
<pre><code>#&gt; # A tibble: 1 × 1
#&gt;   type1
#&gt;   &lt;dbl&gt;
#&gt; 1 0.049</code></pre>
<div class="inline-figure"><img src="07-MC_files/figure-html/unnamed-chunk-10-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="some-parting-thoughts" class="section level3" number="8.3.3">
<h3>
<span class="header-section-number">8.3.3</span> Some parting thoughts<a class="anchor" aria-label="anchor" href="#some-parting-thoughts"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>If <span class="math inline">\(H_A\)</span> is true, the effect size is likely overestimated (true in general, not just for interim analyses).</li>
<li>Symmetry: which is more important, harm or good?</li>
<li>Can split alpha into two halves and apply one method (e.g., Pocock) to stopping early for positive reasons and apply another method (e.g., O’Brien-Fleming) to stopping early for negative reasons.</li>
</ul>
<div id="advice-1" class="section level4" number="8.3.3.1">
<h4>
<span class="header-section-number">8.3.3.1</span> Advice 1<a class="anchor" aria-label="anchor" href="#advice-1"><i class="fas fa-link"></i></a>
</h4>
<p>The following quote is general advice from statistical researchers doing clinical oncology. <span class="citation">(<a href="references.html#ref-crowley" role="doc-biblioref">Green, Benedetti, and Crowley 1997</a>)</span></p>
<blockquote>
<p>Even the specifics of the most basic task of the data monitoring committee, evaluation of interim results for evidence of benefit or harm, are not necessarily obvious. Questions (and our personal answers) include:</p>
</blockquote>
<blockquote>
<ul>
<li>How often should the data monitoring committee review interim data? (The answer to this should depend on how fast additional information becomes available on a trial. we generally recommend monitoring advanced disease studies, or any other study with rapidly accumulating events, every 6 months. Yearly monitoring may be sufficient for adjuvant or slowly accruing studies.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Should the primary outcome data be reviewed each time or should they be reviewed only at times of planned interim analyses? (All data, including primary outcome data, should be reviewed at each time, since the unexpected does occur.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Should treatment arms be blinded to the data monitoring committee or not? (Definitely not. If A looks better than B, the decision to continue could well be different if A is the control arm instead of the experimental arm.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Should a data monitoring committee decision that evidence is sufficient to close a trail be final, or should it be advisory only? (We would say advisory, but rarely overturned.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>If advisory, advisory to whom - the funding agency? an executive group? the investigators? (Reports should go to the individuals with ultimate responsibility for the integrity of the trial.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Should a data monitoring committee be able to make major design changes to a trial? (No, the data monitoring committee may offer suggestions but design is the responsibility of the principal investigators. On the other hand, major design changes initiated by the principal investigators should be approved by the committee.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Are data monitoring committee duties over when study accrual is complete, or should the data monitoring committee also decide when results are to be reported? (It should also decide when results are to be reported - additional follow-up generates additional data that still need to be monitored.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>How much weight should be accorded to outside information versus current information on the study being monitored? (Definitive outside information cannot be ignored - but this begs the question of what is definitive. A single trial of moderate size probably is not definitive; two large trials probably are; a meta-analysis probably is not.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>How much should results of secondary endpoints influence the decision to continue or not? (Not much unless toxic death is considered secondary.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>How scary do results have to be to stop at a time other than a planned interim analysis? (Very scary, or the purpose of interim analyses is defeated.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>When do accrual problems justify early closure? (When results won’t be available until after they are no longer of interest.)</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Should confidential information ever be provided to other data monitoring committees or planning groups? (Sometimes. If study conduct will not be compromised by limited release of information, it might be reasonable to let investigators planning new trials know of potential problems or benefits to treatment arms they are considering. Risk to the ongoing trial includes leaked information or intelligent guesses as to the current status; risk to the new trial includes choosing an inappropriate arm based on early results that don’t hold up.)</li>
</ul>
</blockquote>
<blockquote>
<p>Every monitoring committee functions differently because no two people have the same ethical, scientific, or practical perspectives. This means different committees might well come up with different answers to the same monitoring issues. To ensure some balance of opinions, it is best to have a variety of knowledgeable people as members of the committee.</p>
</blockquote>
</div>
<div id="advice-2" class="section level4" number="8.3.3.2">
<h4>
<span class="header-section-number">8.3.3.2</span> Advice 2<a class="anchor" aria-label="anchor" href="#advice-2"><i class="fas fa-link"></i></a>
</h4>
<p>Duncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, <em>Training Students to Extract Value from Big Data</em>. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. <a href="http://www.nap.edu/catalog.php?record_id=18981" class="uri">http://www.nap.edu/catalog.php?record_id=18981</a></p>
<p><strong>Principles for the Data Science Process</strong>, Duncan Temple Lang, University of California, Davis <span class="citation">(<a href="references.html#ref-duncanTL" role="doc-biblioref"><em>National Research Council: Training Students to Extract Value from Big Data</em> 2014</a>)</span></p>
<p>Duncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science.</p>
<p>Temple Lang then described the data analysis pipeline, outlining the steps in one example of a
data analysis and exploration process:</p>
<ol style="list-style-type: decimal">
<li><p>Asking a general question.</p></li>
<li><p>Refining the question, identifying data, and understanding data and metadata. Temple Lang
noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood.</p></li>
<li><p>Access to data. This is unrelated to the science but does require computational skill.</p></li>
<li><p>Transforming to data structures.</p></li>
<li><p>Exploratory data analyses to understand the data and determine whether the results will scale.
This is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data.</p></li>
<li><p>Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate
this step.</p></li>
<li><p>Modeling and estimation. Temple Lang noted that computer and machine learning scientists
tend to focus more on predictive models than on modeling of physical behavior or characteristics.</p></li>
<li><p>Diagnostics. This helps to understand how well the model fits the data and identifies
anomalies and aspects for further study. This step has similarities to exploratory data analysis.</p></li>
<li><p>Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical
techniques is important for understanding and interpreting models and results.</p></li>
<li><p>Conveying results.</p></li>
</ol>
<p>Temple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student:</p>
<ul>
<li>Mapping the general question to a statistical framework.</li>
<li>Understanding the scope of inference, sampling, biases, and limitations.</li>
<li>Exploratory data analyses, including missing values, data quality, cleaning, matching, and
fusing.</li>
<li>Understanding randomness, variability, and uncertainty. Temple Lang noted that many
students do not understand sampling variability.</li>
<li>Conditional dependence and heterogeneity.</li>
<li>Dimension reduction, variable selection, and sparsity.</li>
<li>Spurious relationships and multiple testing.</li>
<li>Parameter estimation versus “black box” prediction and classification.</li>
<li>Diagnostics, residuals, and comparing models.</li>
<li>Quantifying the uncertainty of a model.</li>
<li>Sampling structure and dependence for data reduction. Temple Lang noted that modeling of
data becomes complicated when variables are not independent, identically distributed.</li>
<li>Statistical accuracy versus computational complexity and efficiency.</li>
</ul>
<p>Temple Lang then briefly discussed some of the practical aspects of computing, including the
following:</p>
<ul>
<li>Accessing data.</li>
<li>Manipulating raw data.</li>
<li>Data structures and storage, including correlated data.</li>
<li>Visualization at all stages (particularly in exploratory data analyses and conveying the
results).</li>
<li>Parallel computing, which can be challenging for a new student.</li>
<li>Translating high-level descriptions to optimal programs.</li>
</ul>
<p>During the discussion, Temple Lang proposed computing statistics on visualizations to examine
data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals.</p>
<p>(More information about scagnostics can be found in <span class="citation">Wilkinson and Wills (<a href="references.html#ref-scagnostics" role="doc-biblioref">2007</a>)</span>.)</p>
<p>A workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets.</p>

</div>
</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="survival-analysis.html"><span class="header-section-number">7</span> Survival Analysis</a></div>
<div class="next"><a href="poisson-regression.html"><span class="header-section-number">9</span> Poisson Regression</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple-comparisons"><span class="header-section-number">8</span> Multiple Comparisons</a></li>
<li>
<a class="nav-link" href="#Ioannidis"><span class="header-section-number">8.1</span> Why Most Published Research Findings are False</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#positive-predictive-value-ppv"><span class="header-section-number">8.1.1</span> Positive Predictive Value (PPV)</a></li>
<li><a class="nav-link" href="#bias"><span class="header-section-number">8.1.2</span> Bias</a></li>
<li><a class="nav-link" href="#multiple-studies"><span class="header-section-number">8.1.3</span> Multiple Studies</a></li>
<li><a class="nav-link" href="#corollaries"><span class="header-section-number">8.1.4</span> Corollaries</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#multcomp"><span class="header-section-number">8.2</span> Multiple Comparisons</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#familywise-error-rate"><span class="header-section-number">8.2.1</span> Familywise Error Rate</a></li>
<li><a class="nav-link" href="#FDR"><span class="header-section-number">8.2.2</span> False Discovery Rate</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#interim"><span class="header-section-number">8.3</span> Interim Analyses</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#different-p-value-cutoff-choices"><span class="header-section-number">8.3.1</span> Different p-value cutoff choices</a></li>
<li><a class="nav-link" href="#simulation-1"><span class="header-section-number">8.3.2</span> Simulation</a></li>
<li><a class="nav-link" href="#some-parting-thoughts"><span class="header-section-number">8.3.3</span> Some parting thoughts</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/hardin47/website/blob/master/07-MC.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/hardin47/website/edit/master/07-MC.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Methods in Biostatistics</strong>" was written by Jo Hardin. It was last built on 2023-01-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
