[
["index.html", "Methods in Biostatistics Class Information", " Methods in Biostatistics Jo Hardin 2019-02-27 Class Information Class notes for Math 150 at Pomona College: Methods in Biostatistics. The notes are based primarily on the text Practicing Statistics, (Kuiper and Sklar 2013). You are responsible for reading your text. Your text is very good &amp; readable, so you should use it. Your text is not, however, overly technical. You should make sure you are coming to class and also reading the materials associated with the activities. Week Topic Book Chapter Notes Section 1/23/19 t-tests / SLR / Intro to R 2 1, 2.1, 2.2 1/28/19 SLR 2 3 1/30/19 3.5, 4.1 2/4/19 Contingency Analysis 6 4.2, 4.4 2/6/19 4.5 2/11/19 Contingency Analysis 6 4.4 2/13/19 4.3 2/18/19 Logistic Regression 7 5.1 2/20/19 5.2, 5.3 2/25/19 Logistic Regression 7 5.4, 5.5 2/27/19 5.6 3/4/19 Logistic Regression 7 5.7 3/6/19 5.8 3/11/19 Catch-up / Review 3/13/19 Midterm (Wednesday) (2, 6, 7) 3/25/19 Survival Analysis 9 4/1/19 Survival Analysis 9 4/8/19 Survival Analysis 9 4/15/19 Survival Analysis 9 4/22/19 Ioannidis &amp; mult. compar. handouts &amp; 1.13 4/29/19 Poisson Regression 8 5/6/19 Poisson Regression 8 References "],
["intro.html", "Chapter 1 Introduction 1.1 Course Goals", " Chapter 1 Introduction 1.1 Course Goals Our goals in this course are: to better evaluate quantitative information with regards to clinical and biological data. We’ll be sure to keep in mind: Careful presentation of data Consideration of variability Meaningful comparisons to be able to critically evaluate the medical literature with respect to design, analysis, and interpretation of results. to understand the role of inherent variability and keep it in perspective when inferring results to a population. to critically evaluate medical results given in the mainstream media. to read published studies with skepticism. Some people (in all fields!) wrongly believe that all studies published in a peer review publication must be 100% accurate and/or well designed studies. In this course, you will learn the tools to recognize, interpret, and critique statistical results in medical literature. Probability vs. Statistics Experimental Design In this class we’ll talk about techniques used to analyze data from medical studies. Along with the computational methods, however, we’ll continue to think about issues of experimental design and interpretation. Descriptive statistics describe the sample at hand with no intent on making generalizations. Inferential statistics use a sample to make claims about a population Simple Random Sample is an unbiased sample. Sample is selected in such a way that every possible sample of size \\(n\\) is equally likely. Blind / double blind when the patient and/or doctor do not know which patient is receiving which treatment. Placebo mock treatment Sample size reduces variability (large samples make small effects easier to discern) Experiment vs. Observational Study whether the treatment was assigned by the researchers; randomized experiments make concluding causation possible Funding of study goals, bias "],
["t-tests-vs-slr.html", "Chapter 2 t-tests vs. SLR 2.1 t-test (book: 2.1) ANOVA 2.2 Simple Linear Regression (book: 2.3) 2.3 Confidence Intervals (section 2.11) 2.4 Random Sample vs. Random allocation", " Chapter 2 t-tests vs. SLR We are going to build on a very basic model of the following form: data = deterministic model + random error planned variability your experimental conditions, hopefully represented by an interesting deterministic model random error natural variability due to individuals. systematic error error that is not contained within the model. It can happen because of poor sampling or poor experimental conditions. Surgery Timing The study, “Operation Timing and 30-Day Mortality After Elective General Surgery”, tested the hypotheses that the risk of 30-day mortality associated with elective general surgery: 1) increases from morning to evening throughout the routine workday; 2) increases from Monday to Friday through the workweek; and 3) is more frequent in July and August than during other months of the year. As a presumed negative control, the investigators also evaluated mortality as a function of the phase of the moon. Secondarily, they evaluated these hypotheses as they pertain to a composite in-hospital morbidity endpoint. The related data set contains 32,001 elective general surgical patients. Age, gender, race, BMI, several comorbidities, several surgical risk indices, the surgical timing predictors (hour, day of week, month,moon phase) and the outcomes (30-day mortality and in-hosptial complication) are provided. The dataset is cleaned and complete (no missing data except for BMI). There are no outliers or data problems. The data are from (Sessler et al. 2011) Note that in the example, mortality rates are compared for patients electing to have surgery in July vs August. We’d like to compare the average age of the participants from the July group to the August group. Even if the mortality difference is significant, we can’t conclude causation because it was an observational study. However, the more similar the groups are based on clinical variables, the more likely any differences in mortality are due to timing. How different are the groups based on clinical variables? surgeryurl &lt;- url(&quot;https://www.causeweb.org/tshs/datasets/surgery_timing.Rdata&quot;) load(surgeryurl) surgery &lt;- stata_data surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% dplyr::group_by(month) %&gt;% dplyr::summarize(agemean = mean(age, na.rm=TRUE), agesd = sd(age, na.rm=TRUE), agen = sum(!is.na(age))) ## # A tibble: 2 x 4 ## month agemean agesd agen ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aug 58.1 15.2 3176 ## 2 Jul 57.6 15.5 2325 2.1 t-test (book: 2.1) A t-test is a test of means. For the surgery timing data, the groups would ideally have similar age distributions. Why? What are the advantages and disadvantages of running a retrospective cohort study? The two-sample t-test starts with the assumption that the population means of the two groups are equal, \\(H_0: \\mu_1 = \\mu_2\\). The sample means \\(\\overline{y}_1\\) and \\(\\overline{y}_2\\) will always be different. How different must the \\(\\overline{y}\\) values be in order to reject the null hypothesis? Definition 2.1 (Model 1) \\[\\begin{align} y_{1j} &amp;= \\mu_{1} + \\epsilon_{1j} \\ \\ \\ \\ j=1, 2, \\ldots, n_1\\\\ y_{2j} &amp;= \\mu_{2} + \\epsilon_{2j} \\ \\ \\ \\ j=1, 2, \\ldots, n_2\\\\ \\epsilon_{ij} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\mu_i \\end{align}\\] That is, we are assuming that for each group the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean. Note that we have assumed that the variances of the two groups are equal. We have also assumed that there is independence between and within the groups. Note: we will assume the population variances are equal if neither sample variance is more than twice as big as the other. Example1 Are the mean ages of the July vs August patients statistically different? (why two sided?) \\[\\begin{align} H_0: \\mu_1 = \\mu_2\\\\ H_1: \\mu_1 \\ne \\mu_2 \\end{align}\\] \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - 0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\\\ s_p &amp;= \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 -2}}\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;\\\\ t &amp;= \\frac{(58.05 - 57.57) - 0}{15.34 \\sqrt{\\frac{1}{3176} + \\frac{1}{2325}}}\\\\ &amp;= 1.15\\\\ s_p &amp;= \\sqrt{ \\frac{(3176-1)15.22^2 + (2325-1) 15.5^2}{3176 + 2325 -2}}\\\\ &amp;= 15.34\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;= 5499\\\\ \\mbox{p-value} &amp;= 2 \\cdot (1-pt(1.15,5499)) = 0.25\\\\ \\end{align}\\] The same analysis can be done in R (with and without tydying the output): surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) ## ## Welch Two Sample t-test ## ## data: age by month ## t = 1.1585, df = 4954.5, p-value = 0.2467 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3366687 1.3092918 ## sample estimates: ## mean in group Aug mean in group Jul ## 58.05220 57.56589 surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) %&gt;% tidy() ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.486 58.1 57.6 1.16 0.247 4954. -0.337 ## # … with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, ## # alternative &lt;chr&gt; Look at SD and SEM What is the statistic? What is the sampling distribution of the statistic? Why do we use the t-distribution? Why is the big p-value important? (It’s a good thing!) How do we interpret the p-value? What can we conclude? applet from (Chance and Rossman 2018): [http://www.rossmanchance.com/applets/SampleMeans/SampleMeans.html] What are the model assumptions? (independence between &amp; within groups, random sample, pop values don’t change, additive error, \\(\\epsilon_{i,j} \\ \\sim \\ iid \\ N(0, \\sigma^2)\\), … basically all the assumptions are given in the original linear model) Considerations when running a t-test: * one-sample t-test * one-sided vs. two-sided hypotheses * t-test with unequal variance (less powerful, more conservative) \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - (\\mu_1 - \\mu_2)}{ \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\\\ df &amp;= \\min(n_1-1, n_2-1)\\\\ \\end{align}\\] two dependent (paired) samples – one sample t-test! Example 2 Assume we have two very small samples: (\\(y_{11}=3, y_{12} = 9, y_{21} = 5, y_{22}=1, y_{23}=9\\)). Find \\(\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\epsilon}_{11}, \\hat{\\epsilon}_{12}, \\hat{\\epsilon}_{21}, \\hat{\\epsilon}_{22}, \\hat{\\epsilon}_{23}, n_1, n_2\\). 2.1.1 What is an Alternative Hypothesis? Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: [https://www.youtube.com/watch?v=b-U_I1DCGEY] In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on. What is the point? Why watch the video? How does it relate the to the material from class? What is the relationship to sampling distributions? [Thanks to Ben Baumer at Smith College for the pointer to the specific video.] ANOVA Skip ANOVA in your text (2.4 and part of 2.9) 2.2 Simple Linear Regression (book: 2.3) Simple Linear Regression is a model (hopefully discussed in introductory statistics) used for describing a {linear} relationship between two variables. It typically has the form of: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i = 1, 2, \\ldots, n\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2)\\\\ E(Y|x) &amp;= \\beta_0 + \\beta_1 x \\end{align}\\] For this model, the deterministic component (\\(\\beta_0 + \\beta_1 x\\)) is a linear function of the two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and the explanatory variable \\(x\\). The random error terms, \\(\\epsilon_i\\), are assumed to be independent and to follow a normal distribution with mean 0 and variance \\(\\sigma^2\\). How can we use this model to describe the two sample means case we discussed on the esophageal data? Consider \\(x\\) to be a dummy variable that takes on the value 0 if the observation is a control and 1 if the observation is a case. Assume we have \\(n_1\\) controls and \\(n_2\\) cases. It turns out that, coded in this way, the regression model and the two-sample t-test model are mathematically equivalent! (For the color game, the natural way to code is 1 for the color distracter and 0 for the standard game. Why?) \\[\\begin{align} \\mu_1 &amp;= \\beta_0 + \\beta_1 (0) = \\beta_0 \\\\ \\mu_2 &amp;= \\beta_0 + \\beta_1 (1) = \\beta_0 + \\beta_1\\\\ \\mu_2 - \\mu_1 &amp;= \\beta_1 \\end{align}\\] Why are they the same? \\[\\begin{align} b_1= \\hat{\\beta}_1 &amp;= \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - (\\sum x_i )^2}\\\\ &amp;= \\frac{n \\sum_2 y_i - n_2 \\sum y_i}{(n n_2-n_2^2)}\\\\ &amp;= \\frac{ n \\sum_2 y_i - n_2 (\\sum_1 y_i + \\sum_2 y_i)}{n_2(n-n_2)}\\\\ &amp;= \\frac{(n_1 + n_2) \\sum_2 y_i - n_2 \\sum_1 y_i - n_2 \\sum_2 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 \\sum_2 y_i - n_2 \\sum_1 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 n_2 \\overline{y}_2 - n_2 n_1 \\overline{y}_1}{n_1 n_2}\\\\ &amp;= \\overline{y}_2 - \\overline{y}_1\\\\ b_0 = \\hat{\\beta}_0 &amp;= \\frac{\\sum y_i - b_1 \\sum x_i}{n}\\\\ &amp;= \\frac{\\sum_1 y_i + \\sum_2 y_i - b_1 n_2}{n}\\\\ &amp;= \\frac{n_1 \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1}{n} = \\overline{y}_1 \\end{align}\\] Definition 2.2 (Model 2) \\[\\begin{align} y_{i} &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i=1, 2, \\ldots, n\\\\ \\epsilon_{i} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\beta_0 + \\beta_1 x_i\\\\ \\hat{y}_i &amp;= b_0 + b_1 x_i \\end{align}\\] That is, we are assuming that for each observation the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean at their value for the explanatory variable, \\(x_i\\). Note that we have assumed that the variance is constant across any level of the explanatory variable. We have also assumed that there is independence across individuals. [Note: there are no assumptions about the distribution of the explanatory variable, \\(X\\)]. What are the similarities in the t-test vs. SLR models? predicting average assuming independent, constant errors errors follow a normal distribution with zero mean and variance \\(\\sigma^2\\) What are the differences in the two models? one subscript versus two (or similarly, two models for the t-test) two samples for the t-test (two variables for the regression… or is that a similarity??) both variables are quantitative in SLR 2.3 Confidence Intervals (section 2.11) [http://www.rossmanchance.com/applets/NewConfsim/Confsim.html] In general, the format of a confidence interval is (INTERPRETATION!!!!): estimate +/- critical value x standard error of the estimate Age data: \\[\\begin{align} 90\\% \\mbox{ CI for } \\mu_1: &amp; \\overline{y}_1 \\pm t^*_{3176-1} \\times \\hat{\\sigma}_{\\overline{y}_1}\\\\ &amp; 58.05 \\pm 1.645 \\times 15.22/\\sqrt{3176}\\\\ &amp; (57.61, 58.49)\\\\ 95\\% \\mbox{ CI for }\\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{5499} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 0.48 \\pm 1.96 \\times 0.42\\\\ &amp; (-0.34, 1.30) \\end{align}\\] Note the CI on pgs 54/55, there is a typo. The correct interval for \\(\\mu_1 - \\mu_2\\) for the games data should be: \\[\\begin{align} 95\\% \\mbox{ CI for } \\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} \\hat{\\sigma}_{\\overline{y}_1 - \\overline{y}_2}\\\\ &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 38.1 - 35.55 \\pm 2.02 \\times \\sqrt{\\frac{(19)3.65^2 + (19)3.39^2}{20+20-2}} \\sqrt{\\frac{1}{20} + \\frac{1}{20}}\\\\ &amp; (0.29 s, 4.81 s) \\end{align}\\] 2.4 Random Sample vs. Random allocation Recall what you’ve learned about how good random samples lead to inference about a population. On the other hand, in order to make a causal conclusion, you need a randomized experiment with random allocation of the treatments (impossible to happen in many settings). Random sampling and random allocation are DIFFERENT ideas that should be clear in your mind. Figure taken from (Chance and Rossman 2018) Note: no ANOVA (section 2.4) or normal probability plots (section 2.8) References "],
["SLR.html", "Chapter 3 Simple Linear Regression 3.1 Transformations 3.2 Fitting the regression line 3.3 Correlation 3.4 Errors 3.5 Intervals 3.6 R Example (SLR): Happy Planet", " Chapter 3 Simple Linear Regression Though we’ve discussed the relationship between tests of means and simple linear regression, we will really consider simple linear regression in a much broader context (one where both the explanatory and response variables are quantitative). The data below represents 10 different variables on health of a country measured on 143 countries. Data taken from (R. Lock et al. 2016), originally from the Happy Planet Index Project [http://www.happyplanetindex.org/]. Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. We are going to investigate happiness and life expectancy. 3.1 Transformations Model assumptions The average value for the response variable is a linear function of the explanatory variable. The error terms follow a normal distribution around the linear model. The error terms have a mean of zero. The error terms have a constant variance of \\(\\sigma^2\\). The error terms are independent (and identically distributed). [http://www.rossmanchance.com/applets/RegSim/RegCoeff.html] How do we tell whether the assumptions are met? We can’t always. But it’s good to look at plots: scatter plot, residual plot, histograms of residuals. We denote the residuals for this model as: \\[\\begin{align} r_i = \\hat{e}_i = y_i - \\hat{y}_i \\end{align}\\] Figs 3.13 and 3.15 taken from Kutner et al. (2004) important note!! The idea behind transformations is to make the model as appropriate as possible for the data at hand. We want to find the correct linear model; we want our assumptions to hold. We are not trying to find the most significant model or big \\(R^2\\). See section 2.9 in your text. No normal probability plots (qq-plots); use histograms or boxplots to assess the symmetry and normality of the residuals. 3.2 Fitting the regression line How do we fit a regression line? Find \\(b_0\\) and \\(b_1\\) that minimize the sum of squared distance of the points to the line (called ordinary least squares): \\[\\begin{align} \\min \\sum (y_i \\hat{y}_i)^2 &amp;=&amp; \\min RSS \\mbox{ residual sum of squares}\\\\ RSS &amp;=&amp; \\sum (y_i - b_0 - b_1 x_i)^2\\\\ \\frac{\\partial RSS}{\\partial b_0} = 0\\\\ \\frac{\\partial RSS}{\\partial b_1} = 0\\\\ b_0 &amp;=&amp; \\overline{y} - b_1 \\overline{x}\\\\ b_1 &amp;=&amp; r(x,y) \\frac{s_x}{s_y}\\\\ \\end{align}\\] Is that the only way to find values for \\(b_0\\) and \\(b_1\\)? (absolute distances, maximum likelihood,…) Resistance to outliers? What is \\(\\hat{y}\\) at \\(\\overline{x}\\)? \\[\\begin{align} \\hat{y} &amp;=&amp; b_0 + b_1 \\overline{x}\\\\ &amp;=&amp; \\overline{y} - b_1 \\overline{x} + b_1 \\overline{x}\\\\ &amp;=&amp; \\overline{y} \\end{align}\\] The regression line will always pass through the point \\((\\overline{x}, \\overline{y})\\). Definition 3.1 An estimate is unbiased if, over many repeated samples drawn from the population, the average value of the estimates based on the different samples would equal the population value of the parameter being estimated. That is, a statistic is unbiased if the mean of its sampling distribution is the population parameter. 3.3 Correlation Consider a scatterplot, you’ll have variability in both directions: \\((x_i - \\overline{x}) \\&amp; (y_i - \\overline{y})\\). \\[\\begin{align} \\mbox{sample covariance}&amp;&amp;\\\\ cov(x,y) &amp;=&amp; \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\ \\mbox{sample correlation}&amp;&amp;\\\\ r(x,y) &amp;=&amp; \\frac{cov(x,y)}{s_x s_y}\\\\ &amp;=&amp; \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\ \\mbox{pop cov} &amp;=&amp; \\sigma_{xy}\\\\ \\mbox{pop cor} &amp;=&amp; \\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\\\ \\end{align}\\] \\(-1 \\leq r \\leq 1 \\&amp; -1 \\leq \\rho \\leq 1\\). No Spearman’s rank correlation or Kendall’s \\(\\tau\\). \\(b_1 = r \\frac{s_y}{s_x}\\) if \\(r=0, b_1=0\\) if \\(r=1, b_1 &gt; 0\\) but can be anything! \\(r &lt; 0 \\leftrightarrow b &lt; 0, r &gt; 0 \\leftrightarrow b &gt; 0\\) Recall that \\(R^2\\) is the proportion of variability explained by the line. 3.4 Errors Recall, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). How do we estimate \\(\\sigma^2\\)? \\[\\begin{align} RSS &amp;=&amp; \\sum (y_i - \\hat{y}_i)^2 \\ \\ \\ \\mbox{ residual sum of squares}\\\\ MSS &amp;=&amp; \\sum (\\hat{y}_i - \\overline{y})^2 \\ \\ \\ \\mbox{ model sum of squares}\\\\ TSS &amp;=&amp; \\sum (y_i - \\overline{y})^2 \\ \\ \\ \\mbox{ total sum of squares}\\\\ s_{y|x}^2 &amp;=&amp; \\hat{\\sigma^2} = \\frac{1}{n-2} RSS\\\\ s_x^2 &amp;=&amp; \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2\\\\ s_y^2 &amp;=&amp; \\frac{1}{n-1} \\sum (y_i - \\overline{y})^2\\\\ var(\\epsilon) &amp;=&amp; s_{y|x}^2 = \\frac{RSS}{n-2} = \\frac{\\sum(y_i - \\hat{y}_i)^2}{n-2} = SE(\\epsilon)\\\\ var(b_1) &amp;=&amp; \\frac{s_{y|x}^2}{(n-1) s_x^2}\\\\ SE(b_1) &amp;=&amp; \\frac{s_{y|x}}{\\sqrt{(n-1)} s_x}\\\\ &amp;=&amp; \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(x_i - \\overline{x})^2}} = \\frac{\\sqrt{\\sum(y_i - \\hat{y}_i)^2/(n-2)}}{\\sqrt{\\sum(x_i - \\overline{x})^2}}\\\\ \\end{align}\\] \\(SE(b_1) \\downarrow\\) as \\(\\sigma \\downarrow\\) \\(SE(b_1) \\downarrow\\) as \\(n \\uparrow\\) \\(SE(b_1) \\downarrow\\) as \\(s_x \\uparrow\\) WHY? What do we mean by \\(SE(b_1)\\)? As we saw above, the correlation and the slope estimates are intimately related. They are also both related to the coefficient of determination. \\[\\begin{align} R^2 = r^2 = \\frac{MSS}{TSS} \\end{align}\\] \\(R^2\\) is the proportion of total variability explained by the regression line (the linear relationship between the explanatory and response variables). If \\(x\\) and \\(y\\) are not at all correlated, \\(\\hat{y}_i \\approx \\overline{y}\\), MSS = 0, \\(R^2=0\\). If \\(x\\) and \\(y\\) are perfectly correlated, \\(\\hat{y}_i = y_i\\), MSS=TSS, \\(R^2 = 1\\). 3.4.1 Testing \\(\\beta_1\\) If \\(H_0: \\beta=0\\) is true, then \\[\\begin{align} \\frac{b_1 - 0}{SE(b_1)} \\sim t_{n-2} \\end{align}\\] Note that the degrees of freedom are now \\(n-2\\) because we are estimating two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)). We can also find a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\beta_1\\): \\[\\begin{align} b_1 \\pm t_{\\alpha/2, n-2} SE(b_1) \\end{align}\\] 3.5 Intervals As with anything that has some type of standard error, we can create intervals that give us some confidence in the statements we are making. 3.5.1 Confidence Intervals In general, confidence intervals are of the form: point estimate +/- multiplier * SE(point estimate) 3.5.2 Slope We can create a CI for the slope parameter, \\(\\beta_1\\): \\[\\begin{align} b_1 &amp;\\pm&amp; t_{\\alpha/2,n-2} SE(b_1)\\\\ b_1 &amp;\\pm&amp; t_{\\alpha/2, n-2} \\frac{s_{y|x}}{\\sqrt{(n-1)}s_x}\\\\ 6.693 &amp;\\pm&amp; t_{.025, 141} 0.375\\\\ t_{.025,141} &amp;=&amp; qt(0.025, 141) = -1.977\\\\ \\mbox{CI} &amp;&amp; (5.95 \\mbox{ years/unit of happy}, 7.43 \\mbox{ years/unit of happy}) \\end{align}\\] How can we interpret the CI? Does it make sense to talk about a unit of happiness? 3.5.3 Mean Response We can also create a CI for the mean response, \\(E[Y|x^*] = \\beta_0 + \\beta_1 x^*\\). Note that the standard error of the point estimate (\\(\\hat{y}=b_0 + b_1 x^*\\)) now depends on the variability associated with two things (\\(b_0, b_1\\)). \\[\\begin{align} SE(\\hat{y(x^*)}) &amp;=&amp; \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2}\\\\ SE(\\hat{y}(\\overline{x})) &amp;=&amp; s_{y|x}/\\sqrt{n}\\\\ SE(\\hat{y}(x)) &amp;\\geq&amp; s_{y|x}/\\sqrt{n} \\ \\ \\ \\forall x \\end{align}\\] How would you interpret the associated interval? 3.5.4 Prediction of an Individual Response As should be obvious, predicting an individual is more variable than predicting a mean. \\[\\begin{align} SE(y(x^*)) &amp;=&amp; \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2 + s^2_{y|x}}\\\\ SE(y(x^*)) &amp;=&amp; \\sqrt{ SE(\\hat{y}(x^*))^2 + s^2_{y|x}}\\\\ \\end{align}\\] How would you interpret the associated interval? 3.5.5 Outlying, High Leverage, and Influential Points We are skipping the rest of this section in the notes. You are not responsible for it. Read section 4.7 (no loess, ignore the multiple predictors part, ) Theorem 3.1 High leverage points are x-outliers with the potential to exert undue influence on regression coefficient estimates. Influential points are points that have exerted undue influence on the regression coefficient estimates. Note: typically we think of more data as better; more values will tend to decrease the sampling variability of our statistic. But if I give you a lot more data and put it all at \\(\\overline{x}\\), \\(SE(b_1)\\) stays exactly the same. Why?? Recall \\[\\begin{align} y_{i} &amp;=&amp; \\beta_0 + \\beta_1 x_i \\ \\ \\ \\epsilon_i \\sim N(0,\\sigma^2)\\\\ e_i &amp;=&amp; y_i - \\hat{y}_i \\end{align}\\] We plot \\(e_i\\) versus \\(\\hat{y}_i\\). (Why? Typically, we want the \\(e_i\\) to be constant at each value of \\(x_i\\). Note that \\(\\hat{y}_i\\) is a simple linear transformation of \\(x_i\\), so the plot is identical.) We want to see if the distributions of the residuals is different across the fitted line (we look for patterns).\\ Not all residuals have an equal effect on the regression line!! 3.5.5.1 leverage \\[\\begin{align} h_i = \\frac{1}{n} +\\frac{(x_i - \\overline{x})^2}{\\sum_{j=1}^n (x_j - \\overline{x})^2}\\\\ \\frac{1}{n} \\leq h_i \\leq 1\\\\ \\end{align}\\] Leverage represents the effect of point \\(x_i\\) on the line. We need large leverage for a particular value to have a large effect. Note: \\[\\begin{align} SE(\\hat{y}(x_i)) &amp;=&amp; s_{y|x} \\sqrt{h_i}\\\\ SE(y(x_i)) &amp;=&amp; s_{y|x} \\sqrt{(h_i + 1)}\\\\ SE(e_i) &amp;=&amp; s_{y|x} \\sqrt{(1-h_i)}\\\\ \\hat{y}(x^*) &amp;\\pm&amp; t_{n-2, .025} (s_{y|x} \\sqrt{h(x^*)+1})\\\\ \\end{align}\\] is a 95% prediction interval at \\(x^*\\). High leverage reduces the variability because the line gets pulled toward the point. 3.5.5.2 standardized residuals \\[\\begin{align} \\frac{e_i}{s_{y|x} \\sqrt{1-h_i}} \\sim t_{n-2}\\\\ \\end{align}\\] 3.5.5.3 studentized residuals \\[\\begin{align} \\frac{e_i}{s_{y|x, (i)} \\sqrt{1-h_i}} &amp;\\sim&amp; t_{n-3}\\\\ s_{y|x, (i)} &amp;=&amp; \\frac{1}{n-3} \\sum_{j \\ne i} (y_j - \\hat{y}_{j(i)})^2 \\end{align}\\] Where do we predict 90% of residuals? \\(\\pm t_{n-2,3 , .05}\\). About \\(\\pm 2\\). 3.5.5.4 DFBETAs DFBETAs represent the change in the parameter estimate due to one observation. \\[\\begin{align} DFBETAS_i &amp;=&amp; \\frac{b_1 - b_{1(i)}}{\\frac{s_{y|x, (i)}}{\\sqrt{(n-1)} s_x}}\\\\ \\end{align}\\] 3.6 R Example (SLR): Happy Planet The data below represents 10 different variables on health of a country measured on 143 countries. Data taken from (R. Lock et al. 2016), originally from the Happy Planet Index Project [http://www.happyplanetindex.org/]. Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. We are going to investigate happiness and life expectancy. 3.6.1 Reading the data into R happy &lt;- read_delim(&quot;~/Dropbox/teaching/math150/spring17/happyPlanet.txt&quot;, delim=&quot;\\t&quot;) glimpse(happy) ## Observations: 143 ## Variables: 11 ## $ Country &lt;chr&gt; &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;Arm… ## $ Region &lt;dbl&gt; 7, 3, 4, 1, 7, 2, 2, 7, 5, 7, 2, 1, 4, 5, 1, 7, 4… ## $ Happiness &lt;dbl&gt; 5.5, 5.6, 4.3, 7.1, 5.0, 7.9, 7.8, 5.3, 5.3, 5.8,… ## $ LifeExpectancy &lt;dbl&gt; 76.2, 71.7, 41.7, 74.8, 71.7, 80.9, 79.4, 67.1, 6… ## $ Footprint &lt;dbl&gt; 2.2, 1.7, 0.9, 2.5, 1.4, 7.8, 5.0, 2.2, 0.6, 3.9,… ## $ HLY &lt;dbl&gt; 41.7, 40.1, 17.8, 53.4, 36.1, 63.7, 61.9, 35.4, 3… ## $ HPI &lt;dbl&gt; 47.91, 51.23, 26.78, 58.95, 48.28, 36.64, 47.69, … ## $ HPIRank &lt;dbl&gt; 54, 40, 130, 15, 48, 102, 57, 85, 31, 104, 64, 27… ## $ GDPperCapita &lt;dbl&gt; 5316, 7062, 2335, 14280, 4945, 31794, 33700, 5016… ## $ HDI &lt;dbl&gt; 0.801, 0.733, 0.446, 0.869, 0.775, 0.962, 0.948, … ## $ Population &lt;dbl&gt; 3.15, 32.85, 16.10, 38.75, 3.02, 20.40, 8.23, 8.3… 3.6.2 Running the linear model (lm) happy.lm = lm(LifeExpectancy ~ Happiness, data=happy) happy.lm %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 28.2 2.28 12.4 2.76e-24 ## 2 Happiness 6.69 0.375 17.8 5.78e-38 3.6.3 Ouptut Some analyses will need the residuals, fitted values, or coefficients individually. happy.lm %&gt;% augment() ## # A tibble: 143 x 9 ## LifeExpectancy Happiness .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 76.2 5.5 65.0 0.537 11.2 0.00765 6.09 1.28e-2 ## 2 71.7 5.6 65.7 0.527 6.00 0.00737 6.14 3.57e-3 ## 3 41.7 4.3 57.0 0.796 -15.3 0.0168 6.02 5.39e-2 ## 4 74.8 7.1 75.7 0.678 -0.944 0.0122 6.16 1.48e-4 ## 5 71.7 5 61.7 0.619 10.0 0.0101 6.10 1.38e-2 ## 6 80.9 7.9 81.1 0.904 -0.198 0.0216 6.16 1.18e-5 ## 7 79.4 7.8 80.4 0.873 -1.03 0.0202 6.16 2.95e-4 ## 8 67.1 5.3 63.7 0.564 3.40 0.00842 6.16 1.32e-3 ## 9 63.1 5.3 63.7 0.564 -0.596 0.00842 6.16 4.04e-5 ## 10 68.7 5.8 67.0 0.515 1.66 0.00705 6.16 2.60e-4 ## # … with 133 more rows, and 1 more variable: .std.resid &lt;dbl&gt; We can plot the main relationship, or we can plot the residuals (to check that technical conditions hold): ggplot(happy, aes(x=Happiness, y=LifeExpectancy)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) happy.lm %&gt;% augment %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept=0) Intervals of interest: mean response, individual response, and parameter(s). predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(&quot;conf&quot;), level=.95) ## fit lwr upr ## 1 54.99531 53.24675 56.74387 ## 2 75.07444 73.78057 76.36830 predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(&quot;pred&quot;), level=.95) ## fit lwr upr ## 1 54.99531 42.72945 67.26117 ## 2 75.07444 62.86510 87.28377 happy.lm %&gt;% tidy(conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 28.2 2.28 12.4 2.76e-24 23.7 32.7 ## 2 Happiness 6.69 0.375 17.8 5.78e-38 5.95 7.43 3.6.3.1 Residuals in R We skipped the residuals section, so you are not responsible for finding residuals in R, but the R code is here for completion in case you are interested: happy.lm %&gt;% augment() ## # A tibble: 143 x 9 ## LifeExpectancy Happiness .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 76.2 5.5 65.0 0.537 11.2 0.00765 6.09 1.28e-2 ## 2 71.7 5.6 65.7 0.527 6.00 0.00737 6.14 3.57e-3 ## 3 41.7 4.3 57.0 0.796 -15.3 0.0168 6.02 5.39e-2 ## 4 74.8 7.1 75.7 0.678 -0.944 0.0122 6.16 1.48e-4 ## 5 71.7 5 61.7 0.619 10.0 0.0101 6.10 1.38e-2 ## 6 80.9 7.9 81.1 0.904 -0.198 0.0216 6.16 1.18e-5 ## 7 79.4 7.8 80.4 0.873 -1.03 0.0202 6.16 2.95e-4 ## 8 67.1 5.3 63.7 0.564 3.40 0.00842 6.16 1.32e-3 ## 9 63.1 5.3 63.7 0.564 -0.596 0.00842 6.16 4.04e-5 ## 10 68.7 5.8 67.0 0.515 1.66 0.00705 6.16 2.60e-4 ## # … with 133 more rows, and 1 more variable: .std.resid &lt;dbl&gt; References "],
["analysis-of-categorical-data-section-6-3.html", "Chapter 4 Analysis of Categorical Data (section 6.3) 4.1 Categorical Inference 4.2 Fisher’s Exact Test (section 6.4) 4.3 Testing independence of two categorical variables (sections 6.5, 6.6, 6.7) 4.4 Parameter Estimation (section 6.8) 4.5 Types of Studies (section 6.9) 4.6 R Example (categorical data): Botox and back pain", " Chapter 4 Analysis of Categorical Data (section 6.3) 4.1 Categorical Inference In either an observational study or a randomized experiment, we are often interested in assessing the statistical significance of the differences we see: Is the observed difference too big to have reasonably occurred just due to chance? To answer the question, we will use simulation mathematical probability models. Example 4.1 Back Pain &amp; Botox, Chance and Rossman (2018), Foster et al. (2001) The randomized clinical trial examined whether the drug botulinum toxin A (Botox) is helpful for reducing pain among patients who suffer from chronic low back pain. The 31 subjects who participated in the study were randomly assigned to one of two treatment groups: 16 received a placebo of normal saline and the other 15 received the drug itself. The subjects’ pain levels were evaluated at the beginning of the study and again after eight weeks. The researchers found that 2 of the 16 subjects who received the saline experienced a substantial reduction in pain, compared to 9 of the 15 subjects who received the actual drug. Is this an experiment or an observational study? Explain the importance of using the “placebo” treatment of saline. Create the two-way table for summarizing the data, putting the explanatory variable as the columns and the response as rows. Calculate the conditional proportions of pain reduction in the two groups. Display the results as a segmented bar graph. Comment on the preliminary analysis. placebo Botox pain reduction 2 9 11 no pain reduction 14 6 20 16 15 31 \\[\\begin{eqnarray*} \\mbox{risk}_{\\mbox{placebo}} = \\frac{2}{16} &amp;=&amp; 0.125\\\\ \\mbox{risk}_{\\mbox{Botox}} = \\frac{9}{15} &amp;=&amp; 0.6\\\\ RR &amp;=&amp; 4.8 \\end{eqnarray*}\\] backpain &lt;- matrix(c(2,14,9,6),ncol=2,byrow=F) backpain ## [,1] [,2] ## [1,] 2 9 ## [2,] 14 6 backpain.bp &lt;- barplot(backpain, plot=FALSE) barplot(backpain, names.arg=c(&quot;placebo&quot;,&quot;botulinum toxin A&quot;)) text(backpain.bp, c(backpain[1,]-1,backpain[2,]+backpain[1,]-1),t(backpain)) If there was no association between the treatment and the back pain relief, about how many of the 11 “successes” would you expect to see in each group? Did the researchers observe more successes in the saline group than expected (if the drug had no effect) or fewer successes than expected? Is this in the direction conjectured by the researchers? Is is possible that the drug has absolutely no effect on back pain? That the differences were simply due to chance or random variability? How likely is that? Simulation 11 red “success” cards (pain reduction); 20 black “failure” cards (no pain reduction) randomly deal out (i.e. shuffle) 15 cards to the treatment group and 16 cards to the placebo group. count how many people in the treatment group were successes? Repeat 5 times. process what do the cards represent? what does shuffling the cards represent? what implicit assumption about the two groups did the shuffling of cards represent? what observational units would be represented by the dots on the dotplot? why would we count the number of repetitions with 9 or more “successes”? Repeat simulation using the two-way table applet: http://www.rossmanchance.com/applets/TwowaySim/TwowaySim.html summary How many reps? How many as extreme as the true data? What proportion are at least as extreme as the true data? Do our data support the researchers conjecture? What if the actual data had been 7 successes in the treatment group (and 4 in the placebo group)? Definition 2.1 p-value The p-value is the probability of seeing our results or more extreme if there is nothing interesting going on with the data. (This is the same definition of p-value that you will always use in this class and in your own research.) Notice that regardless of whether or not the drug has an effect, the data will be different each time (think: new 31 people). The small p-value allows us to draw cause-and-effect conclusions, but doesn’t necessarily allow us to infer to a larger population. Why not? low cutoff p-value high cutoff evidence p-value \\(\\leq\\) 0.001 very strong evidence 0.001 \\(&lt;\\) p-value \\(\\leq\\) 0.01 strong evidence 0.01 \\(&lt;\\) p-value \\(\\leq\\) 0.05 moderate evidence 0.05 \\(&lt;\\) p-value \\(\\leq\\) 0.10 weak but suggestive evidence 0.10 \\(&lt;\\) p-value little or no evidence 4.2 Fisher’s Exact Test (section 6.4) Because we have a fixed sample, we can’t use the Binomial distribution to figure out associated probabilities. Instead, we use the hypergeometric distribution to enumerate the possible ways of choosing our data or more extreme given fixed row and column totals. placebo Botox pain reduction 2 = x 9 11 = n no pain reduction 14 6 20 16 = M 15 = N - M 31 = N To make it simpler, let’s say I have 5 items (N=5), and I want to choose 3 of them (n=3). How many ways can I do that? SSSNN, SSNSN, SSNNS, SNSSN, SNSNS, SNNSS, NSSSN, NSSNS, NSNSS, NNSSS [5!/ 3! 2!] (S = select, N = not selected) So, how many different ways can I select 11 people (out of 31) to be my “pain reduction” group? That is the total number of different groups of size 11 from 31. But really, we want our groups to be of a certain breakdown. We need 2 (of 16) to have gotten the placebo and 9 (of 15) to have gotten the Botox treatment. Definition 4.1 Hyp Geom Prob For any \\(2 \\times 2\\) table when there are N observations with M total successes , the probability of observing x successes in a sample of size n is: \\[\\begin{eqnarray*} P(X=x) = \\frac{\\# \\mbox{ of ways to select x successes and n-x failures}}{\\# \\mbox{ of ways to select n subjects}} = \\frac{ { M \\choose x} {N-M \\choose n-x}}{{N \\choose n}}\\\\ \\end{eqnarray*}\\] Find the P(X=2) We can now find EXACT probabilities associated with the following hypotheses. \\[\\begin{eqnarray*} &amp;&amp;H_0: \\pi_{pl} = \\pi_{Btx}\\\\ &amp;&amp;H_a: \\pi_{pl} &lt; \\pi_{Btx}\\\\ &amp;&amp;\\pi = \\mbox{true probability of no pain}\\\\ \\end{eqnarray*}\\] Is this a one- or two-sided test? Why? [Note: the assumptions here are that the row and column totals are fixed – a conditional test of independence. However, the research project in the back of chapter 6 extends the permutation test to demonstrated that the probabilities hold even under alternative technical conditions. Note also that we get an exact probability with no assumptions about sample size (we can use Fisher’s Exact Test even when true probabilities are close to 0 or 1.] 4.3 Testing independence of two categorical variables (sections 6.5, 6.6, 6.7) 4.3.1 \\(\\chi^2\\) tests (section 6.6) 2x2… but also rxc (\\(\\pi_a = \\pi_b = \\pi_c\\)) We can also use \\(\\chi^2\\) tests to evaluate \\(r \\times c\\) contingency tables. Our main question now will be whether there is an association between two categorical variables of interest. Note that we are now generalizing what we did with the Botox and back pain example. Are the two variables independent? If the two variables are independent, then the state of one variable is not related to the probability of the different outcomes of the other variable. If the data were sampled in such a way that we have random samples of both the explanatory and response variables (e.g., cross classification study), then we typically do a test of association: \\[\\begin{eqnarray*} H_0: &amp;&amp; \\mbox{ the two variables are independent}\\\\ H_a: &amp;&amp; \\mbox{ the two variables are not independent} \\end{eqnarray*}\\] If the data are sampled in such a way that the response is measured across specified populations (as in the example below), we typically do a test of homogeneity of proportions. For example, \\[\\begin{eqnarray*} H_0: &amp;&amp; \\pi_1 = \\pi_2 = \\pi_3\\\\ H_a: &amp;&amp; \\mbox{not } H_0 \\end{eqnarray*}\\] where \\(\\pi=P(\\mbox{success})\\) for each of groups 1,2,3. How do we get expected frequencies? The same mathematics hold regardless of the type of test (i.e., sampling mechanism used to collect the data). If, in fact,the variables are independent, then we should be able to multiply their probabilities. If the probabilities are the same, we expect the overall proportion of each response variable to be the same as the proportion of the response variable in each explanatory group. And the math in the example below follows directly. Example 4.2 The table below show the observed distributions of ABO blood type in three random samples of African Americans living in different locations. The three datasets, collected in the 1950s by three different investigators, are reproduced in (Mourant, Kopec, and Domaniewsa-Sobczak 1976). Location (Florida) (Iowa) (Missouri) Total A 122 1781 353 2256 Blood B 117 1351 269 1737 Type AB 19 289 60 368 O 244 3301 713 4258 Total 502 6722 1395 8619 4.3.1.1 Test of Homogeneity of Proportions (equivalent mathematically to independence) If there is no difference in blood type proportions across the groups, then: \\[\\begin{eqnarray*} P(AB | FL) = P(AB | IA) = P(AB | MO) = P(AB) \\end{eqnarray*}\\] We will use \\(\\hat{P}(AB) = \\frac{368}{8619}\\) as baseline for expectation (under \\(H_0\\)) for all the groups. That is, we would expect, \\[\\begin{eqnarray*} \\# \\mbox{expected for AB blood and Iowa} &amp;=&amp; \\frac{368}{8619} \\cdot 6722\\\\ \\end{eqnarray*}\\] 4.3.1.2 Test of Independence (equivalent mathematically to homogeneity of proporitions) \\[\\begin{eqnarray*} P(cond1 \\mbox{ &amp; } cond2 ) &amp;=&amp; P(cond1) P(cond2) \\mbox{ if variables 1 and 2 are independent}\\\\ P(AB \\mbox{ blood &amp; Iowa}) &amp;=&amp; P(AB \\mbox{ blood}) P(\\mbox{Iowa}) \\\\ &amp;=&amp; \\bigg( \\frac{368}{8619}\\bigg) \\bigg( \\frac{6722}{8619} \\bigg)\\\\ &amp;=&amp; 0.0333\\\\ \\# \\mbox{expected for AB blood and Iowa} &amp;=&amp; 0.033 \\cdot 8619\\\\ &amp;=&amp; \\frac{368 \\cdot 6722}{8619}\\\\ E_{i,j} &amp;=&amp; \\frac{(i \\mbox{ row total})(j \\mbox{ col total})}{\\mbox{grand total}}\\\\ \\end{eqnarray*}\\] And the expected values under the null hypothesis… Location (Florida) (Iowa) (Missouri) Total A 131.4 1759.47 365.14 2256 Blood B 101.17 1354.69 281.14 1737 Type AB 21.43 287.00 59.56 368 O 248.00 3320.83 689.16 4258 Total 502 6722 1395 8619 \\[\\begin{eqnarray*} X^2 &amp;=&amp; \\sum_{all cells} \\frac{( O - E)^2}{E}\\\\ &amp;=&amp; 5.65\\\\ \\mbox{p-value} &amp;=&amp; P(\\chi^2_6 \\geq 5.65) \\\\ &amp;=&amp; 1 - pchisq(5.65, 6)\\\\ &amp;=&amp; 0.464 \\end{eqnarray*}\\] We cannot reject the null hypothesis. Again, we have no evidence against the null hypothesis that blood types are independently distributed in the various regions. How do we know if our test statistic is a big number or not? Well, it turns out that the test statistic (\\(X^2\\)) will have an approximate \\(\\chi^2\\) distribution with degrees of freedom = \\((r- 1)\\cdot (c-1)\\). As long as: We have a random sample from the population. We expect at least 1 observation in every cell (\\(E_i \\geq 1 \\forall i\\)) We expect at least 5 observations in 80% of the cells (\\(E_i \\geq 5\\) for 80% of \\(i\\)) When there are only two populations, the \\(\\chi^2\\) procedure is equivalent to the two-sided z-test for proportions. The chi-squared test statistic is the square of the z-test statistic. That is, the chi-squared test is exactly the same as the two-sided alternative for the z-test. use chi-square if you have multiple populations use z-test if you want one-sided tests or confidence intervals. 4.4 Parameter Estimation (section 6.8) Definition 4.2 Data Types Data are often classified as Categorical - each unit is assigned to a category Quantitative - each observational unit is assigned a numerical value (Binary - a special case of categorical with 2 categories, e.g. male/female) Table 6.6 on page 193 of the textbook is excellent and worth looking at. Example 4.3 Popcorn &amp; Lung Disease Chance and Rossman (2018) How can we tell if popcorn production is related to lung disease? Consider High / Low exposure: low exposure high exposure Airway obstructed 6 15 21 Airway not obstructed 52 43 95 58 58 116 Is 21 a lot of people? Can we compare 6 vs. 15? What should we look at? proportions (always a number between 0 and 1). Look at your data (graphically and numerically). Segmented bar graph (mosaic plot): Is there a difference in the two groups? Look at the difference in proportions or risk: \\[\\begin{eqnarray*} 6/58 = 0.103 &amp; 15/58=0.2586 &amp; \\Delta = 0.156\\\\ p_1 = 0.65 &amp; p_2 = 0.494 &amp; \\Delta = 0.156\\\\ p_1 = 0.001 &amp; p_2 = 0.157 &amp; \\Delta = 0.156\\\\ \\end{eqnarray*}\\] It turns out that the sampling distribution of the difference in the sample proportions (of success) across two independent groups can be modeled by the normal distribution if we have reasonably large sample sizes (CLT). To ensure the accuracy of the test, check whether np and n(1-p) is bigger than 5 in both samples is usually adequate. A more precise check is \\(n_s \\hat{p}_c\\) and \\(n_s(1-\\hat{p}_c)\\) are both greater than 5; \\(n_s\\) is the smaller of the two sample sizes and \\(\\hat{p}_c\\)is the sample proportion when the two samples are combined into one. Note: \\[\\begin{eqnarray*} \\hat{p}_1 - \\hat{p}_2 \\sim N\\Bigg(\\pi_1 - \\pi_2, \\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1} + \\frac{\\pi_2(1-\\pi_2)}{n_2}}\\Bigg) \\end{eqnarray*}\\] When testing independence, we assume that \\(\\pi_1=\\pi_2\\), so we use the pooled estimate of the proportion to calculate the SE: \\[\\begin{eqnarray*} SE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) \\bigg(\\frac{1}{n_1} + \\frac{1}{n_2}\\bigg)} \\end{eqnarray*}\\] So, when testing, the appropriate test statistic is: \\[\\begin{eqnarray*} Z = \\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{ \\sqrt{ \\hat{p}_c(1-\\hat{p}_c) (\\frac{1}{n_1} + \\frac{1}{n_2})}} \\end{eqnarray*}\\] 4.4.1 CI for differences in proportions We can’t pool our estimate for the SE, but everything else stays the same… \\[\\begin{eqnarray*} SE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\end{eqnarray*}\\] The main idea here is to determine whether two categorical variables are independent. That is, does knowledge of the value of one variable tell me something about the probability of the other variable (gender and pregnancy). We’re going to talk about two different ways to approach this problem. 4.4.2 Relative Risk Definition 4.3 Relative Risk The relative risk (RR) is the ratio of risks for each group. We say, “The risk of success is RR times higher for those in group 1 compared to those in group 2.” \\[\\begin{eqnarray*} \\mbox{relative risk} &amp;=&amp; \\frac{\\mbox{risk group 1}}{\\mbox{risk group 2}}\\\\ &amp;=&amp; \\frac{\\mbox{proportion of successes in group 1}}{\\mbox{proportion of successes in group 2}}\\\\ \\mbox{RR} &amp;=&amp; \\frac{p_1}{p_2} = \\frac{\\pi_1}{\\pi_2}\\\\ \\hat{\\mbox{RR}} &amp;=&amp; \\frac{\\hat{p}_1}{\\hat{p}_2} \\end{eqnarray*}\\] \\(\\hat{RR}\\) in the popcorn example is \\(\\frac{15/58}{6/58} = 2.5\\). We say, “The risk of airway obstruction is 2.5 times higher for those in high exposure group compared to those in the low exposure group.” What about sample size? baseline risk? To create confidence intervals for relative risk, we use the fact that: \\[\\begin{eqnarray*} SE(\\ln (\\hat{RR})) &amp;\\approx&amp; \\sqrt{\\frac{(1 - \\hat{p}_1)}{n_1 \\hat{p}_1} + \\frac{(1-\\hat{p}_2)}{n_2 \\hat{p}_2}} \\end{eqnarray*}\\] 4.4.3 Odds Ratios A related concept to risk is odds. It is often used in horse racing, where “success” is typically defined as losing. So, if the odds are 3 to 1 we would expect to lose 3/4 of the time. Definition 4.4 Odds Ratio A related concept to risk is odds. It is often used in horse racing, where “success” is typically defined as losing. So, if the odds are 3 to 1 we would expect to lose 3/4 of the time. The odds ratio (OR) is the ratio of odds for each group. We say, “The odds of success is OR times higher for those in group 1 compared to those group 2.” \\[\\begin{eqnarray*} \\mbox{odds} &amp;=&amp; \\frac{\\mbox{proportion of successes}}{\\mbox{proportion of failures}}\\\\ &amp;=&amp; \\frac{\\mbox{number of successes}}{\\mbox{number of failures}} = \\theta\\\\ \\hat{\\mbox{odds}} &amp;=&amp; \\hat{\\theta}\\\\ \\mbox{odds ratio} &amp;=&amp; \\frac{\\mbox{odds group 1}}{\\mbox{odds group 2}} \\\\ \\mbox{OR} &amp;=&amp; \\frac{\\theta_1}{\\theta_2} = \\frac{p_1/(1-p_1)}{p_2/(1-p_2)}= \\frac{\\pi_1/(1-\\pi_1)}{\\pi_2/(1-\\pi_2)}\\\\ \\hat{\\mbox{OR}} &amp;=&amp; \\frac{\\hat{\\theta}_1}{\\hat{\\theta}_2} = \\frac{\\hat{p}_1/(1-\\hat{p}_1)}{\\hat{p}_2/(1-\\hat{p}_2)}\\\\ \\end{eqnarray*}\\] \\(\\hat{OR}\\) in the popcorn example is \\(\\frac{15/43}{6/52} = 3.02\\). We say, “The odds of airway obstruction are 3 times higher for those in the high exposure group compared to those in the low exposure group.” 4.4.3.1 OR is more extreme than RR Without loss of generality, assume the true \\(RR &gt; 1\\), implying \\(\\pi_1 / \\pi_2 &gt; 1\\) and \\(\\pi_1 &gt; \\pi_2\\). Note the following sequence of consequences: \\[\\begin{eqnarray*} RR = \\frac{\\pi_1}{\\pi_2} &amp;&gt;&amp; 1\\\\ \\frac{1 - \\pi_1}{1 - \\pi_2} &amp;&lt;&amp; 1\\\\ \\frac{ 1 / (1 - \\pi_1)}{1 / (1 - \\pi_2)} &amp;&gt;&amp; 1\\\\ \\frac{\\pi_1}{\\pi_2} \\cdot \\frac{ 1 / (1 - \\pi_1)}{1 / (1 - \\pi_2)} &amp;&gt;&amp; \\frac{\\pi_1}{\\pi_2}\\\\ OR &amp;&gt;&amp; RR \\end{eqnarray*}\\] 4.4.3.2 Other considerations: Observational study (who worked in each place?) Cross sectional (only one point in time) Healthy worker effect (who stayed home sick?) Explanatory variable is one that is a potential explanation for any changes (here exposure level). Response variable is the measured outcome of interest (here airway obstruction). Example 4.4 Smoking &amp; Lung Cancer Chance and Rossman (2018) After World War II, evidence began mounting that there was a link between cigarette smoking and pulmonary carcinoma (lung cancer). In the 1950s, two now classic articles were published on the subject. One of these studies was conducted in the United States by Wynder and Graham (1950). They found records from a large number (684) of patients with proven bronchiogenic carcinoma (a specific form of lung cancer) in hospitals in California, Colorado, Missouri, New Jersey, New York, Ohio, Pennsylvania, and Utah. They personally interviewed 634 of the subjects to identify their smoking habits, occupation, exposure to dust and fumes, alcohol intake, education, and cause of death of parents and siblings. Thirty-three subjects completed mailed questionnaires, and information for the other 17 was obtained from family members or close acquaintances. Of those in the study, the researchers focused on 605 male patients with the same form of lung cancer. Another 1332 hospital patients with similar age and economic distribution (including 780 males) without lung cancer were interviewed by these researchers in St. Louis and by other researchers in Boston, Cleveland, and Hines, Illinois. The following two-way table replicates the counts for the 605 male patients with the same form of cancer and for the “control-group” of 780 males. none light mod heavy heavy excessive chain \\(&lt;\\) 1/day 1-9/day 10-15/day 16-20/day 21-34/day 35\\(+\\)/day patients 8 14 61 213 187 122 controls 114 90 148 278 90 60 Given the results of the study, do you think we can generalize from the sample to the population? Explain and make it clear that you know the difference between a sample and a population. Causation? Case-control study (605 with lung cancer, 780 without… baseline rate?) Group A Group B expl = smoking status expl = lung cancer resp = lung cancer resp = smoking status If lung cancer is considered a success and no smoking is baseline: \\[\\begin{eqnarray*} \\hat{RR} &amp;=&amp; \\frac{122/182}{8/122} = 10.22\\\\ \\hat{OR} &amp;=&amp; \\frac{122/60}{8/114} = 28.9\\\\ \\end{eqnarray*}\\] The risk of lung cancer is 10.22 times higher for those who smoke than for those who don’t smoke. The odds of lung cancer is 28.9 times higher for those who smoke than for those who don’t smoke. If chain smoking is considered a success and healthy is baseline: \\[\\begin{eqnarray*} \\hat{RR} &amp;=&amp; \\frac{122/130}{60/174} = 2.7\\\\ \\hat{OR} &amp;=&amp; \\frac{122/8}{60/114} = 28.9\\\\ \\end{eqnarray*}\\] The risk of smoking is 2.7 times higher for those who have lung cancer than for those who don’t have lung cancer. The odds of smoking is 28.9 times higher for those who have lung cancer than for those who don’t have lung cancer. We know the risk of being a light smoker if you have lung cancer but we do not know the risk of lung cancer if you are a light smoker. Let’s say we have a population of 1,000,000 people: light smoking no smoking cancer 49,000 1,000 50,000 healthy 51,000 899,000 950,000 100,000 900,000 1,000,000 \\[\\begin{eqnarray*} P(\\mbox{light} | \\mbox{lung cancer}) &amp;=&amp; \\frac{49,000}{50,000} = 0.98\\\\ P(\\mbox{lung cancer} | \\mbox{light}) &amp;=&amp; \\frac{49,000}{100,000} = 0.49\\\\ \\end{eqnarray*}\\] What is the explanatory variable? What is the response variable? relative risk? odds ratio? Group A Group B expl = smoking status expl = lung cancer resp = lung cancer resp = smoking status If lung cancer is considered a success and no smoking is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{49/100}{1/900} = 441\\\\ OR &amp;=&amp; \\frac{49/51}{1/899} = 863.75\\\\ \\end{eqnarray*}\\] If light smoking is considered a success and healthy is baseline: \\[\\begin{eqnarray*} RR &amp;=&amp; \\frac{49/50}{51/950} = 18.25\\\\ OR &amp;=&amp; \\frac{49/1}{51/899} = 863.75\\\\ \\end{eqnarray*}\\] OR is the same no matter which variable you choose as explanatory versus response! Though, in general, we still prefer to know baseline odds or baseline risk (which we can’t know with a case-control study). Example 4.5 More on Smoking &amp; Lung Cancer, Chance and Rossman (2018) Now we have a cohort prospective study. (Previously we had a case-control retrospective study). Now do we have baseline risk estimates? Yes! But be careful, we can’t conclude causation, because the study is still observational. 4.4.4 Confidence Interval for OR Due to some theory that we won’t cover: \\[\\begin{eqnarray*} SE(\\ln (\\hat{OR})) &amp;\\approx&amp; \\sqrt{\\frac{1}{n_1 \\hat{p}_1 (1-\\hat{p}_1)} + \\frac{1}{n_2 \\hat{p}_2 (1-\\hat{p}_2)}} \\end{eqnarray*}\\] Note that your book introduces \\(SE(\\ln(\\hat{OR}))\\) in the context of hypothesis testing where the null, \\(H_0: \\pi_1 = \\pi_2\\), is assumed to be true. If the null is true, you’d prefer an estimate for the proportion of success to be based on the entire sample: \\[\\begin{eqnarray*} SE(\\ln (\\hat{OR})) &amp;\\approx&amp; \\sqrt{\\frac{1}{n_1 \\hat{p} (1-\\hat{p})} + \\frac{1}{n_2 \\hat{p}(1-\\hat{p})}} \\end{eqnarray*}\\] So, a \\((1-\\alpha)100\\%\\) CI for the \\(\\ln(OR)\\) is: \\[\\begin{eqnarray*} \\ln(\\hat{OR}) \\pm z_{1-\\alpha/2} SE(\\ln(\\hat{OR})) \\end{eqnarray*}\\] Which gives a \\((1-\\alpha)100\\%\\) CI for the \\(OR\\): \\[\\begin{eqnarray*} (e^{\\ln(OR) - z_{1-\\alpha/2} SE(\\ln(OR))}, e^{\\ln(OR) + z_{1-\\alpha/2} SE(\\ln(OR))}) \\end{eqnarray*}\\] Back to the example… OR = 28.9. \\[\\begin{eqnarray*} SE(\\ln(\\hat{OR})) &amp;=&amp; \\sqrt{\\frac{1}{182*0.67*(1-0.67)} + \\frac{1}{122*0.0656*(1-0.0656)}}\\\\ &amp;=&amp; 0.398\\\\ 90\\% \\mbox{ CI for } \\ln(OR) &amp;&amp; \\ln(28.9) \\pm 1.645 \\cdot 0.398\\\\ &amp;&amp; 3.366 \\pm 1.645 \\cdot 0.398\\\\ &amp;&amp; (2.71, 4.02)\\\\ 90\\% \\mbox{ CI for } OR &amp;&amp; (e^{2.71}, e^{4.02})\\\\ &amp;&amp; (15.04, 55.47)\\\\ \\end{eqnarray*}\\] We are 90% confident that the true \\(\\ln(OR)\\) is between 2.71 and 4.02. We are 90% confident that the true \\(OR\\) is between 15.04 and 55.47. That is, the true odds of getting lung cancer if you smoke are somewhere between 15.04 and 55.47 times higher than if you don’t smoke, with 90% confidence. Note 1: we use the theory which allows us to understand the sampling distribution for the \\(\\ln(\\hat{OR}).\\) We use the process for creating CIs to transform back to \\(OR\\). Note 2: We do not use the t-distribution here because we are not estimating the population standard deviation. Note 3: There are not good general guidelines for checking whether the sample sizes are large enough for the normal approximation. Most authorities agree that one can get away with smaller sample sizes here than for the differences of two proportions. If the sample sizes pass the rough check discussed for \\(\\chi^2\\), they should be large enough to support inferences based on the approximate normality of the log of the estimated odds ratio, too. (Ramsey and Schafer 2012, 541) For the normal approximation to hold, we need the expected counts in each cell to be at least 5. (Pagano and Gauvreau 2000, 355) Note 4: If any of the cells are zero, many people will add 0.5 to that cell’s observed value. Note 5: The OR will always be more extreme than the RR (one more reason to be careful…) \\[\\begin{eqnarray*} \\mbox{assume } &amp;&amp; \\frac{X_1 / n_1}{X_2 / n_2} = RR &gt; 1\\\\ &amp; &amp; \\\\ \\frac{X_1}{n_1} &amp;=&amp; RR \\ \\ \\frac{X_2}{n_2}\\\\ \\frac{X_1}{n_1 - X_1} &amp;=&amp; RR \\ \\ \\bigg( \\frac{n_1}{n_2} \\frac{n_2 - X_2}{n_1 - X_1} \\bigg) \\frac{X_2}{n_2-X_2}\\\\ OR &amp;=&amp; RR \\ \\ \\bigg(\\frac{n_1}{n_2} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\ &amp;=&amp; RR \\ \\ \\bigg(\\frac{1/n_2}{1/n_1} \\bigg) \\frac{n_2 - X_2}{n_1 - X_1}\\\\ &amp;=&amp; RR \\ \\ \\frac{1 - X_2/n_2}{1 - X_1/n_1}\\\\ &amp; &gt; &amp; RR \\end{eqnarray*}\\] [\\(1 - \\frac{X_2}{n_2} &gt; 1 - \\frac{X_1}{n_1} \\rightarrow \\frac{1 - \\frac{X_2}{n_2}}{1 - \\frac{X_1}{n_1}} &gt; 1\\)] Note 6: \\(RR \\approx OR\\) if RR is very small (the denominator of the OR will be very similar to the denominator of the RR). 4.5 Types of Studies (section 6.9) Definition 4.5 Explanatory variable is one that is a potential explanation for any changes in the response variable. Definition 4.6 Response variable is the measured outcome of interest. Definition 4.7 Case-control study: identify observational units by response Definition 4.8 Cohort study: identify observational units by explanatory variable Definition 4.9 Cross-classification study: identify observational units regardless of levels of the variable. 4.5.1 Retrospective versus Prospective Studies After much research (and asking many people who do not all agree!), I finally came across a definition of retrospective that I like. Note, however, that many many books define retrospective as synonymous with case-control. That is, they define a retrospective study to be one in which the observational units were chosen based on their status of the response variable. I disagree with that definition. As you see below, retrospective studies are defined based on the when the variables were measured. I’ve also given a quote from the Kuiper text where retrospective is defined as any study where historic data are collected (I like this definition less). Definition 4.10 A prospective study watches for outcomes, such as the development of a disease, during the study period. The explanatory variables are measured before the response variable occurs. Definition 4.11 A retrospective study looks backwards and examines exposures to suspected risk or protection factors in relation to an outcome that is established at the start of the study. The explanatory variables are measured after the response has happened. Studies can be classified further as either prospective or retrospective. We define a prospective study as one in which exposure and covariate measurements are made before the cases of illness occur. In a retrospective study these measurements are made after the cases have already occurred… Early writers referred to cohort studies as prospective studies and to case-control studies as retrospective studies because cohort studies usually begin with identification of the exposure status and then measure disease occurrence, whereas case-control studies usually begin by identifying cases and controls and then measure exposure status. The terms prospective and retrospective, however, are more usefully employed to describe the timing of disease occurrence with respect to exposure measurement. For example, case-control studies can be either prospective or retrospective. A prospective case-control study uses exposure measurements taken before disease, whereas a retrospective case-control study uses measurements taken after disease. (Rothman and Greenland 1998, 74) Retrospective cohort studies also exist. In these designs past (medical) records are often used to collect data. As with prospective cohort studies, the objective is still to first establish groups based on an explanatory variable. However since these are past records the response variable can be collected at the same time. (Kuiper and Sklar 2013, chap. 6, page 24) Understanding if a study is retrospective or prospective leads to having a sense of the biases within a study. The retrospective aspect may introduce selection bias and misclassification or information bias. With retrospective studies, the temporal relationship is frequently difficult to assess. Disadvantages of Prospective Cohort Studies You may have to follow large numbers of subjects for a long time. They can be very expensive and time consuming. They are not good for rare diseases. They are not good for diseases with a long latency. Differential loss to follow up can introduce bias. Disadvantages of Retrospective Cohort Studies As with prospective cohort studies, they are not good for very rare diseases. If one uses records that were not designed for the study, the available data may be of poor quality. There is frequently an absence of data on potential confounding factors if the data was recorded in the past. It may be difficult to identify an appropriate exposed cohort and an appropriate comparison group. Differential losses to follow up can also bias retrospective cohort studies. Disadvantages from: http://sphweb.bumc.bu.edu/otlt/MPH-Modules/EP/EP713_CohortStudies/EP713_CohortStudies5.html Examples of studies: cross-classification, prospective: NHANES cross-classification, retrospective: death records (if exposure is measured post-hoc) case-control, prospective: the investigator still enrolls based on outcome status, but the investigator must wait to the cases to occur case-control, retrospective: at the start of the study, all cases have already occurred and the investigator goes back to measure the exposure (explanatory) variable cohort, retrospective: the exposure and outcomes have already happened (i.e., death records) cohort, prospective: follows the selected participants to assess the proportion who develop the disease of interest Which test? (section 6.1) It turns out that the tests above (independence, homogeneity of proportions, homogeneity of odds) are typically equivalent with respect to their conclusions. However, they each have particular assumptions about what they are testing, but that we can generally use any of them for our hypotheses of interest. However, we need to be very careful about our interpretations! (no goodness of fit, section 6.11) 4.6 R Example (categorical data): Botox and back pain 4.6.1 Entering and visualizing the data backpain &lt;- matrix(c(2,14,9,6),ncol=2,byrow=F) backpain ## [,1] [,2] ## [1,] 2 9 ## [2,] 14 6 backpain.bp &lt;- barplot(backpain, plot=FALSE) barplot(backpain, names.arg=c(&quot;placebo&quot;,&quot;botulinum toxin A&quot;)) text(backpain.bp, c(backpain[1,]-1,backpain[2,]+backpain[1,]-1),t(backpain)) 4.6.2 Fisher’s Exact Test fisher.test(backpain) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: backpain ## p-value = 0.009147 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.008475855 0.710710371 ## sample estimates: ## odds ratio ## 0.1040127 # their CI is an inversion of the HT # an approximate SE for the ln(OR) is given by: se.lnOR &lt;- sqrt(1/(16*(2/16)*(14/16)) + 1/(15*(9/15)*(6/15))) se.lnOR ## [1] 0.9215239 4.6.3 Chi-squared Analysis chisq.test(backpain) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: backpain ## X-squared = 5.6964, df = 1, p-value = 0.017 References "],
["logistic-regression.html", "Chapter 5 Logistic Regression 5.1 Motivation for Logistic Regression 5.2 Estimating coefficients in logistic regression 5.3 Formal Inference 5.4 Multiple Logistic Regression 5.5 Multicolinearity 5.6 Model Building 5.7 Receiver Operating Characteristic Curves 5.8 Cross Validation 5.9 R: Birdnest Example", " Chapter 5 Logistic Regression 5.1 Motivation for Logistic Regression During investigation of the US space shuttle Challenger disaster, it was learned that project managers had judged the probability of mission failure to be 0.00001, whereas engineers working on the project had estimated failure probability at 0.005. The difference between these two probabilities, 0.00499 was discounted as being too small to worry about. Is a different picture provided by considering odds? How is it interpreted? The logistic regression model is a generalized linear model. That is, a linear model as a function of the expected value of the response variable. We can now model binary response variables. \\[\\begin{eqnarray*} GLM: g(E[Y | X]) = \\beta_0 + \\beta_1 X \\end{eqnarray*}\\] where \\(g(\\cdot)\\) is the link function. For logistic regression, we use the logit link function: \\[\\begin{eqnarray*} \\logit (p) = \\ln \\bigg( \\frac{p}{1-p} \\bigg) \\end{eqnarray*}\\] Example 5.1 Surviving third-degree burns These data refer to 435 adults who were treated for third-degree burns by the University of Southern California General Hospital Burn Center. The patients were grouped according to the area of third-degree burns on the body (measured in square cm). In the table below are recorded, for each midpoint of the groupings log(area +1), the number of patients in the corresponding group who survived, and the number who died from the burns. (Fan, Heckman, and Wand 1995) log(area+1) midpoint survived died prop surv 1.35 13 0 1 1.60 19 0 1 1.75 67 2 0.971 1.85 45 5 0.900 1.95 71 8 0.899 2.05 50 20 0.714 2.15 35 31 0.530 2.25 7 49 0.125 2.35 1 12 0.077 We can see that the logit transformation linearizes the relationship. A first idea might be to model the relationship between the probability of success (that the patient survives) and the explanatory variable log(area +1) as a simple linear regression model. However, the scatterplot of the proportions of patients surviving a third-degree burn against the explanatory variable shows a distinct curved relationship between the two variables, rather than a linear one. It seems that a transformation of the data is in place. Note that the functional form relating x and the probability of success looks like it could be an S shape. But we’d have to do some work to figure out what the form of that S looks like. Below I’ve given some different relationships between x and the probability of success using \\(\\beta_0\\) and \\(\\beta_1\\) values that are yet to be defined. Regardless, we can see that by tuning the functional relationship of the S curve, we can get a good fit to the data. S-curves ( y = exp(linear) / (1+exp(linear)) ) for a variety of different parameter settings. Note that the x-axis is some continuous variable x while the y-axis is the probability of success at that value of x. More on this as we move through this model. Why doesn’t linear regression work here? The response isn’t normal The response isn’t linear (until we transform) The predicted values go outside the bounds of (0,1) Note: it does work to think about values inside (0,1) as probabilities 5.1.1 The logistic model Instead of trying to model the using linear regression, let’s say that we consider the relationship between the variable \\(x\\) and the probability of success to be given by the following generalized linear model. (Note that this is just one model, there isn’t anything magical about it. We do have good reasons for how we defined it, but that doesn’t mean there aren’t other good ways to model the relationship.) \\[\\begin{eqnarray*} p(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1+e^{\\beta_0 + \\beta_1 x}} \\end{eqnarray*}\\] Where \\(p(x)\\) is the probability of success (here surviving a burn). \\(\\beta_1\\) still determines the direction and slope of the line. \\(\\beta_0\\) now determines the location (median survival). Note 1 What is the probability of success for a patient with covariate of \\(x = -\\beta_0 / \\beta_1\\)? \\[\\begin{eqnarray*} x &amp;=&amp; - \\beta_0 / \\beta_1\\\\ \\beta_0 + \\beta_1 x &amp;=&amp; 0\\\\ e^{0} &amp;=&amp; 1\\\\ p(-\\beta_0 / \\beta_1) &amp;=&amp; p(x) = 0.5 \\end{eqnarray*}\\] (for a given \\(\\beta_1\\), \\(\\beta_0\\) determines the median survival value) Note 2 If \\(x=0\\), \\[\\begin{eqnarray*} p(0) = \\frac{e^{\\beta_0}}{1+e^{\\beta_0}} \\end{eqnarray*}\\] \\(x=0\\) can often be thought of as the baseline condition, and the probability at \\(x=0\\) takes the place of thinking about the intercept in a linear regression. Note 3 \\[\\begin{eqnarray*} 1 - p(x) = \\frac{1}{1+e^{\\beta_0 + \\beta_1 x}} \\end{eqnarray*}\\] gives the probability of failure. \\[\\begin{eqnarray*} \\frac{p(x)}{1-p(x)} = e^{\\beta_0 + \\beta_1 x} \\end{eqnarray*}\\] gives the odds of success. \\[\\begin{eqnarray*} \\ln \\bigg( \\frac{p(x)}{1-p(x)} \\bigg) = \\beta_0 + \\beta_1 x \\end{eqnarray*}\\] gives the \\(\\ln\\) odds of success . Note 4 Every type of generalized linear model has a link function. Ours is called the logit. The link is the relationship between the response variable and the linear function in x. \\[\\begin{eqnarray*} \\logit(\\star) = \\ln \\bigg( \\frac{\\star}{1-\\star} \\bigg) \\ \\ \\ \\ 0 &lt; \\star &lt; 1 \\end{eqnarray*}\\] 5.1.1.1 model assumptions Just like in linear regression, our Y response is the only random component. \\[\\begin{eqnarray*} y &amp;=&amp; \\begin{cases} 1 &amp; \\mbox{ died}\\\\ 0 &amp; \\mbox{ survived} \\end{cases} \\end{eqnarray*}\\] \\[\\begin{eqnarray*} Y &amp;\\sim&amp; \\mbox{Bernoulli}(p)\\\\ P(Y=y) &amp;=&amp; p^y(1-p)^{1-y} \\end{eqnarray*}\\] When each person is at risk for a different covariate (i.e., explanatory variable), they each end up with a different probability of success. \\[\\begin{eqnarray*} Y_i \\sim \\mbox{Bernoulli} \\bigg( p(x_i) = \\frac{e^{\\beta_0 + \\beta_1 x_i}}{1+ e^{\\beta_0 + \\beta_1 x_i}}\\bigg) \\end{eqnarray*}\\] independent trials success / failure probability of success is constant for a particular \\(X\\). \\(E[Y|x] = p(x)\\) is given by the logistic function 5.1.1.2 interpreting coefficients Let’s say the log odds of survival for given observed (log) burn areas \\(x\\) and \\(x+1\\) are: \\[\\begin{eqnarray*} \\logit(p(x)) &amp;=&amp; \\beta_0 + \\beta_1 x\\\\ \\logit(p(x+1)) &amp;=&amp; \\beta_0 + \\beta_1 (x+1)\\\\ \\beta_1 &amp;=&amp; \\logit(p(x+1)) - \\logit(p(x))\\\\ &amp;=&amp; \\ln \\bigg(\\frac{p(x+1)}{1-p(x+1)} - \\frac{p(x)}{1-p(x)} \\bigg)\\\\ &amp;=&amp; \\ln \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\ e^{\\beta_1} &amp;=&amp; \\bigg( \\frac{p(x+1) / [1-p(x+1)]}{p(x) / [1-p(x)]} \\bigg)\\\\ \\end{eqnarray*}\\] \\(e^{\\beta_1}\\) is the odds ratio for dying associated with a one unit increase in x. [\\(\\beta_1\\) is the change in log-odds associated with a one unit increase in x. \\[\\begin{eqnarray*} \\logit (\\hat{p}) = 22.708 - 10.662 \\cdot \\ln(\\mbox{ area }+1). \\end{eqnarray*}\\] (Suppose we are interested in comparing the odds of surviving third-degree burns for patients with burns corresponding to log(area +1)= 1.90, and patients with burns corresponding to log(area +1)= 2.00. The odds ratio \\(\\hat{OR}_{1.90, 2.00}\\) is given by \\[\\begin{eqnarray*} \\hat{OR}_{1.90, 2.00} = e^{-10.662} (1.90-2.00) = e^{1.0662} = 2.904 \\end{eqnarray*}\\] That is, the odds of survival for a patient with log(area+1)= 1.90 is 2.9 times higher than the odds of survival for a patient with log(area+1)= 2.0.) What about the RR (relative risk) or difference in risks? Note, it won’t be constant for a given \\(X\\), so it must be calculated as a function of \\(X\\). 5.1.2 constant OR, varying RR The previous model specifies that the OR is constant for any value of \\(X\\) which is not true about RR. Using the burn data, convince yourself that the RR isn’t constant. Try computing the RR at 1.5 versus 2.5, then again at 1 versus 2. \\[\\begin{eqnarray*} \\logit (\\hat{p}) &amp;=&amp; 22.708 - 10.662 \\cdot \\ln(\\mbox{ area }+1)\\\\ \\hat{p(x)} &amp;=&amp; \\frac{e^{22.708 - 10.662 x}}{1+e^{22.708 - 10.662 x}}\\\\ \\end{eqnarray*}\\] \\[\\begin{eqnarray*} \\hat{p}(1) &amp;=&amp; 0.9999941\\\\ \\hat{p}(1.5) &amp;=&amp; 0.9987889\\\\ \\hat{p}(2) &amp;=&amp; 0.7996326\\\\ \\hat{p}(2.5) &amp;=&amp; 0.01894664\\\\ \\hat{RR}_{1, 2} &amp;=&amp; 1.250567\\\\ \\hat{RR}_{1.5, 2.5} &amp;=&amp; 52.71587\\\\ \\end{eqnarray*}\\] \\[\\begin{eqnarray*} \\hat{RR} &amp;=&amp; \\frac{\\frac{e^{b_0 + b_1 x}}{1+e^{b_0 + b_1 x}}}{\\frac{e^{b_0 + b_1 (x+1)}}{1+e^{b_0 + b_1 (x+1)}}}\\\\ &amp;=&amp; \\frac{\\frac{e^{b_0}e^{b_1 x}}{1+e^{b_0}e^{b_1 x}}}{\\frac{e^{b_0} e^{b_1 x} e^{b_1}}{1+e^{b_0}e^{b_1 x} e^{b_1}}}\\\\ &amp;=&amp; \\frac{1+e^{b_0}e^{b_1 x}e^{b_1}}{e^{b_1}(1+e^{b_0}e^{b_1 x})}\\\\ \\end{eqnarray*}\\] (see log-linear model below, 5.1.2.1 ) 5.1.2.1 Alternative strategies for binary outcomes It is quite common to have binary outcomes (response variable) in the medical literature. However, the logit link (logistic regression) is only one of a variety of models that we can use. We see above that the logistic model imposes a constant OR for any value of \\(X\\) (and not a constant RR). complementary log-log The complementary log-log model is used when you have a rate of, for example, infection, model by instances of contact (based on a Poisson model). \\[\\begin{eqnarray*} p(k) &amp;=&amp; 1-(1-\\lambda)^k\\\\ \\ln[ - \\ln (1-p(k))] &amp;=&amp; \\ln[-\\ln(1-\\lambda)] + \\ln(k)\\\\ \\ln[ - \\ln (1-p(k))] &amp;=&amp; \\beta_0 + 1 \\cdot \\ln(k)\\\\ \\ln[ - \\ln (1-p(k))] &amp;=&amp; \\beta_0 + \\beta_1 x\\\\ p(x) &amp;=&amp; 1 - \\exp [ -\\exp(\\beta_0 + \\beta_1 x) ] \\end{eqnarray*}\\] linear The excess (or additive) risk model can modeled by using simple linear regression: \\[\\begin{eqnarray*} p(x) &amp;=&amp; \\beta_0 + \\beta_1 x \\end{eqnarray*}\\] which we have already seen is problematic for a variety of reasons. Note, however, any unit increase in \\(x\\) gives a \\(\\beta_1\\) increase in the risk (for all values of \\(x\\)). log-linear As long as we do not have a case-control study, we can model the risk using a log-linear model. \\[\\begin{eqnarray*} \\ln (p(x)) = \\beta_0 + \\beta_1 x \\end{eqnarray*}\\] The regression coefficient, \\(\\beta_1\\), has the interpretation of the logarithm of the relative risk associated with a unit increase in \\(x\\). Although many software programs will fit this model, it may present numerical difficulties because of the constraint that the sum of terms on the right-hand side must be no greater than zero for the results to make sense (due to the constraint that the outcome probability p(x) must be in the interval [0,1]). As a result, convergence of standard fitting algorithms may be unreliable in some cases. 5.2 Estimating coefficients in logistic regression 5.2.1 Maximum Likelihood Estimation Recall how we estimated the coefficients for linear regression. We minimized the residual sum of squares: \\[\\begin{eqnarray*} RSS &amp;=&amp; \\sum_i (Y_i - \\hat{Y}_i)^2\\\\ &amp;=&amp; \\sum_i (Y_i - (b_0 + b_1 X_i))^2 \\end{eqnarray*}\\] That is, we take derivatives with respect to both \\(b_0\\) and \\(b_1\\), set them equal to zero (take second derivatives to ensure minimums), and solve for \\(b_0\\) and \\(b_1\\). It turns out that we’ve also maximized the normal likelihood. \\[\\begin{eqnarray*} L(\\underline{y} | b_0, b_1, \\underline{x}) &amp;=&amp; \\prod_i \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{(y_i - b_0 - b_1 x_i)^2 / 2 \\sigma}\\\\ &amp;=&amp; \\bigg( \\frac{1}{2 \\pi \\sigma^2} \\bigg)^{n/2} e^{\\sum_i (y_i - b_0 - b_1 x_i)^2 / 2 \\sigma}\\\\ \\end{eqnarray*}\\] What does that even mean? Likelihood? Maximizing the likelihood? WHY??? The likelihood is the probability distribution of the data given specific values of the unknown parameters. Consider a toy example describing, for example, flipping coins. Let’s say \\(X \\sim Bin(p, n=4).\\) We have 4 trials and \\(X=1\\). Would you guess \\(p=0.49\\)?? No, you would guess \\(p=0.25\\)… you maximized the likelihood of seeing your data. \\[\\begin{eqnarray*} P(X=1 | p) &amp;=&amp; {4 \\choose 1} p^1 (1-p)^{4-1}\\\\ P(X=1 | p = 0.9) &amp;=&amp; 0.0036 \\\\ P(X=1 | p = 0.75) &amp;=&amp; 0.047 \\\\ P(X=1 | p = 0.5) &amp;=&amp; 0.25\\\\ P(X=1 | p = 0.05) &amp;=&amp; 0.171\\\\ P(X=1 | p = 0.15) &amp;=&amp; 0.368\\\\ P(X=1 | p = 0.25) &amp;=&amp; 0.422\\\\ \\end{eqnarray*}\\] Or, we can think about it as a set of independent binary responses, \\(Y_1, Y_2, \\ldots Y_n\\). Since each observed response is independent and follows the Bernoulli distribution, the probability of a particular outcome can be found as: \\[\\begin{eqnarray*} P(Y_1=y_1, Y_2=y_2, \\ldots, Y_n=y_n) &amp;=&amp; P(Y_1=y_1) P(Y_2 = y_2) \\cdots P(Y_n = y_n)\\\\ &amp;=&amp; p^{y_1}(1-p)^{1-y_1} p^{y_2}(1-p)^{1-y_2} \\cdots p^{y_n}(1-p)^{1-y_n}\\\\ &amp;=&amp; p^{\\sum_i y_i} (1-p)^{\\sum_i (1-y_i)}\\\\ \\end{eqnarray*}\\] where \\(y_1, y_2, \\ldots, y_n\\) represents a particular observed series of 0 or 1 outcomes and \\(p\\) is a probability \\(0 \\leq p \\leq 1\\). Once \\(y_1, y_2, \\ldots, y_n\\) have been observed, they are fixed values. Maximum likelihood estimates are functions of sample data that are derived by finding the value of \\(p\\) that maximizes the likelihood functions. To maximize the likelihood, we use the natural log of the likelihood (because we know we’ll get the same answer): \\[\\begin{eqnarray*} \\ln L(p) &amp;=&amp; \\ln \\Bigg(p^{\\sum_i y_i} (1-p)^{\\sum_i (1-y_i)} \\Bigg)\\\\ &amp;=&amp; \\sum_i y_i \\ln(p) + (n- \\sum_i y_i) \\ln (1-p)\\\\ \\frac{ \\partial \\ln L(p)}{\\partial p} &amp;=&amp; \\sum_i y_i \\frac{1}{p} + (n - \\sum_i y_i) \\frac{-1}{(1-p)} = 0\\\\ 0 &amp;=&amp; (1-p) \\sum_i y_i + p (n-\\sum_i y_i) \\\\ \\hat{p} &amp;=&amp; \\frac{ \\sum_i y_i}{n} \\end{eqnarray*}\\] Note that when we use the logistic regression model, our likelihood is substantially more complicated because the probability of success changes for each individual. Recall: \\[\\begin{eqnarray*} p_i = p(x_i) &amp;=&amp; \\frac{e^{b_0 + b_1 x_i}}{1+e^{b_0 + b_1 x_i}} \\end{eqnarray*}\\] which gives a likelihood of: \\[\\begin{eqnarray*} L(\\underline{p}) &amp;=&amp; \\prod_i \\Bigg( \\frac{e^{b_0 + b_1 x_i}}{1+e^{b_0 + b_1 x_i}} \\Bigg)^{y_i} \\Bigg(1-\\frac{e^{b_0 + b_1 x_i}}{1+e^{b_0 + b_1 x_i}} \\Bigg)^{(1- y_i)} \\\\ \\mbox{&amp; a loglikelihood of}: &amp;&amp;\\\\ \\ln L(\\underline{p}) &amp;=&amp; \\sum_i y_i \\ln\\Bigg( \\frac{e^{b_0 + b_1 x_i}}{1+e^{b_0 + b_1 x_i}} \\Bigg) + (1- y_i) \\ln \\Bigg(1-\\frac{e^{b_0 + b_1 x_i}}{1+e^{b_0 + b_1 x_i}} \\Bigg)\\\\ \\end{eqnarray*}\\] Why use maximum likelihood estimates? Estimates are essentially unbiased. We can estimate the SE (Wald estimates via Fisher Information). The estimates have low variability. The estimates have an approximately normal sampling distribution for large sample sizes because they are maximum likelihood estimates. Though it is important to note that we cannot find estimates in closed form. 5.3 Formal Inference 5.3.1 Wald Tests &amp; Intervals Because we will use maximum likelihood parameter estimates, we can also use large sample theory to find the SEs and consider the estimates to have normal distributions (for large sample sizes). However, (Menard 1995) warns that for large coefficients, standard error is inflated, lowering the Wald statistic (chi-square) value. (Agresti 1996) states that the likelihood-ratio test is more reliable for small sample sizes than the Wald test. \\[\\begin{eqnarray*} z = \\frac{b_1 - \\beta_1}{SE(b_1)} \\end{eqnarray*}\\] library(tidyverse); library(broom) glm(burnresp~burnexpl, data = burnglm, family=&quot;binomial&quot;) ## ## Call: glm(formula = burnresp ~ burnexpl, family = &quot;binomial&quot;, data = burnglm) ## ## Coefficients: ## (Intercept) burnexpl ## 22.71 -10.66 ## ## Degrees of Freedom: 434 Total (i.e. Null); 433 Residual ## Null Deviance: 525.4 ## Residual Deviance: 335.2 AIC: 339.2 glm(burnresp~burnexpl, data = burnglm, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 22.7 2.27 10.0 1.23e-23 ## 2 burnexpl -10.7 1.08 -9.85 6.95e-23 5.3.2 Likelihood Ratio Tests \\(\\frac{L(p_0)}{L(\\hat{p})}\\) gives us a sense of whether the null value or the observed value produces a higher likelihood. Recall: \\[\\begin{eqnarray*} L(\\hat{\\underline{p}}) &gt; L(p_0) \\end{eqnarray*}\\] always. [Where \\(\\hat{\\underline{p}}\\) is the maximum likelihood estimate for the probability of success (here it will be a vector of probabilities, each based on the same MLE estimates of the linear parameters). ] The above inequality holds because \\(\\hat{\\underline{p}}\\) maximizes the likelihood. We can show that if \\(H_0\\) is true, \\[\\begin{eqnarray*} -2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg) \\sim \\chi^2_1 \\end{eqnarray*}\\] If we are testing only one parameter value. More generally, \\[\\begin{eqnarray*} -2 \\ln \\bigg( \\frac{\\max L_0}{\\max L} \\bigg) \\sim \\chi^2_\\nu \\end{eqnarray*}\\] where \\(\\nu\\) is the number of extra parameters we estimate using the unconstrained likelihood (as compared to the constrained null likelihood). Example 4.2 Consider a data set with 147 people. 49 got cancer and 98 didn’t. Let’s test whether the true proportion of people who get cancer is \\(p=0.25\\). \\[\\begin{eqnarray*} H_0:&amp;&amp; p=0.25\\\\ H_1:&amp;&amp; p \\ne 0.25\\\\ \\hat{p} &amp;=&amp; \\frac{49}{147}\\\\ -2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg) &amp;=&amp; -2 [ \\ln (L(p_0)) - \\ln(L(\\hat{p}))]\\\\ &amp;=&amp; -2 \\Bigg[ \\ln \\bigg( (0.25)^{y} (0.75)^{n-y} \\bigg) - \\ln \\Bigg( \\bigg( \\frac{y}{n} \\bigg)^{y} \\bigg( \\frac{(n-y)}{n} \\bigg)^{n-y} \\Bigg) \\Bigg]\\\\ &amp;=&amp; -2 \\Bigg[ \\ln \\bigg( (0.25)^{49} (0.75)^{98} \\bigg) - \\ln \\Bigg( \\bigg( \\frac{1}{3} \\bigg)^{49} \\bigg( \\frac{2}{3} \\bigg)^{98} \\Bigg) \\Bigg]\\\\ &amp;=&amp; -2 [ \\ln(0.0054) - \\ln(0.0697) ] = 5.11\\\\ P( \\chi^2_1 \\geq 5.11) &amp;=&amp; 0.0238 \\end{eqnarray*}\\] But really, usually likelihood ratio tests are more interesting. In fact, usually, we use them to test whether the coefficients are zero: \\[\\begin{eqnarray*} H_0: &amp;&amp; \\beta_1 =0\\\\ H_1: &amp;&amp; \\beta_1 \\ne 0\\\\ p_0 &amp;=&amp; \\frac{e^{\\hat{\\beta}_0}}{1 + e^{\\hat{\\beta}_0}} \\end{eqnarray*}\\] where \\(\\hat{\\beta}_0\\) is fit from a model without any explanatory variable, \\(x\\). Important note: \\[\\begin{eqnarray*} \\mbox{deviance} = \\mbox{constant} - 2 \\ln(\\mbox{likelihood}) \\end{eqnarray*}\\] That is, the difference in log likelihoods will be the opposite difference in deviances: \\[\\begin{eqnarray*} \\mbox{test stat} &amp;=&amp; \\chi^2\\\\ &amp;=&amp; -2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg)\\\\ &amp;=&amp; -2 [ \\ln(L(p_0)) - \\ln(L(\\hat{p})) ]\\\\ &amp;=&amp; \\mbox{deviance}_0 - \\mbox{deviance}_{model}\\\\ &amp;=&amp; \\mbox{deviance}_{null} - \\mbox{deviance}_{residual}\\\\ &amp;=&amp; \\mbox{deviance}_{reduced} - \\mbox{deviance}_{full}\\\\ \\end{eqnarray*}\\] summary(glm(burnresp~burnexpl, data = burnglm, family=&quot;binomial&quot;)) ## ## Call: ## glm(formula = burnresp ~ burnexpl, family = &quot;binomial&quot;, data = burnglm) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.8518 -0.6998 0.1859 0.5239 2.2089 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 22.708 2.266 10.021 &lt;2e-16 *** ## burnexpl -10.662 1.083 -9.849 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 525.39 on 434 degrees of freedom ## Residual deviance: 335.23 on 433 degrees of freedom ## AIC: 339.23 ## ## Number of Fisher Scoring iterations: 6 glm(burnresp~burnexpl, data = burnglm, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 22.7 2.27 10.0 1.23e-23 ## 2 burnexpl -10.7 1.08 -9.85 6.95e-23 glm(burnresp~burnexpl, data = burnglm, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 525.386 434 -167.616 339.231 347.382 335.231 433 \\[\\begin{eqnarray*} \\mbox{test stat} &amp;=&amp; G\\\\ &amp;=&amp; -2 \\ln \\bigg( \\frac{L(p_0)}{L(\\hat{p})} \\bigg)\\\\ &amp;=&amp; -2 [ \\ln(L(p_0)) - \\ln(L(\\hat{p})) ]\\\\ &amp;=&amp; \\mbox{deviance}_0 - \\mbox{deviance}_{model}\\\\ &amp;=&amp; \\mbox{deviance}_{null} - \\mbox{deviance}_{residual}\\\\ &amp;=&amp; \\mbox{deviance}_{reduced} - \\mbox{deviance}_{full}\\\\ \\end{eqnarray*}\\] So, the LRT here is (see columns of null deviance and deviance): \\[\\begin{eqnarray*} G &amp;=&amp; 525.39 - 335.23 = 190.16\\\\ p-value &amp;=&amp; P(\\chi^2_1 \\geq 190.16) = 0 \\end{eqnarray*}\\] 5.3.2.1 modeling categorical predictors with multiple levels 5.3.2.1.1 Snoring A study was undertaken to investigate whether snoring is related to a heart disease. In the survey, 2484 people were classified according to their proneness to snoring (never, occasionally, often, always) and whether or not they had the heart disease. Variable Description disease (response variable) Binary variable: having disease=1, not having disease=0 snoring (explanatory variable) Categorical variable indicating level of snoring (never=1, occasionally=2, often=3 and always=4) Source: (Norton and Dunn 1985) \\[\\begin{eqnarray*} X_1 = \\begin{cases} 1 &amp; \\text{for occasionally} \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{cases} X_2 = \\begin{cases} 1 &amp; \\text{for often} \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{cases} X_3 = \\begin{cases} 1 &amp; \\text{for always} \\\\ 0 &amp; \\text{otherwise} \\\\ \\end{cases} \\end{eqnarray*}\\] Our new model becomes: \\[\\begin{eqnarray*} \\logit(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 \\end{eqnarray*}\\] We can use the drop-in-deviance test to test the effect of any or all of the parameters (of which there are now four) in the model. See the birdnest example, 5.9 5.4 Multiple Logistic Regression 5.4.1 Interaction Another worry when building models with multiple explanatory variables has to do with variables interacting. That is, for one level of a variable, the relationship of the main predictor on the response is different. Example 4.3 Consider a simple linear regression model on number of hours studied and exam grade. Then addclass year to the model. Note that you would probably have a different slope for each class year in order to model the two variables most effectively. For simplicity, consider only first year students and seniors. \\[\\begin{eqnarray*} E[\\mbox{grade seniors}| \\mbox{hours studied}] &amp;=&amp; \\beta_{0s} + \\beta_{1s} \\mbox{hrs}\\\\ E[\\mbox{grade first years}| \\mbox{hours studied}] &amp;=&amp; \\beta_{0f} + \\beta_{1f} \\mbox{hrs}\\\\ E[\\mbox{grade}| \\mbox{hours studied}] &amp;=&amp; \\beta_{0} + \\beta_{1} \\mbox{hrs} + \\beta_2 I(\\mbox{year=senior}) + \\beta_{3} \\mbox{hrs} I(\\mbox{year = senior})\\\\ \\beta_{0f} &amp;=&amp; \\beta_{0}\\\\ \\beta_{0s} &amp;=&amp; \\beta_0 + \\beta_2\\\\ \\beta_{1f} &amp;=&amp; \\beta_1\\\\ \\beta_{1s} &amp;=&amp; \\beta_1 + \\beta_3 \\end{eqnarray*}\\] Why do we need the \\(I(\\mbox{year=seniors})\\) variable? Definition 4.3 Interaction means that the effect of an explanatory variable on the outcome differs according to the level of another explanatory variable. (Not the case with age on smoking and lung cancer above. With the smoking example, age is a significant variable, but it does not interact with lung cancer.) Example 5.2 The Heart and Estrogen/progestin Replacement Study (HERS) is a randomized, double-blind, placebo-controlled trial designed to test the efficacy and safety of estrogen plus progestin therapy for prevention of recurrent coronary heart disease (CHD) events in women. The participants are postmenopausal women with a uterus and with CHD. Each woman was randomly assigned to receive one tablet containing 0.625 mg conjugated estrogens plus 2.5 mg medroxyprogesterone acetate daily or an identical placebo. The results of the first large randomized clinical trial to examine the effect of hormone replacement therapy (HRT) on women with heart disease appeared in JAMA in 1998 (Hulley et al. 1998). The Heart and Estrogen/Progestin Replacement Study (HERS) found that the use of estrogen plus progestin in postmenopausal women with heart disease did not prevent further heart attacks or death from coronary heart disease (CHD). This occurred despite the positive effect of treatment on lipoproteins: LDL (bad) cholesterol was reduced by 11 percent and HDL (good) cholesterol was increased by 10 percent. The hormone replacement regimen also increased the risk of clots in the veins (deep vein thrombosis) and lungs (pulmonary embolism). The results of HERS are surprising in light of previous observational studies, which found lower rates of CHD in women who take postmenopausal estrogen. Data available at: http://www.biostat.ucsf.edu/vgsm/data/excel/hersdata.xls For now, we will try to predict whether the individuals had a pre-existing medical condition (other than CHD, self reported), medcond. We will use the variables age, weight, diabetes and drinkany. HERS &lt;- read.table(&quot;~/Dropbox/teaching/math150/HERS.csv&quot;, sep=&quot;,&quot;, header=T, na.strings=&quot;.&quot;) glm(medcond ~ age, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.60 0.401 -4.00 0.0000624 ## 2 age 0.0162 0.00597 2.71 0.00664 glm(medcond ~ age + weight, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.17 0.496 -4.37 0.0000124 ## 2 age 0.0189 0.00613 3.09 0.00203 ## 3 weight 0.00528 0.00274 1.93 0.0542 glm(medcond ~ age+diabetes, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.89 0.408 -4.64 0.00000349 ## 2 age 0.0185 0.00603 3.07 0.00217 ## 3 diabetes 0.487 0.0882 5.52 0.0000000330 glm(medcond ~ age*diabetes, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.52 0.478 -5.26 0.000000141 ## 2 age 0.0278 0.00707 3.93 0.0000844 ## 3 diabetes 2.83 0.914 3.10 0.00192 ## 4 age:diabetes -0.0354 0.0137 -2.58 0.00986 glm(medcond ~ age*drinkany, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.991 0.511 -1.94 0.0526 ## 2 age 0.00885 0.00759 1.17 0.244 ## 3 drinkany -1.44 0.831 -1.73 0.0833 ## 4 age:drinkany 0.0168 0.0124 1.36 0.175 Write out a few models by hand, does any of the significance change with respect to interaction? Does the interpretation change with interaction? In the last model, we might want to remove all the age information. Age seems to be less important than drinking status. How do we decide? How do we model? 5.4.2 Simpson’s Paradox Simpson’s paradox is when the association between two variables is opposite the partial association between the same two variables after controlling for one or more other variables. Example 4.5 Back to linear regression to consider Simpson’s Paradox. Consider data on SAT scores across different states with information on educational expenditure. Note that the correlation between SAT score and average teacher salary is negative with the combined data. However, SAT score and average teacher salary is positive after controlling for the fraction of students who take the exam. Note that the fewer students who take the exam, the higher the SAT score. That’s because states whose public universities encourage the ACT have SAT-takers who are leaving the state for college (with their higher SAT scores). ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1159. 57.7 20.1 5.13e-25 ## 2 salary -5.54 1.63 -3.39 1.39e- 3 ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 988. 31.9 31.0 6.20e-33 ## 2 salary 2.18 1.03 2.12 3.94e- 2 ## 3 frac -2.78 0.228 -12.2 4.00e-16 Example 5.3 Consider the example on smoking and 20-year mortality (case) from section 3.4 of Regression Methods in Biostatistics, pg 52-53. age test smoker nonsmoker prob smoke odds smoke empirical OR all case 139 230 0.377 0.604 0.685 control 443 502 0.469 0.882 18-44 case 61 32 0.656 1.906 1.627 control 375 320 0.540 1.172 45-64 case 34 66 0.340 0.515 1.308 control 50 127 0.282 0.394 65+ case 44 132 0.250 0.333 1.019 control 18 55 0.247 0.327 What we see is that the vast majority of the controls were young, and they had a high rate of smoking. A good chunk of the cases were older, and the rate of smoking was substantially lower in the oldest group. However, within each group, the cases were more likely to smoke than the controls. After adjusting for age, smoking is no longer significant. But more importantly, age is a variable that reverses the effect of smoking on cancer - Simpson’s Paradox. Note that the effect is not due to the observational nature of the study, and so it is important to adjust for possible influential variables regardless of the study at hand. What would it mean to adjust for age in this context? It means that we have to include it in the model: death &lt;- c(rep(1,93),rep(0,695), rep(1,100),rep(0,177), rep(1,176), rep(0,73)) smoke &lt;- c(rep(1,61), rep(0,32), rep(1,375), rep(0,320), rep(1,34), rep(0,66), rep(1,50), rep(0,127), rep(1,44), rep(0,132), rep(1,18), rep(0,55)) age &lt;- c(rep(&quot;young&quot;, 788), rep(&quot;middle&quot;, 277), rep(&quot;old&quot;, 249) ) glm( death ~ smoke, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.781 0.0796 -9.80 1.10e-22 ## 2 smoke -0.379 0.126 -3.01 2.59e- 3 glm( death ~ as.factor(age), family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.571 0.125 -4.56 5.01e- 6 ## 2 as.factor(age)old 1.45 0.187 7.75 9.00e-15 ## 3 as.factor(age)young -1.44 0.167 -8.63 6.02e-18 glm( death ~ smoke + as.factor(age), family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.668 0.135 -4.96 7.03e- 7 ## 2 smoke 0.312 0.154 2.03 4.25e- 2 ## 3 as.factor(age)old 1.47 0.188 7.84 4.59e-15 ## 4 as.factor(age)young -1.52 0.173 -8.81 1.26e-18 glm( death ~ smoke * as.factor(age), family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.655 0.152 -4.31 1.61e- 5 ## 2 smoke 0.269 0.269 0.999 3.18e- 1 ## 3 as.factor(age)old 1.53 0.221 6.93 4.29e-12 ## 4 as.factor(age)young -1.65 0.240 -6.88 6.00e-12 ## 5 smoke:as.factor(age)old -0.251 0.420 -0.596 5.51e- 1 ## 6 smoke:as.factor(age)young 0.218 0.355 0.614 5.40e- 1 Using the additive model above: \\[\\begin{eqnarray*} \\logit (p(x_1, x_2) ) &amp;=&amp; \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\\\ OR &amp;=&amp; \\mbox{odds dying if } (x_1, x_2) / \\mbox{odds dying if } (x_1^*, x_2^*) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}{e^{\\beta_0 + \\beta_1 x_1^* + \\beta_2 x_2^*}}\\\\ x_1 &amp;=&amp; \\begin{cases} 0 &amp; \\mbox{ don&#39;t smoke}\\\\ 1 &amp; \\mbox{ smoke}\\\\ \\end{cases}\\\\ x_2 &amp;=&amp; \\begin{cases} \\mbox{young} &amp; \\mbox{18-44 years old}\\\\ \\mbox{middle} &amp; \\mbox{45-64 years old}\\\\ \\mbox{old} &amp; \\mbox{65+ years old}\\\\ \\end{cases} \\end{eqnarray*}\\] where we are modeling the probability of 20-year mortality using smoking status and age group. Note 1: We can see from above that the coefficients for each variable are significantly different from zero. That is, the variables are important in predicting odds of survival. Note 2: We can see that smoking becomes less significant as we add age into the model. That is because age and smoking status are so highly associated (think of the coin example). Note 3: We can estimate any of the OR (of dying for smoke vs not smoke) from the given coefficients: \\[\\begin{eqnarray*} \\mbox{simple model} &amp;&amp;\\\\ \\mbox{overall OR} &amp;=&amp; e^{-0.37858 } = 0.6848332\\\\ &amp;&amp; \\\\ \\mbox{additive model} &amp;&amp;\\\\ \\mbox{young, middle, old OR} &amp;=&amp; e^{ 0.3122} = 1.3664\\\\ &amp;&amp; \\\\ \\mbox{interaction model} &amp;&amp;\\\\ \\mbox{young OR} &amp;=&amp; e^{0.2689 + 0.2177} = 1.626776\\\\ \\mbox{middle OR} &amp;=&amp; e^{0.2689} = 1.308524\\\\ \\mbox{old OR} &amp;=&amp; e^{0.2689 + -0.2505} = 1.018570\\\\ \\end{eqnarray*}\\] What does it mean that the interaction terms are not significant in the last model? 5.5 Multicolinearity Consider the following data set collected from church offering plates in 62 consecutive Sundays. Also noted is whether there was enough change to buy a candy bar for $1.25. glm(CandyYes ~ Coins, data = Offering, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -4.14 0.996 -4.16 0.0000321 ## 2 Coins 0.286 0.0772 3.70 0.000213 glm(CandyYes ~ Small, data = Offering, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.33 0.585 -3.98 0.0000693 ## 2 Small 0.184 0.0576 3.19 0.00142 glm(CandyYes ~ Coins + Small, data = Offering, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -17.0 7.80 -2.18 0.0296 ## 2 Coins 3.49 1.75 1.99 0.0461 ## 3 Small -3.04 1.57 -1.93 0.0531 Notice that the directionality of the low coins changes when it is included in the model that already contains the number of coins total. Lesson of the story: be very very very careful interpreting coefficients when you have multiple explanatory variables. 5.6 Model Building Example 5.4 Suppose that you have to take an exam that covers 100 different topics, and you do not know any of them. The rules, however, state that you can bring two classmates as consultants. Suppose also that you know which topics each of your classmates is familiar with. If you could bring only one consultant, it is easy to figure out who you would bring: it would be the one who knows the most topics (the variable most associated with the answer). Let’s say this is Sage who knows 85 topics. With two consultants you might choose Sage first, and for the second option, it seems reasonable to choose the second most knowledgeable classmate (the second most highly associated variable), for example Bruno, who knows 75 topics. The problem with this strategy is that it may be that the 75 subjects Bruno knows are already included in the 85 that Sage knows, and therefore, Bruno does not provide any knowledge beyond that of Sage. A better strategy is to select the second not by considering what he or she knows regarding the entire agenda, but by looking for the person who knows more about the topics than the first does not know (the variable that best explains the residual of the equation with the variables entered). It may even happen that the best pair of consultants are not the most knowledgeable, as there may be two that complement each other perfectly in such a way that one knows 55 topics and the other knows the remaining 45, while the most knowledgeable does not complement anybody. 5.6.1 Formal Model Building We are going to discuss how to add (or subtract) variables from a model. Before we do that, we can define two criteria used for suggesting an optimal model. AIC: Akaike’s Information Criteria = \\(-2 \\ln\\) likelihood + \\(2p\\) BIC: Bayesian Information Criteria = \\(-2 \\ln\\) likelihood \\(+p \\ln(n)\\) Both techniques suggest choosing a model with the smallest AIC and BIC value; both adjust for the number of parameters in the model and are more likely to select models with fewer variables than the drop-in-deviance test. 5.6.1.1 Stepwise Regression As done previously, we can add and remove variables based on the deviance. Recall, when comparing two nested models, the differences in the deviances can be modeled by a \\(\\chi^2_\\nu\\) variable where \\(\\nu = \\Delta p\\). Consider the HERS data described in your book (page 30); variable description also given on the book website http://www.epibiostat.ucsf.edu/biostat/vgsm/data/hersdata.codebook.txt For now, we will try to predict whether the individuals had a medical condition, medcond (defined as a pre-existing and self-reported medical condition). We will use the variables age, weight, diabetes and drinkany. HERS &lt;- read.table(&quot;~/Dropbox/teaching/math150/HERS.csv&quot;, sep=&quot;,&quot;, header=T, na.strings=&quot;.&quot;) glm(medcond ~ age, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.60 0.401 -4.00 0.0000624 ## 2 age 0.0162 0.00597 2.71 0.00664 glm(medcond ~ age + weight, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.17 0.496 -4.37 0.0000124 ## 2 age 0.0189 0.00613 3.09 0.00203 ## 3 weight 0.00528 0.00274 1.93 0.0542 glm(medcond ~ age+diabetes, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.89 0.408 -4.64 0.00000349 ## 2 age 0.0185 0.00603 3.07 0.00217 ## 3 diabetes 0.487 0.0882 5.52 0.0000000330 glm(medcond ~ age*diabetes, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.52 0.478 -5.26 0.000000141 ## 2 age 0.0278 0.00707 3.93 0.0000844 ## 3 diabetes 2.83 0.914 3.10 0.00192 ## 4 age:diabetes -0.0354 0.0137 -2.58 0.00986 glm(medcond ~ age*drinkany, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.991 0.511 -1.94 0.0526 ## 2 age 0.00885 0.00759 1.17 0.244 ## 3 drinkany -1.44 0.831 -1.73 0.0833 ## 4 age:drinkany 0.0168 0.0124 1.36 0.175 5.6.1.1.1 Forward Selection One idea is to start with an empty model and adding the best available variable at each iteration, checking for needs for transformations. We should also look at interactions which we might suspect. However, looking at all possible interactions (if only 2-way interactions, we could also consider 3-way interactions etc.), things can get out of hand quickly. We start with the response variable versus all variables and find the best predictor. If there are too many, we might just look at the correlation matrix. However, we may miss out of variables that are good predictors but aren’t linearly related. Therefore, if its possible, a scatter plot matrix would be best. We locate the best variable, and regress the response variable on it. If the variable seems to be useful, we keep it and move on to looking for a second. If not, we stop. 5.6.1.1.2 Forward Stepwise Selection This method follows in the same way as Forward Regression, but as each new variable enters the model, we check to see if any of the variables already in the model can now be removed. This is done by specifying two values, \\(\\alpha_e\\) as the \\(\\alpha\\) level needed to enter the model, and \\(\\alpha_l\\) as the \\(\\alpha\\) level needed to leave the model. We require that \\(\\alpha_e&lt;\\alpha_l\\), otherwise, our algorithm could cycle, we add a variable, then immediately decide to delete it, continuing ad infinitum. This is bad. We start with the empty model, and add the best predictor, assuming the p-value associated with it is smaller than \\(\\alpha_e\\). Now, we find the best of the remaining variables, and add it if the p-value is smaller than \\(\\alpha_e\\). If we add it, we also check to see if the first variable can be dropped, by calculating the p-value associated with it (which is different from the first time, because now there are two variables in the model). If its p-value is greater than \\(\\alpha_l\\), we remove the variable. We continue with this process until there are no more variables that meet either requirements. In many situations, this will help us from stopping at a less than desirable model. How do you choose the \\(\\alpha\\) values? If you set \\(\\alpha_e\\) to be very small, you might walk away with no variables in your model, or at least not many. If you set it to be large, you will wander around for a while, which is a good thing, because you will explore more models, but you may end up with variables in your model that aren’t necessary. 5.6.1.1.3 Backward Selection Start with the full model including every term (and possibly every interaction, etc.). Remove the variable that is least significant (biggest p-value) in the model. Continue removing variables until all variables are significant at the chosen \\(\\alpha\\) level. glm(medcond ~ (age + diabetes + weight + drinkany)^2, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 11 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.11 2.16 -0.512 0.609 ## 2 age 0.00851 0.0317 0.269 0.788 ## 3 diabetes 1.89 1.17 1.61 0.107 ## 4 weight -0.0143 0.0290 -0.492 0.623 ## 5 drinkany -0.587 1.08 -0.546 0.585 ## 6 age:diabetes -0.0304 0.0148 -2.06 0.0395 ## 7 age:weight 0.000208 0.000429 0.486 0.627 ## 8 age:drinkany 0.00734 0.0132 0.557 0.578 ## 9 diabetes:weight 0.00787 0.00624 1.26 0.207 ## 10 diabetes:drinkany -0.136 0.205 -0.663 0.507 ## 11 weight:drinkany -0.00161 0.00614 -0.262 0.793 glm(medcond ~ age + diabetes + weight + drinkany, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 5 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.87 0.505 -3.72 0.000203 ## 2 age 0.0184 0.00620 2.96 0.00304 ## 3 diabetes 0.432 0.0924 4.68 0.00000288 ## 4 weight 0.00143 0.00285 0.500 0.617 ## 5 drinkany -0.253 0.0835 -3.03 0.00248 glm(medcond ~ age + diabetes + drinkany, data = HERS, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.72 0.413 -4.17 0.0000300 ## 2 age 0.0176 0.00605 2.90 0.00369 ## 3 diabetes 0.442 0.0895 4.94 0.000000786 ## 4 drinkany -0.252 0.0834 -3.01 0.00257 The big model (with all of the interaction terms) has a deviance of 3585.7; the additive model has a deviance of 3594.8. \\[\\begin{eqnarray*} \\chi^2_6 &amp;=&amp; 3594.8 - 3585.7= 9.1\\\\ p-value &amp;=&amp; P(\\chi^2_6 \\geq 9.1)= 1 - pchisq(9.1, 6) = 0.1680318 \\end{eqnarray*}\\] We cannot reject the null hypothesis, so we know that we don’t need the 6 interaction terms. Next we will check whether we need weight. The additive model has a deviance of 3594.8; the model without weight is 3597.3. \\[\\begin{eqnarray*} \\chi^2_1 &amp;=&amp; 3597.3 - 3594.8 =2.5\\\\ p-value &amp;=&amp; P(\\chi^2_1 \\geq 2.5)= 1 - pchisq(2.5, 1) = 0.1138463 \\end{eqnarray*}\\] We cannot reject the null hypothesis, so we know that we don’t need the weight in the model either. 5.6.2 Getting the Model Right In terms of selecting the variables to model a particular response, four things can happen: The logistic regression model is correct! The logistic regression model is underspecified. The logistic regression model contains extraneous variables. The logistic regression model is overspecified. Underspecified A regression model is underspecified if it is missing one or more important predictor variables. Being underspecified is the worst case scenario because the model ends up being biased and predictions are wrong for virtually every observation. (Think about Simpson’s Paradox and the need for interaction.) Extraneous The third type of variable situation comes when extra variables are included in the model but the variables are neither related to the response nor are they correlated with the other explanatory variables. Generally, extraneous variables are not so problematic because they produce models with unbiased coefficient estimators, unbiased predictions, and unbiased variance estimates. The worst thing that happens is that the error degrees of freedom is lowered which makes confidence intervals wider and p-values bigger (lower power). Also problematic is that the model becomes unnecessarily complicated and harder to interpret. Overspecified When a model is overspecified, there are one or more redundant variables. That is, the variables contain the same information as other variables (i.e., are correlated!). As we’ve seen, correlated variables cause trouble because they inflate the variance of the coefficient estimates. With correlated variables it is still possible to get unbiased prediction estimates, but the coefficients themselves are so variable that they cannot be interpreted (nor can inference be easily performed). Generally: the idea is to use a model building strategy with some criteria (\\(\\chi^2\\)-tests, AIC, BIC, ROC, AUC) to find the middle ground between an underspecified model and an overspecified model. 5.6.2.1 One Model Building Strategy Taken from https://onlinecourses.science.psu.edu/stat501/node/332. Model building is definitely an ``art.&quot; Unsurprisingly, there are many approaches to model building, but here is one strategy, consisting of seven steps, that is commonly used when building a regression model. The first step Decide on the type of model that is needed in order to achieve the goals of the study. In general, there are five reasons one might want to build a regression model. They are: For predictive reasons - that is, the model will be used to predict the response variable from a chosen set of predictors. For theoretical reasons - that is, the researcher wants to estimate a model based on a known theoretical relationship between the response and predictors. For control purposes - that is, the model will be used to control a response variable by manipulating the values of the predictor variables. For inferential reasons - that is, the model will be used to explore the strength of the relationships between the response and the predictors. For data summary reasons - that is, the model will be used merely as a way to summarize a large set of data by a single equation. The second step Decide which explanatory variables and response variable on which to collect the data. Collect the data. The third step Explore the data. That is: On a univariate basis, check for outliers, gross data errors, and missing values. Study bivariate relationships to reveal other outliers, to suggest possible transformations, and to identify possible multicollinearities. I can’t possibly over-emphasize the data exploration step. There’s not a data analyst out there who hasn’t made the mistake of skipping this step and later regretting it when a data point was found in error, thereby nullifying hours of work. The fourth step (The fourth step is very good modeling practice. It gives you a sense of whether or not you’ve overfit the model in the building process.) Randomly divide the data into a training set and a validation set: The training set, with at least 15-20 error degrees of freedom, is used to estimate the model. The validation set is used for cross-validation of the fitted model. The fifth step Using the training set, identify several candidate models: Use best subsets regression. Use stepwise regression, which of course only yields one model unless different alpha-to-remove and alpha-to-enter values are specified. The sixth step Select and evaluate a few “good” models: Select the models based on the criteria we learned, as well as the number and nature of the predictors. Evaluate the selected models for violation of the model conditions. If none of the models provide a satisfactory fit, try something else, such as collecting more data, identifying different predictors, or formulating a different type of model. The seventh and final step Select the final model: A large cross-validation AUC on the validation data is indicative of a good predictive model (for your population of interest). Consider false positive rate, false negative rate, outliers, parsimony, relevance, and ease of measurement of predictors. And, most of all, don’t forget that there is not necessarily only one good model for a given set of data. There might be a few equally satisfactory models. 5.6.2.2 Another Model Building Strategy Another strategy for model building. Figure taken from (Ramsey and Schafer 2012) 5.6.3 Measures of Association With logistic regression, we don’t have residuals, so we don’t have a value like \\(R^2\\). We can, however, measure whether or not the estimated model is consistent with the data. That is, is the model able to discriminate between successes and failures. 5.6.3.1 back to the burn data 5.1: Consider looking at all the pairs of successes and failures. In the burn data we have 308 survivors and 127 deaths = 39,116 pairs of people. Given a particular pair, if the observation corresponding to a survivor has a higher probability of success than the observation corresponding to a death, we call the pair concordant. If the observation corresponding to a survivor has a lower probability of success than the observation corresponding to a death, we call the pair discordant. Tied pairs occur when the observed survivor has the same estimated probability as the observed death. \\(D_{xy}\\): Somers’ D is the number of concordant pairs minus the number of discordant pairs divided by the total number of pairs. gamma: Goodman-Kruskal gamma is the number of concordant pairs minus the number of discordant pairs divided by the total number of pairs excluding ties. tau-a: Kendall’s tau-a is the number of concordant pairs minus the number of discordant pairs divided by the total number of pairs of people (including pairs who both survived or both died). For example: consider a pair of individuals with burn areas of 1.75 and 2.35. \\[\\begin{eqnarray*} p(x=1.75) &amp;=&amp; \\frac{e^{22.7083-10.6624\\cdot 1.75}}{1+e^{22.7083 -10.6624\\cdot 1.75}} = 0.983\\\\ p(x=2.35) &amp;=&amp; \\frac{e^{22.7083-10.6624\\cdot 2.35}}{1+e^{22.7083 -10.6624\\cdot 2.35}} = 0.087 \\end{eqnarray*}\\] The pairs would be concordant if the first individual survived and the second didn’t. The pairs would be discordant if the first individual died and the second survived. # install.packages(c(&quot;Hmisc&quot;, &quot;rms&quot;)) library(rms) # you need this line!! burn.glm &lt;- lrm(burnresp~burnexpl, data = burnglm) print(burn.glm) ## Logistic Regression Model ## ## lrm(formula = burnresp ~ burnexpl, data = burnglm) ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 435 LR chi2 190.15 R2 0.505 C 0.877 ## 0 127 d.f. 1 g 2.576 Dxy 0.753 ## 1 308 Pr(&gt; chi2) &lt;0.0001 gr 13.146 gamma 0.824 ## max |deriv| 8e-11 gp 0.313 tau-a 0.312 ## Brier 0.121 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept 22.7083 2.2661 10.02 &lt;0.0001 ## burnexpl -10.6624 1.0826 -9.85 &lt;0.0001 ## The summary contains the following elements: number of observations used in the fit, maximum absolute value of first derivative of log likelihood, model likelihood ratio chi2, d.f., P-value, \\(c\\) index (area under ROC curve), Somers’ Dxy, Goodman-Kruskal gamma, Kendall’s tau-a rank correlations between predicted probabilities and observed response, the Nagelkerke \\(R^2\\) index, the Brier score computed with respect to Y \\(&gt;\\) its lowest level, the \\(g\\)-index, \\(gr\\) (the \\(g\\)-index on the odds ratio scale), and \\(gp\\) (the \\(g\\)-index on the probability scale using the same cutoff used for the Brier score). 5.7 Receiver Operating Characteristic Curves Recall that logistic regression can be used to predict the outcome of a binary event (your response variable). A Receiver Operating Characteristic (ROC) Curve is a graphical representation of the relationship between Truth positive negative Predicted positive true positive false positive \\(P&#39;\\) negative false negative true negative \\(N&#39;\\) \\(P\\) \\(N\\) type I error = FP type II error = FN sensitivity = power = true positive rate (TPR) = TP / P = TP / (TP+FN) false positive rate (FPR) = FP / N = FP / (FP + TN) specificity = 1 - FPR = TN / (FP + TN) accuracy (acc) = (TP+TN) / (P+N) positive predictive value (PPV) = precision = TP / (TP + FP) negative predictive value (NPV) = TN / (TN + FN) false discovery rate = 1 - PPV = FP / (FP + TP) Example 5.5 For example: consider a pair of individuals with burn areas of 1.75 and 2.35. \\[\\begin{eqnarray*} p(x=1.75) &amp;=&amp; \\frac{e^{22.7083-10.6624\\cdot 1.75}}{1+e^{22.7083 -10.6624\\cdot 1.75}} = 0.983\\\\ p(x=2.35) &amp;=&amp; \\frac{e^{22.7083-10.6624\\cdot 2.35}}{1+e^{22.7083 -10.6624\\cdot 2.35}} = 0.087\\\\ x &amp;=&amp; \\mbox{log area burned} \\end{eqnarray*}\\] What value would we assign to 1.75 or 2.35 or 15 for log(area) burned? By changing our cutoff, we can fit an entire curve. We want the curve to be as far in the upper left corner as possible (sensitivity = 1, specificity = 1). Notice that the color band represents the probability cutoff for predicting a ``success.“ A: Let’s say we use prob=0.25 as a cutoff: truth yes no predicted yes 300 66 no 8 61 308 127 \\[\\begin{eqnarray*} \\mbox{sensitivity} &amp;=&amp; TPR = 300/308 = 0.974\\\\ \\mbox{specificity} &amp;=&amp; 61 / 127 = 0.480, \\mbox{1 - specificity} = FPR = 0.520\\\\ \\end{eqnarray*}\\] B: Let’s say we use prob=0.7 as a cutoff: truth yes no predicted yes 265 35 no 43 92 308 127 \\[\\begin{eqnarray*} \\mbox{sensitivity} &amp;=&amp; TPR = 265/308 = 0.860\\\\ \\mbox{specificity} &amp;=&amp; 92/127 = 0.724, \\mbox{1 - specificity} = FPR = 0.276\\\\ \\end{eqnarray*}\\] C: Let’s say we use prob=0.9 as a cutoff: truth yes no predicted yes 144 7 no 164 120 308 127 \\[\\begin{eqnarray*} \\mbox{sensitivity} &amp;=&amp; TPR = 144/308 = 0.467\\\\ \\mbox{specificity} &amp;=&amp; 120/127 = 0.945, \\mbox{1 - specificity} = FPR = 0.055\\\\ \\end{eqnarray*}\\] D: all models will go through (0,0) \\(\\rightarrow\\) predict everything negative, prob=1 as your cutoff E: all models will go through (1,1) \\(\\rightarrow\\) predict everything positive, prob=0 as your cutoff F: you have a model that gives perfect sensitivity (no FN!) and specificity (no FP) G: random guessing. If classifier randomly guess, it should get half the positives correct and half the negatives correct. If it guesses 90% of the positives correctly, it will also guess 90% of the negatives to be positive. H: is worse than random guessing. Note that the opposite classifier to (H) might be quite good! 5.8 Cross Validation 5.8.1 Overfitting Imagine you are preparing for your statistics exam. Helpfully, Professor Hardin has made previous exam papers and their worked answers available online. You begin by trying to answer the questions from previous papers and comparing your answers with the model answers provided. Unfortunately, you get carried away and spend all your time on memorizing the model answers to all past questions. Now, if the upcoming exam completely consists of past questions, you are certain to do very well. But if the new exam asks different questions about the same material, you would be ill-prepared and get a much lower mark than with a more traditional preparation. In this case, one could say that you were **overfitting* the past exam papers and that the knowledge gained didn’t generalize to future exam questions. 5.8.2 Model Assessment Cross validation is commonly used to perform two different tasks: 1. To assess a model’s accuracy (model assessment). 2. To build a model (model selection). We will focus here only on model assessment. Suppose that we build a classifier (logistic regression model) on a given data set. We’d like to know how well the model classifies observations, but if we test on the samples at hand, the error rate will be much lower than the model’s inherent accuracy rate. Instead, we’d like to predict new observations that were not used to create the model. There are various ways of creating test or validation sets of data: one training set, one test set [two drawbacks: estimate of error is highly variable because it depends on which points go into the training set; and because the training data set is smaller than the full data set, the error rate is biased in such a way that it overestimates the actual error rate of the modeling technique.] leave one out cross validation (LOOCV) [LOOCV is a special case of \\(k\\)-fold CV with \\(k=n\\)] remove one observation build the model using the remaining n-1 points predict class membership for the observation which was removed repeat by removing each observation one at a time (time consuming to keep building models) \\(k\\)-fold cross validation (\\(k\\)-fold CV) like LOOCV except that the algorithm is run \\(k\\) times on each group (of approximately equal size) from a partition of the data set. advantage of \\(k\\)-fold is computational \\(k\\)-fold often has a better bias-variance trade-off [bias is lower with LOOCV. however, because LOOCV predicts \\(n\\) observations from \\(n\\) models which are all basically the same, the variability will be higher. with \\(k\\)-fold, prediction is on \\(n\\) values from \\(k\\) models which are much less correlated. the effect is to average out the predicted values in such a way that there will be less variability from data set to data set. 5.9 R: Birdnest Example Length of Bird Nest This example is from problem E1 in your text and includes 99 species of N. American passerine birds. Recall that the response variable is binary and represents whether there is a small opening (closed=1) or a large opening (closed=0) for the nest. The explanatory variable of interest was the length of the bird. nests &lt;- read_csv(&quot;~/Dropbox/teaching/math150/PracStatCD/Data Sets/Chapter 07/CSV Files/C7 Birdnest.csv&quot;, na=&quot;*&quot;) 5.9.1 Drop-in-deviance (Likelihood Ratio Test, LRT) \\(\\chi^2\\): The Likelihood ratio test also tests whether the response is explained by the explanatory variable. We can output the deviance ( = K - 2 * log-likelihood) for both the full (maximum likelihood!) and reduced (null) models. \\[\\begin{eqnarray*} G &amp;=&amp; 2 \\cdot \\ln(L(MLE)) - 2 \\cdot \\ln(L(null))\\\\ &amp;=&amp; \\mbox{null (restricted) deviance - residual (full model) deviance}\\\\ G &amp;\\sim&amp; \\chi^2_{\\nu} \\ \\ \\ \\mbox{when the null hypothesis is true} \\end{eqnarray*}\\] where \\(\\nu\\) represents the difference in the number of parameters needed to estimate in the full model versus the null model. glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.457 0.753 0.607 0.544 ## 2 Length -0.0677 0.0425 -1.59 0.112 glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 119.992 94 -58.4399 120.88 125.987 116.88 93 5.9.2 Difference between tidy and augment and glance Note that tidy contains the same number of rows as the number of coefficients. augment contains the same number of rows as number of observations. glance always has one row (containing overall model information). glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.457 0.753 0.607 0.544 ## 2 Length -0.0677 0.0425 -1.59 0.112 glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% augment() ## # A tibble: 95 x 10 ## .rownames Closed. Length .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 -0.896 0.258 -0.827 0.0137 1.12 0.00288 ## 2 2 1 20 -0.896 0.258 1.57 0.0137 1.11 0.0173 ## 3 4 1 20 -0.896 0.258 1.57 0.0137 1.11 0.0173 ## 4 5 1 22.5 -1.07 0.325 1.65 0.0202 1.11 0.0305 ## 5 6 0 18.5 -0.795 0.232 -0.863 0.0116 1.12 0.00267 ## 6 7 1 17 -0.693 0.222 1.48 0.0110 1.12 0.0112 ## 7 8 0 17 -0.693 0.222 -0.900 0.0110 1.12 0.00280 ## 8 9 0 15 -0.558 0.237 -0.951 0.0130 1.12 0.00381 ## 9 10 0 15 -0.558 0.237 -0.951 0.0130 1.12 0.00381 ## 10 11 1 11 -0.287 0.336 1.30 0.0276 1.12 0.0194 ## # … with 85 more rows, and 1 more variable: .std.resid &lt;dbl&gt; glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 119.992 94 -58.4399 120.88 125.987 116.88 93 5.9.3 Looking at variables in a few different ways. Length as a continuous explanatory variable: glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.457 0.753 0.607 0.544 ## 2 Length -0.0677 0.0425 -1.59 0.112 glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 119.992 94 -58.4399 120.88 125.987 116.88 93 Length as a categorical explanatory variables: glm(`Closed?` ~ as.factor(Length), data = nests, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 34 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 19.6 10754. 1.82e- 3 0.999 ## 2 as.factor(Length)10 0.000000432 13171. 3.28e-11 1.000 ## 3 as.factor(Length)10.5 0.000000430 15208. 2.82e-11 1.000 ## 4 as.factor(Length)11 -18.9 10754. -1.75e- 3 0.999 ## 5 as.factor(Length)12 -21.2 10754. -1.97e- 3 0.998 ## 6 as.factor(Length)12.5 0.000000431 15208. 2.83e-11 1.000 ## 7 as.factor(Length)13 -20.3 10754. -1.88e- 3 0.998 ## 8 as.factor(Length)13.5 -20.7 10754. -1.92e- 3 0.998 ## 9 as.factor(Length)14 -19.3 10754. -1.79e- 3 0.999 ## 10 as.factor(Length)14.5 -39.1 13171. -2.97e- 3 0.998 ## # … with 24 more rows glm(`Closed?` ~ as.factor(Length), data = nests, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 119.992 94 -36.8776 141.755 228.587 73.7552 61 Length plus a few other explanatory variables: glm(`Closed?` ~ Length + Incubate + Color, data = nests, family=&quot;binomial&quot;) %&gt;% tidy() ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.64 2.06 -1.28 0.201 ## 2 Length -0.114 0.0527 -2.17 0.0302 ## 3 Incubate 0.314 0.172 1.82 0.0684 ## 4 Color -0.420 0.609 -0.690 0.490 glm(`Closed?` ~ Length + Incubate + Color, data = nests, family=&quot;binomial&quot;) %&gt;% glance() %&gt;% print.data.frame(digits=6) ## null.deviance df.null logLik AIC BIC deviance df.residual ## 1 110.086 87 -51.6633 111.327 121.236 103.327 84 5.9.4 Predicting Response bird.glm &lt;- glm(`Closed?` ~ Length, data = nests, family=&quot;binomial&quot;) bird.glm %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.457 0.753 0.607 0.544 ## 2 Length -0.0677 0.0425 -1.59 0.112 # predicting the linear part: # reasonable to use the SE to create CIs predict(bird.glm, newdata = list(Length = 47), se.fit = TRUE, type = &quot;link&quot;) ## $fit ## 1 ## -2.72 ## ## $se.fit ## [1] 1.3 ## ## $residual.scale ## [1] 1 # predicting the probability of success (on the `scale` of the response variable): # do NOT use the SE to create a CI for the predicted value # instead, use the SE from `type=&quot;link&quot; ` and transform the interval predict(bird.glm, newdata = list(Length = 47), se.fit = TRUE, type = &quot;response&quot;) ## $fit ## 1 ## 0.0616 ## ## $se.fit ## 1 ## 0.0751 ## ## $residual.scale ## [1] 1 5.9.5 Measues of association # install.packages(c(&quot;Hmisc&quot;, &quot;rms&quot;)) library(rms) # you need this line!! birds.glm &lt;- lrm(`Closed?` ~ Length, data = nests) print(birds.glm) ## Frequencies of Missing Values Due to Each Variable ## Closed? Length ## 0 4 ## ## Logistic Regression Model ## ## lrm(formula = `Closed?` ~ Length, data = nests) ## ## ## Model Likelihood Discrimination Rank Discrim. ## Ratio Test Indexes Indexes ## Obs 95 LR chi2 3.11 R2 0.045 C 0.638 ## 0 64 d.f. 1 g 0.455 Dxy 0.276 ## 1 31 Pr(&gt; chi2) 0.0777 gr 1.576 gamma 0.288 ## max |deriv| 2e-07 gp 0.088 tau-a 0.123 ## Brier 0.210 ## ## Coef S.E. Wald Z Pr(&gt;|Z|) ## Intercept 0.4571 0.7530 0.61 0.5438 ## Length -0.0677 0.0425 -1.59 0.1117 ## References "],
["survival-analysis.html", "Chapter 6 Survival Analysis", " Chapter 6 Survival Analysis more soon "],
["multiple-comparisons.html", "Chapter 7 Multiple Comparisons", " Chapter 7 Multiple Comparisons "],
["references.html", "References", " References "]
]
