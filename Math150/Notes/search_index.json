[
["index.html", "Methods in Biostatistics Prerequisites", " Methods in Biostatistics Jo Hardin 2019-01-27 Prerequisites Class notes for Math 150 at Pomona College: Statistics for Clinical Trials. The notes are based primarily on the text Practicing Statistics, (Kuiper and Sklar 2013). You are responsible for reading your text. Your text is very good &amp; readable, so you should use it. Your text is not, however, overly technical. You should make sure you are coming to class and also reading the materials associated with the activities. Week Topic Book Chapter Notes Section 1/23/19 t-tests / SLR / Intro to R 2 1, 2.1, 2.2 1/28/19 SLR 2 3 1/30/19 2/4/19 Contingency Analysis 6 2/11/19 Contingency Analysis 6 2/18/19 Logistic Regression 7 2/25/19 Logistic Regression 7 3/4/19 Logistic Regression 7 3/11/19 Logistic Regression 7 3/13/19 Midterm (Wednesday) (2, 6, 7 (ish)) 3/25/19 Survival Analysis 9 4/1/19 Survival Analysis 9 4/8/19 Survival Analysis 9 4/15/19 Survival Analysis 9 4/22/19 Ioannidis &amp; mult. compar. handouts &amp; 1.13 4/29/19 Poisson Regression 8 5/6/19 Poisson Regression 8 References "],
["intro.html", "Chapter 1 Introduction 1.1 Course Goals", " Chapter 1 Introduction Agenda syllabus course goals / background t-test 1.1 Course Goals Our goals in this course are: to better evaluate quantitative information with regards to clinical and biological data. We’ll be sure to keep in mind: Careful presentation of data Consideration of variability Meaningful comparisons to be able to critically evaluate the medical literature with respect to design, analysis, and interpretation of results. to understand the role of inherent variability and keep it in perspective when inferring results to a population. to critically evaluate medical results given in the mainstream media. to read published studies with skepticism. Some people (in all fields!) wrongly believe that all studies published in a peer review publication must be 100% accurate and/or well designed studies. In this course, you will learn the tools to recognize, interpret, and critique statistical results in medical literature. Probability vs. Statistics Experimental Design In this class we’ll talk about techniques used to analyze data from medical studies. Along with the computational methods, however, we’ll continue to think about issues of experimental design and interpretation. Descriptive statistics describe the sample at hand with no intent on making generalizations. Inferential statistics use a sample to make claims about a population Simple Random Sample is an unbiased sample. Sample is selected in such a way that every possible sample of size \\(n\\) is equally likely. Blind / double blind when the patient and/or doctor do not know which patient is receiving which treatment. Placebo mock treatment Sample size reduces variability (large samples make small effects easier to discern) Experiment vs. Observational Study whether the treatment was assigned by the researchers; randomized experiments make concluding causation possible Funding of study goals, bias "],
["t-tests-vs-slr.html", "Chapter 2 t-tests vs. SLR 2.1 t-test (book: 2.1) ANOVA 2.2 Simple Linear Regression (book: 2.3) 2.3 Confidence Intervals (section 2.11) 2.4 Random Sample vs. Random allocation", " Chapter 2 t-tests vs. SLR We are going to build on a very basic model of the following form: data = deterministic model + random error planned variability your experimental conditions, hopefully represented by an interesting deterministic model random error natural variability due to individuals. systematic error error that is not contained within the model. It can happen because of poor sampling or poor experimental conditions. Surgery Timing The study, “Operation Timing and 30-Day Mortality After Elective General Surgery”, tested the hypotheses that the risk of 30-day mortality associated with elective general surgery: 1) increases from morning to evening throughout the routine workday; 2) increases from Monday to Friday through the workweek; and 3) is more frequent in July and August than during other months of the year. As a presumed negative control, the investigators also evaluated mortality as a function of the phase of the moon. Secondarily, they evaluated these hypotheses as they pertain to a composite in-hospital morbidity endpoint. The related data set contains 32,001 elective general surgical patients. Age, gender, race, BMI, several comorbidities, several surgical risk indices, the surgical timing predictors (hour, day of week, month,moon phase) and the outcomes (30-day mortality and in-hosptial complication) are provided. The dataset is cleaned and complete (no missing data except for BMI). There are no outliers or data problems. The data are from (Sessler et al. 2011) Note that in the example, mortality rates are compared for patients electing to have surgery in July vs August. We’d like to compare the average age of the participants from the July group to the August group. Even if the mortality difference is significant, we can’t conclude causation because it was an observational study. However, the more similar the groups are based on clinical variables, the more likely any differences in mortality are due to timing. How different are the groups based on clinical variables? surgeryurl &lt;- url(&quot;https://www.causeweb.org/tshs/datasets/surgery_timing.Rdata&quot;) load(surgeryurl) surgery &lt;- stata_data surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% group_by(month) %&gt;% summarize(agemean = mean(age, na.rm=TRUE), agesd = sd(age, na.rm=TRUE), agen = sum(!is.na(age))) ## # A tibble: 2 x 4 ## month agemean agesd agen ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Aug 58.1 15.2 3176 ## 2 Jul 57.6 15.5 2325 2.1 t-test (book: 2.1) A t-test is a test of means. For the surgery timing data, the groups would ideally have similar age distributions. Why? What are the advantages and disadvantages of running a retrospective cohort study? The two-sample t-test starts with the assumption that the population means of the two groups are equal, \\(H_0: \\mu_1 = \\mu_2\\). The sample means \\(\\overline{y}_1\\) and \\(\\overline{y}_2\\) will always be different. How different must the \\(\\overline{y}\\) values be in order to reject the null hypothesis? Definition 2.1 (Model 1) \\[\\begin{align} y_{1j} &amp;= \\mu_{1} + \\epsilon_{1j} \\ \\ \\ \\ j=1, 2, \\ldots, n_1\\\\ y_{2j} &amp;= \\mu_{2} + \\epsilon_{2j} \\ \\ \\ \\ j=1, 2, \\ldots, n_2\\\\ \\epsilon_{ij} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\mu_i \\end{align}\\] That is, we are assuming that for each group the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean. Note that we have assumed that the variances of the two groups are equal. We have also assumed that there is independence between and within the groups. Note: we will assume the population variances are equal if neither sample variance is more than twice as big as the other. Example1 Are the mean ages of the July vs August patients statistically different? (why two sided?) \\[\\begin{align} H_0: \\mu_1 = \\mu_2\\\\ H_1: \\mu_1 \\ne \\mu_2 \\end{align}\\] \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - 0}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\\\ s_p &amp;= \\sqrt{ \\frac{(n_1 - 1)s_1^2 + (n_2-1) s_2^2}{n_1 + n_2 -2}}\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;\\\\ t &amp;= \\frac{(58.05 - 57.57) - 0}{15.34 \\sqrt{\\frac{1}{3176} + \\frac{1}{2325}}}\\\\ &amp;= 1.15\\\\ s_p &amp;= \\sqrt{ \\frac{(3176-1)15.22^2 + (2325-1) 15.5^2}{3176 + 2325 -2}}\\\\ &amp;= 15.34\\\\ df &amp;= n_1 + n_2 -2\\\\ &amp;= 5499\\\\ \\mbox{p-value} &amp;= 2 \\cdot (1-pt(1.15,5499)) = 0.25\\\\ \\end{align}\\] The same analysis can be done in R (with and without tydying the output): surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) ## ## Welch Two Sample t-test ## ## data: age by month ## t = 1.1585, df = 4954.5, p-value = 0.2467 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.3366687 1.3092918 ## sample estimates: ## mean in group Aug mean in group Jul ## 58.05220 57.56589 surgery %&gt;% dplyr::filter(month %in% c(&quot;Jul&quot;, &quot;Aug&quot;)) %&gt;% t.test(age ~ month, data = .) %&gt;% tidy() ## # A tibble: 1 x 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.486 58.1 57.6 1.16 0.247 4954. -0.337 ## # … with 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, ## # alternative &lt;chr&gt; Look at SD and SEM What is the statistic? What is the sampling distribution of the statistic? Why do we use the t-distribution? Why is the big p-value important? (It’s a good thing!) How do we interpret the p-value? What can we conclude? applet from (Chance and Rossman 2018): [http://www.rossmanchance.com/applets/SampleMeans/SampleMeans.html] What are the model assumptions? (independence between &amp; within groups, random sample, pop values don’t change, additive error, \\(\\epsilon_{i,j} \\ \\sim \\ iid \\ N(0, \\sigma^2)\\), … basically all the assumptions are given in the original linear model) Considerations when running a t-test: * one-sample t-test * one-sided vs. two-sided hypotheses * t-test with unequal variance (less powerful, more conservative) \\[\\begin{align} t &amp;= \\frac{(\\overline{y}_1 - \\overline{y}_2) - (\\mu_1 - \\mu_2)}{ \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\\\ df &amp;= \\min(n_1-1, n_2-1)\\\\ \\end{align}\\] two dependent (paired) samples – one sample t-test! Example 2 Assume we have two very small samples: (\\(y_{11}=3, y_{12} = 9, y_{21} = 5, y_{22}=1, y_{23}=9\\)). Find \\(\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\epsilon}_{11}, \\hat{\\epsilon}_{12}, \\hat{\\epsilon}_{21}, \\hat{\\epsilon}_{22}, \\hat{\\epsilon}_{23}, n_1, n_2\\). 2.1.1 What is an Alternative Hypothesis? Consider the brief video from the movie Slacker, an early movie by Richard Linklater (director of Boyhood, School of Rock, Before Sunrise, etc.). You can view the video here from starting at 2:22 and ending at 4:30: [https://www.youtube.com/watch?v=b-U_I1DCGEY] In the video, a rider in the back of a taxi (played by Linklater himself) muses about alternate realities that could have happened as he arrived in Austin on the bus. What if instead of taking a taxi, he had found a ride with a woman at the bus station? He could have take a different road into a different alternate reality, and in that reality his current reality would be an alternate reality. And so on. What is the point? Why watch the video? How does it relate the to the material from class? What is the relationship to sampling distributions? [Thanks to Ben Baumer at Smith College for the pointer to the specific video.] ANOVA Skip ANOVA in your text (2.4 and part of 2.9) Agenda SLR t-test as SLR assumptions SLR in R 2.2 Simple Linear Regression (book: 2.3) Simple Linear Regression is a model (hopefully discussed in introductory statistics) used for describing a {linear} relationship between two variables. It typically has the form of: \\[\\begin{align} y_i &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i = 1, 2, \\ldots, n\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2)\\\\ E(Y|x) &amp;= \\beta_0 + \\beta_1 x \\end{align}\\] For this model, the deterministic component (\\(\\beta_0 + \\beta_1 x\\)) is a linear function of the two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and the explanatory variable \\(x\\). The random error terms, \\(\\epsilon_i\\), are assumed to be independent and to follow a normal distribution with mean 0 and variance \\(\\sigma^2\\). How can we use this model to describe the two sample means case we discussed on the esophageal data? Consider \\(x\\) to be a dummy variable that takes on the value 0 if the observation is a control and 1 if the observation is a case. Assume we have \\(n_1\\) controls and \\(n_2\\) cases. It turns out that, coded in this way, the regression model and the two-sample t-test model are mathematically equivalent! (For the color game, the natural way to code is 1 for the color distracter and 0 for the standard game. Why?) \\[\\begin{align} \\mu_1 &amp;= \\beta_0 + \\beta_1 (0) = \\beta_0 \\\\ \\mu_2 &amp;= \\beta_0 + \\beta_1 (1) = \\beta_0 + \\beta_1\\\\ \\mu_2 - \\mu_1 &amp;= \\beta_1 \\end{align}\\] Why are they the same? \\[\\begin{align} b_1= \\hat{\\beta}_1 &amp;= \\frac{n \\sum x_i y_i - \\sum x_i \\sum y_i}{n \\sum x_i^2 - (\\sum x_i )^2}\\\\ &amp;= \\frac{n \\sum_2 y_i - n_2 \\sum y_i}{(n n_2-n_2^2)}\\\\ &amp;= \\frac{ n \\sum_2 y_i - n_2 (\\sum_1 y_i + \\sum_2 y_i)}{n_2(n-n_2)}\\\\ &amp;= \\frac{(n_1 + n_2) \\sum_2 y_i - n_2 \\sum_1 y_i - n_2 \\sum_2 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 \\sum_2 y_i - n_2 \\sum_1 y_i}{n_1 n_2}\\\\ &amp;= \\frac{n_1 n_2 \\overline{y}_2 - n_2 n_1 \\overline{y}_1}{n_1 n_2}\\\\ &amp;= \\overline{y}_2 - \\overline{y}_1\\\\ b_0 = \\hat{\\beta}_0 &amp;= \\frac{\\sum y_i - b_1 \\sum x_i}{n}\\\\ &amp;= \\frac{\\sum_1 y_i + \\sum_2 y_i - b_1 n_2}{n}\\\\ &amp;= \\frac{n_1 \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1 + n_2 \\overline{y}_2 - n_2 \\overline{y}_2 + n_2 \\overline{y}_1}{n}\\\\ &amp;= \\frac{n \\overline{y}_1}{n} = \\overline{y}_1 \\end{align}\\] Definition 2.2 (Model 2) \\[\\begin{align} y_{i} &amp;= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ i=1, 2, \\ldots, n\\\\ \\epsilon_{i} &amp;\\sim N(0,\\sigma^2)\\\\ E[Y_i] &amp;= \\beta_0 + \\beta_1 x_i\\\\ \\hat{y}_i &amp;= b_0 + b_1 x_i \\end{align}\\] That is, we are assuming that for each observation the true population average is fixed and an individual that is randomly selected will have some amount of random error away from the true population mean at their value for the explanatory variable, \\(x_i\\). Note that we have assumed that the variance is constant across any level of the explanatory variable. We have also assumed that there is independence across individuals. [Note: there are no assumptions about the distribution of the explanatory variable, \\(X\\)]. What are the similarities in the t-test vs. SLR models? predicting average assuming independent, constant errors errors follow a normal distribution with zero mean and variance \\(\\sigma^2\\) What are the differences in the two models? one subscript versus two (or similarly, two models for the t-test) two samples for the t-test (two variables for the regression… or is that a similarity??) both variables are quantitative in SLR 2.3 Confidence Intervals (section 2.11) [http://www.rossmanchance.com/applets/NewConfsim/Confsim.html] In general, the format of a confidence interval is (INTERPRETATION!!!!): estimate +/- critical value x standard error of the estimate Age data: \\[\\begin{align} 90\\% \\mbox{ CI for } \\mu_1: &amp; \\overline{y}_1 \\pm t^*_{3176-1} \\times \\hat{\\sigma}_{\\overline{y}_1}\\\\ &amp; 58.05 \\pm 1.645 \\times 15.22/\\sqrt{3176}\\\\ &amp; (57.61, 58.49)\\\\ 95\\% \\mbox{ CI for }\\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{5499} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 0.48 \\pm 1.96 \\times 0.42\\\\ &amp; (-0.34, 1.30) \\end{align}\\] Note the CI on pgs 54/55, there is a typo. The correct interval for \\(\\mu_1 - \\mu_2\\) for the games data should be: \\[\\begin{align} 95\\% \\mbox{ CI for } \\mu_1 - \\mu_2: &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} \\hat{\\sigma}_{\\overline{y}_1 - \\overline{y}_2}\\\\ &amp; \\overline{y}_1 - \\overline{y}_2 \\pm t^*_{38} s_p \\sqrt{1/n_1 + 1/n_2}\\\\ &amp; 38.1 - 35.55 \\pm 2.02 \\times \\sqrt{\\frac{(19)3.65^2 + (19)3.39^2}{20+20-2}} \\sqrt{\\frac{1}{20} + \\frac{1}{20}}\\\\ &amp; (0.29 s, 4.81 s) \\end{align}\\] 2.4 Random Sample vs. Random allocation Recall what you’ve learned about how good random samples lead to inference about a population. On the other hand, in order to make a causal conclusion, you need a randomized experiment with random allocation of the treatments (impossible to happen in many settings). Random sampling and random allocation are DIFFERENT ideas that should be clear in your mind. Figure taken from (Chance and Rossman 2018) Note: no ANOVA (section 2.4) or normal probability plots (section 2.8) References "],
["SLR.html", "Chapter 3 Simple Linear Regression 3.1 Transformations 3.2 Fitting the regression line 3.3 Correlation 3.4 Errors 3.5 Intervals 3.6 R Example (SLR): Happy Planet", " Chapter 3 Simple Linear Regression Agenda sampling distributions transformations confidence / prediction intervals Though we’ve discussed the relationship between tests of means and simple linear regression, we will really consider simple linear regression in a much broader context (one where both the explanatory and response variables are quantitative). The data below represents 10 different variables on health of a country measured on 143 countries. Data taken from (R. Lock et al. 2016), originally from the Happy Planet Index Project [http://www.happyplanetindex.org/]. Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. We are going to investigate happiness and life expectancy. 3.1 Transformations Model assumptions The average value for the response variable is a linear function of the explanatory variable. The error terms follow a normal distribution around the linear model. The error terms have a mean of zero. The error terms have a constant variance of \\(\\sigma^2\\). The error terms are independent (and identically distributed). [http://www.rossmanchance.com/applets/RegSim/RegCoeff.html] How do we tell whether the assumptions are met? We can’t always. But it’s good to look at plots: scatter plot, residual plot, histograms of residuals. We denote the residuals for this model as: \\[\\begin{align} r_i = \\hat{e}_i = y_i - \\hat{y}_i \\end{align}\\] Figs 3.13 and 3.15 taken from Kutner et al. (2004) important note!! The idea behind transformations is to make the model as appropriate as possible for the data at hand. We want to find the correct linear model; we want our assumptions to hold. We are not trying to find the most significant model or big \\(R^2\\). See section 2.9 in your text. No normal probability plots (qq-plots); use histograms or boxplots to assess the symmetry and normality of the residuals. 3.2 Fitting the regression line How do we fit a regression line? Find \\(b_0\\) and \\(b_1\\) that minimize the sum of squared distance of the points to the line (called ordinary least squares): \\[\\begin{align} \\min \\sum (y_i \\hat{y}_i)^2 &amp;=&amp; \\min RSS \\mbox{ residual sum of squares}\\\\ RSS &amp;=&amp; \\sum (y_i - b_0 - b_1 x_i)^2\\\\ \\frac{\\partial RSS}{\\partial b_0} = 0\\\\ \\frac{\\partial RSS}{\\partial b_1} = 0\\\\ b_0 &amp;=&amp; \\overline{y} - b_1 \\overline{x}\\\\ b_1 &amp;=&amp; r(x,y) \\frac{s_x}{s_y}\\\\ \\end{align}\\] Is that the only way to find values for \\(b_0\\) and \\(b_1\\)? (absolute distances, maximum likelihood,…) Resistance to outliers? What is \\(\\hat{y}\\) at \\(\\overline{x}\\)? \\[\\begin{align} \\hat{y} &amp;=&amp; b_0 + b_1 \\overline{x}\\\\ &amp;=&amp; \\overline{y} - b_1 \\overline{x} + b_1 \\overline{x}\\\\ &amp;=&amp; \\overline{y} \\end{align}\\] The regression line will always pass through the point \\((\\overline{x}, \\overline{y})\\). Definition 3.1 An estimate is unbiased if, over many repeated samples drawn from the population, the average value of the estimates based on the different samples would equal the population value of the parameter being estimated. That is, a statistic is unbiased if the mean of its sampling distribution is the population parameter. 3.3 Correlation Consider a scatterplot, you’ll have variability in both directions: \\((x_i - \\overline{x}) \\&amp; (y_i - \\overline{y})\\). \\[\\begin{align} \\mbox{sample covariance}&amp;&amp;\\\\ cov(x,y) &amp;=&amp; \\frac{1}{n-1}\\sum (x_i - \\overline{x}) (y_i - \\overline{y})\\\\ \\mbox{sample correlation}&amp;&amp;\\\\ r(x,y) &amp;=&amp; \\frac{cov(x,y)}{s_x s_y}\\\\ &amp;=&amp; \\frac{\\frac{1}{n-1} \\sum (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sqrt{\\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\frac{\\sum(y_i - \\overline{y})^2}{n-1}}}\\\\ \\mbox{pop cov} &amp;=&amp; \\sigma_{xy}\\\\ \\mbox{pop cor} &amp;=&amp; \\rho = \\frac{\\sigma_{xy}}{\\sigma_x \\sigma_y}\\\\ \\end{align}\\] \\(-1 \\leq r \\leq 1 \\&amp; -1 \\leq \\rho \\leq 1\\). No Spearman’s rank correlation or Kendall’s \\(\\tau\\). \\(b_1 = r \\frac{s_y}{s_x}\\) if \\(r=0, b_1=0\\) if \\(r=1, b_1 &gt; 0\\) but can be anything! \\(r &lt; 0 \\leftrightarrow b &lt; 0, r &gt; 0 \\leftrightarrow b &gt; 0\\) Recall that \\(R^2\\) is the proportion of variability explained by the line. 3.4 Errors Recall, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). How do we estimate \\(\\sigma^2\\)? \\[\\begin{align} RSS &amp;=&amp; \\sum (y_i - \\hat{y}_i)^2 \\ \\ \\ \\mbox{ residual sum of squares}\\\\ MSS &amp;=&amp; \\sum (\\hat{y}_i - \\overline{y})^2 \\ \\ \\ \\mbox{ model sum of squares}\\\\ TSS &amp;=&amp; \\sum (y_i - \\overline{y})^2 \\ \\ \\ \\mbox{ total sum of squares}\\\\ s_{y|x}^2 &amp;=&amp; \\hat{\\sigma^2} = \\frac{1}{n-2} RSS\\\\ s_x^2 &amp;=&amp; \\frac{1}{n-1} \\sum (x_i - \\overline{x})^2\\\\ s_y^2 &amp;=&amp; \\frac{1}{n-1} \\sum (y_i - \\overline{y})^2\\\\ var(\\epsilon) &amp;=&amp; s_{y|x}^2 = \\frac{RSS}{n-2} = \\frac{\\sum(y_i - \\hat{y}_i)^2}{n-2} = SE(\\epsilon)\\\\ var(b_1) &amp;=&amp; \\frac{s_{y|x}^2}{(n-1) s_x^2}\\\\ SE(b_1) &amp;=&amp; \\frac{s_{y|x}}{\\sqrt{(n-1)} s_x}\\\\ &amp;=&amp; \\frac{\\hat{\\sigma}}{\\sqrt{\\sum(x_i - \\overline{x})^2}} = \\frac{\\sqrt{\\sum(y_i - \\hat{y}_i)^2/(n-2)}}{\\sqrt{\\sum(x_i - \\overline{x})^2}}\\\\ \\end{align}\\] \\(SE(b_1) \\downarrow\\) as \\(\\sigma \\downarrow\\) \\(SE(b_1) \\downarrow\\) as \\(n \\uparrow\\) \\(SE(b_1) \\downarrow\\) as \\(s_x \\uparrow\\) WHY? What do we mean by \\(SE(b_1)\\)? As we saw above, the correlation and the slope estimates are intimately related. They are also both related to the coefficient of determination. \\[\\begin{align} R^2 = r^2 = \\frac{MSS}{TSS} \\end{align}\\] \\(R^2\\) is the proportion of total variability explained by the regression line (the linear relationship between the explanatory and response variables). If \\(x\\) and \\(y\\) are not at all correlated, \\(\\hat{y}_i \\approx \\overline{y}\\), MSS = 0, \\(R^2=0\\). If \\(x\\) and \\(y\\) are perfectly correlated, \\(\\hat{y}_i = y_i\\), MSS=TSS, \\(R^2 = 1\\). 3.4.1 Testing \\(\\beta_1\\) If \\(H_0: \\beta=0\\) is true, then \\[\\begin{align} \\frac{b_1 - 0}{SE(b_1)} \\sim t_{n-2} \\end{align}\\] Note that the degrees of freedom are now \\(n-2\\) because we are estimating two parameters (\\(\\beta_0\\) and \\(\\beta_1\\)). We can also find a \\((1-\\alpha)100\\%\\) confidence interval for \\(\\beta_1\\): \\[\\begin{align} b_1 \\pm t_{\\alpha/2, n-2} SE(b_1) \\end{align}\\] 3.5 Intervals As with anything that has some type of standard error, we can create intervals that give us some confidence in the statements we are making. 3.5.1 Confidence Intervals In general, confidence intervals are of the form: point estimate +/- multiplier * SE(point estimate) 3.5.2 Slope We can create a CI for the slope parameter, \\(\\beta_1\\): \\[\\begin{align} b_1 &amp;\\pm&amp; t_{\\alpha/2,n-2} SE(b_1)\\\\ b_1 &amp;\\pm&amp; t_{\\alpha/2, n-2} \\frac{s_{y|x}}{\\sqrt{(n-1)}s_x}\\\\ 6.693 &amp;\\pm&amp; t_{.025, 141} 0.375\\\\ t_{.025,141} &amp;=&amp; qt(0.025, 141) = -1.977\\\\ \\mbox{CI} &amp;&amp; (5.95 \\mbox{ years/unit of happy}, 7.43 \\mbox{ years/unit of happy}) \\end{align}\\] How can we interpret the CI? Does it make sense to talk about a unit of happiness? 3.5.3 Mean Response We can also create a CI for the mean response, \\(E[Y|x^*] = \\beta_0 + \\beta_1 x^*\\). Note that the standard error of the point estimate (\\(\\hat{y}=b_0 + b_1 x^*\\)) now depends on the variability associated with two things (\\(b_0, b_1\\)). \\[\\begin{align} SE(\\hat{y(x^*)}) &amp;=&amp; \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2}\\\\ SE(\\hat{y}(\\overline{x})) &amp;=&amp; s_{y|x}/\\sqrt{n}\\\\ SE(\\hat{y}(x)) &amp;\\geq&amp; s_{y|x}/\\sqrt{n} \\ \\ \\ \\forall x \\end{align}\\] How would you interpret the associated interval? 3.5.4 Prediction of an Individual Response As should be obvious, predicting an individual is more variable than predicting a mean. \\[\\begin{align} SE(y(x^*)) &amp;=&amp; \\sqrt{ \\frac{s^2_{y|x}}{n} + (x^* - \\overline{x})^2 SE(b_1)^2 + s^2_{y|x}}\\\\ SE(y(x^*)) &amp;=&amp; \\sqrt{ SE(\\hat{y}(x^*))^2 + s^2_{y|x}}\\\\ \\end{align}\\] How would you interpret the associated interval? 3.5.5 Outlying, High Leverage, and Influential Points We are skipping the rest of this section in the notes. You are not responsible for it. Read section 4.7 (no loess, ignore the multiple predictors part, ) Theorem 3.1 High leverage points are x-outliers with the potential to exert undue influence on regression coefficient estimates. Influential points are points that have exerted undue influence on the regression coefficient estimates. Note: typically we think of more data as better; more values will tend to decrease the sampling variability of our statistic. But if I give you a lot more data and put it all at \\(\\overline{x}\\), \\(SE(b_1)\\) stays exactly the same. Why?? Recall \\[\\begin{align} y_{i} &amp;=&amp; \\beta_0 + \\beta_1 x_i \\ \\ \\ \\epsilon_i \\sim N(0,\\sigma^2)\\\\ e_i &amp;=&amp; y_i - \\hat{y}_i \\end{align}\\] We plot \\(e_i\\) versus \\(\\hat{y}_i\\). (Why? Typically, we want the \\(e_i\\) to be constant at each value of \\(x_i\\). Note that \\(\\hat{y}_i\\) is a simple linear transformation of \\(x_i\\), so the plot is identical.) We want to see if the distributions of the residuals is different across the fitted line (we look for patterns).\\ Not all residuals have an equal effect on the regression line!! 3.5.5.1 leverage \\[\\begin{align} h_i = \\frac{1}{n} +\\frac{(x_i - \\overline{x})^2}{\\sum_{j=1}^n (x_j - \\overline{x})^2}\\\\ \\frac{1}{n} \\leq h_i \\leq 1\\\\ \\end{align}\\] Leverage represents the effect of point \\(x_i\\) on the line. We need large leverage for a particular value to have a large effect. Note: \\[\\begin{align} SE(\\hat{y}(x_i)) &amp;=&amp; s_{y|x} \\sqrt{h_i}\\\\ SE(y(x_i)) &amp;=&amp; s_{y|x} \\sqrt{(h_i + 1)}\\\\ SE(e_i) &amp;=&amp; s_{y|x} \\sqrt{(1-h_i)}\\\\ \\hat{y}(x^*) &amp;\\pm&amp; t_{n-2, .025} (s_{y|x} \\sqrt{h(x^*)+1})\\\\ \\end{align}\\] is a 95% prediction interval at \\(x^*\\). High leverage reduces the variability because the line gets pulled toward the point. 3.5.5.2 standardized residuals \\[\\begin{align} \\frac{e_i}{s_{y|x} \\sqrt{1-h_i}} \\sim t_{n-2}\\\\ \\end{align}\\] 3.5.5.3 studentized residuals \\[\\begin{align} \\frac{e_i}{s_{y|x, (i)} \\sqrt{1-h_i}} &amp;\\sim&amp; t_{n-3}\\\\ s_{y|x, (i)} &amp;=&amp; \\frac{1}{n-3} \\sum_{j \\ne i} (y_j - \\hat{y}_{j(i)})^2 \\end{align}\\] Where do we predict 90% of residuals? \\(\\pm t_{n-2,3 , .05}\\). About \\(\\pm 2\\). 3.5.5.4 DFBETAs DFBETAs represent the change in the parameter estimate due to one observation. \\[\\begin{align} DFBETAS_i &amp;=&amp; \\frac{b_1 - b_{1(i)}}{\\frac{s_{y|x, (i)}}{\\sqrt{(n-1)} s_x}}\\\\ \\end{align}\\] 3.6 R Example (SLR): Happy Planet The data below represents 10 different variables on health of a country measured on 143 countries. Data taken from (R. Lock et al. 2016), originally from the Happy Planet Index Project [http://www.happyplanetindex.org/]. Region of the world is coded as 1 = Latin America, 2 = Western nations, 3 = Middle East, 4 = Sub-Saharan Africa, 5 = South Asia, 6 = East Asia, 7 = former Communist countries. We are going to investigate happiness and life expectancy. 3.6.1 Reading the data into R happy &lt;- read_delim(&quot;~/Dropbox/teaching/math150/spring17/happyPlanet.txt&quot;, delim=&quot;\\t&quot;) glimpse(happy) ## Observations: 143 ## Variables: 11 ## $ Country &lt;chr&gt; &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;Arm… ## $ Region &lt;dbl&gt; 7, 3, 4, 1, 7, 2, 2, 7, 5, 7, 2, 1, 4, 5, 1, 7, 4… ## $ Happiness &lt;dbl&gt; 5.5, 5.6, 4.3, 7.1, 5.0, 7.9, 7.8, 5.3, 5.3, 5.8,… ## $ LifeExpectancy &lt;dbl&gt; 76.2, 71.7, 41.7, 74.8, 71.7, 80.9, 79.4, 67.1, 6… ## $ Footprint &lt;dbl&gt; 2.2, 1.7, 0.9, 2.5, 1.4, 7.8, 5.0, 2.2, 0.6, 3.9,… ## $ HLY &lt;dbl&gt; 41.7, 40.1, 17.8, 53.4, 36.1, 63.7, 61.9, 35.4, 3… ## $ HPI &lt;dbl&gt; 47.91, 51.23, 26.78, 58.95, 48.28, 36.64, 47.69, … ## $ HPIRank &lt;dbl&gt; 54, 40, 130, 15, 48, 102, 57, 85, 31, 104, 64, 27… ## $ GDPperCapita &lt;dbl&gt; 5316, 7062, 2335, 14280, 4945, 31794, 33700, 5016… ## $ HDI &lt;dbl&gt; 0.801, 0.733, 0.446, 0.869, 0.775, 0.962, 0.948, … ## $ Population &lt;dbl&gt; 3.15, 32.85, 16.10, 38.75, 3.02, 20.40, 8.23, 8.3… 3.6.2 Running the linear model (lm) happy.lm = lm(LifeExpectancy ~ Happiness, data=happy) happy.lm %&gt;% tidy() ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 28.2 2.28 12.4 2.76e-24 ## 2 Happiness 6.69 0.375 17.8 5.78e-38 3.6.3 Ouptut Some analyses will need the residuals, fitted values, or coefficients individually. happy.lm %&gt;% augment() ## # A tibble: 143 x 9 ## LifeExpectancy Happiness .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 76.2 5.5 65.0 0.537 11.2 0.00765 6.09 1.28e-2 ## 2 71.7 5.6 65.7 0.527 6.00 0.00737 6.14 3.57e-3 ## 3 41.7 4.3 57.0 0.796 -15.3 0.0168 6.02 5.39e-2 ## 4 74.8 7.1 75.7 0.678 -0.944 0.0122 6.16 1.48e-4 ## 5 71.7 5 61.7 0.619 10.0 0.0101 6.10 1.38e-2 ## 6 80.9 7.9 81.1 0.904 -0.198 0.0216 6.16 1.18e-5 ## 7 79.4 7.8 80.4 0.873 -1.03 0.0202 6.16 2.95e-4 ## 8 67.1 5.3 63.7 0.564 3.40 0.00842 6.16 1.32e-3 ## 9 63.1 5.3 63.7 0.564 -0.596 0.00842 6.16 4.04e-5 ## 10 68.7 5.8 67.0 0.515 1.66 0.00705 6.16 2.60e-4 ## # … with 133 more rows, and 1 more variable: .std.resid &lt;dbl&gt; We can plot the main relationship, or we can plot the residuals (to check that technical conditions hold): ggplot(happy, aes(x=Happiness, y=LifeExpectancy)) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) happy.lm %&gt;% augment %&gt;% ggplot(aes(x = .fitted, y = .resid)) + geom_point() + geom_hline(yintercept=0) Intervals of interest: mean response, individual response, and parameter(s). predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(&quot;conf&quot;), level=.95) ## fit lwr upr ## 1 54.99531 53.24675 56.74387 ## 2 75.07444 73.78057 76.36830 predict.lm(happy.lm, newdata=list(Happiness=c(4,7)),interval=c(&quot;pred&quot;), level=.95) ## fit lwr upr ## 1 54.99531 42.72945 67.26117 ## 2 75.07444 62.86510 87.28377 happy.lm %&gt;% tidy(conf.int = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 28.2 2.28 12.4 2.76e-24 23.7 32.7 ## 2 Happiness 6.69 0.375 17.8 5.78e-38 5.95 7.43 3.6.3.1 Residuals in R We skipped the residuals section, so you are not responsible for finding residuals in R, but the R code is here for completion in case you are interested: happy.lm %&gt;% augment() ## # A tibble: 143 x 9 ## LifeExpectancy Happiness .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 76.2 5.5 65.0 0.537 11.2 0.00765 6.09 1.28e-2 ## 2 71.7 5.6 65.7 0.527 6.00 0.00737 6.14 3.57e-3 ## 3 41.7 4.3 57.0 0.796 -15.3 0.0168 6.02 5.39e-2 ## 4 74.8 7.1 75.7 0.678 -0.944 0.0122 6.16 1.48e-4 ## 5 71.7 5 61.7 0.619 10.0 0.0101 6.10 1.38e-2 ## 6 80.9 7.9 81.1 0.904 -0.198 0.0216 6.16 1.18e-5 ## 7 79.4 7.8 80.4 0.873 -1.03 0.0202 6.16 2.95e-4 ## 8 67.1 5.3 63.7 0.564 3.40 0.00842 6.16 1.32e-3 ## 9 63.1 5.3 63.7 0.564 -0.596 0.00842 6.16 4.04e-5 ## 10 68.7 5.8 67.0 0.515 1.66 0.00705 6.16 2.60e-4 ## # … with 133 more rows, and 1 more variable: .std.resid &lt;dbl&gt; References "],
["analysis-of-categorical-data.html", "Chapter 4 Analysis of Categorical Data", " Chapter 4 Analysis of Categorical Data more soon "],
["logistic-regression.html", "Chapter 5 Logistic Regression", " Chapter 5 Logistic Regression more soon "],
["survival-analysis.html", "Chapter 6 Survival Analysis", " Chapter 6 Survival Analysis more soon "],
["multiple-comparisons.html", "Chapter 7 Multiple Comparisons", " Chapter 7 Multiple Comparisons "],
["references.html", "References", " References "]
]
