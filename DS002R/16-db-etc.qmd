# **SQL** extras {#sec-db-etc}


```{r}
#| include: false

source("_common.R")
fontawesome::fa_html_dependency()
```


```{r}
#| echo: false

con_air <- DBI::dbConnect(
  RMariaDB::MariaDB(),
  dbname = "airlines",
  host = Sys.getenv("MDSR_HOST"),
  user = Sys.getenv("MDSR_USER"),
  password = Sys.getenv("MDSR_PWD")
)
```



#### Back to the flights {-}

The examples below use the `airlines` database, including the `flights`, `carriers`, `airports`, and `planes` tables.

## Efficiencies

It is worth pointing out a few aspects to loading data into **SQL**: keys, indexes, and partitioning.

Before we get to the definitions, consider this analogy:

> Each library (`database`) has books (`table`s). Each book (`table`) has pages (rows). Each page (row) has a unique page number to identify it (`key` value); to find a particular page, you sort through the page numbers (`key` values). But it isn't immediately obvious where the particular page of interest is, you might have to page through the book a little bit to find the page of interest.  It would be easier if you had several bookmarks throughout the book to anchor some of the page numbers.  For example, if you want page 1047 and you have a bookmark on page 1050, you only have to turn back three pages.  The bookmark is an `index`, it helps you find the desired rows much more quickly.^[Analogy taken from: https://www.quora.com/profile/Lara-Mazilu]


### Key

Keys are unique identifiers for each row, used primarily for connecting tables. Keys are generally not helpful for efficiency, but they are important for data integrity and relationships between tables. A key is a pointer that identifies a record. In practice, a key is one or more columns that are earmarked to uniquely identify a record in a table. Keys serve two main purposes:

1. They provide constraints on the column such as that it can't store duplicate or null values.
2. They are also used to generate relationships among different tables.

* `PRIMARY KEY` is a column or set of columns that uniquely identify each row.  Primary keys cannot be `NULL`. Each table must always have one (and only one) PK. The PK can be made up of one column, but if that isn't enough to uniquely identify the row, more columns may be added. Sometimes it is easier to designate a numeric column (e.g., row number) to be the PK.
* `FOREIGN KEY` is a column or set of columns that reference a primary key in a different table.  The FK links two tables together, and the link is called a relationship.

(There are other keys such as: Super Key, Minimal Super Key, Candidate Key, Unique Key, Alternate Key, Composite Key, Natural Key, Surrogate Key.)



### Index

Indexes are the crux of why **SQL** is so much more efficient than, say, **R**.  An index is a lookup table that helps **SQL** keep track of which records contain certain values.  By indexing the rows, **SQL** is able to optimize sorting and joining tables.  The index is created in advance (when the table is created) and saved to disk, which can take up substantial space on the disk.  Sometimes more than one variable is used to index the table. There are trade-offs to having a lot of indexes (disk space but fast wrangling) versus a few indexes (slow wrangling but less space).

A table may have more than one index but you shouldn't add indexes to every column in a table, as these have to be updated for every addition/update/delete to the column. Rather, indexes should be added to columns that are frequently included in queries.

Indexes may not make much difference for small databases, but, as tables grow in size, queries benefit more from indexes.

In **MySQL** the commands `SHOW KEYS` and `SHOW INDEXES` provide information about the keys and indexes for each table (the two operations are synonymous in **MySQL**).  Neither operation is available in **DuckDB**.

Notice that the `planes` table has a single `PRIMARY` key.  That primary key is used to index the table.  The `flights` table has no `PRIMARY` key, but it does have six different indexes: `Year`, `Date`, `Origin`, `Dest`, `Carrier`, and `tailNum`.

```{sql}
#| connection: con_air

SHOW INDEXES FROM planes;
```

```{sql}
#| connection: con_air

SHOW INDEXES FROM flights;
```

The values output by `SHOW INDEXES` are:^[Taken from: https://dev.mysql.com/doc/refman/8.0/en/show-index.html]

* `Table`: The name of the table.

* `Non_unique`: 0 if the index cannot contain duplicates, 1 if it can.

* `Key_name`: The name of the index. If the index is the primary key, the name is always `PRIMARY`.

* `Seq_in_index`: The column sequence number in the index, starting with 1.

* `Column_name`: The column name. See also the description for the Expression column.

* `Collation`: How the column is sorted in the index. This can have values A (ascending), D (descending), or NULL (not sorted).

* `Cardinality`: An estimate of the number of unique values in the index.  
Cardinality is counted based on statistics stored as integers, so the value is not necessarily exact even for small tables. The higher the cardinality, the greater the chance that **MySQL** uses the index when doing joins. Indexing with a high cardinality variable will be particularly useful for increased efficiency (if that variable is being queried).

* `Sub_part`: The index prefix. That is, the number of indexed characters if the column is only partly indexed, NULL if the entire column is indexed.

* `Packed`: Indicates how the key is packed. `NULL` if it is not.

* `Null`: Contains `YES` if the column may contain `NULL` values and '' if not.

* `Index_type`: The index method used (`BTREE`, `FULLTEXT`, `HASH`, `RTREE`).

* `Comment`: Information about the index not described in its own column, such as disabled if the index is disabled.

* `Index_comment`: Any comment provided for the index with a `COMMENT` attribute when the index was created.



### Partitioning

Another way to speed up query retrievals is to partition the data tables.  If, for example, the SNL queries were always done by year, then the `episodes` table could be partitioned such that they are stored as separate tables (one per `year`).  The partitioning functions as an index on `year`.  The user would not be able to tell the difference between the unpartitioned `episodes` table and the partitioned one.  However, queries done by `year` would be faster.  Queries done grouped in another way would be slower.

### Querying

Indexes are built to accommodate the specific queries that are most likely to be run.  However, you might not know which queries are going to be run, so it isn't always obviously how to index a table.

For the flights table, it seems likely that many queries will involve searching for flights from a particular origin, or to a particular destination, or during a particular year (or range of years), or on a specific carrier, and so indexes have been built on each of those columns. There is also a Date index, since it seems likely that people would want to search for flights on a certain date. However, it does not seem so likely that people would search for flights in a specific month across all years, and thus we have not built an index on month alone. The Date index does contain the month column, but the index can only be used if year is also part of the query.

```{sql}
#| connection: con_air
#| label: show-index
#| output.var: "show_index"

SHOW INDEXES FROM flights;
```


```{r}
#| label: tbl-show-index
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because distance is not indexed, all 48 million rows must be searched."

show_index |>
  select(Table, Non_unique, Key_name, Seq_in_index, Column_name, Cardinality) |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```

#### efficiencies in `SELECT`ing {-}

**MySQL** provides information about how it is going to perform a query using the `EXPLAIN` syntax. The information communicates how onerous the query is, without actually running itâ€”saving you the time of having to wait for it to execute. @tbl-explain-query-dist provides output that reflects the query plan returned by the **MySQL** server.

```{sql}
#| connection: con_air
#| label: explain-query-dist
#| output.var: "explain_query_dist"

EXPLAIN SELECT * FROM flights WHERE distance > 3000;
```

```{r}
#| label: tbl-explain-query-dist
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because distance is not indexed, all 48 million rows must be searched."

explain_query_dist |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```

If we were to run a query for long flights using the `distance` column the server will have to inspect **each** of the 48 million rows, because `distance` is not indexed. A query on a non-indexed variable is the slowest possible search and is often called a table scan. The 48 million number that you see in the rows column is an estimate of the number of rows that **MySQL** will have to consult in order to process your query. In general, more rows mean a slower query.


On the other hand, a search for recent flights using the `year` column, which has an index built on it, considers many fewer rows (about 6.3 million, those flights in 2013).

```{sql}
#| connection: con_air
#| label: explain-query-year
#| output.var: "explain_query_year"

EXPLAIN SELECT * FROM flights WHERE year = 2013;
```

```{r}
#| label: tbl-explain-query-year
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because year is indexed, only 6 million rows (those in 2013) must be searched."

explain_query_year |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```


In a search by `year` and `month`, **SQL** uses the `Date` index. Only 700,000 rows are searched, those in June of 2013.

```{sql}
#| connection: con_air
#| label: explain-query-date
#| output.var: "explain_query_date"

EXPLAIN SELECT * FROM flights WHERE year = 2013 AND month = 6;
```

```{r}
#| label: tbl-explain-query-date
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because year and month are both indexed, only 700,000 rows (those in June of 2013) must be searched."

explain_query_date |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```


If we search for particular months across all years, the indexing does not help at all.  The query results in a table scan.

```{sql}
#| connection: con_air
#| label: explain-query-month
#| output.var: "explain_query_month"

EXPLAIN SELECT * FROM flights WHERE month = 6;
```

```{r}
#| label: tbl-explain-query-month
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because month is not indexed on its own, all rows (48 million!) must be searched."

explain_query_month |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```


Although month is part of the `Date` index, it is the **second** column in the index, and thus it doesn't help us when we aren't filtering on year. Thus, if it were common for our users to search on month without year, it would probably be worth building an index on month. Were we to actually run these queries, there would be a significant difference in computational time.

#### efficiencies in `JOIN`ing {-}

Using indexes is especially important for efficiency when performing `JOIN` operations on large tables. In the two examples below, both queries use indexes. However, because the cardinality of the index on `tailnum` is larger that the cardinality of the index on `year` (see @tbl-show-index), the number of rows in `flights` associated with each unique value of `tailnum` is smaller than for each unique value of `year`. Thus, the first query runs faster.


```{sql}
#| connection: con_air
#| label: explain-join-tail
#| output.var: "explain_join_tail"

EXPLAIN 
  SELECT * FROM planes p 
  LEFT JOIN flights o ON p.tailnum = o.TailNum
  WHERE manufacturer = 'BOEING';
```

```{r}
#| label: tbl-explain-join-tail
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because month is not indexed on its own, all rows (48 million!) must be searched."

explain_join_tail |>
  select(id, select_type, table, rows, key, key_len, ref, everything()) |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```



```{sql}
#| connection: con_air
#| label: explain-join-year
#| output.var: "explain_join_year"

EXPLAIN 
  SELECT * FROM planes p 
  LEFT JOIN flights o ON p.Year = o.Year
  WHERE manufacturer = 'BOEING';
```

```{r}
#| label: tbl-explain-join-year
#| echo: false
#| tbl-cap: "SQL information about how the query will be run.  Because month is not indexed on its own, all rows (48 million!) must be searched."

explain_join_year |>
  select(id, select_type, table, rows, key, key_len, ref, everything()) |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```



## **SQL** in **dbplyr**


### Median

Let's start with an example, calculating the median `alt`itude in the `airports` table.  (Using `airports` instead of `flights` just because the `airports` table is so much smaller.)^[Example taken from: https://sebhastian.com/mysql-median/]

```{r}
airports <- tbl(con_air, "airports")

head(airports)
```

It seems as though the **SQL** query to calculate the median should be reasonably straightforward.

```{r}
median_query <- airports |>
  summarize(med_alt = median(alt, na.rm = TRUE))

show_query(median_query)
```

But when the **SQL** query is applied to the `airports` table, we get an error.

```{sql}
#| connection: con_air
#| error: true

SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `alt`) AS `med_alt`
FROM `airports`;
```


Additionally, the **R** code itself doesn't run and gives the same error.  Huh, maybe calculating the median is actually harder than it seems (yes, it is).

```{r}
#| error: true

airports |>
  summarize(med_alt = median(alt, na.rm = TRUE))
```



One way to calculate the median is to use a row index on the sorted values.  Note that attaching a row index to the sorted values requires the values to be sorted (sorting can take a long time).


Below is the full code to calculate the median.

```{sql}
#| connection: con_air

SET @row_index := -1;
```

```{sql}
#| connection: con_air

SELECT AVG(subquery.alt) AS median_value
FROM (
SELECT @row_index:=@row_index + 1 AS row_index, alt
  FROM airports
  ORDER BY alt
) AS subquery
WHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));
```


But let's break down what the code is doing...  First, set the `row_index` to -1 and iterate through by adding +1 for each row.  Then concatenate the `row_index` information onto our table of interest.


```{sql}
#| connection: con_air

SET @row_index := -1;
```

```{sql}
#| connection: con_air

SELECT @row_index:=@row_index + 1 AS row_index, alt
  FROM airports
  ORDER BY alt
  LIMIT 10;
```


Next, filter the data to include only the middle row or two rows.

```{sql}
#| connection: con_air

SET @row_index := -1;
```

```{sql}
#| connection: con_air

SELECT *
FROM (
SELECT @row_index:=@row_index + 1 AS row_index, alt
  FROM airports
  ORDER BY alt
) AS subquery
WHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));
```


The last step is to average the middle row(s).  If only one row is pulled out in the previous query, then only one row will be averaged (which the computer does happily).

```{sql}
#| connection: con_air

SET @row_index := -1;
```

```{sql}
#| connection: con_air

SELECT AVG(subquery.alt) AS median_value
FROM (
SELECT @row_index:=@row_index + 1 AS row_index, alt
  FROM airports
  ORDER BY alt
) AS subquery
WHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));
```

**For a computer to calculate the median**: If sorting the numbers and then finding the (average of the) middle numbers, the task will take $O(nlog(n))$ time.  That is, a dataset with 1000 records will be $(1000 \cdot log(1000)) / (100 \cdot log(100)) = 10 \cdot log(900)$ times slower than a data set with 100 records.  There are some caveats:  (1) some sorting algorithms are faster, and (2) you don't need to sort every number to get the median.  But generally, sorting is a slow operation!

**For a computer to calculate the mean**:  Averaging is just summing, and it happens in linear time, $O(n)$.  That means that a dataset with 1000 records will be $1000/100 = 10$ times slower than a dataset with 100 records.

**Verdict:** Generally, for a computer, it is easier to calculate a mean than a median, because of the need to sort in finding a median.


### `CASE WHEN` 

Consider the various R functions that create new variables based on an original variable.  The `CASE WHEN` function in **SQL** plays the role of multiple R functions including `ifelse()`, `case_when()`, and `cut()`.  

#### `ifelse()` {-}

With `ifelse()` the **R** translates directly to `CASE WHEN` in **SQL**, and the **SQL** query runs easily.

```{r}
airports |>
  mutate(sea = ifelse(alt > 500, "above sea", "near sea")) |>
  head(5)
```


```{r}
if_query <- airports |>
  mutate(sea = ifelse(alt > 500, "above sea", "near sea"))

show_query(if_query)
```


```{sql}
#| connection: con_air

SELECT *,
CASE WHEN (`alt` > 500.0) THEN 'above sea' WHEN NOT (`alt` > 500.0) THEN 'near sea' END AS `sea`
FROM `airports` 
LIMIT 5;
```


#### `case_when()`

With `case_when()` the **R** translates directly to `CASE WHEN` in **SQL**, and the **SQL** query runs easily.

```{r}
airports |>
  mutate(sea = case_when(
    alt < 500 ~ "near sea",
    alt < 2000 ~ "low alt",
    alt < 3000 ~ "mod alt",
    alt < 5500 ~ "high alt",
    alt > 5500 ~ "extreme alt")) |>
  head(5)
```



```{r}
cw_query <- airports |>
  mutate(sea = case_when(
    alt < 500 ~ "near sea",
    alt < 2000 ~ "low alt",
    alt < 3000 ~ "mod alt",
    alt < 5500 ~ "high alt",
    alt > 5500 ~ "extreme alt"))

show_query(cw_query)
```


```{sql}
#| connection: con_air

SELECT
  *,
  CASE
WHEN (`alt` < 500.0) THEN 'near sea'
WHEN (`alt` < 2000.0) THEN 'low alt'
WHEN (`alt` < 3000.0) THEN 'mod alt'
WHEN (`alt` < 5500.0) THEN 'high alt'
WHEN (`alt` > 5500.0) THEN 'extreme alt'
END AS `sea`
FROM `airports`
LIMIT 5;
```


#### `cut()` {-}

With `cut()` the **R** translates directly to `CASE WHEN` in **SQL**, and the **SQL** query runs easily.

```{r}
airports |>
  mutate(sea = cut(
    alt,
    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),
    labels = c("near sea", "low alt", "mod alt", "high alt", "extreme alt")
  )
)|>
  head(5)
```



```{r}
cw_query <- airports |>
  mutate(sea = cut(
    alt,
    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),
    labels = c("near sea", "low alt", "mod alt", "high alt", "extreme alt")
  )
)

show_query(cw_query)
```


```{sql}
#| connection: con_air

SELECT
  *,
  CASE
WHEN (`alt` <= 500.0) THEN 'near sea'
WHEN (`alt` <= 2000.0) THEN 'low alt'
WHEN (`alt` <= 3000.0) THEN 'mod alt'
WHEN (`alt` <= 5500.0) THEN 'high alt'
WHEN (`alt` > 5500.0) THEN 'extreme alt'
END AS `sea`
FROM `airports`
LIMIT 5;
```


### `SELECT DISTINCT`

How many distinct time zones are there in the `airports` table?  `distinct()` translates to `SELECT DISTINCT` which runs easily on our `tbl`.

```{r}
airports |>
  select(tz) |>
  distinct()
```


```{r}
dist_query <- airports |>
  select(tz) |>
  distinct()

show_query(dist_query)
```



```{sql}
#| connection: con_air

SELECT DISTINCT `tz`
FROM `airports`;
```




### `LIMIT` 

As expected, `head()` translates to `LIMIT` which we've already used many many times.

```{r}
airports |>
  head(5)
```


```{r}
head_query <- airports |>
  head(5)

show_query(head_query)
```


```{sql}
#| connection: con_air

SELECT *
FROM `airports`
LIMIT 5;
```

### Plotting (!?!?!)

What if we want to plot the values in a `tbl`?  Seems like `ggplot()` will support using a `tbl` as input.

```{r}
airports |>
  ggplot(aes(x = lon, y = lat)) +
  geom_point()
```

Let's ignore the airports outside of the US.

```{r}
airports |>
  filter(lon < 0) |>
  ggplot(aes(x = lon, y = lat)) +
  geom_point()
```

What does the `ggplot()` code translate to in **SQL**?  Of course there is an error!  **SQL** doesn't have any plotting mechanism, so there couldn't be any translation into **SQL**.  Which reminds us that different programming languages have different advantages and disadvantages.  The more we know about them, the better adept we will be at using the right language at the right time.

```{r}
#| error: true
gg_query <- airports |>
  filter(lon < 0) |>
  ggplot(aes(x = lon, y = lat)) +
  geom_point()

show_query(gg_query)
```


## Best practice

It is always a good idea to terminate the **SQL** connection when you are done with it.

```{r}
dbDisconnect(con_air, shutdown = TRUE)
```


## <i class="fas fa-lightbulb"></i> Reflection questions  

1. What is the main purpose of a `KEY`?  Can a `KEY` ever be `NULL`?  (Hint: does it depend on what type of `KEY` it is?)

2. What is the main purpose of an `INDEX`?  Can an `INDEX` ever be `NULL`?

3. How should we approach indexing?  That is, what variables are good candidates for choosing to be the index?

4. Why is it harder to calculate the median than the mean?

5. Why are there three functions in **R** (`ifelse()`, `case_when()`, and `cut()`) to do the job of a single function in **SQL** (`CASE WHEN`)?

6. Why won't **dbplyr** provide the syntax for the **SQL** translation of `ggplot()`?


## <i class="fas fa-balance-scale"></i> Ethics considerations 

1. What types of tasks is **R** best suited for?  What types of tasks is **SQL** best suited for?  Does it matter which one you use if they can both accomplish the same goal?

2. When setting up a database in **SQL** why is it important to consider potential users of the database and their potential questions / queries? That is, how is the *set-up* of the database related to the *use* of the database?


