[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science in R",
    "section": "",
    "text": "Welcome to Foundations of Data Science in R\nDaily class notes for Foundations of Data Science in R, by Jo Hardin, DS002R at Pomona College. Built on Modern Data Science with R by Baumer, Kaplan, and Horton, 3rd ed. and R for Data Science by Wickham, Çetinkaya-Rundel, and Grolemund, 2nd ed.\nYou are responsible for reading your texts. They are both free, excellent, and readable, so you should use them. That said, you should also make sure you are coming to class and asking lots of questions.\nMore information and course details can be found at the DS002R website.\n\nCopyright © 2024.\nVersion date: August 18, 2024.\nThe notes are available under a Creative Commons Attribution 4.0 United States License. License details are available at the Creative Commons website: https://creativecommons.org/licenses/by/4.0/.\nSource files for these notes can be found on GitHub at: https://github.com/hardin47/website/tree/gh-pages/DS002R.",
    "crumbs": [
      "Welcome to Foundations of Data Science in R"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Course Logistics\nWhat is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nProbability vs. Statistics\nWhat is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years.\nWho should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science.\nWhat are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended.\nIt is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nIs there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability?\nWhen should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors.\nWhat is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class.\nWhat software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R (using RStudio as a front end, called an integrated development environment, IDE). You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing.\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-logistics",
    "href": "01-intro.html#course-logistics",
    "title": "1  Introduction",
    "section": "",
    "text": "descriptive statistics describe the sample at hand with no intent on making generalizations.\n\ninferential statistics use a sample to make claims about a population\n\n\n\n\n\n\n\n\n\n\nYou may use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.)\nIf you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/.\nAll assignments should be turned in using R Markdown compiled to pdf.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-content",
    "href": "01-intro.html#course-content",
    "title": "1  Introduction",
    "section": "\n1.2 Course Content",
    "text": "1.2 Course Content\n\n1.2.1 Topics\nComputational Statistics can be a very large umbrella for many ideas. Indeed, sometimes the topics can seem somewhat disjointed. Below, I’ve categorized the topics we will cover into four groups. The four different broad topics all play different roles and can be more or less important depending on the problem at hand. None of the topics should exist on their own, because only with the bigger focus on all topics will any sort of data analysis / interpretation be accurate and compelling.\nLetting the computer help: R, RStudio, Git, GitHub, Reproducibility, Data Viz, Data Wrangling\nStatistics: Simulating, Randomization / Permutation Tests, Bootstrapping, Ethics\nMachine Learning: Classification, Clustering, Regular Expressions\nMathematics: Support Vector Machines\n\n1.2.2 Vocabulary\n\nA statistic is a numerical measurement we get from the sample, a function of the data.\nA parameter is a numerical measurement of the population. We never know the true value of the parameter.\nAn estimator is a function of the unobserved data that tries to approximate the unknown parameter value.\nAn estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.]\n\n\nOne of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015)\n\nWith thanks to Ben Baumer for perspective and sharing course materials.\n\n1.2.3 The Workflow\n\n\n\n\n\n\n\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\n\n\n\n\n\n\n\n\n\n\nStitch Fix Algorithms Tour\n\n\n\n\n1.2.4 Principles for the Data Science Process tl;dr\n(Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.)\nDuncan Temple Lang, University of California, Davis\nDuncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.]\nDuncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science.\nTemple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process:\n\nAsking a general question.\nRefining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood.\nAccess to data. This is unrelated to the science but does require computational skill.\nTransforming to data structures.\nExploratory data analyses to understand the data and determine whether the results will scale.\n\nThis is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results.\nTemple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student:\n\nMapping the general question to a statistical framework.\nUnderstanding the scope of inference, sampling, biases, and limitations.\nExploratory data analyses, including missing values, data quality, cleaning, matching, and fusing.\nUnderstanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability.\nConditional dependence and heterogeneity.\nDimension reduction, variable selection, and sparsity.\nSpurious relationships and multiple testing.\nParameter estimation versus “black box” prediction and classification.\nDiagnostics, residuals, and comparing models.\nQuantifying the uncertainty of a model.\nSampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed.\nStatistical accuracy versus computational complexity and efficiency.\n\nTemple Lang then briefly discussed some of the practical aspects of computing, including the following:\n\nAccessing data.\nManipulating raw data.\nData structures and storage, including correlated data.\nVisualization at all stages (particularly in exploratory data analyses and conveying the results).\nParallel computing, which can be challenging for a new student.\nTranslating high-level descriptions to optimal programs.\n\nDuring the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).]\nA workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#repro",
    "href": "01-intro.html#repro",
    "title": "1  Introduction",
    "section": "\n1.3 Reproducibility",
    "text": "1.3 Reproducibility\nReproducibility has long been considered an important topic for consideration in any research project. However, recently there has been increased press and available examples for understanding the impact that non-reproducible science can have.\nKitzes, Turek, and Deniz (2018) provide a full textbook on the structure of reproducible research as well as dozens of case studies to help hone skills and consider different aspects of the reproducible pipeline. Below are a handful of examples to get us started.\n\n1.3.1 Need for Reproducibility\n\n\n\n\n\n\n\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\n\n\n\nExample 1\nScience retracts gay marriage paper without agreement of lead author LaCour\n\n\n\n\n\n\n\n\n\nIn May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior.\nScience Editor-in-Chief Marcia McNutt:\n\nOriginal survey data not made available for independent reproduction of results.\nSurvey incentives misrepresented.\nSponsorship statement false.\n\n\nTwo Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked.\nMethods we’ll discuss can’t prevent this, but they can make it easier to discover issues.\nSource: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent\nExample 2\nSeizure study retracted after authors realize data got “terribly mixed”\n\n\n\n\n\n\n\n\n\nFrom the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates:\n\n\nThe article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness.\n\n\nSource: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/\nExample 3\nBad spreadsheet merge kills depression paper, quick fix resurrects it\n\n\n\n\n\n\n\n\n\nThe authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct.\n\n\nOriginal conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression …\n\n\nRevised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression …\n\n\nSource: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/\nExample 4\nPNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/\n\n\n\n\n\n\n\n\n\n1.3.2 The reproducible data analysis process\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\n\nScripting and literate programming\nDonald Knuth “Literate Programming” (1983)\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do.\n\n\nThe ideas of literate programming have been around for many years!\nand tools for putting them to practice have also been around\nbut they have never been as accessible as the current tools\nReproducibility checklist\n\nAre the tables and figures reproducible from the code and data?\nDoes the code actually do what you think it does?\nIn addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?)\nCan the code be used for other data?\nCan you extend the code to do other things?\nTools: R & R Studio\nSee this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.be\n\nYou must use both R and RStudio software programs\nR does the programming\nR Studio brings everything together\nYou may use Pomona’s server: https://rstudio.pomona.edu/\nSee course website for getting started: http://research.pomona.edu/johardin/math154f19/\n\n\n\n\n\n\n\n\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\n\n\n\n\n\n\n\n\n\n\nJessica Ward, PhD student at Newcastle University\n\n\n\nTools: Git & GitHub\n\nYou must submit your assignments via GitHub\nFollow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/\nClass specific instructions at https://m154-comp-stats.netlify.app/github.html\n\nAdmittedly, there is a steep learning curve with Git. However, it is among the tools which you are most likely to use in your future endeavors, so spending a little time focusing on the concepts now may pay off big time in the future. Beyond practicing and working through http://happygitwithr.com/, you may want to read a little bit about waht Git is doing behind the scences. This reference: Learn git concepts, not commands is very good and accessible.\nTools: a GitHub merge conflict (demo)\n\nOn GitHub (on the web) edit the README document and Commit it with a message describing what you did.\nThen, in RStudio also edit the README document with a different change.\n\nCommit your changes\nTry to push \\(\\rightarrow\\) you’ll get an error!\nTry pulling\nResolve the merge conflict and then commit and push\n\n\nAs you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important.\n\n\n\n\n\n\n\n\nhttps://xkcd.com/1597/\n\n\n\nSteps for weekly homework\n\nYou will get a link to the new assignment (clicking on the link will create a new private repo)\n\nUse R (within R Studio)\n\nNew Project, version control, Git\n\nClone the repo using SSH\n\n\n\nIf it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd\n\nDo the assignment\n\n\ncommit and push after every problem\n\n\n\nAll necessary files must be in the same folder (e.g., data)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-examples",
    "href": "01-intro.html#data-examples",
    "title": "1  Introduction",
    "section": "\n1.4 Data Examples",
    "text": "1.4 Data Examples\nWhat can/can’t Data Science Do?\n\nCan model the data at hand!\nCan find patterns & visualizations in large datasets.\nCan’t establish causation.\nCan’t represent data if it isn’t there.\nStats / Data Science / Math are not apolitical/agnostic\n\n“Inner city crime is reaching record levels” (Donald Trump, 8/30/16)\n“The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16)\n“Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16)\n“We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16)\n“If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16)\n“Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16)\n\nSource: http://www.politifact.com/truth-o-meter/statements/\n\n1.4.1 College Rankings Systems\nCheating\n\nBucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html)\n\nGaming the system\n\nPoint by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/)\n\nNo way to measure “quality of education”\nWhat is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!).\n\n1.4.2 Trump and Twitter\nAnalysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone.\n\nAug 9, 2016 http://varianceexplained.org/r/trump-tweets/\n\n\nMy analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures.\n\n\nAug 9, 2017 http://varianceexplained.org/r/trump-followup/\n\n\nThere is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post.\n\n\nI’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it.\n\n\nI’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement.\n\n\n1.4.3 Can Twitter Predict Election Results?\n\nIn 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013)\nWhat makes using Tweets to predict elections relevant to our class? (See Baumer (2015).)\n\nThe data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet.\nThe analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets).\nThe dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed.\nThe researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses.\n\nActivity\nSpend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions.\n\nworking paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423\npublished in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449.\neditorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html\neditorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html\neditorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Hat\n\nWrite a sentence summarizing the findings of the paper.\nDiscuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently?\n\nshould be proportion for the response variable. The bizarre scaling could dramatically change the results\ndots could then be scaled in proportion to the number of tweets\nlinear fit may be questionable.\nHow would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement?\n\n\nInterpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units.\nDiscuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election?\n\n\n\\(R^2\\) is way higher after control variables are included, but duh!\nthe full model will likely do a better job of predicting\n\n\nWhy do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election?\n\nAfter controlling for how many Republicans are in the district, most of the effect disappears\nWhile the coefficient of the main term is still statistically significant, the size of the coefficient\n(155 +/- 43 votes) is of little practical significance\n\n\nDo you think the study holds water? Why or why not? What are the shortcomings of this study?\n\nNot really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model?\n\n\nData Scientist Hat\nImagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor.\n\nWhat steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform.\nWhat computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities & knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in?\n\nHow much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\nCheap\nCan measure any political race (not just the wealthy ones).\nDisadvantages\n\nIs it really reflective of the voting populace? Who would it bias toward?\nDoes simple mention of a candidate always reflect voting patterns? When wouldn’t it?\nMargin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\).\nTweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling?\nCan’t measure any demographic information.\nWhat could be done differently?\n\nGelman: look only at close races\nGelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.”\nGelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.”\nGelman: Make the data publicly available so that others can try to reproduce the results\nTweeting and R\nThe twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#reflection-questions",
    "href": "01-intro.html#reflection-questions",
    "title": "1  Introduction",
    "section": "\n1.5  Reflection questions",
    "text": "1.5  Reflection questions",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#ethics-considerations",
    "href": "01-intro.html#ethics-considerations",
    "title": "1  Introduction",
    "section": "\n1.6  Ethics considerations",
    "text": "1.6  Ethics considerations\n\n\n\nProbability vs. Statistics\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\nStitch Fix Algorithms Tour\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nhttps://xkcd.com/1597/\n\n\n\nBaumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” The American Statistician.\n\n\nDiGrazia, Joseph, Karissa McKelvey, Johan Bollen, and Fabio Rojas. 2013. “More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior.” PLoS ONE 8 (11): e79449.\n\n\nKitzes, Justin, Daniel Turek, and Fatma Deniz, eds. 2018. In The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. University of California Press.\n\n\nRojas, Fabio. 2013. “How Twitter Can Predict and Election.” The Washington Post.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "16-reprex.html",
    "href": "16-reprex.html",
    "title": "16  Reproducible examples",
    "section": "",
    "text": "16.1 reprex()\nIn order to create a reproducible example …\nStep 1. Copy code onto the clipboard\nStep 2. Type reprex() into the Console\nStep 3. Look at the Viewer to the right. Copy the Viewer output into GitHub, Piazza, an email, stackexchange, etc.\nSome places to learn more about reprex include",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible examples</span>"
    ]
  },
  {
    "objectID": "16-reprex.html#reprex",
    "href": "16-reprex.html#reprex",
    "title": "16  Reproducible examples",
    "section": "",
    "text": "Help me help you\n\n\n\n\n\n\n\nA blog about it: https://teachdatascience.com/reprex/\nThe reprex vignette: https://reprex.tidyverse.org/index.html\n\nreprex dos and donts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nJenny Bryan webinar on reprex: “Help me help you. Creating reproducible examples” https://resources.rstudio.com/webinars/help-me-help-you-creating-reproducible-examples-jenny-bryan\nSome advice: https://stackoverflow.com/help/minimal-reproducible-example\n\n\n16.1.0.1 reprex demo\nreprex(\n  jan31 + months(0:11) + days(31)\n)\nmultiple lines of code:\nreprex({\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})\nreprex({\n  library(lubridate)\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible examples</span>"
    ]
  },
  {
    "objectID": "16-reprex.html#reflection-questions",
    "href": "16-reprex.html#reflection-questions",
    "title": "16  Reproducible examples",
    "section": "\n16.2  Reflection questions",
    "text": "16.2  Reflection questions",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible examples</span>"
    ]
  },
  {
    "objectID": "16-reprex.html#ethics-considerations",
    "href": "16-reprex.html#ethics-considerations",
    "title": "16  Reproducible examples",
    "section": "\n16.3  Ethics considerations",
    "text": "16.3  Ethics considerations",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reproducible examples</span>"
    ]
  },
  {
    "objectID": "15-R-tip-of-the-day.html",
    "href": "15-R-tip-of-the-day.html",
    "title": "15  R tip-of-the-day",
    "section": "",
    "text": "15.1  Reflection questions",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R tip-of-the-day</span>"
    ]
  },
  {
    "objectID": "15-R-tip-of-the-day.html#ethics-considerations",
    "href": "15-R-tip-of-the-day.html#ethics-considerations",
    "title": "15  R tip-of-the-day",
    "section": "\n15.2  Ethics considerations",
    "text": "15.2  Ethics considerations",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>R tip-of-the-day</span>"
    ]
  },
  {
    "objectID": "14-version-control.html",
    "href": "14-version-control.html",
    "title": "14  Version control",
    "section": "",
    "text": "14.1  Reflection questions",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Version control</span>"
    ]
  },
  {
    "objectID": "14-version-control.html#ethics-considerations",
    "href": "14-version-control.html#ethics-considerations",
    "title": "14  Version control",
    "section": "\n14.2  Ethics considerations",
    "text": "14.2  Ethics considerations",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Version control</span>"
    ]
  },
  {
    "objectID": "13-permutation-tests.html",
    "href": "13-permutation-tests.html",
    "title": "13  Permutation tests",
    "section": "",
    "text": "13.1  Reflection questions",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Permutation tests</span>"
    ]
  },
  {
    "objectID": "13-permutation-tests.html#ethics-considerations",
    "href": "13-permutation-tests.html#ethics-considerations",
    "title": "13  Permutation tests",
    "section": "\n13.2  Ethics considerations",
    "text": "13.2  Ethics considerations",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Permutation tests</span>"
    ]
  },
  {
    "objectID": "12-iteration.html",
    "href": "12-iteration.html",
    "title": "12  Iteration",
    "section": "",
    "text": "12.1  Reflection questions",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "12-iteration.html#ethics-considerations",
    "href": "12-iteration.html#ethics-considerations",
    "title": "12  Iteration",
    "section": "\n12.2  Ethics considerations",
    "text": "12.2  Ethics considerations",
    "crumbs": [
      "Regular expressions",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Iteration</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html",
    "href": "11-ggplot.html",
    "title": "11  ggplot",
    "section": "",
    "text": "11.1 Examples\nThe first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html#examples",
    "href": "11-ggplot.html#examples",
    "title": "11  ggplot",
    "section": "",
    "text": "An aside\nGenerally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi\nAs mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (Tufte 1997).\n\n“An essential analytic task in making decisions based on evidence is to understand how things work.”\nMaking decisions based on evidence requires the appropriate display of that evidence.”\n\nBack to the examples…\n\n11.1.1 Cholera via Tufte\nIn September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after.\nWhy was John Snow successful at solving the problem? Some thoughts to consider (as reported in Tufte (1997)):\n\nThe bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means.\nData in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area.\nComparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease.\nAlternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case.\nDid removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline?\nAssessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told?\n\n11.1.2 Challenger via Tufte\nJohn Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F).\nUnlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated!\nThe biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what??\n\n\nThe engineers who understood the problem created tables and engineering graphs which were\n\nNot visually appealing.\nNot decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”)\nThere was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility.\n\n\n\nThe information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure.\nMissing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.)\nAnecdotal evidence was misconstrued: SRM-15 at 57F had the most damage, but SRM-22 at 75F had the second most damage.\nIn the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F.\nTufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order.\n\nAs with the cholera outbreak, a persuasive argument could have been made if the visualizations had\n\nbeen in context plot data versus temperature not time!,\nused appropriate comparisons: as compared with what?,\nconsider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and\nthe graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?.\n\nTufte (Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms.\n\n\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html#thoughts",
    "href": "11-ggplot.html#thoughts",
    "title": "11  ggplot",
    "section": "\n11.2 Thoughts on Plotting",
    "text": "11.2 Thoughts on Plotting\n\n11.2.1 Advice\n\nBasic plotting\n\nAvoid having other graph elements interfere with data\nUse visually prominent symbols\nAvoid over-plotting (One way to avoid over plotting: Jitter the values)\nDifferent values of data may obscure each other\nInclude all or nearly all of the data\nFill data region\n\n\nEliminate superfluous material\n\nChart junk & stuff that adds no meaning, e.g. butterflies on top of barplots, background images\nExtra tick marks and grid lines\nUnnecessary text and arrows\nDecimal places beyond the measurement error or the level of difference\n\n\nFacilitate Comparisons\n\nPut juxtaposed plots on same scale\nMake it easy to distinguish elements of superposed plots (e.g. color)\nEmphasizes the important difference\nComparison: volume, area, height (be careful, volume can seem bigger than you mean it to)\n\n\nChoosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.)\n\nKeep scales on x and y axes the same for both plots to facilitate the comparison\nZoom in to focus on the region that contains the bulk of the data\nKeep the scale the same throughout the plot (i.e. don’t change it mid-axis)\nOrigin need not be on the scale\nChoose a scale that improves resolution\nAvoid jiggling the baseline\n\n\nHow to make a plot information rich\n\nDescribe what you see in the caption\nAdd context with reference markers (lines and points) including text\nAdd legends and labels\nUse color and plotting symbols to add more information\nPlot the same thing more than once in different ways/scales\nReduce clutter\n\n\nCaptions should\n\nBe comprehensive\nSelf-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graph\n\n\nGood Plot Making Practice\n\nPut major conclusions in graphical form\nProvide reference information\nProof read for clarity and consistency\nGraphing is an iterative process\nMultiplicity is OK, i.e. two plots of the same variable may provide different messages\nMake plots data rich\n\n\n\nCreating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data.\n\n11.2.2 An example from Information is Beautiful\n(See HW2 for details on R code)\nConsider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph.\n\n\n\n\n\n\n\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\n\n\n\nI have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations.\n\n\n\n\n\n\n\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nData retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0\n\n11.2.2.1 Fonts Matter\nAt RStudio::conf 2020, The Glamour of Graphics, Will Chase makes some very important points about how and why making good graphics matters. The talk might be summarized by the plot below: fonts matter.\n\n\n\n\n\n\n\n\n\n11.2.3 Assessing Graphics (and Other Analyses)\n\n\n\n\n\n\n\n\nCritical Task\nNeeds Improvement\nBasic\nSurpassed\n\n\n\n\nComputation Perform computations\nComputations contain errors and extraneous code\nComputations are correct but contain extraneous / unnecessary computations\nComputations are correct and properly identified and labeled\n\n\n\nAnalysis Choose and carry out analysis appropriate for data and content(s)\nChoice of analysis is overly simplistic, irrelevant, or missing key component\nAnalysis appropriate, but incomplete, or not important features and assumptions not made explicit\nAnalysis appropriate, complete, advanced, relevant, and informative\n\n\n\nSynthesis Identify key features of the analysis, and interpret results (including context)\nConclusions are missing, incorrect, or not made based on results of analysis\nConclusions reasonable, but is partially correct or partially complete\nMake relevant conclusions explicitly connect to analysis and to context\n\n\n\nVisual presentation Communicate findings graphically clearly, precisely, and concisely\nInappropriate choice of plots; poorly labeled plots; plots missing\nPlots convey information correctly but lack context for interpretation\nPlots convey information correctly with adequate / appropriate reference information\n\n\n\nWritten Communicate findings clearly, precisely, and concisely\nExplanation is illogical, incorrect, or incoherent\nExplanation is partially correct but incomplete or unconvincing\nExplanation is correct, complete, and convincing\n\n\n\nA rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016).",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html#deconstruct",
    "href": "11-ggplot.html#deconstruct",
    "title": "11  ggplot",
    "section": "\n11.3 Deconstructing a graph",
    "text": "11.3 Deconstructing a graph\n\n11.3.1 The Grammar of Graphics (gg)\nYau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence.\nOne great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?)\nThe most important questions you can ask with respect to creating figures are:\n\nWhat do we want R to do? (What is the goal?)\nWhat does R need to know?\n\nYau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2021), chapter 2.)\n\nVisual Cues: the aspects of the figure where we should focus.Position (numerical) where in relation to other things?Length (numerical) how big (in one dimension)?Angle (numerical) how wide? parallel to something else?Direction (numerical) at what slope? In a time series, going up or down?Shape (categorical) belonging to what group?Area (numerical) how big (in two dimensions)? Beware of improper scaling!Volume (numerical) how big (in three dimensions)? Beware of improper scaling!Shade (either) to what extent? how severely?Color (either) to what extent? how severely? Beware of red/green color blindness.\nCoordinate System: rectangular, polar, geographic, etc.\nScale: numeric (linear? logarithmic?), categorical (ordered?), time\nContext: in comparison to what (think back to ideas from Tufte)\n\n\n\n\n\n\n\n\n\nOrder Matters\n\n\n\n\n\n\n\n\nCues Together\n\n\n\n\n\n\n\n\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\n\n11.3.1.1 The grammar of graphics in ggplot2\n\ngeom: the geometric “shape” used to display data\n\nbar, point, line, ribbon, text, etc.\n\naesthetic: an attribute controlling how geom is displayed with respect to variables\n\nx position, y position, color, fill, shape, size, etc.\n\nscale: adjust information in the aesthetic to map onto the plot\n\n\nparticular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values.\n\nguide: helps user convert visual data back into raw data (legends, axes)\nstat: a transformation applied to data before geom gets it\n\nexample: histograms work on binned data\n\n11.3.2 ggplot2\n\nIn ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2021). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births.\n\nGoals\nWhat I will try to do\n\ngive a tour of ggplot2\nexplain how to think about plots the ggplot2 way\nprepare/encourage you to learn more later\n\nWhat I can’t do in one session\n\nshow every bell and whistle\nmake you an expert at using ggplot2\nGetting help\n\nOne of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/\nggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nLook at the end of the presentation. More help options there.\n\n\n\n\n\n\n\n\n\n\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)\n\n        date births wday year month day_of_year day_of_month day_of_week\n1 1978-01-01   7701  Sun 1978     1           1            1           1\n2 1978-01-02   7527  Mon 1978     1           2            2           2\n3 1978-01-03   8825  Tue 1978     1           3            3           3\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nTwo Questions:\n\nWhat do we want R to do? (What is the goal?)\n\nWhat does R need to know?\n\ndata source: Births78\n\naesthetics:\n\ndate -&gt; x\nbirths -&gt; y\npoints (!)\n\n\n\n\n\nGoal: scatterplot = a plot with points\n\nggplot() + geom_point()\n\n\n\nWhat does R need to know?\n\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nWhat has changed?\n\nnew aesthetic: mapping color to day of week\nAdding day of week to the data set\nThe wday() function in the lubridate package computes the day of the week from a date.\n\nBirths78 &lt;-  \n  Births78 %&gt;% \n  mutate(wday = lubridate::wday(date, label=TRUE))\n\n\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we use lines instead of dots\n\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we have two layers, one with points and one with lines\n\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\nThe layers are placed one on top of the other: the points are below and the lines are above.\ndata and aes specified in ggplot() affect all geoms\nAlternative Syntax\n\nBirths78 %&gt;% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nWhat does adding the color argument do?\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\n\nBecause there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy.\nSetting vs. Mapping\nIf we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable:\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\nNote that color = \"navy\" is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting.\nHow can we make the plot?\n\n\n\n\n\n\n\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")\n\n\nggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults.\ngood practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah()\nSetting vs. Mapping (again)\nInformation gets passed to the plot via:\n\nmap the variable information inside the aes (aesthetic) command\nset the non-variable information outside the aes (aesthetic) command\nOther geoms\n\napropos(\"^geom_\")\n\n [1] \"geom_abline\"            \"geom_area\"              \"geom_ash\"              \n [4] \"geom_bar\"               \"geom_bin_2d\"            \"geom_bin2d\"            \n [7] \"geom_blank\"             \"geom_boxplot\"           \"geom_col\"              \n[10] \"geom_contour\"           \"geom_contour_filled\"    \"geom_count\"            \n[13] \"geom_crossbar\"          \"geom_curve\"             \"geom_density\"          \n[16] \"geom_density_2d\"        \"geom_density_2d_filled\" \"geom_density2d\"        \n[19] \"geom_density2d_filled\"  \"geom_dotplot\"           \"geom_errorbar\"         \n[22] \"geom_errorbarh\"         \"geom_freqpoly\"          \"geom_function\"         \n[25] \"geom_hex\"               \"geom_histogram\"         \"geom_hline\"            \n[28] \"geom_jitter\"            \"geom_label\"             \"geom_line\"             \n[31] \"geom_linerange\"         \"geom_lm\"                \"geom_map\"              \n[34] \"geom_path\"              \"geom_point\"             \"geom_pointrange\"       \n[37] \"geom_polygon\"           \"geom_qq\"                \"geom_qq_line\"          \n[40] \"geom_quantile\"          \"geom_raster\"            \"geom_rect\"             \n[43] \"geom_ribbon\"            \"geom_rug\"               \"geom_segment\"          \n[46] \"geom_sf\"                \"geom_sf_label\"          \"geom_sf_text\"          \n[49] \"geom_smooth\"            \"geom_spline\"            \"geom_spoke\"            \n[52] \"geom_step\"              \"geom_text\"              \"geom_tile\"             \n[55] \"geom_violin\"            \"geom_vline\"            \n\n\nhelp pages will tell you their aesthetics, default stats, etc.\n\n?geom_area             # for example\n\nLet’s try geom_area\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nUsing area does not produce a good plot\n\nover plotting is hiding much of the data\nextending y-axis to 0 may or may not be desirable.\nSide note: what makes a plot good?\nMost (all?) graphics are intended to help us make comparisons\n\nHow does something change over time?\nDo my treatments matter? How much?\nDo men and women respond the same way?\n\nKey plot metric: Does my plot make the comparisons I am interested in\n\neasily, and\naccurately?\nTime for some different data\nHELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial\n\nhead(HELPrct)\n\n  age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n1  37            1    yes   49  3        177      225        0  NA      0\n2  37            1    yes   30 22          2       NA        0  NA      0\n3  26            1    yes   39  0          3      365       20  NA      0\n4  39            1    yes   15  2        189      343        0   1      1\n5  32            1    yes   39 12          2       57        0   1      0\n6  47            1    yes    6  1         31      365        0  NA      1\n     sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n2   male yes homeless 56 62  2     43         NA &lt;NA&gt; 26.670307 36.03694      1\n3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n  racegrp satreat sexrisk substance treat avg_drinks max_drinks\n1   black      no       4   cocaine   yes         13         26\n2   white      no       7   alcohol   yes         56         62\n3   black      no       2    heroin    no          0          0\n4   white     yes       4    heroin    no          5          5\n5   black      no       6   cocaine    no         10         13\n6   black      no       5   cocaine   yes          4          4\n  hospitalizations\n1                3\n2               22\n3                0\n4                2\n5               12\n6                1\n\n\nSubjects admitted for treatment for addiction to one of three substances.\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\nHmm. What’s up with y?\n\n\nstat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values.\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nlibrary(scales)\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice the messages\n\nstat_bin: Histograms are not mapping the raw data but binned data.stat_bin() performs the data transformation.\nbinwidth: a default binwidth has been selected, but we should really choose our own.\nSetting the binwidth manually\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study? – Other geoms\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery geom comes with a default stat\n\nfor simple cases, the stat is stat_identity() which does nothing\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery stat comes with a default geom, every geom with a default stat\n\nwe can specify stats instead of geom, if we prefer\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMore combinations\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow much do they drink? (i1)\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nCovariates: Adding in more variables\nUsing color and linetype:\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nUsing color and facets\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nBoxplots\nBoxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”.\nThe quantitative variable must be y, and there must be an additional x variable.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHorizontal boxplots\nHorizontal boxplots are obtained by flipping the coordinate system:\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\ncoord_flip() may be used with other plots as well to reverse the roles of x and y on the plot.\nAxes scaling with boxplots\nWe can scale the continuous axis\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nGive me some space\nWe’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nIssues with bigger data\n\nrequire(NHANES)\ndim(NHANES)\n\n[1] 10000    76\n\nNHANES %&gt;%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\n\nAlthough we can see a generally positive association (as we would expect), the over plotting may be hiding information.\nUsing alpha (opacity)\nOne way to deal with over plotting is to set the opacity low.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\ngeom_density2d\nAlternatively (or simultaneously) we might prefer a different geom altogether.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nThings I haven’t mentioned (much)\n\ncoords (coord_flip() is good to know about)\nthemes (for customizing appearance)\nposition (position_dodge(), position_jitterdodge(), position_stack(), etc.)\ntransforming axes\n\n\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\n\n\n\n\n\n\n\n\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nA little bit of everything\n\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWant to learn more?\n\ndocs.ggplot2.org/\nWinston Chang’s: R Graphics Cookbook\n\n\n\n\n\n\n\n\n\nWhat else can we do?\nshiny\n\ninteractive graphics / modeling\nhttps://shiny.rstudio.com/\n\nplotly\n\nPlotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function.\n\n\nhttps://plot.ly/ggplot2/getting-started/\n\nDynamic documents\n\ncombination of RMarkdown, ggvis, and shiny",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html#reflection-questions",
    "href": "11-ggplot.html#reflection-questions",
    "title": "11  ggplot",
    "section": "\n11.4  Reflection questions",
    "text": "11.4  Reflection questions",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "11-ggplot.html#ethics-considerations",
    "href": "11-ggplot.html#ethics-considerations",
    "title": "11  ggplot",
    "section": "\n11.5  Ethics considerations",
    "text": "11.5  Ethics considerations\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nBaumer, Ben, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. CRC Press. https://mdsr-book.github.io/mdsr2e/.\n\n\nGelman, Andrew. 2011. “Rejoinder.” Journal of Computational and Graphical Statistics 20: 36–40. http://arxiv.org/abs/1503.00781.\n\n\nNolan, Deborah, and Jamis Perrett. 2016. “Teaching and Learning Data Visualization: Ideas and Assignments.” The American Statistician.\n\n\nTufte, Edward. 1997. “Visual Explanations: Images and Quantities, Evidence and Narrative.” In, 27–54. Graphics Press, LLC. www.edwardtufte.com.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.\n\n\nYau, Nathan. 2013. Data Points: Visualization That Means Something. Wiley.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>ggplot</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html",
    "href": "10-grammar-graphics.html",
    "title": "10  Grammar of graphics",
    "section": "",
    "text": "10.1 Examples\nThe first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html#examples",
    "href": "10-grammar-graphics.html#examples",
    "title": "10  Grammar of graphics",
    "section": "",
    "text": "An aside\nGenerally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi\nAs mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (Tufte 1997).\n\n“An essential analytic task in making decisions based on evidence is to understand how things work.”\nMaking decisions based on evidence requires the appropriate display of that evidence.”\n\nBack to the examples…\n\n10.1.1 Cholera via Tufte\nIn September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after.\nWhy was John Snow successful at solving the problem? Some thoughts to consider (as reported in Tufte (1997)):\n\nThe bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means.\nData in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area.\nComparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease.\nAlternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case.\nDid removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline?\nAssessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told?\n\n10.1.2 Challenger via Tufte\nJohn Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F).\nUnlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated!\nThe biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what??\n\n\nThe engineers who understood the problem created tables and engineering graphs which were\n\nNot visually appealing.\nNot decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”)\nThere was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility.\n\n\n\nThe information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure.\nMissing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.)\nAnecdotal evidence was misconstrued: SRM-15 at 57F had the most damage, but SRM-22 at 75F had the second most damage.\nIn the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F.\nTufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order.\n\nAs with the cholera outbreak, a persuasive argument could have been made if the visualizations had\n\nbeen in context plot data versus temperature not time!,\nused appropriate comparisons: as compared with what?,\nconsider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and\nthe graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?.\n\nTufte (Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms.\n\n\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html#thoughts",
    "href": "10-grammar-graphics.html#thoughts",
    "title": "10  Grammar of graphics",
    "section": "\n10.2 Thoughts on Plotting",
    "text": "10.2 Thoughts on Plotting\n\n10.2.1 Advice\n\nBasic plotting\n\nAvoid having other graph elements interfere with data\nUse visually prominent symbols\nAvoid over-plotting (One way to avoid over plotting: Jitter the values)\nDifferent values of data may obscure each other\nInclude all or nearly all of the data\nFill data region\n\n\nEliminate superfluous material\n\nChart junk & stuff that adds no meaning, e.g. butterflies on top of barplots, background images\nExtra tick marks and grid lines\nUnnecessary text and arrows\nDecimal places beyond the measurement error or the level of difference\n\n\nFacilitate Comparisons\n\nPut juxtaposed plots on same scale\nMake it easy to distinguish elements of superposed plots (e.g. color)\nEmphasizes the important difference\nComparison: volume, area, height (be careful, volume can seem bigger than you mean it to)\n\n\nChoosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.)\n\nKeep scales on x and y axes the same for both plots to facilitate the comparison\nZoom in to focus on the region that contains the bulk of the data\nKeep the scale the same throughout the plot (i.e. don’t change it mid-axis)\nOrigin need not be on the scale\nChoose a scale that improves resolution\nAvoid jiggling the baseline\n\n\nHow to make a plot information rich\n\nDescribe what you see in the caption\nAdd context with reference markers (lines and points) including text\nAdd legends and labels\nUse color and plotting symbols to add more information\nPlot the same thing more than once in different ways/scales\nReduce clutter\n\n\nCaptions should\n\nBe comprehensive\nSelf-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graph\n\n\nGood Plot Making Practice\n\nPut major conclusions in graphical form\nProvide reference information\nProof read for clarity and consistency\nGraphing is an iterative process\nMultiplicity is OK, i.e. two plots of the same variable may provide different messages\nMake plots data rich\n\n\n\nCreating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data.\n\n10.2.2 An example from Information is Beautiful\n(See HW2 for details on R code)\nConsider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph.\n\n\n\n\n\n\n\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\n\n\n\nI have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations.\n\n\n\n\n\n\n\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nData retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0\n\n10.2.2.1 Fonts Matter\nAt RStudio::conf 2020, The Glamour of Graphics, Will Chase makes some very important points about how and why making good graphics matters. The talk might be summarized by the plot below: fonts matter.\n\n\n\n\n\n\n\n\n\n10.2.3 Assessing Graphics (and Other Analyses)\n\n\n\n\n\n\n\n\nCritical Task\nNeeds Improvement\nBasic\nSurpassed\n\n\n\n\nComputation Perform computations\nComputations contain errors and extraneous code\nComputations are correct but contain extraneous / unnecessary computations\nComputations are correct and properly identified and labeled\n\n\n\nAnalysis Choose and carry out analysis appropriate for data and content(s)\nChoice of analysis is overly simplistic, irrelevant, or missing key component\nAnalysis appropriate, but incomplete, or not important features and assumptions not made explicit\nAnalysis appropriate, complete, advanced, relevant, and informative\n\n\n\nSynthesis Identify key features of the analysis, and interpret results (including context)\nConclusions are missing, incorrect, or not made based on results of analysis\nConclusions reasonable, but is partially correct or partially complete\nMake relevant conclusions explicitly connect to analysis and to context\n\n\n\nVisual presentation Communicate findings graphically clearly, precisely, and concisely\nInappropriate choice of plots; poorly labeled plots; plots missing\nPlots convey information correctly but lack context for interpretation\nPlots convey information correctly with adequate / appropriate reference information\n\n\n\nWritten Communicate findings clearly, precisely, and concisely\nExplanation is illogical, incorrect, or incoherent\nExplanation is partially correct but incomplete or unconvincing\nExplanation is correct, complete, and convincing\n\n\n\nA rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016).",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html#deconstruct",
    "href": "10-grammar-graphics.html#deconstruct",
    "title": "10  Grammar of graphics",
    "section": "\n10.3 Deconstructing a graph",
    "text": "10.3 Deconstructing a graph\n\n10.3.1 The Grammar of Graphics (gg)\nYau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence.\nOne great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?)\nThe most important questions you can ask with respect to creating figures are:\n\nWhat do we want R to do? (What is the goal?)\nWhat does R need to know?\n\nYau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2021), chapter 2.)\n\nVisual Cues: the aspects of the figure where we should focus.Position (numerical) where in relation to other things?Length (numerical) how big (in one dimension)?Angle (numerical) how wide? parallel to something else?Direction (numerical) at what slope? In a time series, going up or down?Shape (categorical) belonging to what group?Area (numerical) how big (in two dimensions)? Beware of improper scaling!Volume (numerical) how big (in three dimensions)? Beware of improper scaling!Shade (either) to what extent? how severely?Color (either) to what extent? how severely? Beware of red/green color blindness.\nCoordinate System: rectangular, polar, geographic, etc.\nScale: numeric (linear? logarithmic?), categorical (ordered?), time\nContext: in comparison to what (think back to ideas from Tufte)\n\n\n\n\n\n\n\n\n\nOrder Matters\n\n\n\n\n\n\n\n\nCues Together\n\n\n\n\n\n\n\n\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\n\n10.3.1.1 The grammar of graphics in ggplot2\n\ngeom: the geometric “shape” used to display data\n\nbar, point, line, ribbon, text, etc.\n\naesthetic: an attribute controlling how geom is displayed with respect to variables\n\nx position, y position, color, fill, shape, size, etc.\n\nscale: adjust information in the aesthetic to map onto the plot\n\n\nparticular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values.\n\nguide: helps user convert visual data back into raw data (legends, axes)\nstat: a transformation applied to data before geom gets it\n\nexample: histograms work on binned data\n\n10.3.2 ggplot2\n\nIn ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2021). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births.\n\nGoals\nWhat I will try to do\n\ngive a tour of ggplot2\nexplain how to think about plots the ggplot2 way\nprepare/encourage you to learn more later\n\nWhat I can’t do in one session\n\nshow every bell and whistle\nmake you an expert at using ggplot2\nGetting help\n\nOne of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/\nggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nLook at the end of the presentation. More help options there.\n\n\n\n\n\n\n\n\n\n\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)\n\n        date births wday year month day_of_year day_of_month day_of_week\n1 1978-01-01   7701  Sun 1978     1           1            1           1\n2 1978-01-02   7527  Mon 1978     1           2            2           2\n3 1978-01-03   8825  Tue 1978     1           3            3           3\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nTwo Questions:\n\nWhat do we want R to do? (What is the goal?)\n\nWhat does R need to know?\n\ndata source: Births78\n\naesthetics:\n\ndate -&gt; x\nbirths -&gt; y\npoints (!)\n\n\n\n\n\nGoal: scatterplot = a plot with points\n\nggplot() + geom_point()\n\n\n\nWhat does R need to know?\n\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nWhat has changed?\n\nnew aesthetic: mapping color to day of week\nAdding day of week to the data set\nThe wday() function in the lubridate package computes the day of the week from a date.\n\nBirths78 &lt;-  \n  Births78 %&gt;% \n  mutate(wday = lubridate::wday(date, label=TRUE))\n\n\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we use lines instead of dots\n\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we have two layers, one with points and one with lines\n\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\nThe layers are placed one on top of the other: the points are below and the lines are above.\ndata and aes specified in ggplot() affect all geoms\nAlternative Syntax\n\nBirths78 %&gt;% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nWhat does adding the color argument do?\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\n\nBecause there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy.\nSetting vs. Mapping\nIf we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable:\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\nNote that color = \"navy\" is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting.\nHow can we make the plot?\n\n\n\n\n\n\n\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")\n\n\nggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults.\ngood practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah()\nSetting vs. Mapping (again)\nInformation gets passed to the plot via:\n\nmap the variable information inside the aes (aesthetic) command\nset the non-variable information outside the aes (aesthetic) command\nOther geoms\n\napropos(\"^geom_\")\n\n [1] \"geom_abline\"            \"geom_area\"              \"geom_ash\"              \n [4] \"geom_bar\"               \"geom_bin_2d\"            \"geom_bin2d\"            \n [7] \"geom_blank\"             \"geom_boxplot\"           \"geom_col\"              \n[10] \"geom_contour\"           \"geom_contour_filled\"    \"geom_count\"            \n[13] \"geom_crossbar\"          \"geom_curve\"             \"geom_density\"          \n[16] \"geom_density_2d\"        \"geom_density_2d_filled\" \"geom_density2d\"        \n[19] \"geom_density2d_filled\"  \"geom_dotplot\"           \"geom_errorbar\"         \n[22] \"geom_errorbarh\"         \"geom_freqpoly\"          \"geom_function\"         \n[25] \"geom_hex\"               \"geom_histogram\"         \"geom_hline\"            \n[28] \"geom_jitter\"            \"geom_label\"             \"geom_line\"             \n[31] \"geom_linerange\"         \"geom_lm\"                \"geom_map\"              \n[34] \"geom_path\"              \"geom_point\"             \"geom_pointrange\"       \n[37] \"geom_polygon\"           \"geom_qq\"                \"geom_qq_line\"          \n[40] \"geom_quantile\"          \"geom_raster\"            \"geom_rect\"             \n[43] \"geom_ribbon\"            \"geom_rug\"               \"geom_segment\"          \n[46] \"geom_sf\"                \"geom_sf_label\"          \"geom_sf_text\"          \n[49] \"geom_smooth\"            \"geom_spline\"            \"geom_spoke\"            \n[52] \"geom_step\"              \"geom_text\"              \"geom_tile\"             \n[55] \"geom_violin\"            \"geom_vline\"            \n\n\nhelp pages will tell you their aesthetics, default stats, etc.\n\n?geom_area             # for example\n\nLet’s try geom_area\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nUsing area does not produce a good plot\n\nover plotting is hiding much of the data\nextending y-axis to 0 may or may not be desirable.\nSide note: what makes a plot good?\nMost (all?) graphics are intended to help us make comparisons\n\nHow does something change over time?\nDo my treatments matter? How much?\nDo men and women respond the same way?\n\nKey plot metric: Does my plot make the comparisons I am interested in\n\neasily, and\naccurately?\nTime for some different data\nHELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial\n\nhead(HELPrct)\n\n  age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n1  37            1    yes   49  3        177      225        0  NA      0\n2  37            1    yes   30 22          2       NA        0  NA      0\n3  26            1    yes   39  0          3      365       20  NA      0\n4  39            1    yes   15  2        189      343        0   1      1\n5  32            1    yes   39 12          2       57        0   1      0\n6  47            1    yes    6  1         31      365        0  NA      1\n     sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n2   male yes homeless 56 62  2     43         NA &lt;NA&gt; 26.670307 36.03694      1\n3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n  racegrp satreat sexrisk substance treat avg_drinks max_drinks\n1   black      no       4   cocaine   yes         13         26\n2   white      no       7   alcohol   yes         56         62\n3   black      no       2    heroin    no          0          0\n4   white     yes       4    heroin    no          5          5\n5   black      no       6   cocaine    no         10         13\n6   black      no       5   cocaine   yes          4          4\n  hospitalizations\n1                3\n2               22\n3                0\n4                2\n5               12\n6                1\n\n\nSubjects admitted for treatment for addiction to one of three substances.\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\nHmm. What’s up with y?\n\n\nstat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values.\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nlibrary(scales)\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice the messages\n\nstat_bin: Histograms are not mapping the raw data but binned data.stat_bin() performs the data transformation.\nbinwidth: a default binwidth has been selected, but we should really choose our own.\nSetting the binwidth manually\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study? – Other geoms\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery geom comes with a default stat\n\nfor simple cases, the stat is stat_identity() which does nothing\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery stat comes with a default geom, every geom with a default stat\n\nwe can specify stats instead of geom, if we prefer\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMore combinations\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow much do they drink? (i1)\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nCovariates: Adding in more variables\nUsing color and linetype:\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nUsing color and facets\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nBoxplots\nBoxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”.\nThe quantitative variable must be y, and there must be an additional x variable.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHorizontal boxplots\nHorizontal boxplots are obtained by flipping the coordinate system:\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\ncoord_flip() may be used with other plots as well to reverse the roles of x and y on the plot.\nAxes scaling with boxplots\nWe can scale the continuous axis\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nGive me some space\nWe’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nIssues with bigger data\n\nrequire(NHANES)\ndim(NHANES)\n\n[1] 10000    76\n\nNHANES %&gt;%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\n\nAlthough we can see a generally positive association (as we would expect), the over plotting may be hiding information.\nUsing alpha (opacity)\nOne way to deal with over plotting is to set the opacity low.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\ngeom_density2d\nAlternatively (or simultaneously) we might prefer a different geom altogether.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nThings I haven’t mentioned (much)\n\ncoords (coord_flip() is good to know about)\nthemes (for customizing appearance)\nposition (position_dodge(), position_jitterdodge(), position_stack(), etc.)\ntransforming axes\n\n\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\n\n\n\n\n\n\n\n\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nA little bit of everything\n\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWant to learn more?\n\ndocs.ggplot2.org/\nWinston Chang’s: R Graphics Cookbook\n\n\n\n\n\n\n\n\n\nWhat else can we do?\nshiny\n\ninteractive graphics / modeling\nhttps://shiny.rstudio.com/\n\nplotly\n\nPlotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function.\n\n\nhttps://plot.ly/ggplot2/getting-started/\n\nDynamic documents\n\ncombination of quarto, ggvis, and shiny",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html#reflection-questions",
    "href": "10-grammar-graphics.html#reflection-questions",
    "title": "10  Grammar of graphics",
    "section": "\n10.4  Reflection questions",
    "text": "10.4  Reflection questions",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "10-grammar-graphics.html#ethics-considerations",
    "href": "10-grammar-graphics.html#ethics-considerations",
    "title": "10  Grammar of graphics",
    "section": "\n10.5  Ethics considerations",
    "text": "10.5  Ethics considerations\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nBaumer, Ben, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. CRC Press. https://mdsr-book.github.io/mdsr2e/.\n\n\nGelman, Andrew. 2011. “Rejoinder.” Journal of Computational and Graphical Statistics 20: 36–40. http://arxiv.org/abs/1503.00781.\n\n\nNolan, Deborah, and Jamis Perrett. 2016. “Teaching and Learning Data Visualization: Ideas and Assignments.” The American Statistician.\n\n\nTufte, Edward. 1997. “Visual Explanations: Images and Quantities, Evidence and Narrative.” In, 27–54. Graphics Press, LLC. www.edwardtufte.com.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.\n\n\nYau, Nathan. 2013. Data Points: Visualization That Means Something. Wiley.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Grammar of graphics</span>"
    ]
  },
  {
    "objectID": "08-text-analysis.html",
    "href": "08-text-analysis.html",
    "title": "8  Text analysis",
    "section": "",
    "text": "8.1  Reflection questions",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text analysis</span>"
    ]
  },
  {
    "objectID": "08-text-analysis.html#ethics-considerations",
    "href": "08-text-analysis.html#ethics-considerations",
    "title": "8  Text analysis",
    "section": "\n8.2  Ethics considerations",
    "text": "8.2  Ethics considerations",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Text analysis</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html",
    "href": "06-sql-joins.html",
    "title": "6  Combining tables in SQL",
    "section": "",
    "text": "6.1 Subqueries\nA SQL subquery is a query used as a data source in the FROM clause, instead of the usual table. There was a subquery in ?tbl-length-time2 when the task required a function of the results set within the SELECT clause.\nWe could do something similar if we wanted to transform the variables in the select column. The example is a little bit forced, and there are other ways to obtain the same results. But hopefully the idea of a subquery is becoming more clear. Again, a subquery is just a query that becomes the data source for FROM.\nChapter 9 will cover regular expressions in some detail. Here we use the function REGEXP_REPLACE to remove any characters which are not letters, comma, or space. The function LOWER converts any upper case letters to lower case.\nSELECT name,\n       name_clean,\n       SUBSTRING_INDEX(name_clean, ',', 1) AS last_name,\n       SUBSTRING_INDEX(name_clean, ',', -1) AS first_name\nFROM (\nSELECT LOWER(REGEXP_REPLACE(name, '[^a-z,. ]', '')) AS name_clean,\n       name,\n       id, person_id\nFROM aka_name) AS temp_subquery\nLIMIT 0, 30;\nTable 6.3: A subquery is used so that the variable in the subquery can be used and transformed in the SELECT clause.\n\n\n\n\n\nname\nname_clean\nlast_name\nfirst_name\n\n\n\nSmith, Jessica Noel\nsmith, jessica noel\nsmith\njessica noel\n\n\nPain, L. $ham\npain, l. ham\npain\nl. ham\n\n\nBoy, $hutter\nboy, hutter\nboy\nhutter\n\n\nDollasign, Ty\ndollasign, ty\ndollasign\nty\n\n\nSign, Ty Dolla\nsign, ty dolla\nsign\nty dolla\n\n\nMoore, Brandon\nmoore, brandon\nmoore\nbrandon\n\n\n$torm, Country\ntorm, country\ntorm\ncountry\n\n\n'Hooper', Simon P.J. Kelly\nhooper, simon p.j. kelly\nhooper\nsimon p.j. kelly\n\n\nHooper\nhooper\nhooper\nhooper\n\n\nKelly, Simon P.J.\nkelly, simon p.j.\nkelly\nsimon p.j.\n\n\nAbdul-Hamid, Jaffar\nabdulhamid, jaffar\nabdulhamid\njaffar\n\n\nAl-Hamid, Jaffar Abd\nalhamid, jaffar abd\nalhamid\njaffar abd\n\n\nSvensson, Acke\nsvensson, acke\nsvensson\nacke\n\n\nViera, Michael 'Power'\nviera, michael power\nviera\nmichael power\n\n\nBuguelo\nbuguelo\nbuguelo\nbuguelo\n\n\n'El Burro' Rankin', Jorge Van\nel burro rankin, jorge van\nel burro rankin\njorge van\n\n\nBurro, El\nburro, el\nburro\nel\n\n\nVan Rankin, Jorge 'Burro'\nvan rankin, jorge burro\nvan rankin\njorge burro\n\n\nVan Rankin, Jorge\nvan rankin, jorge\nvan rankin\njorge\n\n\nvan Rankin, Jorge 'El Burro'\nvan rankin, jorge el burro\nvan rankin\njorge el burro\n\n\nSeigal, Jason\nseigal, jason\nseigal\njason\n\n\nKaufman, Murray\nkaufman, murray\nkaufman\nmurray\n\n\n'Knoccout'Madison, Kareim\nknoccoutmadison, kareim\nknoccoutmadison\nkareim\n\n\nStarks, Johnny\nstarks, johnny\nstarks\njohnny\n\n\nKraemer, 'Logan' Howard\nkraemer, logan howard\nkraemer\nlogan howard\n\n\nGee, Emm\ngee, emm\ngee\nemm\n\n\nCusick, Maura\ncusick, maura\ncusick\nmaura\n\n\nMaura, Maude Cusick\nmaura, maude cusick\nmaura\nmaude cusick\n\n\nWheeler, Mackenzie\nwheeler, mackenzie\nwheeler\nmackenzie\n\n\nMonkey\nmonkey\nmonkey\nmonkey",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html#all-the-joins",
    "href": "06-sql-joins.html#all-the-joins",
    "title": "6  Combining tables in SQL",
    "section": "\n6.2 All the JOINs",
    "text": "6.2 All the JOINs\nRecall that SQL is a query language that works on relational databases. One of its major strengths is being able to efficiently store information in separate tables that can be easily connected as needed. The syntax for tying together information from multiple tables is done with a JOIN clause.\nEach JOIN clause needs four specific pieces of information:\n\nThe name of the first table you want to JOIN.\nThe type of JOIN being used.\nThe name of the second table you want to JOIN.\nThe condition(s) under which you want the records in the first table to match records in the second table.\n\nSome types of JOINs available in MySQL include the following, which are represented as Venn diagrams in Figure 6.1.\n\n\nJOIN: include all of the rows that exist in both tables (similar to inner_join() in R, the intersection of the two tables). INNER JOIN is alternative, and identical, function to JOIN.\n\nLEFT JOIN: include all of the rows in the first table. Connect them, as much as possible, to the rows in the second table. Rows that have no match in the second table will have a value of NULL for the new “second table” variables.\n\nRIGHT JOIN: include all of the rows in the second table. Connect them, as much as possible, to the rows in the first table. Rows that have no match in the first table will have a value of NULL for the new “first table” variables. A RIGHT JOIN with the tables in the opposite order is the same as a LEFT JOIN with the tables in the original order.\n\nFULL OUTER JOIN: include all rows in either table. Rows that have no match in the other table will have a value of NULL for the other table variables. (similar to full_join() in R, the union of the two tables). The functionality doesn’t exist in MySQL but can be created using joins and UNION.\n\nCROSS JOIN: match each row of the first table with each row in the second table.\n\nFigure 6.1 shows Venn diagrams of the different types of joins. Figure 6.2 shows four of the JOIN functions with mini data tables. Note that in SQL the missing values will be labeled as NULL (not NA).\n\n\n\n\n\n\n\nFigure 6.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\n\n\n\n\n\n6.2.1 A toy example\nWe will head to R for just a minute so as to understand joins using a small toy dataset on rock bands from the 60s, The Beatles and The Rolling Stones. The function sqldf() in the sqldf R package allows for SQL commands on R objects.\nConsider the following datasets which are available in the dplyr package.\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nInner join\nAn inner join combines two datasets returning only the observations that exist in both of the original datasets.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                     inst.plays\n              FROM band_members AS star\n              JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 John Beatles guitar\n2 Paul Beatles   bass\n\n\nFull join\nA full join combines two datasets returning every observation that exists in either one of the original datasets. Note that in the results, Mick’s instrument is missing, and Keith’s band is missing.\nThe full_join() function does not have an equivalent in MySQL. See Section 6.3.1.1 for using JOINs and UNIONs to produce a full join.\n\nband_members |&gt;\n  full_join(band_instruments)\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nLeft join\nA left join combines two datasets returning every observation that exists in the left (or first) original dataset. Note that in the results, Mick’s instrument is missing.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              LEFT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\n\nRight join\nA right join combines two datasets returning every observation that exists in the right (or second) original dataset. Note that in the results, Keith’s band is missing.\n\nsqldf::sqldf(\"SELECT inst.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              RIGHT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\n\n\n6.2.2 JOIN\n\nIn the imdb database, the title table includes information about the 4,626,322 titles in the database, including the id, title, kind_id (indicator for the kind of ID it is), and production_year. It does not, however, include the review of the title. See Table 6.4.\n\nSELECT * FROM title LIMIT 0, 10;\n\n\n\n\nTable 6.4: SELECT to glance at the title table in the imdb database.\n\n\n\n\n\nid\ntitle\nimdb_index\nkind_id\nproduction_year\nimdb_id\nphonetic_code\nepisode_of_id\nseason_nr\nepisode_nr\nseries_years\nmd5sum\n\n\n\n78460\nAdults Recat to the Simpsons (30th Anniversary)\n\n7\n2017\n\nA3432\n78406\n\n\n\n2ae09eed7d576cc2c24774fed5b18168\n\n\n70273\n(2016-05-18)\n\n7\n2016\n\n\n68058\n\n\n\n511dfc14cfff7589d29a95abb30cd66a\n\n\n60105\n(2014-04-11)\n\n7\n2014\n\n\n59138\n\n\n\nc6cdce7e667e07713e431805c407feed\n\n\n32120\n(2008-05-01)\n\n7\n2008\n\n\n32060\n\n\n\n100df65742caf5afd092b2e0ead67d8e\n\n\n97554\nSchmÃ¶lders Traum\n\n7\n2001\n\nS2543\n97302\n10\n1\n\n46862a2f96f9fb2d59e8c9a11ecfdd28\n\n\n57966\n(#1.1)\n\n7\n2013\n\n\n57965\n1\n1\n\n409c37703766c4b24f8a86162fd9cf85\n\n\n76391\nAnniversary\n\n7\n1971\n\nA5162\n76385\n4\n9\n\n5e12ce73fac1d1dcf94136b6e9acd8f8\n\n\n11952\nAngus Black/Lester Barrie/DC Curry\n\n7\n2009\n\nA5214\n11937\n4\n7\n\n9c38b9e5601dc154444b73b518034aa1\n\n\n1554\nNew Orleans\n\n7\n2003\n\nN6452\n1508\n2\n11\n\n621bea735740a547e862e4a3226f35d2\n\n\n58442\nKiss Me Kate\n\n7\n2011\n\nK2523\n58436\n1\n10\n\n293e8c75c7f35a4035abf617962be5a9\n\n\n\n\n\n\n\n\n\nThe movie_info_idx table does not contain much information about each particular film. It does, however, have an indicator for the movie ID (given by movie_id) as well as the number of votes (given by info where type_id = 100). See Table 6.5.\n\nSELECT * FROM movie_info_idx LIMIT 0, 6;\n\n\n\n\nTable 6.5: SELECT to glance at the movie_info_idx table in the imdb database.\n\n\n\n\n\nid\nmovie_id\ninfo_type_id\ninfo\nnote\n\n\n\n1\n1\n99\n31.2.1..2.\n\n\n\n2\n1\n100\n9\n\n\n\n3\n1\n101\n4.1\n\n\n\n4\n2\n99\n1000000102\n\n\n\n5\n2\n100\n61\n\n\n\n6\n2\n101\n6.4\n\n\n\n\n\n\n\n\n\n\nLet’s say we want to combine the titles with the number of votes so that each title with user votes is included. That is, only keep the titles that have a corresponding votes. And also, only keep the votes if there is an associated title (which means we use INNER JOIN or just plain JOIN).\nRemember that WHERE will work on the raw variables, and HAVING works on the results set.\nSome aspects of the query are worth pointing out:\n* The variables in the output are given in the SELECT clause. The id and title (both from the title table) and the info from the movie_info_idx which represents the number of IMDb votes. * The variables are preceded by the table from which they came. While not always necessary, it is good practice so as to avoid confusion. * The JOIN happens by linking the id variable in the title table with the movie_id variable in the movie_info_idx table. * The LIMIT wasn’t necessary (there are only 12 observations), but it’s good practice so that we don’t end up with unwieldy query results. * The WHERE clause happens before the JOIN action, despite being written after. * In the WHERE clause, we keep only movies, only 2015 production year, and only at least 150,000 votes.\n\nSELECT title.id,\n       title.title,\n       movie_info_idx.info\nFROM title\nJOIN movie_info_idx ON title.id = movie_info_idx.movie_id \nWHERE title.production_year = 2015 \n    AND title.kind_id = 1                  # movies only\n    AND movie_info_idx.info_type_id = 100  # info_type is votes\n    AND movie_info_idx.info &gt; 150000       # at least 150,000 votes\nORDER BY movie_info_idx.info DESC\nLIMIT 0, 20;\n\n\n\n\nTable 6.6: Movies from 2015 that have at least 150,000 votes in the imdb database.\n\n\n\n\n\nid\ntitle\ninfo\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n\n\n3915213\nMad Max: Fury Road\n666484\n\n\n4389619\nThe Martian\n583987\n\n\n3313672\nAvengers: Age of Ultron\n540606\n\n\n4414139\nThe Revenant\n526189\n\n\n3787790\nJurassic World\n471237\n\n\n3752999\nInside Out\n443051\n\n\n3292159\nAnt-Man\n390965\n\n\n4364483\nThe Hateful Eight\n363199\n\n\n4251736\nSpectre\n319875\n\n\n3630368\nFurious Seven\n310970\n\n\n4255450\nSpotlight\n290362\n\n\n3961438\nMission: Impossible - Rogue Nation\n266759\n\n\n4321769\nThe Big Short\n262598\n\n\n4221220\nSicario\n260996\n\n\n3600120\nFifty Shades of Grey\n250962\n\n\n4164324\nRoom\n244210\n\n\n3379559\nBridge of Spies\n229440\n\n\n4368666\nThe Hunger Games: Mockingjay - Part 2\n214569\n\n\n4387967\nThe Man from U.N.C.L.E.\n213754\n\n\n\n\n\n\n\n\n\nLet’s say we also want to obtain information about the actors and actresses in each of the movies. In the cast_info table, there is a person_id, a movie_id, and person_role_id is 1 if actor and 2 if actress.\n\nSELECT * FROM cast_info LIMIT 0, 10;\n\n\n\n\nTable 6.7: SELECT to glance at the cast_info table in the imdb database.\n\n\n\n\n\nid\nperson_id\nmovie_id\nperson_role_id\nnote\nnr_order\nrole_id\n\n\n\n1\n1\n3432997\n1\n\n31\n1\n\n\n2\n2\n1901690\n2\n\n\n1\n\n\n3\n3\n4027567\n2\n\n25\n1\n\n\n4\n3\n4282876\n3\n\n22\n1\n\n\n5\n4\n3542672\n\n\n12\n1\n\n\n6\n5\n3331520\n4\n(as $hutter Boy)\n10\n1\n\n\n7\n5\n4027191\n2\n(as $hutter Boy)\n1\n1\n\n\n8\n5\n4195731\n5\n(uncredited)\n\n1\n\n\n9\n5\n4263956\n6\n(uncredited)\n\n1\n\n\n10\n5\n4267787\n7\n(uncredited)\n\n1\n\n\n\n\n\n\n\n\n\nWe also want the name of the actress which is in the table aka_name. Note that there is no movie information in the aka_name table!\n\nSELECT * FROM aka_name LIMIT 0, 10;\n\n\n\n\nTable 6.8: SELECT to glance at the aka_name table in the imdb database.\n\n\n\n\n\nid\nperson_id\nname\nimdb_index\nname_pcode_cf\nname_pcode_nf\nsurname_pcode\nmd5sum\n\n\n\n1\n6188450\nSmith, Jessica Noel\n\nS5325\nJ2542\nS53\n25c9d464e3ff2957533546aa92b397ed\n\n\n2\n5125059\nPain, L. $ham\n\nP545\nL515\nP5\n569b1e885ccb51211c01753f0dad9b2c\n\n\n3\n5\nBoy, $hutter\n\nB36\nH361\nB\n35092b5604ce378fc48c8a6fc0038a49\n\n\n4\n4152053\nDollasign, Ty\n\nD4253\nT3425\nD425\n0f565a2d8027cfb8ed6c5f4bba719fcd\n\n\n5\n4152053\nSign, Ty Dolla\n\nS2534\nT3425\nS25\n2eded1b021b96333b4b74e0fec959650\n\n\n6\n6\nMoore, Brandon\n\nM6165\nB6535\nM6\n193a6f5adf4756320f622162d2475608\n\n\n7\n8\n$torm, Country\n\nT6525\nC5363\nT65\n1654400b707d34323ea392b87060e6cc\n\n\n8\n19\n'Hooper', Simon P.J. Kelly\n\nH1625\nS5124\nH16\n3fd8885372c23f8c74e583da91d1fd05\n\n\n9\n19\nHooper\n\nH16\n\n\n24ddc68ab605ee95857ad45b65ffa2d8\n\n\n10\n19\nKelly, Simon P.J.\n\nK4251\nS5124\nK4\n33d976f22e276b73c61513bc5f6e72a6\n\n\n\n\n\n\n\n\n\nConnecting the most popular movies of 2015 with the actresses in those movies requires a series of JOINs. Note that to make the code less onerous, the title table has been aliased by t, the movie_info_idx table has been aliased by idx, the cast_info table has been aliased by a, and the aka_name table has been aliased by n.\nThere is a lot of data cleaning to do as some of the person_id values are one to many!! That is, the person_id matches multiple names in the aka_name database.\n\nSELECT t.title,\n       idx.info,\n       a.person_id,\n       n.name\nFROM title AS t\nJOIN movie_info_idx AS idx ON t.id = idx.movie_id \nJOIN cast_info AS a ON idx.movie_id = a.movie_id\nJOIN aka_name AS n ON a.person_id = n.person_id\nWHERE t.production_year = 2015 \n    AND t.kind_id = 1           # movies only\n    AND idx.info_type_id = 100  # info_type is votes\n    AND idx.info &gt; 150000       # at least 150,000 votes\n    AND a.role_id = 2           # actresses only\nORDER BY idx.info DESC\nLIMIT 0, 50;\n\n\n\n\nTable 6.9: Movies from 2015 that have at least 150,000 votes in the imdb database with the actress name joined.\n\n\n\n\n\ntitle\ninfo\nperson_id\nname\n\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2698188\nSam\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2806101\nGillespie, Hilary Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2959609\nCuzner, Natalie\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3089483\nFisher, Carrie Frances\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nClass, Clare\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nGlass, Claire\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3231758\nHenwick, Jessica Yu Li\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3265686\nHui, Karen\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3305561\nKamen, Hannah John\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nBilly\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nLourd, Billie Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3569409\nFran\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyongo, Lupita\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyong'o, Lupita Amondi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3785240\nRidley, Daisy Jazz Isobel\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nGiagrande, Meredith J.\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalinger, Meredith\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalenger, Meredith Dawn\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3850834\nPhi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3875581\nFox, Claudia\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3879039\nArti\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSlade, Sandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy Slade\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3938795\nRyan, Karol Lesley\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Dame Harriet\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Harriet Mary\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4094732\nWhite, Kelsey Marie\n\n\nMad Max: Fury Road\n666484\n2681098\nMichelle, Debra\n\n\nMad Max: Fury Road\n666484\n2782138\nAli\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helena\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helene\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, HÃ©lÃ¨ne Vania\n\n\nMad Max: Fury Road\n666484\n2957052\nCunico, Lillie\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3146859\nGilles, Coco Jack\n\n\nMad Max: Fury Road\n666484\n3146859\nGillies, Coco\n\n\nMad Max: Fury Road\n666484\n3268456\nRose\n\n\nMad Max: Fury Road\n666484\n3268456\nHuntington-Whiteley, Rosie Alice\n\n\nMad Max: Fury Road\n666484\n3343489\nKellerman, Antoinette\n\n\nMad Max: Fury Road\n666484\n3348513\nRiley\n\n\n\n\n\n\n\n\n\n\n6.2.3 Other JOINs\nConsider the following two tables. The first has seven movies in it (from 2015 with at least 400,000 IMDb votes). The second consists of almost 3 million actresses (person_role_id = 2). In order to find a subset of actresses, the person_id &gt; 3900000 was set arbitrarily (in order to have a smaller group with which to work).\nmovies:\n\nSELECT t.id,\n       t.title,\n       idx.info,\n       (SELECT COUNT(*)\n       FROM title AS t\n       JOIN movie_info_idx AS idx ON idx.movie_id = t.id\n       WHERE t.production_year = 2015  \n             AND t.kind_id = 1\n             AND idx.info_type_id = 100\n             AND idx.info &gt; 400000) AS row_count\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1             # movies only\n    AND idx.info_type_id = 100    # info_type is votes\n    AND idx.info &gt; 400000         # at least 400,000 votes\nORDER BY idx.info DESC\n\n\n\n\nTable 6.10: Movies from 2015 that have at least 400,000 votes in the imdb database.\n\n\n\n\n\nid\ntitle\ninfo\nrow_count\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n7\n\n\n3915213\nMad Max: Fury Road\n666484\n7\n\n\n4389619\nThe Martian\n583987\n7\n\n\n3313672\nAvengers: Age of Ultron\n540606\n7\n\n\n4414139\nThe Revenant\n526189\n7\n\n\n3787790\nJurassic World\n471237\n7\n\n\n3752999\nInside Out\n443051\n7\n\n\n\n\n\n\n\n\n\nactresses:\n\nSELECT a.person_id,\n       a.movie_id,\n       n.name,\n       (SELECT COUNT(*)\n       FROM cast_info AS a\n       JOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 390000) AS row_count\nFROM cast_info AS a\nJOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 3900000\nLIMIT 0, 20;\n\n\n\n\nTable 6.11: Actresses whose person_id is greater than 400000. Note that some actresses have different spelling or phrasing of their names.\n\n\n\n\n\nperson_id\nmovie_id\nname\nrow_count\n\n\n\n3900141\n759802\nSimons, Rita Joanne\n2904759\n\n\n3902258\n4365829\nSinger, Rabbi Tovia\n2904759\n\n\n3902699\n3109788\nSingh, Sabine Erika\n2904759\n\n\n3903035\n3215866\nVal\n2904759\n\n\n3904831\n2468067\nMasha\n2904759\n\n\n3904928\n3654347\nFei, Siu Yin\n2904759\n\n\n3904928\n3654347\nHsiao, Yen-fei\n2904759\n\n\n3904928\n3654347\nSiu, Yinfei\n2904759\n\n\n3904928\n3654347\nXiao, Yanfei\n2904759\n\n\n3904928\n3654347\nYin-Fai, Siu\n2904759\n\n\n3905289\n115191\nCoso, Cosondra\n2904759\n\n\n3905289\n115191\nSjostrom, Cossondra\n2904759\n\n\n3905289\n115191\nCoso\n2904759\n\n\n3909355\n2939100\nSlovÃ¡ckovÃ¡, Anna Julie\n2904759\n\n\n3911826\n4379610\nMeador, Constance June\n2904759\n\n\n3912134\n2675144\nDJ\n2904759\n\n\n3912134\n2675144\nSmith, DJ\n2904759\n\n\n3912134\n2675144\nSmith, Dujonette\n2904759\n\n\n3912134\n2675144\nDJ Smith\n2904759\n\n\n3913519\n1678444\nKeely, Dorothy Jacqueline\n2904759\n\n\n\n\n\n\n\n\n\nUsing subqueries, we can JOIN the two datasets using different JOIN techniques.\nInner JOIN\n\nWith an inner JOIN, there are 32 rows corresponding to all the actresses in the seven 2015 films with the most votes. Because the JOIN is an intersection of the two tables, only the actresses with person_id above 3900000 are included.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nINNER JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 6.12: Inner JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nRIGHT JOIN\nWith a RIGHT JOIN, there are more than 300 rows (the LIMIT clause keeps us from knowing how many rows, but there are a LOT!) corresponding to all the actresses whose person_id above 3900000 are included. Those actresses who acted in one of the seven top 2015 films are also included in the full results table, but they don’t happen to be in the truncated output here.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nRIGHT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 6.13: RIGHT JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nLEFT JOIN\nWith a LEFT JOIN, there are 33 rows corresponding to the actresses in the seven top 2015 movies. Only The Revenant did not have any actresses whose person_id is greater than 3900000.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nLEFT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 6.14: LEFT JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nCounting repeat actresses\nWe might, for example, want to know how many names / spellings of a name with a specific person_id (above 3900000) exist for each person_id in each of the top voted seven films of 2015.\nIn Table 6.15 why isn’t there a column indicating the name of the actress? (There can’t be such a column. Why not?)\n\nSELECT acts.person_id, \n       COUNT(*) AS num_repeat_names\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY acts.person_id;\n\n\n\n\nTable 6.15: For each person_id (&gt; 3900000) in the seven top voted 2015 films, how many names / spellings are associated with the person_id?\n\n\n\n\n\nperson_id\nnum_repeat_names\n\n\n\n3916648\n1\n\n\n4122876\n1\n\n\n3938423\n2\n\n\n3950111\n1\n\n\n4079047\n2\n\n\n4084626\n3\n\n\n4099458\n1\n\n\n3958614\n1\n\n\n3990819\n2\n\n\n4081131\n2\n\n\n3907812\n3\n\n\n3938795\n1\n\n\n3970637\n8\n\n\n4073883\n2\n\n\n4094732\n1\n\n\n4098918\n1\n\n\n\n\n\n\n\n\n\nCounting number of actresses per film\nWe might, for example, want to know how many actresses with a specific person_id (above 3900000) are in each of the top voted seven films of 2015.\n\nSELECT movs.id, \n       movs.title,\n       COUNT(*) AS num_actress\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY movs.id;\n\n\n\n\nTable 6.16: Number of actresses (with person_id &gt; 3900000) in each of the seven top voted films of 2015. Recall that The Revenant had no actresses with person_id &gt; 3900000, so there are only six movies listed.\n\n\n\n\n\nid\ntitle\nnum_actress\n\n\n\n3313672\nAvengers: Age of Ultron\n1\n\n\n3752999\nInside Out\n1\n\n\n3787790\nJurassic World\n9\n\n\n3915213\nMad Max: Fury Road\n5\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n15\n\n\n4389619\nThe Martian\n1",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html#unioning",
    "href": "06-sql-joins.html#unioning",
    "title": "6  Combining tables in SQL",
    "section": "\n6.3 UNIONing",
    "text": "6.3 UNIONing\nIn SQL a UNION clause combines two different tables by their rows (whereas JOIN combines two tables by columns). Think about UNION similarly to the bind_rows() command in R.\n\n\n\n\n\n\n\nFigure 6.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html\n\n\n\n\n\n6.3.1 UNIONs\nUNION does not check the names of the columns to make sure they match. UNION requires that the number of columns be the same and that the variable type be the same for all columns in the two tables being combined.\nTable 6.17 contains a silly example. The first table has 1 as bar and the second table has 20 as bar. But when the tables are UNIONed, the bar column contains c(1, 10). SQL took the column names from the first table and appended the second table without considering the variable names.\n\nSELECT \n    1 AS bar,\n    2 AS foo\n\nUNION\n\nSELECT \n    10 AS foo,\n    20 AS bar;\n\n\n\n\nTable 6.17: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\n\n\n\n\nbar\nfoo\n\n\n\n1\n2\n\n\n10\n20\n\n\n\n\n\n\n\n\n\nUNION is specifically designed to bind rows from two different SELECT queries where the variables have been selected in the same order. If the two SELECT clauses are done from the same table with the same order of variables, you do not need to worry about the order of the variables matching up in the UNION. If you are UNIONing two very different subqueries, you do need to worry about the variables and their order.\nUNION\nLet’s say we want to combine the top voted movies from 2015 with the top voted movies from 2019. However, to account for time, we require the movies from 2015 to have more votes (400,000) than the movies from 2017 (200,000). That is, the WHERE clause is different for the two subqueries.\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2017  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 200000)\nLIMIT 0, 100;\n   \n\n\n\n\nTable 6.18: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nThe Martian\n2015\n583987\n\n\nThe Revenant\n2015\n526189\n\n\nDunkirk\n2017\n229089\n\n\nGuardians of the Galaxy Vol. 2\n2017\n281845\n\n\nLogan\n2017\n397056\n\n\nSpider-Man: Homecoming\n2017\n209930\n\n\nWonder Woman\n2017\n306611\n\n\n\n\n\n\n\n\n\nUNION ALL\nUNION does check, however, to see if any of the rows in the two tables are identical. If the goal is to include duplicates across two tables, use UNION ALL instead of UNION.\nLet’s say that the first table is all movies with production year after 2012 and number of votes greater than 500,000. The second table is movies with production year equal to 2015 and number of votes greater than 400,000. Even though the Martian would have been in both tables, the results table lists The Marian only once in Table 6.19.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\nTable 6.19: Using UNION to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\n\nWhen UNION ALL is applied in the same context, The Martian is listed twice in the results table given in Table 6.20.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION ALL\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\nTable 6.20: Using UNION ALL to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\n\n\n6.3.1.1 FULL OUTER JOIN via UNION\n\nMySQL doesn’t have a FULL OUTER JOIN (although other implementations of SQL do have full join functionality). However, we can mimic a full join using right and left joins with UNION.\nRecall the ideas of RIGHT JOIN (which keeps all observations in the right table) and LEFT JOIN (which keeps all observations in the left table). By UNIONing the right and left joins, all of the observations are obtained (i.e., a full join). Using the function sqldf() in the sqldf R package, the full join will be demonstrated using the 1960s rock bands.\nNotice that in the RIGHT JOIN the name column must come from the right table (not the left table).\nAlso notice that UNION ALL keeps the duplicate rows which is probably not what we want.\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  John Beatles guitar\n2 Keith    &lt;NA&gt; guitar\n3  Mick  Stones   &lt;NA&gt;\n4  Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION ALL\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  Mick  Stones   &lt;NA&gt;\n2  John Beatles guitar\n3  Paul Beatles   bass\n4  John Beatles guitar\n5  Paul Beatles   bass\n6 Keith    &lt;NA&gt; guitar",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html#best-practice",
    "href": "06-sql-joins.html#best-practice",
    "title": "6  Combining tables in SQL",
    "section": "\n6.4 Best practice",
    "text": "6.4 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_imdb, shutdown = TRUE)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html#reflection-questions",
    "href": "06-sql-joins.html#reflection-questions",
    "title": "6  Combining tables in SQL",
    "section": "\n6.5  Reflection questions",
    "text": "6.5  Reflection questions\n\nWhat are the different types of joins? Which data from which table gets kept and which gets removed for each type of join?\nWhat is the difference between a join and a union?\nWhen working with multiple tables, how (and why) is a variable linked to its table?\nConsider a RIGHT JOIN. If there are records in the right table that are not in the left table, what will the value of the left table variable be for those records?",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "06-sql-joins.html#ethics-considerations",
    "href": "06-sql-joins.html#ethics-considerations",
    "title": "6  Combining tables in SQL",
    "section": "\n6.6  Ethics considerations",
    "text": "6.6  Ethics considerations\n\nWhat can happen if a UNION is done without carefully matching up the columns of the two tables being UNIONed?\nHow will you know if JOINing removed some records? What if the JOIN produced missing values for some of the variables? How should we deal with missing data or arbitrarily removed records?\n\n\n\n\nFigure 6.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\nFigure 6.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nFigure 6.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html",
    "href": "05-sql-verbs.html",
    "title": "5  SQL clauses",
    "section": "",
    "text": "5.1 Looking at the tables in the database\nConsider a database of taxi rides from the Yellow Cab company in NYC in March of 2014.\nlibrary(mdsr)\ncon_taxi &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"nyctaxi\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\nSHOW TABLES;\nTable 5.1: SHOW all the TABLES in the nyctaxi database.\nThere is only one table in the nyctaxi database, called yellow_old.\nDESCRIBE yellow_old;\nTable 5.2: DESCRIBE variables in the yellow_old table.\nSimilarly, the DESCRIBE command shows the 18 field names (variables) in the yellow_old table. Some of the variables are characters (text) and some are numeric (either double or bigint)\nMost engagements with SQL are done through queries. Queries in SQL start with the SELECT keyword and consist of several clauses, which must be written in the following order:1",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#looking-at-the-tables-in-the-database",
    "href": "05-sql-verbs.html#looking-at-the-tables-in-the-database",
    "title": "5  SQL clauses",
    "section": "",
    "text": "Watch out!\n\n\n\nSQL clauses must be written in the following order.\n\n\n\n\n\nSELECT allows you to list the columns, or functions operating on columns, that you want to retrieve. This is an analogous operation to the select() verb in dplyr, potentially combined with mutate() or summarize().\n\nFROM specifies the table where the data are.\n\nJOIN allows you to stitch together two or more tables using a key. This is analogous to the inner_join() and left_join() commands in dplyr. More details of JOIN are given in Chapter 6.\n\nWHERE allows you to filter the records according to some criteria and is an analogous operation to the filter() verb in dplyr. Note, even though the WHERE clause is written after SELECT and JOIN, it is actually evaluated before the SELECT or JOIN clauses (which is why WHERE only works on the original data, not the results set).\n\nGROUP BY allows you to aggregate the records according to some shared value and is an analogous operation to the group_by() verb in dplyr.\n\nHAVING is like a WHERE clause that operates on the result set—not the records themselves and is analogous to applying a second filter() command in dplyr, after the rows have already been aggregated.\n\nORDER BY is exactly what it sounds like—it specifies a condition for ordering the rows of the result set and is analogous to the arrange() verb in dplyr.\n\nLIMIT restricts the number of rows in the output and is similar to the R commands head() and slice().",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#sec-select",
    "href": "05-sql-verbs.html#sec-select",
    "title": "5  SQL clauses",
    "section": "\n5.2 SELECT … FROM",
    "text": "5.2 SELECT … FROM\n\n\n\n\n\n\nR function: select()\n\n\n\nA SQL query starts with a SELECT command and has a corresponding FROM to indicate the table being queried. Columns may be specified, or the * will indicate that every column in the table should be returned.\nThe shortest SQL query is the following SELECT command. Do not run this command!!! The yellow_old table has 15 million rows, and we do not want to look at them simultaneously.\n\nDO NOT RUN:  SELECT * FROM yellow_old;\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDo not run the following command unless you are certain that the table from which you are querying is small enough so that the query results fit easily into your memory.\nSELECT * FROM table;\n\n\nInstead, to look at the top of the table, SELECT the first few rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14).\n\nSELECT * FROM yellow_old LIMIT 0, 14;\n\n\n\nTable 5.3: SELECT the first 14 rows of the table.\n\n\n\n\n\nSpeaking of which, how many rows are there in the yellow_old table? That is, how many taxi rides are recorded? Now SELECT is used with a summary function, COUNT(). Instead of using a separate summary function (like mutate() or summarize()), all the work is done inside the SELECT call.\n\nSELECT COUNT(*) FROM yellow_old;\n\n\n\nTable 5.4: COUNT(*) the number of rows in the entire yellow_old table.\n\n\n\n\n\nYikes, more than 15 million taxi rides!!!!\nYou might have noticed that the yellow_old table has two different datetime variables (one for pickup, the other for drop-off). We can use the information to assess the length of each ride (in time, not distance). However, the variables are stored in SQL as character strings instead of in a DateTime format (even though they look like they are stored in a DateTime format!), see Table 5.2. Fortunately for us, SQL has functionality to convert a text Type into DateTime type (POSIXct is a special type of DateTime formatting).\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old\n   LIMIT 0, 10;\n\n\n\nTable 5.5: Convert the pickup and drop-off times to date objects using STR_TO_DATE.\n\n\n\n\n\nNow that the variables are no longer strings, we can subtract them to figure out the number of minutes for each taxi ride. Unfortunately, the following code won’t run because neither of the variables pickup or dropoff are in the table yellow_old.\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff.\n      TIMEDIFF(pickup, dropoff) AS length_time\n   FROM yellow_old\n   LIMIT 0, 10;\n\nInstead, we need two layers of SELECT commands so that the first SELECT (i.e., inside) layer creates the new variables, and the second SELECT (i.e., outside) layer subtracts the two times.\n\nSELECT \n   pickup,\n   dropoff, \n   TIMEDIFF(pickup, dropoff) AS length_time \nFROM (\n   SELECT\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old)\n   AS subquery_table\nLIMIT 0, 20;\n\n\n\nTable 5.6: Use TIMEDIFF to find the length (time) of the ride.\n\n\n\n\n\nAlternatively, the STR_TO_DATE() function can be applied inside the TIMEDIFF() function so that the full (now only) SELECT command is being used only on variables that are in the original table.\n\nSELECT \n   pickup_datetime,\n   dropoff_datetime, \n   TIMEDIFF(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\"), \n            STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\")) AS length_time \nFROM yellow_old\nLIMIT 0, 20;\n\n\n\nTable 5.7: Alternative method to find the length (time) of the ride.\n\n\n\n\n\nKeep in mind that there is a distinction between clauses that operate on the variables of the original table versus those that operate on the variables of the results set. The variables pickup_datetime and dropoff_datetime are columns in the original table - they are written to disk on the SQL server. The variables pickup, dropoff, and length_time exist only in the results set, which is passed from the server (SQL server) to the client (e.g., RStudio or DBeaver) and is not written to disk.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#select-distinct",
    "href": "05-sql-verbs.html#select-distinct",
    "title": "5  SQL clauses",
    "section": "\n5.3 SELECT DISTINCT",
    "text": "5.3 SELECT DISTINCT\nSELECT DISTINCT returns only unique rows. That is, it filters out all the duplicates of a variable or a combination of variables. Note that I have a larger limit on the query that I needed, just to make sure I got all the levels.\n\nSELECT DISTINCT payment_type\nFROM yellow_old\nLIMIT 0, 20;\n\n\n\nTable 5.8: The distinct values of payment types. CRD is credit card; CSH is cash; NOC is no charge; DIS is dispute.\n\n\n\n\n\n\nSELECT DISTINCT vendor_id, payment_type\nFROM yellow_old\nLIMIT 0, 20;\n\n\n\nTable 5.9: The distinct values of vendor ID and payment types, combined. VTS is Verifone Transportation Systems and CMT is Mobile Knowledge Systems Inc. CRD is credit card; CSH is cash; NOC is no charge; DIS is dispute.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#where",
    "href": "05-sql-verbs.html#where",
    "title": "5  SQL clauses",
    "section": "\n5.4 WHERE",
    "text": "5.4 WHERE\n\n\n\n\n\n\nR function: filter()\n\n\n\nThe WHERE clause is analogous to the filter() function in dplyr. However, keep in mind that there are two SQL commands that resemble the dplyr filter() function. WHERE operates on the original data in the table and HAVING operates on the result set. See below for examples using HAVING.\nWhat was the fare for those taxi rides where the tip_amount was more than $10 and the person used cash? (Note that in SQL the equality logical is = and in R the equality logical is ==.)\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount &gt; 10\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\nTable 5.10: WHERE to subset the queried rows.\n\n\n\n\n\nBETWEEN can be used to specify a range of values for a numeric value. BETWEEN is inclusive.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 12\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\nTable 5.11: BETWEEN in the WHERE clause.\n\n\n\n\n\nIN is similar to the dplyr %in% function which specifies distinct values for the variable.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount IN (10, 12)\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\nTable 5.12: IN in the WHERE clause.\n\n\n\n\n\nThe WHERE clause can be established by a number of logical commands combined using either AND or OR. Usually it is important to use parentheses with OR logicals to make sure the desired query is return. Consider the difference between the following queries. In SQL (as in many programming languages), AND takes precedent over OR in the order of operations, when there are no parentheses. (I was taught to remember order of operations using “please excuse my dear aunt Sally.”) The order of operations on the first query groups the second two conditions into one because AND take precedence over OR (as if the query was tip_amount BETWEEN 10 and 12 OR (total_amount BETWEEN 100 and 112 AND payment_type = \"CSH\")).\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 12 OR \n      total_amount BETWEEN 100 and 112 AND \n      payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\nTable 5.13: OR and AND without parentheses.\n\n\n\n\n\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE (tip_amount BETWEEN 10 and 12 OR \n      total_amount BETWEEN 100 and 112 ) AND \n      payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\nTable 5.14: OR and AND with parentheses.\n\n\n\n\n\n\n5.4.1 NULL in WHERE\nSQL considers NULL values to be unknown. Therefore, when searching for a NULL value, you need to ask SQL if the value IS NULL. Asking if the value is equal to NULL doesn’t work because NULL values don’t equal anything (they are unknown). To keep all values that are not NULL values, use IS NOT NULL in the WHERE clause.\n\n\n\n\n\n\n Watch out!\n\n\n\nIn order to find the records that are NULL use WHERE variable IS NULL.\n\n\n\n5.4.1.1 A NULL example2\n\nThe logic of NULL:\n\nIf you do anything with NULL, you’ll just get NULL. For instance if \\(x\\) is NULL, then \\(x &gt; 3\\), \\(1 = x\\), and \\(x + 4\\) all evaluate to NULL. Even \\(x =\\) NULL evaluates to NULL! if you want to check whether \\(x\\) is NULL, use x IS NULL or x IS NOT NULL.\n\nNULL short-circuits with boolean operators. That means a boolean expression involving NULL will evaluate to:\n\nTRUE, if it’d evaluate to TRUE regardless of whether the NULL value is really TRUE or FALSE.\nFALSE, if it’d evaluate to FALSE regardless of whether the NULL value is really TRUE or FALSE.\nOr NULL, if it depends on the NULL value.\n\n\n\nConsider the following table and SQL query:\n\nSELECT * FROM (\n   SELECT 'Ace' AS name, 20 AS age, 4 as num_dogs\n   UNION\n   SELECT 'Ada' AS name, NULL AS age, 3 as num_dogs   \n   UNION\n   SELECT 'Ben' AS name, NULL AS age, NULL as num_dogs\n   UNION\n   SELECT 'Cho' AS name, 27 AS age, NULL as num_dogs\n   ) AS temptable;\n\n\nSELECT * FROM (\n   SELECT 'Ace' AS name, 20 AS age, 4 as num_dogs\n   UNION\n   SELECT 'Ada' AS name, NULL AS age, 3 as num_dogs   \n   UNION\n   SELECT 'Ben' AS name, NULL AS age, NULL as num_dogs\n   UNION\n   SELECT 'Cho' AS name, 27 AS age, NULL as num_dogs\n   ) AS temptable\nWHERE age &lt;= 20 OR num_dogs = 3;\n\nWhere does the WHERE clause do? It tells us that we only want to keep the rows satisfying the age &lt;= 20 OR num_dogs = 3. Let’s consider each row one at a time:\n\nFor Ace, age &lt;= 20 evaluates to TRUE so the claim is satisfied.\nFor Ada, age &lt;= 20 evaluates to NULL but num_dogs = 3 evaluates to TRUE so the claim is satisfied.\nFor Ben, age &lt;= 20 evaluates to NULL and num_dogs = 3 evaluates to NULL so the overall expression is NULL which has a FALSE value.\nFor Cho, age &lt;= 20 evaluates to FALSE and num_dogs = 3 evaluates to NULL so the overall expression evaluates to NULL (because it depends on the value of the NULL).\n\nThus we keep only Ace and Ada.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE payment_type IS NULL\nLIMIT 0, 10;\n\n\n\nTable 5.15: There is ONE record with a NULL value for payment_type. Note that the way to find NULL values is via IS NULL.\n\n\n\n\n\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE payment_type = NULL\nLIMIT 0, 10;\n\n\n\nTable 5.16: NO rows are selected when the WHERE command is specified to indicate if the variable equals NULL.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#group-by",
    "href": "05-sql-verbs.html#group-by",
    "title": "5  SQL clauses",
    "section": "\n5.5 GROUP BY",
    "text": "5.5 GROUP BY\n\n\n\n\n\n\nR function: group_by()\n\n\n\nThe GROUP BY clause will direct SQL to carry out the query separately for each category in the grouped variable. Using GROUP BY is particularly important when aggregating multiple rows into a single number. Some aggregate functions include COUNT(), SUM(), MAX(), MIN(), and AVG().\nNote that SUM(1) adds (sums) the number 1 for each row. Which is the same as counting the number of rows. SUM(2) adds (sums) the number 2 for each row which returns twice as many transactions.\n\nSELECT COUNT(*) AS num_transactions, \n       SUM(1) AS num_transactions_also,\n       SUM(2) AS double_transactions,\n       payment_type \nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 20\nGROUP BY payment_type;\n\n\n\nTable 5.17: GROUP BY on payment_type.\n\n\n\n\n\nFor those people who tipped between $10 and $20, what was the lowest and highest fare for each of the types of payments?\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type \nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 20\nGROUP BY payment_type;\n\n\n\nTable 5.18: GROUP BY with aggregate functions.\n\n\n\n\n\nGROUP BY will work applied to multiple columns. Let’s tabulate the same results, now broken down by payment_type and day of week. Except that we don’t have a day of week variable! We need to convert the pickup_datetime variable to a DateTime object and then pull out the day of the week, using DAYNAME. (Note: DAYOFWEEK will give you the day of the week as an integer. Use your internet sleuthing skills if you are looking for functions that might help your desired query.)\n\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday;\n\n\n\nTable 5.19: GROUP BY with payment_type and wday.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#order-by",
    "href": "05-sql-verbs.html#order-by",
    "title": "5  SQL clauses",
    "section": "\n5.6 ORDER BY",
    "text": "5.6 ORDER BY\n\n\n\n\n\n\nR function: arrange()\n\n\n\nThe ORDER BY command can be used with or without the GROUP BY and aggregation commands. It allows us to look at interesting aspects of the data by sorting the data.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY lowest_fare ASC;\n\n\n\nTable 5.20: ORDER BY lowest_fare, ascending.\n\n\n\n\n\nWHAT?!?!! How in the world was one of the fares -$612.40? It doesn’t make any sense that a fare would be negative. Some additional inquiry into the observation corresponding to a fare of -$612.40 is absolutely warranted. If the observation is found to be a typo, it would need to be removed from the data set. If the observation is somehow legitimate, it would need to be included in the analysis, with the information provided about its legitimacy.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY highest_fare DESC;\n\n\n\nTable 5.21: ORDER BY highest_fare, descending\n\n\n\n\n\n$950 is a lot to pay for a cab ride! But in NYC, I’d believe it.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY wday, payment_type;\n\n\n\nTable 5.22: ORDER BY wday and payment_type.\n\n\n\n\n\n\n\n\n\n\n\nNote that both GROUP BY and ORDER BY evaluate the data after it has been retrieved. Therefore, the functions operate on the results set, not the original rows of the data.\n\n\n\nAs above, we were able to GROUP BY and ORDER BY on the new variables we had created, wday.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#having",
    "href": "05-sql-verbs.html#having",
    "title": "5  SQL clauses",
    "section": "\n5.7 HAVING",
    "text": "5.7 HAVING\n\n\n\n\n\n\nR function: filter()\n\n\n\nRecall that WHERE acts only on the original data. If we are interested in rides that took place on Friday, we need to use the derived variable wday instead of the raw variable pickup_datetime. Fortunately, HAVING works on the results set. Note that SQL uses '' for strings, not \"\". In SQL, \"\" is used to identify variables (not values of variables), like R’s &grave;&grave;.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nHAVING wday = 'Friday';\n\n\n\nTable 5.23: HAVING to filter only Friday rides.\n\n\n\n\n\nWhile it worked out quite well for us that HAVING was able to filter the data based on the results set, the use of HAVING was quite onerous because the entire data set was considered before the filter was applied. That is, if the filter can be done on the original data using WHERE, the query will be much faster and more efficient.\nNote: HAVING requires a GROUP BY clause. And the variable(s) used in HAVING must also be part of the GROUP BY clause.\n\n\n\n\n\n\nWhenever possible, use WHERE instead of HAVING to make your queries as efficient as possible.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#limit",
    "href": "05-sql-verbs.html#limit",
    "title": "5  SQL clauses",
    "section": "\n5.8 LIMIT",
    "text": "5.8 LIMIT\n\n\n\n\n\n\nR function: head() or slice()\n\n\n\nAs we’ve seen, LIMIT truncates the query to specified rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14). The query below shows the last 10 rows of the entire data set.\n\nSELECT * FROM yellow_old LIMIT 15428118, 10;\n\n\n\nTable 5.24: LIMIT on the last 10 rows of the table.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#saving-sql-queries-as-r-objects",
    "href": "05-sql-verbs.html#saving-sql-queries-as-r-objects",
    "title": "5  SQL clauses",
    "section": "\n5.9 Saving SQL queries as R objects",
    "text": "5.9 Saving SQL queries as R objects\nIf you are working in R to run SQL commands, you may want to use the query output for further analysis or visualizations. In that case, use #|output.var: \"name_of_variable\" inside the {sql} chunk. The variable called name_of_variable will then be available to be used in the R environment.\n\n```{sql}\n#| connection: con_taxi\n#| label: new-table\n#| output.var: \"new_table\"\n#| eval: false\n\nSELECT *, DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old \nLIMIT 0, 1000;\n\n```\n\n\n\nTable 5.25: New data.frame saved to R called new_table.\n\n\n\n\n\n\n```{r}\n#| eval: false\n\nnew_table |&gt;\n  drop_na(wday) |&gt;\n  ggplot(aes(x = fare_amount, y = tip_amount, color = wday)) + \n  geom_point() \n```",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#best-practice",
    "href": "05-sql-verbs.html#best-practice",
    "title": "5  SQL clauses",
    "section": "\n5.10 Best practice",
    "text": "5.10 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_taxi, shutdown = TRUE)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#reflection-questions",
    "href": "05-sql-verbs.html#reflection-questions",
    "title": "5  SQL clauses",
    "section": "\n5.11  Reflection questions",
    "text": "5.11  Reflection questions\n\nWhy don’t we usually want to run the query: SELECT * FROM table;?\nWhat is the difference between the original table and the results set?\nIn SQL does the WHERE clause use = or == to indicate equality?\nDoes BETWEEN work only on numeric variables or also on character strings?\nWhat syntax is used to direct ORDER BY to sort by biggest to smallest or smallest to biggest?\nWhat is the difference between WHERE and HAVING?",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#ethics-considerations",
    "href": "05-sql-verbs.html#ethics-considerations",
    "title": "5  SQL clauses",
    "section": "\n5.12  Ethics considerations",
    "text": "5.12  Ethics considerations\n\nWhat are different ways to look at the dataset to identify possible typos or rogue values?\nWhy are such tasks so much harder with large datasets (versus small datasets)?\nWhy are such tasks to much more important with large datasets (versus small datasets)?",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "05-sql-verbs.html#footnotes",
    "href": "05-sql-verbs.html#footnotes",
    "title": "5  SQL clauses",
    "section": "",
    "text": "Taken directly from Modern Data Science with R↩︎\ntaken from: https://cs186berkeley.net/notes/note1/#filtering-null-values↩︎",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>SQL clauses</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html",
    "href": "04-sql-in-R.html",
    "title": "4  SQL in R and DBeaver",
    "section": "",
    "text": "4.1 Translating dplyr code into SQL\nLet’s go back to the airlines database to try out some things that we already know how to do in R. Recall that we need the DBI and RMariaDB packages to connect to R; we need the dbplyr package to translate SQL code into R.\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dbplyr)\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\nThe function dbListTables() in the DBI package will tell us what tables exist in the airlines database.\nDBI::dbListTables(con_air)\n\n[1] \"planes\"          \"carriers\"        \"airports\"        \"flights_summary\"\n[5] \"flights\"        \n\nflights &lt;- tbl(con_air, \"flights\")\ncarriers &lt;- tbl(con_air, \"carriers\")\nLet’s ask a few questions about the data set using data wrangling techniques that should already be familiar.\nTo start, let’s write the commands using tidy dplyr code.\nyrs &lt;- flights |&gt;\n  summarize(min_year = min(year), max_year = max(year))\n\nyrs\n\n# Source:   SQL [1 x 2]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  min_year max_year\n     &lt;int&gt;    &lt;int&gt;\n1     2013     2015\nBecause flights is not actually a data.frame in R (but instead a tbl in SQL), the work that was done above was actually performed in SQL. To see the SQL code, we can use the function show_query.\nshow_query(yrs)\n\n&lt;SQL&gt;\nSELECT MIN(`year`) AS `min_year`, MAX(`year`) AS `max_year`\nFROM `flights`\nNote the similarity between the R code and the SQL code. We can see SELECT and MIN and MAX which are familiar. The AS function is new, but maybe it that AS does the job of assigning a new name to the output columns. FROM is also new and does the job of piping in a data set to use.\nla_bos &lt;- flights |&gt;\n  filter(year == 2012 & ((origin == \"LAX\" & dest == \"BOS\") | \n           (origin == \"BOS\" & dest == \"LAX\"))) \n\n\nla_bos\n\n# Source:   SQL [0 x 21]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n# ℹ 21 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;,\n#   sched_dep_time &lt;int&gt;, dep_delay &lt;int&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\nshow_query(la_bos)\n\n&lt;SQL&gt;\nSELECT `flights`.*\nFROM `flights`\nWHERE (`year` = 2012.0 AND ((`origin` = 'LAX' AND `dest` = 'BOS') OR (`origin` = 'BOS' AND `dest` = 'LAX')))\nThe WHERE function in SQL acts as filter() did in R; & has been translated to AND, and | has been translated to OR.\nAs might be expected, dbplyr doesn’t translate every R command into SQL. After all, SQL is not a statistical software and doesn’t, for example, have a mechanism for creating data visualizations. To track which R commands are connected to SQL see the dbplyr reference sheet.\nBecause the data set has been subsetted substantially, we could pull it into R to create an R object. Note that now R is aware of the size of the entire data frame (7064 rows and 21 columns). The la_bos object now exists in the R environment and can be explored through the IDE.\nla_bos &lt;- la_bos |&gt;\n  collect()\n\nla_bos\n\n# A tibble: 0 × 21\n# ℹ 21 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;,\n#   sched_dep_time &lt;int&gt;, dep_delay &lt;int&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\nChapter 5 will explore more SQL queries and using SQL verbs. For now, let’s continue learning about the different ways R can talk to SQL.\nAlways a good idea to terminate the SQL connection when you are done with it.\ndbDisconnect(con_air, shutdown = TRUE)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#sec-dplyr-seq",
    "href": "04-sql-in-R.html#sec-dplyr-seq",
    "title": "4  SQL in R and DBeaver",
    "section": "",
    "text": "Over what years is the flights data taken?\n\n\n\n\n\n\n\nCreate a data set containing only flights between LAX and BOS in 2012.\n\n\n\n\n\n\n\n\n\n\n\n\n Watch out!\n\n\n\nBe careful with collect(). Don’t use collect() on large data frames that won’t fit in an R environment.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#sql-queries-through-the-dbi-package",
    "href": "04-sql-in-R.html#sql-queries-through-the-dbi-package",
    "title": "4  SQL in R and DBeaver",
    "section": "\n4.2 SQL queries through the DBI package",
    "text": "4.2 SQL queries through the DBI package\nUsing R as a wrapper, we can send actual SQL code to query data from the connection. It is okay if you aren’t yet able to write SQL commands from scratch, but try to figure out what the command is asking for. As mentioned above, we will start from scratch to learn SQL commands in Chapter 5.\nStart by setting up the SQL connection in the same way.\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\n\nLook at the first few rows of the flights data.\n\nBecause the flights data is not an R object, we can’t open it in R to explore the variables. If we want to see a small bit of the data, we can SELECT everything (i.e, *) from the flights table but LIMIT the query to only the first eight observations.\nNote that the code in the dbGetQuery() R function is written in SQL not in R.\nA semicolon (;) is typically used to indicate the termination of a SQL statement. They are not always required (particularly when only one statement is being sent), however, it is good practice to use a semicolon at the end of each SQL statement. (Indeed, some SQL dialects require the semicolon at the end of every statement, regardless of whether or not there are more statements following.)\n\nDBI::dbGetQuery(con_air,\n                \"SELECT * FROM flights LIMIT 8;\")\n\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2013    10   1        2             10        -8      453            505\n2 2013    10   1        4           2359         5      730            729\n3 2013    10   1       11             15        -4      528            530\n4 2013    10   1       14           2355        19      544            540\n5 2013    10   1       16             17        -1      515            525\n6 2013    10   1       22             20         2      552            554\n7 2013    10   1       29             35        -6      808            816\n8 2013    10   1       29             35        -6      449            458\n  arr_delay carrier tailnum flight origin dest air_time distance cancelled\n1       -12      AA  N201AA   2400    LAX  DFW      149     1235         0\n2         1      FL  N344AT    710    SFO  ATL      247     2139         0\n3        -2      AA  N3KMAA   1052    SFO  DFW      182     1464         0\n4         4      AA  N3ENAA   2392    SEA  ORD      191     1721         0\n5       -10      UA  N38473   1614    LAX  IAH      157     1379         0\n6        -2      UA  N458UA    291    SFO  IAH      188     1635         0\n7        -8      US  N551UW    436    LAX  CLT      256     2125         0\n8        -9      AS  N402AS    108    ANC  SEA      181     1448         0\n  diverted hour minute           time_hour\n1        0    0     10 2013-10-01 00:10:00\n2        0   23     59 2013-10-01 23:59:00\n3        0    0     15 2013-10-01 00:15:00\n4        0   23     55 2013-10-01 23:55:00\n5        0    0     17 2013-10-01 00:17:00\n6        0    0     20 2013-10-01 00:20:00\n7        0    0     35 2013-10-01 00:35:00\n8        0    0     35 2013-10-01 00:35:00\n\n\n\nHow many flights per year are in the flights table?\n\n\ndbGetQuery(con_air, \n  \"SELECT year, count(*) AS num_flights FROM flights GROUP BY year ORDER BY num_flights;\")\n\n  year num_flights\n1 2015     5819079\n2 2014     5819811\n3 2013     6369482\n\n\nNote that we’ve now SELECTed two variables: year and num_flights (which we created along the way using count(*) which is written as n() in R) FROM the flights table. Then we GROUP BY the year variable which retroactively acts on the count(*) function. And last, we ORDER BY (which is similar to arrange()) the new num_flights variable.\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#direct-sql-queries-through-a-sql-chunk",
    "href": "04-sql-in-R.html#direct-sql-queries-through-a-sql-chunk",
    "title": "4  SQL in R and DBeaver",
    "section": "\n4.3 Direct SQL queries through a sql chunk",
    "text": "4.3 Direct SQL queries through a sql chunk\nNotice that the formatting of the next few chunks is slightly different. Instead of reporting only the inside / code of the chunk, the entire chunk is printed. The SQL chunks are given by {sql} instead of {r} and each SQL chunk is required to connect to a particular database (through the con_air connection).\nThe same queries have been run.\nStart by setting up the SQL connection in the same way.\n\n```{r}\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n```\n\n\n```{sql}\n#| connection: con_air\n\nSELECT * FROM flights LIMIT 8;\n```\n\n\n8 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\ntailnum\nflight\norigin\ndest\nair_time\ndistance\ncancelled\ndiverted\nhour\nminute\ntime_hour\n\n\n\n2013\n10\n1\n2\n10\n-8\n453\n505\n-12\nAA\nN201AA\n2400\nLAX\nDFW\n149\n1235\n0\n0\n0\n10\n2013-10-01 00:10:00\n\n\n2013\n10\n1\n4\n2359\n5\n730\n729\n1\nFL\nN344AT\n710\nSFO\nATL\n247\n2139\n0\n0\n23\n59\n2013-10-01 23:59:00\n\n\n2013\n10\n1\n11\n15\n-4\n528\n530\n-2\nAA\nN3KMAA\n1052\nSFO\nDFW\n182\n1464\n0\n0\n0\n15\n2013-10-01 00:15:00\n\n\n2013\n10\n1\n14\n2355\n19\n544\n540\n4\nAA\nN3ENAA\n2392\nSEA\nORD\n191\n1721\n0\n0\n23\n55\n2013-10-01 23:55:00\n\n\n2013\n10\n1\n16\n17\n-1\n515\n525\n-10\nUA\nN38473\n1614\nLAX\nIAH\n157\n1379\n0\n0\n0\n17\n2013-10-01 00:17:00\n\n\n2013\n10\n1\n22\n20\n2\n552\n554\n-2\nUA\nN458UA\n291\nSFO\nIAH\n188\n1635\n0\n0\n0\n20\n2013-10-01 00:20:00\n\n\n2013\n10\n1\n29\n35\n-6\n808\n816\n-8\nUS\nN551UW\n436\nLAX\nCLT\n256\n2125\n0\n0\n0\n35\n2013-10-01 00:35:00\n\n\n2013\n10\n1\n29\n35\n-6\n449\n458\n-9\nAS\nN402AS\n108\nANC\nSEA\n181\n1448\n0\n0\n0\n35\n2013-10-01 00:35:00\n\n\n\n\n\n\n```{sql}\n#| connection: con_air\n\nSELECT year, count(*) AS num_flights FROM flights GROUP BY year ORDER BY num_flights;\n```\n\n\n3 records\n\nyear\nnum_flights\n\n\n\n2015\n5819079\n\n\n2014\n5819811\n\n\n2013\n6369482\n\n\n\n\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#dbeaver",
    "href": "04-sql-in-R.html#dbeaver",
    "title": "4  SQL in R and DBeaver",
    "section": "\n4.4 DBeaver",
    "text": "4.4 DBeaver\nDBeaver is a free SQL client that supports MySQL (as well as other dialects like MariaDB, PostgreSQL, and SQLite). While writing SQL code in R has some benefits (e.g., piping results tables into ggplot2 for visualizations), using a SQL client that is designed for SQL queries has benefits as well. In order to use DBeaver, download the client onto your computer and open it from your Applications.\n\n4.4.1 New database connection\nUsing the pull-down menus, navigate to a new database connection (Database -&gt; New Database Connection). Click on the MySQL icon (and click next). You should see an image similar to Figure 4.1.\n\n\n\n\n\n\n\nFigure 4.1: Connection settings for a MySQL connection via DBeaver.\n\n\n\n\n\nKeep the Host radio button toggled (don’t click on URL)\nWhere currently it says Server Host: localhost change localhost to the URL for the MySQL server to which you want to connect.\nChange the Username to the appropriate username for the server.\nChange the Password to the appropriate password for the server.\nOptional: in the Database: box, include the database you will query.\nClick Finish.\n\nOnce the connection is established, you should be able to navigate through the databases and their tables on the left side of the DBeaver window.\n\n4.4.2 Writing SQL queries\nPull up a SQL script by clicking ont he SQL button as seen in Figure 4.2.\n\n\n\n\n\n\n\nFigure 4.2: Click on SQL to initiate a SQL script.\n\n\n\n\nWrite SQL code. Click on the orange triangle to run the code.\nEach lab should be saved as a .sql files that can be turned in. The SQL queries (in the .sql file) should be able to be run by someone else. Use the hashtag (#) to comment out lines so that you can identify particular problems or comment on the query results.\nIf you did not specify which database to use when you set up the connection, the database can be specified at the top of the .sql file as USE database; (for example, you might want USE airlines;, with the semi-colon, before running your lines of SQL code).\nTo write text use /* write text here ... */, the slash and asterisk, for any commenting in the .sql file.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#reflection-questions",
    "href": "04-sql-in-R.html#reflection-questions",
    "title": "4  SQL in R and DBeaver",
    "section": "\n4.5  Reflection questions",
    "text": "4.5  Reflection questions\n\nWhat are the three main ways to write a SQL query using the RStudio interface?\nHow is DBeaver similar and/or different from writing queries using **R*?\nWhy can’t you use collect() to pull the flights data into your R session?",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "04-sql-in-R.html#ethics-considerations",
    "href": "04-sql-in-R.html#ethics-considerations",
    "title": "4  SQL in R and DBeaver",
    "section": "\n4.6  Ethics considerations",
    "text": "4.6  Ethics considerations\n\nHow / why is Sys.getenv() used to protect the username and password for the SQL connection?\nIf SQL databases are expensive to maintain, who will then have access to important data? Does it matter?\n\n\n\n\nFigure 4.1: Connection settings for a MySQL connection via DBeaver.\nFigure 4.2: Click on SQL to initiate a SQL script.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>SQL in R and DBeaver</span>"
    ]
  },
  {
    "objectID": "03-rvest.html",
    "href": "03-rvest.html",
    "title": "3  Web scraping",
    "section": "",
    "text": "3.1  Reflection questions",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "03-rvest.html#ethics-considerations",
    "href": "03-rvest.html#ethics-considerations",
    "title": "3  Web scraping",
    "section": "\n3.2  Ethics considerations",
    "text": "3.2  Ethics considerations",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Web scraping</span>"
    ]
  },
  {
    "objectID": "02-github.html",
    "href": "02-github.html",
    "title": "2  GitHub",
    "section": "",
    "text": "2.1  Reflection questions",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "02-github.html#ethics-considerations",
    "href": "02-github.html#ethics-considerations",
    "title": "2  GitHub",
    "section": "\n2.2  Ethics considerations",
    "text": "2.2  Ethics considerations",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>GitHub</span>"
    ]
  },
  {
    "objectID": "p5-data-communicate.html",
    "href": "p5-data-communicate.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions are sequences of characters that define search patterns. Symbolic notation is used to find particular sequences of interest. Regular expressions are used in many different contexts including SQL queries, web scraping, and data wrangling. 9  Regular expressions covers regular expressions within R, but the syntax for pattern matching is identical across almost all platforms and programming languages. One thing to note is that to escape a metacharacter in R, two backslashes are needed. For example, \\\\d is the correct syntax to denote a digit.",
    "crumbs": [
      "Regular expressions"
    ]
  },
  {
    "objectID": "p1-data-acquire.html",
    "href": "p1-data-acquire.html",
    "title": "Introduction to databases and SQL",
    "section": "",
    "text": "Databases are a powerful structure for holding huge amounts of data that can easily be accessed. Databases are made up of tables which are efficiently stored using indexing and avoiding unnecessary replication of information.\n?sec-db covers details of databases and reviews R functionality (mostly through the dplyr package) for working with data frames that mimics SQL queries for working with tables.\n4  SQL in R and DBeaver covers three different ways to run SQL queries in R. R code can be translated into SQL; the DBI package can send SQL queries through an r chunk; and SQL queries can be sent directly to a SQL server through a sql chunk. Additionally, DBeaver is introduced as a SQL client which supports MySQL.\nSQL code is written as a series of statements. Later in the text, ?sec-create-db and ?sec-change-db cover statements like CREATE for defining new tables and INSERT for adding data. In this part, 5  SQL clauses covers SELECT statements, which are arguably the most useful statements for data scientists. SELECT statements are also called queries because they query table(s), with particular characteristics, from databases.\nA query is made up of clauses. Every query must have a SELECT and FROM clause. Other clauses include WHERE, GROUP BY, HAVING, and ORDER BY, all of which will be covered in 5  SQL clauses.\nBecause of the efficiency of data storage in SQL databases, it is often necessary to combine information held across two or more tables. 6  Combining tables in SQL covers combining tables via JOIN (combining columns) and via UNION (combining rows).",
    "crumbs": [
      "Introduction to databases and SQL"
    ]
  },
  {
    "objectID": "p2-data-explore.html",
    "href": "p2-data-explore.html",
    "title": "Data exploration",
    "section": "",
    "text": "Regular expressions are sequences of characters that define search patterns. Symbolic notation is used to find particular sequences of interest. Regular expressions are used in many different contexts including SQL queries, web scraping, and data wrangling. 9  Regular expressions covers regular expressions within R, but the syntax for pattern matching is identical across almost all platforms and programming languages. One thing to note is that to escape a metacharacter in R, two backslashes are needed. For example, \\\\d is the correct syntax to denote a digit.",
    "crumbs": [
      "Data exploration"
    ]
  }
]