[
  {
    "objectID": "01-db.html#quarto",
    "href": "01-db.html#quarto",
    "title": "\n1  What is a Database?\n",
    "section": "\n1.1 Quarto",
    "text": "1.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "01-db.html#running-code",
    "href": "01-db.html#running-code",
    "title": "\n1  What is a Database?\n",
    "section": "\n1.2 Running Code",
    "text": "1.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "02-sql-in-R.html",
    "href": "02-sql-in-R.html",
    "title": "\n2  SQL in R\n",
    "section": "",
    "text": "Note that there exists an R interface to work with SQL commands from within an R Markdown file. For consistency with the class notes, we’ve continued to use the R Markdown structure to demonstrate the course material.\n(Taken from the Teach Data Science blog: https://teachdatascience.com/sql/, this entry written by Nick Horton)\nSQL (pronounced sequel) stands for Structured Query Language; it is a language designed to manage data in a relational database system.\nWe will use a public facing MySQL database containing wideband acoustic immittance (WAI) measures made on normal ears of adults. (The project is funded by the National Institutes of Health, NIDCD, and hosted on a server at Smith College, PI Susan Voss, R15 DC014129-01.) The database was created to enable auditory researchers to share WAI measurements and combine analyses over multiple datasets.\nWe begin by demonstrating how SQL queries can be sent to a database. It is necessary to set up a connection using the dbConnect() function.\n\nlibrary(mosaic)\nlibrary(RMySQL)  \ncon &lt;- dbConnect(\n  RMySQL::MySQL(), host = \"scidb.smith.edu\", user = \"waiuser\", \n  password = \"smith_waiDB\", dbname = \"wai\")\n\nNext a series of SQL queries can be sent to the database using the DBI::dbGetQuery() function: each query returns an R dataframe.\n\nclass(dbGetQuery(con, \"SHOW TABLES\"))\n\nThere are multiple tables within the wai database.\n\ndbGetQuery(con, \"SHOW TABLES\")\n\nThe EXPLAIN command describes the ten field names (variables) in the PI_Info table.\n\ndbGetQuery(con, \"EXPLAIN PI_Info\")\n\nThe SELECT statement can be used to select all fields for eight observations in the Measurements table.\n\neightobs &lt;- dbGetQuery(con, \"SELECT * FROM Measurements LIMIT 8\")\neightobs\n\nMore interesting and complicated SELECT calls can be used to undertake grouping and aggregation. Here we calculate the sample size for each study\n\ndbGetQuery(con, \n  \"SELECT Identifier, count(*) AS NUM FROM Measurements GROUP BY Identifier ORDER BY NUM\")\n\nAccessing a database using dplyr commands\nAlternatively, a connection can be made to the server by creating a series of dplyr tbl objects. Connecting with familiar dplyr syntax is attractive because, as Hadley Wickham has noted, SQL and R have similar syntax (but sufficiently different to be confusing).\nThe setup process looks similar.\n\nMeasurements &lt;- tbl(con, \"Measurements\")\nclass(Measurements)\nPI_Info &lt;- tbl(con, \"PI_Info\")\nSubject &lt;- tbl(con, \"Subjects\")\n\nWe explore the PI_Info table using the collect() function used to force computation on the database (and return the results). One attractive aspect of database systems is that they feature lazy evaluation, where computation is optimized and postponed as long as possible.\n\nPI_Info %&gt;% collect() %&gt;% data.frame()   \n# be careful with collect() when dealing with large tables!\n\nNote how the number of rows is unknown (?? at the top of the output above) for the lazy query.\nSimilarly, we can explore the Subjects table.\n\n#Subject  %&gt;% summarise(total = n())\nSubject %&gt;% collect()  # be careful with collect() with large tables!\n\nLet’s explore the Measurements table.\n\n#Measurements %&gt;% summarise(total = n())\nMeasurements %&gt;% collect()\n\nThere are more than a quarter million observations.\nIn the next step, we will download the data from a given subject for a specific study, in this case a paper by Rosowski et al. (2012) entitled “Ear-canal reflectance, umbo velocity, and tympanometry in normal-hearing adults”.\nArbitrarily we choose to collect data from subject number three.\n\nonesubj &lt;- \n  Measurements %&gt;% \n  filter(Identifier == \"Rosowski_2012\", Sub_Number == 3) %&gt;%\n  collect %&gt;%\n  mutate(SessionNum = as.factor(Session))\nhead(onesubj)\n\nFinally we can display the results of the measurements as a function of frequency and which ear (left or right) that was used.\n\nonesubj &lt;- mutate(onesubj, Ear = ifelse(Left_Ear == 1, \"Left\", \"Right\"))\nggplot(onesubj, aes(x = Freq, y = Absorbance)) + geom_point() +\n  aes(colour = Ear) + scale_x_log10() + labs(title=\"Absorbance by ear Rosowski subject 3\")\n\nAlways a good idea to terminate the SQL connection.\n\ndbDisconnect(con)\n\nWe note that a number of relational database systems exist, including MySQL (illustrated here), PostgreSQL, and SQLite. More information about databases within R can be found in the CRAN Databases with R Task View.\nSetting up and managing a database is a topic for a different day: here we focused on how SQL can be used within R to access data in a flexible and powerful manner.\nLearn more\n\nhttps://chance.amstat.org/2015/04/setting-the-stage/ (Setting the stage for data technologies)\nhttps://www.w3schools.com/sql/sql_intro.asp (Intro to SQL)\nhttp://www.science.smith.edu/wai-database/home/about/ (WAI SQL Database)\nhttps://cran.r-project.org/web/views/Databases.html (CRAN Task View on Databases with R)\nhttps://db.rstudio.com (RStudio Database resources)\nhttps://dbplyr.tidyverse.org/articles/dbplyr.html (dbplyr package)"
  },
  {
    "objectID": "04-sql-verbs.html#quarto",
    "href": "04-sql-verbs.html#quarto",
    "title": "\n3  SQL verbs\n",
    "section": "\n3.1 Quarto",
    "text": "3.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "04-sql-verbs.html#running-code",
    "href": "04-sql-verbs.html#running-code",
    "title": "\n3  SQL verbs\n",
    "section": "\n3.2 Running Code",
    "text": "3.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "07-reg-expr.html",
    "href": "07-reg-expr.html",
    "title": "7  Regular Expressions",
    "section": "",
    "text": "think about using regular expressions when scraping data from a website\nuse TidyTuesday from 11/29/23 to link Dr. Who rankings with IMDb rankings.\n\n\nA regular expression … is a sequence of characters that define a search pattern. Usually such patterns are used by string searching algorithms for “find” or “find and replace” operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory. [From https://en.wikipedia.org/wiki/Regular_expression]\n\nMain tasks in character matching:\n\nbasic string operations\npattern matching (regular expressions)\nsentiment analysis\n\nMany of the ideas below are taken from Jenny Bryan’s STAT545 class.\nR packages to make your life easier\n\n\nstringr package A core package in the tidyverse. It is installed via install.packages(\"tidyverse\") and also loaded via library(tidyverse). Of course, you can also install or load it individually.\n\nMany of the main functions start with str_. Auto-complete is your friend.\nReplacements for base functions re: string manipulation and regular expressions (see below).\nMain advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task.\n\nstringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf\n\n\n\ntidyr package Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract().\nBase functions: nchar(), strsplit(), substr(), paste(), paste0().\nThe glue package is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package.\nString functions related to regular expression\nRegular expression is a pattern that describes a specific set of strings with a common structure. It is heavily used for string matching / replacing in all programming languages, although specific syntax may differ a bit. It is truly the heart and soul for string operations. In R, many string functions in base R as well as in stringr package use regular expressions, even RStudio’s search and replace allows regular expression:\n\nidentify match to a pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()\n\nextract match to a pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()\n\nlocate pattern within a string, i.e. give the start position of matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()\n\nreplace a pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()\n\nsplit a string using a pattern: strsplit(), stringr::str_split()\n\n\nRegular expressions typically specify characters (or character classes) to seek out, possibly with information about repeats and location within the string. This is accomplished with the help of metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. We will use some small examples to introduce regular expression syntax and what these metacharacters mean.\nEscape sequences\nThere are some special characters in R that cannot be directly coded in a string. For example, let’s say you specify your pattern with single quotes and you want to find countries with the single quote '. You would have to “escape” the single quote in the pattern, by preceding it with \\, so it is clear that it is not part of the string-specifying machinery.\nThere are other characters in R that require escaping, and this rule applies to all string functions in R, including regular expressions. See here for a complete list of R escape sequences.\n\n\n\\': single quote. You don’t need to escape single quote inside a double-quoted string, so we can also use \" ' \" in the previous example.\n\n\n\\\": double quote. Similarly, double quotes can be used inside a single-quoted string, i.e. ' \" '.\n\n\n\\n: newline.\n\n\n\\r: carriage return.\n\n\n\\t: tab character.\n\n\nNote: cat() and print() handle escape sequences differently, if you want to print a string out with these sequences interpreted, use cat().\n\n\nprint(\"a\\nb\")\n\n[1] \"a\\nb\"\n\ncat(\"a\\nb\")\n\na\nb\n\n\nQuantifiers\nQuantifiers specify how many repetitions of the pattern.\n\n\n*: matches at least 0 times.\n\n\n+: matches at least 1 times.\n\n\n?: matches at most 1 times.\n\n\n{n}: matches exactly n times.\n\n\n{n,}: matches at least n times.\n\n\n{n,m}: matches between n and m times.\n\n\n(strings &lt;- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\"))\n\n[1] \"a\"      \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = TRUE)\n\n[1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = FALSE)\n\n[1] 2 3 4 5 6\n\ngrep(\"ac+b\", strings, value = TRUE)\n\n[1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac?b\", strings, value = TRUE)\n\n[1] \"ab\"  \"acb\"\n\ngrep(\"ac{2}b\", strings, value = TRUE)\n\n[1] \"accb\"\n\ngrep(\"ac{2}b\", strings, value = FALSE)\n\n[1] 4\n\ngrep(\"ac{2,}b\", strings, value = TRUE)\n\n[1] \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac{2,3}b\", strings, value = TRUE)\n\n[1] \"accb\"  \"acccb\"\n\n\nPosition of pattern within the string\n\n\n^: matches the start of the string.\n\n\n$: matches the end of the string.\n\n\n\\b: matches the empty string at either edge of a word. Don’t confuse it with ^ $ which marks the edge of a string.\n\n\n\\B: matches the empty string provided it is not at an edge of a word.\n\n\n(strings &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\"))\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"ab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"abcd\"\n\ngrep(\"ab$\", strings, value = TRUE)\n\n[1] \"cdab\"\n\ngrep(\"\\\\bab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"c abd\"\n\n\nOperators\n\n\n.: matches any single character, as shown in the first example.\n\n[...]: a character list, matches any one of the characters inside the square brackets. We can also use - inside the brackets to specify a range of characters.\n\n\n[^...]: an inverted character list, similar to [...], but matches any characters except those inside the square brackets.\n\n\n\\: suppress the special meaning of metacharacters in regular expression, i.e. $ * + . ? [ ] ^ { } | ( ) \\, similar to its usage in escape sequences. Since \\ itself needs to be escaped in R, we need to escape these metacharacters with double backslash like \\\\$.\n\n\n|: an “or” operator, matches patterns on either side of the |.\n\n\n(...): grouping in regular expressions. This allows you to retrieve the bits that matched various parts of your regular expression so you can alter them or use them for building up a new string. Each group can than be refer using \\\\N, with N being the No. of (...) used. This is called backreference.\n\n\n(strings &lt;- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\"))\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab.\", strings, value = TRUE)\n\n[1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab[c-e]\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\" \"abe\"\n\ngrep(\"ab[^c]\", strings, value = TRUE)\n\n[1] \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"\\\\^ab\", strings, value = TRUE)\n\n[1] \"^ab\"\n\ngrep(\"abc|abd\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\"\n\ngsub(\"(ab) 12\", \"\\\\1 34\", strings)\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 34\"\n\n\nCharacter classes\nCharacter classes allow specifying entire classes of characters, such as numbers, letters, etc. There are two flavors of character classes, one uses [: and :] around a predefined name inside square brackets and the other uses \\ and a special character. They are sometimes interchangeable.\n\n\n[:digit:] or \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent to [0-9].\n\n\n\\D: non-digits, equivalent to [^0-9].\n\n\n[:lower:]: lower-case letters, equivalent to [a-z].\n\n\n[:upper:]: upper-case letters, equivalent to [A-Z].\n\n\n[:alpha:]: alphabetic characters, equivalent to [[:lower:][:upper:]] or [A-z].\n\n\n[:alnum:]: alphanumeric characters, equivalent to [[:alpha:][:digit:]] or [A-z0-9].\n\n\n\\w: word characters, equivalent to [[:alnum:]_] or [A-z0-9_].\n\n\n\\W: not word, equivalent to [^A-z0-9_].\n\n\n[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f, equivalent to [0-9A-Fa-f].\n\n[:blank:]: blank characters, i.e. space and tab.\n\n\n[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\n\n\\s: space, .\n\n\n\\S: not space.\n\n\n[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~.\n\n[:graph:]: graphical (human readable) characters: equivalent to [[:alnum:][:punct:]].\n\n[:print:]: printable characters, equivalent to [[:alnum:][:punct:]\\\\s].\n\n[:cntrl:]: control characters, like \\n or \\r, [\\x00-\\x1F\\x7F].\n\nNote:\n* [:...:] has to be used inside square brackets, e.g. [[:digit:]].\n* \\ itself is a special character that needs escape, e.g. \\\\d. Do not confuse these regular expressions with R escape sequences such as \\t.\nstringr\nIn many cases, you will want to use the incredibly useful and tidy set of functions available in the stringr package. (stringr is a core package in the tidyverse.) For example, below we’ve extracted the first (and then last) word as a character string from the StreetName variable.\n\n\nstringr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/strings.pdf\n\n\nlibrary(Stat2Data)\ndata(RailsTrails)\nRailsTrails &lt;- RailsTrails |&gt; \n  select(HouseNum, Bedrooms, Price2014, StreetName) \nRailsTrails |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName\n1        1        3       211 Acrebrook Drive\n2        2        3       204       Autumn Dr\n3        3        3       339     Bridge Road\n4        4        3       276     Bridge Road\n5        5        4       169     Bridge Road\n6        6        3       211 Brierwood Drive\n\nRailsTrails |&gt;\n  mutate(first_piece = stringr::word(StreetName, start = 1)) |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName first_piece\n1        1        3       211 Acrebrook Drive   Acrebrook\n2        2        3       204       Autumn Dr      Autumn\n3        3        3       339     Bridge Road      Bridge\n4        4        3       276     Bridge Road      Bridge\n5        5        4       169     Bridge Road      Bridge\n6        6        3       211 Brierwood Drive   Brierwood\n\nRailsTrails |&gt;\n  mutate(last_piece = stringr::word(StreetName, start = -1)) |&gt; head()\n\n  HouseNum Bedrooms Price2014      StreetName last_piece\n1        1        3       211 Acrebrook Drive      Drive\n2        2        3       204       Autumn Dr         Dr\n3        3        3       339     Bridge Road       Road\n4        4        3       276     Bridge Road       Road\n5        5        4       169     Bridge Road       Road\n6        6        3       211 Brierwood Drive      Drive\n\n\nAn example from my work\nBelow are a handful of string characters that represent genomic sequences which were measured in an RNA Sequencing dataset. The task below is to find intergenic regions (IGR) and identify which coding sequences (CDS) bookend the intergenic regions. Note that IGRs do not code for proteins while CDSs do. Additionally, AS refers to anti-sense which identifies the genomic sequence in the opposite orientation (e.g., CGGATCC vs CCTAGGC). [The code below was written by Madison Hobbs, Scripps ’19.]\nThe names of the genomic pieces\n\nallCounts &lt;- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\n\nFirst, it is important to identify which are IGR, CDS, and anti-sense.\n\nallCounts &lt;- allCounts |&gt; tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts\n\n  feature\n1     CDS\n2     CDS\n3     IGR\n4  AS_IGR\n5     IGR\n                                                                                                                                                                                       rest\n1                                                                                                                                                                                     b2743\n2                                                                                                                                                                                     b2764\n3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                                                                                                                     GeneidBackup\n1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nWe keep only the IGR and AS_IGR strings, and we separate the two bookends. Note, the separation comes at the backslash.\n\nigr &lt;- allCounts |&gt; filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr &lt;- igr |&gt; tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)\n\n[1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nFor each of the two bookend Genes, we need to separate out the feature from the rest. Note that we write over feature1 in the second line of code below. Both of the bookends for all sequences are CDS elements.\n\nigr$feature1 &lt;- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 &lt;- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 &lt;- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)\n\n[1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2\n1      CDS      CDS\n2      CDS      CDS\n3      CDS      CDS\n\n\nAs CDS, it is now important to find the actual genenames for each of the IGR sequences. We also keep each element’s bnum which represents a unique gene identifier in E. coli.\nbnum, genename, rna.name act as place holders for the types of elements that we will need to identify the bookends of the IGRs.\n\nbnum = \"b[0-9]{4}\"\nbnum\n\n[1] \"b[0-9]{4}\"\n\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\n\n\nigr$start.gene &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr &lt;- igr |&gt; tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma) |&gt; \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma)\nnames(igr)\n\n [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"  \n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2 start.gene end.gene start.bnum end.bnum\n1      CDS      CDS        mlc     ynfL      b1594    b1595\n2      CDS      CDS       talB      mog      b0008    b0009\n3      CDS      CDS       yoaA     yoaB      b1808    b1809\n\n\nHelpful tutorials/files\n\n\nstringr vignette: https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html\nstringr package\nJenny Bryan’s STAT 545 notes: https://stat545.com/character-vectors.html\nJenny Bryan’s STAT 545 lab: http://stat545.com/block022_regular-expression.html\nHadley Wickham’s book R for Data Science\n\n\nregexpal\n\n\nRegExr\n\nRegular expression in R official document.\nFun examples\n\nThe name Hilary: https://hilaryparker.com/2013/01/30/hilary-the-most-poisoned-baby-name-in-us-history/\nTrump’s tweets: http://varianceexplained.org/r/trump-tweets/\nTrump’s tweets, take two: http://varianceexplained.org/r/trump-followup/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science in R",
    "section": "",
    "text": "Welcome to Foundations of Data Science in R\nDaily class notes for Foundations of Data Science in R, by Jo Hardin, DS002R at Pomona College. Built on Modern Data Science with R by Baumer, Kaplan, and Horton, 3rd ed. and R for Data Science by Wickham, Çetinkaya-Rundel, and Grolemund, 2nd ed.\nYou are responsible for reading your texts. They are both free, excellent, and readable, so you should use them. That said, you should also make sure you are coming to class and asking lots of questions.\nMore information and course details can be found at the DS002R website.\n\nCopyright © 2024.\nVersion date: August 01, 2024.\nThe notes are available under a Creative Commons Attribution-ShareAlike 3.0 Unported United States License. License details are available at the Creative Commons website:creativecommons.org.\nSource files for these notes can be found on GitHub athttps://github.com/hardin47/website/tree/gh-pages/DS002R.",
    "crumbs": [
      "Welcome to Foundations of Data Science in R"
    ]
  },
  {
    "objectID": "intro-to-sql.html#quarto",
    "href": "intro-to-sql.html#quarto",
    "title": "Untitled",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "intro-to-sql.html#running-code",
    "href": "intro-to-sql.html#running-code",
    "title": "Untitled",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "using-sql.html#quarto",
    "href": "using-sql.html#quarto",
    "title": "Untitled",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "using-sql.html#running-code",
    "href": "using-sql.html#running-code",
    "title": "Untitled",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "reg-expr.html#quarto",
    "href": "reg-expr.html#quarto",
    "title": "Untitled",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "reg-expr.html#running-code",
    "href": "reg-expr.html#running-code",
    "title": "Untitled",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "01-db.html#what-is-a-database",
    "href": "01-db.html#what-is-a-database",
    "title": "1  Databases",
    "section": "\n1.1 What is a database?",
    "text": "1.1 What is a database?\nYou are likely already familiar with the concept of tidy data. (If you have never encountered tidy data, see Chapter 5 Data tidying in R for Data Science.) Tidy data typically live in data frames or tables; where, importantly, they consist of columns of variables (where every column is the same type!) and the rows of observational units.\nConceptually, a table in a database is no different from the data frames which are used in R. They will always be rectangles with the same row and column structure.\nThere are two big differences between a stand alone data frame (e.g., in R) and a table which lives in a database (e.g., in SQL).\n\n\nMost importantly, tables in databases can be arbitrarily large, primarily due to being stored on disk. Indeed, they are typially not stored on your computer’s disk, they are stored remotely on a hard drive outside of your own computer and work space. Data frames are stored in memory (on your computer) and can be quite limited in size.\n\nMemory (RAM) is the amount of data that a computer can work on simultaneously. My computer has 32 GB of RAM. The important thing about memory is that the computer has easy access to its own memory and will access it quickly (tens of GBs per second).\nHard Disk is the amount of data that a computer can store permanently; it is your storage space. My computer has 2 TB of storage. Accessing the disk is much slower (hundreds of MBs per second) than accessing the memory. Accessing disk space is even slower if the storage lives on a different computer and is accessed virtually (via WiFi).\n\n\nThe tables in a database are usually linked with a key. We will cover join() functions in Chapter 4, but keep in mind that in order to get information from one table to connect to information in another table, we need to somehow relate the rows of the first table to the rows of the second table.\n\nTo demonstrate the difference between data frames in memory and tables in storage, we will consider the airlines data consisting of millions of individual flights going back to 1987. The full dataset occupies almost 20GB when they are saved as CSV (comma separated value) files, a common way to hold data that can be represented as columns of text which are separated by commas."
  },
  {
    "objectID": "01-db.html#tables-in-sql",
    "href": "01-db.html#tables-in-sql",
    "title": "1  Databases",
    "section": "\n1.3 Tables in SQL\n",
    "text": "1.3 Tables in SQL\n\nTo look at the airlines data tables, we first need to connect to the database remotely. The function dbConnect_scidb() in the mdsr package allows us to connect to the databases which are stored for use with the text Modern Data Science with R.\nThe tbl() function (in the dplyr package) maps the tables called flights and carriers from the database to an object in R.\n\nlibrary(tidyverse)\nlibrary(mdsr)\ncon_air &lt;- mdsr::dbConnect_scidb(\"airlines\")\nflights &lt;- tbl(con_air, \"flights\")\ncarriers &lt;- tbl(con_air, \"carriers\")\n\nAlternatively, the SQL connection can be set up directly using information specific to the mdsr R package database.\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\nWe can now use the objects flights and carriers as if they were data frames, but they are not actually the R version of data.frame. Instead, they exist as a tbl which is a special object that behaves similarly to a data.frame.\nThe carriers data represents the name of each airline and the associated carrier code. Note that when the data are printed to screen, the number of rows is given by ??, indicating that they are unknown. Indeed, R just needed the first few rows to print to screen, no need to spend precious computing resources looking at the entire table.\n\ncarriers \n\n# Source:   table&lt;carriers&gt; [?? x 2]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  carrier name                         \n  &lt;chr&gt;   &lt;chr&gt;                        \n1 02Q     Titan Airways                \n2 04Q     Tradewind Aviation           \n3 05Q     Comlux Aviation, AG          \n4 06Q     Master Top Linhas Aereas Ltd.\n5 07Q     Flair Airlines Ltd.          \n6 09Q     Swift Air, LLC               \n# ℹ more rows\n\n\nWith the carriers data, it is possible, however, to load the entire object into R using collect(). Now, when the data are printed to screen, the object is a tibble and R knows that it has 1,610 rows.\n\ncarriers |&gt;\n  collect() \n\n# A tibble: 1,610 × 2\n  carrier name                         \n  &lt;chr&gt;   &lt;chr&gt;                        \n1 02Q     Titan Airways                \n2 04Q     Tradewind Aviation           \n3 05Q     Comlux Aviation, AG          \n4 06Q     Master Top Linhas Aereas Ltd.\n5 07Q     Flair Airlines Ltd.          \n6 09Q     Swift Air, LLC               \n# ℹ 1,604 more rows\n\n\nAs already mentioned, working in R versus working remotely has trade-offs. Remember that the object takes up much more space in your memory if you load it into R. Consider the following which demonstrates how much more memory intensive it is to hold an object in R.\n\n# carriers lives in the SQL database and is linked remotely\ncarriers |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n5.2 Kb\n\n\n\n# carriers lives in R\ncarriers |&gt;\n  collect() |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n234.8 Kb\n\n\nIndeed, the flights data set contains all of the flights and is much larger than the carriers data set. When pulled into R it takes up almost 5 GB of memory (!), but when it exists as only a connection to the SQL database is uses just a few Kb of memory.\n\n# flights lives in the database and is linked remotely\nflights |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n6.5 Kb\n\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\nlibrary(DBI)\ndbDisconnect(con_air, shutdown = TRUE)"
  },
  {
    "objectID": "01-db.html#many-sql-interfaces",
    "href": "01-db.html#many-sql-interfaces",
    "title": "1  Databases",
    "section": "\n1.3 Many SQL interfaces",
    "text": "1.3 Many SQL interfaces\nSQL (Structured Query Language) is a programming language for working with relational databases. (The relational part means that the datasets are connected in a meaningful way, the database part means that we have lots of tables living somewhere remotely on a hard drive.) SQL has been around since the 1970s and is extremely powerful for data wrangling tasks.\nAlthough SQL has been a standard for the American National Standards Institute (ANSI) since 1986, there exist many dialects of SQL. Translating between the dialects is not always easy although once you learn how to program in one dialect, you will be able to pick up any of the other SQL dialects. We will use MySQL in this class. MySQL is among the most popular implementations of SQL and it is open source."
  },
  {
    "objectID": "01-db.html#engaging-with-a-database",
    "href": "01-db.html#engaging-with-a-database",
    "title": "1  Databases",
    "section": "\n1.4 Engaging with a database",
    "text": "1.4 Engaging with a database\n\n1.4.1 Many SQL implementations\nSQL (Structured Query Language) is a query language for working with relational databases. (The relational part means that the data sets are connected in a meaningful way, the database part means that we have lots of tables living somewhere remotely on a hard drive.) SQL has been around since the 1970s and is extremely powerful for data wrangling tasks.\nAlthough SQL has been a standard for the American National Standards Institute (ANSI) since 1986, there exist many dialects of SQL. Translating between the dialects is not always easy although once you learn how to program in one dialect, you will be able to pick up any of the other SQL dialects. We will use MySQL in this class. MySQL is among the most popular implementations of SQL and it is open source.\n\n1.4.2 SQL interfaces\nMySQL is based on a client-server model. The data live on a powerful computer (the server) and you connect to the data from your own computer (the client). In this class:\n\n\nSQL code will be written in the MySQL dialect\nusing both RStudio and DBeaver as the interface to\nconnect to many different remote servers.\n\n1.4.3 SQL in-process\nAnother approach to engaging with SQL is where the client and the server are both on a single computer (called in-process). You may want to set up a database on your own computer to try things out and avoid monthly charges associated with buying server space in the cloud. Indeed, for the end of the semester project, if you choose to set up a database on your own computer, a good free database management system is Duckdb"
  },
  {
    "objectID": "01-db.html#section",
    "href": "01-db.html#section",
    "title": "1  Databases",
    "section": "\n1.4 ",
    "text": "1.4"
  },
  {
    "objectID": "01-db.html#sec-what-db",
    "href": "01-db.html#sec-what-db",
    "title": "1  Databases",
    "section": "\n1.1 What is a database?",
    "text": "1.1 What is a database?\nA database is a structured collection of data that is organized in such a way that facilitates efficient storage, retrieval, and management of information. They are particularly important for industries with exceptionally large amounts of data which can be partitioned into different tables. Additionally, databases allow for multiple users to access the data simultaneously.\nYou are likely already familiar with the concept of tidy data. (If you have never encountered tidy data, see Chapter 5 Data tidying in R for Data Science.) Tidy data typically live in data frames or tables; where, importantly, they consist of columns of variables (where every column is the same type!) and the rows of observational units.\nConceptually, a table in a database is no different from the data frames which are used in R. They will always be rectangles with the same row and column structure.\nThere are two big differences between a stand alone data frame (e.g., in R) and a table which lives in a database (e.g., in SQL).\n\n\nMost importantly, tables in databases can be arbitrarily large, primarily due to being stored on disk. Indeed, they are typically not stored on your computer’s disk, they are stored remotely on a hard drive outside of your own computer and work space. Data frames are stored in memory (on your computer) and can be quite limited in size.\n\nMemory (RAM) is the amount of data that a computer can work on simultaneously. My computer has 32 GB of RAM. The important thing about memory is that the computer has easy access to its own memory and will access it quickly (tens of GBs per second).\nHard Disk is the amount of data that a computer can store permanently; it is your storage space. My computer has 2 TB of storage. Accessing the disk is much slower (hundreds of MBs per second) than accessing the memory. Accessing disk space is even slower if the storage lives on a different computer and is accessed virtually (via WiFi).\n\n\nThe tables in a database are usually linked with a key. We will cover join() functions in Chapter 4, but keep in mind that in order to get information from one table to connect to information in another table, we need to somehow relate the rows of the first table to the rows of the second table."
  },
  {
    "objectID": "01-db.html#connecting-to-a-database",
    "href": "01-db.html#connecting-to-a-database",
    "title": "1  Databases",
    "section": "\n1.5 Connecting to a database",
    "text": "1.5 Connecting to a database\nWhen using R to connect to a database, we need two R packages. DBI is a low-level interface that connects to databases and executes SQL; RMariaDB is a package specifically tailored for MySQL which translates generic DBI commands into the specific syntax needed for MySQL.\nA third R package, dbplyr, is a high-level interface that translates dplyr code (i.e., R code) to SQL queries then executes them with DBI.\n\n1.5.1 Creating a database connection\nBefore we can do anything, we need to set up a connection to a MySQL database. We will call the connection con, and the syntax will look something like what is written below. However, the code below won’t run because values must be set for dbname, host, user, and password.\n\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"  \",\n  host = \"  \",\n  user = \"  \",\n  password = \"  \"\n)\n\nNote that the function dbConnect_scidb() in the mdsr package is just a wrapper of the dbConnect() function where all the arguments are filled in to connect to the mdsr database.\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con, shutdown = TRUE)\n\n\n1.5.2 Connecting to Duckdb\nConnecting to Duckdb is reasonably straightforward because the default values of the duckdb() function in the duckdb package create a temporary database that is deleted when you quit R.\n\ncon_duckdb &lt;- DBI::dbConnect(duckdb::duckdb())\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)"
  },
  {
    "objectID": "01-db.html#sec-dplyr-seq",
    "href": "01-db.html#sec-dplyr-seq",
    "title": "1  Databases",
    "section": "\n1.5 Translating dplyr code into SQL",
    "text": "1.5 Translating dplyr code into SQL\nLet’s go back to the airlines database to try out some things that we already know how to do in R. Recall that we need the DBI and RMariaDB packages to connect to R; we need the dbplyr package to translate SQL code into R.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dbplyr)\n\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = \"mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com\",\n  user = \"mdsr_public\",\n  password = \"ImhsmflMDSwR\"\n)\n\nThe function dbListTables() in the DBI package will tell us what tables exist in the airlines database.\n\nDBI::dbListTables(con)\n\n[1] \"airports\" \"carriers\" \"flights\"  \"planes\""
  },
  {
    "objectID": "05-creating-db.html",
    "href": "05-creating-db.html",
    "title": "5  Creating databases",
    "section": "",
    "text": "see 21.3.2 in R4DS https://r4ds.hadley.nz/databases#sec-load-data"
  },
  {
    "objectID": "02-sql-in-R.html#sec-dplyr-seq",
    "href": "02-sql-in-R.html#sec-dplyr-seq",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.1 Translating dplyr code into SQL\n",
    "text": "2.1 Translating dplyr code into SQL\n\nLet’s go back to the airlines database to try out some things that we already know how to do in R. Recall that we need the DBI and RMariaDB packages to connect to R; we need the dbplyr package to translate SQL code into R.\n\nlibrary(DBI)\nlibrary(RMariaDB)\nlibrary(dbplyr)\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\nThe function dbListTables() in the DBI package will tell us what tables exist in the airlines database.\n\nDBI::dbListTables(con_air)\n\n[1] \"airports\" \"carriers\" \"flights\"  \"planes\"  \n\nflights &lt;- tbl(con_air, \"flights\")\ncarriers &lt;- tbl(con_air, \"carriers\")\n\nLet’s ask a few questions about the data set using data wrangling techniques that should already be familiar.\n\nOver what years is the flights data taken?\n\nTo start, let’s write the commands using tidy dplyr code.\n\nyrs &lt;- flights |&gt;\n  summarize(min_year = min(year), max_year = max(year))\n\nyrs\n\n# Source:   SQL [1 x 2]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  min_year max_year\n     &lt;int&gt;    &lt;int&gt;\n1     2010     2017\n\n\nBecause flights is not actually a data.frame in R (but instead a tbl in SQL), the work that was done above was actually performed in SQL. To see the SQL code, we can use the function show_query.\n\nshow_query(yrs)\n\n&lt;SQL&gt;\nSELECT MIN(`year`) AS `min_year`, MAX(`year`) AS `max_year`\nFROM `flights`\n\n\nNote the similarity between the R code and the SQL code. We can see SELECT and MIN and MAX which are familiar. The AS function is new, but maybe it that AS does the job of assigning a new name to the output columns. FROM is also new and does the job of piping in a data set to use.\n\nCreate a data set containing only flights between LAX and BOS in 2012.\n\n\nla_bos &lt;- flights |&gt;\n  filter(year == 2012 & ((origin == \"LAX\" & dest == \"BOS\") | \n           (origin == \"BOS\" & dest == \"LAX\"))) \n\n\nla_bos\n\n# Source:   SQL [?? x 21]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n1  2012    10     1      710            710         0     1538           1540\n2  2012    10     1      818            820        -2     1644           1645\n3  2012    10     1      855            844        11     1742           1712\n4  2012    10     1     1219           1210         9     2043           2038\n5  2012    10     1     1302           1300         2     2133           2125\n6  2012    10     1     1436           1445        -9     2331           2309\n# ℹ more rows\n# ℹ 13 more variables: arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\n\n\n\nshow_query(la_bos)\n\n&lt;SQL&gt;\nSELECT *\nFROM `flights`\nWHERE (`year` = 2012.0 AND ((`origin` = 'LAX' AND `dest` = 'BOS') OR (`origin` = 'BOS' AND `dest` = 'LAX')))\n\n\nThe WHERE function in SQL acts as filter() did in R; & has been translated to AND, and | has been translated to OR.\nAs might be expected, dbplyr doesn’t translate every R command into SQL. After all, SQL is not a statistical software and doesn’t, for example, have a mechanism for creating data visualizations. To track which R commands are connected to SQL see the dbplyr reference sheet.\nBecause the data set has been subsetted substantially, we could pull it into R to create an R object. Note that now R is aware of the size of the entire data frame (7064 rows and 21 columns). The la_bos object now exists in the R environment and can be explored through the IDE.\n\n\n\n\n\n\n Watch out!\n\n\n\nBe careful with collect(). Don’t use collect() on large data frames that won’t fit in an R environment.\n\n\n\nla_bos &lt;- la_bos |&gt;\n  collect()\n\nla_bos\n\n# A tibble: 7,064 × 21\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n1  2012    10     1      710            710         0     1538           1540\n2  2012    10     1      818            820        -2     1644           1645\n3  2012    10     1      855            844        11     1742           1712\n4  2012    10     1     1219           1210         9     2043           2038\n5  2012    10     1     1302           1300         2     2133           2125\n6  2012    10     1     1436           1445        -9     2331           2309\n# ℹ 7,058 more rows\n# ℹ 13 more variables: arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\n\n\nChapter 3 will explore more SQL queries and using SQL verbs. For now, let’s continue learning about the different ways R can talk to SQL.\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)"
  },
  {
    "objectID": "02-sql-in-R.html#sql-queries-through-the-dbi-package",
    "href": "02-sql-in-R.html#sql-queries-through-the-dbi-package",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.2 SQL queries through the DBI package",
    "text": "2.2 SQL queries through the DBI package\nUsing R as a wrapper, we can send actual SQL code to query data from the connection. It is okay if you aren’t yet able to write SQL commands from scratch, but try to figure out what the command is asking for. As mentioned above, we will start from scratch to learn SQL commands in Chapter 3.\nStart by setting up the SQL connection in the same way.\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\n\nLook at the first few rows of the flights data.\n\nBecause the flights data is not an R object, we can’t open it in R to explore the variables. If we want to see a small bit of the data, we can SELECT everything (i.e, *) from the flights table but LIMIT the query to only the first eight observations.\nNote that the code in the dbGetQuery() R function is written in SQL not in R.\nA semicolon (;) is typically used to indicate the termination of a SQL statement. They are not always required (particularly when only one statement is being sent), however, it is good practice to use a semicolon at the end of each SQL statement. (Indeed, some SQL dialects require the semicolon at the end of every statement, regardless of whether or not there are more statements following.)\n\nDBI::dbGetQuery(con_air,\n                \"SELECT * FROM flights LIMIT 8;\")\n\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2010    10   1        1           2100       181      159           2320\n2 2010    10   1        1           1920       281      230           2214\n3 2010    10   1        3           2355         8      339            334\n4 2010    10   1        5           2200       125       41           2249\n5 2010    10   1        7           2245        82      104           2347\n6 2010    10   1        7             10        -3      451            500\n7 2010    10   1        7           2150       137      139           2337\n8 2010    10   1        8             15        -7      538            537\n  arr_delay carrier tailnum flight origin dest air_time distance cancelled\n1       159      XE  N11137   2558    EWR  OMA      162     1133         0\n2       256      B6  N659JB    562    FLL  SWF      131     1119         0\n3         5      B6  N563JB    701    JFK  SJU      196     1597         0\n4       112      XE  N16559   5982    IAD  BNA       82      542         0\n5        77      OO  N908SW   6433    LAX  FAT       37      209         0\n6        -9      AA  N3FRAA    700    LAX  DFW      150     1235         0\n7       122      DL  N347NW   1752    ATL  IAD       70      533         0\n8         1      CO  N73283   1740    SMF  IAH      193     1609         0\n  diverted hour minute           time_hour\n1        0   21      0 2010-10-01 21:00:00\n2        0   19     20 2010-10-01 19:20:00\n3        0   23     55 2010-10-01 23:55:00\n4        0   22      0 2010-10-01 22:00:00\n5        0   22     45 2010-10-01 22:45:00\n6        0    0     10 2010-10-01 00:10:00\n7        0   21     50 2010-10-01 21:50:00\n8        0    0     15 2010-10-01 00:15:00\n\n\n\nHow many flights per year are in the flights table?\n\n\ndbGetQuery(con_air, \n  \"SELECT year, count(*) AS num_flights FROM flights GROUP BY year ORDER BY num_flights;\")\n\n  year num_flights\n1 2016     5617658\n2 2017     5674621\n3 2015     5819079\n4 2014     5819811\n5 2011     6085281\n6 2012     6096762\n7 2013     6369482\n8 2010     6450117\n\n\nNote that we’ve now SELECTed two variables: year and num_flights (which we created along the way using count(*) which is written as n() in R) FROM the flights table. Then we GROUP BY the year variable which retroactively acts on the count(*) function. And last, we ORDER BY (which is similar to arrange()) the new num_flights variable.\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)"
  },
  {
    "objectID": "02-sql-in-R.html#direct-sql-queries-through-a-sql-chunk",
    "href": "02-sql-in-R.html#direct-sql-queries-through-a-sql-chunk",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.3 Direct SQL queries through a sql chunk",
    "text": "2.3 Direct SQL queries through a sql chunk\nNotice that the formatting of the next few chunks is slightly different. Instead of reporting only the inside / code of the chunk, the entire chunk is printed. The SQL chunks are given by {sql} instead of {r} and each SQL chunk is required to connect to a particular database (through the con_air connection).\nThe same queries have been run.\nStart by setting up the SQL connection in the same way.\n\n```{r}\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n```\n\n\n```{sql}\n#| connection: con_air\n\nSELECT * FROM flights LIMIT 8;\n```\n\n\n\n\n8 records\n\n\n\nyear\n\n\nmonth\n\n\nday\n\n\ndep_time\n\n\nsched_dep_time\n\n\ndep_delay\n\n\narr_time\n\n\nsched_arr_time\n\n\narr_delay\n\n\ncarrier\n\n\ntailnum\n\n\nflight\n\n\norigin\n\n\ndest\n\n\nair_time\n\n\ndistance\n\n\ncancelled\n\n\ndiverted\n\n\nhour\n\n\nminute\n\n\ntime_hour\n\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n1\n\n\n2100\n\n\n181\n\n\n159\n\n\n2320\n\n\n159\n\n\nXE\n\n\nN11137\n\n\n2558\n\n\nEWR\n\n\nOMA\n\n\n162\n\n\n1133\n\n\n0\n\n\n0\n\n\n21\n\n\n0\n\n\n2010-10-01 21:00:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n1\n\n\n1920\n\n\n281\n\n\n230\n\n\n2214\n\n\n256\n\n\nB6\n\n\nN659JB\n\n\n562\n\n\nFLL\n\n\nSWF\n\n\n131\n\n\n1119\n\n\n0\n\n\n0\n\n\n19\n\n\n20\n\n\n2010-10-01 19:20:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n3\n\n\n2355\n\n\n8\n\n\n339\n\n\n334\n\n\n5\n\n\nB6\n\n\nN563JB\n\n\n701\n\n\nJFK\n\n\nSJU\n\n\n196\n\n\n1597\n\n\n0\n\n\n0\n\n\n23\n\n\n55\n\n\n2010-10-01 23:55:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n5\n\n\n2200\n\n\n125\n\n\n41\n\n\n2249\n\n\n112\n\n\nXE\n\n\nN16559\n\n\n5982\n\n\nIAD\n\n\nBNA\n\n\n82\n\n\n542\n\n\n0\n\n\n0\n\n\n22\n\n\n0\n\n\n2010-10-01 22:00:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n7\n\n\n2245\n\n\n82\n\n\n104\n\n\n2347\n\n\n77\n\n\nOO\n\n\nN908SW\n\n\n6433\n\n\nLAX\n\n\nFAT\n\n\n37\n\n\n209\n\n\n0\n\n\n0\n\n\n22\n\n\n45\n\n\n2010-10-01 22:45:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n7\n\n\n10\n\n\n-3\n\n\n451\n\n\n500\n\n\n-9\n\n\nAA\n\n\nN3FRAA\n\n\n700\n\n\nLAX\n\n\nDFW\n\n\n150\n\n\n1235\n\n\n0\n\n\n0\n\n\n0\n\n\n10\n\n\n2010-10-01 00:10:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n7\n\n\n2150\n\n\n137\n\n\n139\n\n\n2337\n\n\n122\n\n\nDL\n\n\nN347NW\n\n\n1752\n\n\nATL\n\n\nIAD\n\n\n70\n\n\n533\n\n\n0\n\n\n0\n\n\n21\n\n\n50\n\n\n2010-10-01 21:50:00\n\n\n\n\n2010\n\n\n10\n\n\n1\n\n\n8\n\n\n15\n\n\n-7\n\n\n538\n\n\n537\n\n\n1\n\n\nCO\n\n\nN73283\n\n\n1740\n\n\nSMF\n\n\nIAH\n\n\n193\n\n\n1609\n\n\n0\n\n\n0\n\n\n0\n\n\n15\n\n\n2010-10-01 00:15:00\n\n\n\n\n\n\n\n```{sql}\n#| connection: con_air\n\nSELECT year, count(*) AS num_flights FROM flights GROUP BY year ORDER BY num_flights;\n```\n\n\n\n\n8 records\n\n\n\nyear\n\n\nnum_flights\n\n\n\n\n\n2016\n\n\n5617658\n\n\n\n\n2017\n\n\n5674621\n\n\n\n\n2015\n\n\n5819079\n\n\n\n\n2014\n\n\n5819811\n\n\n\n\n2011\n\n\n6085281\n\n\n\n\n2012\n\n\n6096762\n\n\n\n\n2013\n\n\n6369482\n\n\n\n\n2010\n\n\n6450117\n\n\n\n\n\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)"
  },
  {
    "objectID": "02-sql-in-R.html#fa-triangle-exclamation-be-aware",
    "href": "02-sql-in-R.html#fa-triangle-exclamation-be-aware",
    "title": "2  SQL in R",
    "section": "\n2.2 Be aware",
    "text": "2.2 Be aware\nBe careful with collect()! Don’t use collect() on large dataframes that won’t fit in an R environment. :::\n\nla_bos &lt;- la_bos |&gt;\n  collect()\n\nla_bos\n\n# A tibble: 7,064 × 21\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n1  2012    10     1      710            710         0     1538           1540\n2  2012    10     1      818            820        -2     1644           1645\n3  2012    10     1      855            844        11     1742           1712\n4  2012    10     1     1219           1210         9     2043           2038\n5  2012    10     1     1302           1300         2     2133           2125\n6  2012    10     1     1436           1445        -9     2331           2309\n# ℹ 7,058 more rows\n# ℹ 13 more variables: arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\n\n\n?sec-sql-verbs will explore more SQL queries and using SQL verbs. For now, let’s continue learning about the different ways R can talk to SQL."
  },
  {
    "objectID": "02-sql-in-R.html#be-aware",
    "href": "02-sql-in-R.html#be-aware",
    "title": "2  SQL in R",
    "section": "\n2.2 Be aware",
    "text": "2.2 Be aware\n Be careful with collect()! Don’t use collect() on large dataframes that won’t fit in an R environment. :::\n\nla_bos &lt;- la_bos |&gt;\n  collect()\n\nla_bos\n\n# A tibble: 7,064 × 21\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;int&gt;    &lt;int&gt;          &lt;int&gt;\n1  2012    10     1      710            710         0     1538           1540\n2  2012    10     1      818            820        -2     1644           1645\n3  2012    10     1      855            844        11     1742           1712\n4  2012    10     1     1219           1210         9     2043           2038\n5  2012    10     1     1302           1300         2     2133           2125\n6  2012    10     1     1436           1445        -9     2331           2309\n# ℹ 7,058 more rows\n# ℹ 13 more variables: arr_delay &lt;int&gt;, carrier &lt;chr&gt;, tailnum &lt;chr&gt;,\n#   flight &lt;int&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;int&gt;, distance &lt;int&gt;,\n#   cancelled &lt;int&gt;, diverted &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;, time_hour &lt;dttm&gt;\n\n\n?sec-sql-verbs will explore more SQL queries and using SQL verbs. For now, let’s continue learning about the different ways R can talk to SQL."
  },
  {
    "objectID": "03-sql-verbs.html",
    "href": "03-sql-verbs.html",
    "title": "3  SQL clauses",
    "section": "",
    "text": "4 Saving SQL queries as R objects\nIf you are working in R to run SQL commands, you may want to use the query output for further analysis or visualizations. In that case, use #|output.var: \"name_of_variable\" inside the {sql} chunk. The variable called name_of_variable will then be available to be used in the R environment.\n```{sql}\n#| connection: con_taxi\n#| label: new-table\n#| output.var: \"new_table\"\n\nSELECT *, DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old \nLIMIT 0, 1000;\n\n```\nTable 4.1: New data.frame saved to R called new_table.\n\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\ntrip_distance\npickup_longitude\npickup_latitude\nrate_code\nstore_and_fwd_flag\ndropoff_longitude\ndropoff_latitude\npayment_type\nfare_amount\nsurcharge\nmta_tax\ntip_amount\ntolls_amount\ntotal_amount\nwday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMT\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n1\n2.0\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nSaturday\n\n\nCMT\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.00\n0.00\n8.00\nSaturday\n\n\nCMT\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n3\n0.5\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nSaturday\n\n\nCMT\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n3\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.5\n0.5\n3.00\n0.00\n18.00\nSaturday\n\n\nCMT\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.00\n0.00\n12.50\nSaturday\n\n\nCMT\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n0.50\n0.00\n5.50\nSaturday\n\n\nCMT\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n2\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.5\n0.5\n3.10\n0.00\n18.60\nSaturday\n\n\nCMT\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n1\n5.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n20.0\n0.5\n0.5\n3.00\n0.00\n24.00\nSaturday\n\n\nCMT\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.5\n0.5\n2.90\n0.00\n17.40\nSaturday\n\n\nCMT\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n3\n5.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.5\n0.5\n2.00\n0.00\n24.50\nSaturday\n\n\nCMT\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nSaturday\n\n\nCMT\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n3\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n2.80\n0.00\n16.80\nSaturday\n\n\nCMT\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n1\n2.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.5\n0.5\n1.50\n0.00\n13.50\nSaturday\n\n\nCMT\n2014-03-01 01:25:34\n2014-03-01 02:01:03\n1\n7.6\n-73.9\n40.7\n1\nN\n-73.9\n40.7\nCRD\n29.5\n0.5\n0.5\n6.10\n0.00\n36.60\nSaturday\n\n\nCMT\n2014-03-01 01:26:39\n2014-03-01 01:30:03\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.5\n0.5\n1.00\n0.00\n7.00\nSaturday\n\n\nCMT\n2014-03-01 01:27:16\n2014-03-01 01:46:59\n1\n11.1\n-74.0\n40.8\n1\nN\n-73.9\n40.9\nCRD\n31.0\n0.5\n0.5\n6.00\n2.44\n40.44\nSaturday\n\n\nCMT\n2014-03-01 01:28:39\n2014-03-01 01:30:53\n1\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nSaturday\n\n\nCMT\n2014-03-01 01:29:40\n2014-03-01 01:35:01\n2\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n1.30\n0.00\n7.80\nSaturday\n\n\nCMT\n2014-03-01 01:28:51\n2014-03-01 01:43:06\n1\n5.1\n-74.0\n40.8\n1\nN\n-73.9\n40.7\nCRD\n16.5\n0.5\n0.5\n2.00\n0.00\n19.50\nSaturday\n\n\nCMT\n2014-03-01 01:30:00\n2014-03-01 02:12:12\n1\n8.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n33.5\n0.5\n0.5\n8.62\n0.00\n43.12\nSaturday\n\n\nCMT\n2014-03-01 09:10:00\n2014-03-01 09:28:55\n1\n7.9\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n25.0\n0.0\n0.5\n3.00\n0.00\n28.50\nSaturday\n\n\nCMT\n2014-03-01 09:11:28\n2014-03-01 09:18:46\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nSaturday\n\n\nCMT\n2014-03-01 09:13:07\n2014-03-01 09:18:12\n2\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nSaturday\n\n\nCMT\n2014-03-01 01:30:45\n2014-03-01 01:34:24\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nSaturday\n\n\nCMT\n2014-03-01 09:14:31\n2014-03-01 09:17:13\n1\n0.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.0\n0.5\n1.12\n0.00\n5.62\nSaturday\n\n\nCMT\n2014-03-01 09:16:08\n2014-03-01 09:33:46\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.90\n0.00\n17.40\nSaturday\n\n\nCMT\n2014-03-01 01:00:01\n2014-03-01 01:15:42\n1\n3.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.5\n0.5\n0.5\n3.10\n0.00\n18.60\nSaturday\n\n\nCMT\n2014-03-01 01:01:01\n2014-03-01 01:37:37\n1\n4.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n24.0\n0.5\n0.5\n4.00\n0.00\n29.00\nSaturday\n\n\nCMT\n2014-03-01 01:00:27\n2014-03-01 01:18:47\n1\n8.2\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n25.5\n0.5\n0.5\n5.30\n0.00\n31.80\nSaturday\n\n\nCMT\n2014-03-01 01:01:03\n2014-03-01 01:06:59\n2\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nSaturday\n\n\nCMT\n2014-03-01 01:01:40\n2014-03-01 01:04:58\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n0.75\n0.00\n6.25\nSaturday\n\n\nCMT\n2014-03-01 01:03:29\n2014-03-01 01:09:26\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nSaturday\n\n\nCMT\n2014-03-01 01:03:33\n2014-03-01 01:28:46\n1\n4.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n20.0\n0.5\n0.5\n4.20\n0.00\n25.20\nSaturday\n\n\nCMT\n2014-03-01 01:04:59\n2014-03-01 01:20:54\n1\n4.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n16.0\n0.5\n0.5\n3.40\n0.00\n20.40\nSaturday\n\n\nCMT\n2014-03-01 01:04:08\n2014-03-01 01:08:31\n2\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.5\n0.5\n1.00\n0.00\n6.50\nSaturday\n\n\nCMT\n2014-03-01 01:05:15\n2014-03-01 01:21:06\n2\n3.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.5\n0.5\n1.91\n0.00\n17.91\nSaturday\n\n\nCMT\n2014-03-01 01:07:19\n2014-03-01 01:16:56\n1\n4.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n14.0\n0.5\n0.5\n1.00\n0.00\n16.00\nSaturday\n\n\nCMT\n2014-03-01 01:18:36\n2014-03-01 01:23:38\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.75\n0.00\n8.75\nSaturday\n\n\nCMT\n2014-03-01 01:18:36\n2014-03-01 01:39:26\n1\n6.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n22.5\n0.5\n0.5\n1.50\n0.00\n25.00\nSaturday\n\n\nCMT\n2014-03-01 01:18:53\n2014-03-01 01:27:15\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.00\n0.00\n12.50\nSaturday\n\n\nCMT\n2014-03-01 01:19:55\n2014-03-01 01:39:18\n4\n4.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.0\n0.5\n0.5\n3.80\n0.00\n22.80\nSaturday\n\n\nCMT\n2014-03-01 01:20:30\n2014-03-01 01:42:32\n1\n7.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n25.0\n0.5\n0.5\n2.00\n5.33\n33.33\nSaturday\n\n\nCMT\n2014-03-01 01:21:30\n2014-03-01 01:45:41\n1\n7.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n26.5\n0.5\n0.5\n5.50\n0.00\n33.00\nSaturday\n\n\nCMT\n2014-03-01 01:21:46\n2014-03-01 01:36:18\n2\n5.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n18.5\n0.5\n0.5\n3.90\n0.00\n23.40\nSaturday\n\n\nCMT\n2014-03-01 01:24:28\n2014-03-01 01:32:50\n1\n2.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.5\n0.5\n1.50\n0.00\n13.00\nSaturday\n\n\nCMT\n2014-03-01 01:24:29\n2014-03-01 01:36:26\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n2.85\n0.00\n14.35\nSaturday\n\n\nCMT\n2014-03-01 01:25:12\n2014-03-01 01:43:17\n1\n4.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.5\n0.5\n0.5\n5.25\n0.00\n22.75\nSaturday\n\n\nCMT\n2014-03-01 01:33:04\n2014-03-01 01:59:04\n1\n4.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n20.5\n0.5\n0.5\n1.00\n0.00\n22.50\nSaturday\n\n\nCMT\n2014-03-01 01:33:33\n2014-03-01 01:43:22\n3\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nSaturday\n\n\nCMT\n2014-03-01 09:19:58\n2014-03-01 09:25:35\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nSaturday\n\n\nCMT\n2014-03-01 09:19:36\n2014-03-01 09:27:05\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 01:35:11\n2014-03-01 02:12:34\n2\n5.9\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n27.0\n0.5\n0.5\n5.60\n0.00\n33.60\nSaturday\n\n\nCMT\n2014-03-01 12:02:58\n2014-03-01 12:08:05\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nSaturday\n\n\nCMT\n2014-03-01 04:07:49\n2014-03-01 04:27:46\n1\n6.5\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n21.0\n0.5\n0.5\n5.50\n0.00\n27.50\nSaturday\n\n\nCMT\n2014-03-01 04:08:58\n2014-03-01 04:25:48\n2\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.5\n0.5\n1.60\n0.00\n17.60\nSaturday\n\n\nCMT\n2014-03-01 04:08:44\n2014-03-01 04:17:58\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nSaturday\n\n\nCMT\n2014-03-01 12:03:11\n2014-03-01 12:12:21\n2\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nSaturday\n\n\nCMT\n2014-03-01 12:04:00\n2014-03-01 12:17:07\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.0\n0.5\n2.50\n0.00\n15.00\nSaturday\n\n\nCMT\n2014-03-01 12:04:22\n2014-03-01 12:07:42\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.37\n0.00\n6.87\nSaturday\n\n\nCMT\n2014-03-01 04:11:51\n2014-03-01 04:16:01\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.5\n0.5\n1.10\n0.00\n6.60\nSaturday\n\n\nCMT\n2014-03-01 12:03:18\n2014-03-01 12:07:52\n3\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nSaturday\n\n\nCMT\n2014-03-01 04:15:14\n2014-03-01 04:27:06\n1\n3.8\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n13.0\n0.5\n0.5\n2.80\n0.00\n16.80\nSaturday\n\n\nCMT\n2014-03-01 04:12:39\n2014-03-01 04:18:21\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.5\n0.5\n1.08\n0.00\n9.08\nSaturday\n\n\nCMT\n2014-03-01 04:13:38\n2014-03-01 04:15:41\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nSaturday\n\n\nCMT\n2014-03-01 04:17:08\n2014-03-01 04:29:21\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.5\n0.5\n0.50\n0.00\n13.50\nSaturday\n\n\nCMT\n2014-03-01 04:16:45\n2014-03-01 04:22:06\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.5\n0.5\n1.60\n0.00\n8.10\nSaturday\n\n\nCMT\n2014-03-01 04:19:02\n2014-03-01 04:20:56\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n2.00\n0.00\n7.00\nSaturday\n\n\nCMT\n2014-03-01 04:16:51\n2014-03-01 04:25:58\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nSaturday\n\n\nCMT\n2014-03-01 04:18:27\n2014-03-01 04:32:09\n2\n3.6\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n13.5\n0.5\n0.5\n2.90\n0.00\n17.40\nSaturday\n\n\nCMT\n2014-03-01 04:19:06\n2014-03-01 04:29:26\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n3.00\n0.00\n13.00\nSaturday\n\n\nCMT\n2014-03-01 04:37:39\n2014-03-01 04:43:56\n1\n2.2\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n8.5\n0.5\n0.5\n2.00\n0.00\n11.50\nSaturday\n\n\nCMT\n2014-03-01 04:40:45\n2014-03-01 04:53:36\n1\n3.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n14.0\n0.5\n0.5\n3.75\n0.00\n18.75\nSaturday\n\n\nCMT\n2014-03-01 04:41:08\n2014-03-01 04:45:49\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 04:41:52\n2014-03-01 04:57:30\n1\n5.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n18.0\n0.5\n0.5\n0.00\n0.00\n19.00\nSaturday\n\n\nCMT\n2014-03-01 04:46:36\n2014-03-01 04:53:17\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n1.80\n0.00\n10.80\nSaturday\n\n\nCMT\n2014-03-01 04:50:15\n2014-03-01 04:56:59\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 00:16:24\n2014-03-01 00:23:35\n1\n1.5\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nSaturday\n\n\nCMT\n2014-03-01 04:57:01\n2014-03-01 05:14:01\n1\n5.5\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n19.0\n0.5\n0.5\n1.00\n0.00\n21.00\nSaturday\n\n\nCMT\n2014-03-01 04:29:34\n2014-03-01 04:44:01\n1\n5.5\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n17.5\n0.5\n0.5\n3.70\n0.00\n22.20\nSaturday\n\n\nCMT\n2014-03-01 04:29:02\n2014-03-01 04:37:05\n2\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n0.00\n0.00\n11.00\nSaturday\n\n\nCMT\n2014-03-01 12:08:09\n2014-03-01 12:25:02\n1\n4.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n17.5\n0.0\n0.5\n3.60\n0.00\n21.60\nSaturday\n\n\nCMT\n2014-03-01 12:09:16\n2014-03-01 12:35:52\n2\n6.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n24.5\n0.0\n0.5\n2.00\n0.00\n27.00\nSaturday\n\n\nCMT\n2014-03-01 12:07:18\n2014-03-01 12:28:56\n2\n6.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n21.0\n0.0\n0.5\n2.00\n0.00\n23.50\nSaturday\n\n\nCMT\n2014-03-01 12:09:33\n2014-03-01 12:11:34\n1\n0.4\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n3.5\n0.0\n0.5\n1.00\n0.00\n5.00\nSaturday\n\n\nCMT\n2014-03-01 12:10:53\n2014-03-01 12:16:30\n2\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nSaturday\n\n\nCMT\n2014-03-01 04:34:23\n2014-03-01 04:37:05\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.5\n0.5\n1.10\n0.00\n6.60\nSaturday\n\n\nCMT\n2014-03-01 12:09:44\n2014-03-01 12:13:24\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nSaturday\n\n\nCMT\n2014-03-01 00:00:19\n2014-03-01 00:28:20\n1\n8.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n27.5\n0.5\n0.5\n5.70\n0.00\n34.20\nSaturday\n\n\nCMT\n2014-03-01 00:00:37\n2014-03-01 00:13:09\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nSaturday\n\n\nCMT\n2014-03-01 12:18:58\n2014-03-01 12:35:36\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n1.50\n0.00\n14.50\nSaturday\n\n\nCMT\n2014-03-01 05:20:29\n2014-03-01 05:28:40\n1\n3.5\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n11.5\n0.5\n0.5\n1.20\n0.00\n13.70\nSaturday\n\n\nCMT\n2014-03-01 00:01:11\n2014-03-01 00:07:56\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 00:02:54\n2014-03-01 00:28:02\n2\n6.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n22.5\n0.5\n0.5\n4.50\n0.00\n28.00\nSaturday\n\n\nCMT\n2014-03-01 12:19:33\n2014-03-01 12:23:01\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n0.50\n0.00\n5.50\nSaturday\n\n\nCMT\n2014-03-01 00:02:58\n2014-03-01 00:12:51\n2\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.08\n0.00\n10.58\nSaturday\n\n\nCMT\n2014-03-01 12:19:28\n2014-03-01 12:31:10\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n2.55\n0.00\n11.05\nSaturday\n\n\nCMT\n2014-03-01 05:32:03\n2014-03-01 05:32:39\n1\n0.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n3.0\n0.5\n0.5\n0.80\n0.00\n4.80\nSaturday\n\n\nCMT\n2014-03-01 05:40:04\n2014-03-01 05:54:17\n2\n3.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.5\n0.5\n3.00\n0.00\n18.00\nSaturday\n\n\nCMT\n2014-03-01 05:34:18\n2014-03-01 05:42:20\n1\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nSaturday\n\n\nCMT\n2014-03-01 00:03:45\n2014-03-01 00:36:00\n1\n11.1\n-74.0\n40.7\n1\nN\n-74.0\n40.6\nCRD\n35.5\n0.5\n0.5\n6.00\n0.00\n42.50\nSaturday\n\n\nCMT\n2014-03-01 12:20:29\n2014-03-01 12:36:49\n1\n4.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n15.0\n0.0\n0.5\n3.10\n0.00\n18.60\nSaturday\n\n\nCMT\n2014-03-01 12:19:45\n2014-03-01 12:22:40\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.12\n0.00\n5.62\nSaturday\n\n\nCMT\n2014-03-01 00:05:16\n2014-03-01 00:13:06\n2\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n1.80\n0.00\n10.80\nSaturday\n\n\nCMT\n2014-03-01 05:54:19\n2014-03-01 06:19:37\n2\n19.0\n-74.0\n40.7\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n10.50\n0.00\n63.00\nSaturday\n\n\nCMT\n2014-03-01 01:00:21\n2014-03-01 01:02:04\n2\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nSaturday\n\n\nCMT\n2014-03-01 01:00:51\n2014-03-01 01:09:53\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nSaturday\n\n\nCMT\n2014-03-01 01:02:35\n2014-03-01 01:08:32\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 01:02:29\n2014-03-01 01:08:36\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.00\n0.00\n8.50\nSaturday\n\n\nCMT\n2014-03-01 01:03:14\n2014-03-01 01:30:29\n1\n5.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n23.5\n0.5\n0.5\n7.35\n0.00\n31.85\nSaturday\n\n\nCMT\n2014-03-01 01:04:54\n2014-03-01 01:18:10\n3\n2.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.5\n0.5\n2.50\n0.00\n15.00\nSaturday\n\n\nCMT\n2014-03-01 01:06:59\n2014-03-01 01:10:18\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.5\n0.5\n1.65\n0.00\n7.15\nSaturday\n\n\nCMT\n2014-03-01 01:08:01\n2014-03-01 01:10:57\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.5\n0.5\n1.65\n0.00\n7.15\nSaturday\n\n\nCMT\n2014-03-01 01:18:47\n2014-03-01 01:26:34\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n2.10\n0.00\n10.60\nSaturday\n\n\nCMT\n2014-03-01 01:19:36\n2014-03-01 01:47:39\n1\n5.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n23.0\n0.5\n0.5\n5.00\n0.00\n29.00\nSaturday\n\n\nCMT\n2014-03-01 01:20:42\n2014-03-01 01:34:16\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.5\n0.5\n2.00\n0.00\n16.00\nSaturday\n\n\nCMT\n2014-03-01 01:21:24\n2014-03-01 01:29:39\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n2.00\n0.00\n11.00\nSaturday\n\n\nCMT\n2014-03-01 01:21:40\n2014-03-01 01:34:31\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.5\n0.5\n2.40\n0.00\n14.40\nSaturday\n\n\nCMT\n2014-03-01 01:23:13\n2014-03-01 01:27:43\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nSaturday\n\n\nCMT\n2014-03-01 01:23:25\n2014-03-01 01:27:38\n2\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nSaturday\n\n\nCMT\n2014-03-01 01:33:31\n2014-03-01 01:50:53\n1\n4.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n16.5\n0.5\n0.5\n3.50\n0.00\n21.00\nSaturday\n\n\nCMT\n2014-03-01 09:17:43\n2014-03-01 09:23:06\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nSaturday\n\n\nCMT\n2014-03-01 09:19:45\n2014-03-01 09:33:34\n1\n4.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n3.20\n0.00\n19.20\nSaturday\n\n\nCMT\n2014-03-01 09:21:15\n2014-03-01 09:27:29\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nSaturday\n\n\nCMT\n2014-03-17 22:44:24\n2014-03-17 23:01:51\n1\n5.3\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n18.0\n0.5\n0.5\n0.00\n0.00\n19.00\nMonday\n\n\nCMT\n2014-03-17 22:29:26\n2014-03-17 22:39:39\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.5\n0.5\n1.00\n0.00\n14.00\nMonday\n\n\nCMT\n2014-03-17 21:40:16\n2014-03-17 22:00:08\n2\n6.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n20.5\n0.5\n0.5\n3.00\n0.00\n24.50\nMonday\n\n\nCMT\n2014-03-18 05:21:12\n2014-03-18 05:46:44\n1\n6.9\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n24.5\n0.5\n0.5\n3.50\n0.00\n29.00\nTuesday\n\n\nCMT\n2014-03-17 22:13:27\n2014-03-17 22:21:37\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.50\n0.00\n11.00\nMonday\n\n\nCMT\n2014-03-17 22:02:08\n2014-03-17 22:10:55\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.5\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-18 00:17:08\n2014-03-18 00:29:20\n1\n4.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.5\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-17 20:35:05\n2014-03-17 20:45:53\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.60\n0.00\n13.10\nMonday\n\n\nCMT\n2014-03-17 21:01:36\n2014-03-17 21:03:59\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nMonday\n\n\nCMT\n2014-03-17 21:59:18\n2014-03-17 22:14:10\n1\n3.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.0\n0.5\n0.5\n3.00\n0.00\n18.00\nMonday\n\n\nCMT\n2014-03-17 20:02:17\n2014-03-17 20:10:58\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.5\n0.5\n1.00\n0.00\n11.50\nMonday\n\n\nCMT\n2014-03-18 03:31:36\n2014-03-18 03:41:07\n1\n4.8\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n14.5\n0.5\n0.5\n3.10\n0.00\n18.60\nTuesday\n\n\nCMT\n2014-03-17 23:52:33\n2014-03-18 00:02:19\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-18 05:19:57\n2014-03-18 05:28:36\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n1.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 19:48:40\n2014-03-17 19:54:14\n1\n1.6\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n7.0\n1.0\n0.5\n1.00\n0.00\n9.50\nMonday\n\n\nCMT\n2014-03-17 23:56:11\n2014-03-18 00:10:26\n1\n4.3\n-73.9\n40.7\n1\nN\n-73.9\n40.7\nCRD\n15.5\n0.5\n0.5\n3.30\n0.00\n19.80\nMonday\n\n\nCMT\n2014-03-18 04:50:21\n2014-03-18 05:00:44\n2\n3.1\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n11.5\n0.5\n0.5\n0.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 06:21:00\n2014-03-18 06:29:17\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-17 22:11:22\n2014-03-17 22:27:25\n1\n5.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n18.0\n0.0\n0.5\n3.00\n0.00\n21.50\nMonday\n\n\nCMT\n2014-03-18 09:34:32\n2014-03-18 09:52:17\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n3.62\n0.00\n18.12\nTuesday\n\n\nCMT\n2014-03-18 07:25:57\n2014-03-18 07:31:16\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 08:33:34\n2014-03-18 08:56:51\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.0\n0.5\n0.00\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-18 12:49:10\n2014-03-18 12:57:20\n1\n0.9\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 12:56:28\n2014-03-18 13:02:11\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.50\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 08:31:13\n2014-03-18 08:51:52\n1\n3.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n16.0\n0.0\n0.5\n3.30\n0.00\n19.80\nTuesday\n\n\nCMT\n2014-03-18 07:14:34\n2014-03-18 07:25:54\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n2.85\n0.00\n12.35\nTuesday\n\n\nCMT\n2014-03-18 09:50:36\n2014-03-18 10:00:23\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n2.12\n0.00\n10.62\nTuesday\n\n\nCMT\n2014-03-18 11:02:31\n2014-03-18 11:21:29\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n2.80\n0.00\n16.80\nTuesday\n\n\nCMT\n2014-03-18 11:52:18\n2014-03-18 12:01:36\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 06:46:10\n2014-03-18 06:59:14\n1\n2.8\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n1.50\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 12:25:37\n2014-03-18 12:33:45\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 06:57:08\n2014-03-18 07:12:09\n1\n6.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n20.0\n0.0\n0.5\n4.10\n0.00\n24.60\nTuesday\n\n\nCMT\n2014-03-18 08:42:55\n2014-03-18 08:49:13\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 08:30:55\n2014-03-18 08:36:26\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 05:23:09\n2014-03-18 05:29:46\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n5.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 10:22:21\n2014-03-18 10:27:22\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 08:32:56\n2014-03-18 08:35:52\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.0\n0.5\n0.90\n0.00\n5.40\nTuesday\n\n\nCMT\n2014-03-18 09:43:28\n2014-03-18 09:59:23\n1\n3.5\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n14.5\n0.0\n0.5\n3.75\n0.00\n18.75\nTuesday\n\n\nCMT\n2014-03-18 12:11:53\n2014-03-18 12:22:26\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 15:45:27\n2014-03-18 15:48:01\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n1.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 06:53:10\n2014-03-18 06:57:02\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 06:53:46\n2014-03-18 06:59:17\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 06:30:24\n2014-03-18 06:35:28\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 14:08:03\n2014-03-18 14:16:04\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 08:18:35\n2014-03-18 08:34:24\n1\n4.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.5\n0.0\n0.5\n2.00\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-18 06:48:27\n2014-03-18 06:55:23\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 11:42:25\n2014-03-18 11:48:23\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 08:20:06\n2014-03-18 08:32:32\n2\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 06:49:14\n2014-03-18 06:53:23\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 08:59:07\n2014-03-18 09:09:07\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 10:11:54\n2014-03-18 10:29:26\n1\n3.3\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.5\n0.0\n0.5\n3.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-18 07:01:15\n2014-03-18 07:07:45\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.25\n0.00\n9.25\nTuesday\n\n\nCMT\n2014-03-18 08:13:59\n2014-03-18 08:17:02\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n0.90\n0.00\n5.40\nTuesday\n\n\nCMT\n2014-03-18 07:08:18\n2014-03-18 07:16:56\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.0\n0.5\n2.85\n0.00\n12.35\nTuesday\n\n\nCMT\n2014-03-18 10:26:54\n2014-03-18 10:43:48\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n2.70\n0.00\n16.20\nTuesday\n\n\nCMT\n2014-03-18 06:41:38\n2014-03-18 06:52:00\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 13:16:00\n2014-03-18 13:30:27\n4\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n2.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 08:46:24\n2014-03-18 08:54:42\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n2.25\n0.00\n11.25\nTuesday\n\n\nCMT\n2014-03-18 11:32:52\n2014-03-18 11:42:29\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 09:20:06\n2014-03-18 09:37:00\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.70\n0.00\n11.70\nTuesday\n\n\nCMT\n2014-03-18 03:32:51\n2014-03-18 04:04:49\n1\n5.1\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n24.5\n0.5\n0.5\n7.65\n0.00\n33.15\nTuesday\n\n\nCMT\n2014-03-18 01:46:22\n2014-03-18 02:00:49\n1\n8.0\n-74.0\n40.8\n1\nN\n-73.9\n40.9\nCRD\n24.0\n0.5\n0.5\n3.00\n0.00\n28.00\nTuesday\n\n\nCMT\n2014-03-18 12:19:01\n2014-03-18 12:33:31\n1\n2.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 14:46:52\n2014-03-18 14:59:30\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.5\n0.0\n0.5\n2.80\n0.00\n16.80\nTuesday\n\n\nCMT\n2014-03-18 07:43:38\n2014-03-18 07:46:26\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.10\n0.00\n5.60\nTuesday\n\n\nCMT\n2014-03-18 10:46:32\n2014-03-18 10:50:52\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 09:12:52\n2014-03-18 09:22:14\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 15:19:05\n2014-03-18 15:57:48\n1\n10.0\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n35.0\n0.0\n0.5\n10.20\n5.33\n51.03\nTuesday\n\n\nCMT\n2014-03-18 06:31:39\n2014-03-18 06:34:28\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 10:11:29\n2014-03-18 10:16:35\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 11:08:21\n2014-03-18 11:19:49\n3\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 15:45:03\n2014-03-18 15:55:10\n1\n1.5\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 09:14:26\n2014-03-18 09:23:26\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n1.0\n0.5\n2.75\n0.00\n13.75\nTuesday\n\n\nCMT\n2014-03-18 13:48:46\n2014-03-18 14:02:55\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n1.0\n0.5\n1.00\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 06:40:44\n2014-03-18 06:58:43\n1\n5.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.5\n1.0\n0.5\n1.50\n0.00\n22.50\nTuesday\n\n\nCMT\n2014-03-18 12:07:32\n2014-03-18 12:20:59\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.0\n0.5\n3.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 13:09:27\n2014-03-18 13:11:16\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n2.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 07:41:43\n2014-03-18 07:47:16\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 10:46:07\n2014-03-18 10:53:32\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 12:36:35\n2014-03-18 12:50:25\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 06:21:38\n2014-03-18 06:39:57\n1\n5.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n20.0\n0.0\n0.5\n1.00\n0.00\n21.50\nTuesday\n\n\nCMT\n2014-03-18 15:10:11\n2014-03-18 15:20:35\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n0.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 06:24:00\n2014-03-18 06:30:30\n2\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 07:37:33\n2014-03-18 07:45:21\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 07:47:31\n2014-03-18 07:56:00\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 08:37:02\n2014-03-18 08:54:48\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n3.25\n0.00\n16.25\nTuesday\n\n\nCMT\n2014-03-18 08:31:53\n2014-03-18 08:42:17\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n2.10\n0.00\n10.60\nTuesday\n\n\nCMT\n2014-03-18 11:04:00\n2014-03-18 11:19:48\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n1.00\n0.00\n13.50\nTuesday\n\n\nCMT\n2014-03-18 11:24:50\n2014-03-18 11:30:35\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.00\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 07:17:40\n2014-03-18 07:35:14\n1\n7.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n24.5\n0.0\n0.5\n5.00\n0.00\n30.00\nTuesday\n\n\nCMT\n2014-03-18 12:13:34\n2014-03-18 12:31:04\n2\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 07:16:13\n2014-03-18 07:21:13\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 12:15:57\n2014-03-18 12:32:10\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n2.40\n0.00\n14.40\nTuesday\n\n\nCMT\n2014-03-18 10:57:48\n2014-03-18 11:07:45\n2\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 10:37:02\n2014-03-18 10:49:51\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 04:31:21\n2014-03-18 04:38:24\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.62\n0.00\n13.12\nTuesday\n\n\nCMT\n2014-03-18 04:19:28\n2014-03-18 04:23:27\n2\n1.9\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 09:12:22\n2014-03-18 09:21:33\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 09:49:07\n2014-03-18 10:03:38\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n2.50\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 07:56:22\n2014-03-18 08:09:26\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 04:03:53\n2014-03-18 04:18:59\n1\n4.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n16.5\n0.5\n0.5\n4.35\n0.00\n21.85\nTuesday\n\n\nCMT\n2014-03-18 07:29:14\n2014-03-18 07:33:52\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 14:51:50\n2014-03-18 14:59:49\n1\n0.9\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n7.0\n0.0\n0.5\n2.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 08:06:33\n2014-03-18 08:18:36\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n0.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 08:49:31\n2014-03-18 08:59:55\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 12:11:23\n2014-03-18 12:24:47\n1\n3.0\n-74.0\n40.7\n1\nY\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n2.40\n0.00\n14.40\nTuesday\n\n\nCMT\n2014-03-18 08:31:15\n2014-03-18 09:16:37\n1\n12.0\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n39.5\n0.0\n0.5\n9.05\n5.33\n54.38\nTuesday\n\n\nCMT\n2014-03-18 04:50:39\n2014-03-18 05:08:31\n1\n6.4\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n21.5\n0.5\n0.5\n2.00\n0.00\n24.50\nTuesday\n\n\nCMT\n2014-03-18 06:18:06\n2014-03-18 06:23:57\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.75\n0.00\n8.75\nTuesday\n\n\nCMT\n2014-03-18 09:56:12\n2014-03-18 10:10:48\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 08:14:58\n2014-03-18 08:23:15\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-17 20:17:16\n2014-03-17 21:25:17\n1\n8.4\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n25.5\n0.5\n0.5\n6.36\n5.33\n38.19\nMonday\n\n\nCMT\n2014-03-17 21:48:06\n2014-03-17 21:51:57\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.5\n0.5\n1.30\n0.00\n7.80\nMonday\n\n\nCMT\n2014-03-18 00:19:28\n2014-03-18 00:21:32\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.5\n0.5\n0.50\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-17 22:56:01\n2014-03-17 23:03:21\n1\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-17 18:07:01\n2014-03-17 18:12:00\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n1.0\n0.5\n1.00\n0.00\n8.00\nMonday\n\n\nCMT\n2014-03-17 20:54:17\n2014-03-17 21:07:06\n2\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-17 20:17:30\n2014-03-17 20:25:09\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 22:41:06\n2014-03-17 23:03:12\n1\n5.5\n-74.0\n40.8\n1\nN\n-73.9\n40.7\nCRD\n20.0\n0.5\n0.5\n6.30\n0.00\n27.30\nMonday\n\n\nCMT\n2014-03-17 20:24:32\n2014-03-17 20:35:26\n1\n2.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.5\n0.5\n2.40\n0.00\n14.40\nMonday\n\n\nCMT\n2014-03-17 22:47:05\n2014-03-17 22:54:03\n2\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n1.00\n0.00\n10.00\nMonday\n\n\nCMT\n2014-03-18 10:56:18\n2014-03-18 11:06:47\n1\n1.8\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 09:00:32\n2014-03-18 09:10:06\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 10:14:18\n2014-03-18 10:46:35\n1\n4.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n21.0\n0.0\n0.5\n2.50\n0.00\n24.00\nTuesday\n\n\nCMT\n2014-03-18 06:39:45\n2014-03-18 06:55:45\n1\n4.7\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n16.0\n0.0\n0.5\n1.00\n0.00\n17.50\nTuesday\n\n\nCMT\n2014-03-18 07:14:24\n2014-03-18 07:17:12\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 14:21:21\n2014-03-18 14:43:55\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.00\n0.00\n16.50\nTuesday\n\n\nCMT\n2014-03-18 08:32:04\n2014-03-18 08:51:53\n1\n5.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n19.0\n0.0\n0.5\n3.90\n0.00\n23.40\nTuesday\n\n\nCMT\n2014-03-18 10:06:24\n2014-03-18 10:42:33\n1\n17.0\n-74.0\n40.8\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n11.55\n5.33\n69.38\nTuesday\n\n\nCMT\n2014-03-18 07:46:04\n2014-03-18 07:53:25\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 06:52:21\n2014-03-18 06:58:37\n1\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 12:15:04\n2014-03-18 12:29:44\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 14:30:16\n2014-03-18 14:33:30\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 15:17:22\n2014-03-18 15:20:51\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 07:07:16\n2014-03-18 07:33:07\n1\n10.1\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n30.5\n0.0\n0.5\n10.85\n5.33\n47.18\nTuesday\n\n\nCMT\n2014-03-17 20:57:22\n2014-03-17 21:14:17\n1\n6.7\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n20.5\n0.5\n0.5\n8.00\n5.33\n34.83\nMonday\n\n\nCMT\n2014-03-18 01:12:13\n2014-03-18 01:19:14\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n1.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 00:06:11\n2014-03-18 00:09:23\n2\n0.6\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n2.00\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-17 20:59:18\n2014-03-17 21:12:39\n1\n2.8\n-74.0\n40.6\n1\nN\n-74.0\n40.6\nCRD\n12.5\n0.5\n0.5\n2.00\n0.00\n15.50\nMonday\n\n\nCMT\n2014-03-17 19:56:37\n2014-03-17 20:37:56\n1\n17.5\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n10.50\n0.00\n63.00\nMonday\n\n\nCMT\n2014-03-17 20:55:23\n2014-03-17 21:05:28\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 21:53:02\n2014-03-17 22:04:13\n3\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-17 23:07:31\n2014-03-17 23:19:57\n2\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-17 20:51:04\n2014-03-17 21:13:08\n1\n7.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n23.5\n0.5\n0.5\n6.12\n0.00\n30.62\nMonday\n\n\nCMT\n2014-03-17 20:19:37\n2014-03-17 20:44:32\n1\n5.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.5\n0.5\n4.50\n0.00\n27.00\nMonday\n\n\nCMT\n2014-03-17 19:45:45\n2014-03-17 19:49:53\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n1.0\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 21:56:21\n2014-03-17 22:01:39\n3\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-18 03:21:56\n2014-03-18 03:32:25\n1\n4.0\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n13.0\n0.5\n0.5\n1.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-17 21:49:25\n2014-03-17 21:59:37\n3\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-18 02:33:53\n2014-03-18 02:44:57\n2\n3.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.5\n0.5\n1.50\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-17 23:15:22\n2014-03-17 23:35:24\n1\n11.0\n-74.0\n40.8\n1\nN\n-73.8\n40.8\nCRD\n31.0\n0.5\n0.5\n7.46\n5.33\n44.79\nMonday\n\n\nCMT\n2014-03-17 17:11:17\n2014-03-17 17:23:05\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n1.0\n0.5\n2.40\n0.00\n14.40\nMonday\n\n\nCMT\n2014-03-17 21:48:50\n2014-03-17 21:52:24\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nMonday\n\n\nCMT\n2014-03-17 22:42:36\n2014-03-17 22:48:51\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 17:47:59\n2014-03-17 18:01:30\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n1.0\n0.5\n2.40\n0.00\n14.40\nMonday\n\n\nCMT\n2014-03-17 22:38:30\n2014-03-17 22:46:15\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 23:13:07\n2014-03-17 23:33:47\n1\n12.8\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n35.5\n0.5\n0.5\n9.10\n0.00\n45.60\nMonday\n\n\nCMT\n2014-03-17 19:46:45\n2014-03-17 19:53:32\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n2.25\n0.00\n11.25\nMonday\n\n\nCMT\n2014-03-17 20:20:05\n2014-03-17 20:33:28\n1\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.5\n0.5\n1.50\n0.00\n14.50\nMonday\n\n\nCMT\n2014-03-17 19:38:20\n2014-03-17 20:00:10\n3\n3.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.0\n1.0\n0.5\n5.25\n0.00\n22.75\nMonday\n\n\nCMT\n2014-03-17 22:51:04\n2014-03-17 22:59:06\n2\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.00\n0.00\n9.50\nMonday\n\n\nCMT\n2014-03-17 22:35:15\n2014-03-17 22:47:05\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.5\n0.5\n2.50\n0.00\n15.00\nMonday\n\n\nCMT\n2014-03-17 19:05:28\n2014-03-17 19:16:19\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n1.0\n0.5\n2.75\n0.00\n13.75\nMonday\n\n\nCMT\n2014-03-18 00:33:29\n2014-03-18 01:07:10\n3\n21.1\n-73.8\n40.6\n5\nN\n-74.0\n40.7\nCRD\n52.0\n0.0\n0.0\n3.00\n5.33\n60.33\nTuesday\n\n\nCMT\n2014-03-17 22:47:08\n2014-03-17 23:15:35\n1\n11.6\n-73.8\n40.6\n1\nN\n-73.9\n40.7\nCRD\n35.0\n0.5\n0.5\n7.20\n0.00\n43.20\nMonday\n\n\nCMT\n2014-03-17 22:47:29\n2014-03-17 23:07:14\n1\n11.2\n-74.0\n40.7\n1\nN\n-74.0\n40.6\nCRD\n31.5\n0.5\n0.5\n8.10\n0.00\n40.60\nMonday\n\n\nCMT\n2014-03-18 09:21:16\n2014-03-18 09:29:56\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 11:23:42\n2014-03-18 11:36:06\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.0\n0.5\n2.37\n0.00\n11.87\nTuesday\n\n\nCMT\n2014-03-18 00:16:12\n2014-03-18 00:36:18\n1\n12.7\n-74.0\n40.8\n1\nN\n-73.8\n40.7\nCRD\n36.0\n0.5\n0.5\n5.67\n5.33\n48.00\nTuesday\n\n\nCMT\n2014-03-18 01:10:57\n2014-03-18 01:16:56\n1\n1.8\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n8.0\n0.5\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 00:28:45\n2014-03-18 00:40:04\n1\n6.6\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.5\n0.5\n0.5\n4.10\n0.00\n24.60\nTuesday\n\n\nCMT\n2014-03-17 22:59:27\n2014-03-17 23:13:28\n1\n3.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n0.00\n0.00\n14.00\nMonday\n\n\nCMT\n2014-03-17 23:19:36\n2014-03-17 23:34:02\n1\n4.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n15.0\n0.5\n0.5\n3.20\n0.00\n19.20\nMonday\n\n\nCMT\n2014-03-18 01:02:29\n2014-03-18 01:20:36\n1\n7.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n24.0\n0.5\n0.5\n5.00\n0.00\n30.00\nTuesday\n\n\nCMT\n2014-03-18 00:06:52\n2014-03-18 00:13:24\n2\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-17 23:48:02\n2014-03-17 23:50:35\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n1.37\n0.00\n6.87\nMonday\n\n\nCMT\n2014-03-17 23:08:57\n2014-03-17 23:19:16\n3\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-18 00:15:42\n2014-03-18 00:18:51\n4\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-17 23:44:11\n2014-03-18 00:00:40\n1\n7.2\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n22.0\n0.5\n0.5\n4.60\n0.00\n27.60\nMonday\n\n\nCMT\n2014-03-18 01:41:33\n2014-03-18 01:49:10\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.5\n0.5\n2.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 01:48:14\n2014-03-18 01:53:32\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-17 23:59:54\n2014-03-18 00:26:34\n1\n17.1\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n5.55\n0.00\n58.05\nMonday\n\n\nCMT\n2014-03-18 00:02:38\n2014-03-18 00:18:23\n1\n5.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n17.5\n0.5\n0.5\n4.60\n0.00\n23.10\nTuesday\n\n\nCMT\n2014-03-18 00:46:19\n2014-03-18 01:01:26\n2\n3.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.5\n0.5\n0.5\n3.10\n0.00\n18.60\nTuesday\n\n\nCMT\n2014-03-18 08:27:29\n2014-03-18 08:49:33\n1\n4.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.0\n0.0\n0.5\n3.70\n0.00\n22.20\nTuesday\n\n\nCMT\n2014-03-18 08:16:36\n2014-03-18 08:28:26\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 06:26:05\n2014-03-18 06:31:38\n1\n0.7\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 14:29:18\n2014-03-18 14:40:21\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 07:28:35\n2014-03-18 07:34:50\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 14:31:12\n2014-03-18 14:45:09\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 10:56:50\n2014-03-18 11:09:37\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n1.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 07:27:29\n2014-03-18 07:30:15\n3\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.0\n0.5\n1.12\n0.00\n5.62\nTuesday\n\n\nCMT\n2014-03-18 13:15:55\n2014-03-18 13:23:21\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n2.25\n0.00\n9.75\nTuesday\n\n\nCMT\n2014-03-18 11:41:01\n2014-03-18 11:49:17\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n0.50\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 15:01:42\n2014-03-18 15:17:38\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n3.00\n0.00\n16.00\nTuesday\n\n\nCMT\n2014-03-18 08:49:22\n2014-03-18 09:07:14\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 07:33:53\n2014-03-18 07:41:21\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 10:19:35\n2014-03-18 10:33:18\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 13:29:35\n2014-03-18 13:54:02\n1\n3.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.5\n0.0\n0.5\n1.50\n0.00\n18.50\nTuesday\n\n\nCMT\n2014-03-18 14:04:06\n2014-03-18 14:11:39\n3\n0.9\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 14:31:58\n2014-03-18 14:52:19\n3\n2.5\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n14.0\n0.0\n0.5\n2.90\n0.00\n17.40\nTuesday\n\n\nCMT\n2014-03-18 09:36:23\n2014-03-18 09:48:59\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.25\n0.00\n11.25\nTuesday\n\n\nCMT\n2014-03-18 07:48:11\n2014-03-18 08:00:11\n1\n3.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n2.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 11:56:57\n2014-03-18 12:12:47\n1\n2.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 15:19:02\n2014-03-18 15:29:30\n1\n2.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.87\n0.00\n14.37\nTuesday\n\n\nCMT\n2014-03-18 12:57:39\n2014-03-18 13:00:54\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 09:09:29\n2014-03-18 09:24:21\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n1.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 08:35:30\n2014-03-18 08:43:24\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n2.10\n0.00\n9.10\nTuesday\n\n\nCMT\n2014-03-18 08:25:44\n2014-03-18 08:31:54\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 11:09:39\n2014-03-18 11:27:36\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.0\n0.5\n3.37\n0.00\n16.87\nTuesday\n\n\nCMT\n2014-03-18 07:24:45\n2014-03-18 07:34:55\n1\n1.8\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 09:54:18\n2014-03-18 10:09:01\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n1.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 23:00:40\n2014-03-17 23:09:42\n3\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.90\n0.00\n11.40\nMonday\n\n\nCMT\n2014-03-17 19:57:00\n2014-03-17 20:10:37\n1\n8.2\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n23.0\n1.0\n0.5\n5.96\n5.33\n35.79\nMonday\n\n\nCMT\n2014-03-18 01:30:15\n2014-03-18 01:48:28\n2\n8.0\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n24.5\n0.5\n0.5\n5.00\n0.00\n30.50\nTuesday\n\n\nCMT\n2014-03-18 01:58:49\n2014-03-18 02:06:46\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-17 22:00:41\n2014-03-17 22:08:48\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 20:07:45\n2014-03-17 20:18:30\n1\n1.0\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n8.0\n0.5\n0.5\n1.80\n0.00\n10.80\nMonday\n\n\nCMT\n2014-03-17 22:40:40\n2014-03-17 22:49:00\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.5\n0.5\n1.90\n0.00\n11.40\nMonday\n\n\nCMT\n2014-03-17 22:59:10\n2014-03-17 23:10:08\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.50\n0.00\n11.00\nMonday\n\n\nCMT\n2014-03-18 01:22:56\n2014-03-18 01:29:08\n2\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.75\n0.00\n8.75\nTuesday\n\n\nCMT\n2014-03-17 19:15:47\n2014-03-17 19:24:56\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n1.0\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 19:37:17\n2014-03-17 19:45:27\n1\n1.9\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n8.0\n1.0\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 20:24:48\n2014-03-17 20:33:53\n1\n2.9\n-74.0\n40.8\n1\nY\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n3.36\n5.33\n20.19\nMonday\n\n\nCMT\n2014-03-17 21:49:44\n2014-03-17 21:51:54\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.5\n0.5\n1.00\n0.00\n7.00\nMonday\n\n\nCMT\n2014-03-17 19:58:33\n2014-03-17 20:08:00\n1\n1.5\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 20:34:32\n2014-03-17 20:49:19\n1\n8.2\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n23.5\n0.5\n0.5\n2.00\n0.00\n26.50\nMonday\n\n\nCMT\n2014-03-17 21:04:30\n2014-03-17 21:05:09\n2\n0.0\n0.0\n0.0\n5\nN\n0.0\n0.0\nCRD\n75.0\n0.0\n0.0\n5.00\n0.00\n80.00\nMonday\n\n\nCMT\n2014-03-17 22:43:38\n2014-03-17 23:06:41\n2\n9.9\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n29.5\n0.5\n0.5\n7.16\n5.33\n42.99\nMonday\n\n\nCMT\n2014-03-17 21:16:39\n2014-03-17 21:38:46\n1\n11.4\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n32.5\n0.5\n0.5\n7.75\n5.33\n46.58\nMonday\n\n\nCMT\n2014-03-17 21:05:44\n2014-03-17 21:19:19\n1\n5.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.5\n0.5\n0.5\n5.85\n0.00\n25.35\nMonday\n\n\nCMT\n2014-03-17 19:44:36\n2014-03-17 19:59:18\n1\n3.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n14.0\n1.0\n0.5\n3.10\n0.00\n18.60\nMonday\n\n\nCMT\n2014-03-17 20:12:53\n2014-03-17 20:20:11\n1\n3.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.5\n0.5\n1.50\n0.00\n13.50\nMonday\n\n\nCMT\n2014-03-17 23:38:33\n2014-03-17 23:52:10\n1\n4.2\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n15.0\n0.5\n0.5\n2.00\n0.00\n18.00\nMonday\n\n\nCMT\n2014-03-17 21:30:41\n2014-03-17 21:32:26\n1\n0.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.5\n0.5\n1.00\n0.00\n5.50\nMonday\n\n\nCMT\n2014-03-18 14:39:36\n2014-03-18 15:03:04\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n15.0\n0.0\n0.5\n3.85\n0.00\n19.35\nTuesday\n\n\nCMT\n2014-03-18 12:25:21\n2014-03-18 12:42:19\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n3.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 14:53:32\n2014-03-18 15:00:17\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 06:37:31\n2014-03-18 06:45:34\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n2.35\n0.00\n11.85\nTuesday\n\n\nCMT\n2014-03-18 07:15:48\n2014-03-18 07:20:59\n1\n0.8\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n0.70\n0.00\n6.70\nTuesday\n\n\nCMT\n2014-03-18 12:47:54\n2014-03-18 12:56:01\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 12:41:56\n2014-03-18 12:49:17\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 07:21:41\n2014-03-18 07:26:45\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 12:57:42\n2014-03-18 13:17:10\n1\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n17.5\n0.0\n0.5\n3.60\n0.00\n21.60\nTuesday\n\n\nCMT\n2014-03-18 15:54:48\n2014-03-18 16:08:47\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 09:30:54\n2014-03-18 10:21:57\n1\n8.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n36.5\n0.0\n0.5\n4.00\n0.00\n41.00\nTuesday\n\n\nCMT\n2014-03-17 21:01:51\n2014-03-17 21:31:01\n1\n18.7\n-73.8\n40.6\n2\nN\n-74.0\n40.7\nCRD\n52.0\n0.0\n0.5\n10.50\n0.00\n63.00\nMonday\n\n\nCMT\n2014-03-17 21:12:15\n2014-03-17 21:29:08\n1\n9.7\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n27.5\n0.5\n0.5\n8.55\n0.00\n37.05\nMonday\n\n\nCMT\n2014-03-17 17:45:22\n2014-03-17 18:26:40\n5\n17.6\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n11.56\n5.33\n69.39\nMonday\n\n\nCMT\n2014-03-17 19:58:49\n2014-03-17 20:05:10\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 19:13:18\n2014-03-17 19:17:46\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n1.0\n0.5\n1.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 23:11:53\n2014-03-17 23:22:01\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.50\n0.00\n13.00\nMonday\n\n\nCMT\n2014-03-17 19:26:16\n2014-03-17 19:43:36\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.0\n1.0\n0.5\n2.00\n0.00\n16.50\nMonday\n\n\nCMT\n2014-03-17 20:08:49\n2014-03-17 20:20:32\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n1.0\n0.5\n2.75\n0.00\n13.75\nMonday\n\n\nCMT\n2014-03-17 23:37:30\n2014-03-17 23:48:29\n1\n3.7\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n13.0\n0.5\n0.5\n3.50\n0.00\n17.50\nMonday\n\n\nCMT\n2014-03-17 20:13:47\n2014-03-17 20:14:10\n1\n0.0\n-74.0\n40.7\n2\nN\n-74.0\n40.7\nCRD\n52.0\n0.0\n0.5\n11.56\n5.33\n69.39\nMonday\n\n\nCMT\n2014-03-17 23:53:11\n2014-03-18 00:04:36\n1\n4.4\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n15.0\n0.5\n0.5\n3.50\n0.00\n19.50\nMonday\n\n\nCMT\n2014-03-17 19:59:09\n2014-03-17 20:05:46\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.00\n0.00\n8.50\nMonday\n\n\nCMT\n2014-03-18 11:38:48\n2014-03-18 11:57:33\n1\n3.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.0\n0.5\n3.10\n0.00\n18.60\nTuesday\n\n\nCMT\n2014-03-18 07:11:00\n2014-03-18 07:29:27\n1\n4.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n17.0\n0.0\n0.5\n3.50\n0.00\n21.00\nTuesday\n\n\nCMT\n2014-03-18 12:36:36\n2014-03-18 12:58:30\n1\n9.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n29.0\n0.0\n0.5\n5.00\n0.00\n34.50\nTuesday\n\n\nCMT\n2014-03-18 07:22:36\n2014-03-18 07:31:14\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.25\n0.00\n11.25\nTuesday\n\n\nCMT\n2014-03-18 07:35:31\n2014-03-18 07:43:29\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.50\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-17 22:13:16\n2014-03-17 22:32:19\n1\n4.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.5\n0.5\n0.5\n3.50\n0.00\n21.00\nMonday\n\n\nCMT\n2014-03-17 19:31:03\n2014-03-17 19:42:22\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n1.0\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-17 21:48:12\n2014-03-17 22:01:23\n1\n3.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.5\n0.5\n1.50\n0.00\n15.00\nMonday\n\n\nCMT\n2014-03-17 23:43:41\n2014-03-17 23:52:12\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.5\n0.5\n2.37\n0.00\n11.87\nMonday\n\n\nCMT\n2014-03-17 22:17:20\n2014-03-17 22:32:29\n2\n5.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.0\n0.5\n0.5\n3.80\n0.00\n22.80\nMonday\n\n\nCMT\n2014-03-17 18:17:00\n2014-03-17 18:29:28\n1\n3.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n1.0\n0.5\n2.70\n0.00\n16.20\nMonday\n\n\nCMT\n2014-03-17 18:51:36\n2014-03-17 18:55:41\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n1.0\n0.5\n1.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-18 00:45:43\n2014-03-18 00:52:29\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.50\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-17 21:01:19\n2014-03-17 21:26:17\n1\n12.4\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n36.0\n0.5\n0.5\n7.40\n0.00\n44.40\nMonday\n\n\nCMT\n2014-03-17 09:56:35\n2014-03-17 10:07:38\n1\n0.4\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nMonday\n\n\nCMT\n2014-03-17 18:00:50\n2014-03-17 18:09:54\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n1.0\n0.5\n2.00\n0.00\n13.00\nMonday\n\n\nCMT\n2014-03-17 18:05:16\n2014-03-17 18:08:51\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n1.0\n0.5\n1.60\n0.00\n8.10\nMonday\n\n\nCMT\n2014-03-17 18:28:09\n2014-03-17 18:43:39\n1\n2.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n1.0\n0.5\n1.00\n0.00\n14.50\nMonday\n\n\nCMT\n2014-03-17 19:04:18\n2014-03-17 19:26:38\n1\n3.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.0\n1.0\n0.5\n3.50\n0.00\n21.00\nMonday\n\n\nCMT\n2014-03-18 03:02:45\n2014-03-18 03:16:09\n1\n2.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n12.0\n0.5\n0.5\n3.00\n0.00\n16.00\nTuesday\n\n\nCMT\n2014-03-17 22:47:27\n2014-03-17 22:53:49\n2\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-18 00:31:17\n2014-03-18 00:34:58\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 00:47:52\n2014-03-18 01:12:58\n1\n6.7\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n24.0\n0.5\n0.5\n5.00\n0.00\n30.00\nTuesday\n\n\nCMT\n2014-03-17 21:29:40\n2014-03-17 21:39:58\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 20:41:25\n2014-03-17 20:56:45\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n14.0\n0.5\n0.5\n3.00\n0.00\n18.00\nMonday\n\n\nCMT\n2014-03-18 02:46:16\n2014-03-18 02:50:57\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 00:04:10\n2014-03-18 00:28:45\n1\n7.4\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n24.5\n0.5\n0.5\n2.50\n0.00\n28.00\nTuesday\n\n\nCMT\n2014-03-17 20:16:26\n2014-03-17 20:25:10\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 22:40:07\n2014-03-17 22:53:34\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n5.00\n0.00\n16.00\nMonday\n\n\nCMT\n2014-03-18 08:44:13\n2014-03-18 08:47:57\n1\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.0\n0.5\n0.50\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 09:15:43\n2014-03-18 09:29:53\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n1.80\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 13:13:58\n2014-03-18 13:19:15\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 14:46:05\n2014-03-18 15:00:13\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n0.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 11:02:53\n2014-03-18 11:17:12\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.87\n0.00\n14.37\nTuesday\n\n\nCMT\n2014-03-18 13:19:20\n2014-03-18 13:40:16\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n3.50\n0.00\n17.50\nTuesday\n\n\nCMT\n2014-03-18 06:47:52\n2014-03-18 06:59:12\n1\n4.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.5\n0.0\n0.5\n1.00\n0.00\n17.00\nTuesday\n\n\nCMT\n2014-03-18 06:55:18\n2014-03-18 06:58:28\n1\n0.8\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 16:07:05\n2014-03-18 16:22:39\n1\n4.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n1.0\n0.5\n3.40\n0.00\n20.40\nTuesday\n\n\nCMT\n2014-03-18 11:16:09\n2014-03-18 11:30:17\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 06:18:24\n2014-03-18 06:34:20\n1\n4.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n17.0\n0.0\n0.5\n2.00\n0.00\n19.50\nTuesday\n\n\nCMT\n2014-03-18 03:20:39\n2014-03-18 03:36:00\n2\n6.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n20.0\n0.5\n0.5\n1.00\n0.00\n22.00\nTuesday\n\n\nCMT\n2014-03-17 19:16:23\n2014-03-17 19:26:16\n2\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n2.50\n0.00\n12.50\nMonday\n\n\nCMT\n2014-03-17 23:53:43\n2014-03-18 00:00:53\n4\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.5\n0.5\n2.40\n0.00\n10.40\nMonday\n\n\nCMT\n2014-03-17 21:02:34\n2014-03-17 21:08:58\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.5\n0.5\n1.00\n0.00\n8.50\nMonday\n\n\nCMT\n2014-03-17 19:48:07\n2014-03-17 20:05:03\n1\n4.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.0\n1.0\n0.5\n3.50\n0.00\n21.00\nMonday\n\n\nCMT\n2014-03-17 21:03:17\n2014-03-17 21:09:33\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.00\n0.00\n8.00\nMonday\n\n\nCMT\n2014-03-17 20:00:33\n2014-03-17 20:16:54\n1\n3.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.5\n0.5\n3.10\n0.00\n18.60\nMonday\n\n\nCMT\n2014-03-17 19:10:22\n2014-03-17 19:23:39\n2\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.0\n1.0\n0.5\n3.12\n0.00\n15.62\nMonday\n\n\nCMT\n2014-03-17 19:00:27\n2014-03-17 19:13:56\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.0\n1.0\n0.5\n1.50\n0.00\n15.00\nMonday\n\n\nCMT\n2014-03-17 19:09:30\n2014-03-17 19:30:03\n1\n8.1\n-74.0\n40.8\n1\nN\n-73.9\n40.7\nCRD\n25.5\n1.0\n0.5\n6.75\n0.00\n33.75\nMonday\n\n\nCMT\n2014-03-18 02:10:33\n2014-03-18 02:24:41\n2\n4.3\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n15.0\n0.5\n0.5\n2.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-17 22:45:11\n2014-03-17 22:59:07\n1\n1.9\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n11.5\n0.5\n0.5\n0.00\n0.00\n12.50\nMonday\n\n\nCMT\n2014-03-17 21:52:46\n2014-03-17 21:59:06\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nMonday\n\n\nCMT\n2014-03-17 18:03:05\n2014-03-17 18:27:23\n2\n6.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n24.5\n1.0\n0.5\n6.00\n0.00\n32.00\nMonday\n\n\nCMT\n2014-03-18 00:21:46\n2014-03-18 00:26:22\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-17 21:53:00\n2014-03-17 21:57:12\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.5\n0.5\n1.00\n0.00\n7.00\nMonday\n\n\nCMT\n2014-03-17 18:17:33\n2014-03-17 18:27:21\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n1.0\n0.5\n1.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 19:01:30\n2014-03-17 19:06:30\n3\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n1.0\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 18:02:46\n2014-03-17 18:05:22\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n1.0\n0.5\n1.20\n0.00\n7.20\nMonday\n\n\nCMT\n2014-03-17 23:55:55\n2014-03-18 00:04:34\n2\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n0.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-18 01:17:02\n2014-03-18 01:26:57\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.5\n0.5\n1.00\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-17 21:47:27\n2014-03-17 21:55:25\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nMonday\n\n\nCMT\n2014-03-17 18:30:38\n2014-03-17 18:37:54\n1\n1.4\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n7.5\n1.0\n0.5\n2.25\n0.00\n11.25\nMonday\n\n\nCMT\n2014-03-18 09:45:23\n2014-03-18 09:50:31\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 08:36:08\n2014-03-18 09:21:19\n1\n14.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n46.0\n0.0\n0.5\n5.00\n5.33\n56.83\nTuesday\n\n\nCMT\n2014-03-18 08:10:05\n2014-03-18 08:19:01\n2\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n2.70\n0.00\n11.70\nTuesday\n\n\nCMT\n2014-03-18 08:45:13\n2014-03-18 09:07:29\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-18 05:56:09\n2014-03-18 05:59:54\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 10:43:23\n2014-03-18 10:49:55\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 07:29:07\n2014-03-18 07:38:20\n1\n1.9\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n3.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 17:26:19\n2014-03-17 17:36:57\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n1.0\n0.5\n2.00\n0.00\n13.50\nMonday\n\n\nCMT\n2014-03-18 01:00:33\n2014-03-18 01:07:21\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.5\n0.5\n1.87\n0.00\n9.37\nTuesday\n\n\nCMT\n2014-03-17 21:46:47\n2014-03-17 21:50:33\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n2.00\n0.00\n7.50\nMonday\n\n\nCMT\n2014-03-17 17:45:20\n2014-03-17 17:47:58\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n1.0\n0.5\n1.10\n0.00\n6.60\nMonday\n\n\nCMT\n2014-03-17 23:39:57\n2014-03-17 23:49:00\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-18 12:50:44\n2014-03-18 13:15:21\n1\n6.4\n-74.0\n40.8\n1\nN\n-73.9\n40.9\nCRD\n24.0\n0.0\n0.5\n4.90\n0.00\n29.40\nTuesday\n\n\nCMT\n2014-03-18 14:07:08\n2014-03-18 14:22:52\n1\n2.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n13.0\n0.0\n0.5\n1.95\n0.00\n15.45\nTuesday\n\n\nCMT\n2014-03-18 08:25:14\n2014-03-18 08:33:59\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 09:39:30\n2014-03-18 10:01:39\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.90\n0.00\n17.40\nTuesday\n\n\nCMT\n2014-03-18 15:35:34\n2014-03-18 15:46:06\n3\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n3.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 08:59:37\n2014-03-18 09:26:15\n1\n6.8\n-74.0\n40.8\n1\nY\n-74.0\n40.7\nCRD\n25.5\n0.0\n0.5\n5.20\n0.00\n31.20\nTuesday\n\n\nCMT\n2014-03-18 07:51:51\n2014-03-18 08:03:25\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 07:56:58\n2014-03-18 08:13:38\n1\n7.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n23.5\n0.0\n0.5\n2.50\n0.00\n26.50\nTuesday\n\n\nCMT\n2014-03-18 11:10:07\n2014-03-18 11:21:17\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 07:54:29\n2014-03-18 08:00:50\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 06:50:11\n2014-03-18 07:01:40\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.75\n0.00\n13.75\nTuesday\n\n\nCMT\n2014-03-18 12:06:13\n2014-03-18 12:22:22\n1\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.0\n0.5\n2.50\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 14:37:40\n2014-03-18 14:42:03\n1\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.65\n0.00\n7.15\nTuesday\n\n\nCMT\n2014-03-18 14:53:05\n2014-03-18 15:15:24\n1\n3.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.0\n0.0\n0.5\n1.00\n0.00\n17.50\nTuesday\n\n\nCMT\n2014-03-18 10:30:46\n2014-03-18 10:48:02\n1\n7.2\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n23.5\n0.0\n0.5\n1.00\n0.00\n25.00\nTuesday\n\n\nCMT\n2014-03-18 07:52:26\n2014-03-18 07:59:27\n1\n1.1\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 07:35:13\n2014-03-18 07:42:21\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 08:31:37\n2014-03-18 08:45:22\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 09:51:42\n2014-03-18 09:59:45\n2\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.50\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 15:06:53\n2014-03-18 15:19:07\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 05:20:36\n2014-03-18 05:31:44\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.5\n0.5\n1.00\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 07:57:10\n2014-03-18 08:02:02\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 09:12:19\n2014-03-18 09:21:31\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 09:32:59\n2014-03-18 10:15:47\n1\n5.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n28.0\n0.0\n0.5\n5.70\n0.00\n34.20\nTuesday\n\n\nCMT\n2014-03-18 11:26:26\n2014-03-18 11:41:57\n2\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 12:21:19\n2014-03-18 12:45:57\n2\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n2.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-18 05:40:01\n2014-03-18 05:46:54\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 11:00:17\n2014-03-18 11:02:37\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n0.50\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 08:30:26\n2014-03-18 08:45:23\n1\n3.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.0\n0.5\n3.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-18 14:34:45\n2014-03-18 14:46:07\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.0\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 15:31:17\n2014-03-18 15:47:18\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n3.00\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 08:29:49\n2014-03-18 08:49:19\n1\n2.6\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.5\n0.0\n0.5\n2.00\n0.00\n17.00\nTuesday\n\n\nCMT\n2014-03-18 06:37:43\n2014-03-18 06:52:06\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n2.50\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 16:34:56\n2014-03-18 16:41:18\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n1.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 09:32:20\n2014-03-18 09:39:26\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 07:03:58\n2014-03-18 07:09:35\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 08:13:03\n2014-03-18 08:22:09\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 09:25:04\n2014-03-18 09:37:42\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.0\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 08:26:05\n2014-03-18 08:40:45\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n3.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 06:50:22\n2014-03-18 06:54:34\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.37\n0.00\n6.87\nTuesday\n\n\nCMT\n2014-03-18 06:57:20\n2014-03-18 07:05:17\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 07:57:52\n2014-03-18 08:00:32\n1\n0.7\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 08:44:04\n2014-03-18 08:48:17\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 06:17:57\n2014-03-18 06:36:41\n1\n10.3\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n29.5\n0.0\n0.5\n7.06\n5.33\n42.39\nTuesday\n\n\nCMT\n2014-03-18 13:13:24\n2014-03-18 13:22:55\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 10:55:06\n2014-03-18 11:03:42\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n2.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 13:02:30\n2014-03-18 13:08:20\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n0.11\n0.00\n6.11\nTuesday\n\n\nCMT\n2014-03-18 08:11:49\n2014-03-18 08:26:48\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n3.75\n0.00\n16.25\nTuesday\n\n\nCMT\n2014-03-18 12:28:28\n2014-03-18 12:47:53\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n2.70\n0.00\n16.20\nTuesday\n\n\nCMT\n2014-03-18 08:59:29\n2014-03-18 09:04:57\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 07:06:46\n2014-03-18 07:08:27\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n1.00\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 09:34:12\n2014-03-18 09:39:22\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 06:38:49\n2014-03-18 06:45:11\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 15:48:38\n2014-03-18 15:58:43\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 06:43:27\n2014-03-18 06:46:14\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.12\n0.00\n5.62\nTuesday\n\n\nCMT\n2014-03-18 11:52:54\n2014-03-18 12:00:45\n1\n0.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 08:58:41\n2014-03-18 09:14:02\n1\n2.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n4.05\n0.00\n17.55\nTuesday\n\n\nCMT\n2014-03-18 07:00:27\n2014-03-18 07:06:56\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 16:37:00\n2014-03-18 16:41:24\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n1.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 08:15:35\n2014-03-18 08:33:49\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n1.00\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-18 13:35:31\n2014-03-18 13:41:46\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n2.20\n0.00\n9.20\nTuesday\n\n\nCMT\n2014-03-18 06:54:14\n2014-03-18 06:57:42\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n5.50\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 10:10:54\n2014-03-18 10:29:32\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.90\n0.00\n17.40\nTuesday\n\n\nCMT\n2014-03-18 08:51:55\n2014-03-18 08:55:48\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n2.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 08:31:03\n2014-03-18 08:40:56\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 06:47:00\n2014-03-18 06:51:37\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.00\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 06:23:09\n2014-03-18 06:34:22\n1\n3.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.0\n0.5\n2.00\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-18 10:41:30\n2014-03-18 10:51:59\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 06:35:59\n2014-03-18 06:44:47\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 14:14:32\n2014-03-18 14:27:23\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.25\n0.00\n11.75\nTuesday\n\n\nCMT\n2014-03-18 11:42:31\n2014-03-18 11:56:57\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 12:02:36\n2014-03-18 12:07:37\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-17 20:27:14\n2014-03-17 20:40:30\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.5\n0.5\n1.50\n0.00\n15.50\nMonday\n\n\nCMT\n2014-03-17 19:48:23\n2014-03-17 19:54:59\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n1.0\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 19:37:53\n2014-03-17 19:46:34\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n2.50\n0.00\n12.50\nMonday\n\n\nCMT\n2014-03-17 21:03:49\n2014-03-17 21:27:09\n1\n7.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n25.0\n0.5\n0.5\n5.20\n0.00\n31.20\nMonday\n\n\nCMT\n2014-03-17 20:55:36\n2014-03-17 21:00:31\n3\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nMonday\n\n\nCMT\n2014-03-18 13:29:58\n2014-03-18 13:49:57\n2\n4.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n19.0\n0.0\n0.5\n2.00\n0.00\n21.50\nTuesday\n\n\nCMT\n2014-03-18 16:45:25\n2014-03-18 16:50:42\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n1.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 14:26:20\n2014-03-18 14:30:02\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 13:24:27\n2014-03-18 13:31:16\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n2.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 06:52:58\n2014-03-18 07:02:41\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.25\n0.00\n11.25\nTuesday\n\n\nCMT\n2014-03-18 07:46:36\n2014-03-18 08:30:08\n1\n2.4\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n1.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 11:39:51\n2014-03-18 12:03:56\n1\n2.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n16.5\n0.0\n0.5\n3.40\n0.00\n20.40\nTuesday\n\n\nCMT\n2014-03-18 12:31:11\n2014-03-18 12:41:38\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 06:40:29\n2014-03-18 06:50:17\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n0.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 09:04:49\n2014-03-18 09:09:59\n3\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 11:37:42\n2014-03-18 12:01:18\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-18 11:20:06\n2014-03-18 11:34:15\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 06:43:28\n2014-03-18 06:50:46\n1\n1.9\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 14:27:00\n2014-03-18 14:39:49\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n3.15\n0.00\n13.65\nTuesday\n\n\nCMT\n2014-03-18 13:15:51\n2014-03-18 13:21:52\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 05:34:49\n2014-03-18 05:49:06\n1\n4.4\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.5\n0.5\n4.12\n0.00\n20.62\nTuesday\n\n\nCMT\n2014-03-18 08:54:39\n2014-03-18 08:59:12\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 17:31:57\n2014-03-18 17:46:15\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n1.0\n0.5\n3.45\n0.00\n14.95\nTuesday\n\n\nCMT\n2014-03-18 10:48:12\n2014-03-18 10:58:12\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n2.12\n0.00\n10.62\nTuesday\n\n\nCMT\n2014-03-18 13:39:36\n2014-03-18 13:42:27\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 12:06:56\n2014-03-18 12:17:28\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.50\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 10:00:26\n2014-03-18 10:13:59\n1\n7.6\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n22.5\n0.0\n0.5\n5.66\n5.33\n33.99\nTuesday\n\n\nCMT\n2014-03-18 09:50:19\n2014-03-18 10:00:42\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 07:52:06\n2014-03-18 07:58:58\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.60\n0.00\n8.10\nTuesday\n\n\nCMT\n2014-03-18 15:01:05\n2014-03-18 15:08:41\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 15:48:14\n2014-03-18 15:55:56\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 06:50:21\n2014-03-18 06:56:11\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 05:46:46\n2014-03-18 05:52:18\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 05:44:28\n2014-03-18 05:46:57\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 15:15:35\n2014-03-18 15:23:53\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.50\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 08:47:12\n2014-03-18 08:50:45\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 12:46:52\n2014-03-18 13:31:10\n1\n13.7\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n44.5\n0.0\n0.5\n0.00\n0.00\n45.00\nTuesday\n\n\nCMT\n2014-03-18 08:54:49\n2014-03-18 09:19:15\n2\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n16.5\n0.0\n0.5\n4.25\n0.00\n21.25\nTuesday\n\n\nCMT\n2014-03-18 14:32:20\n2014-03-18 14:44:46\n3\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 14:07:02\n2014-03-18 14:25:55\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n3.00\n0.00\n17.00\nTuesday\n\n\nCMT\n2014-03-18 07:05:23\n2014-03-18 07:28:52\n1\n4.9\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n19.0\n0.0\n0.5\n1.00\n0.00\n20.50\nTuesday\n\n\nCMT\n2014-03-18 08:23:18\n2014-03-18 08:33:28\n1\n2.2\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.50\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 10:35:18\n2014-03-18 10:40:50\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.95\n0.00\n8.45\nTuesday\n\n\nCMT\n2014-03-18 11:34:41\n2014-03-18 11:48:31\n1\n1.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n1.50\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 06:14:10\n2014-03-18 06:22:43\n1\n2.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 07:29:18\n2014-03-18 07:36:38\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 06:25:56\n2014-03-18 06:48:50\n1\n5.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n19.0\n0.0\n0.5\n5.85\n0.00\n25.35\nTuesday\n\n\nCMT\n2014-03-18 08:41:53\n2014-03-18 08:54:51\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 07:34:31\n2014-03-18 07:44:46\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n0.95\n0.00\n10.45\nTuesday\n\n\nCMT\n2014-03-18 07:49:13\n2014-03-18 08:02:24\n1\n2.8\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n3.75\n0.00\n16.25\nTuesday\n\n\nCMT\n2014-03-18 08:14:09\n2014-03-18 08:18:48\n3\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.62\n0.00\n8.12\nTuesday\n\n\nCMT\n2014-03-18 12:32:01\n2014-03-18 12:44:31\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 09:57:10\n2014-03-18 10:55:40\n1\n20.7\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n11.56\n5.33\n69.39\nTuesday\n\n\nCMT\n2014-03-18 06:11:12\n2014-03-18 06:42:13\n1\n17.4\n-74.0\n40.7\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n6.00\n0.00\n58.50\nTuesday\n\n\nCMT\n2014-03-18 10:26:07\n2014-03-18 10:33:44\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 13:24:10\n2014-03-18 13:37:44\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 09:16:39\n2014-03-18 09:32:10\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n2.40\n0.00\n14.40\nTuesday\n\n\nCMT\n2014-03-18 06:45:39\n2014-03-18 06:51:07\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 10:38:10\n2014-03-18 10:47:34\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 11:55:31\n2014-03-18 12:17:07\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.0\n0.5\n4.50\n0.00\n19.50\nTuesday\n\n\nCMT\n2014-03-18 15:03:11\n2014-03-18 15:06:12\n1\n1.1\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 07:49:47\n2014-03-18 07:54:50\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 08:33:43\n2014-03-18 08:54:48\n2\n5.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n19.5\n0.0\n0.5\n5.00\n0.00\n25.00\nTuesday\n\n\nCMT\n2014-03-18 12:19:30\n2014-03-18 12:24:55\n2\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 07:49:04\n2014-03-18 07:55:55\n2\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 08:37:21\n2014-03-18 08:50:18\n2\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n1.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 09:10:53\n2014-03-18 09:22:25\n1\n0.9\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n8.5\n0.0\n0.5\n2.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 06:55:25\n2014-03-18 07:13:09\n1\n5.5\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n19.5\n0.0\n0.5\n6.33\n5.33\n31.66\nTuesday\n\n\nCMT\n2014-03-18 16:31:29\n2014-03-18 16:39:09\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n1.0\n0.5\n2.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 17:44:20\n2014-03-18 17:57:30\n1\n3.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n1.0\n0.5\n2.00\n0.00\n14.50\nTuesday\n\n\nCMT\n2014-03-18 10:36:25\n2014-03-18 10:43:28\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 10:55:00\n2014-03-18 11:07:47\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 16:11:36\n2014-03-18 16:23:38\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n1.0\n0.5\n3.45\n0.00\n14.95\nTuesday\n\n\nCMT\n2014-03-18 08:04:38\n2014-03-18 08:13:39\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 16:13:49\n2014-03-18 16:24:55\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n3.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 11:07:08\n2014-03-18 11:26:40\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n3.37\n0.00\n16.87\nTuesday\n\n\nCMT\n2014-03-18 07:56:33\n2014-03-18 08:00:47\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n0.50\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 06:43:12\n2014-03-18 06:45:08\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.12\n0.00\n5.62\nTuesday\n\n\nCMT\n2014-03-18 10:53:20\n2014-03-18 11:05:43\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n1.50\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 14:49:37\n2014-03-18 14:57:02\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 13:39:17\n2014-03-18 13:50:24\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 07:46:08\n2014-03-18 07:48:54\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 06:43:14\n2014-03-18 06:58:32\n1\n8.8\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n25.0\n0.0\n0.5\n6.15\n5.33\n36.98\nTuesday\n\n\nCMT\n2014-03-18 05:52:28\n2014-03-18 05:58:36\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 15:58:53\n2014-03-18 16:01:41\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n3.00\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 11:59:15\n2014-03-18 12:09:21\n1\n1.3\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.50\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 08:40:28\n2014-03-18 08:58:55\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 02:39:56\n2014-03-18 02:43:17\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-17 23:54:08\n2014-03-18 00:01:05\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n1.00\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-18 12:11:59\n2014-03-18 12:23:11\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 10:47:18\n2014-03-18 12:36:39\n1\n11.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 07:30:58\n2014-03-18 07:42:44\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 09:04:14\n2014-03-18 09:16:35\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 15:50:50\n2014-03-18 16:01:45\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n2.62\n0.00\n13.12\nTuesday\n\n\nCMT\n2014-03-18 08:04:49\n2014-03-18 08:19:05\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 08:52:43\n2014-03-18 09:00:08\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 08:22:57\n2014-03-18 08:36:56\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 06:09:28\n2014-03-18 06:11:37\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.10\n0.00\n5.60\nTuesday\n\n\nCMT\n2014-03-18 07:08:34\n2014-03-18 07:15:28\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 06:32:35\n2014-03-18 06:42:35\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 10:55:48\n2014-03-18 11:40:37\n2\n17.1\n-74.0\n40.7\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n11.55\n5.33\n69.38\nTuesday\n\n\nCMT\n2014-03-18 06:50:04\n2014-03-18 06:55:10\n1\n0.8\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 06:05:28\n2014-03-18 06:15:29\n1\n3.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n2.70\n0.00\n16.20\nTuesday\n\n\nCMT\n2014-03-18 06:24:37\n2014-03-18 06:35:03\n1\n2.8\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n2.22\n0.00\n13.22\nTuesday\n\n\nCMT\n2014-03-18 15:11:43\n2014-03-18 15:26:28\n2\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 09:00:05\n2014-03-18 09:12:46\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 12:39:13\n2014-03-18 12:46:35\n2\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.50\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 08:06:43\n2014-03-18 08:25:10\n1\n5.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.5\n0.0\n0.5\n4.00\n0.00\n24.00\nTuesday\n\n\nCMT\n2014-03-18 07:47:47\n2014-03-18 07:58:19\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 07:34:32\n2014-03-18 07:43:38\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 06:25:10\n2014-03-18 06:52:40\n2\n7.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n26.5\n0.0\n0.5\n5.40\n0.00\n32.40\nTuesday\n\n\nCMT\n2014-03-18 07:19:55\n2014-03-18 07:20:43\n1\n0.0\n-74.1\n40.8\n5\nY\n-74.1\n40.8\nCRD\n55.0\n0.0\n0.0\n9.00\n11.00\n75.00\nTuesday\n\n\nCMT\n2014-03-18 06:52:06\n2014-03-18 07:23:22\n1\n17.6\n-74.0\n40.7\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n14.45\n5.33\n72.28\nTuesday\n\n\nCMT\n2014-03-18 06:31:58\n2014-03-18 06:45:33\n1\n5.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.0\n0.0\n0.5\n1.50\n0.00\n20.00\nTuesday\n\n\nCMT\n2014-03-18 13:06:11\n2014-03-18 13:17:33\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 07:38:47\n2014-03-18 07:58:16\n1\n7.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n22.5\n0.0\n0.5\n2.00\n0.00\n25.00\nTuesday\n\n\nCMT\n2014-03-18 09:59:14\n2014-03-18 10:05:25\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 08:09:54\n2014-03-18 08:15:54\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.00\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 12:58:55\n2014-03-18 13:18:10\n1\n3.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.5\n0.0\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-18 08:29:11\n2014-03-18 08:31:57\n1\n0.5\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.00\n0.00\n5.50\nTuesday\n\n\nCMT\n2014-03-18 06:47:32\n2014-03-18 06:49:32\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n2.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 09:55:51\n2014-03-18 10:13:41\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n3.50\n0.00\n17.50\nTuesday\n\n\nCMT\n2014-03-18 09:00:33\n2014-03-18 09:12:26\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 06:43:18\n2014-03-18 06:45:29\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n0.80\n0.00\n4.80\nTuesday\n\n\nCMT\n2014-03-18 10:17:15\n2014-03-18 10:20:28\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n0.50\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 09:24:27\n2014-03-18 09:38:05\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 09:12:19\n2014-03-18 09:19:13\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 06:17:01\n2014-03-18 06:25:02\n1\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n1.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 07:45:15\n2014-03-18 07:52:07\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 15:05:17\n2014-03-18 15:09:47\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 12:03:10\n2014-03-18 12:08:03\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 06:13:52\n2014-03-18 06:18:45\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 07:04:07\n2014-03-18 07:09:31\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.95\n0.00\n8.45\nTuesday\n\n\nCMT\n2014-03-18 09:24:33\n2014-03-18 09:34:04\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 09:59:52\n2014-03-18 10:01:58\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n0.50\n0.00\n4.50\nTuesday\n\n\nCMT\n2014-03-18 10:12:47\n2014-03-18 10:26:13\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n1.60\n0.00\n13.60\nTuesday\n\n\nCMT\n2014-03-17 21:49:20\n2014-03-17 21:58:25\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-18 02:11:13\n2014-03-18 02:12:18\n1\n0.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.0\n0.5\n0.5\n0.80\n0.00\n4.80\nTuesday\n\n\nCMT\n2014-03-17 18:59:06\n2014-03-17 19:05:58\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n1.0\n0.5\n2.55\n0.00\n11.05\nMonday\n\n\nCMT\n2014-03-17 17:54:00\n2014-03-17 18:03:54\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n1.0\n0.5\n2.37\n0.00\n11.87\nMonday\n\n\nCMT\n2014-03-17 19:37:29\n2014-03-17 19:43:05\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n1.0\n0.5\n1.75\n0.00\n8.75\nMonday\n\n\nCMT\n2014-03-17 23:40:15\n2014-03-17 23:47:29\n1\n2.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n3.15\n0.00\n13.65\nMonday\n\n\nCMT\n2014-03-17 17:09:10\n2014-03-17 17:26:11\n2\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.0\n1.0\n0.5\n1.75\n0.00\n17.25\nMonday\n\n\nCMT\n2014-03-17 18:25:59\n2014-03-17 18:37:34\n1\n2.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n1.0\n0.5\n3.12\n0.00\n15.62\nMonday\n\n\nCMT\n2014-03-17 21:53:55\n2014-03-17 22:02:50\n2\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n1.00\n0.00\n10.00\nMonday\n\n\nCMT\n2014-03-17 22:37:58\n2014-03-17 22:48:52\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-18 00:02:19\n2014-03-18 00:16:02\n1\n4.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.5\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-18 00:35:31\n2014-03-18 00:43:11\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 05:43:16\n2014-03-18 05:50:07\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 09:52:16\n2014-03-18 09:57:24\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.00\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 13:08:49\n2014-03-18 13:22:53\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n2.00\n0.00\n14.50\nTuesday\n\n\nCMT\n2014-03-18 12:28:43\n2014-03-18 12:36:07\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 05:43:10\n2014-03-18 05:53:29\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.50\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 09:18:21\n2014-03-18 09:25:30\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 06:39:11\n2014-03-18 06:48:46\n1\n3.8\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n12.0\n0.0\n0.5\n1.00\n0.00\n13.50\nTuesday\n\n\nCMT\n2014-03-18 10:47:42\n2014-03-18 10:59:49\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n3.30\n0.00\n14.30\nTuesday\n\n\nCMT\n2014-03-18 13:08:01\n2014-03-18 13:15:12\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 07:26:51\n2014-03-18 07:37:10\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 08:31:08\n2014-03-18 08:33:24\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 12:23:45\n2014-03-18 12:32:26\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 14:21:04\n2014-03-18 14:23:27\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n0.90\n0.00\n5.40\nTuesday\n\n\nCMT\n2014-03-18 11:33:13\n2014-03-18 11:53:34\n1\n6.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n22.0\n0.0\n0.5\n2.70\n0.00\n25.20\nTuesday\n\n\nCMT\n2014-03-18 15:03:43\n2014-03-18 15:08:16\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-17 20:05:20\n2014-03-17 20:15:27\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n1.90\n0.00\n11.40\nMonday\n\n\nCMT\n2014-03-17 19:41:43\n2014-03-17 19:52:10\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n1.0\n0.5\n2.60\n0.00\n13.10\nMonday\n\n\nCMT\n2014-03-17 23:37:58\n2014-03-17 23:50:43\n1\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.5\n0.5\n0.01\n0.00\n14.51\nMonday\n\n\nCMT\n2014-03-18 00:09:04\n2014-03-18 00:15:38\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.75\n0.00\n8.75\nTuesday\n\n\nCMT\n2014-03-18 10:41:28\n2014-03-18 10:50:24\n2\n0.9\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 13:35:44\n2014-03-18 13:44:35\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-17 20:45:07\n2014-03-17 20:49:11\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.5\n0.5\n1.50\n0.00\n7.50\nMonday\n\n\nCMT\n2014-03-17 23:23:07\n2014-03-17 23:56:18\n2\n14.6\n-74.0\n40.8\n1\nN\n-73.8\n40.8\nCRD\n43.0\n0.5\n0.5\n11.00\n0.00\n55.00\nMonday\n\n\nCMT\n2014-03-17 22:47:24\n2014-03-17 22:53:35\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.5\n0.5\n2.37\n0.00\n11.87\nMonday\n\n\nCMT\n2014-03-17 21:15:36\n2014-03-17 21:46:53\n1\n9.0\n-74.0\n40.7\n1\nY\n-73.9\n40.7\nCRD\n30.0\n0.5\n0.5\n3.88\n0.00\n34.88\nMonday\n\n\nCMT\n2014-03-17 19:58:33\n2014-03-17 20:04:07\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n1.0\n0.5\n2.00\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 20:05:00\n2014-03-17 20:21:02\n1\n2.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.5\n0.5\n1.00\n0.00\n14.00\nMonday\n\n\nCMT\n2014-03-17 21:51:51\n2014-03-17 21:58:42\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.5\n0.5\n1.00\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 21:12:00\n2014-03-17 21:31:25\n2\n5.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n18.0\n0.5\n0.5\n2.00\n5.33\n26.33\nMonday\n\n\nCMT\n2014-03-17 19:58:52\n2014-03-17 20:10:56\n1\n7.9\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n22.0\n0.5\n0.5\n5.66\n5.33\n33.99\nMonday\n\n\nCMT\n2014-03-17 19:38:59\n2014-03-17 19:47:02\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n1.0\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 22:58:43\n2014-03-17 23:02:32\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.50\n0.00\n7.50\nMonday\n\n\nCMT\n2014-03-18 02:08:08\n2014-03-18 02:12:25\n2\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 02:14:41\n2014-03-18 02:22:22\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-17 20:41:14\n2014-03-17 20:46:50\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n0.80\n0.00\n7.30\nMonday\n\n\nCMT\n2014-03-18 03:34:39\n2014-03-18 03:50:28\n1\n9.2\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n26.5\n0.5\n0.5\n8.20\n5.33\n41.03\nTuesday\n\n\nCMT\n2014-03-18 04:05:43\n2014-03-18 04:12:41\n1\n2.0\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n8.5\n0.5\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 13:21:21\n2014-03-18 13:35:06\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 15:14:13\n2014-03-18 15:19:41\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 09:16:47\n2014-03-18 09:22:34\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n0.50\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 12:52:18\n2014-03-18 13:06:33\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n2.40\n0.00\n14.40\nTuesday\n\n\nCMT\n2014-03-18 13:31:45\n2014-03-18 13:42:21\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n2.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 11:41:19\n2014-03-18 12:16:28\n1\n15.3\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n45.0\n0.0\n0.5\n10.15\n5.33\n60.98\nTuesday\n\n\nCMT\n2014-03-18 07:19:49\n2014-03-18 07:24:18\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n0.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 06:48:46\n2014-03-18 06:55:34\n1\n1.1\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 08:37:52\n2014-03-18 08:43:42\n2\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 13:44:25\n2014-03-18 14:01:42\n1\n4.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n16.0\n0.0\n0.5\n1.50\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-18 15:48:39\n2014-03-18 16:00:27\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 09:26:30\n2014-03-18 09:30:32\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 13:19:25\n2014-03-18 13:54:17\n1\n5.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n25.5\n0.0\n0.5\n5.20\n0.00\n31.20\nTuesday\n\n\nCMT\n2014-03-18 07:45:52\n2014-03-18 07:48:57\n2\n0.5\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 08:35:54\n2014-03-18 08:51:06\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 14:30:08\n2014-03-18 14:44:47\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.5\n0.0\n0.5\n1.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 12:50:45\n2014-03-18 13:18:03\n1\n15.2\n-74.0\n40.8\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n10.00\n5.33\n67.83\nTuesday\n\n\nCMT\n2014-03-18 11:08:04\n2014-03-18 11:33:34\n1\n3.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n17.0\n0.0\n0.5\n3.50\n0.00\n21.00\nTuesday\n\n\nCMT\n2014-03-18 07:47:04\n2014-03-18 07:58:50\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 08:39:32\n2014-03-18 08:39:56\n1\n0.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n2.5\n0.0\n0.5\n2.00\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 15:39:53\n2014-03-18 15:47:42\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n2.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 08:40:18\n2014-03-18 08:57:13\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n1.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 07:35:03\n2014-03-18 07:43:32\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.00\n0.00\n9.50\nTuesday\n\n\nCMT\n2014-03-18 08:51:04\n2014-03-18 09:24:13\n1\n10.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n34.5\n0.0\n0.5\n6.00\n5.33\n46.33\nTuesday\n\n\nCMT\n2014-03-18 06:07:49\n2014-03-18 06:29:51\n1\n5.2\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.0\n0.0\n0.5\n2.00\n0.00\n21.50\nTuesday\n\n\nCMT\n2014-03-18 07:36:32\n2014-03-18 07:45:10\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 06:46:36\n2014-03-18 06:55:05\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.50\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 10:00:16\n2014-03-18 10:14:22\n1\n1.3\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n10.0\n0.0\n0.5\n1.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 07:38:13\n2014-03-18 08:03:51\n1\n10.7\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n32.0\n0.0\n0.5\n7.50\n5.33\n45.33\nTuesday\n\n\nCMT\n2014-03-18 08:33:17\n2014-03-18 09:01:41\n1\n3.0\n-73.9\n40.7\n1\nN\n-74.0\n40.8\nCRD\n18.0\n0.0\n0.5\n3.70\n0.00\n22.20\nTuesday\n\n\nCMT\n2014-03-18 08:35:10\n2014-03-18 08:45:24\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 07:05:20\n2014-03-18 07:22:06\n1\n3.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.0\n0.5\n3.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-17 23:03:10\n2014-03-17 23:06:25\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nMonday\n\n\nCMT\n2014-03-17 19:11:51\n2014-03-17 19:13:45\n1\n0.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n1.0\n0.5\n2.00\n0.00\n7.00\nMonday\n\n\nCMT\n2014-03-18 00:19:23\n2014-03-18 00:26:30\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n3.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 21:36:24\n2014-03-17 21:42:47\n1\n1.9\n-74.0\n40.8\n1\nY\n-73.9\n40.8\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 19:52:40\n2014-03-17 19:55:45\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n1.0\n0.5\n0.50\n0.00\n7.00\nMonday\n\n\nCMT\n2014-03-17 19:27:20\n2014-03-17 19:40:17\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.5\n1.0\n0.5\n2.60\n0.00\n15.60\nMonday\n\n\nCMT\n2014-03-17 19:44:20\n2014-03-17 19:51:44\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n1.0\n0.5\n1.25\n0.00\n12.25\nMonday\n\n\nCMT\n2014-03-17 18:45:39\n2014-03-17 19:23:54\n1\n21.0\n-74.0\n40.8\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n8.00\n5.33\n65.83\nMonday\n\n\nCMT\n2014-03-18 00:14:54\n2014-03-18 00:30:21\n1\n4.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.0\n0.5\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-17 22:01:38\n2014-03-17 22:15:50\n1\n5.3\n-74.0\n40.8\n1\nN\n-73.9\n40.9\nCRD\n18.0\n0.5\n0.5\n8.00\n2.44\n29.44\nMonday\n\n\nCMT\n2014-03-17 21:34:44\n2014-03-17 21:49:41\n1\n3.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n3.50\n0.00\n17.50\nMonday\n\n\nCMT\n2014-03-17 19:41:27\n2014-03-17 19:46:58\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.0\n1.0\n0.5\n1.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 20:00:18\n2014-03-17 20:09:26\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n1.0\n0.5\n1.80\n0.00\n10.80\nMonday\n\n\nCMT\n2014-03-17 18:41:36\n2014-03-17 18:56:08\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n1.0\n0.5\n2.70\n0.00\n16.20\nMonday\n\n\nCMT\n2014-03-17 21:53:47\n2014-03-17 22:06:12\n1\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.5\n0.5\n1.50\n0.00\n14.00\nMonday\n\n\nCMT\n2014-03-17 21:14:58\n2014-03-17 21:24:23\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n1.50\n0.00\n11.50\nMonday\n\n\nCMT\n2014-03-17 18:48:56\n2014-03-17 18:50:59\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n1.0\n0.5\n1.00\n0.00\n6.50\nMonday\n\n\nCMT\n2014-03-18 09:06:04\n2014-03-18 09:41:15\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.0\n0.5\n2.00\n0.00\n24.00\nTuesday\n\n\nCMT\n2014-03-18 08:33:07\n2014-03-18 08:48:44\n1\n3.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.00\n0.00\n16.50\nTuesday\n\n\nCMT\n2014-03-18 09:05:48\n2014-03-18 09:11:17\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 12:32:46\n2014-03-18 12:46:06\n2\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n2.20\n0.00\n13.20\nTuesday\n\n\nCMT\n2014-03-18 07:41:58\n2014-03-18 07:55:11\n1\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n0.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 08:36:07\n2014-03-18 09:32:22\n1\n7.2\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n34.0\n0.0\n0.5\n3.00\n0.00\n37.50\nTuesday\n\n\nCMT\n2014-03-18 09:20:54\n2014-03-18 09:29:47\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 09:28:49\n2014-03-18 10:05:15\n1\n10.4\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n34.0\n0.0\n0.5\n7.95\n5.33\n47.78\nTuesday\n\n\nCMT\n2014-03-18 07:31:25\n2014-03-18 07:46:30\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n2.00\n0.00\n16.00\nTuesday\n\n\nCMT\n2014-03-18 11:59:13\n2014-03-18 12:15:52\n2\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n1.00\n0.00\n13.00\nTuesday\n\n\nCMT\n2014-03-18 07:43:00\n2014-03-18 07:47:19\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 05:03:22\n2014-03-18 05:18:33\n1\n4.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n16.0\n0.5\n0.5\n2.00\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-18 10:58:31\n2014-03-18 11:07:19\n1\n2.2\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n1.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 07:16:16\n2014-03-18 07:22:44\n1\n1.6\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 11:36:58\n2014-03-18 12:03:48\n1\n5.3\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.0\n0.5\n5.50\n0.00\n27.50\nTuesday\n\n\nCMT\n2014-03-18 10:15:57\n2014-03-18 10:23:26\n1\n1.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 06:14:22\n2014-03-18 06:35:38\n1\n8.0\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n25.5\n0.0\n0.5\n3.00\n0.00\n29.00\nTuesday\n\n\nCMT\n2014-03-17 22:16:45\n2014-03-17 22:22:59\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.5\n0.5\n2.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 21:10:34\n2014-03-17 21:21:04\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n0.50\n0.00\n11.50\nMonday\n\n\nCMT\n2014-03-18 01:22:08\n2014-03-18 01:40:20\n1\n5.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.0\n0.5\n0.5\n5.00\n0.00\n25.00\nTuesday\n\n\nCMT\n2014-03-17 22:22:19\n2014-03-17 22:48:05\n1\n8.7\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n28.0\n0.5\n0.5\n7.25\n0.00\n36.25\nMonday\n\n\nCMT\n2014-03-17 19:41:18\n2014-03-17 19:50:53\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n1.0\n0.5\n1.90\n0.00\n11.40\nMonday\n\n\nCMT\n2014-03-17 21:53:42\n2014-03-17 22:10:37\n1\n7.2\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n22.5\n0.5\n0.5\n4.70\n0.00\n28.20\nMonday\n\n\nCMT\n2014-03-17 19:29:22\n2014-03-17 19:37:59\n2\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n1.00\n0.00\n11.00\nMonday\n\n\nCMT\n2014-03-17 20:35:09\n2014-03-17 20:44:35\n1\n2.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-17 19:35:22\n2014-03-17 19:48:17\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.5\n1.0\n0.5\n2.40\n0.00\n14.40\nMonday\n\n\nCMT\n2014-03-17 20:43:17\n2014-03-17 21:08:00\n1\n7.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n24.0\n0.5\n0.5\n5.00\n0.00\n30.00\nMonday\n\n\nCMT\n2014-03-17 19:39:28\n2014-03-17 19:51:10\n1\n2.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n1.0\n0.5\n2.40\n0.00\n14.40\nMonday\n\n\nCMT\n2014-03-17 22:18:13\n2014-03-17 22:23:59\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 20:10:24\n2014-03-17 20:20:38\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 19:42:06\n2014-03-17 19:44:21\n1\n0.5\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n4.0\n1.0\n0.5\n1.00\n0.00\n6.50\nMonday\n\n\nCMT\n2014-03-17 22:57:11\n2014-03-17 23:00:20\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.50\n0.00\n7.50\nMonday\n\n\nCMT\n2014-03-18 03:31:59\n2014-03-18 03:51:46\n1\n7.3\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n23.5\n0.5\n0.5\n4.00\n0.00\n28.50\nTuesday\n\n\nCMT\n2014-03-17 19:53:43\n2014-03-17 19:57:04\n1\n0.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nMonday\n\n\nCMT\n2014-03-17 22:05:22\n2014-03-17 22:12:09\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n2.00\n0.00\n11.00\nMonday\n\n\nCMT\n2014-03-17 20:24:05\n2014-03-17 20:30:11\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.5\n0.5\n1.85\n0.00\n9.35\nMonday\n\n\nCMT\n2014-03-17 21:12:11\n2014-03-17 21:16:24\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n1.30\n0.00\n7.80\nMonday\n\n\nCMT\n2014-03-17 20:37:03\n2014-03-17 20:55:10\n1\n8.5\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n26.0\n0.5\n0.5\n6.45\n5.33\n38.78\nMonday\n\n\nCMT\n2014-03-17 21:34:08\n2014-03-17 22:14:30\n1\n14.1\n-74.0\n40.7\n1\nN\n-73.9\n40.6\nCRD\n44.5\n0.5\n0.5\n0.00\n0.00\n45.50\nMonday\n\n\nCMT\n2014-03-18 05:11:19\n2014-03-18 05:23:57\n1\n10.1\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n28.0\n0.5\n0.5\n6.86\n5.33\n41.19\nTuesday\n\n\nCMT\n2014-03-17 21:28:54\n2014-03-17 22:08:30\n1\n14.7\n-73.9\n40.8\n1\nN\n-74.0\n40.6\nCRD\n45.5\n0.5\n0.5\n11.62\n0.00\n58.12\nMonday\n\n\nCMT\n2014-03-17 21:19:25\n2014-03-17 21:27:54\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.0\n0.5\n0.5\n1.00\n0.00\n11.00\nMonday\n\n\nCMT\n2014-03-17 19:40:20\n2014-03-17 20:04:09\n1\n5.2\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.5\n1.0\n0.5\n4.20\n0.00\n25.20\nMonday\n\n\nCMT\n2014-03-18 01:27:42\n2014-03-18 01:37:54\n1\n3.4\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n12.0\n0.5\n0.5\n2.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 02:18:43\n2014-03-18 02:27:33\n1\n2.4\n-73.8\n40.7\n1\nN\n-73.9\n40.7\nCRD\n9.5\n0.5\n0.5\n2.60\n0.00\n13.10\nTuesday\n\n\nCMT\n2014-03-17 23:25:59\n2014-03-17 23:49:10\n1\n8.0\n-73.9\n40.8\n1\nN\n-73.9\n40.7\nCRD\n25.5\n0.5\n0.5\n5.30\n0.00\n31.80\nMonday\n\n\nCMT\n2014-03-18 04:02:19\n2014-03-18 04:16:19\n1\n4.9\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n16.0\n0.5\n0.5\n2.00\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-17 21:29:14\n2014-03-17 21:33:42\n1\n0.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n1.62\n0.00\n8.12\nMonday\n\n\nCMT\n2014-03-18 03:10:28\n2014-03-18 03:44:22\n1\n19.2\n-74.0\n40.8\n1\nN\n-74.0\n40.6\nCRD\n53.5\n0.5\n0.5\n5.00\n0.00\n59.50\nTuesday\n\n\nCMT\n2014-03-17 19:30:30\n2014-03-17 19:44:42\n1\n6.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n21.0\n1.0\n0.5\n4.00\n0.00\n26.50\nMonday\n\n\nCMT\n2014-03-17 20:52:58\n2014-03-17 21:07:18\n1\n3.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.5\n0.5\n2.70\n0.00\n16.20\nMonday\n\n\nCMT\n2014-03-17 19:26:56\n2014-03-17 19:34:50\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n1.0\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 22:44:26\n2014-03-17 22:54:05\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-17 19:23:44\n2014-03-17 19:45:12\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n15.5\n1.0\n0.5\n2.00\n0.00\n19.00\nMonday\n\n\nCMT\n2014-03-17 23:12:01\n2014-03-17 23:24:45\n1\n3.9\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.5\n0.5\n2.90\n0.00\n17.40\nMonday\n\n\nCMT\n2014-03-17 19:52:05\n2014-03-17 20:07:31\n1\n2.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.5\n0.5\n2.80\n0.00\n16.80\nMonday\n\n\nCMT\n2014-03-17 20:30:16\n2014-03-17 20:38:33\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.5\n0.5\n1.00\n0.00\n10.00\nMonday\n\n\nCMT\n2014-03-17 22:13:51\n2014-03-17 22:53:01\n1\n11.2\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n39.5\n0.5\n0.5\n2.00\n0.00\n42.50\nMonday\n\n\nCMT\n2014-03-18 07:54:53\n2014-03-18 08:04:12\n2\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n2.55\n0.00\n11.05\nTuesday\n\n\nCMT\n2014-03-18 16:37:45\n2014-03-18 16:42:20\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n1.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 10:35:57\n2014-03-18 10:38:40\n1\n0.3\n0.0\n0.0\n1\nN\n0.0\n0.0\nCRD\n4.0\n0.0\n0.5\n0.45\n0.00\n4.95\nTuesday\n\n\nCMT\n2014-03-18 13:05:37\n2014-03-18 13:15:43\n2\n2.0\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 12:50:47\n2014-03-18 13:12:19\n2\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n15.0\n0.0\n0.5\n3.10\n0.00\n18.60\nTuesday\n\n\nCMT\n2014-03-18 14:58:50\n2014-03-18 15:26:56\n1\n2.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n18.0\n0.0\n0.5\n2.00\n0.00\n20.50\nTuesday\n\n\nCMT\n2014-03-18 12:16:34\n2014-03-18 12:35:44\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n1.50\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 12:16:04\n2014-03-18 12:29:59\n2\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n0.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 14:23:05\n2014-03-18 14:37:02\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 05:57:52\n2014-03-18 06:01:21\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 08:34:22\n2014-03-18 08:38:09\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 06:23:45\n2014-03-18 07:06:44\n1\n18.7\n-74.0\n40.8\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n8.00\n0.00\n60.50\nTuesday\n\n\nCMT\n2014-03-18 07:46:41\n2014-03-18 07:52:28\n1\n1.1\n-73.9\n40.8\n1\nN\n-73.9\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-17 20:51:15\n2014-03-17 20:57:00\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 22:44:13\n2014-03-17 23:04:01\n2\n9.6\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n28.0\n0.5\n0.5\n8.58\n5.33\n42.91\nMonday\n\n\nCMT\n2014-03-18 01:13:32\n2014-03-18 01:15:33\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-17 22:33:41\n2014-03-17 22:40:54\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nMonday\n\n\nCMT\n2014-03-17 22:04:29\n2014-03-17 22:17:58\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n1.50\n0.00\n15.50\nMonday\n\n\nCMT\n2014-03-18 00:30:27\n2014-03-18 00:34:21\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 00:14:04\n2014-03-18 00:35:52\n1\n7.1\n-74.0\n40.8\n1\nN\n-73.9\n40.7\nCRD\n23.0\n0.5\n0.5\n6.00\n0.00\n30.00\nTuesday\n\n\nCMT\n2014-03-17 18:02:22\n2014-03-17 18:11:21\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n1.0\n0.5\n1.80\n0.00\n10.80\nMonday\n\n\nCMT\n2014-03-17 18:11:34\n2014-03-17 18:15:21\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n1.0\n0.5\n1.00\n0.00\n8.00\nMonday\n\n\nCMT\n2014-03-17 21:15:20\n2014-03-17 21:21:07\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n0.00\n0.00\n7.50\nMonday\n\n\nCMT\n2014-03-18 01:29:25\n2014-03-18 01:40:11\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.5\n0.5\n1.73\n0.00\n13.23\nTuesday\n\n\nCMT\n2014-03-17 21:03:49\n2014-03-17 21:09:15\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.40\n0.00\n8.40\nMonday\n\n\nCMT\n2014-03-17 23:04:52\n2014-03-17 23:09:49\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.5\n0.5\n1.00\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 21:05:03\n2014-03-17 21:14:56\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n9.5\n0.5\n0.5\n1.00\n0.00\n11.50\nMonday\n\n\nCMT\n2014-03-17 17:55:03\n2014-03-17 17:58:50\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n1.0\n0.5\n1.20\n0.00\n7.20\nMonday\n\n\nCMT\n2014-03-17 21:33:46\n2014-03-17 21:41:57\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nMonday\n\n\nCMT\n2014-03-17 21:19:06\n2014-03-17 21:24:46\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.5\n0.5\n1.50\n0.00\n9.00\nMonday\n\n\nCMT\n2014-03-17 20:27:37\n2014-03-17 20:47:19\n1\n7.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n23.0\n0.5\n0.5\n4.80\n0.00\n28.80\nMonday\n\n\nCMT\n2014-03-17 23:54:30\n2014-03-18 00:09:29\n1\n3.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.5\n0.5\n0.5\n2.90\n0.00\n17.40\nMonday\n\n\nCMT\n2014-03-17 22:04:36\n2014-03-17 22:11:11\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.5\n0.5\n1.00\n0.00\n8.50\nMonday\n\n\nCMT\n2014-03-17 22:21:00\n2014-03-17 22:28:12\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n2.12\n0.00\n10.62\nMonday\n\n\nCMT\n2014-03-17 23:31:01\n2014-03-17 23:38:11\n1\n2.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.5\n0.5\n1.00\n0.00\n10.50\nMonday\n\n\nCMT\n2014-03-17 22:53:36\n2014-03-17 22:55:39\n1\n0.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.5\n0.5\n0.22\n0.00\n4.72\nMonday\n\n\nCMT\n2014-03-17 21:36:26\n2014-03-17 22:04:22\n1\n11.8\n-74.0\n40.7\n1\nN\n-73.9\n40.9\nCRD\n36.0\n0.5\n0.5\n2.00\n0.00\n39.00\nMonday\n\n\nCMT\n2014-03-17 19:18:16\n2014-03-17 19:47:11\n1\n14.6\n-74.0\n40.8\n2\nN\n-73.8\n40.7\nCRD\n52.0\n0.0\n0.5\n7.17\n5.33\n65.00\nMonday\n\n\nCMT\n2014-03-17 22:16:16\n2014-03-17 22:26:37\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n1.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 19:43:00\n2014-03-17 19:53:35\n1\n4.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n1.0\n0.5\n2.00\n0.00\n18.00\nMonday\n\n\nCMT\n2014-03-18 06:52:15\n2014-03-18 06:56:48\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n0.50\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 11:45:47\n2014-03-18 11:57:49\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n2.37\n0.00\n11.87\nTuesday\n\n\nCMT\n2014-03-18 12:41:58\n2014-03-18 12:54:47\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 07:42:25\n2014-03-18 07:46:47\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 07:20:36\n2014-03-18 07:36:29\n1\n3.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.0\n0.5\n2.00\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-18 09:13:30\n2014-03-18 09:28:42\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 08:18:30\n2014-03-18 08:29:05\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n2.55\n0.00\n11.05\nTuesday\n\n\nCMT\n2014-03-18 09:27:36\n2014-03-18 09:57:54\n1\n3.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n19.5\n0.0\n0.5\n4.00\n0.00\n24.00\nTuesday\n\n\nCMT\n2014-03-18 08:58:18\n2014-03-18 09:12:39\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.30\n0.00\n13.80\nTuesday\n\n\nCMT\n2014-03-18 08:22:32\n2014-03-18 08:39:47\n1\n3.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.0\n0.0\n0.5\n1.00\n0.00\n16.50\nTuesday\n\n\nCMT\n2014-03-18 06:43:21\n2014-03-18 06:46:44\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.50\n0.00\n6.50\nTuesday\n\n\nCMT\n2014-03-18 10:59:09\n2014-03-18 11:10:36\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.0\n0.0\n0.5\n1.50\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 08:21:43\n2014-03-18 08:28:47\n1\n1.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.50\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 10:40:17\n2014-03-18 10:45:19\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.0\n0.0\n0.5\n1.37\n0.00\n6.87\nTuesday\n\n\nCMT\n2014-03-18 09:21:20\n2014-03-18 09:37:48\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n3.00\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 15:36:48\n2014-03-18 15:57:44\n1\n8.8\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n27.0\n0.0\n0.5\n9.84\n5.33\n42.67\nTuesday\n\n\nCMT\n2014-03-18 16:42:50\n2014-03-18 17:01:09\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.5\n1.0\n0.5\n3.00\n0.00\n18.00\nTuesday\n\n\nCMT\n2014-03-18 09:05:55\n2014-03-18 09:16:36\n1\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n8.5\n0.0\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 13:04:54\n2014-03-18 13:06:47\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n1.00\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 07:24:27\n2014-03-18 07:29:02\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 14:38:58\n2014-03-18 14:49:40\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 13:19:29\n2014-03-18 13:36:56\n1\n2.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n2.50\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 08:01:12\n2014-03-18 08:10:53\n1\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n1.00\n0.00\n11.50\nTuesday\n\n\nCMT\n2014-03-18 11:30:23\n2014-03-18 11:43:35\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 10:19:49\n2014-03-18 10:29:48\n1\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.5\n0.0\n0.5\n3.30\n0.00\n14.30\nTuesday\n\n\nCMT\n2014-03-18 16:29:37\n2014-03-18 16:41:26\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n1.0\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-18 14:30:30\n2014-03-18 14:35:15\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 08:52:34\n2014-03-18 09:12:53\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n2.10\n0.00\n16.60\nTuesday\n\n\nCMT\n2014-03-18 09:26:23\n2014-03-18 10:03:49\n1\n12.3\n-74.0\n40.8\n1\nN\n-74.0\n40.6\nCRD\n40.0\n0.0\n0.5\n0.00\n0.00\n40.50\nTuesday\n\n\nCMT\n2014-03-18 12:22:35\n2014-03-18 12:42:56\n3\n3.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n3.20\n0.00\n19.20\nTuesday\n\n\nCMT\n2014-03-18 09:32:28\n2014-03-18 09:44:13\n1\n1.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 09:50:28\n2014-03-18 09:59:39\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.0\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-18 15:34:04\n2014-03-18 15:55:33\n1\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.0\n0.5\n5.00\n0.00\n19.50\nTuesday\n\n\nCMT\n2014-03-18 12:05:05\n2014-03-18 12:13:43\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 07:18:01\n2014-03-18 07:23:08\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.40\n0.00\n8.40\nTuesday\n\n\nCMT\n2014-03-18 14:28:52\n2014-03-18 14:38:13\n1\n1.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 11:54:21\n2014-03-18 12:25:08\n1\n9.4\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n31.0\n0.0\n0.5\n7.36\n5.33\n44.19\nTuesday\n\n\nCMT\n2014-03-18 09:00:07\n2014-03-18 09:03:15\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.0\n0.5\n1.00\n0.00\n5.50\nTuesday\n\n\nCMT\n2014-03-18 13:02:09\n2014-03-18 13:08:53\n2\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 08:16:42\n2014-03-18 08:31:02\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.0\n0.5\n2.00\n0.00\n16.00\nTuesday\n\n\nCMT\n2014-03-18 07:11:05\n2014-03-18 07:17:41\n2\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.75\n0.00\n8.75\nTuesday\n\n\nCMT\n2014-03-18 08:20:44\n2014-03-18 08:24:41\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.35\n0.00\n6.85\nTuesday\n\n\nCMT\n2014-03-18 08:37:48\n2014-03-18 09:04:46\n1\n6.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n24.5\n0.0\n0.5\n5.00\n0.00\n30.00\nTuesday\n\n\nCMT\n2014-03-18 05:16:17\n2014-03-18 05:28:25\n1\n4.6\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n15.0\n0.5\n0.5\n3.00\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-18 13:31:56\n2014-03-18 13:49:24\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.0\n0.0\n0.5\n2.50\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 10:31:17\n2014-03-18 11:09:44\n1\n11.0\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n36.5\n0.0\n0.5\n5.00\n0.00\n42.00\nTuesday\n\n\nCMT\n2014-03-18 07:10:06\n2014-03-18 07:14:23\n1\n0.5\n-73.9\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n0.50\n0.00\n5.50\nTuesday\n\n\nCMT\n2014-03-18 08:06:32\n2014-03-18 08:27:23\n1\n4.0\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n16.5\n0.0\n0.5\n4.00\n0.00\n21.00\nTuesday\n\n\nCMT\n2014-03-18 08:03:52\n2014-03-18 08:16:26\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.0\n0.5\n1.50\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 07:39:09\n2014-03-18 07:49:04\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n2.00\n0.00\n13.50\nTuesday\n\n\nCMT\n2014-03-18 06:21:55\n2014-03-18 06:31:32\n1\n4.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.5\n0.0\n0.5\n2.00\n0.00\n17.00\nTuesday\n\n\nCMT\n2014-03-18 05:49:49\n2014-03-18 05:51:47\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.5\n0.5\n1.00\n0.00\n5.50\nTuesday\n\n\nCMT\n2014-03-18 11:23:43\n2014-03-18 11:30:41\n1\n1.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 10:49:18\n2014-03-18 11:00:44\n1\n1.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 14:55:26\n2014-03-18 14:57:30\n1\n0.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n3.5\n0.0\n0.5\n1.00\n0.00\n5.00\nTuesday\n\n\nCMT\n2014-03-18 16:59:17\n2014-03-18 17:09:14\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n1.0\n0.5\n1.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 20:14:34\n2014-03-18 20:35:00\n1\n4.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n16.5\n0.5\n0.5\n1.50\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-18 07:47:08\n2014-03-18 08:04:26\n1\n2.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.0\n0.5\n1.50\n0.00\n15.00\nTuesday\n\n\nCMT\n2014-03-18 09:09:31\n2014-03-18 09:17:14\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.5\n0.0\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 12:00:09\n2014-03-18 12:11:51\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n2.50\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 14:58:26\n2014-03-18 15:29:56\n2\n9.8\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n32.0\n0.0\n0.5\n6.00\n5.33\n43.83\nTuesday\n\n\nCMT\n2014-03-18 13:32:55\n2014-03-18 13:41:33\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 12:43:26\n2014-03-18 13:09:00\n1\n12.1\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n34.0\n0.0\n0.5\n6.90\n0.00\n41.40\nTuesday\n\n\nCMT\n2014-03-18 05:52:44\n2014-03-18 06:29:16\n1\n20.6\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n11.56\n5.33\n69.39\nTuesday\n\n\nCMT\n2014-03-18 08:01:35\n2014-03-18 08:07:36\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 14:14:45\n2014-03-18 14:32:44\n1\n4.2\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n16.5\n0.0\n0.5\n3.40\n0.00\n20.40\nTuesday\n\n\nCMT\n2014-03-18 05:56:08\n2014-03-18 06:20:13\n1\n9.6\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n30.0\n0.5\n0.5\n7.26\n5.33\n43.59\nTuesday\n\n\nCMT\n2014-03-18 08:16:55\n2014-03-18 08:22:43\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.87\n0.00\n9.37\nTuesday\n\n\nCMT\n2014-03-18 15:50:21\n2014-03-18 16:03:50\n1\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 22:37:40\n2014-03-18 22:42:07\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n2.10\n0.00\n9.10\nTuesday\n\n\nCMT\n2014-03-18 11:24:24\n2014-03-18 11:36:29\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.00\n0.00\n12.00\nTuesday\n\n\nCMT\n2014-03-18 10:50:47\n2014-03-18 11:01:39\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n2.00\n0.00\n11.00\nTuesday\n\n\nCMT\n2014-03-18 05:55:59\n2014-03-18 05:59:48\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.0\n0.5\n1.10\n0.00\n6.60\nTuesday\n\n\nCMT\n2014-03-18 14:37:32\n2014-03-18 14:49:49\n1\n0.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 08:37:02\n2014-03-18 08:42:47\n1\n1.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 06:28:52\n2014-03-18 06:42:42\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 06:29:09\n2014-03-18 06:33:32\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 14:09:45\n2014-03-18 14:16:41\n1\n0.7\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.0\n0.5\n1.50\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-18 08:07:32\n2014-03-18 08:25:21\n1\n2.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.5\n0.0\n0.5\n1.50\n0.00\n15.50\nTuesday\n\n\nCMT\n2014-03-18 13:27:45\n2014-03-18 13:44:45\n2\n2.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.5\n0.0\n0.5\n2.60\n0.00\n15.60\nTuesday\n\n\nCMT\n2014-03-18 15:25:44\n2014-03-18 15:44:59\n1\n3.3\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n15.5\n0.0\n0.5\n3.00\n0.00\n19.00\nTuesday\n\n\nCMT\n2014-03-18 10:34:30\n2014-03-18 10:38:29\n1\n0.3\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 06:19:18\n2014-03-18 06:24:33\n1\n0.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-18 06:30:13\n2014-03-18 06:38:01\n1\n2.7\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.0\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 04:41:27\n2014-03-18 04:43:54\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.0\n0.5\n0.5\n1.00\n0.00\n6.00\nTuesday\n\n\nCMT\n2014-03-18 06:07:58\n2014-03-18 06:38:13\n1\n17.7\n-74.0\n40.8\n2\nN\n-73.8\n40.6\nCRD\n52.0\n0.0\n0.5\n10.00\n5.33\n67.83\nTuesday\n\n\nCMT\n2014-03-18 04:58:57\n2014-03-18 05:08:53\n1\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n12.5\n0.5\n0.5\n2.70\n0.00\n16.20\nTuesday\n\n\nCMT\n2014-03-18 05:37:34\n2014-03-18 05:40:43\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.5\n0.5\n0.5\n1.30\n0.00\n7.80\nTuesday\n\n\nCMT\n2014-03-18 05:03:19\n2014-03-18 05:23:40\n2\n10.9\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n32.0\n0.5\n0.5\n7.66\n5.33\n45.99\nTuesday\n\n\nCMT\n2014-03-18 08:32:52\n2014-03-18 08:37:53\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.0\n0.5\n1.50\n0.00\n7.50\nTuesday\n\n\nCMT\n2014-03-18 06:50:27\n2014-03-18 06:57:03\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.00\n0.00\n8.50\nTuesday\n\n\nCMT\n2014-03-18 05:46:31\n2014-03-18 05:49:24\n1\n1.4\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n1.00\n0.00\n8.00\nTuesday\n\n\nCMT\n2014-03-17 23:01:35\n2014-03-17 23:43:43\n1\n18.6\n-73.8\n40.6\n1\nN\n-74.0\n40.7\nCRD\n55.0\n0.5\n0.5\n1.00\n5.33\n62.33\nMonday\n\n\nCMT\n2014-03-17 23:17:36\n2014-03-17 23:26:09\n1\n2.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.5\n0.5\n2.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 23:13:25\n2014-03-17 23:43:18\n1\n14.6\n-74.0\n40.8\n1\nN\n-74.0\n40.6\nCRD\n42.0\n0.5\n0.5\n5.00\n0.00\n48.00\nMonday\n\n\nCMT\n2014-03-18 01:19:23\n2014-03-18 01:28:59\n1\n6.2\n-73.8\n40.6\n1\nN\n-73.8\n40.7\nCRD\n18.0\n0.5\n0.5\n3.80\n0.00\n22.80\nTuesday\n\n\nCMT\n2014-03-18 00:00:11\n2014-03-18 00:15:33\n1\n4.1\n-74.0\n40.8\n1\nN\n-73.9\n40.7\nCRD\n15.5\n0.5\n0.5\n4.36\n5.33\n26.19\nTuesday\n\n\nCMT\n2014-03-17 23:08:27\n2014-03-17 23:11:49\n1\n0.5\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.5\n0.5\n0.50\n0.00\n6.00\nMonday\n\n\nCMT\n2014-03-17 22:59:25\n2014-03-17 23:23:44\n1\n9.5\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n30.0\n0.5\n0.5\n6.20\n0.00\n37.20\nMonday\n\n\nCMT\n2014-03-18 00:27:27\n2014-03-18 00:39:38\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n12.0\n0.5\n0.5\n1.50\n0.00\n14.50\nTuesday\n\n\nCMT\n2014-03-17 22:55:47\n2014-03-17 23:07:28\n2\n2.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.5\n0.5\n2.00\n0.00\n14.50\nMonday\n\n\nCMT\n2014-03-17 23:57:55\n2014-03-18 00:10:26\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.0\n0.5\n0.5\n1.00\n0.00\n15.00\nMonday\n\n\nCMT\n2014-03-18 00:17:24\n2014-03-18 00:43:28\n1\n6.9\n-74.0\n40.7\n1\nN\n-73.9\n40.8\nCRD\n24.0\n0.5\n0.5\n3.30\n0.00\n28.30\nTuesday\n\n\nCMT\n2014-03-17 23:45:57\n2014-03-17 23:52:07\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n7.0\n0.5\n0.5\n1.60\n0.00\n9.60\nMonday\n\n\nCMT\n2014-03-18 07:54:57\n2014-03-18 07:58:09\n1\n0.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n0.50\n0.00\n5.50\nTuesday\n\n\nCMT\n2014-03-18 00:48:35\n2014-03-18 01:02:18\n1\n4.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n15.5\n0.5\n0.5\n3.00\n0.00\n19.50\nTuesday\n\n\nCMT\n2014-03-18 00:53:55\n2014-03-18 01:00:20\n2\n2.7\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n9.5\n0.5\n0.5\n2.10\n0.00\n12.60\nTuesday\n\n\nCMT\n2014-03-18 01:59:00\n2014-03-18 02:04:22\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.5\n0.5\n1.70\n0.00\n10.20\nTuesday\n\n\nCMT\n2014-03-17 23:41:29\n2014-03-17 23:50:10\n3\n2.6\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n10.0\n0.5\n0.5\n2.20\n0.00\n13.20\nMonday\n\n\nCMT\n2014-03-17 11:59:36\n2014-03-17 12:49:33\n1\n5.5\n-73.9\n40.8\n1\nY\n-74.0\n40.7\nCRD\n31.0\n0.0\n0.5\n6.30\n0.00\n37.80\nMonday\n\n\nCMT\n2014-03-18 08:11:25\n2014-03-18 08:25:59\n1\n2.2\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n11.5\n0.0\n0.5\n2.40\n0.00\n14.40\nTuesday\n\n\nCMT\n2014-03-18 11:01:24\n2014-03-18 11:08:53\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.5\n0.0\n0.5\n1.80\n0.00\n10.80\nTuesday\n\n\nCMT\n2014-03-18 08:30:13\n2014-03-18 08:41:19\n1\n1.6\n-74.0\n40.8\n1\nN\n-73.9\n40.8\nCRD\n9.0\n0.0\n0.5\n1.90\n0.00\n11.40\nTuesday\n\n\nCMT\n2014-03-18 07:59:55\n2014-03-18 08:10:16\n1\n2.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.0\n0.5\n2.50\n0.00\n14.00\nTuesday\n\n\nCMT\n2014-03-18 07:30:05\n2014-03-18 07:47:52\n1\n4.9\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n17.5\n0.0\n0.5\n3.60\n0.00\n21.60\nTuesday\n\n\nCMT\n2014-03-18 08:31:55\n2014-03-18 08:38:41\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n7.0\n0.0\n0.5\n1.87\n0.00\n9.37\nTuesday\n\n\nCMT\n2014-03-18 09:23:59\n2014-03-18 09:31:55\n1\n0.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.5\n0.0\n0.5\n2.00\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 07:37:01\n2014-03-18 07:46:03\n1\n2.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.0\n0.0\n0.5\n1.00\n0.00\n10.50\nTuesday\n\n\nCMT\n2014-03-18 07:26:38\n2014-03-18 07:34:07\n1\n1.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n7.5\n0.0\n0.5\n1.60\n0.00\n9.60\nTuesday\n\n\nCMT\n2014-03-18 05:22:53\n2014-03-18 05:52:37\n1\n20.7\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n14.45\n5.33\n72.28\nTuesday\n\n\nCMT\n2014-03-18 06:49:17\n2014-03-18 07:32:54\n1\n21.3\n-73.8\n40.6\n2\nN\n-74.0\n40.8\nCRD\n52.0\n0.0\n0.5\n9.00\n5.33\n66.83\nTuesday\n\n\nCMT\n2014-03-18 11:23:05\n2014-03-18 11:55:03\n1\n2.9\n-74.0\n40.8\n1\nY\n-74.0\n40.7\nCRD\n19.5\n0.0\n0.5\n5.00\n0.00\n25.00\nTuesday\n\n\nCMT\n2014-03-18 13:20:27\n2014-03-18 13:27:59\n1\n0.8\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n7.0\n0.0\n0.5\n1.50\n0.00\n9.00\nTuesday\n\n\nCMT\n2014-03-18 07:02:38\n2014-03-18 07:13:45\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n11.0\n0.0\n0.5\n1.00\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 20:26:44\n2014-03-17 20:32:37\n1\n0.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n6.0\n0.5\n0.5\n2.10\n0.00\n9.10\nMonday\n\n\nCMT\n2014-03-18 00:08:36\n2014-03-18 00:30:33\n1\n14.2\n-74.0\n40.8\n1\nN\n-73.8\n40.8\nCRD\n39.5\n0.5\n0.5\n8.00\n5.33\n53.83\nTuesday\n\n\nCMT\n2014-03-18 00:40:54\n2014-03-18 00:44:04\n1\n1.1\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.20\n0.00\n7.20\nTuesday\n\n\nCMT\n2014-03-17 19:56:49\n2014-03-17 20:13:07\n1\n3.0\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n13.5\n1.0\n0.5\n3.00\n0.00\n18.00\nMonday\n\n\nCMT\n2014-03-17 19:23:56\n2014-03-17 19:45:59\n1\n9.8\n-73.9\n40.8\n1\nN\n-74.0\n40.7\nCRD\n29.0\n1.0\n0.5\n7.16\n5.33\n42.99\nMonday\n\n\nCMT\n2014-03-18 03:04:24\n2014-03-18 03:16:58\n1\n5.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n17.5\n0.5\n0.5\n3.70\n0.00\n22.20\nTuesday\n\n\nCMT\n2014-03-17 23:47:58\n2014-03-17 23:51:21\n1\n1.1\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n5.5\n0.5\n0.5\n0.90\n0.00\n7.40\nMonday\n\n\nCMT\n2014-03-17 19:59:38\n2014-03-17 20:13:46\n1\n3.3\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n2.80\n0.00\n16.80\nMonday\n\n\nCMT\n2014-03-17 19:46:48\n2014-03-17 19:55:22\n1\n1.4\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n7.5\n1.0\n0.5\n3.00\n0.00\n12.00\nMonday\n\n\nCMT\n2014-03-17 20:14:23\n2014-03-17 20:31:15\n1\n3.0\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.5\n0.5\n0.5\n2.00\n0.00\n16.50\nMonday\n\n\nCMT\n2014-03-17 20:55:29\n2014-03-17 21:02:55\n1\n1.7\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n2.25\n0.00\n11.25\nMonday\n\n\nCMT\n2014-03-18 01:05:07\n2014-03-18 01:11:45\n3\n1.6\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n8.0\n0.5\n0.5\n1.00\n0.00\n10.00\nTuesday\n\n\nCMT\n2014-03-18 00:36:35\n2014-03-18 00:40:22\n1\n0.6\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.00\n0.00\n7.00\nTuesday\n\n\nCMT\n2014-03-18 08:11:35\n2014-03-18 08:23:05\n1\n1.8\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n9.5\n0.0\n0.5\n2.50\n0.00\n12.50\nTuesday\n\n\nCMT\n2014-03-17 14:13:16\n2014-03-17 14:16:39\n1\n0.4\n-74.0\n40.8\n1\nY\n-74.0\n40.8\nCRD\n4.5\n0.0\n0.5\n1.00\n0.00\n6.00\nMonday\n```{r}\nnew_table |&gt;\n  drop_na(wday) |&gt;\n  ggplot(aes(x = fare_amount, y = tip_amount, color = wday)) + \n  geom_point() \n```\nIt is always a good idea to terminate the SQL connection when you are done with it.\ndbDisconnect(con_taxi)"
  },
  {
    "objectID": "06-const-change-db.html#quarto",
    "href": "06-const-change-db.html#quarto",
    "title": "\n6  Untitled\n",
    "section": "\n6.1 Quarto",
    "text": "6.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "06-const-change-db.html#running-code",
    "href": "06-const-change-db.html#running-code",
    "title": "\n6  Untitled\n",
    "section": "\n6.2 Running Code",
    "text": "6.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "03-sql-verbs.html#looking-at-the-tables-in-the-database",
    "href": "03-sql-verbs.html#looking-at-the-tables-in-the-database",
    "title": "3  SQL clauses",
    "section": "\n3.1 Looking at the tables in the database",
    "text": "3.1 Looking at the tables in the database\nConsider a database of taxi rides from the Yellow Cab company in NYC in March of 2014.\n\n\nlibrary(mdsr)\ncon_taxi &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"nyctaxi\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\n\nSHOW TABLES;\n\n\n\n\n\nTable 3.1: SHOW all the TABLES in the nyctaxi database.\n\nTables_in_nyctaxi\n\n\nyellow_old\n\n\n\n\n\n\n\nThere is only one table in the nyctaxi database, called yellow_old.\n\nDESCRIBE yellow_old;\n\n\n\n\n\nTable 3.2: DESCRIBE variables in the yellow_old table.\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\nvendor_id\ntext\nYES\n\n\n\n\n\npickup_datetime\ntext\nYES\n\n\n\n\n\ndropoff_datetime\ntext\nYES\n\n\n\n\n\npassenger_count\nbigint(20)\nYES\n\n\n\n\n\ntrip_distance\ndouble\nYES\n\n\n\n\n\npickup_longitude\ndouble\nYES\n\n\n\n\n\npickup_latitude\ndouble\nYES\n\n\n\n\n\nrate_code\nbigint(20)\nYES\n\n\n\n\n\nstore_and_fwd_flag\ntext\nYES\n\n\n\n\n\ndropoff_longitude\ndouble\nYES\n\n\n\n\n\ndropoff_latitude\ndouble\nYES\n\n\n\n\n\npayment_type\ntext\nYES\n\n\n\n\n\nfare_amount\ndouble\nYES\n\n\n\n\n\nsurcharge\ndouble\nYES\n\n\n\n\n\nmta_tax\ndouble\nYES\n\n\n\n\n\ntip_amount\ndouble\nYES\n\n\n\n\n\ntolls_amount\ndouble\nYES\n\n\n\n\n\ntotal_amount\ndouble\nYES\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, the DESCRIBE command shows the 18 field names (variables) in the yellow_old table. Some of the variables are characters (text) and some are numeric (either double or bigint)\n\n\n\n\n\n\n Watch out!\n\n\n\nSQL clauses must be written in the following order.\n\n\nMost engagements with SQL are done through queries. Queries in SQL start with the SELECT keyword and consist of several clauses, which must be written in the following order:1\n\n\nSELECT allows you to list the columns, or functions operating on columns, that you want to retrieve. This is an analogous operation to the select() verb in dplyr, potentially combined with mutate() or summarize().\n\nFROM specifies the table where the data are.\n\nJOIN allows you to stitch together two or more tables using a key. This is analogous to the inner_join() and left_join() commands in dplyr. More details of JOIN are given in Chapter 4.\n\nWHERE allows you to filter the records according to some criteria and is an analogous operation to the filter() verb in dplyr. Note, even though the WHERE clause is written after SELECT and JOIN, it is actually evaluated before the SELECT or JOIN clauses (which is why WHERE only works on the original data, not the results set).\n\nGROUP BY allows you to aggregate the records according to some shared value and is an analogous operation to the group_by() verb in dplyr.\n\nHAVING is like a WHERE clause that operates on the result set—not the records themselves and is analogous to applying a second filter() command in dplyr, after the rows have already been aggregated.\n\nORDER BY is exactly what it sounds like—it specifies a condition for ordering the rows of the result set and is analogous to the arrange() verb in dplyr.\n\nLIMIT restricts the number of rows in the output and is similar to the R commands head() and slice()."
  },
  {
    "objectID": "03-sql-verbs.html#select-from",
    "href": "03-sql-verbs.html#select-from",
    "title": "3  SQL verbs",
    "section": "\n3.2 SELECT … FROM",
    "text": "3.2 SELECT … FROM\n\n\n\n\n\n\nR function: select()\n\n\n\nA SQL query starts with a SELECT command and has a corresponding FROM to indicate the table being queried. Columns may be specified, or the * will indicate that every column in the table should be returned.\nThe shortest SQL query is the following SELECT command. Do not run this command!!! The yellow_old table has 15 million rows, and we do not want to look at them simultaneously.\n\nDO NOT RUN:  SELECT * FROM yellow_old;\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDo not run the following command unless you are certain that the table from which you are querying is small enough so that the query results fit easily into your memory.\nSELECT * FROM table;\n\n\nInstead, to look at the top of the table, SELECT the first few rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14).\n\nSELECT * FROM yellow_old LIMIT 0, 14;\n\n\n\n\n\nTable 3.3: SELECT the first 14 rows of the table\n\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\ntrip_distance\npickup_longitude\npickup_latitude\nrate_code\nstore_and_fwd_flag\ndropoff_longitude\ndropoff_latitude\npayment_type\nfare_amount\nsurcharge\nmta_tax\ntip_amount\ntolls_amount\ntotal_amount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMT\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n1\n2.0\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n9.0\n0.5\n0.5\n2.0\n0\n12.0\n\n\nCMT\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.0\n0\n8.0\n\n\nCMT\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n3\n0.5\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.2\n0\n7.2\n\n\nCMT\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n3\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.5\n0.5\n3.0\n0\n18.0\n\n\nCMT\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.0\n0\n12.5\n\n\nCMT\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n0.5\n0\n5.5\n\n\nCMT\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n2\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.5\n0.5\n3.1\n0\n18.6\n\n\nCMT\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n1\n5.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n20.0\n0.5\n0.5\n3.0\n0\n24.0\n\n\nCMT\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.5\n0.5\n2.9\n0\n17.4\n\n\nCMT\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n3\n5.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.5\n0.5\n2.0\n0\n24.5\n\n\nCMT\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.2\n0\n13.2\n\n\nCMT\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n3\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n2.8\n0\n16.8\n\n\nCMT\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n1\n2.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.5\n0.5\n1.5\n0\n13.5\n\n\n\n\n\n\n\n\nSpeaking of which, how many rows are there in the yellow_old table? That is, how many taxi rides are recorded? Now SELECT is used with a summary function, COUNT(). Instead of using a separate summary function (like mutate() or summarize()), all the work is done inside the SELECT call.\n\nSELECT COUNT(*) FROM yellow_old;\n\n\n\n\n\nTable 3.4: COUNT(*) the number of rows in the entire yellow_old table.\n\nCOUNT(*)\n\n\n15428128\n\n\n\n\n\n\n\nYikes, more than 15 million taxi rides!!!!\nYou might have noticed that the yellow_old table has two different datetime variables (one for pickup, the other for dropoff). We can use the information to assess the length of each ride (in time, not distance). However, the variables are stored in SQL as character strings instead of in a DateTime format (even though they look like they are stored in a DateTime format!), see Table 3.2. Fortunately for us, SQL has functionality to convert a text Type into DateTime type (POSIXct is a special type of DateTime formatting).\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old\n   LIMIT 0, 10;\n\n\n\n\n\nTable 3.5: Convert the pickup and dropoff times to date objects using STR_TO_DATE.\n\npickup_datetime\ndropoff_datetime\npickup\ndropoff\n\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n\n\n\n\n\n\n\n\nNow that the variables are no longer strings, we can subtract them to figure out the number of minutes for each taxi ride. Unfortunately, the following code won’t run because neither of the variables pickup or dropoff are in the table yellow_old.\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff.\n      TIMEDIFF(pickup, dropoff) AS length_time\n   FROM yellow_old\n   LIMIT 0, 10;\n\n\n?(caption)\n\n\n\nInstead, we need two layers of SELECT commands so that the first SELECT (i.e., inside) layer creates the new variables, and the second SELECT (i.e., outside) layer subtracts the two times.\n\nSELECT \n   pickup,\n   dropoff, \n   TIMEDIFF(pickup, dropoff) AS length_time \nFROM (\n   SELECT\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old)\n   AS subquery_table\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.6: Use TIMEDIFF to find the length (time) of the ride.\n\npickup\ndropoff\nlength_time\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n00:08:48\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n00:04:48\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n00:04:27\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n00:14:58\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n00:12:48\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n00:02:57\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n00:15:27\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n00:20:59\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n00:14:19\n\n\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n00:25:44\n\n\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n00:11:36\n\n\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n00:14:42\n\n\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n00:11:37\n\n\n2014-03-01 01:25:34\n2014-03-01 02:01:03\n00:35:29\n\n\n2014-03-01 01:26:39\n2014-03-01 01:30:03\n00:03:24\n\n\n2014-03-01 01:27:16\n2014-03-01 01:46:59\n00:19:43\n\n\n2014-03-01 01:28:39\n2014-03-01 01:30:53\n00:02:14\n\n\n2014-03-01 01:29:40\n2014-03-01 01:35:01\n00:05:21\n\n\n2014-03-01 01:28:51\n2014-03-01 01:43:06\n00:14:15\n\n\n\n\n\n\n\n\nAlternatively, the STR_TO_DATE() function can be applied inside the TIMEDIFF() function so that the full (now only) SELECT command is being used only on variables that are in the original table.\n\nSELECT \n   pickup_datetime,\n   dropoff_datetime, \n   TIMEDIFF(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\"), \n            STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\")) AS length_time \nFROM yellow_old\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.7: Alternative method to find the length (time) of the ride.\n\npickup_datetime\ndropoff_datetime\nlength_time\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n00:08:48\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n00:04:48\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n00:04:27\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n00:14:58\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n00:12:48\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n00:02:57\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n00:15:27\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n00:20:59\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n00:14:19\n\n\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n00:25:44\n\n\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n00:11:36\n\n\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n00:14:42\n\n\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n00:11:37\n\n\n2014-03-01 01:25:34\n2014-03-01 02:01:03\n00:35:29\n\n\n2014-03-01 01:26:39\n2014-03-01 01:30:03\n00:03:24\n\n\n2014-03-01 01:27:16\n2014-03-01 01:46:59\n00:19:43\n\n\n2014-03-01 01:28:39\n2014-03-01 01:30:53\n00:02:14\n\n\n2014-03-01 01:29:40\n2014-03-01 01:35:01\n00:05:21\n\n\n2014-03-01 01:28:51\n2014-03-01 01:43:06\n00:14:15\n\n\n\n\n\n\n\n\nKeep in mind that there is a distinction between clauses that operate on the variables of the original table versus those that operate on the variables of the results set. The variables pickup_datetime and dropoff_datetime are columns in the original table - they are written to disk on the SQL server. The variables pickup, dropoff, and length_time exist only in the results set, which is passed from the server (SQL server) to the client (e.g., RStudio or DBeaver) and is not written to disk."
  },
  {
    "objectID": "03-sql-verbs.html#where",
    "href": "03-sql-verbs.html#where",
    "title": "3  SQL clauses",
    "section": "\n3.4 WHERE",
    "text": "3.4 WHERE\n\n\n\n\n\n\nR function: filter()\n\n\n\nThe WHERE clause is analogous to the filter() function in dplyr. However, keep in mind that there are two SQL commands that resemble the dplyr filter() function. WHERE operates on the original data in the table and HAVING operates on the result set. See below for examples using HAVING.\nWhat was the fare for those taxi rides where the tip_amount was more than $10 and the person used cash? (Note that in SQL the equality logical is = and in R the equality logical is ==.)\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount &gt; 10\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.10: WHERE to subset the queried rows.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\nCSH\n65.5\n15.3\n91.8\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n55.0\n16.2\n81.2\n\n\nCSH\n71.5\n20.0\n103.5\n\n\nCSH\n70.0\n16.2\n97.1\n\n\nCSH\n95.0\n21.9\n131.2\n\n\nCSH\n62.5\n15.5\n93.0\n\n\nCSH\n66.0\n15.0\n90.0\n\n\nCSH\n65.0\n13.2\n79.2\n\n\n\n\n\n\n\n\nBETWEEN can be used to specify a range of values for a numeric value. BETWEEN is inclusive.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 12\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.11: BETWEEN in the WHERE clause.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n88.0\n10.0\n107.0\n\n\nCSH\n72.0\n10.0\n94.0\n\n\nCSH\n64.5\n10.0\n85.5\n\n\nCSH\n66.0\n12.0\n93.0\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n69.0\n10.0\n88.0\n\n\nCSH\n90.0\n10.0\n100.0\n\n\nCSH\n52.0\n11.6\n69.4\n\n\n\n\n\n\n\n\nIN is similar to the dplyr %in% function which specifies distinct values for the variable.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount IN (10, 12)\n   AND payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.12: IN in the WHERE clause.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\nCSH\n88.0\n10\n107.0\n\n\nCSH\n72.0\n10\n94.0\n\n\nCSH\n64.5\n10\n85.5\n\n\nCSH\n66.0\n12\n93.0\n\n\nCSH\n69.0\n10\n88.0\n\n\nCSH\n90.0\n10\n100.0\n\n\nCSH\n74.5\n10\n90.3\n\n\nCSH\n89.0\n10\n118.1\n\n\nCSH\n52.0\n10\n67.8\n\n\nCSH\n66.0\n12\n90.0\n\n\n\n\n\n\n\n\nThe WHERE clause can be established by a number of logical commands combined using either AND or OR. Usually it is important to use parentheses with OR logicals to make sure the desired query is return. Consider the difference between the following queries. In SQL (as in many programming languages), AND takes precedent over OR in the order of operations, when there are no parentheses. (I was taught to remember order of operations using “please excuse my dear aunt Sally.”) The order of operations on the first query groups the second two conditions into one because AND take precedence over OR (as if the query was tip_amount BETWEEN 10 and 12 OR (total_amount BETWEEN 100 and 112 AND payment_type = \"CSH\")).\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 12 OR \n      total_amount BETWEEN 100 and 112 AND \n      payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.13: OR and AND without parentheses.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\nCRD\n52.0\n10.5\n63.0\n\n\nCRD\n35.0\n10.2\n51.0\n\n\nCRD\n52.0\n11.6\n69.4\n\n\nCRD\n30.5\n10.8\n47.2\n\n\nCRD\n52.0\n10.5\n63.0\n\n\nCRD\n52.0\n10.5\n63.0\n\n\nCRD\n52.0\n11.6\n69.4\n\n\nCRD\n52.0\n11.6\n69.4\n\n\nCRD\n52.0\n11.6\n69.4\n\n\nCRD\n52.0\n11.6\n69.4\n\n\n\n\n\n\n\n\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE (tip_amount BETWEEN 10 and 12 OR \n      total_amount BETWEEN 100 and 112 ) AND \n      payment_type = \"CSH\"\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.14: OR and AND with parentheses.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\nCSH\n107.0\n0.0\n108.0\n\n\nCSH\n92.5\n0.0\n103.5\n\n\nCSH\n99.5\n0.0\n105.3\n\n\nCSH\n92.0\n0.0\n106.3\n\n\nCSH\n103.0\n0.0\n109.3\n\n\nCSH\n107.0\n0.0\n107.0\n\n\nCSH\n104.5\n0.0\n105.5\n\n\nCSH\n112.0\n0.0\n112.0\n\n\nCSH\n52.0\n11.6\n69.4\n\n\nCSH\n52.0\n11.6\n69.4\n\n\n\n\n\n\n\n\n\n3.4.1 NULL in WHERE\nSQL considers NULL values to be unknown. Therefore, when searching for a NULL value, you need to ask SQL if the value IS NULL. Asking if the value is equal to NULL doesn’t work because NULL values don’t equal anything (they are unknown). To keep all values that are not NULL values, use IS NOT NULL in the WHERE clause.\n\n\n\n\n\n\n Watch out!\n\n\n\nIn order to find the records that are NULL use WHERE variable IS NULL.\n\n\n\n3.4.1.1 A NULL example2\n\nThe logic of NULL:\n\nIf you do anything with NULL, you’ll just get NULL. For instance if \\(x\\) is NULL, then \\(x &gt; 3\\), \\(1 = x\\), and \\(x + 4\\) all evaluate to NULL. Even \\(x =\\) NULL evaluates to NULL! if you want to check whether \\(x\\) is NULL, use x IS NULL or x IS NOT NULL.\n\nNULL short-circuits with boolean operators. That means a boolean expression involving NULL will evaluate to:\n\nTRUE, if it’d evaluate to TRUE regardless of whether the NULL value is really TRUE or FALSE.\nFALSE, if it’d evaluate to FALSE regardless of whether the NULL value is really TRUE or FALSE.\nOr NULL, if it depends on the NULL value.\n\n\n\nConsider the following table and SQL query:\n\nSELECT * FROM (\n   SELECT 'Ace' AS name, 20 AS age, 4 as num_dogs\n   UNION\n   SELECT 'Ada' AS name, NULL AS age, 3 as num_dogs   \n   UNION\n   SELECT 'Ben' AS name, NULL AS age, NULL as num_dogs\n   UNION\n   SELECT 'Cho' AS name, 27 AS age, NULL as num_dogs\n   ) AS temptable;\n\n\n\n\n4 records\n\n\n\nname\n\n\nage\n\n\nnum_dogs\n\n\n\n\n\nAce\n\n\n20\n\n\n4\n\n\n\n\nAda\n\n\n\n\n3\n\n\n\n\nBen\n\n\n\n\n\n\n\n\nCho\n\n\n27\n\n\n\n\n\n\n\n\n\nSELECT * FROM (\n   SELECT 'Ace' AS name, 20 AS age, 4 as num_dogs\n   UNION\n   SELECT 'Ada' AS name, NULL AS age, 3 as num_dogs   \n   UNION\n   SELECT 'Ben' AS name, NULL AS age, NULL as num_dogs\n   UNION\n   SELECT 'Cho' AS name, 27 AS age, NULL as num_dogs\n   ) AS temptable\nWHERE age &lt;= 20 OR num_dogs = 3;\n\n\n\n\n2 records\n\n\n\nname\n\n\nage\n\n\nnum_dogs\n\n\n\n\n\nAce\n\n\n20\n\n\n4\n\n\n\n\nAda\n\n\n\n\n3\n\n\n\n\n\n\nWhere does the WHERE clause do? It tells us that we only want to keep the rows satisfying the age &lt;= 20 OR num_dogs = 3. Let’s consider each row one at a time:\n\nFor Ace, age &lt;= 20 evaluates to TRUE so the claim is satisfied.\nFor Ada, age &lt;= 20 evaluates to NULL but num_dogs = 3 evaluates to TRUE so the claim is satisfied.\nFor Ben, age &lt;= 20 evaluates to NULL and num_dogs = 3 evaluates to NULL so the overall expression is NULL which has a FALSE value.\nFor Cho, age &lt;= 20 evaluates to FALSE and num_dogs = 3 evaluates to NULL so the overall expression evaluates to NULL (because it depends on the value of the NULL).\n\nThus we keep only Ace and Ada.\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE payment_type IS NULL\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.15: There is ONE record with a NULL value for payment_type. Note that the way to find NULL values is via IS NULL.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSELECT payment_type, fare_amount, tip_amount, total_amount\nFROM yellow_old\nWHERE payment_type = NULL\nLIMIT 0, 10;\n\n\n\n\n\nTable 3.16: NO rows are selected when the WHERE command is specified to indicate if the variable equals NULL.\n\npayment_type\nfare_amount\ntip_amount\ntotal_amount"
  },
  {
    "objectID": "03-sql-verbs.html#group-by",
    "href": "03-sql-verbs.html#group-by",
    "title": "3  SQL clauses",
    "section": "\n3.5 GROUP BY",
    "text": "3.5 GROUP BY\n\n\n\n\n\n\nR function: group_by()\n\n\n\nThe GROUP BY clause will direct SQL to carry out the query separately for each category in the grouped variable. Using GROUP BY is particularly important when aggregating multiple rows into a single number. Some aggregate functions include COUNT(), SUM(), MAX(), MIN(), and AVG().\nNote that SUM(1) adds (sums) the number 1 for each row. Which is the same as counting the number of rows. SUM(2) adds (sums) the number 2 for each row which returns twice as many transactions.\n\nSELECT COUNT(*) AS num_transactions, \n       SUM(1) AS num_transactions_also,\n       SUM(2) AS double_transactions,\n       payment_type \nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 20\nGROUP BY payment_type;\n\n\n\n\n\nTable 3.17: GROUP BY on payment_type.\n\nnum_transactions\nnum_transactions_also\ndouble_transactions\npayment_type\n\n\n\n213872\n213872\n427744\nCRD\n\n\n78\n78\n156\nCSH\n\n\n3\n3\n6\nDIS\n\n\n7\n7\n14\nNOC\n\n\n609\n609\n1218\nUNK\n\n\n\n\n\n\n\n\nFor those people who tipped between $10 and $20, what was the lowest and highest fare for each of the types of payments?\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type \nFROM yellow_old\nWHERE tip_amount BETWEEN 10 and 20\nGROUP BY payment_type;\n\n\n\n\n\nTable 3.18: GROUP BY with aggregate functions.\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\n\n\n\n213872\n0.0\n370.0\nCRD\n\n\n78\n52.0\n102.0\nCSH\n\n\n3\n52.0\n79.5\nDIS\n\n\n7\n58.0\n94.0\nNOC\n\n\n609\n4.5\n147.0\nUNK\n\n\n\n\n\n\n\n\nGROUP BY will work applied to multiple columns. Let’s tabulate the same results, now broken down by payment_type and day of week. Except that we don’t have a day of week variable! We need to convert the pickup_datetime variable to a DateTime object and then pull out the day of the week, using DAYNAME. (Note: DAYOFWEEK will give you the day of the week as an integer. Use your internet sleuthing skills if you are looking for functions that might help your desired query.)\n\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday;\n\n\n\n\n\nTable 3.19: GROUP BY with payment_type and wday.\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\nwday\n\n\n\n1\n\n\n\n\n\n\n1247397\n2.5\n477\nCRD\nFriday\n\n\n1278362\n-612.4\n500\nCRD\nMonday\n\n\n1533796\n2.5\n420\nCRD\nSaturday\n\n\n1324394\n2.5\n480\nCRD\nSunday\n\n\n1258098\n2.5\n500\nCRD\nThursday\n\n\n1121081\n2.5\n500\nCRD\nTuesday\n\n\n1192892\n2.5\n400\nCRD\nWednesday\n\n\n860920\n2.5\n444\nCSH\nFriday\n\n\n918653\n0.0\n873\nCSH\nMonday\n\n\n1207305\n2.5\n350\nCSH\nSaturday\n\n\n1020438\n2.5\n425\nCSH\nSunday\n\n\n813813\n2.5\n475\nCSH\nThursday\n\n\n751769\n2.5\n300\nCSH\nTuesday\n\n\n775823\n2.5\n400\nCSH\nWednesday\n\n\n1592\n2.5\n255\nDIS\nFriday\n\n\n1537\n0.0\n102\nDIS\nMonday\n\n\n2236\n2.5\n200\nDIS\nSaturday\n\n\n1821\n2.5\n200\nDIS\nSunday\n\n\n1357\n2.5\n165\nDIS\nThursday\n\n\n1222\n2.5\n475\nDIS\nTuesday\n\n\n1295\n2.5\n373\nDIS\nWednesday\n\n\n5252\n2.5\n229\nNOC\nFriday\n\n\n5440\n0.0\n950\nNOC\nMonday\n\n\n7217\n2.5\n295\nNOC\nSaturday\n\n\n6383\n2.5\n300\nNOC\nSunday\n\n\n4840\n2.5\n223\nNOC\nThursday\n\n\n4123\n2.5\n384\nNOC\nTuesday\n\n\n4482\n2.5\n200\nNOC\nWednesday\n\n\n10131\n2.5\n130\nUNK\nFriday\n\n\n11263\n2.5\n95\nUNK\nMonday\n\n\n12813\n2.5\n147\nUNK\nSaturday\n\n\n11003\n2.5\n114\nUNK\nSunday\n\n\n10197\n2.5\n200\nUNK\nThursday\n\n\n9643\n2.5\n138\nUNK\nTuesday\n\n\n9539\n2.5\n133\nUNK\nWednesday"
  },
  {
    "objectID": "03-sql-verbs.html#order-by",
    "href": "03-sql-verbs.html#order-by",
    "title": "3  SQL clauses",
    "section": "\n3.6 ORDER BY",
    "text": "3.6 ORDER BY\n\n\n\n\n\n\nR function: arrange()\n\n\n\nThe ORDER BY command can be used with or without the GROUP BY and aggregation commands. It allows us to look at interesting aspects of the data by sorting the data.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY lowest_fare ASC;\n\n\n\n\n\nTable 3.20: ORDER BY lowest_fare, ascending.\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\nwday\n\n\n\n1\n\n\n\n\n\n\n1278362\n-612.4\n500\nCRD\nMonday\n\n\n918653\n0.0\n873\nCSH\nMonday\n\n\n5440\n0.0\n950\nNOC\nMonday\n\n\n1537\n0.0\n102\nDIS\nMonday\n\n\n1533796\n2.5\n420\nCRD\nSaturday\n\n\n1121081\n2.5\n500\nCRD\nTuesday\n\n\n1192892\n2.5\n400\nCRD\nWednesday\n\n\n1258098\n2.5\n500\nCRD\nThursday\n\n\n1247397\n2.5\n477\nCRD\nFriday\n\n\n1324394\n2.5\n480\nCRD\nSunday\n\n\n813813\n2.5\n475\nCSH\nThursday\n\n\n1207305\n2.5\n350\nCSH\nSaturday\n\n\n751769\n2.5\n300\nCSH\nTuesday\n\n\n775823\n2.5\n400\nCSH\nWednesday\n\n\n1020438\n2.5\n425\nCSH\nSunday\n\n\n7217\n2.5\n295\nNOC\nSaturday\n\n\n860920\n2.5\n444\nCSH\nFriday\n\n\n1295\n2.5\n373\nDIS\nWednesday\n\n\n1821\n2.5\n200\nDIS\nSunday\n\n\n4123\n2.5\n384\nNOC\nTuesday\n\n\n1222\n2.5\n475\nDIS\nTuesday\n\n\n5252\n2.5\n229\nNOC\nFriday\n\n\n6383\n2.5\n300\nNOC\nSunday\n\n\n4840\n2.5\n223\nNOC\nThursday\n\n\n4482\n2.5\n200\nNOC\nWednesday\n\n\n1357\n2.5\n165\nDIS\nThursday\n\n\n1592\n2.5\n255\nDIS\nFriday\n\n\n2236\n2.5\n200\nDIS\nSaturday\n\n\n11003\n2.5\n114\nUNK\nSunday\n\n\n12813\n2.5\n147\nUNK\nSaturday\n\n\n11263\n2.5\n95\nUNK\nMonday\n\n\n9643\n2.5\n138\nUNK\nTuesday\n\n\n9539\n2.5\n133\nUNK\nWednesday\n\n\n10197\n2.5\n200\nUNK\nThursday\n\n\n10131\n2.5\n130\nUNK\nFriday\n\n\n\n\n\n\n\n\nWHAT?!?!! How in the world was one of the fares -$612.40? It doesn’t make any sense that a fare would be negative. Some additional inquiry into the observation corresponding to a fare of -$612.40 is absolutely warranted. If the observation is found to be a typo, it would need to be removed from the data set. If the observation is somehow legitimate, it would need to be included in the analysis, with the information provided about its legitimacy.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY highest_fare DESC;\n\n\n\n\n\nTable 3.21: ORDER BY highest_fare, descending\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\nwday\n\n\n\n5440\n0.0\n950\nNOC\nMonday\n\n\n918653\n0.0\n873\nCSH\nMonday\n\n\n1278362\n-612.4\n500\nCRD\nMonday\n\n\n1121081\n2.5\n500\nCRD\nTuesday\n\n\n1258098\n2.5\n500\nCRD\nThursday\n\n\n1324394\n2.5\n480\nCRD\nSunday\n\n\n1247397\n2.5\n477\nCRD\nFriday\n\n\n1222\n2.5\n475\nDIS\nTuesday\n\n\n813813\n2.5\n475\nCSH\nThursday\n\n\n860920\n2.5\n444\nCSH\nFriday\n\n\n1020438\n2.5\n425\nCSH\nSunday\n\n\n1533796\n2.5\n420\nCRD\nSaturday\n\n\n1192892\n2.5\n400\nCRD\nWednesday\n\n\n775823\n2.5\n400\nCSH\nWednesday\n\n\n4123\n2.5\n384\nNOC\nTuesday\n\n\n1295\n2.5\n373\nDIS\nWednesday\n\n\n1207305\n2.5\n350\nCSH\nSaturday\n\n\n751769\n2.5\n300\nCSH\nTuesday\n\n\n6383\n2.5\n300\nNOC\nSunday\n\n\n7217\n2.5\n295\nNOC\nSaturday\n\n\n1592\n2.5\n255\nDIS\nFriday\n\n\n5252\n2.5\n229\nNOC\nFriday\n\n\n4840\n2.5\n223\nNOC\nThursday\n\n\n4482\n2.5\n200\nNOC\nWednesday\n\n\n1821\n2.5\n200\nDIS\nSunday\n\n\n2236\n2.5\n200\nDIS\nSaturday\n\n\n10197\n2.5\n200\nUNK\nThursday\n\n\n1357\n2.5\n165\nDIS\nThursday\n\n\n12813\n2.5\n147\nUNK\nSaturday\n\n\n9643\n2.5\n138\nUNK\nTuesday\n\n\n9539\n2.5\n133\nUNK\nWednesday\n\n\n10131\n2.5\n130\nUNK\nFriday\n\n\n11003\n2.5\n114\nUNK\nSunday\n\n\n1537\n0.0\n102\nDIS\nMonday\n\n\n11263\n2.5\n95\nUNK\nMonday\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n$950 is a lot to pay for a cab ride! But in NYC, I’d believe it.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nORDER BY wday, payment_type;\n\n\n\n\n\nTable 3.22: ORDER BY wday and payment_type.\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\nwday\n\n\n\n1\n\n\n\n\n\n\n1247397\n2.5\n477\nCRD\nFriday\n\n\n860920\n2.5\n444\nCSH\nFriday\n\n\n1592\n2.5\n255\nDIS\nFriday\n\n\n5252\n2.5\n229\nNOC\nFriday\n\n\n10131\n2.5\n130\nUNK\nFriday\n\n\n1278362\n-612.4\n500\nCRD\nMonday\n\n\n918653\n0.0\n873\nCSH\nMonday\n\n\n1537\n0.0\n102\nDIS\nMonday\n\n\n5440\n0.0\n950\nNOC\nMonday\n\n\n11263\n2.5\n95\nUNK\nMonday\n\n\n1533796\n2.5\n420\nCRD\nSaturday\n\n\n1207305\n2.5\n350\nCSH\nSaturday\n\n\n2236\n2.5\n200\nDIS\nSaturday\n\n\n7217\n2.5\n295\nNOC\nSaturday\n\n\n12813\n2.5\n147\nUNK\nSaturday\n\n\n1324394\n2.5\n480\nCRD\nSunday\n\n\n1020438\n2.5\n425\nCSH\nSunday\n\n\n1821\n2.5\n200\nDIS\nSunday\n\n\n6383\n2.5\n300\nNOC\nSunday\n\n\n11003\n2.5\n114\nUNK\nSunday\n\n\n1258098\n2.5\n500\nCRD\nThursday\n\n\n813813\n2.5\n475\nCSH\nThursday\n\n\n1357\n2.5\n165\nDIS\nThursday\n\n\n4840\n2.5\n223\nNOC\nThursday\n\n\n10197\n2.5\n200\nUNK\nThursday\n\n\n1121081\n2.5\n500\nCRD\nTuesday\n\n\n751769\n2.5\n300\nCSH\nTuesday\n\n\n1222\n2.5\n475\nDIS\nTuesday\n\n\n4123\n2.5\n384\nNOC\nTuesday\n\n\n9643\n2.5\n138\nUNK\nTuesday\n\n\n1192892\n2.5\n400\nCRD\nWednesday\n\n\n775823\n2.5\n400\nCSH\nWednesday\n\n\n1295\n2.5\n373\nDIS\nWednesday\n\n\n4482\n2.5\n200\nNOC\nWednesday\n\n\n9539\n2.5\n133\nUNK\nWednesday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that both GROUP BY and ORDER BY evaluate the data after it has been retrieved. Therefore, the functions operate on the results set, not the original rows of the data.\n\n\n\nAs above, we were able to GROUP BY and ORDER BY on the new variables we had created, wday."
  },
  {
    "objectID": "03-sql-verbs.html#having",
    "href": "03-sql-verbs.html#having",
    "title": "3  SQL clauses",
    "section": "\n3.7 HAVING",
    "text": "3.7 HAVING\n\n\n\n\n\n\nR function: filter()\n\n\n\nRecall that WHERE acts only on the original data. If we are interested in rides that took place on Friday, we need to use the derived variable wday instead of the raw variable pickup_datetime. Fortunately, HAVING works on the results set. Note that SQL uses '' for strings, not \"\". In SQL, \"\" is used to identify variables (not values of variables), like R’s &grave;&grave;.\n\nSELECT COUNT(*) AS num_transactions, \n       MIN(fare_amount) AS lowest_fare,\n       MAX(fare_amount) AS highest_fare,\n       payment_type,\n       DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old\nGROUP BY payment_type, wday\nHAVING wday = 'Friday';\n\n\n\n\n\nTable 3.23: HAVING to filter only Friday rides.\n\nnum_transactions\nlowest_fare\nhighest_fare\npayment_type\nwday\n\n\n\n1247397\n2.5\n477\nCRD\nFriday\n\n\n860920\n2.5\n444\nCSH\nFriday\n\n\n1592\n2.5\n255\nDIS\nFriday\n\n\n5252\n2.5\n229\nNOC\nFriday\n\n\n10131\n2.5\n130\nUNK\nFriday\n\n\n\n\n\n\n\n\nWhile it worked out quite well for us that HAVING was able to filter the data based on the results set, the use of HAVING was quite onerous because the entire data set was considered before the filter was applied. That is, if the filter can be done on the original data using WHERE, the query will be much faster and more efficient.\nNote: HAVING requires a GROUP BY clause. And the variable(s) used in HAVING must also be part of the GROUP BY clause.\n\n\n\n\n\n\nWhenever possible, use WHERE instead of HAVING to make your queries as efficient as possible."
  },
  {
    "objectID": "03-sql-verbs.html#limit",
    "href": "03-sql-verbs.html#limit",
    "title": "3  SQL clauses",
    "section": "\n3.8 LIMIT",
    "text": "3.8 LIMIT\n\n\n\n\n\n\nR function: head() or slice()\n\n\n\nAs we’ve seen, LIMIT truncates the query to specified rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14). The query below shows the last 10 rows of the entire data set.\n\nSELECT * FROM yellow_old LIMIT 15428118, 10;\n\n\n\n\n\nTable 3.24: LIMIT on the last 10 rows of the table.\n\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\ntrip_distance\npickup_longitude\npickup_latitude\nrate_code\nstore_and_fwd_flag\ndropoff_longitude\ndropoff_latitude\npayment_type\nfare_amount\nsurcharge\nmta_tax\ntip_amount\ntolls_amount\ntotal_amount\n\n\n\nCMT\n2014-03-18 14:35:21\n2014-03-18 14:52:01\n1\n2.7\n0\n0.0\n1\nN\n0\n0.0\nCRD\n13.0\n0\n0.5\n2.50\n0.00\n16.0\n\n\nCMT\n2014-03-18 14:08:23\n2014-03-18 14:19:29\n2\n1.3\n0\n0.0\n1\nN\n0\n0.0\nCRD\n9.0\n0\n0.5\n1.90\n0.00\n11.4\n\n\nCMT\n2014-03-18 09:18:38\n2014-03-18 09:19:41\n1\n0.2\n-74\n40.8\n1\nN\n-74\n40.8\nCRD\n3.0\n0\n0.5\n1.00\n0.00\n4.5\n\n\nCMT\n2014-03-18 06:28:12\n2014-03-18 06:49:49\n1\n9.9\n0\n0.0\n1\nN\n0\n0.0\nCRD\n30.0\n0\n0.5\n7.16\n5.33\n43.0\n\n\nCMT\n2014-03-18 17:39:28\n2014-03-18 17:53:01\n1\n4.9\n-74\n40.8\n1\nN\n-74\n40.7\nCRD\n16.5\n1\n0.5\n3.00\n0.00\n21.0\n\n\nCMT\n2014-03-18 18:14:19\n2014-03-18 18:27:22\n1\n0.3\n-74\n40.7\n1\nN\n-74\n40.7\nCRD\n3.5\n1\n0.5\n6.00\n0.00\n11.0\n\n\nCMT\n2014-03-18 10:12:33\n2014-03-18 10:28:09\n1\n3.3\n-74\n40.8\n1\nN\n-74\n40.8\nCRD\n13.5\n0\n0.5\n4.20\n0.00\n18.2\n\n\nCMT\n2014-03-18 09:02:37\n2014-03-18 09:16:29\n1\n6.2\n-74\n40.8\n1\nN\n-74\n40.7\nCRD\n19.5\n0\n0.5\n1.00\n0.00\n21.0\n\n\nCMT\n2014-03-18 10:10:19\n2014-03-18 10:19:25\n1\n1.7\n-74\n40.8\n1\nN\n-74\n40.8\nCRD\n8.5\n0\n0.5\n1.80\n0.00\n10.8\n\n\nCMT\n2014-03-18 15:24:53\n2014-03-18 15:42:42\n1\n1.7\n-74\n40.8\n1\nN\n-74\n40.7\nCRD\n12.5\n0\n0.5\n2.60\n0.00\n15.6"
  },
  {
    "objectID": "03-sql-verbs.html#footnotes",
    "href": "03-sql-verbs.html#footnotes",
    "title": "3  SQL clauses",
    "section": "",
    "text": "Taken directly from Modern Data Science with R↩︎\ntaken from: https://cs186berkeley.net/notes/note1/#filtering-null-values↩︎"
  },
  {
    "objectID": "03-sql-verbs.html#section",
    "href": "03-sql-verbs.html#section",
    "title": "3  SQL verbs",
    "section": "\n3.3 ",
    "text": "3.3 \nR function: select()\n:::\nThe shortest SQL query is the following SELECT command. Do not run this command!!! The yellow_old table has 15 million rows, and we do not want to look at them simultaneously.\n\nDO NOT RUN:  SELECT * FROM yellow_old;\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDo not run the following command unless you are certain that the table from which you are querying is small enough so that the query results fit easily into your memory.\nSELECT * FROM table;\n\n\nInstead, to look at the top of the table, SELECT the first few rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14).\n\nSELECT * FROM yellow_old LIMIT 0, 14;\n\n\n\n\nDisplaying records 1 - 10\n\n\n\nvendor_id\n\n\npickup_datetime\n\n\ndropoff_datetime\n\n\npassenger_count\n\n\ntrip_distance\n\n\npickup_longitude\n\n\npickup_latitude\n\n\nrate_code\n\n\nstore_and_fwd_flag\n\n\ndropoff_longitude\n\n\ndropoff_latitude\n\n\npayment_type\n\n\nfare_amount\n\n\nsurcharge\n\n\nmta_tax\n\n\ntip_amount\n\n\ntolls_amount\n\n\ntotal_amount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMT\n\n\n2014-03-01 01:07:38\n\n\n2014-03-01 01:16:26\n\n\n1\n\n\n2.0\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-73.9\n\n\n40.7\n\n\nCRD\n\n\n9.0\n\n\n0.5\n\n\n0.5\n\n\n2.0\n\n\n0\n\n\n12.0\n\n\n\n\nCMT\n\n\n2014-03-01 01:08:03\n\n\n2014-03-01 01:12:51\n\n\n2\n\n\n1.2\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n6.0\n\n\n0.5\n\n\n0.5\n\n\n1.0\n\n\n0\n\n\n8.0\n\n\n\n\nCMT\n\n\n2014-03-01 01:08:51\n\n\n2014-03-01 01:13:18\n\n\n3\n\n\n0.5\n\n\n-73.9\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n5.0\n\n\n0.5\n\n\n0.5\n\n\n1.2\n\n\n0\n\n\n7.2\n\n\n\n\nCMT\n\n\n2014-03-01 01:09:20\n\n\n2014-03-01 01:24:18\n\n\n3\n\n\n3.5\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.8\n\n\nCRD\n\n\n14.0\n\n\n0.5\n\n\n0.5\n\n\n3.0\n\n\n0\n\n\n18.0\n\n\n\n\nCMT\n\n\n2014-03-01 01:09:46\n\n\n2014-03-01 01:22:34\n\n\n1\n\n\n1.8\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n10.5\n\n\n0.5\n\n\n0.5\n\n\n1.0\n\n\n0\n\n\n12.5\n\n\n\n\nCMT\n\n\n2014-03-01 01:12:41\n\n\n2014-03-01 01:15:38\n\n\n1\n\n\n0.5\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n4.0\n\n\n0.5\n\n\n0.5\n\n\n0.5\n\n\n0\n\n\n5.5\n\n\n\n\nCMT\n\n\n2014-03-01 01:12:11\n\n\n2014-03-01 01:27:38\n\n\n2\n\n\n3.7\n\n\n-74.0\n\n\n40.8\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n14.5\n\n\n0.5\n\n\n0.5\n\n\n3.1\n\n\n0\n\n\n18.6\n\n\n\n\nCMT\n\n\n2014-03-01 01:13:55\n\n\n2014-03-01 01:34:54\n\n\n1\n\n\n5.4\n\n\n-74.0\n\n\n40.8\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.7\n\n\nCRD\n\n\n20.0\n\n\n0.5\n\n\n0.5\n\n\n3.0\n\n\n0\n\n\n24.0\n\n\n\n\nCMT\n\n\n2014-03-01 01:14:06\n\n\n2014-03-01 01:28:25\n\n\n1\n\n\n3.5\n\n\n-74.0\n\n\n40.7\n\n\n1\n\n\nN\n\n\n-74.0\n\n\n40.8\n\n\nCRD\n\n\n13.5\n\n\n0.5\n\n\n0.5\n\n\n2.9\n\n\n0\n\n\n17.4\n\n\n\n\n\n\n\n\n\n\nSELECT COUNT(*) FROM yellow_old;\n\n\n\n\n1 records\n\n\n\nCOUNT(*)\n\n\n\n\n15428128"
  },
  {
    "objectID": "03-sql-verbs.html#sec-select",
    "href": "03-sql-verbs.html#sec-select",
    "title": "3  SQL clauses",
    "section": "\n3.2 SELECT … FROM",
    "text": "3.2 SELECT … FROM\n\n\n\n\n\n\nR function: select()\n\n\n\nA SQL query starts with a SELECT command and has a corresponding FROM to indicate the table being queried. Columns may be specified, or the * will indicate that every column in the table should be returned.\nThe shortest SQL query is the following SELECT command. Do not run this command!!! The yellow_old table has 15 million rows, and we do not want to look at them simultaneously.\n\nDO NOT RUN:  SELECT * FROM yellow_old;\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDo not run the following command unless you are certain that the table from which you are querying is small enough so that the query results fit easily into your memory.\nSELECT * FROM table;\n\n\nInstead, to look at the top of the table, SELECT the first few rows. The LIMIT command specifies which rows to select: the first number is the number of rows to skip (0 rows skipped), the second number is the number of rows to print up to (up to row 14).\n\nSELECT * FROM yellow_old LIMIT 0, 14;\n\n\n\n\n\nTable 3.3: SELECT the first 14 rows of the table.\n\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\ntrip_distance\npickup_longitude\npickup_latitude\nrate_code\nstore_and_fwd_flag\ndropoff_longitude\ndropoff_latitude\npayment_type\nfare_amount\nsurcharge\nmta_tax\ntip_amount\ntolls_amount\ntotal_amount\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMT\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n1\n2.0\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n9.0\n0.5\n0.5\n2.0\n0\n12.0\n\n\nCMT\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.0\n0\n8.0\n\n\nCMT\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n3\n0.5\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.2\n0\n7.2\n\n\nCMT\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n3\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.5\n0.5\n3.0\n0\n18.0\n\n\nCMT\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.0\n0\n12.5\n\n\nCMT\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n1\n0.5\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n4.0\n0.5\n0.5\n0.5\n0\n5.5\n\n\nCMT\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n2\n3.7\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n14.5\n0.5\n0.5\n3.1\n0\n18.6\n\n\nCMT\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n1\n5.4\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n20.0\n0.5\n0.5\n3.0\n0\n24.0\n\n\nCMT\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n1\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n13.5\n0.5\n0.5\n2.9\n0\n17.4\n\n\nCMT\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n3\n5.9\n-74.0\n40.8\n1\nN\n-74.0\n40.7\nCRD\n21.5\n0.5\n0.5\n2.0\n0\n24.5\n\n\nCMT\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n1\n1.9\n-74.0\n40.8\n1\nN\n-74.0\n40.8\nCRD\n10.0\n0.5\n0.5\n2.2\n0\n13.2\n\n\nCMT\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n3\n3.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n13.0\n0.5\n0.5\n2.8\n0\n16.8\n\n\nCMT\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n1\n2.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n11.0\n0.5\n0.5\n1.5\n0\n13.5\n\n\n\n\n\n\n\n\nSpeaking of which, how many rows are there in the yellow_old table? That is, how many taxi rides are recorded? Now SELECT is used with a summary function, COUNT(). Instead of using a separate summary function (like mutate() or summarize()), all the work is done inside the SELECT call.\n\nSELECT COUNT(*) FROM yellow_old;\n\n\n\n\n\nTable 3.4: COUNT(*) the number of rows in the entire yellow_old table.\n\nCOUNT(*)\n\n\n15428128\n\n\n\n\n\n\n\nYikes, more than 15 million taxi rides!!!!\nYou might have noticed that the yellow_old table has two different datetime variables (one for pickup, the other for drop-off). We can use the information to assess the length of each ride (in time, not distance). However, the variables are stored in SQL as character strings instead of in a DateTime format (even though they look like they are stored in a DateTime format!), see Table 3.2. Fortunately for us, SQL has functionality to convert a text Type into DateTime type (POSIXct is a special type of DateTime formatting).\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old\n   LIMIT 0, 10;\n\n\n\n\n\nTable 3.5: Convert the pickup and drop-off times to date objects using STR_TO_DATE.\n\npickup_datetime\ndropoff_datetime\npickup\ndropoff\n\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n\n\n\n\n\n\n\n\nNow that the variables are no longer strings, we can subtract them to figure out the number of minutes for each taxi ride. Unfortunately, the following code won’t run because neither of the variables pickup or dropoff are in the table yellow_old.\n\nSELECT\n      pickup_datetime, dropoff_datetime,\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff.\n      TIMEDIFF(pickup, dropoff) AS length_time\n   FROM yellow_old\n   LIMIT 0, 10;\n\nInstead, we need two layers of SELECT commands so that the first SELECT (i.e., inside) layer creates the new variables, and the second SELECT (i.e., outside) layer subtracts the two times.\n\nSELECT \n   pickup,\n   dropoff, \n   TIMEDIFF(pickup, dropoff) AS length_time \nFROM (\n   SELECT\n      STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\") AS pickup,\n      STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\") AS dropoff\n   FROM yellow_old)\n   AS subquery_table\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.6: Use TIMEDIFF to find the length (time) of the ride.\n\npickup\ndropoff\nlength_time\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n00:08:48\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n00:04:48\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n00:04:27\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n00:14:58\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n00:12:48\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n00:02:57\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n00:15:27\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n00:20:59\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n00:14:19\n\n\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n00:25:44\n\n\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n00:11:36\n\n\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n00:14:42\n\n\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n00:11:37\n\n\n2014-03-01 01:25:34\n2014-03-01 02:01:03\n00:35:29\n\n\n2014-03-01 01:26:39\n2014-03-01 01:30:03\n00:03:24\n\n\n2014-03-01 01:27:16\n2014-03-01 01:46:59\n00:19:43\n\n\n2014-03-01 01:28:39\n2014-03-01 01:30:53\n00:02:14\n\n\n2014-03-01 01:29:40\n2014-03-01 01:35:01\n00:05:21\n\n\n2014-03-01 01:28:51\n2014-03-01 01:43:06\n00:14:15\n\n\n\n\n\n\n\n\nAlternatively, the STR_TO_DATE() function can be applied inside the TIMEDIFF() function so that the full (now only) SELECT command is being used only on variables that are in the original table.\n\nSELECT \n   pickup_datetime,\n   dropoff_datetime, \n   TIMEDIFF(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\"), \n            STR_TO_DATE(dropoff_datetime, \"%Y-%m-%d %T\")) AS length_time \nFROM yellow_old\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.7: Alternative method to find the length (time) of the ride.\n\npickup_datetime\ndropoff_datetime\nlength_time\n\n\n\n\n\n\n\n\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n00:08:48\n\n\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n00:04:48\n\n\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n00:04:27\n\n\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n00:14:58\n\n\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n00:12:48\n\n\n2014-03-01 01:12:41\n2014-03-01 01:15:38\n00:02:57\n\n\n2014-03-01 01:12:11\n2014-03-01 01:27:38\n00:15:27\n\n\n2014-03-01 01:13:55\n2014-03-01 01:34:54\n00:20:59\n\n\n2014-03-01 01:14:06\n2014-03-01 01:28:25\n00:14:19\n\n\n2014-03-01 01:13:10\n2014-03-01 01:38:54\n00:25:44\n\n\n2014-03-01 01:14:13\n2014-03-01 01:25:49\n00:11:36\n\n\n2014-03-01 01:15:22\n2014-03-01 01:30:04\n00:14:42\n\n\n2014-03-01 01:16:28\n2014-03-01 01:28:05\n00:11:37\n\n\n2014-03-01 01:25:34\n2014-03-01 02:01:03\n00:35:29\n\n\n2014-03-01 01:26:39\n2014-03-01 01:30:03\n00:03:24\n\n\n2014-03-01 01:27:16\n2014-03-01 01:46:59\n00:19:43\n\n\n2014-03-01 01:28:39\n2014-03-01 01:30:53\n00:02:14\n\n\n2014-03-01 01:29:40\n2014-03-01 01:35:01\n00:05:21\n\n\n2014-03-01 01:28:51\n2014-03-01 01:43:06\n00:14:15\n\n\n\n\n\n\n\n\nKeep in mind that there is a distinction between clauses that operate on the variables of the original table versus those that operate on the variables of the results set. The variables pickup_datetime and dropoff_datetime are columns in the original table - they are written to disk on the SQL server. The variables pickup, dropoff, and length_time exist only in the results set, which is passed from the server (SQL server) to the client (e.g., RStudio or DBeaver) and is not written to disk."
  },
  {
    "objectID": "04-sql-joins.html#join",
    "href": "04-sql-joins.html#join",
    "title": "4  Combining tables in SQL",
    "section": "\n4.1 JOIN\n",
    "text": "4.1 JOIN\n\nRecall that SQL is a programming language that works on relational databases. One of its major strengths is being able to efficiently store information in separate tables that can be easily connected as needed. The syntax for tying together information from multiple tables is done with a JOIN clause.\nEach JOIN clause needs four specific pieces of information:\n\nThe name of the first table you want to JOIN.\nThe type of JOIN being used.\nThe name of the second table you want to JOIN.\nThe condition(s) under which you want the records in the first table to match records in the second table.\n\nSome types of JOINs available in MySQL include the following, which are represented as Venn diagrams in Figure 4.1.\n\n\nJOIN: include all of the rows that exist in both tables (similar to inner_join() in R, the intersection of the two tables). INNER JOIN is alternative, and identical, function to JOIN.\n\nLEFT JOIN: include all of the rows in the first table. Connect them, as much as possible, to the rows in the second table. Rows that have no match in the second table will have a value of NULL for the new “second table” variables.\n\nRIGHT JOIN: include all of the rows in the second table. Connect them, as much as possible, to the rows in the first table. Rows that have no match in the first table will have a value of NULL for the new “first table” variables. A RIGHT JOIN with the tables in the opposite order is the same as a LEFT JOIN with the tables in the original order.\n\nFULL OUTER JOIN: include all rows in either table. Rows that have no match in the other table will have a value of NULL for the other table variables.\n\nCROSS JOIN: match each row of the first table with each row in the second table.\n\nFigure 4.1 shows Venn diagrams of the different types of joins. Figure 4.2 shows four of the JOIN functions with mini data tables. Note that in SQL the missing values will be labeled as NULL (not NA).\n\n\n\n\nFigure 4.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\n\n\n\n\n\n\n\nFigure 4.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti"
  },
  {
    "objectID": "04-sql-joins.html#union",
    "href": "04-sql-joins.html#union",
    "title": "4  Combining tables in SQL",
    "section": "\n4.3 UNION\n",
    "text": "4.3 UNION\n\n\n\n\n\nFigure 4.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html"
  },
  {
    "objectID": "04-sql-joins.html#subqueries",
    "href": "04-sql-joins.html#subqueries",
    "title": "4  Combining tables in SQL",
    "section": "\n4.1 Subqueries",
    "text": "4.1 Subqueries\nA SQL subquery is a query used as a data source in the FROM clause, instead of the usual table. There was a subquery in Table 3.6 when the task required a function of the results set within the SELECT clause.\nWe could do something similar if we wanted to transform the variables in the select column. The example is a little bit forced, and there are other ways to obtain the same results. But hopefully the idea of a subquery is becoming more clear. Again, a subquery is just a query that becomes the data source for FROM.\nChapter 8 will cover regular expressions in some detail. Here we use the function REGEXP_REPLACE to remove any characters which are not letters, comma, or space. The function LOWER converts any upper case letters to lower case.\n\nSELECT name,\n       name_clean,\n       SUBSTRING_INDEX(name_clean, ',', 1) AS last_name,\n       SUBSTRING_INDEX(name_clean, ',', -1) AS first_name\nFROM (\nSELECT LOWER(REGEXP_REPLACE(name, '[^a-z,. ]', '')) AS name_clean,\n       name,\n       id, person_id\nFROM aka_name) AS temp_subquery\nLIMIT 0, 30;\n\n\n\n\n\nTable 4.3: A subquery is used so that the variable in the subquery can be used and transformed in the SELECT clause.\n\nname\nname_clean\nlast_name\nfirst_name\n\n\n\nSmith, Jessica Noel\nsmith, jessica noel\nsmith\njessica noel\n\n\nPain, L. $ham\npain, l. ham\npain\nl. ham\n\n\nBoy, $hutter\nboy, hutter\nboy\nhutter\n\n\nDollasign, Ty\ndollasign, ty\ndollasign\nty\n\n\nSign, Ty Dolla\nsign, ty dolla\nsign\nty dolla\n\n\nMoore, Brandon\nmoore, brandon\nmoore\nbrandon\n\n\n$torm, Country\ntorm, country\ntorm\ncountry\n\n\n'Hooper', Simon P.J. Kelly\nhooper, simon p.j. kelly\nhooper\nsimon p.j. kelly\n\n\nHooper\nhooper\nhooper\nhooper\n\n\nKelly, Simon P.J.\nkelly, simon p.j.\nkelly\nsimon p.j.\n\n\nAbdul-Hamid, Jaffar\nabdulhamid, jaffar\nabdulhamid\njaffar\n\n\nAl-Hamid, Jaffar Abd\nalhamid, jaffar abd\nalhamid\njaffar abd\n\n\nSvensson, Acke\nsvensson, acke\nsvensson\nacke\n\n\nViera, Michael 'Power'\nviera, michael power\nviera\nmichael power\n\n\nBuguelo\nbuguelo\nbuguelo\nbuguelo\n\n\n'El Burro' Rankin', Jorge Van\nel burro rankin, jorge van\nel burro rankin\njorge van\n\n\nBurro, El\nburro, el\nburro\nel\n\n\nVan Rankin, Jorge 'Burro'\nvan rankin, jorge burro\nvan rankin\njorge burro\n\n\nVan Rankin, Jorge\nvan rankin, jorge\nvan rankin\njorge\n\n\nvan Rankin, Jorge 'El Burro'\nvan rankin, jorge el burro\nvan rankin\njorge el burro\n\n\nSeigal, Jason\nseigal, jason\nseigal\njason\n\n\nKaufman, Murray\nkaufman, murray\nkaufman\nmurray\n\n\n'Knoccout'Madison, Kareim\nknoccoutmadison, kareim\nknoccoutmadison\nkareim\n\n\nStarks, Johnny\nstarks, johnny\nstarks\njohnny\n\n\nKraemer, 'Logan' Howard\nkraemer, logan howard\nkraemer\nlogan howard\n\n\nGee, Emm\ngee, emm\ngee\nemm\n\n\nCusick, Maura\ncusick, maura\ncusick\nmaura\n\n\nMaura, Maude Cusick\nmaura, maude cusick\nmaura\nmaude cusick\n\n\nWheeler, Mackenzie\nwheeler, mackenzie\nwheeler\nmackenzie\n\n\nMonkey\nmonkey\nmonkey\nmonkey"
  },
  {
    "objectID": "04-sql-joins.html#all-the-joins",
    "href": "04-sql-joins.html#all-the-joins",
    "title": "4  Combining tables in SQL",
    "section": "\n4.2 All the JOINs",
    "text": "4.2 All the JOINs\nRecall that SQL is a query language that works on relational databases. One of its major strengths is being able to efficiently store information in separate tables that can be easily connected as needed. The syntax for tying together information from multiple tables is done with a JOIN clause.\nEach JOIN clause needs four specific pieces of information:\n\nThe name of the first table you want to JOIN.\nThe type of JOIN being used.\nThe name of the second table you want to JOIN.\nThe condition(s) under which you want the records in the first table to match records in the second table.\n\nSome types of JOINs available in MySQL include the following, which are represented as Venn diagrams in Figure 4.1.\n\n\nJOIN: include all of the rows that exist in both tables (similar to inner_join() in R, the intersection of the two tables). INNER JOIN is alternative, and identical, function to JOIN.\n\nLEFT JOIN: include all of the rows in the first table. Connect them, as much as possible, to the rows in the second table. Rows that have no match in the second table will have a value of NULL for the new “second table” variables.\n\nRIGHT JOIN: include all of the rows in the second table. Connect them, as much as possible, to the rows in the first table. Rows that have no match in the first table will have a value of NULL for the new “first table” variables. A RIGHT JOIN with the tables in the opposite order is the same as a LEFT JOIN with the tables in the original order.\n\nFULL OUTER JOIN: include all rows in either table. Rows that have no match in the other table will have a value of NULL for the other table variables. (similar to full_join() in R, the union of the two tables). The functionality doesn’t exist in MySQL but can be created using joins and UNION.\n\nCROSS JOIN: match each row of the first table with each row in the second table.\n\nFigure 4.1 shows Venn diagrams of the different types of joins. Figure 4.2 shows four of the JOIN functions with mini data tables. Note that in SQL the missing values will be labeled as NULL (not NA).\n\n\n\n\nFigure 4.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\n\n\n\n\n\n\n\nFigure 4.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\n\n\n\n\n4.2.1 A toy example\nWe will head to R for just a minute so as to understand joins using a small toy dataset on rock bands from the 60s, The Beatles and The Rolling Stones. The function sqldf() in the sqldf R package allows for SQL commands on R objects.\nConsider the following datasets which are available in the dplyr package.\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nInner join\nAn inner join combines two datasets returning only the observations that exist in both of the original datasets.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                     inst.plays\n              FROM band_members AS star\n              JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 John Beatles guitar\n2 Paul Beatles   bass\n\n\nFull join\nA full join combines two datasets returning every observation that exists in either one of the original datasets. Note that in the results, Mick’s instrument is missing, and Keith’s band is missing.\nThe full_join() function does not have an equivalent in MySQL. See Section 4.3.1.1 for using JOINs and UNIONs to produce a full join.\n\nband_members |&gt;\n  full_join(band_instruments)\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nLeft join\nA left join combines two datasets returning every observation that exists in the left (or first) original dataset. Note that in the results, Mick’s instrument is missing.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              LEFT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\n\nRight join\nA right join combines two datasets returning every observation that exists in the right (or second) original dataset. Note that in the results, Keith’s band is missing.\n\nsqldf::sqldf(\"SELECT inst.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              RIGHT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\n\n\n4.2.2 JOIN\n\nIn the imdb database, the title table includes information about the 4,626,322 titles in the database, including the id, title, kind_id (indicator for the kind of ID it is), and production_year. It does not, however, include the review of the title. See Table 4.4.\n\nSELECT * FROM title LIMIT 0, 10;\n\n\n\n\n\nTable 4.4: SELECT to glance at the title table in the imdb database.\n\nid\ntitle\nimdb_index\nkind_id\nproduction_year\nimdb_id\nphonetic_code\nepisode_of_id\nseason_nr\nepisode_nr\nseries_years\nmd5sum\n\n\n\n78460\nAdults Recat to the Simpsons (30th Anniversary)\n\n7\n2017\n\nA3432\n78406\n\n\n\n2ae09eed7d576cc2c24774fed5b18168\n\n\n70273\n(2016-05-18)\n\n7\n2016\n\n\n68058\n\n\n\n511dfc14cfff7589d29a95abb30cd66a\n\n\n60105\n(2014-04-11)\n\n7\n2014\n\n\n59138\n\n\n\nc6cdce7e667e07713e431805c407feed\n\n\n32120\n(2008-05-01)\n\n7\n2008\n\n\n32060\n\n\n\n100df65742caf5afd092b2e0ead67d8e\n\n\n97554\nSchmÃ¶lders Traum\n\n7\n2001\n\nS2543\n97302\n10\n1\n\n46862a2f96f9fb2d59e8c9a11ecfdd28\n\n\n57966\n(#1.1)\n\n7\n2013\n\n\n57965\n1\n1\n\n409c37703766c4b24f8a86162fd9cf85\n\n\n76391\nAnniversary\n\n7\n1971\n\nA5162\n76385\n4\n9\n\n5e12ce73fac1d1dcf94136b6e9acd8f8\n\n\n11952\nAngus Black/Lester Barrie/DC Curry\n\n7\n2009\n\nA5214\n11937\n4\n7\n\n9c38b9e5601dc154444b73b518034aa1\n\n\n1554\nNew Orleans\n\n7\n2003\n\nN6452\n1508\n2\n11\n\n621bea735740a547e862e4a3226f35d2\n\n\n58442\nKiss Me Kate\n\n7\n2011\n\nK2523\n58436\n1\n10\n\n293e8c75c7f35a4035abf617962be5a9\n\n\n\n\n\n\n\n\nThe movie_info_idx table does not contain much information about each particular film. It does, however, have an indicator for the movie ID (given by movie_id) as well as the number of votes (given by info where type_id = 100). See Table 4.5.\n\nSELECT * FROM movie_info_idx LIMIT 0, 6;\n\n\n\n\n\nTable 4.5: SELECT to glance at the movie_info_idx table in the imdb database.\n\nid\nmovie_id\ninfo_type_id\ninfo\nnote\n\n\n\n1\n1\n99\n31.2.1..2.\n\n\n\n2\n1\n100\n9\n\n\n\n3\n1\n101\n4.1\n\n\n\n4\n2\n99\n1000000102\n\n\n\n5\n2\n100\n61\n\n\n\n6\n2\n101\n6.4\n\n\n\n\n\n\n\n\n\nLet’s say we want to combine the titles with the number of votes so that each title with user votes is included. That is, only keep the titles that have a corresponding votes. And also, only keep the votes if there is an associated title (which means we use INNER JOIN or just plain JOIN).\nRemember that WHERE will work on the raw variables, and HAVING works on the results set.\nSome aspects of the query are worth pointing out:\n* The variables in the output are given in the SELECT clause. The id and title (both from the title table) and the info from the movie_info_idx which represents the number of IMDb votes. * The variables are preceded by the table from which they came. While not always necessary, it is good practice so as to avoid confusion. * The JOIN happens by linking the id variable in the title table with the movie_id variable in the movie_info_idx table. * The LIMIT wasn’t necessary (there are only 12 observations), but it’s good practice so that we don’t end up with unwieldy query results. * The WHERE clause happens before the JOIN action, despite being written after. * In the WHERE clause, we keep only movies, only 2015 production year, and only at least 150,000 votes.\n\nSELECT title.id,\n       title.title,\n       movie_info_idx.info\nFROM title\nJOIN movie_info_idx ON title.id = movie_info_idx.movie_id \nWHERE title.production_year = 2015 \n    AND title.kind_id = 1                  # movies only\n    AND movie_info_idx.info_type_id = 100  # info_type is votes\n    AND movie_info_idx.info &gt; 150000       # at least 150,000 votes\nORDER BY movie_info_idx.info DESC\nLIMIT 0, 20;\n\n\n\n\n\nTable 4.6: Movies from 2015 that have at least 150,000 votes in the imdb database.\n\nid\ntitle\ninfo\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n\n\n3915213\nMad Max: Fury Road\n666484\n\n\n4389619\nThe Martian\n583987\n\n\n3313672\nAvengers: Age of Ultron\n540606\n\n\n4414139\nThe Revenant\n526189\n\n\n3787790\nJurassic World\n471237\n\n\n3752999\nInside Out\n443051\n\n\n3292159\nAnt-Man\n390965\n\n\n4364483\nThe Hateful Eight\n363199\n\n\n4251736\nSpectre\n319875\n\n\n3630368\nFurious Seven\n310970\n\n\n4255450\nSpotlight\n290362\n\n\n3961438\nMission: Impossible - Rogue Nation\n266759\n\n\n4321769\nThe Big Short\n262598\n\n\n4221220\nSicario\n260996\n\n\n3600120\nFifty Shades of Grey\n250962\n\n\n4164324\nRoom\n244210\n\n\n3379559\nBridge of Spies\n229440\n\n\n4368666\nThe Hunger Games: Mockingjay - Part 2\n214569\n\n\n4387967\nThe Man from U.N.C.L.E.\n213754\n\n\n\n\n\n\n\n\nLet’s say we also want to obtain information about the actors and actresses in each of the movies. In the cast_info table, there is a person_id, a movie_id, and person_role_id is 1 if actor and 2 if actress.\n\nSELECT * FROM cast_info LIMIT 0, 10;\n\n\n\n\n\nTable 4.7: SELECT to glance at the cast_info table in the imdb database.\n\nid\nperson_id\nmovie_id\nperson_role_id\nnote\nnr_order\nrole_id\n\n\n\n1\n1\n3432997\n1\n\n31\n1\n\n\n2\n2\n1901690\n2\n\n\n1\n\n\n3\n3\n4027567\n2\n\n25\n1\n\n\n4\n3\n4282876\n3\n\n22\n1\n\n\n5\n4\n3542672\n\n\n12\n1\n\n\n6\n5\n3331520\n4\n(as $hutter Boy)\n10\n1\n\n\n7\n5\n4027191\n2\n(as $hutter Boy)\n1\n1\n\n\n8\n5\n4195731\n5\n(uncredited)\n\n1\n\n\n9\n5\n4263956\n6\n(uncredited)\n\n1\n\n\n10\n5\n4267787\n7\n(uncredited)\n\n1\n\n\n\n\n\n\n\n\nWe also want the name of the actress which is in the table aka_name. Note that there is no movie information in the aka_name table!\n\nSELECT * FROM aka_name LIMIT 0, 10;\n\n\n\n\n\nTable 4.8: SELECT to glance at the aka_name table in the imdb database.\n\nid\nperson_id\nname\nimdb_index\nname_pcode_cf\nname_pcode_nf\nsurname_pcode\nmd5sum\n\n\n\n1\n6188450\nSmith, Jessica Noel\n\nS5325\nJ2542\nS53\n25c9d464e3ff2957533546aa92b397ed\n\n\n2\n5125059\nPain, L. $ham\n\nP545\nL515\nP5\n569b1e885ccb51211c01753f0dad9b2c\n\n\n3\n5\nBoy, $hutter\n\nB36\nH361\nB\n35092b5604ce378fc48c8a6fc0038a49\n\n\n4\n4152053\nDollasign, Ty\n\nD4253\nT3425\nD425\n0f565a2d8027cfb8ed6c5f4bba719fcd\n\n\n5\n4152053\nSign, Ty Dolla\n\nS2534\nT3425\nS25\n2eded1b021b96333b4b74e0fec959650\n\n\n6\n6\nMoore, Brandon\n\nM6165\nB6535\nM6\n193a6f5adf4756320f622162d2475608\n\n\n7\n8\n$torm, Country\n\nT6525\nC5363\nT65\n1654400b707d34323ea392b87060e6cc\n\n\n8\n19\n'Hooper', Simon P.J. Kelly\n\nH1625\nS5124\nH16\n3fd8885372c23f8c74e583da91d1fd05\n\n\n9\n19\nHooper\n\nH16\n\n\n24ddc68ab605ee95857ad45b65ffa2d8\n\n\n10\n19\nKelly, Simon P.J.\n\nK4251\nS5124\nK4\n33d976f22e276b73c61513bc5f6e72a6\n\n\n\n\n\n\n\n\nConnecting the most popular movies of 2015 with the actresses in those movies requires a series of JOINs. Note that to make the code less onerous, the title table has been aliased by t, the movie_info_idx table has been aliased by idx, the cast_info table has been aliased by a, and the aka_name table has been aliased by n.\nThere is a lot of data cleaning to do as some of the person_id values are one to many!! That is, the person_id matches multiple names in the aka_name database.\n\nSELECT t.title,\n       idx.info,\n       a.person_id,\n       n.name\nFROM title AS t\nJOIN movie_info_idx AS idx ON t.id = idx.movie_id \nJOIN cast_info AS a ON idx.movie_id = a.movie_id\nJOIN aka_name AS n ON a.person_id = n.person_id\nWHERE t.production_year = 2015 \n    AND t.kind_id = 1           # movies only\n    AND idx.info_type_id = 100  # info_type is votes\n    AND idx.info &gt; 150000       # at least 150,000 votes\n    AND a.role_id = 2           # actresses only\nORDER BY idx.info DESC\nLIMIT 0, 50;\n\n\n\n\n\nTable 4.9: Movies from 2015 that have at least 150,000 votes in the imdb database with the actress name joined.\n\ntitle\ninfo\nperson_id\nname\n\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2698188\nSam\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2806101\nGillespie, Hilary Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2959609\nCuzner, Natalie\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3089483\nFisher, Carrie Frances\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nClass, Clare\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nGlass, Claire\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3231758\nHenwick, Jessica Yu Li\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3265686\nHui, Karen\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3305561\nKamen, Hannah John\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nBilly\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nLourd, Billie Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3569409\nFran\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyongo, Lupita\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyong'o, Lupita Amondi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3785240\nRidley, Daisy Jazz Isobel\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nGiagrande, Meredith J.\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalinger, Meredith\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalenger, Meredith Dawn\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3850834\nPhi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3875581\nFox, Claudia\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3879039\nArti\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSlade, Sandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy Slade\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3938795\nRyan, Karol Lesley\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Dame Harriet\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Harriet Mary\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4094732\nWhite, Kelsey Marie\n\n\nMad Max: Fury Road\n666484\n2681098\nMichelle, Debra\n\n\nMad Max: Fury Road\n666484\n2782138\nAli\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helena\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helene\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, HÃ©lÃ¨ne Vania\n\n\nMad Max: Fury Road\n666484\n2957052\nCunico, Lillie\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3146859\nGilles, Coco Jack\n\n\nMad Max: Fury Road\n666484\n3146859\nGillies, Coco\n\n\nMad Max: Fury Road\n666484\n3268456\nRose\n\n\nMad Max: Fury Road\n666484\n3268456\nHuntington-Whiteley, Rosie Alice\n\n\nMad Max: Fury Road\n666484\n3343489\nKellerman, Antoinette\n\n\nMad Max: Fury Road\n666484\n3348513\nRiley\n\n\n\n\n\n\n\n\n\n4.2.3 Other JOINs\nConsider the following two tables. The first has seven movies in it (from 2015 with at least 400,000 IMDb votes). The second consists of almost 3 million actresses (person_role_id = 2). In order to find a subset of actresses, the person_id &gt; 3900000 was set arbitrarily (in order to have a smaller group with which to work).\nmovies:\n\nSELECT t.id,\n       t.title,\n       idx.info,\n       (SELECT COUNT(*)\n       FROM title AS t\n       JOIN movie_info_idx AS idx ON idx.movie_id = t.id\n       WHERE t.production_year = 2015  \n             AND t.kind_id = 1\n             AND idx.info_type_id = 100\n             AND idx.info &gt; 400000) AS row_count\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1             # movies only\n    AND idx.info_type_id = 100    # info_type is votes\n    AND idx.info &gt; 400000         # at least 400,000 votes\nORDER BY idx.info DESC\n\n\n\n\n\nTable 4.10: Movies from 2015 that have at least 400,000 votes in the imdb database.\n\nid\ntitle\ninfo\nrow_count\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n7\n\n\n3915213\nMad Max: Fury Road\n666484\n7\n\n\n4389619\nThe Martian\n583987\n7\n\n\n3313672\nAvengers: Age of Ultron\n540606\n7\n\n\n4414139\nThe Revenant\n526189\n7\n\n\n3787790\nJurassic World\n471237\n7\n\n\n3752999\nInside Out\n443051\n7\n\n\n\n\n\n\n\n\nactresses:\n\nSELECT a.person_id,\n       a.movie_id,\n       n.name,\n       (SELECT COUNT(*)\n       FROM cast_info AS a\n       JOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 390000) AS row_count\nFROM cast_info AS a\nJOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 3900000\nLIMIT 0, 20;\n\n\n\n\n\nTable 4.11: Actresses whose person_id is greater than 400000. Note that some actresses have different spelling or phrasing of their names.\n\nperson_id\nmovie_id\nname\nrow_count\n\n\n\n3900141\n759802\nSimons, Rita Joanne\n2904759\n\n\n3902258\n4365829\nSinger, Rabbi Tovia\n2904759\n\n\n3902699\n3109788\nSingh, Sabine Erika\n2904759\n\n\n3903035\n3215866\nVal\n2904759\n\n\n3904831\n2468067\nMasha\n2904759\n\n\n3904928\n3654347\nFei, Siu Yin\n2904759\n\n\n3904928\n3654347\nHsiao, Yen-fei\n2904759\n\n\n3904928\n3654347\nSiu, Yinfei\n2904759\n\n\n3904928\n3654347\nXiao, Yanfei\n2904759\n\n\n3904928\n3654347\nYin-Fai, Siu\n2904759\n\n\n3905289\n115191\nCoso, Cosondra\n2904759\n\n\n3905289\n115191\nSjostrom, Cossondra\n2904759\n\n\n3905289\n115191\nCoso\n2904759\n\n\n3909355\n2939100\nSlovÃ¡ckovÃ¡, Anna Julie\n2904759\n\n\n3911826\n4379610\nMeador, Constance June\n2904759\n\n\n3912134\n2675144\nDJ\n2904759\n\n\n3912134\n2675144\nSmith, DJ\n2904759\n\n\n3912134\n2675144\nSmith, Dujonette\n2904759\n\n\n3912134\n2675144\nDJ Smith\n2904759\n\n\n3913519\n1678444\nKeely, Dorothy Jacqueline\n2904759\n\n\n\n\n\n\n\n\nUsing subqueries, we can JOIN the two datasets using different JOIN techniques.\nInner JOIN\n\nWith an inner JOIN, there are 32 rows corresponding to all the actresses in the seven 2015 films with the most votes. Because the JOIN is an intersection of the two tables, only the actresses with person_id above 3900000 are included.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nINNER JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\nRIGHT JOIN\nWith a RIGHT JOIN, there are more than 300 rows (the LIMIT clause keeps us from knowing how many rows, but there are a LOT!) corresponding to all the actresses whose person_id above 3900000 are included. Those actresses who acted in one of the seven top 2015 films are also included in the full results table, but they don’t happen to be in the truncated output here.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nRIGHT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\nLEFT JOIN\nWith a LEFT JOIN, there are 33 rows corresponding to the actresses in the seven top 2015 movies. Only The Revenant did not have any actresses whose person_id is greater than 3900000.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nLEFT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\n\nCounting repeat actresses\nWe might, for example, want to know how many names / spellings of a name with a specific person_id (above 3900000) exist for each person_id in each of the top voted seven films of 2015.\nIn Table 4.12 why isn’t there a column indicating the name of the actress? (There can’t be such a column. Why not?)\n\nSELECT acts.person_id, \n       COUNT(*) AS num_repeat_names\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY acts.person_id;\n\n\n\n\n\nTable 4.12: For each person_id (&gt; 3900000) in the seven top voted 2015 films, how many names / spellings are associated with the person_id?\n\nperson_id\nnum_repeat_names\n\n\n\n3916648\n1\n\n\n4122876\n1\n\n\n3938423\n2\n\n\n3950111\n1\n\n\n4079047\n2\n\n\n4084626\n3\n\n\n4099458\n1\n\n\n3958614\n1\n\n\n3990819\n2\n\n\n4081131\n2\n\n\n3907812\n3\n\n\n3938795\n1\n\n\n3970637\n8\n\n\n4073883\n2\n\n\n4094732\n1\n\n\n4098918\n1\n\n\n\n\n\n\n\n\nCounting number of actresses per film\nWe might, for example, want to know how many actresses with a specific person_id (above 3900000) are in each of the top voted seven films of 2015.\n\nSELECT movs.id, \n       movs.title,\n       COUNT(*) AS num_actress\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY movs.id;\n\n\n\n\n\nTable 4.13: Number of actresses (with person_id &gt; 3900000) in each of the seven top voted films of 2015. Recall that The Revenant had no actresses with person_id &gt; 3900000, so there are only six movies listed.\n\nid\ntitle\nnum_actress\n\n\n\n3313672\nAvengers: Age of Ultron\n1\n\n\n3752999\nInside Out\n1\n\n\n3787790\nJurassic World\n9\n\n\n3915213\nMad Max: Fury Road\n5\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n15\n\n\n4389619\nThe Martian\n1"
  },
  {
    "objectID": "04-sql-joins.html#union-all",
    "href": "04-sql-joins.html#union-all",
    "title": "4  Combining tables in SQL",
    "section": "\n4.4 UNION ALL\n",
    "text": "4.4 UNION ALL\n\n\n4.4.1 FULL OUTER JOIN\n\nMySQL doesn’t have a FULL OUTER JOIN (although other implementations of SQL do have full join functionality). However, we can mimick a full join using right and left joins with UNION.\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_imdb)"
  },
  {
    "objectID": "04-sql-joins.html#unioning",
    "href": "04-sql-joins.html#unioning",
    "title": "4  Combining tables in SQL",
    "section": "\n4.3 UNIONing",
    "text": "4.3 UNIONing\nIn SQL a UNION clause combines two different tables by their rows (whereas JOIN combines two tables by columns). Think about UNION similarly to the bind_rows() command in R.\n\n\n\n\nFigure 4.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html\n\n\n\n\n4.3.1 UNIONs\nUNION does not check the names of the columns to make sure they match. UNION requires that the number of columns be the same and that the variable type be the same for all columns in the two tables being combined.\nTable 4.14 contains a silly example. The first table has 1 as bar and the second table has 20 as bar. But when the tables are UNIONed, the bar column contains c(1, 10). SQL took the column names from the first table and appended the second table without considering the variable names.\n\nSELECT \n    1 AS bar,\n    2 AS foo\n\nUNION\n\nSELECT \n    10 AS foo,\n    20 AS bar;\n\n\n\n\n\nTable 4.14: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\nbar\nfoo\n\n\n\n1\n2\n\n\n10\n20\n\n\n\n\n\n\n\n\nUNION is specifically designed to bind rows from two different SELECT queries where the variables have been selected in the same order. If the two SELECT clauses are done from the same table with the same order of variables, you do not need to worry about the order of the variables matching up in the UNION. If you are UNIONing two very different subqueries, you do need to worry about the variables and their order.\nUNION\nLet’s say we want to combine the top voted movies from 2015 with the top voted movies from 2019. However, to account for time, we require the movies from 2015 to have more votes (400,000) than the movies from 2017 (200,000). That is, the WHERE clause is different for the two subqueries.\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2017  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 200000)\nLIMIT 0, 100;\n   \n\n\n\n\n\nTable 4.15: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\ntitle\nproduction_year\nnum_votes\n\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nThe Martian\n2015\n583987\n\n\nThe Revenant\n2015\n526189\n\n\nDunkirk\n2017\n229089\n\n\nGuardians of the Galaxy Vol. 2\n2017\n281845\n\n\nLogan\n2017\n397056\n\n\nSpider-Man: Homecoming\n2017\n209930\n\n\nWonder Woman\n2017\n306611\n\n\n\n\n\n\n\n\nUNION ALL\nUNION does check, however, to see if any of the rows in the two tables are identical. If the goal is to include duplicates across two tables, use UNION ALL instead of UNION.\nLet’s say that the first table is all movies with production year after 2012 and number of votes greater than 500,000. The second table is movies with production year equal to 2015 and number of votes greater than 400,000. Even though the Martian would have been in both tables, the results table lists The Marian only once in Table 4.16.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\n\nTable 4.16: Using UNION to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\nWhen UNION ALL is applied in the same context, The Martian is listed twice in the results table given in Table 4.17.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION ALL\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\n\nTable 4.17: Using UNION ALL to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\n\n4.3.1.1 FULL OUTER JOIN via UNION\n\nMySQL doesn’t have a FULL OUTER JOIN (although other implementations of SQL do have full join functionality). However, we can mimic a full join using right and left joins with UNION.\nRecall the ideas of RIGHT JOIN (which keeps all observations in the right table) and LEFT JOIN (which keeps all observations in the left table). By UNIONing the right and left joins, all of the observations are obtained (i.e., a full join). Using the function sqldf() in the sqldf R package, the full join will be demonstrated using the 1960s rock bands.\nNotice that in the RIGHT JOIN the name column must come from the right table (not the left table).\nAlso notice that UNION ALL keeps the duplicate rows which is probably not what we want.\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  John Beatles guitar\n2 Keith    &lt;NA&gt; guitar\n3  Mick  Stones   &lt;NA&gt;\n4  Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION ALL\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  Mick  Stones   &lt;NA&gt;\n2  John Beatles guitar\n3  Paul Beatles   bass\n4  John Beatles guitar\n5  Paul Beatles   bass\n6 Keith    &lt;NA&gt; guitar"
  },
  {
    "objectID": "04-sql-joins.html#best-practice",
    "href": "04-sql-joins.html#best-practice",
    "title": "4  Combining tables in SQL",
    "section": "\n4.4 Best practice",
    "text": "4.4 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_imdb, shutdown = TRUE)"
  },
  {
    "objectID": "02-sql-in-R.html#dbeaver",
    "href": "02-sql-in-R.html#dbeaver",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.4 DBeaver",
    "text": "2.4 DBeaver\nDBeaver is a free SQL client that supports MySQL (as well as other dialects like MariaDB, PostgreSQL, and SQLite). While writing SQL code in R has some benefits (e.g., piping results tables into ggplot2 for visualizations), using a SQL client that is designed for SQL queries has benefits as well. In order to use DBeaver, download the client onto your computer and open it from your Applications.\n\n2.4.1 New database connection\nUsing the pull-down menus, navigate to a new database connection (Database -&gt; New Database Connection). Click on the MySQL icon (and click next). You should see an image similar to Figure 2.1.\n\n\n\n\nFigure 2.1: Connection settings for a MySQL connection via DBeaver.\n\n\n\n\nKeep the Host radio button toggled (don’t click on URL)\nWhere currently it says Server Host: localhost change localhost to the URL for the MySQL server to which you want to connect.\nChange the Username to the appropriate username for the server.\nChange the Password to the appropriate password for the server.\nOptional: in the Database: box, include the database you will query.\nClick Finish.\n\nOnce the connection is established, you should be able to navigate through the databases and their tables on the left side of the DBeaver window.\n\n2.4.2 Writing SQL queries\nPull up a SQL script by clicking ont he SQL button as seen in Figure 2.2.\n\n\n\n\nFigure 2.2: Click on SQL to initiate a SQL script.\n\n\n\nWrite SQL code. Click on the orange triangle to run the code.\nEach lab should be saved as a .sql files that can be turned in. The SQL queries (in the .sql file) should be able to be run by someone else. Use the hashtag (#) to comment out lines so that you can identify particular problems or comment on the query results.\nIf you did not specify which database to use when you set up the connection, the database can be specified at the top of the .sql file as USE database; (for example, you might want USE airlines;, with the semi-colon, before running your lines of SQL code).\nTo write text use /* write text here ... */, the slash and asterisk, for any commenting in the .sql file."
  },
  {
    "objectID": "p1-intro-to-sql.html",
    "href": "p1-intro-to-sql.html",
    "title": "Introduction to databases and SQL",
    "section": "",
    "text": "Databases are a powerful structure for holding huge amounts of data that can easily be accessed. Databases are made up of tables which are efficiently stored using indexing and avoiding unnecessary replication of information.\nChapter 1 covers details of databases and reviews R functionality (mostly through the dplyr package) for working with data frames that mimics SQL queries for working with tables.\nChapter 2 covers three different ways to run SQL queries in R. R code can be translated into SQL; the DBI package can send SQL queries through an r chunk; and SQL queries can be sent directly to a SQL server through a sql chunk. Additionally, DBeaver is introduced as a SQL client which supports MySQL."
  },
  {
    "objectID": "p2-using-sql.html",
    "href": "p2-using-sql.html",
    "title": "Using SQL",
    "section": "",
    "text": "SQL code is written as a series of statements. Later in the text, Chapter 5 and Chapter 6 cover statements like CREATE for defining new tables and INSERT for adding data. In this part, Chapter 3 covers SELECT statements, which are arguably the most useful statements for data scientists. SELECT statements are also called queries because they query table(s), with particular characteristics, from databases.\nA query is made up of clauses. Every query must have a SELECT and FROM clause. Other clauses include WHERE, GROUP BY, HAVING, and ORDER BY, all of which will be covered in Chapter 3.\nBecause of the efficiency of data storage in SQL databases, it is often necessary to combine information held across two or more tables. Chapter 4 covers combining tables via JOIN (combining columns) and via UNION (combining rows)."
  },
  {
    "objectID": "p4-reg-expr.html#quarto",
    "href": "p4-reg-expr.html#quarto",
    "title": "Regular expressions",
    "section": "Quarto",
    "text": "Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "p4-reg-expr.html#running-code",
    "href": "p4-reg-expr.html#running-code",
    "title": "Regular expressions",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "p3-build-db.html",
    "href": "p3-build-db.html",
    "title": "Building databases",
    "section": "",
    "text": "Along with querying and combining tables in SQL, building a SQL database takes care. In order to practice building a SQL database, we will work with DuckDB. DuckDB is a platform that creates a local server (on your own computer) and uses syntax which is slightly different from MySQL.\nIn Chapter 5 we walk through creating a database using DuckDB. We note that some of the syntax is identical to MySQL and other syntax is different. Keys, indices, and partitioning help optimize the database for efficient querying.\nWith an existing database, Chapter 6 walks through how to UPDATE, INSERT, and REPLACE data. Additionally, temporary tables are used to break down complicated queries into shorter more succinct queries."
  },
  {
    "objectID": "05-creating-db.html#loading-data",
    "href": "05-creating-db.html#loading-data",
    "title": "5  Creating databases",
    "section": "\n5.1 Loading data",
    "text": "5.1 Loading data\nThe duckdb database is currently empty, so we need to load in some data. The duckdb_read_csv() function in the duckdb R package allows us to load the .csv file (available on GitHub) directly into the database without being loaded as an R object first.\nAs an example, consider the Saturday Night Live datasets available on the snldb GitHub repo. Data is scraped from http://www.snlarchives.net and http://www.imdb.com/title/tt0072562 by Hendrik Hilleckes and Colin Morris. Notice that there are eleven .csv files available in the output folder.\nAs an example, let’s consider the casts.csv file.\nBefore we get into loading data into a MySQL database, let’s look at the casts file in R, so that we understand the data we want to load. glimpse() provides the variable names and the variables types. The variables types are a mix of character strings, numeric, and logical. Variable types are very important for inputting data into a MySQL server.\n\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\nglimpse(casts)\n\nRows: 614\nColumns: 8\n$ aid             &lt;chr&gt; \"A. Whitney Brown\", \"A. Whitney Brown\", \"A. Whitney Br…\n$ sid             &lt;dbl&gt; 11, 12, 13, 14, 15, 16, 5, 39, 40, 41, 42, 45, 46, 21,…\n$ featured        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ first_epid      &lt;dbl&gt; 19860222, NA, NA, NA, NA, NA, 19800409, 20140118, NA, …\n$ last_epid       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ update_anchor   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ n_episodes      &lt;dbl&gt; 8, 20, 13, 20, 20, 20, 5, 11, 21, 21, 21, 18, 17, 20, …\n$ season_fraction &lt;dbl&gt; 0.444, 1.000, 1.000, 1.000, 1.000, 1.000, 0.250, 0.524…\n\n\nNow we are ready to import data into the database we’ve created. MySQL is not the default SQL dialect for DuckDB, so let’s install and load the MySQL extension before committing to USE the duck_datab on our server connection.\n\nINSTALL mysql;\n\n\nLOAD mysql;\n\n\nUSE duck_datab;\n\n\nDROP TABLE IF EXISTS casts;"
  },
  {
    "objectID": "05-creating-db.html#best-practice",
    "href": "05-creating-db.html#best-practice",
    "title": "5  Creating databases",
    "section": "\n5.2 Best practice",
    "text": "5.2 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)"
  },
  {
    "objectID": "05-creating-db.html#preparing-to-load-data",
    "href": "05-creating-db.html#preparing-to-load-data",
    "title": "5  Creating databases",
    "section": "\n5.1 Preparing to load data",
    "text": "5.1 Preparing to load data\nThe duckdb database is currently empty, so we need to load in some data. The duckdb_read_csv() function in the duckdb R package allows us to load the .csv file (available on GitHub) directly into the database without being loaded as an R object first. The function, duckdb_read_csv() does some of the work for us to find data types. However, we will first learn what data types are and how to use them, and most dialects of SQL require you to specify the data types before loading in data (usually done using LOAD DATA but using COPY in Duckdb).\nRecall that in Table 3.2 we used DESCRIBE to display the variable types of the database table(s). The list includes the variable name (Field), its Type, whether there are NULL values allowed, and whether there are keys or indexes defined on the variable. See Table 5.2 for the DESCRIBE output on the table we are about to import.\nUnlike R, when creating a new data table, SQL requires that you communicate each future variable (column) and that variable’s type. Variable types are not automatically generated!\nAs an example, consider the Saturday Night Live datasets available on the snldb GitHub repo. Data is scraped from http://www.snlarchives.net and http://www.imdb.com/title/tt0072562 by Hendrik Hilleckes and Colin Morris. Notice that there are eleven .csv files available in the output folder.\nSpecifically, let’s consider the casts.csv file.\nBefore we get into loading data into a SQL database, let’s look at the casts file in R, so that we understand the data we want to load. glimpse() provides the variable names and the variables types. The variables types are a mix of character strings, numeric, and logical. Variable types are very important for inputting data into a SQL server.\n\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\nglimpse(casts)\n\nRows: 614\nColumns: 8\n$ aid             &lt;chr&gt; \"A. Whitney Brown\", \"A. Whitney Brown\", \"A. Whitney Br…\n$ sid             &lt;dbl&gt; 11, 12, 13, 14, 15, 16, 5, 39, 40, 41, 42, 45, 46, 21,…\n$ featured        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ first_epid      &lt;dbl&gt; 19860222, NA, NA, NA, NA, NA, 19800409, 20140118, NA, …\n$ last_epid       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ update_anchor   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ n_episodes      &lt;dbl&gt; 8, 20, 13, 20, 20, 20, 5, 11, 21, 21, 21, 18, 17, 20, …\n$ season_fraction &lt;dbl&gt; 0.444, 1.000, 1.000, 1.000, 1.000, 1.000, 0.250, 0.524…\n\n\n\n5.1.1 Variable types\nThe glance() function indicates that there are different variables types, here chr (character string), dbl (numerical value with double precision), and lgl (logical value). What are the data types in SQL?1\nNumbers\n\nexact numeric data types (INTEGER, SMALLINT, DECIMAL, and NUMERIC)2\n\napproximate numeric data types (FLOAT, REAL, and DOUBLE PRECISION)3\n\ninteger types INTEGER (or INT) and SMALLINT, TINYINT, MEDIUMINT, and BIGINT\n\n\nDECIMAL(precision, scale). For example, in DECIMAL(5,2) 5 is the precision and 2 is the scale. The precision represents the number of significant digits that are stored for values, and the scale represents the number of digits that can be stored following the decimal point. DECIMAL(5,2) must store any value with five digits and two decimals, so values that can be stored in the salary column range from -999.99 to 999.99. DECIMAL(M) is equivalent to DECIMAL(M,0). Similarly, the syntax DECIMAL is equivalent to DECIMAL(M,0), where the default value of M is 10. If the scale is 0, DECIMAL values contain no decimal point or fractional part.\nStrings\n\nstring data types are CHAR, VARCHAR, BINARY, VARBINARY, BLOB, TEXT, ENUM, and SET\n\n\nCHAR and VARCHAR types are similar, but differ in the way they are stored and retrieved. They also differ in maximum length and in whether trailing spaces are retained.\n\nCHAR and VARCHAR types are declared with a length that indicates the maximum number of characters you want to store. For example, CHAR(30) can hold up to 30 characters.\nThe length of a CHAR column is fixed to the length that you declare when you create the table.\nValues in VARCHAR columns are variable-length strings. The length can be specified as a value from 0 to 65,535.\n\nBINARY and VARBINARY types are similar to CHAR and VARCHAR, except that they store binary strings rather than nonbinary strings. That is, they store byte strings rather than character strings.\nA BLOB is a binary large object that can hold a variable amount of data. The four BLOB types are TINYBLOB, BLOB, MEDIUMBLOB, and LONGBLOB. These differ only in the maximum length of the values they can hold. The four TEXT types are TINYTEXT, TEXT, MEDIUMTEXT, and LONGTEXT. These correspond to the four BLOB types and have the same maximum lengths and storage requirements.\nDate\n\ndate and time data types for representing temporal values are DATE, TIME, DATETIME, TIMESTAMP, and YEAR\n\n\nDATE type is used for values with a date part but no time part. MySQL retrieves and displays DATE values in ‘YYYY-MM-DD’ format. The supported range is ‘1000-01-01’ to ‘9999-12-31’.\n\nDATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in ‘YYYY-MM-DD hh:mm:ss’ format. The supported range is ‘1000-01-01 00:00:00’ to ‘9999-12-31 23:59:59’.\n\nTIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of ‘1970-01-01 00:00:01’ UTC to ‘2038-01-19 03:14:07’ UTC.\n\nTIME values in ‘hh:mm:ss’ format (or ‘hhh:mm:ss’ format for large hours values). TIME values may range from ‘-838:59:59’ to ‘838:59:59’. The hours part may be so large because the TIME type can be used not only to represent a time of day (which must be less than 24 hours), but also elapsed time or a time interval between two events (which may be much greater than 24 hours, or even negative).\n\nYEAR type is a 1-byte type used to represent year values with a display width of four characters.\n\n5.1.2 CHECK constraints\nWhile implementing CREATE TABLE, constraints can be added either to individual variables (CountryPopulation &gt; 0) or to the table as a whole (LastCensus &lt; NextCensus).4\nIf an attempt is made to load data that violate the CHECK constraints, an error will be given.\n\nCREATE TABLE CountryListCensus (\n    Id INT,\n    CountryName VARCHAR(255) NOT NULL,\n    CountryPopulation INT CHECK(CountryPopulation &gt; 0),\n    LastCensus DATE,\n    NextCensus DATE,\n    CHECK(LastCensus&lt;NextCensus),\n    PRIMARY KEY (Id)\n);\n\n\n5.1.3 Creating KEYs\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY col1,\n  FOREIGN KEY col2 REFERENCES table2(table2col1)\n);\n\nEither or both of the KEYs could be multiple columns.\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY (col1, col3),\n  FOREIGN KEY (col1, col2) REFERENCES table2(table2col1, table2col4)\n);\n\n\n5.1.4 Creating INDEXes\nIndexes can be created on one or more variable. A table does not need to have an INDEX (or a KEY).\n\nCREATE INDEX name_of_index ON table (col1);\n\n\nCREATE INDEX name_of_index ON table (col1, col2);\n\n\n5.1.5 Loading data\nOnce the database is set up, you will be ready to import .csv files into the database as tables. Importing .csv files as tables requires a series of steps:5\n\na USE statement that ensures we are in the right schema/database.\na series of DROP TABLE statements that drop any old tables with the same names as the ones we are going to create.\na series of CREATE TABLE statements that specify the table structures.\na series of COPY statements that read the data from the .csv files into the appropriate tables.\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDuckDB has its own dialect of SQL. To load data into a MySQL server, the final statement would be LOAD DATA instead of COPY. See MDSR for more information on loading data into a remote MySQL server.\n\n\nLoading step 1\nUse the local database that we’ve called duck_datab.\n\nUSE duck_datab;\n\nLoading step 2\nMake sure to “refresh” the table, in case it already exists. However, be very careful with the DROP TABLE statement, as it will remove the casts table.\n\nDROP TABLE IF EXISTS casts;\n\nLoading step 3\nCarefully define the variable types, whether or not they allow missing values, and what a default value is for that variable. Additionally, identify the key for accessing information.\nMySQL doesn’t actually have a BOOLEAN datatype (you would use TINYINT(1) instead). But DuckDB does have a BOOLEAN datatype!\n\nCREATE TABLE casts (\n  aid VARCHAR(255) NOT NULL DEFAULT ' ',\n  sid INTEGER NOT NULL DEFAULT 0,\n  featured BOOLEAN NOT NULL DEFAULT 'false',\n  first_epid INTEGER DEFAULT 0,\n  last_epid INTEGER DEFAULT 0,\n  update_anchor BOOLEAN NOT NULL DEFAULT 0,\n  n_episodes INTEGER NOT NULL DEFAULT 0,\n  season_fraction DECIMAL(21,20) NOT NULL DEFAULT 0,\n  PRIMARY KEY (sid, aid)\n);\n\nLoading step 4\nThe .csv file lives on my computer, so I load it in directly. Note that the statement to load in data is slightly different in MySQL.\n\nCOPY casts FROM 'data/casts.csv' HEADER;\n\nChecking the loading\n\nSELECT * FROM casts LIMIT 8;\n\n\n\n\n\nTable 5.1: After CREATE TABLE where variable types are set, the COPY command pulls the data into the table. SELECT shows us that the table is as expected.\n\naid\nsid\nfeatured\nfirst_epid\nlast_epid\nupdate_anchor\nn_episodes\nseason_fraction\n\n\n\nA. Whitney Brown\n11\nTRUE\n19860222\n\nFALSE\n8\n0.444\n\n\nA. Whitney Brown\n12\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n13\nTRUE\n\n\nFALSE\n13\n1.000\n\n\nA. Whitney Brown\n14\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n15\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n16\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nAlan Zweibel\n5\nTRUE\n19800409\n\nFALSE\n5\n0.250\n\n\nSasheer Zamata\n39\nTRUE\n20140118\n\nFALSE\n11\n0.524\n\n\n\n\n\n\n\n\nCheck\nLet’s make sure that the database exists and that the table in the database exists.\n\nSHOW DATABASES;\n\n\n\n\n1 records\n\n\n\ndatabase_name\n\n\n\n\nduck_datab\n\n\n\n\n\n\nSHOW TABLES;\n\n\n\n\n1 records\n\n\n\nname\n\n\n\n\ncasts\n\n\n\n\n\n\nDESCRIBE casts;\n\n\n\n\n\nTable 5.2: DESCRIBE variables in the casts table.\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\naid\nVARCHAR\nNO\nPRI\n' '\n\n\n\nsid\nINTEGER\nNO\nPRI\n0\n\n\n\nfeatured\nBOOLEAN\nNO\n\n'false'\n\n\n\nfirst_epid\nINTEGER\nYES\n\n0\n\n\n\nlast_epid\nINTEGER\nYES\n\n0\n\n\n\nupdate_anchor\nBOOLEAN\nNO\n\n0\n\n\n\nn_episodes\nINTEGER\nNO\n\n0\n\n\n\nseason_fraction\nDECIMAL(21,20)\nNO\n\n0\n\n\n\n\n\n\n\n\n\n\n5.1.6 Using DuckDB for loading data\nThe steps given in Section 5.1.5 are general to many SQL dialects and are important when working with most SQL clients. It is important to have control over the variables configurations as they make up the SQL database. However, using the duckdb package in R allows for shorthand entry of data from .csv files into the DuckDB database. Here, we take advantage of working with the DuckDB functionality in R.\n\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"episodes\", files = \"data/episodes.csv\")\n\nChecking the loading\n\nSHOW TABLES;\n\n\n\n\n3 records\n\n\n\nname\n\n\n\n\n\ncasts\n\n\n\n\nepisodes\n\n\n\n\nhosts"
  },
  {
    "objectID": "05-creating-db.html#footnotes",
    "href": "05-creating-db.html#footnotes",
    "title": "5  Creating databases",
    "section": "",
    "text": "Taken from https://dev.mysql.com/doc/refman/8.0/en/data-types.html↩︎\nThe keyword INT is a synonym for INTEGER, and the keywords DEC and FIXED are synonyms for DECIMAL↩︎\n MySQL treats DOUBLE as a synonym for DOUBLE PRECISION. MySQL also treats REAL as a synonym for DOUBLE PRECISION.↩︎\nExample from https://www.sqlshack.com/how-to-use-sql-check-constraints/↩︎\ntaken from MDSR.↩︎"
  },
  {
    "objectID": "03-sql-verbs.html#saving-sql-queries-as-r-objects",
    "href": "03-sql-verbs.html#saving-sql-queries-as-r-objects",
    "title": "3  SQL clauses",
    "section": "\n3.9 Saving SQL queries as R objects",
    "text": "3.9 Saving SQL queries as R objects\nIf you are working in R to run SQL commands, you may want to use the query output for further analysis or visualizations. In that case, use #|output.var: \"name_of_variable\" inside the {sql} chunk. The variable called name_of_variable will then be available to be used in the R environment.\n\n```{sql}\n#| connection: con_taxi\n#| label: new-table\n#| output.var: \"new_table\"\n\nSELECT *, DAYNAME(STR_TO_DATE(pickup_datetime, \"%Y-%m-%d %T\")) AS wday\nFROM yellow_old \nLIMIT 0, 1000;\n\n```\n\n\n\n\n\nTable 3.25: New data.frame saved to R called new_table.\n\nvendor_id\npickup_datetime\ndropoff_datetime\npassenger_count\ntrip_distance\npickup_longitude\npickup_latitude\nrate_code\nstore_and_fwd_flag\ndropoff_longitude\ndropoff_latitude\npayment_type\nfare_amount\nsurcharge\nmta_tax\ntip_amount\ntolls_amount\ntotal_amount\nwday\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCMT\n2014-03-01 01:07:38\n2014-03-01 01:16:26\n1\n2.0\n-74.0\n40.7\n1\nN\n-73.9\n40.7\nCRD\n9.0\n0.5\n0.5\n2.0\n0\n12.0\nSaturday\n\n\nCMT\n2014-03-01 01:08:03\n2014-03-01 01:12:51\n2\n1.2\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n6.0\n0.5\n0.5\n1.0\n0\n8.0\nSaturday\n\n\nCMT\n2014-03-01 01:08:51\n2014-03-01 01:13:18\n3\n0.5\n-73.9\n40.7\n1\nN\n-74.0\n40.7\nCRD\n5.0\n0.5\n0.5\n1.2\n0\n7.2\nSaturday\n\n\nCMT\n2014-03-01 01:09:20\n2014-03-01 01:24:18\n3\n3.5\n-74.0\n40.7\n1\nN\n-74.0\n40.8\nCRD\n14.0\n0.5\n0.5\n3.0\n0\n18.0\nSaturday\n\n\nCMT\n2014-03-01 01:09:46\n2014-03-01 01:22:34\n1\n1.8\n-74.0\n40.7\n1\nN\n-74.0\n40.7\nCRD\n10.5\n0.5\n0.5\n1.0\n0\n12.5\nSaturday\n\n\n\n\n\n\n\n\n\n```{r}\nnew_table |&gt;\n  drop_na(wday) |&gt;\n  ggplot(aes(x = fare_amount, y = tip_amount, color = wday)) + \n  geom_point() \n```"
  },
  {
    "objectID": "03-sql-verbs.html#best-practice",
    "href": "03-sql-verbs.html#best-practice",
    "title": "3  SQL clauses",
    "section": "\n3.10 Best practice",
    "text": "3.10 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_taxi, shutdown = TRUE)"
  },
  {
    "objectID": "05-creating-db.html#saving-tables",
    "href": "05-creating-db.html#saving-tables",
    "title": "5  Creating databases",
    "section": "\n5.2 Saving tables",
    "text": "5.2 Saving tables\ngenerally, the tables are saved, even when the connection is closed. we can, however, save temporary tables that get deleted when the connection is closed. see “temporary tables” https://duckdb.org/docs/sql/statements/create_table.html"
  },
  {
    "objectID": "05-creating-db.html#temporary-tables",
    "href": "05-creating-db.html#temporary-tables",
    "title": "5  Creating databases",
    "section": "\n5.2 Temporary tables",
    "text": "5.2 Temporary tables\nTemporary tables are used to break down complex queries into smaller, more manageable steps. For example, let’s say we want to JOIN two tables after each has been filtered using different WHERE clauses. The filtered tables can each be saved into their own temporary tables and then the temporary tables can be merged.\nNote that tables in DuckDB are saved (to disk), even when the connection is closed. However, temporary tables are saved in memory (instead of on disk) and are deleted when the connection is closed. Specific configuration of the temporary directory allows for temporary tables to be saved, even when the connection is closed.\n\n5.2.1 Using DuckDB for loading data\nThe steps given in Section 5.1.1 are general to many SQL dialects and are important when working with most SQL clients. It is important to have control over the variables configurations as they make up the SQL database. However, using the duckdb package in R allows for shorthand entry of data from .csv files into the DuckDB database. Here, we take advantage of working with the DuckDB functionality in R.\n\nduckdb_read_csv(con = con_duckdb, name = \"actors\", files = \"data/actors.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"seasons\", files = \"data/seasons.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"titles\", files = \"data/titles.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"episodes\", files = \"data/episodes.csv\")\n\nChecking the loading\n\nSHOW TABLES;\n\n\n\n\n6 records\n\n\n\nname\n\n\n\n\n\nactors\n\n\n\n\ncasts\n\n\n\n\nepisodes\n\n\n\n\nhosts\n\n\n\n\nseasons\n\n\n\n\ntitles\n\n\n\n\n\n\nNotice that most of the tables have some kind of ID which allows JOINing across tables.\n\nSELECT * FROM actors LIMIT 10;\n\n\n\n\n\nTable 5.3: Note the aid identifier in the actors table.\n\naid\nurl\ntype\ngender\n\n\n\nKate McKinnon\n/Cast/?KaMc\ncast\nfemale\n\n\nAlex Moffat\n/Cast/?AlMo\ncast\nmale\n\n\nEgo Nwodim\n/Cast/?EgNw\ncast\nunknown\n\n\nChris Redd\n/Cast/?ChRe\ncast\nmale\n\n\nKenan Thompson\n/Cast/?KeTh\ncast\nmale\n\n\nCarey Mulligan\n/Guests/?3677\nguest\nandy\n\n\nMarcus Mumford\n/Guests/?3679\nguest\nmale\n\n\nAidy Bryant\n/Cast/?AiBr\ncast\nfemale\n\n\nSteve Higgins\n/Crew/?StHi\ncrew\nmale\n\n\nMikey Day\n/Cast/?MiDa\ncast\nmale\n\n\n\n\n\n\n\n\n\nSELECT * FROM episodes LIMIT 10;\n\n\n\n\n\nTable 5.4: Note the sid and epid identifiers in the episodes table.\n\nsid\nepid\naired\nepno\n\n\n\n46\n20210410\nApril 10, 2021\n17\n\n\n46\n20210403\nApril 3, 2021\n16\n\n\n46\n20210327\nMarch 27, 2021\n15\n\n\n46\n20210227\nFebruary 27, 2021\n14\n\n\n46\n20210220\nFebruary 20, 2021\n13\n\n\n46\n20210213\nFebruary 13, 2021\n12\n\n\n46\n20210206\nFebruary 6, 2021\n11\n\n\n46\n20210130\nJanuary 30, 2021\n10\n\n\n46\n20201219\nDecember 19, 2020\n9\n\n\n46\n20201212\nDecember 12, 2020\n8\n\n\n\n\n\n\n\n\n\nSELECT * FROM titles LIMIT 10;\n\n\n\n\n\nTable 5.5: Note the epid, tid, skid, and sid identifiers in the titles table.\n\norder\nepid\ntid\nname\ncategory\nskid\nsid\n\n\n\n0\n20210410\n2.02e+08\nEye On Minnesota\nCold Opening\n\n46\n\n\n1\n20210410\n2.02e+08\n\nMonologue\n\n46\n\n\n2\n20210410\n2.02e+08\nWhat's Wrong With This Picture\nGame Show\n300\n46\n\n\n3\n20210410\n2.02e+08\nTremfalta\nCommercial\n\n46\n\n\n4\n20210410\n2.02e+08\nStudy Buddy\nSketch\n\n46\n\n\n5\n20210410\n2.02e+08\nWeird Little Flute\nMusic Video\n\n46\n\n\n6\n20210410\n2.02e+08\n\"Tequila Shots\"\nMusical Performance\n\n46\n\n\n7\n20210410\n2.02e+08\n\nWeekend Update\n\n46\n\n\n8\n20210410\n2.02e+08\n\nIn Memoriam\n\n46\n\n\n9\n20210410\n2.02e+09\nStarcharter Andromeda\nShow\n\n46\n\n\n\n\n\n\n\n\n\nSELECT * FROM hosts LIMIT 10;\n\n\n\n\n\nTable 5.6: Note the epid and aid identifiers in the hosts table.\n\nepid\naid\n\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n20201212\nTimothee Chalamet\n\n\n\n\n\n\n\n\n\n5.2.2 Creating a temporary table\nThe episodes table has an aired column which includes the data. Recall that if we create a new variable (e.g., year) using aired, we cannot use year in the WHERE clause (WHERE only works on the original table, not the results set).\nIn MySQL the function STR_TO_DATE allowed us to create a datetime variable from which year could be extracted. However, in DuckDB, it is more complicated to convert the character string of “April 10, 2020” to “2020-04-10”. Don’t worry about the code too much, but note that we wouldn’t want to wrangle the character date string every time we wanted to filter for year.\nWhat does POSITION do?\nIn case you are curious about the date wrangling code… consider SUBSTRING(aired, POSITION(',' IN aired) + 2)\n\nPOSITION(',' IN aired): This part of the expression uses the POSITION function to find the position of the first occurrence of the comma (,) in the string aired. The result is the index (position) of the comma within the string.\nPOSITION(',' IN aired) + 2: This adds 2 to the index of the comma. The + 2 is used to move the starting point of the substring two positions to the right of the comma. This is done to exclude the comma itself and any following spaces.\nSUBSTRING(aired, POSITION(',' IN aired) + 2): This part uses the SUBSTRING function to extract a substring from the string aired. The starting position of the substring is determined by POSITION(',' IN aired) + 2, and it goes until the end of the string. This effectively removes the part of the string that comes before and including the first comma.\n\nIn summary, the entire expression is extracting a substring from the original string aired, starting from two positions to the right of the first comma and continuing until the end of the string. This can be useful in scenarios where you want to remove or isolate part of a string based on the position of a specific character (in this case, the comma).\n\nCREATE TEMP TABLE episodes_date AS\n    SELECT *, CASE\n             WHEN POSITION(',' IN aired) &gt; 0 THEN\n    EXTRACT(YEAR FROM CAST(\n                SUBSTRING(aired, POSITION(',' IN aired) + 2) || '-' ||\n                CASE\n                    WHEN POSITION('January' IN aired) &gt; 0 THEN '01'\n                    WHEN POSITION('February' IN aired) &gt; 0 THEN '02'\n                    WHEN POSITION('March' IN aired) &gt; 0 THEN '03'\n                    WHEN POSITION('April' IN aired) &gt; 0 THEN '04'\n                    WHEN POSITION('May' IN aired) &gt; 0 THEN '05'\n                    WHEN POSITION('June' IN aired) &gt; 0 THEN '06'\n                    WHEN POSITION('July' IN aired) &gt; 0 THEN '07'\n                    WHEN POSITION('August' IN aired) &gt; 0 THEN '08'\n                    WHEN POSITION('September' IN aired) &gt; 0 THEN '09'\n                    WHEN POSITION('October' IN aired) &gt; 0 THEN '10'\n                    WHEN POSITION('November' IN aired) &gt; 0 THEN '11'\n                    WHEN POSITION('December' IN aired) &gt; 0 THEN '12'\n                    ELSE '01' -- Default to January if no month is found\n                END || '-' ||\n                SUBSTRING(aired, POSITION(' ' IN aired) + 1, 2) AS DATE\n            ))\n            END AS year FROM episodes;\n\n\nSELECT * FROM episodes_date LIMIT 10;\n\n\n\n\n\nTable 5.7: The temporary table called episodes_date that has identifiers of sid, epid, and epno.\n\nsid\nepid\naired\nepno\nyear\n\n\n\n46\n20210410\nApril 10, 2021\n17\n2021\n\n\n46\n20210403\nApril 3, 2021\n16\n2021\n\n\n46\n20210327\nMarch 27, 2021\n15\n2021\n\n\n46\n20210227\nFebruary 27, 2021\n14\n2021\n\n\n46\n20210220\nFebruary 20, 2021\n13\n2021\n\n\n46\n20210213\nFebruary 13, 2021\n12\n2021\n\n\n46\n20210206\nFebruary 6, 2021\n11\n2021\n\n\n46\n20210130\nJanuary 30, 2021\n10\n2021\n\n\n46\n20201219\nDecember 19, 2020\n9\n2020\n\n\n46\n20201212\nDecember 12, 2020\n8\n2020\n\n\n\n\n\n\n\n\n\n5.2.3 Using a temporary table\nNow that the year variable has been created in the new temporary table called episodes_date, we can use episode_date to query and find, for example, all of the hosts in 2019.\n\nSELECT hosts.aid, ep.aired, ep.year FROM hosts \nJOIN episodes_date AS ep ON hosts.epid = ep.epid\nWHERE year = 2019\nLIMIT 25;\n\n\n\n\n\nTable 5.8: SNL hosts in 2019.\n\naid\naired\nyear\n\n\n\nEddie Murphy\nDecember 21, 2019\n2019\n\n\nScarlett Johansson\nDecember 14, 2019\n2019\n\n\nJennifer Lopez\nDecember 7, 2019\n2019\n\n\nWill Ferrell\nNovember 23, 2019\n2019\n\n\nHarry Styles\nNovember 16, 2019\n2019\n\n\nKristen Stewart\nNovember 2, 2019\n2019\n\n\nChance the Rapper\nOctober 26, 2019\n2019\n\n\nDavid Harbour\nOctober 12, 2019\n2019\n\n\nPhoebe Waller-Bridge\nOctober 5, 2019\n2019\n\n\nWoody Harrelson\nSeptember 28, 2019\n2019\n\n\nPaul Rudd\nMay 18, 2019\n2019\n\n\nEmma Thompson\nMay 11, 2019\n2019\n\n\nAdam Sandler\nMay 4, 2019\n2019\n\n\nEmma Stone\nApril 13, 2019\n2019\n\n\nKit Harington\nApril 6, 2019\n2019\n\n\nSandra Oh\nMarch 30, 2019\n2019\n\n\nIdris Elba\nMarch 9, 2019\n2019\n\n\nJohn Mulaney\nMarch 2, 2019\n2019\n\n\nDon Cheadle\nFebruary 16, 2019\n2019\n\n\nHalsey\nFebruary 9, 2019\n2019\n\n\nJames McAvoy\nJanuary 26, 2019\n2019\n\n\nRachel Brosnahan\nJanuary 19, 2019\n2019"
  },
  {
    "objectID": "05-creating-db.html#efficiencies",
    "href": "05-creating-db.html#efficiencies",
    "title": "5  Creating databases",
    "section": "\n5.2 Efficiencies",
    "text": "5.2 Efficiencies\nIt is worth pointing out a few aspects to loading data into SQL: keys, indexes, and partitioning.\nBefore we get to the definitions, consider this analogy:\n\nEach library (database) has books (tables). Each book (table) has pages (rows). Each page (row) has a unique page number to identify it (key value); to find a particular page, you sort through the page numbers (key values). But it isn’t immediately obvious where the particular page of interest is, you might have to page through the book a little bit to find the page of interest. It would be easier if you had several bookmarks throughout the book to anchor some of the page numbers. For example, if you want page 1047 and you have a bookmark on page 1050, you only have to turn back three pages. The bookmark is an index, it helps you find the desired rows much more quickly.2\n\n\n5.2.1 Key\nKeys are unique identifiers for each row, used primarily for connecting tables. Keys are generally not helpful for efficiency, but they are important for data integrity and relationships between tables. A key is a pointer that identifies a record. In practice, a key is one or more columns that are earmarked to uniquely identify a record in a table. Keys serve two main purposes:\n\nThey provide constraints on the column such as that it can’t store duplicate or null values.\nThey are also used to generate relationships among different tables.\n\n\n\nPRIMARY KEY is a column or set of columns that uniquely identify each row. Primary keys cannot be NULL. Each table must always have one (and only one) PK. The PK can be made up of one column, but if that isn’t enough to uniquely identify the row, more columns may be added. Sometimes it is easier to designate a numeric column (e.g., row number) to be the PK.\n\nFOREIGN KEY is a column or set of columns that reference a primary key in a different table. The FK linkes two tables together, and the link is called a relationship.\n\n(There are other keys such as: Super Key, Minimal Super Key, Candidate Key, Unique Key, Alternate Key, Composite Key, Natural Key, Surrogate Key.)\n\n5.2.2 Index\nIndexes are the crux of why SQL is so much more efficient than, say, R. An index is a lookup table that helps SQL keep track of which records contain certain values. By indexing the rows, SQL is able to optimize sorting and joining tables. The index is created in advance (when the table is created) and saved to disk, which can take up substantial space on the disk. Sometimes more than one variable is used to index the table. There are trade-offs to having a lot of indexes (disk space but fast wrangling) versus a few indexes (slow wrangling but less space).\nA table may have more than one index but you shouldn’t add indexes to every column in a table, as these have to be updated for every addition/update/delete to the column. Rather, indexes should be added to columns that are frequently included in queries.\nIndexes may not make much difference for small databases, but, as tables grow in size, queries benefit more from indexes.\nIn MySQL the commands SHOW KEYS and SHOW INDEXES provide information about the keys and indexes for each table. (Neither operation is available in DuckDB.)\n\n5.2.3 Partitioning\nAnother way to speed up query retrievals is to partition the data tables. If, for example, the SNL queries were always done by year, then the episodes table could be partitioned such that they are stored as separate tables (one per year). The partitioning functions as an index on year. The user would not be able to tell the difference between the unpartitioned episodes table and the partitioned one. However, queries done by year would be faster. Queries done grouped in another way would be slower."
  },
  {
    "objectID": "06-change-db.html",
    "href": "06-change-db.html",
    "title": "6  Changing databases",
    "section": "",
    "text": "from mdsr changing data (update) and adding data (insert, insert ignore, replace)\nalso, go back to some of the select queries, do complicated things, save as a new table, then join."
  },
  {
    "objectID": "p4-reg-expr.html",
    "href": "p4-reg-expr.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions are sequences of characters that define search patterns. Symbolic notation is used to find particular sequences of interest. Regular expressions are used in many different contexts including SQL queries, web scraping, and data wrangling. Chapter 7 covers regular expressions within R, but the syntax for pattern matching is identical across almost all platforms and programming languages. One thing to note is that to escape a metacharacter in R, two backslashes are needed. For example, \\\\d is the correct syntax to denote a digit."
  },
  {
    "objectID": "06-change-db.html#temporary-tables",
    "href": "06-change-db.html#temporary-tables",
    "title": "6  Changing databases",
    "section": "\n6.5 Temporary tables",
    "text": "6.5 Temporary tables\nTemporary tables are used to break down complex queries into smaller, more manageable steps. For example, let’s say we want to JOIN two tables after each has been filtered using different WHERE clauses. The filtered tables can each be saved into their own temporary tables and then the temporary tables can be merged.\nNote that tables in DuckDB are saved (to disk), even when the connection is closed. However, temporary tables are saved in memory (instead of on disk) and are deleted when the connection is closed. Specific configuration of the temporary directory allows for temporary tables to be saved, even when the connection is closed.\nNotice that most of the tables have some kind of ID which allows JOINing across tables.\n\nSELECT * FROM hosts LIMIT 10;\n\n\n\n\n\nTable 6.7: Note the epid and aid identifiers in the hosts table.\n\nepid\naid\n\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n20201212\nTimothee Chalamet\n\n\n\n\n\n\n\n\n\nSELECT * FROM episodes LIMIT 10;\n\n\n\n\n\nTable 6.8: Note the sid and epid identifiers in the episodes table.\n\nsid\nepid\naired\nepno\n\n\n\n46\n20210410\nApril 10, 2021\n17\n\n\n46\n20210403\nApril 3, 2021\n16\n\n\n46\n20210327\nMarch 27, 2021\n15\n\n\n46\n20210227\nFebruary 27, 2021\n14\n\n\n46\n20210220\nFebruary 20, 2021\n13\n\n\n46\n20210213\nFebruary 13, 2021\n12\n\n\n46\n20210206\nFebruary 6, 2021\n11\n\n\n46\n20210130\nJanuary 30, 2021\n10\n\n\n46\n20201219\nDecember 19, 2020\n9\n\n\n46\n20201212\nDecember 12, 2020\n8\n\n\n\n\n\n\n\n\n\n6.5.1 Creating a temporary table\nThe episodes table has an aired column which includes the data. Recall that if we create a new variable (e.g., year) using aired, we cannot use year in the WHERE clause (WHERE only works on the original table, not the results set).\nIn MySQL the function STR_TO_DATE allowed us to create a datetime variable from which year could be extracted. However, in DuckDB, it is more complicated to convert the character string of “April 10, 2020” to “2020-04-10”. Don’t worry about the code too much, but note that we wouldn’t want to wrangle the character date string every time we wanted to filter for year.\nWhat does POSITION do?\nIn case you are curious about the date wrangling code… consider SUBSTRING(aired, POSITION(',' IN aired) + 2)\n\nPOSITION(',' IN aired): This part of the expression uses the POSITION function to find the position of the first occurrence of the comma (,) in the string aired. The result is the index (position) of the comma within the string.\nPOSITION(',' IN aired) + 2: This adds 2 to the index of the comma. The + 2 is used to move the starting point of the substring two positions to the right of the comma. This is done to exclude the comma itself and any following spaces.\nSUBSTRING(aired, POSITION(',' IN aired) + 2): This part uses the SUBSTRING function to extract a substring from the string aired. The starting position of the substring is determined by POSITION(',' IN aired) + 2, and it goes until the end of the string. This effectively removes the part of the string that comes before and including the first comma.\n\nIn summary, the entire expression is extracting a substring from the original string aired, starting from two positions to the right of the first comma and continuing until the end of the string. This can be useful in scenarios where you want to remove or isolate part of a string based on the position of a specific character (in this case, the comma).\n\nCREATE TEMP TABLE episodes_date AS\n    SELECT *, CASE\n             WHEN POSITION(',' IN aired) &gt; 0 THEN\n    EXTRACT(YEAR FROM CAST(\n                SUBSTRING(aired, POSITION(',' IN aired) + 2) || '-' ||\n                CASE\n                    WHEN POSITION('January' IN aired) &gt; 0 THEN '01'\n                    WHEN POSITION('February' IN aired) &gt; 0 THEN '02'\n                    WHEN POSITION('March' IN aired) &gt; 0 THEN '03'\n                    WHEN POSITION('April' IN aired) &gt; 0 THEN '04'\n                    WHEN POSITION('May' IN aired) &gt; 0 THEN '05'\n                    WHEN POSITION('June' IN aired) &gt; 0 THEN '06'\n                    WHEN POSITION('July' IN aired) &gt; 0 THEN '07'\n                    WHEN POSITION('August' IN aired) &gt; 0 THEN '08'\n                    WHEN POSITION('September' IN aired) &gt; 0 THEN '09'\n                    WHEN POSITION('October' IN aired) &gt; 0 THEN '10'\n                    WHEN POSITION('November' IN aired) &gt; 0 THEN '11'\n                    WHEN POSITION('December' IN aired) &gt; 0 THEN '12'\n                    ELSE '01' -- Default to January if no month is found\n                END || '-' ||\n                SUBSTRING(aired, POSITION(' ' IN aired) + 1, 2) AS DATE\n            ))\n            END AS year FROM episodes;\n\n\nSELECT * FROM episodes_date LIMIT 10;\n\n\n\n\n\nTable 6.9: The temporary table called episodes_date that has identifiers of sid, epid, and epno.\n\nsid\nepid\naired\nepno\nyear\n\n\n\n46\n20210410\nApril 10, 2021\n17\n2021\n\n\n46\n20210403\nApril 3, 2021\n16\n2021\n\n\n46\n20210327\nMarch 27, 2021\n15\n2021\n\n\n46\n20210227\nFebruary 27, 2021\n14\n2021\n\n\n46\n20210220\nFebruary 20, 2021\n13\n2021\n\n\n46\n20210213\nFebruary 13, 2021\n12\n2021\n\n\n46\n20210206\nFebruary 6, 2021\n11\n2021\n\n\n46\n20210130\nJanuary 30, 2021\n10\n2021\n\n\n46\n20201219\nDecember 19, 2020\n9\n2020\n\n\n46\n20201212\nDecember 12, 2020\n8\n2020\n\n\n\n\n\n\n\n\n\n6.5.2 Using a temporary table\nNow that the year variable has been created in the new temporary table called episodes_date, we can use episode_date to query and find, for example, all of the hosts in 2019.\n\nSELECT hosts.aid, ep.aired, ep.year FROM hosts \nJOIN episodes_date AS ep ON hosts.epid = ep.epid\nWHERE year = 2019\nLIMIT 25;\n\n\n\n\n\nTable 6.10: SNL hosts in 2019.\n\naid\naired\nyear\n\n\n\nEddie Murphy\nDecember 21, 2019\n2019\n\n\nScarlett Johansson\nDecember 14, 2019\n2019\n\n\nJennifer Lopez\nDecember 7, 2019\n2019\n\n\nWill Ferrell\nNovember 23, 2019\n2019\n\n\nHarry Styles\nNovember 16, 2019\n2019\n\n\nKristen Stewart\nNovember 2, 2019\n2019\n\n\nChance the Rapper\nOctober 26, 2019\n2019\n\n\nDavid Harbour\nOctober 12, 2019\n2019\n\n\nPhoebe Waller-Bridge\nOctober 5, 2019\n2019\n\n\nWoody Harrelson\nSeptember 28, 2019\n2019\n\n\nPaul Rudd\nMay 18, 2019\n2019\n\n\nEmma Thompson\nMay 11, 2019\n2019\n\n\nAdam Sandler\nMay 4, 2019\n2019\n\n\nEmma Stone\nApril 13, 2019\n2019\n\n\nKit Harington\nApril 6, 2019\n2019\n\n\nSandra Oh\nMarch 30, 2019\n2019\n\n\nIdris Elba\nMarch 9, 2019\n2019\n\n\nJohn Mulaney\nMarch 2, 2019\n2019\n\n\nDon Cheadle\nFebruary 16, 2019\n2019\n\n\nHalsey\nFebruary 9, 2019\n2019\n\n\nJames McAvoy\nJanuary 26, 2019\n2019\n\n\nRachel Brosnahan\nJanuary 19, 2019\n2019"
  },
  {
    "objectID": "06-change-db.html#best-practice",
    "href": "06-change-db.html#best-practice",
    "title": "6  Changing databases",
    "section": "\n6.6 Best practice",
    "text": "6.6 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)"
  },
  {
    "objectID": "06-change-db.html#changing-data",
    "href": "06-change-db.html#changing-data",
    "title": "6  Changing databases",
    "section": "\n6.1 Changing data",
    "text": "6.1 Changing data\nThe UPDATE function allows you to change a value in a table across all rows that match a certain criteria. The impressions table has a name column indicating the person being impersonated. Let’s say, for whatever reason, that Ivanka Trump decides she doesn’t want to be affiliated with the Trump name and she changes her name to her husband’s name, becoming Ivanka Kushner. You might want to UPDATE the file to indicate the impressions were of Ivanka Kushner instead of Ivanka Trump. (See Section 5.1.6 for loading csv files into DuckDB directly.)\n\nduckdb_read_csv(con = con_duckdb, name = \"impressions\", files = \"data/impressions.csv\")\n\n\nSELECT * FROM impressions \n   WHERE name LIKE 'Ivanka%';\n\n\n\n\n\nTable 6.1: Finding the impersonations of Ivanka Trump.\n\nimpid\naid\nname\n\n\n\n2598\nScarlett Johansson\nIvanka Trump\n\n\n3716\nEmily Blunt\nIvanka Trump\n\n\n3694\nMargot Robbie\nIvanka Trump\n\n\n3679\nVanessa Bayer\nIvanka Trump\n\n\n2340\nMaya Rudolph\nIvanka Trump\n\n\n\n\n\n\n\n\nWe can use the UPDATE function to change the value of Ivanka’s name to Ivanka Kushner throughout the database. Note that all rows which match the WHERE clause get updated.\n\nUPDATE impressions\n   SET name = 'Ivanka Kushner'\n   WHERE name LIKE 'Ivanka%';\n\n\nSELECT * FROM impressions \n   WHERE name LIKE 'Ivanka%';\n\n\n\n\n\nTable 6.2: Ivanka’s last name has been UPDATEd to Kushner.\n\nimpid\naid\nname\n\n\n\n2598\nScarlett Johansson\nIvanka Kushner\n\n\n3716\nEmily Blunt\nIvanka Kushner\n\n\n3694\nMargot Robbie\nIvanka Kushner\n\n\n3679\nVanessa Bayer\nIvanka Kushner\n\n\n2340\nMaya Rudolph\nIvanka Kushner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Watch out!\n\n\n\nBe careful with UPDATE. A careless UPDATE could write over all of the data in your table. There is no undo function."
  },
  {
    "objectID": "06-change-db.html#inserting-data",
    "href": "06-change-db.html#inserting-data",
    "title": "6  Changing databases",
    "section": "\n6.2 Inserting data",
    "text": "6.2 Inserting data\nLet’s say we want to include the more recent hosts in the hosts table. First, we scrape the SNL archives which lists the episode id (the date) and the host. The R package rvest allows us to pull out the appropriate html elements. The epid and aid are joined together in a tibble, and filtered to only include episodes which are not already in the episodes table. (See Section 5.1.6 for loading csv files into DuckDB directly.)\n\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\n\nBy searching the SNL archives, we can see that the next host, chronologically was Elon Musk on May 8, 2021.\n\nSELECT * FROM hosts\n    ORDER BY epid DESC\n    LIMIT 10;\n\n\n\n\n\nTable 6.3: Most recent hosts in the original hosts table.\n\nepid\naid\n\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n20201212\nTimothee Chalamet\n\n\n\n\n\n\n\n\nINSERT allows us to add the relevant information associated with the episode of SNL that Elon Musk hosted.\n\nINSERT INTO hosts (epid, aid)\n   VALUES ('20210508', 'Elon Musk');\n\n\nSELECT * FROM hosts\n    ORDER BY epid DESC\n    LIMIT 10;\n\n\n\n\n\nTable 6.4: hosts table including the added observation from May 8, 2021.\n\nepid\naid\n\n\n\n20210508\nElon Musk\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n\n\n\n\n\n\nIt would be tedious to INSERT all of the most recent host information by hand. Instead, we’ll scrape the SNL archives using the R package rvest, which allows us to pull out the appropriate html elements. The epid and aid are joined together in a tibble, and filtered to only include episodes which are not already in the episodes table.\n\nlibrary(rvest)\n\nrecent_hosts &lt;- read_html(\"http://www.snlarchives.net/Episodes/\") |&gt;\n  html_nodes(\"tr\") |&gt;\n  purrr::map_df( ~ tibble(\n    epid = .x |&gt; html_node(\"a.ms-2.me-2\") |&gt;\n      html_attr(\"href\") |&gt;\n      str_extract(\"\\\\d+\"),\n    aid = .x |&gt; html_node(\"td:nth-child(2)\") |&gt;\n      html_text2() |&gt;\n      str_extract(\"[\\\\w\\\\. \\\\w\\\\.]+(?=/|$)\")\n  )) |&gt;\n  filter(epid &gt; 20210508)\n\n\nwrite_csv(recent_hosts, \"data/recent_hosts.csv\")\n\n\nINSERT INTO hosts\n   SELECT *\n   FROM READ_CSV('data/recent_hosts.csv', AUTO_DETECT = TRUE);\n\n\nSELECT * FROM hosts\n  ORDER BY epid DESC\n  LIMIT 10;\n\n\n\n\n\nTable 6.5: The full hosts table, updated through December 16, 2023.\n\nepid\naid\n\n\n\n20231216\nKate McKinnon\n\n\n20231209\nAdam Driver\n\n\n20231202\nEmma Stone\n\n\n20231118\nJason Momoa\n\n\n20231111\nTimothée Chalamet\n\n\n20231028\nNate Bargatze\n\n\n20231021\nBad Bunny\n\n\n20231014\nPete Davidson\n\n\n20230415\nAna de Armas\n\n\n20230408\nMolly Shannon"
  },
  {
    "objectID": "06-change-db.html#deleting-data",
    "href": "06-change-db.html#deleting-data",
    "title": "6  Changing databases",
    "section": "\n6.3 Deleting data",
    "text": "6.3 Deleting data\nYou might change your mind and decide that you really only want hosts from years up to 2022. The DELETE function deletes any rows specified by the WHERE clause.\n\nDELETE FROM hosts\n   WHERE epid &gt; 20221231\n\n\nSELECT * FROM hosts\n  ORDER BY epid DESC\n  LIMIT 10;\n\n\n\n\n\nTable 6.6: The hosts table, after 2023 has been DELETEd.\n\nepid\naid\n\n\n\n20221217\nAustin Butler\n\n\n20221210\nMartin Short\n\n\n20221203\nKeke Palmer\n\n\n20221112\nDave Chappelle\n\n\n20221105\nAmy Schumer\n\n\n20221029\nJack Harlow\n\n\n20221015\nMegan Thee Stallion\n\n\n20221008\nBrendan Gleeson\n\n\n20221001\nMiles Teller\n\n\n20220521\nNatasha Lyonne"
  },
  {
    "objectID": "01-db.html#fa-lightbulb-reflection-questions",
    "href": "01-db.html#fa-lightbulb-reflection-questions",
    "title": "1  Databases",
    "section": "\n1.5 reflection questions",
    "text": "1.5 reflection questions"
  },
  {
    "objectID": "01-db.html#fa-balance-scale-ethics-considerations",
    "href": "01-db.html#fa-balance-scale-ethics-considerations",
    "title": "1  Databases",
    "section": "\n1.6 ethics considerations",
    "text": "1.6 ethics considerations"
  },
  {
    "objectID": "01-db.html#reflection-questions",
    "href": "01-db.html#reflection-questions",
    "title": "1  Databases",
    "section": "\n1.6  Reflection questions",
    "text": "1.6  Reflection questions\n\nWhy is SQL such an important tool for data scientists? That is, what are the characteristics that make it useful?\nWhat is a relational database?\n(Maybe best to answer after learning a few more concepts.) What are the main differences between working with a data frame in R and a table in SQL?\nHow does one connect to a SQL database? Using R? Using DuckDB?"
  },
  {
    "objectID": "01-db.html#ethics-considerations",
    "href": "01-db.html#ethics-considerations",
    "title": "1  Databases",
    "section": "\n1.7  Ethics considerations",
    "text": "1.7  Ethics considerations\n\nWhy should you need a password to access a SQL server?\nWhat other skills for working with databases are important, beyond accessing, wrangling, and creating databases? (E.g., provenance of the data, purpose of the data’s use, protection of data, privacy of data… etc. Why / how are those important?)"
  },
  {
    "objectID": "04-sql-joins.html#reflection-questions",
    "href": "04-sql-joins.html#reflection-questions",
    "title": "4  Combining tables in SQL",
    "section": "\n4.5  Reflection questions",
    "text": "4.5  Reflection questions\n\nWhat are the different types of joins? Which data from which table gets kept and which gets removed for each type of join?\nWhat is the difference between a join and a union?\nWhen working with multiple tables, how (and why) is a variable linked to its table?\nConsider a RIGHT JOIN. If there are records in the right table that are not in the left table, what will the value of the left table variable be for those records?"
  },
  {
    "objectID": "04-sql-joins.html#ethics-considerations",
    "href": "04-sql-joins.html#ethics-considerations",
    "title": "4  Combining tables in SQL",
    "section": "\n4.6  Ethics considerations",
    "text": "4.6  Ethics considerations\n\nWhat can happen if a UNION is done without carefully matching up the columns of the two tables being UNIONed?\nHow will you know if JOINing removed some records? What if the JOIN produced missing values for some of the variables? How should we deal with missing data or arbitrarily removed records?"
  },
  {
    "objectID": "03-sql-verbs.html#reflection-questions",
    "href": "03-sql-verbs.html#reflection-questions",
    "title": "3  SQL clauses",
    "section": "\n3.11  Reflection questions",
    "text": "3.11  Reflection questions\n\nWhy don’t we usually want to run the query: SELECT * FROM table;?\nWhat is the difference between the original table and the results set?\nIn SQL does the WHERE clause use = or == to indicate equality?\nDoes BETWEEN work only on numeric variables or also on character strings?\nWhat syntax is used to direct ORDER BY to sort by biggest to smallest or smallest to biggest?\nWhat is the difference between WHERE and HAVING?"
  },
  {
    "objectID": "03-sql-verbs.html#ethics-considerations",
    "href": "03-sql-verbs.html#ethics-considerations",
    "title": "3  SQL clauses",
    "section": "\n3.12  Ethics considerations",
    "text": "3.12  Ethics considerations\n\nWhat are different ways to look at the dataset to identify possible typos or rogue values?\nWhy are such tasks so much harder with large datasets (versus small datasets)?\nWhy are such tasks to much more important with large datasets (versus small datasets)?"
  },
  {
    "objectID": "02-sql-in-R.html#reflection-questions",
    "href": "02-sql-in-R.html#reflection-questions",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.5  Reflection questions",
    "text": "2.5  Reflection questions\n\nWhat are the three main ways to write a SQL query using the RStudio interface?\nHow is DBeaver similar and/or different from writing queries using **R*?\nWhy can’t you use collect() to pull the flights data into your R session?"
  },
  {
    "objectID": "02-sql-in-R.html#ethics-considerations",
    "href": "02-sql-in-R.html#ethics-considerations",
    "title": "2  SQL in R and DBeaver",
    "section": "\n2.6  Ethics considerations",
    "text": "2.6  Ethics considerations\n\nHow / why is Sys.getenv() used to protect the username and password for the SQL connection?\nIf SQL databases are expensive to maintain, who will then have access to important data? Does it matter?"
  },
  {
    "objectID": "01-db.html#the-airlines-database",
    "href": "01-db.html#the-airlines-database",
    "title": "1  Databases",
    "section": "\n1.2 The airlines database",
    "text": "1.2 The airlines database\nTo demonstrate the difference between data frames in memory and tables in storage, we will consider the airlines data consisting of millions of individual flights between 2010 and 2017. The flights are downloaded from the Bureau of Transportation Statistics, US Department of Transportation. The database is a superset of the nycflights13 R package that tracks only flights in and out of airports serving New York City in 2013.\nThe full data set occupies almost 20GB when they are saved as CSV (comma separated value) files, a common way to hold data that can be represented as columns of text which are separated by commas."
  },
  {
    "objectID": "07-reg-expr.html#r-packages-to-make-your-life-easier",
    "href": "07-reg-expr.html#r-packages-to-make-your-life-easier",
    "title": "7  Regular Expressions",
    "section": "\n7.1 R packages to make your life easier",
    "text": "7.1 R packages to make your life easier\n\n\nstringr package A core package in the tidyverse. It is installed via install.packages(\"tidyverse\") and also loaded via library(tidyverse). Of course, you can also install or load it individually.\n\nMany of the main functions start with str_. Auto-complete is your friend.\nReplacements for base functions re: string manipulation and regular expressions (see below).\nMain advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task.\nstringr cheat sheet\n\n\n\ntidyr package Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract().\nBase functions: nchar(), strsplit(), substr(), paste(), paste0().\nThe glue package is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package.\n\nString functions related to regular expression\nRegular expression is a pattern that describes a specific set of strings with a common structure. It is heavily used for string matching / replacing in all programming languages, although specific syntax may differ a bit. It is truly the heart and soul for string operations. In R, many string functions in base R as well as in stringr package use regular expressions, even RStudio’s search and replace allows regular expression:\n\nidentify match to a pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()\n\nextract match to a pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()\n\nlocate pattern within a string, i.e. give the start position of matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()\n\nreplace a pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()\n\nsplit a string using a pattern: strsplit(), stringr::str_split()\n\n\nRegular expressions typically specify characters (or character classes) to seek out, possibly with information about repeats and location within the string. This is accomplished with the help of metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. We will use some small examples to introduce regular expression syntax and what these metacharacters mean."
  },
  {
    "objectID": "07-reg-expr.html#tools-for-characterizing-a-regular-expression",
    "href": "07-reg-expr.html#tools-for-characterizing-a-regular-expression",
    "title": "7  Regular Expressions",
    "section": "\n7.2 Tools for characterizing a regular expression",
    "text": "7.2 Tools for characterizing a regular expression\n\n7.2.1 Escape sequences\nThere are some special characters in R that cannot be directly coded in a string. For example, let’s say you specify your pattern with single quotes and you want to find countries with the single quote '. You would have to “escape” the single quote in the pattern, by preceding it with \\, so it is clear that it is not part of the string-specifying machinery.\nThere are other characters in R that require escaping, and this rule applies to all string functions in R, including regular expressions. See here for a complete list of R escape sequences.\n\n\n\\': single quote. You don’t need to escape single quote inside a double-quoted string, so we can also use \" ' \" in the previous example.\n\n\n\\\": double quote. Similarly, double quotes can be used inside a single-quoted string, i.e. ' \" '.\n\n\n\\n: newline.\n\n\n\\r: carriage return.\n\n\n\\t: tab character.\n\n\nNote: cat() and print() handle escape sequences differently, if you want to print a string out with the interpretation of the sequences above, use cat().\n\n\nprint(\"a\\nb\")\n\n[1] \"a\\nb\"\n\ncat(\"a\\nb\")\n\na\nb\n\n\n\n7.2.2 Quantifiers\nQuantifiers specify how many repetitions of the pattern.\n\n\n*: matches at least 0 times.\n\n\n+: matches at least 1 times.\n\n\n?: matches at most 1 times.\n\n\n{n}: matches exactly n times.\n\n\n{n,}: matches at least n times.\n\n\n{n,m}: matches between n and m times.\n\n\n(strings &lt;- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\"))\n\n[1] \"a\"      \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = TRUE)\n\n[1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = FALSE)\n\n[1] 2 3 4 5 6\n\ngrep(\"ac+b\", strings, value = TRUE)\n\n[1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac?b\", strings, value = TRUE)\n\n[1] \"ab\"  \"acb\"\n\ngrep(\"ac{2}b\", strings, value = TRUE)\n\n[1] \"accb\"\n\ngrep(\"ac{2}b\", strings, value = FALSE)\n\n[1] 4\n\ngrep(\"ac{2,}b\", strings, value = TRUE)\n\n[1] \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac{2,3}b\", strings, value = TRUE)\n\n[1] \"accb\"  \"acccb\"\n\n\n\n7.2.3 Position of pattern within the string\n\n\n^: matches the start of the string.\n\n\n$: matches the end of the string.\n\n\n\\b: matches the empty string at either edge of a word. Don’t confuse it with ^ $ which marks the edge of a string.\n\n\n\\B: matches the empty string provided it is not at an edge of a word.\n\n\n(strings &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\"))\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"ab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"abcd\"\n\ngrep(\"ab$\", strings, value = TRUE)\n\n[1] \"cdab\"\n\ngrep(\"\\\\bab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"c abd\"\n\n\n\n7.2.4 Operators\n\n\n.: matches any single character, as shown in the first example.\n\n[...]: a character list, matches any one of the characters inside the square brackets. We can also use - inside the brackets to specify a range of characters.\n\n\n[^...]: an inverted character list, similar to [...], but matches any characters except those inside the square brackets.\n\n\n\\: suppress the special meaning of metacharacters in regular expression, i.e. $ * + . ? [ ] ^ { } | ( ) \\, similar to its usage in escape sequences. Since \\ itself needs to be escaped in R, we need to escape these metacharacters with double backslash like \\\\$.\n\n\n|: an “or” operator, matches patterns on either side of the |.\n\n\n(...): grouping in regular expressions. This allows you to retrieve the bits that matched various parts of your regular expression so you can alter them or use them for building up a new string. Each group can than be refer using \\\\N, with N being the No. of (...) used. This is called backreference.\n\n\n(strings &lt;- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\"))\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab.\", strings, value = TRUE)\n\n[1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab[c-e]\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\" \"abe\"\n\ngrep(\"ab[^c]\", strings, value = TRUE)\n\n[1] \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"\\\\^ab\", strings, value = TRUE)\n\n[1] \"^ab\"\n\ngrep(\"abc|abd\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\"\n\ngsub(\"(ab) 12\", \"\\\\1 34\", strings)\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 34\"\n\n\n\n7.2.5 Character classes\nCharacter classes allow specifying entire classes of characters, such as numbers, letters, etc. There are two flavors of character classes, one uses [: and :] around a predefined name inside square brackets and the other uses \\ and a special character. They are sometimes interchangeable.\n\n\n[:digit:] or \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent to [0-9].\n\n\n\\D: non-digits, equivalent to [^0-9].\n\n\n[:lower:]: lower-case letters, equivalent to [a-z].\n\n\n[:upper:]: upper-case letters, equivalent to [A-Z].\n\n\n[:alpha:]: alphabetic characters, equivalent to [[:lower:][:upper:]] or [A-z].\n\n\n[:alnum:]: alphanumeric characters, equivalent to [[:alpha:][:digit:]] or [A-z0-9].\n\n\n\\w: word characters, equivalent to [[:alnum:]_] or [A-z0-9_].\n\n\n\\W: not word, equivalent to [^A-z0-9_].\n\n\n[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f, equivalent to [0-9A-Fa-f].\n\n[:blank:]: blank characters, i.e. space and tab.\n\n\n[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\n\n\\s: space, . Matches any whitespace (space, tab, newline, and carriage return).\n\n\\S: not space.\n\n\n[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~.\n\n[:graph:]: graphical (human readable) characters: equivalent to [[:alnum:][:punct:]].\n\n[:print:]: printable characters, equivalent to [[:alnum:][:punct:]\\\\s].\n\n[:cntrl:]: control characters, like \\n or \\r, [\\x00-\\x1F\\x7F].\n\nNote:\n* [:...:] has to be used inside square brackets, e.g. [[:digit:]].\n* \\ itself is a special character that needs escape, e.g. \\\\d. Do not confuse these regular expressions with R escape sequences such as \\t."
  },
  {
    "objectID": "07-reg-expr.html#reflection-questions",
    "href": "07-reg-expr.html#reflection-questions",
    "title": "7  Regular Expressions",
    "section": "\n7.8  Reflection questions",
    "text": "7.8  Reflection questions"
  },
  {
    "objectID": "07-reg-expr.html#ethics-considerations",
    "href": "07-reg-expr.html#ethics-considerations",
    "title": "7  Regular Expressions",
    "section": "\n7.9  Ethics considerations",
    "text": "7.9  Ethics considerations"
  },
  {
    "objectID": "07-reg-expr.html#examples-to-work-through",
    "href": "07-reg-expr.html#examples-to-work-through",
    "title": "7  Regular Expressions",
    "section": "\n7.3 Examples to work through",
    "text": "7.3 Examples to work through\nI have found that the best way to truly understand regular expressions is to work through as many examples as possible (actually, maybe this is true about learning anything new!). For the following examples, try to figure out the solution on your own before looking at the footnote which contains the solution.\n\n7.3.1 Proper times and dates\n\nMatch dates like 01/15/24 and also like 01.15.24 and like 01-15-24.1\nMatch a time of day such as “9:17 am” or “12:30 pm”. Require that the time be a valid time (not “99:99 pm”). Assume no leading zeros (i.e., “09:17 am”).2\n\n7.3.2 Alternation operator\nThe “or” operator, | has the lowest precedence and parentheses have the highest precedence, which means that parentheses get evaluated before “or”.\n\nWhat is the difference between “\\bMary|Jane|Sue\\b” and “\\b(Mary|Jane|Sue)\\b”?3\n\n\n7.3.3 An example from my work\nBelow are a handful of string characters that represent genomic sequences which were measured in an RNA Sequencing dataset. The task below is to find intergenic regions (IGR) and identify which coding sequences (CDS) bookend the intergenic regions. Note that IGRs do not code for proteins while CDSs do. Additionally, AS refers to anti-sense which identifies the genomic sequence in the opposite orientation (e.g., CGGATCC vs CCTAGGC). [The code below was written by Madison Hobbs, Scripps ’19.]\nThe names of the genomic pieces\n\nallCounts &lt;- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\n\nFirst, it is important to identify which are IGR, CDS, and anti-sense.\n\nallCounts &lt;- allCounts |&gt; tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts\n\n  feature\n1     CDS\n2     CDS\n3     IGR\n4  AS_IGR\n5     IGR\n                                                                                                                                                                                       rest\n1                                                                                                                                                                                     b2743\n2                                                                                                                                                                                     b2764\n3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                                                                                                                     GeneidBackup\n1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nWe keep only the IGR and AS_IGR strings, and we separate the two bookends. Note, the separation comes at the backslash.\n\nigr &lt;- allCounts |&gt; filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr &lt;- igr |&gt; tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)\n\n[1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nFor each of the two bookend Genes, we need to separate out the feature from the rest. Note that we write over feature1 in the second line of code below. Both of the bookends for all sequences are CDS elements.\n\nigr$feature1 &lt;- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 &lt;- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 &lt;- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)\n\n[1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2\n1      CDS      CDS\n2      CDS      CDS\n3      CDS      CDS\n\n\nAs CDS, it is now important to find the actual genenames for each of the IGR sequences. We also keep each element’s bnum which represents a unique gene identifier in E. coli.\nbnum, genename, rna.name act as place holders for the types of elements that we will need to identify the bookends of the IGRs.\n\nbnum = \"b[0-9]{4}\"\nbnum\n\n[1] \"b[0-9]{4}\"\n\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\n\n\nigr$start.gene &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr &lt;- igr |&gt; tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma) |&gt; \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma)\nnames(igr)\n\n [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"  \n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2 start.gene end.gene start.bnum end.bnum\n1      CDS      CDS        mlc     ynfL      b1594    b1595\n2      CDS      CDS       talB      mog      b0008    b0009\n3      CDS      CDS       yoaA     yoaB      b1808    b1809"
  },
  {
    "objectID": "07-reg-expr.html#footnotes",
    "href": "07-reg-expr.html#footnotes",
    "title": "7  Regular Expressions",
    "section": "",
    "text": "“\\d\\d.\\d\\d.\\d\\d” will work, but it will also match 123456. It is better to replace the dot with the characters of interest: “\\d\\d[/.\\-]\\d\\d[/.\\-]\\d\\d”. Remember that a dot inside a character class is just a dot. ↩︎\n“(1[012]|[1-9]):[0-5][0-9] (am|pm)”↩︎\nIn the former, the regex will search for “\\bMary” or “Jane” or “Sue\\b”. In the latter, the regex will search for “\\bMary\\b or”\\bJane\\b” or “\\bSue\\b”. That is, Janet will match the former but not the latter.↩︎\nThanks to Ciaran Evans at Wake Forest University for example and code, https://sta279-f23.github.io/↩︎"
  },
  {
    "objectID": "07-reg-expr.html#lookaround",
    "href": "07-reg-expr.html#lookaround",
    "title": "7  Regular Expressions",
    "section": "\n7.4 Lookaround",
    "text": "7.4 Lookaround\nA lookaround specifies a place in the regular expression that will anchor the string you’d like to match. There are four types of lookarounds: positive lookahead, positive lookbehind, negative lookahead, and negative lookbehind.\n\n“x(?=y)” – positive lookahead (matches ‘x’ when it is followed by ‘y’)\n“x(?!y)” – negative lookahead (matches ‘x’ when it is not followed by ‘y’)\n“x(?&lt;=y)” – positive lookbehind (matches ‘x’ when it is preceded by ‘y’)\n“x(?&lt;!y)” – negative lookbehind (matches ‘x’ when it is not preceded by ‘y’)\n\nNote that the lookaround specifies a place in the string which means it does not return the details of the lookaround. Using lookarounds, you can test strings against patterns without including the lookaround pattern in the resulting match.\n\n\n\n\nFigure 7.1: Image credit: Stefan Judis https://www.stefanjudis.com/blog/a-regular-expression-lookahead-lookbehind-cheat-sheet/"
  },
  {
    "objectID": "07-reg-expr.html#example---taskmaster",
    "href": "07-reg-expr.html#example---taskmaster",
    "title": "7  Regular Expressions",
    "section": "\n7.5 Example - Taskmaster",
    "text": "7.5 Example - Taskmaster\nIn the following example, we will wrangle some data scraped from the wiki site for the TV series, Taskmaster. We won’t cover the html scraping here, but I include the code for completeness.\n\n\n\n\nFigure 7.2: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\n\n7.5.1 Scraping and wrangling Taskmaster\nGoal: to scrape the Taskmaster wiki into a dataframe including task, description, episode, episode name, air date, contestant, score, and series.4\n\nresults &lt;- read_html(\"https://taskmaster.fandom.com/wiki/Series_11\") |&gt;\n  html_element(\".tmtable\") |&gt; \n  html_table() |&gt;\n  mutate(episode = ifelse(startsWith(Task, \"Episode\"), Task, NA)) |&gt;\n  fill(episode, .direction = \"down\") |&gt;\n  filter(!startsWith(Task, \"Episode\"), \n         !(Task %in% c(\"Total\", \"Grand Total\"))) |&gt;\n  pivot_longer(cols = -c(Task, Description, episode),\n               names_to = \"contestant\",\n               values_to = \"score\") |&gt;\n  mutate(series = 11)\n\n\nresults |&gt; \n  select(Task, Description, episode, contestant, score, series) |&gt;\n  head(10)\n\n# A tibble: 10 × 6\n  Task  Description                              episode contestant score series\n  &lt;chr&gt; &lt;chr&gt;                                    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1 1     Prize: Best thing you can carry, but on… Episod… Charlotte… 1         11\n2 1     Prize: Best thing you can carry, but on… Episod… Jamali Ma… 2         11\n3 1     Prize: Best thing you can carry, but on… Episod… Lee Mack   4         11\n4 1     Prize: Best thing you can carry, but on… Episod… Mike Wozn… 5         11\n5 1     Prize: Best thing you can carry, but on… Episod… Sarah Ken… 3         11\n6 2     Do the most impressive thing under the … Episod… Charlotte… 2         11\n# ℹ 4 more rows\n\n\nmore succinct results\n\n   Task  Description         episode   contestant score series\n  1     Prize: Best thing…  Episode 1… Charlotte… 1         11\n  1     Prize: Best thing…  Episode 1… Jamali Ma… 2         11\n  1     Prize: Best thing…  Episode 1… Lee Mack   4         11\n  1     Prize: Best thing…  Episode 1… Mike Wozn… 5         11\n  1     Prize: Best thing…  Episode 1… Sarah Ken… 3         11\n  2     Do the most…        Episode 1… Charlotte… 2         11\n  2     Do the most…        Episode 1… Jamali Ma… 3[1]      11\n  2     Do the most…        Episode 1… Lee Mack   3         11\n  2     Do the most…        Episode 1… Mike Wozn… 5         11\n  2     Do the most…        Episode 1… Sarah Ken… 4         11\n\nCurrently, the episode column contains entries like\n\n\"Episode 1: It's not your fault. (18 March 2021)\"\n\n\n7.5.2 Cleaning the score column\n\ntable(results$score)\n\n\n   –    ✔    ✘    0    1    2    3 3[1] 3[2]    4 4[2]    5   DQ \n   7    1    1   11   37   42   48    1    3   50    1   55   13 \n\n\nHow should the scores be stored? What is the cleaning task?\n\n\n\n\nFigure 7.3: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\nExtracting numeric information\nSuppose we have the following string:\n\n\"3[1]\"\n\nAnd we want to extract just the number “3”:\n\nstr_extract(\"3[1]\", \"3\")\n\n[1] \"3\"\n\n\nWhat if we don’t know which number to extract?\n\nstr_extract(\"3[1]\", \"\\\\d\")\n\n[1] \"3\"\n\n\n\nstr_extract(\"4[1]\", \"\\\\d\")\n\n[1] \"4\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d\")\n\n[1] \"1\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d+\")\n\n[1] \"10\"\n\n\n\nstr_extract(\"DQ\", \"\\\\d\")\n\n[1] NA\n\n\nstr_extract()\nstr_extract() is an R function in the stringr package which finds regular expressions in strings of text.\n\nstr_extract(\"My cat is 3 years old\", \"cat\")\n\n[1] \"cat\"\n\n\n\nstr_extract(\"My cat is 3 years old\", \"3\")\n\n[1] \"3\"\n\n\nMatching multiple options\nstr_extract() returns the first match; str_extract_all() allows more than one match.\n\nstr_extract(\"My cat is 3 years old\", \"cat|dog\")\n\n[1] \"cat\"\n\nstr_extract(\"My dog is 10 years old\", \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract(\"My dog is 10 years old, my cat is 3 years old\", \n            \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract_all(\"My dog is 10 years old, my cat is 3 years old\", \n                \"cat|dog\")\n\n[[1]]\n[1] \"dog\" \"cat\"\n\n\nMatching groups of characters\nWhat if I want to extract a number?\n\nstr_extract(\"My cat is 3 years old\", \"\\\\d\")\n\n[1] \"3\"\n\n\nWhat will the result be for the following code?\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n[1] \"1\"\n\n\nThe + symbol in a regular expression means “repeated one or more times”\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d+\")\n\n[1] \"10\"\n\n\nExtracting from multiple strings\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"3\"  \"10\"\n\n\nWhat if we have multiple instances across multiple strings? We need to be careful working with lists (instead of vectors).\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\w+\")\n\n[1] \"My\" \"My\"\n\nstr_extract_all(strings, \"\\\\w+\")\n\n[[1]]\n[1] \"My\"    \"cat\"   \"is\"    \"3\"     \"years\" \"old\"  \n\n[[2]]\n[1] \"My\"    \"dog\"   \"is\"    \"10\"    \"years\" \"old\""
  },
  {
    "objectID": "07-reg-expr.html#goal-taskmaster-information",
    "href": "07-reg-expr.html#goal-taskmaster-information",
    "title": "7  Regular Expressions",
    "section": "\n7.6 Goal: Taskmaster information",
    "text": "7.6 Goal: Taskmaster information\nOur goal is to scrape the Taskmaster wiki to create a dataframe which includes the task, description, episode, episode name, air date, contestant, score, and series.4\n\n   Task  Description     episode episode_name air_date contestant score series\n 1 1     Prize: Best th… 1       \"It's not y… 18 Marc… Charlotte… 1         11\n 2 1     Prize: Best th… 1       \"It's not y… 18 Marc… Jamali Ma… 2         11\n 3 1     Prize: Best th… 1       \"It's not y… 18 Marc… Lee Mack   4         11\n 4 1     Prize: Best th… 1       \"It's not y… 18 Marc… Mike Wozn… 5         11\n 5 1     Prize: Best th… 1       \"It's not y… 18 Marc… Sarah Ken… 3         11\n 6 2     Do the most im… 1       \"It's not y… 18 Marc… Charlotte… 2         11\n 7 2     Do the most im… 1       \"It's not y… 18 Marc… Jamali Ma… 3         11\n 8 2     Do the most im… 1       \"It's not y… 18 Marc… Lee Mack   3         11\n 9 2     Do the most im… 1       \"It's not y… 18 Marc… Mike Wozn… 5         11\n10 2     Do the most im… 1       \"It's not y… 18 Marc… Sarah Ken… 4         11"
  },
  {
    "objectID": "07-reg-expr.html#scraping-and-wrangling-taskmaster-data",
    "href": "07-reg-expr.html#scraping-and-wrangling-taskmaster-data",
    "title": "7  Regular Expressions",
    "section": "\n7.7 Scraping and wrangling Taskmaster data",
    "text": "7.7 Scraping and wrangling Taskmaster data\n\nresults &lt;- read_html(\"https://taskmaster.fandom.com/wiki/Series_11\") |&gt;\n  html_element(\".tmtable\") |&gt; \n  html_table() |&gt;\n  mutate(episode = ifelse(startsWith(Task, \"Episode\"), Task, NA)) |&gt;\n  fill(episode, .direction = \"down\") |&gt;\n  filter(!startsWith(Task, \"Episode\"), \n         !(Task %in% c(\"Total\", \"Grand Total\"))) |&gt;\n  pivot_longer(cols = -c(Task, Description, episode),\n               names_to = \"contestant\",\n               values_to = \"score\") |&gt;\n  mutate(series = 11)"
  },
  {
    "objectID": "07-reg-expr.html#scraping-and-wrangling-taskmaster-data---results",
    "href": "07-reg-expr.html#scraping-and-wrangling-taskmaster-data---results",
    "title": "7  Regular Expressions",
    "section": "\n7.7 Scraping and wrangling Taskmaster data - results",
    "text": "7.7 Scraping and wrangling Taskmaster data - results\n\nresults |&gt; \n  select(Task, Description, episode, contestant, score, series) |&gt;\n  head(10)\n\n# A tibble: 10 × 6\n  Task  Description                              episode contestant score series\n  &lt;chr&gt; &lt;chr&gt;                                    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1 1     Prize: Best thing you can carry, but on… Episod… Charlotte… 1         11\n2 1     Prize: Best thing you can carry, but on… Episod… Jamali Ma… 2         11\n3 1     Prize: Best thing you can carry, but on… Episod… Lee Mack   4         11\n4 1     Prize: Best thing you can carry, but on… Episod… Mike Wozn… 5         11\n5 1     Prize: Best thing you can carry, but on… Episod… Sarah Ken… 3         11\n6 2     Do the most impressive thing under the … Episod… Charlotte… 2         11\n# ℹ 4 more rows"
  },
  {
    "objectID": "07-reg-expr.html#more-succinct-results",
    "href": "07-reg-expr.html#more-succinct-results",
    "title": "7  Regular Expressions",
    "section": "\n7.8 more succinct results",
    "text": "7.8 more succinct results\n\n   Task  Description         episode   contestant score series\n  1     Prize: Best thing…  Episode 1… Charlotte… 1         11\n  1     Prize: Best thing…  Episode 1… Jamali Ma… 2         11\n  1     Prize: Best thing…  Episode 1… Lee Mack   4         11\n  1     Prize: Best thing…  Episode 1… Mike Wozn… 5         11\n  1     Prize: Best thing…  Episode 1… Sarah Ken… 3         11\n  2     Do the most…        Episode 1… Charlotte… 2         11\n  2     Do the most…        Episode 1… Jamali Ma… 3[1]      11\n  2     Do the most…        Episode 1… Lee Mack   3         11\n  2     Do the most…        Episode 1… Mike Wozn… 5         11\n  2     Do the most…        Episode 1… Sarah Ken… 4         11\n\nCurrently, the episode column contains entries like\n\n\"Episode 1: It's not your fault. (18 March 2021)\""
  },
  {
    "objectID": "07-reg-expr.html#cleaning-the-score-column",
    "href": "07-reg-expr.html#cleaning-the-score-column",
    "title": "7  Regular Expressions",
    "section": "\n7.9 Cleaning the score column",
    "text": "7.9 Cleaning the score column\n\ntable(results$score)\n\n\n   –    ✔    ✘    0    1    2    3 3[1] 3[2]    4 4[2]    5   DQ \n   7    1    1   11   37   42   48    1    3   50    1   55   13 \n\n\nHow should the scores be stored? What is the cleaning task?\n\n\n\n\nFigure 7.3: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11"
  },
  {
    "objectID": "07-reg-expr.html#extracting-numeric-information",
    "href": "07-reg-expr.html#extracting-numeric-information",
    "title": "7  Regular Expressions",
    "section": "\n7.10 Extracting numeric information",
    "text": "7.10 Extracting numeric information\nSuppose we have the following string:\n\n\"3[1]\"\n\nAnd we want to extract just the number “3”:\n\nstr_extract(\"3[1]\", \"3\")\n\n[1] \"3\"\n\n\nWhat if we don’t know which number to extract?\n\nstr_extract(\"3[1]\", \"\\\\d\")\n\n[1] \"3\"\n\n\n\nstr_extract(\"4[1]\", \"\\\\d\")\n\n[1] \"4\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d\")\n\n[1] \"1\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d+\")\n\n[1] \"10\"\n\n\n\nstr_extract(\"DQ\", \"\\\\d\")\n\n[1] NA"
  },
  {
    "objectID": "07-reg-expr.html#extracting-numeric-information-1",
    "href": "07-reg-expr.html#extracting-numeric-information-1",
    "title": "7  Regular Expressions",
    "section": "\n7.12 Extracting numeric information",
    "text": "7.12 Extracting numeric information\nSuppose we have the following string:\n\n\"3[1]\"\n\nWhat if we don’t know which number to extract?\n\nstr_extract(\"3[1]\", \"\\\\d\")\n\n[1] \"3\"\n\n\n\nstr_extract(\"4[1]\", \"\\\\d\")\n\n[1] \"4\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d\")\n\n[1] \"1\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d+\")\n\n[1] \"10\"\n\n\n\nstr_extract(\"DQ\", \"\\\\d\")\n\n[1] NA"
  },
  {
    "objectID": "07-reg-expr.html#str_extract",
    "href": "07-reg-expr.html#str_extract",
    "title": "7  Regular Expressions",
    "section": "\n7.11 str_extract()\n",
    "text": "7.11 str_extract()\n\nstr_extract() is an R function in the stringr package which finds regular expressions in strings of text.\n\nstr_extract(\"My cat is 3 years old\", \"cat\")\n\n[1] \"cat\"\n\n\n\nstr_extract(\"My cat is 3 years old\", \"3\")\n\n[1] \"3\""
  },
  {
    "objectID": "07-reg-expr.html#matching-multiple-options",
    "href": "07-reg-expr.html#matching-multiple-options",
    "title": "7  Regular Expressions",
    "section": "\n7.12 Matching multiple options",
    "text": "7.12 Matching multiple options\nstr_extract() returns the first match; str_extract_all() allows more than one match.\n\nstr_extract(\"My cat is 3 years old\", \"cat|dog\")\n\n[1] \"cat\"\n\nstr_extract(\"My dog is 10 years old\", \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract(\"My dog is 10 years old, my cat is 3 years old\", \n            \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract_all(\"My dog is 10 years old, my cat is 3 years old\", \n                \"cat|dog\")\n\n[[1]]\n[1] \"dog\" \"cat\""
  },
  {
    "objectID": "07-reg-expr.html#matching-groups-of-characters",
    "href": "07-reg-expr.html#matching-groups-of-characters",
    "title": "7  Regular Expressions",
    "section": "\n7.13 Matching groups of characters",
    "text": "7.13 Matching groups of characters\nWhat if I want to extract a number?\n\nstr_extract(\"My cat is 3 years old\", \"\\\\d\")\n\n[1] \"3\"\n\n\nWhat will the result be for the following code?\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n[1] \"1\"\n\n\nThe + symbol in a regular expression means “repeated one or more times”\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d+\")\n\n[1] \"10\""
  },
  {
    "objectID": "07-reg-expr.html#matching-groups-of-characters-1",
    "href": "07-reg-expr.html#matching-groups-of-characters-1",
    "title": "7  Regular Expressions",
    "section": "\n7.14 Matching groups of characters",
    "text": "7.14 Matching groups of characters\nWhat if I want to extract a number?\n\nstr_extract(\"My cat is 3 years old\", \"\\\\d\")\n\n[1] \"3\"\n\n\nWhat will happen when I run the following code?\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n[1] \"1\""
  },
  {
    "objectID": "07-reg-expr.html#matching-groups-of-characters-2",
    "href": "07-reg-expr.html#matching-groups-of-characters-2",
    "title": "7  Regular Expressions",
    "section": "\n7.15 Matching groups of characters",
    "text": "7.15 Matching groups of characters\nThe + symbol in a regular expression means “repeated one or more times”\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d+\")\n\n[1] \"10\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-from-multiple-strings",
    "href": "07-reg-expr.html#extracting-from-multiple-strings",
    "title": "7  Regular Expressions",
    "section": "\n7.14 Extracting from multiple strings",
    "text": "7.14 Extracting from multiple strings\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"3\"  \"10\"\n\n\nWhat if we have multiple instances across multiple strings? We need to be careful working with lists (instead of vectors).\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\w+\")\n\n[1] \"My\" \"My\"\n\nstr_extract_all(strings, \"\\\\w+\")\n\n[[1]]\n[1] \"My\"    \"cat\"   \"is\"    \"3\"     \"years\" \"old\"  \n\n[[2]]\n[1] \"My\"    \"dog\"   \"is\"    \"10\"    \"years\" \"old\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information",
    "href": "07-reg-expr.html#extracting-episode-information",
    "title": "7  Regular Expressions",
    "section": "\n7.6 Extracting episode information",
    "text": "7.6 Extracting episode information\nCurrently, the episode column contains entries like:\n\n\"Episode 2: The pie whisperer. (4 August 2015)\"\n\nHow would I extract just the episode number?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\d+\")\n\n[1] \"2\"\n\n\nHow would I extract the episode name?\nGoal: find a pattern to match: anything that starts with a :, ends with a .\nLet’s break down that task into pieces.\nHow can we find the period at the end of the sentence? What does each of these lines of code return?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".\")\n\n[1] \"E\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".+\")\n\n[1] \"Episode 2: The pie whisperer. (4 August 2015)\"\n\n\nWe use an escape character when we actually want to choose a period:\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\.\")\n\n[1] \".\"\n\n\nRecall the goal: find a pattern to match: anything that starts with a :, ends with a .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \":.+\\\\.\")\n\n[1] \": The pie whisperer.\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information-1",
    "href": "07-reg-expr.html#extracting-episode-information-1",
    "title": "7  Regular Expressions",
    "section": "\n7.16 Extracting episode information",
    "text": "7.16 Extracting episode information\nCurrently, the episode column contains entries like:\n\n\"Episode 2: The pie whisperer. (4 August 2015)\"\n\nHow would I extract just the episode number?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\d+\")\n\n[1] \"2\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information-2",
    "href": "07-reg-expr.html#extracting-episode-information-2",
    "title": "7  Regular Expressions",
    "section": "\n7.17 Extracting episode information",
    "text": "7.17 Extracting episode information\nCurrently, the episode column contains entries like:\n\n\"Episode 2: The pie whisperer. (4 August 2015)\"\n\nHow would I extract the episode name?\nGoal: find a pattern to match: anything that starts with a :, ends with a .\nLet’s break down that task into pieces.\nHow can we find the period at the end of the sentence? What does each of these lines of code return?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".\")\n\n[1] \"E\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".+\")\n\n[1] \"Episode 2: The pie whisperer. (4 August 2015)\"\n\n\nWe use an escape character when we actually want to choose a period:\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\.\")\n\n[1] \".\"\n\n\nRecall the goal: find a pattern to match: anything that starts with a :, ends with a .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \":.+\\\\.\")\n\n[1] \": The pie whisperer.\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information-3",
    "href": "07-reg-expr.html#extracting-episode-information-3",
    "title": "7  Regular Expressions",
    "section": "\n7.20 Extracting episode information",
    "text": "7.20 Extracting episode information\nGetting everything between the : and the .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+(?=\\\\.)\")\n\n[1] \"The pie whisperer\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information---solution",
    "href": "07-reg-expr.html#extracting-episode-information---solution",
    "title": "7  Regular Expressions",
    "section": "\n7.21 Extracting episode information - solution",
    "text": "7.21 Extracting episode information - solution\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".\")\n\n[1] \"E\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".+\")\n\n[1] \"Episode 2: The pie whisperer. (4 August 2015)\"\n\n\nWe use an escape character when we actually want to choose a period:\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\.\")\n\n[1] \".\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information-4",
    "href": "07-reg-expr.html#extracting-episode-information-4",
    "title": "7  Regular Expressions",
    "section": "\n7.22 Extracting episode information",
    "text": "7.22 Extracting episode information\nGoal: find a pattern to match: anything that starts with a :, ends with a .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \":.+\\\\.\")\n\n[1] \": The pie whisperer.\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-episode-information-5",
    "href": "07-reg-expr.html#extracting-episode-information-5",
    "title": "7  Regular Expressions",
    "section": "\n7.23 Extracting episode information",
    "text": "7.23 Extracting episode information\nGetting everything between the : and the .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+(?=\\\\.)\")\n\n[1] \"The pie whisperer\""
  },
  {
    "objectID": "07-reg-expr.html#lookbehinds",
    "href": "07-reg-expr.html#lookbehinds",
    "title": "7  Regular Expressions",
    "section": "\n7.18 Lookbehinds",
    "text": "7.18 Lookbehinds\n(?&lt;=) is a positive lookbehind. It is used to identify expressions which are preceded by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+\")\n\n[1] \"The pie whisperer. (4 August 2015)\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\. ).+\")\n\n[1] \"(4 August 2015)\""
  },
  {
    "objectID": "07-reg-expr.html#lookaheads",
    "href": "07-reg-expr.html#lookaheads",
    "title": "7  Regular Expressions",
    "section": "\n7.19 Lookaheads",
    "text": "7.19 Lookaheads\n(?=) is a positive lookahead. It is used to identify expressions which are followed by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=\\\\.)\")\n\n[1] \"Episode 2: The pie whisperer\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=:)\")\n\n[1] \"Episode 2\""
  },
  {
    "objectID": "07-reg-expr.html#extracting-air-date",
    "href": "07-reg-expr.html#extracting-air-date",
    "title": "7  Regular Expressions",
    "section": "\n7.21 Extracting air date",
    "text": "7.21 Extracting air date\nI want to extract just the air date. What pattern do I want to match?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", ...)"
  },
  {
    "objectID": "07-reg-expr.html#extracting-air-date-1",
    "href": "07-reg-expr.html#extracting-air-date-1",
    "title": "7  Regular Expressions",
    "section": "\n7.22 Extracting air date",
    "text": "7.22 Extracting air date\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\().+(?=\\\\))\")\n\n[1] \"4 August 2015\""
  },
  {
    "objectID": "07-reg-expr.html#wrangling-the-episode-info",
    "href": "07-reg-expr.html#wrangling-the-episode-info",
    "title": "7  Regular Expressions",
    "section": "\n7.23 Wrangling the episode info",
    "text": "7.23 Wrangling the episode info\nCurrently:\n\n\n# A tibble: 270 × 1\n  episode                                        \n  &lt;chr&gt;                                          \n1 Episode 1: It's not your fault. (18 March 2021)\n2 Episode 1: It's not your fault. (18 March 2021)\n3 Episode 1: It's not your fault. (18 March 2021)\n4 Episode 1: It's not your fault. (18 March 2021)\n5 Episode 1: It's not your fault. (18 March 2021)\n6 Episode 1: It's not your fault. (18 March 2021)\n# ℹ 264 more rows"
  },
  {
    "objectID": "07-reg-expr.html#wrangling-the-episode-info-1",
    "href": "07-reg-expr.html#wrangling-the-episode-info-1",
    "title": "7  Regular Expressions",
    "section": "\n7.24 Wrangling the episode info",
    "text": "7.24 Wrangling the episode info\nOne option:\n\nresults |&gt;\n  select(episode) |&gt;\n  mutate(episode_name = str_extract(episode, \"(?&lt;=: ).+(?=\\\\.)\"),\n         air_date = str_extract(episode, \"(?&lt;=\\\\().+(?=\\\\))\"),\n         episode = str_extract(episode, \"\\\\d+\"))\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows"
  },
  {
    "objectID": "07-reg-expr.html#wrangling-the-episode-info-2",
    "href": "07-reg-expr.html#wrangling-the-episode-info-2",
    "title": "7  Regular Expressions",
    "section": "\n7.25 Wrangling the episode info",
    "text": "7.25 Wrangling the episode info\nAnother option:\n\nresults |&gt;\n  separate_wider_regex(episode, \n                       patterns = c(\".+ \", \n                                    episode = \"\\\\d+\", \n                                    \": \", \n                                    episode_name = \".+\", \n                                    \"\\\\. \\\\(\", \n                                    air_date = \".+\", \n                                    \"\\\\)\"))\n\n\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows"
  },
  {
    "objectID": "07-reg-expr.html#scraping-and-wrangling-taskmaster",
    "href": "07-reg-expr.html#scraping-and-wrangling-taskmaster",
    "title": "7  Regular Expressions",
    "section": "\n7.6 Scraping and wrangling Taskmaster",
    "text": "7.6 Scraping and wrangling Taskmaster\nGoal: to scrape the Taskmaster wiki into a dataframe including task, description, episode, episode name, air date, contestant, score, and series.4\n\nresults &lt;- read_html(\"https://taskmaster.fandom.com/wiki/Series_11\") |&gt;\n  html_element(\".tmtable\") |&gt; \n  html_table() |&gt;\n  mutate(episode = ifelse(startsWith(Task, \"Episode\"), Task, NA)) |&gt;\n  fill(episode, .direction = \"down\") |&gt;\n  filter(!startsWith(Task, \"Episode\"), \n         !(Task %in% c(\"Total\", \"Grand Total\"))) |&gt;\n  pivot_longer(cols = -c(Task, Description, episode),\n               names_to = \"contestant\",\n               values_to = \"score\") |&gt;\n  mutate(series = 11)"
  },
  {
    "objectID": "07-reg-expr.html#lookaround-again",
    "href": "07-reg-expr.html#lookaround-again",
    "title": "7  Regular Expressions",
    "section": "\n7.7 Lookaround (again)",
    "text": "7.7 Lookaround (again)\n\n7.7.1 Lookbehinds\n(?&lt;=) is a positive lookbehind. It is used to identify expressions which are preceded by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+\")\n\n[1] \"The pie whisperer. (4 August 2015)\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\. ).+\")\n\n[1] \"(4 August 2015)\"\n\n\n\n7.7.2 Lookaheads\n(?=) is a positive lookahead. It is used to identify expressions which are followed by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=\\\\.)\")\n\n[1] \"Episode 2: The pie whisperer\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=:)\")\n\n[1] \"Episode 2\"\n\n\nExtracting episode information\nGetting everything between the : and the .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+(?=\\\\.)\")\n\n[1] \"The pie whisperer\"\n\n\nExtracting air date\nI want to extract just the air date. What pattern do I want to match?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", ...)\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\().+(?=\\\\))\")\n\n[1] \"4 August 2015\"\n\n\nWrangling the episode info\nCurrently:\n\n\n# A tibble: 270 × 1\n  episode                                        \n  &lt;chr&gt;                                          \n1 Episode 1: It's not your fault. (18 March 2021)\n2 Episode 1: It's not your fault. (18 March 2021)\n3 Episode 1: It's not your fault. (18 March 2021)\n4 Episode 1: It's not your fault. (18 March 2021)\n5 Episode 1: It's not your fault. (18 March 2021)\n6 Episode 1: It's not your fault. (18 March 2021)\n# ℹ 264 more rows\n\n\nOne option:\n\nresults |&gt;\n  select(episode) |&gt;\n  mutate(episode_name = str_extract(episode, \"(?&lt;=: ).+(?=\\\\.)\"),\n         air_date = str_extract(episode, \"(?&lt;=\\\\().+(?=\\\\))\"),\n         episode = str_extract(episode, \"\\\\d+\"))\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows\n\n\nAnother option:\n\nresults |&gt;\n  separate_wider_regex(episode, \n                       patterns = c(\".+ \", \n                                    episode = \"\\\\d+\", \n                                    \": \", \n                                    episode_name = \".+\", \n                                    \"\\\\. \\\\(\", \n                                    air_date = \".+\", \n                                    \"\\\\)\"))\n\n\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows"
  },
  {
    "objectID": "07-regex.html#r-packages-to-make-your-life-easier",
    "href": "07-regex.html#r-packages-to-make-your-life-easier",
    "title": "7  Regular Expressions",
    "section": "\n7.1 R packages to make your life easier",
    "text": "7.1 R packages to make your life easier\n\n\nstringr package A core package in the tidyverse. It is installed via install.packages(\"tidyverse\") and also loaded via library(tidyverse). Of course, you can also install or load it individually.\n\nMany of the main functions start with str_. Auto-complete is your friend.\nReplacements for base functions re: string manipulation and regular expressions (see below).\nMain advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task.\nstringr cheat sheet\n\n\n\ntidyr package Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract().\nBase functions: nchar(), strsplit(), substr(), paste(), paste0().\nThe glue package is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package.\n\nString functions related to regular expression\nRegular expression is a pattern that describes a specific set of strings with a common structure. It is heavily used for string matching / replacing in many programming languages, although specific syntax may differ a bit. It is truly the heart and soul for string operations. In R, many string functions in base R as well as in stringr package use regular expressions, even RStudio’s search and replace allows regular expression:\n\nidentify match to a pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()\n\nextract match to a pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()\n\nlocate pattern within a string, i.e. give the start position of matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()\n\nreplace a pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()\n\nsplit a string using a pattern: strsplit(), stringr::str_split()\n\n\nRegular expressions typically specify characters (or character classes) to seek out, possibly with information about repeats and location within the string. This is accomplished with the help of metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. We will use some small examples to introduce regular expression syntax and what these metacharacters mean.\ngrep() stands for “global regular expression print”. grep() returns a character vector containing the selected elements, grepl() returns a logical vector of TRUE/FALSE for whether or not there was a match."
  },
  {
    "objectID": "07-regex.html#tools-for-characterizing-a-regular-expression",
    "href": "07-regex.html#tools-for-characterizing-a-regular-expression",
    "title": "7  Regular Expressions",
    "section": "\n7.2 Tools for characterizing a regular expression",
    "text": "7.2 Tools for characterizing a regular expression\n\n7.2.1 Escape sequences\nThere are some special characters in R that cannot be directly coded in a string. For example, let’s say you specify your pattern with single quotes and you want to find countries with the single quote '. You would have to “escape” the single quote in the pattern, by preceding it with \\, so it is clear that it is not part of the string-specifying machinery.\nThere are other characters in R that require escaping, and this rule applies to all string functions in R, including regular expressions. See here for a complete list of R escape sequences.\n\n\n\\': single quote. You don’t need to escape single quote inside a double-quoted string, so we can also use \" ' \".\n\n\n\\\": double quote. Similarly, double quotes can be used inside a single-quoted string, i.e. ' \" '.\n\n\n\\n: newline.\n\n\n\\r: carriage return.\n\n\n\\t: tab character.\n\n\nNote: cat() and print() handle escape sequences differently, if you want to print a string out with the interpretation of the sequences above, use cat().\n\n\nprint(\"a\\nb\")\n\n[1] \"a\\nb\"\n\ncat(\"a\\nb\")\n\na\nb\n\n\n\n7.2.2 Quantifiers\nQuantifiers specify how many repetitions of the pattern.\n\n\n*: matches at least 0 times.\n\n\n+: matches at least 1 times.\n\n\n?: matches at most 1 times.\n\n\n{n}: matches exactly n times.\n\n\n{n,}: matches at least n times.\n\n\n{n,m}: matches between n and m times.\n\n\nstrings &lt;- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\")\ngrep(\"ac*b\", strings, value = TRUE)\n\n[1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = FALSE)\n\n[1] 2 3 4 5 6\n\ngrep(\"ac+b\", strings, value = TRUE)\n\n[1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac?b\", strings, value = TRUE)\n\n[1] \"ab\"  \"acb\"\n\ngrep(\"ac{2}b\", strings, value = TRUE)\n\n[1] \"accb\"\n\ngrep(\"ac{2,}b\", strings, value = TRUE)\n\n[1] \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac{2,3}b\", strings, value = TRUE)\n\n[1] \"accb\"  \"acccb\"\n\n\n\n7.2.3 Position of pattern within the string\n\n\n^: matches the start of the string.\n\n\n$: matches the end of the string.\n\n\n\\b: matches the boundary of a word. Don’t confuse it with ^ $ which marks the edge of a string.\n\n\n\\B: matches the empty string provided it is not at an edge of a word.\n\n\nstrings &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\")\ngrep(\"ab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"abcd\"\n\ngrep(\"ab$\", strings, value = TRUE)\n\n[1] \"cdab\"\n\ngrep(\"\\\\bab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"c abd\"\n\n\n\n7.2.4 Operators\n\n\n.: matches any single character, as shown in the first example.\n\n[...]: a character list, matches any one of the characters inside the square brackets. We can also use - inside the brackets to specify a range of characters.\n\n\n[^...]: an inverted character list, similar to [...], but matches any characters except those inside the square brackets.\n\n\n\\: suppress the special meaning of metacharacters in regular expression, i.e. $ * + . ? [ ] ^ { } | ( ) \\, similar to its usage in escape sequences. Since \\ itself needs to be escaped in R, we need to escape these metacharacters with double backslash like \\\\$.\n\n\n|: an “or” operator, matches patterns on either side of the |.\n\n\n(...): grouping in regular expressions. This allows you to retrieve the bits that matched various parts of your regular expression so you can alter them or use them for building up a new string. Each group can than be refer using \\\\N, with N being the No. of (...) used. This is called backreference.\n\n\nstrings &lt;- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\", \"a|b\")\ngrep(\"ab.\", strings, value = TRUE)\n\n[1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab[c-e]\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\" \"abe\"\n\ngrep(\"ab[^c]\", strings, value = TRUE)\n\n[1] \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"\\\\^ab\", strings, value = TRUE)\n\n[1] \"^ab\"\n\ngrep(\"abc|abd\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\"\n\ngrep(\"a[b|c]\", strings, value = TRUE)\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\" \"a|b\"  \n\nstr_extract(strings, \"a[b|c]\")\n\n[1] \"ab\" \"ab\" \"ab\" \"ab\" \"ab\" \"ab\" \"a|\"\n\n\n\n7.2.5 Character classes\nCharacter classes allow specifying entire classes of characters, such as numbers, letters, etc. There are two flavors of character classes, one uses [: and :] around a predefined name inside square brackets and the other uses \\ and a special character. They are sometimes interchangeable.\n\n(?i) before the string indicates that the match should be case insensitive.\n\n[:digit:] or \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent to [0-9].\n\n\n\\D: non-digits, equivalent to [^0-9].\n\n\n[:lower:]: lower-case letters, equivalent to [a-z].\n\n\n[:upper:]: upper-case letters, equivalent to [A-Z].\n\n\n[:alpha:]: alphabetic characters, equivalent to [[:lower:][:upper:]] or [A-z].\n\n\n[:alnum:]: alphanumeric characters, equivalent to [[:alpha:][:digit:]] or [A-z0-9].\n\n\n\\w: word characters, equivalent to [[:alnum:]_] or [A-z0-9_] (letter, number, or underscore).\n\n\\W: not word, equivalent to [^A-z0-9_].\n\n\n[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f, equivalent to [0-9A-Fa-f].\n\n[:blank:]: blank characters, i.e. space and tab.\n\n\n[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\n\n\\s: space, . Matches any whitespace (space, tab, newline, and carriage return).\n\n\\S: not space.\n\n\n[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~.\n\n[:graph:]: graphical (human readable) characters: equivalent to [[:alnum:][:punct:]].\n\n[:print:]: printable characters, equivalent to [[:alnum:][:punct:]\\\\s].\n\n[:cntrl:]: control characters, like \\n or \\r, [\\x00-\\x1F\\x7F].\n\nNote:\n* [:...:] has to be used inside square brackets, e.g. [[:digit:]].\n* \\ itself is a special character that needs escape, e.g. \\\\d. Do not confuse these regular expressions with R escape sequences such as \\t."
  },
  {
    "objectID": "07-regex.html#examples-to-work-through",
    "href": "07-regex.html#examples-to-work-through",
    "title": "7  Regular Expressions",
    "section": "\n7.3 Examples to work through",
    "text": "7.3 Examples to work through\nI have found that the best way to truly understand regular expressions is to work through as many examples as possible (actually, maybe this is true about learning anything new!). For the following examples, try to figure out the solution on your own before looking at the footnote which contains the solution.\n\n7.3.1 Case insenstive\n\nMatch only the word meter in “The cemetery is 1 meter from the stop sign.” Also match Meter in “The cemetery is 1 Meter from the stop sign.”\n\n\nstring &lt;- c(\"The cemetery is 1 meter from the stop sign.\", \n            \"The cemetery is 1 Meter from the stop sign.\")\n\nstr_extract(string, \"(?i)\\\\bmeter\\\\b\")\n\n[1] \"meter\" \"Meter\"\n\n\n\n7.3.2 Proper times and dates\n\nMatch dates like 01/15/24 and also like 01.15.24 and like 01-15-24.1\n\n\n\nstring &lt;- c(\"01/15/24\", \"01.15.24\", \"01-15-24\", \"01 15 24\", \"011524\", \"January 15, 2024\")\n\nstr_extract(string, \"\\\\d\\\\d.\\\\d\\\\d.\\\\d\\\\d\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" \"01 15 24\" NA         NA        \n\nstr_extract(string, \"\\\\d\\\\d[/.\\\\-]\\\\d\\\\d[/.\\\\-]\\\\d\\\\d\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" NA         NA         NA        \n\nstr_extract(string, \"\\\\d{2}[/.\\\\-]\\\\d{2}[/.\\\\-]\\\\d{2}\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" NA         NA         NA        \n\n\n\nMatch a time of day such as “9:17 am” or “12:30 pm”. Require that the time be a valid time (not “99:99 pm”). Assume no leading zeros (i.e., “09:17 am”).2\n\n\n\nstring &lt;- c(\"9:17 am\", \"12:30 pm\", \"99:99 pm\", \"09:17 am\")\n\nstr_extract(string, \"(1[012]|[1-9]):[0-5][0-9] (am|pm)\")\n\n[1] \"9:17 am\"  \"12:30 pm\" NA         \"9:17 am\" \n\nstr_extract(string, \"^(1[012]|[1-9]):[0-5][0-9] (am|pm)$\")\n\n[1] \"9:17 am\"  \"12:30 pm\" NA         NA        \n\n\n\n7.3.3 Alternation operator\nThe “or” operator, | has the lowest precedence and parentheses have the highest precedence, which means that parentheses get evaluated before “or”.\n\nWhat is the difference between \\bMary|Jane|Sue\\b and \\b(Mary|Jane|Sue)\\b?3\n\n\n\nstring &lt;- c(\"Mary\", \"Mar\", \"Janet\", \"jane\", \"Susan\", \"Sue\")\n\nstr_extract(string, \"\\\\bMary|Jane|Sue\\\\b\")\n\n[1] \"Mary\" NA     \"Jane\" NA     NA     \"Sue\" \n\nstr_extract(string, \"\\\\b(Mary|Jane|Sue)\\\\b\")\n\n[1] \"Mary\" NA     NA     NA     NA     \"Sue\" \n\n\n\n7.3.4 An example from my work\nBelow are a handful of string characters that represent genomic sequences which were measured in an RNA Sequencing dataset. The task below is to find intergenic regions (IGR) and identify which coding sequences (CDS) bookend the intergenic regions. Note that IGRs do not code for proteins while CDSs do. Additionally, AS refers to anti-sense which identifies the genomic sequence in the opposite orientation (e.g., CGGATCC vs CCTAGGC). [The code below was written by Madison Hobbs, Scripps ’19.]\nThe names of the genomic pieces\n\nallCounts &lt;- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\n\nFirst, it is important to identify which are IGR, CDS, and anti-sense.\n\nallCounts &lt;- allCounts |&gt; tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts\n\n  feature\n1     CDS\n2     CDS\n3     IGR\n4  AS_IGR\n5     IGR\n                                                                                                                                                                                       rest\n1                                                                                                                                                                                     b2743\n2                                                                                                                                                                                     b2764\n3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                                                                                                                     GeneidBackup\n1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nWe keep only the IGR and AS_IGR strings, and we separate the two bookends. Note, the separation comes at the backslash.\n\nigr &lt;- allCounts |&gt; filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr &lt;- igr |&gt; tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)\n\n[1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nFor each of the two bookend Genes, we need to separate out the feature from the rest. Note that we write over feature1 in the second line of code below. Both of the bookends for all sequences are CDS elements.\n\nigr$feature1 &lt;- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 &lt;- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 &lt;- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)\n\n[1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2\n1      CDS      CDS\n2      CDS      CDS\n3      CDS      CDS\n\n\nAs CDS, it is now important to find the actual genenames for each of the IGR sequences. We also keep each element’s bnum which represents a unique gene identifier in E. coli.\nbnum, genename, rna.name act as place holders for the types of elements that we will need to identify the bookends of the IGRs.\n\nbnum = \"b[0-9]{4}\"\nbnum\n\n[1] \"b[0-9]{4}\"\n\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\n\n\nigr$start.gene &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr &lt;- igr |&gt; tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma) |&gt; \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma)\nnames(igr)\n\n [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"  \n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2 start.gene end.gene start.bnum end.bnum\n1      CDS      CDS        mlc     ynfL      b1594    b1595\n2      CDS      CDS       talB      mog      b0008    b0009\n3      CDS      CDS       yoaA     yoaB      b1808    b1809"
  },
  {
    "objectID": "07-regex.html#lookaround",
    "href": "07-regex.html#lookaround",
    "title": "7  Regular Expressions",
    "section": "\n7.4 Lookaround",
    "text": "7.4 Lookaround\nA lookaround specifies a place in the regular expression that will anchor the string you’d like to match. There are four types of lookarounds: positive lookahead, positive lookbehind, negative lookahead, and negative lookbehind.\n\n“x(?=y)” – positive lookahead (matches ‘x’ when it is followed by ‘y’)\n“x(?!y)” – negative lookahead (matches ‘x’ when it is not followed by ‘y’)\n“(?&lt;=y)x” – positive lookbehind (matches ‘x’ when it is preceded by ‘y’)\n“(?&lt;!y)x” – negative lookbehind (matches ‘x’ when it is not preceded by ‘y’)\n\nNote that the lookaround specifies a place in the string which means it does not return the details of the lookaround. Using lookarounds, you can test strings against patterns without including the lookaround pattern in the resulting match.\n\n\n\n\nFigure 7.1: Image credit: Stefan Judis https://www.stefanjudis.com/blog/a-regular-expression-lookahead-lookbehind-cheat-sheet/"
  },
  {
    "objectID": "07-regex.html#example---taskmaster",
    "href": "07-regex.html#example---taskmaster",
    "title": "7  Regular Expressions",
    "section": "\n7.5 Example - Taskmaster",
    "text": "7.5 Example - Taskmaster\nIn the following example, we will wrangle some data scraped from the wiki site for the TV series, Taskmaster. We won’t cover the html scraping here, but I include the code for completeness.\n\n\n\n\nFigure 7.2: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\n\n7.5.1 Scraping and wrangling Taskmaster\nGoal: to scrape the Taskmaster wiki into a dataframe including task, description, episode, episode name, air date, contestant, score, and series.4\n\nresults &lt;- read_html(\"https://taskmaster.fandom.com/wiki/Series_11\") |&gt;\n  html_element(\".tmtable\") |&gt; \n  html_table() |&gt;\n  mutate(episode = ifelse(startsWith(Task, \"Episode\"), Task, NA)) |&gt;\n  fill(episode, .direction = \"down\") |&gt;\n  filter(!startsWith(Task, \"Episode\"), \n         !(Task %in% c(\"Total\", \"Grand Total\"))) |&gt;\n  pivot_longer(cols = -c(Task, Description, episode),\n               names_to = \"contestant\",\n               values_to = \"score\") |&gt;\n  mutate(series = 11)\n\n\nresults |&gt; \n  select(Task, Description, episode, contestant, score, series) |&gt;\n  head(10)\n\n# A tibble: 10 × 6\n  Task  Description                              episode contestant score series\n  &lt;chr&gt; &lt;chr&gt;                                    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1 1     Prize: Best thing you can carry, but on… Episod… Charlotte… 1         11\n2 1     Prize: Best thing you can carry, but on… Episod… Jamali Ma… 2         11\n3 1     Prize: Best thing you can carry, but on… Episod… Lee Mack   4         11\n4 1     Prize: Best thing you can carry, but on… Episod… Mike Wozn… 5         11\n5 1     Prize: Best thing you can carry, but on… Episod… Sarah Ken… 3         11\n6 2     Do the most impressive thing under the … Episod… Charlotte… 2         11\n# ℹ 4 more rows\n\n\nmore succinct results\n\n   Task  Description         episode   contestant score series\n  1     Prize: Best thing…  Episode 1… Charlotte… 1         11\n  1     Prize: Best thing…  Episode 1… Jamali Ma… 2         11\n  1     Prize: Best thing…  Episode 1… Lee Mack   4         11\n  1     Prize: Best thing…  Episode 1… Mike Wozn… 5         11\n  1     Prize: Best thing…  Episode 1… Sarah Ken… 3         11\n  2     Do the most…        Episode 1… Charlotte… 2         11\n  2     Do the most…        Episode 1… Jamali Ma… 3[1]      11\n  2     Do the most…        Episode 1… Lee Mack   3         11\n  2     Do the most…        Episode 1… Mike Wozn… 5         11\n  2     Do the most…        Episode 1… Sarah Ken… 4         11\n\nCurrently, the episode column contains entries like\n\n\"Episode 1: It's not your fault. (18 March 2021)\"\n\n\n7.5.2 Cleaning the score column\n\ntable(results$score)\n\n\n   –    ✔    ✘    0    1    2    3 3[1] 3[2]    4 4[2]    5   DQ \n   7    1    1   11   37   42   48    1    3   50    1   55   13 \n\n\nHow should the scores be stored? What is the cleaning task?\n\n\n\n\nFigure 7.3: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\nExtracting numeric information\nSuppose we have the following string:\n\n\"3[1]\"\n\nAnd we want to extract just the number “3”:\n\nstr_extract(\"3[1]\", \"3\")\n\n[1] \"3\"\n\n\nWhat if we don’t know which number to extract?\n\nstr_extract(\"3[1]\", \"\\\\d\")\n\n[1] \"3\"\n\n\n\nstr_extract(\"4[1]\", \"\\\\d\")\n\n[1] \"4\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d\")\n\n[1] \"1\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d+\")\n\n[1] \"10\"\n\n\n\nstr_extract(\"DQ\", \"\\\\d\")\n\n[1] NA\n\n\nstr_extract()\nstr_extract() is an R function in the stringr package which finds regular expressions in strings of text.\n\nstr_extract(\"My cat is 3 years old\", \"cat\")\n\n[1] \"cat\"\n\n\n\nstr_extract(\"My cat is 3 years old\", \"3\")\n\n[1] \"3\"\n\n\nMatching multiple options\nstr_extract() returns the first match; str_extract_all() allows more than one match.\n\nstr_extract(\"My cat is 3 years old\", \"cat|dog\")\n\n[1] \"cat\"\n\nstr_extract(\"My dog is 10 years old\", \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract(\"My dog is 10 years old, my cat is 3 years old\", \n            \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract_all(\"My dog is 10 years old, my cat is 3 years old\", \n                \"cat|dog\")\n\n[[1]]\n[1] \"dog\" \"cat\"\n\n\nMatching groups of characters\nWhat if I want to extract a number?\n\nstr_extract(\"My cat is 3 years old\", \"\\\\d\")\n\n[1] \"3\"\n\n\nWhat will the result be for the following code?\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n[1] \"1\"\n\n\nThe + symbol in a regular expression means “repeated one or more times”\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d+\")\n\n[1] \"10\"\n\n\nExtracting from multiple strings\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"3\"  \"10\"\n\n\nWhat if we have multiple instances across multiple strings? We need to be careful working with lists (instead of vectors).\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\w+\")\n\n[1] \"My\" \"My\"\n\nstr_extract_all(strings, \"\\\\w+\")\n\n[[1]]\n[1] \"My\"    \"cat\"   \"is\"    \"3\"     \"years\" \"old\"  \n\n[[2]]\n[1] \"My\"    \"dog\"   \"is\"    \"10\"    \"years\" \"old\""
  },
  {
    "objectID": "07-regex.html#extracting-episode-information",
    "href": "07-regex.html#extracting-episode-information",
    "title": "7  Regular Expressions",
    "section": "\n7.6 Extracting episode information",
    "text": "7.6 Extracting episode information\nCurrently, the episode column contains entries like:\n\n\"Episode 2: The pie whisperer. (4 August 2015)\"\n\nHow would I extract just the episode number?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\d+\")\n\n[1] \"2\"\n\n\nHow would I extract the episode name?\nGoal: find a pattern to match: anything that starts with a :, ends with a .\nLet’s break down that task into pieces.\nHow can we find the period at the end of the sentence? What does each of these lines of code return?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".\")\n\n[1] \"E\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".+\")\n\n[1] \"Episode 2: The pie whisperer. (4 August 2015)\"\n\n\nWe use an escape character when we actually want to choose a period:\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\.\")\n\n[1] \".\"\n\n\nRecall the goal: find a pattern to match: anything that starts with a :, ends with a .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \":.+\\\\.\")\n\n[1] \": The pie whisperer.\""
  },
  {
    "objectID": "07-regex.html#lookaround-again",
    "href": "07-regex.html#lookaround-again",
    "title": "7  Regular Expressions",
    "section": "\n7.7 Lookaround (again)",
    "text": "7.7 Lookaround (again)\n\n7.7.1 Lookbehinds\n(?&lt;=) is a positive lookbehind. It is used to identify expressions which are preceded by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+\")\n\n[1] \"The pie whisperer. (4 August 2015)\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\. ).+\")\n\n[1] \"(4 August 2015)\"\n\n\n\n7.7.2 Lookaheads\n(?=) is a positive lookahead. It is used to identify expressions which are followed by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=\\\\.)\")\n\n[1] \"Episode 2: The pie whisperer\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=:)\")\n\n[1] \"Episode 2\"\n\n\nExtracting episode information\nGetting everything between the : and the .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+(?=\\\\.)\")\n\n[1] \"The pie whisperer\"\n\n\nExtracting air date\nI want to extract just the air date. What pattern do I want to match?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", ...)\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\().+(?=\\\\))\")\n\n[1] \"4 August 2015\"\n\n\nWrangling the episode info\nCurrently:\n\n\n# A tibble: 270 × 1\n  episode                                        \n  &lt;chr&gt;                                          \n1 Episode 1: It's not your fault. (18 March 2021)\n2 Episode 1: It's not your fault. (18 March 2021)\n3 Episode 1: It's not your fault. (18 March 2021)\n4 Episode 1: It's not your fault. (18 March 2021)\n5 Episode 1: It's not your fault. (18 March 2021)\n6 Episode 1: It's not your fault. (18 March 2021)\n# ℹ 264 more rows\n\n\nOne option:\n\nresults |&gt;\n  select(episode) |&gt;\n  mutate(episode_name = str_extract(episode, \"(?&lt;=: ).+(?=\\\\.)\"),\n         air_date = str_extract(episode, \"(?&lt;=\\\\().+(?=\\\\))\"),\n         episode = str_extract(episode, \"\\\\d+\"))\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows\n\n\nAnother option:\n\nresults |&gt;\n  separate_wider_regex(episode, \n                       patterns = c(\".+ \", \n                                    episode = \"\\\\d+\", \n                                    \": \", \n                                    episode_name = \".+\", \n                                    \"\\\\. \\\\(\", \n                                    air_date = \".+\", \n                                    \"\\\\)\"))\n\n\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows"
  },
  {
    "objectID": "07-regex.html#reflection-questions",
    "href": "07-regex.html#reflection-questions",
    "title": "7  Regular Expressions",
    "section": "\n7.9  Reflection questions",
    "text": "7.9  Reflection questions"
  },
  {
    "objectID": "07-regex.html#ethics-considerations",
    "href": "07-regex.html#ethics-considerations",
    "title": "7  Regular Expressions",
    "section": "\n7.10  Ethics considerations",
    "text": "7.10  Ethics considerations"
  },
  {
    "objectID": "07-regex.html#footnotes",
    "href": "07-regex.html#footnotes",
    "title": "7  Regular Expressions",
    "section": "",
    "text": "\\d\\d.\\d\\d.\\d\\d will work, but it will also match 123456. It is better to replace the dot with the characters of interest: \\d\\d[/.\\-]\\d\\d[/.\\-]\\d\\d. Remember that a dot inside a character class is just a dot. ↩︎\n^(1[012]|[1-9]):[0-5][0-9] (am|pm)$↩︎\nIn the former, the regex will search for \\bMary or Jane or Sue\\b. In the latter, the regex will search for \\bMary\\b or \\bJane\\b or \\bSue\\b. That is, Janet will match the former but not the latter.↩︎\nThanks to Ciaran Evans at Wake Forest University for example and code, https://sta279-f23.github.io/↩︎"
  },
  {
    "objectID": "p4-regex.html",
    "href": "p4-regex.html",
    "title": "Regular expressions",
    "section": "",
    "text": "Regular expressions are sequences of characters that define search patterns. Symbolic notation is used to find particular sequences of interest. Regular expressions are used in many different contexts including SQL queries, web scraping, and data wrangling. Chapter 8 covers regular expressions within R, but the syntax for pattern matching is identical across almost all platforms and programming languages. One thing to note is that to escape a metacharacter in R, two backslashes are needed. For example, \\\\d is the correct syntax to denote a digit."
  },
  {
    "objectID": "03-sql-verbs.html#select-distinct",
    "href": "03-sql-verbs.html#select-distinct",
    "title": "3  SQL clauses",
    "section": "\n3.3 SELECT DISTINCT",
    "text": "3.3 SELECT DISTINCT\nSELECT DISTINCT returns only unique rows. That is, it filters out all the duplicates of a variable or a combination of variables. Note that I have a larger limit on the query that I needed, just to make sure I got all the levels.\n\nSELECT DISTINCT payment_type\nFROM yellow_old\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.8: The distinct values of payment types. CRD is credit card; CSH is cash; NOC is no charge; DIS is dispute.\n\npayment_type\n\n\n\n\n\n\nCRD\n\n\nCSH\n\n\nNOC\n\n\nDIS\n\n\nUNK\n\n\n\n\n\n\n\n\n\nSELECT DISTINCT vendor_id, payment_type\nFROM yellow_old\nLIMIT 0, 20;\n\n\n\n\n\nTable 3.9: The distinct values of vendor ID and payment types, combined. VTS is Verifone Transportation Systems and CMT is Mobile Knowledge Systems Inc. CRD is credit card; CSH is cash; NOC is no charge; DIS is dispute.\n\nvendor_id\npayment_type\n\n\n\n\n\n\n\nCMT\nCRD\n\n\nCMT\nCSH\n\n\nCMT\nNOC\n\n\nCMT\nDIS\n\n\nVTS\nCRD\n\n\nVTS\nCSH\n\n\nVTS\nUNK"
  },
  {
    "objectID": "07-regex.html#regular-expressions-and-sql",
    "href": "07-regex.html#regular-expressions-and-sql",
    "title": "7  Regular Expressions",
    "section": "\n7.8 Regular expressions and SQL\n",
    "text": "7.8 Regular expressions and SQL\n\nBack to the IMDb database…\n\nSELECT production_year, title\n  FROM title\n  WHERE kind_id = 1 AND\n        title REGEXP '(?i)star'\n  LIMIT 0, 20;\n\n\n\n\nDisplaying records 1 - 10\n\n\n\nproduction_year\n\n\ntitle\n\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (I)\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (II)\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (III)\n\n\n\n\n2017\n\n\n\"Girl Starter\" (II)\n\n\n\n\n2001\n\n\n\"Popstars\" (I)\n\n\n\n\n2001\n\n\n\"Popstars\" (II)\n\n\n\n\n2002\n\n\n\"Popstars\" (I)\n\n\n\n\n2000\n\n\n\"Popstars\" (I)\n\n\n\n\n1959\n\n\n\"Startime\" (II)\n\n\n\n\n1959\n\n\n\"Startime\" (I)"
  },
  {
    "objectID": "06-change-db.html#altering-the-table",
    "href": "06-change-db.html#altering-the-table",
    "title": "6  Changing databases",
    "section": "\n6.4 Altering the table",
    "text": "6.4 Altering the table\nALTER TABLE changes the structure of a table. For example, you can add or delete columns, create or destroy indexes, change the type of existing columns, or rename columns or the table itself. (Syntax below is for MySQL. Unfortunately, DuckDB is finicky when ALTERing tables, so the commands below may not work on the tables created using DuckDB.)1\nMultiple ADD, ALTER, DROP, and CHANGE clauses are permitted in a single ALTER TABLE statement, separated by commas.\n\nALTER TABLE t1\nDROP COLUMN col1,\nDROP COLUMN col2;\n\nTo alter a column to change both its name and definition, use CHANGE, specifying the old and new names and the new definition. For example, to rename an INT NOT NULL column from a to b and change its definition to use the BIGINT data type while retaining the NOT NULL attribute, do this:\n\nALTER TABLE t1 CHANGE a b BIGINT NOT NULL;\n\nTo change a column definition but not its name, use CHANGE or MODIFY. With CHANGE, the syntax requires two column names, so you must specify the same name twice to leave the name unchanged. For example, to change the definition of column b:\n\nALTER TABLE t1 CHANGE b b INT NOT NULL;\n\nMODIFY is more convenient to change the definition without changing the name because it requires the column name only once:\n\nALTER TABLE t1 MODIFY b INT NOT NULL;\n\nTo change a column name but not its definition, use CHANGE or RENAME COLUMN. With CHANGE, the syntax requires a column definition, so to leave the definition unchanged, you must re-specify the definition the column currently has. For example, to rename an INT NOT NULL column from b to a:\n\nALTER TABLE t1 CHANGE b a INT NOT NULL;\n\nRENAME COLUMN is more convenient to change the name without changing the definition because it requires only the old and new names:\n\nALTER TABLE t1 RENAME COLUMN b TO a;\n\nIn general, you cannot rename a column to a name that already exists in the table. However, this is sometimes not the case, such as when you swap names or move them through a cycle. If a table has columns named a, b, and c, the following are valid operations:\n\n/* swap a and b */\nALTER TABLE t1 RENAME COLUMN a TO b,\n               RENAME COLUMN b TO a;\n\n/* \"rotate\" a, b, c through a cycle */\nALTER TABLE t1 RENAME COLUMN a TO b,\n               RENAME COLUMN b TO c,\n               RENAME COLUMN c TO a;"
  },
  {
    "objectID": "06-change-db.html#reflection-questions",
    "href": "06-change-db.html#reflection-questions",
    "title": "6  Changing databases",
    "section": "\n6.7  Reflection questions",
    "text": "6.7  Reflection questions\n\nHow can you update the value of a particular variable? What if you want to update a variable for many rows?\nWhy/when would you use CHANGE instead of RENAME COLUMN? Why/when would you use CHANGE instead of MODIFY?\nWhen are temporary tables useful? Can you always create temporary tables if you are working in SQL? Explain the hierarchy of tables, temporary tables, and subqueries."
  },
  {
    "objectID": "06-change-db.html#ethics-considerations",
    "href": "06-change-db.html#ethics-considerations",
    "title": "6  Changing databases",
    "section": "\n6.8  Ethics considerations",
    "text": "6.8  Ethics considerations\n\nWho should have the ability / access to insert, delete, or update tables? Should everyone who accesses a table also have the ability to edit the table? Why or why not?\nWhat can you do if you accidentally DELETE the wrong rows or DROP the wrong the columns?"
  },
  {
    "objectID": "06-change-db.html#footnotes",
    "href": "06-change-db.html#footnotes",
    "title": "6  Changing databases",
    "section": "",
    "text": "Information and examples in this section taken from https://dev.mysql.com/doc/refman/8.0/en/alter-table.html#alter-table-add-drop-column↩︎"
  },
  {
    "objectID": "05-creating-db.html#reflection-questions",
    "href": "05-creating-db.html#reflection-questions",
    "title": "5  Creating databases",
    "section": "\n5.3  Reflection questions",
    "text": "5.3  Reflection questions\n\nWhat is the difference between R and SQL in terms of communicating the different data types?\nWhy does it matter if the variable type is specified correctly? For example, why would it be better for a date column to be specified as DATETIME instead of VARCHAR?\nIf you are Googling some SQL syntax, why do you need to specify the dialect, for example, DuckDB or MySQL?\nWhy do we often include the DROP TABLE operation before the CREATE TABLE operation?"
  },
  {
    "objectID": "05-creating-db.html#ethics-considerations",
    "href": "05-creating-db.html#ethics-considerations",
    "title": "5  Creating databases",
    "section": "\n5.4  Ethics considerations",
    "text": "5.4  Ethics considerations\n\nWhen creating a database why should we worry about the provenance (origin) of the data?\nHow does a SQL database automatically hold extra information about the database (e.g., provenance)? Spoiler: it doesn’t. So what can we do?"
  },
  {
    "objectID": "08-regex.html#r-packages-to-make-your-life-easier",
    "href": "08-regex.html#r-packages-to-make-your-life-easier",
    "title": "8  Regular Expressions",
    "section": "\n8.1 R packages to make your life easier",
    "text": "8.1 R packages to make your life easier\n\n\nstringr package A core package in the tidyverse. It is installed via install.packages(\"tidyverse\") and also loaded via library(tidyverse). Of course, you can also install or load it individually.\n\nMany of the main functions start with str_. Auto-complete is your friend.\nReplacements for base functions re: string manipulation and regular expressions (see below).\nMain advantages over base functions: greater consistency about inputs and outputs. Outputs are more ready for your next analytical task.\nstringr cheat sheet\n\n\n\ntidyr package Especially useful for functions that split one character vector into many and vice versa: separate(), unite(), extract().\nBase functions: nchar(), strsplit(), substr(), paste(), paste0().\nThe glue package is fantastic for string interpolation. If stringr::str_interp() doesn’t get your job done, check out the glue package.\n\nString functions related to regular expression\nRegular expression is a pattern that describes a specific set of strings with a common structure. It is heavily used for string matching / replacing in many programming languages, although specific syntax may differ a bit. It is truly the heart and soul for string operations. In R, many string functions in base R as well as in stringr package use regular expressions, even RStudio’s search and replace allows regular expression:\n\nidentify match to a pattern: grep(..., value = FALSE), grepl(), stringr::str_detect()\n\nextract match to a pattern: grep(..., value = TRUE), stringr::str_extract(), stringr::str_extract_all()\n\nlocate pattern within a string, i.e. give the start position of matched patterns. regexpr(), gregexpr(), stringr::str_locate(), string::str_locate_all()\n\nreplace a pattern: sub(), gsub(), stringr::str_replace(), stringr::str_replace_all()\n\nsplit a string using a pattern: strsplit(), stringr::str_split()\n\n\nRegular expressions typically specify characters (or character classes) to seek out, possibly with information about repeats and location within the string. This is accomplished with the help of metacharacters that have specific meaning: $ * + . ? [ ] ^ { } | ( ) \\. We will use some small examples to introduce regular expression syntax and what these metacharacters mean.\ngrep() stands for “global regular expression print”. grep() returns a character vector containing the selected elements, grepl() returns a logical vector of TRUE/FALSE for whether or not there was a match."
  },
  {
    "objectID": "08-regex.html#tools-for-characterizing-a-regular-expression",
    "href": "08-regex.html#tools-for-characterizing-a-regular-expression",
    "title": "8  Regular Expressions",
    "section": "\n8.2 Tools for characterizing a regular expression",
    "text": "8.2 Tools for characterizing a regular expression\n\n8.2.1 Escape sequences\nThere are some special characters in R that cannot be directly coded in a string. For example, let’s say you specify your pattern with single quotes and you want to find countries with the single quote '. You would have to “escape” the single quote in the pattern, by preceding it with \\, so it is clear that it is not part of the string-specifying machinery.\nThere are other characters in R that require escaping, and this rule applies to all string functions in R, including regular expressions. See here for a complete list of R escape sequences.\n\n\n\\': single quote. You don’t need to escape single quote inside a double-quoted string, so we can also use \" ' \".\n\n\n\\\": double quote. Similarly, double quotes can be used inside a single-quoted string, i.e. ' \" '.\n\n\n\\n: newline.\n\n\n\\r: carriage return.\n\n\n\\t: tab character.\n\n\nNote: cat() and print() handle escape sequences differently, if you want to print a string out with the interpretation of the sequences above, use cat().\n\n\nprint(\"a\\nb\")\n\n[1] \"a\\nb\"\n\ncat(\"a\\nb\")\n\na\nb\n\n\n\n8.2.2 Quantifiers\nQuantifiers specify how many repetitions of the pattern.\n\n\n*: matches at least 0 times.\n\n\n+: matches at least 1 times.\n\n\n?: matches at most 1 times.\n\n\n{n}: matches exactly n times.\n\n\n{n,}: matches at least n times.\n\n\n{n,m}: matches between n and m times.\n\n\nstrings &lt;- c(\"a\", \"ab\", \"acb\", \"accb\", \"acccb\", \"accccb\")\ngrep(\"ac*b\", strings, value = TRUE)\n\n[1] \"ab\"     \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac*b\", strings, value = FALSE)\n\n[1] 2 3 4 5 6\n\ngrep(\"ac+b\", strings, value = TRUE)\n\n[1] \"acb\"    \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac?b\", strings, value = TRUE)\n\n[1] \"ab\"  \"acb\"\n\ngrep(\"ac{2}b\", strings, value = TRUE)\n\n[1] \"accb\"\n\ngrep(\"ac{2,}b\", strings, value = TRUE)\n\n[1] \"accb\"   \"acccb\"  \"accccb\"\n\ngrep(\"ac{2,3}b\", strings, value = TRUE)\n\n[1] \"accb\"  \"acccb\"\n\n\n\n8.2.3 Position of pattern within the string\n\n\n^: matches the start of the string.\n\n\n$: matches the end of the string.\n\n\n\\b: matches the boundary of a word. Don’t confuse it with ^ $ which marks the edge of a string.\n\n\n\\B: matches the empty string provided it is not at an edge of a word.\n\n\nstrings &lt;- c(\"abcd\", \"cdab\", \"cabd\", \"c abd\")\ngrep(\"ab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"cdab\"  \"cabd\"  \"c abd\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"abcd\"\n\ngrep(\"ab$\", strings, value = TRUE)\n\n[1] \"cdab\"\n\ngrep(\"\\\\bab\", strings, value = TRUE)\n\n[1] \"abcd\"  \"c abd\"\n\n\n\n8.2.4 Operators\n\n\n.: matches any single character, as shown in the first example.\n\n[...]: a character list, matches any one of the characters inside the square brackets. We can also use - inside the brackets to specify a range of characters.\n\n\n[^...]: an inverted character list, similar to [...], but matches any characters except those inside the square brackets.\n\n\n\\: suppress the special meaning of metacharacters in regular expression, i.e. $ * + . ? [ ] ^ { } | ( ) \\, similar to its usage in escape sequences. Since \\ itself needs to be escaped in R, we need to escape these metacharacters with double backslash like \\\\$.\n\n\n|: an “or” operator, matches patterns on either side of the |.\n\n\n(...): grouping in regular expressions. This allows you to retrieve the bits that matched various parts of your regular expression so you can alter them or use them for building up a new string. Each group can than be refer using \\\\N, with N being the No. of (...) used. This is called backreference.\n\n\nstrings &lt;- c(\"^ab\", \"ab\", \"abc\", \"abd\", \"abe\", \"ab 12\", \"a|b\")\ngrep(\"ab.\", strings, value = TRUE)\n\n[1] \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"ab[c-e]\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\" \"abe\"\n\ngrep(\"ab[^c]\", strings, value = TRUE)\n\n[1] \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"^ab\", strings, value = TRUE)\n\n[1] \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\"\n\ngrep(\"\\\\^ab\", strings, value = TRUE)\n\n[1] \"^ab\"\n\ngrep(\"abc|abd\", strings, value = TRUE)\n\n[1] \"abc\" \"abd\"\n\ngrep(\"a[b|c]\", strings, value = TRUE)\n\n[1] \"^ab\"   \"ab\"    \"abc\"   \"abd\"   \"abe\"   \"ab 12\" \"a|b\"  \n\nstr_extract(strings, \"a[b|c]\")\n\n[1] \"ab\" \"ab\" \"ab\" \"ab\" \"ab\" \"ab\" \"a|\"\n\n\n\n8.2.5 Character classes\nCharacter classes allow specifying entire classes of characters, such as numbers, letters, etc. There are two flavors of character classes, one uses [: and :] around a predefined name inside square brackets and the other uses \\ and a special character. They are sometimes interchangeable.\n\n(?i) before the string indicates that the match should be case insensitive.\n\n[:digit:] or \\d: digits, 0 1 2 3 4 5 6 7 8 9, equivalent to [0-9].\n\n\n\\D: non-digits, equivalent to [^0-9].\n\n\n[:lower:]: lower-case letters, equivalent to [a-z].\n\n\n[:upper:]: upper-case letters, equivalent to [A-Z].\n\n\n[:alpha:]: alphabetic characters, equivalent to [[:lower:][:upper:]] or [A-z].\n\n\n[:alnum:]: alphanumeric characters, equivalent to [[:alpha:][:digit:]] or [A-z0-9].\n\n\n\\w: word characters, equivalent to [[:alnum:]_] or [A-z0-9_] (letter, number, or underscore).\n\n\\W: not word, equivalent to [^A-z0-9_].\n\n\n[:xdigit:]: hexadecimal digits (base 16), 0 1 2 3 4 5 6 7 8 9 A B C D E F a b c d e f, equivalent to [0-9A-Fa-f].\n\n[:blank:]: blank characters, i.e. space and tab.\n\n\n[:space:]: space characters: tab, newline, vertical tab, form feed, carriage return, space.\n\n\\s: space, . Matches any whitespace (space, tab, newline, and carriage return).\n\n\\S: not space.\n\n\n[:punct:]: punctuation characters, ! ” # $ % & ’ ( ) * + , - . / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~.\n\n[:graph:]: graphical (human readable) characters: equivalent to [[:alnum:][:punct:]].\n\n[:print:]: printable characters, equivalent to [[:alnum:][:punct:]\\\\s].\n\n[:cntrl:]: control characters, like \\n or \\r, [\\x00-\\x1F\\x7F].\n\nNote:\n* [:...:] has to be used inside square brackets, e.g. [[:digit:]].\n* \\ itself is a special character that needs escape, e.g. \\\\d. Do not confuse these regular expressions with R escape sequences such as \\t."
  },
  {
    "objectID": "08-regex.html#examples-to-work-through",
    "href": "08-regex.html#examples-to-work-through",
    "title": "8  Regular Expressions",
    "section": "\n8.3 Examples to work through",
    "text": "8.3 Examples to work through\nI have found that the best way to truly understand regular expressions is to work through as many examples as possible (actually, maybe this is true about learning anything new!). For the following examples, try to figure out the solution on your own before looking at the footnote which contains the solution.\n\n8.3.1 Case insenstive\n\nMatch only the word meter in “The cemetery is 1 meter from the stop sign.” Also match Meter in “The cemetery is 1 Meter from the stop sign.”\n\n\nstring &lt;- c(\"The cemetery is 1 meter from the stop sign.\", \n            \"The cemetery is 1 Meter from the stop sign.\")\n\nstr_extract(string, \"(?i)\\\\bmeter\\\\b\")\n\n[1] \"meter\" \"Meter\"\n\n\n\n8.3.2 Proper times and dates\n\nMatch dates like 01/15/24 and also like 01.15.24 and like 01-15-24.1\n\n\n\nstring &lt;- c(\"01/15/24\", \"01.15.24\", \"01-15-24\", \"01 15 24\", \"011524\", \"January 15, 2024\")\n\nstr_extract(string, \"\\\\d\\\\d.\\\\d\\\\d.\\\\d\\\\d\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" \"01 15 24\" NA         NA        \n\nstr_extract(string, \"\\\\d\\\\d[/.\\\\-]\\\\d\\\\d[/.\\\\-]\\\\d\\\\d\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" NA         NA         NA        \n\nstr_extract(string, \"\\\\d{2}[/.\\\\-]\\\\d{2}[/.\\\\-]\\\\d{2}\")\n\n[1] \"01/15/24\" \"01.15.24\" \"01-15-24\" NA         NA         NA        \n\n\n\nMatch a time of day such as “9:17 am” or “12:30 pm”. Require that the time be a valid time (not “99:99 pm”). Assume no leading zeros (i.e., “09:17 am”).2\n\n\n\nstring &lt;- c(\"9:17 am\", \"12:30 pm\", \"99:99 pm\", \"09:17 am\")\n\nstr_extract(string, \"(1[012]|[1-9]):[0-5][0-9] (am|pm)\")\n\n[1] \"9:17 am\"  \"12:30 pm\" NA         \"9:17 am\" \n\nstr_extract(string, \"^(1[012]|[1-9]):[0-5][0-9] (am|pm)$\")\n\n[1] \"9:17 am\"  \"12:30 pm\" NA         NA        \n\n\n\n8.3.3 Alternation operator\nThe “or” operator, | has the lowest precedence and parentheses have the highest precedence, which means that parentheses get evaluated before “or”.\n\nWhat is the difference between \\bMary|Jane|Sue\\b and \\b(Mary|Jane|Sue)\\b?3\n\n\n\nstring &lt;- c(\"Mary\", \"Mar\", \"Janet\", \"jane\", \"Susan\", \"Sue\")\n\nstr_extract(string, \"\\\\bMary|Jane|Sue\\\\b\")\n\n[1] \"Mary\" NA     \"Jane\" NA     NA     \"Sue\" \n\nstr_extract(string, \"\\\\b(Mary|Jane|Sue)\\\\b\")\n\n[1] \"Mary\" NA     NA     NA     NA     \"Sue\" \n\n\n\n8.3.4 An example from my work\nBelow are a handful of string characters that represent genomic sequences which were measured in an RNA Sequencing dataset. The task below is to find intergenic regions (IGR) and identify which coding sequences (CDS) bookend the intergenic regions. Note that IGRs do not code for proteins while CDSs do. Additionally, AS refers to anti-sense which identifies the genomic sequence in the opposite orientation (e.g., CGGATCC vs CCTAGGC). [The code below was written by Madison Hobbs, Scripps ’19.]\nThe names of the genomic pieces\n\nallCounts &lt;- data.frame(Geneid = c(\"CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\",\n            \"CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\",\n            \"IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\",\n            \"AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\",\n            \"IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\"))\n\nallCounts$GeneidBackup = allCounts$Geneid\n\nFirst, it is important to identify which are IGR, CDS, and anti-sense.\n\nallCounts &lt;- allCounts |&gt; tidyr::separate(Geneid, c(\"feature\", \"rest\"), sep=\"[:]\")\nallCounts\n\n  feature\n1     CDS\n2     CDS\n3     IGR\n4  AS_IGR\n5     IGR\n                                                                                                                                                                                       rest\n1                                                                                                                                                                                     b2743\n2                                                                                                                                                                                     b2764\n3 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n4                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n5                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                                                                                                                     GeneidBackup\n1                                                                                                                CDS:b2743:pcm:L-isoaspartate_protein_carboxylmethyltransferase_type_II:cds2705:-:626:NC_000913.3\n2                                                                                                                      CDS:b2764:cysJ:sulfite_reductase2C_alpha_subunit2C_flavoprotein:cds2726:-:1799:NC_000913.3\n3 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n4                                                      AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n5                                       IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nWe keep only the IGR and AS_IGR strings, and we separate the two bookends. Note, the separation comes at the backslash.\n\nigr &lt;- allCounts |&gt; filter(feature %in% c(\"IGR\", \"AS_IGR\"))\nigr &lt;- igr |&gt; tidyr::separate(GeneidBackup, c(\"Geneid1\", \"Geneid2\"), sep = \"[/]\")\nnames(igr)\n\n[1] \"feature\" \"rest\"    \"Geneid1\" \"Geneid2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n\n\nFor each of the two bookend Genes, we need to separate out the feature from the rest. Note that we write over feature1 in the second line of code below. Both of the bookends for all sequences are CDS elements.\n\nigr$feature1 &lt;- tidyr::separate(igr, Geneid1, c(\"feature1\", \"rest\"), sep = \"[,]\")$feature1\nigr$feature1 &lt;- tidyr::separate(igr, feature1, c(\"rest\", \"feature1\"), sep = \"[()]\")$feature1\nigr$feature2 &lt;- tidyr::separate(igr, Geneid2, c(\"feature2\", \"rest\"), sep = \"[,]\")$feature2\nnames(igr)\n\n[1] \"feature\"  \"rest\"     \"Geneid1\"  \"Geneid2\"  \"feature1\" \"feature2\"\n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2\n1      CDS      CDS\n2      CDS      CDS\n3      CDS      CDS\n\n\nAs CDS, it is now important to find the actual genenames for each of the IGR sequences. We also keep each element’s bnum which represents a unique gene identifier in E. coli.\nbnum, genename, rna.name act as place holders for the types of elements that we will need to identify the bookends of the IGRs.\n\nbnum = \"b[0-9]{4}\"\nbnum\n\n[1] \"b[0-9]{4}\"\n\ngenename = \",[a-z]{3}[A-Z,].\"\nrna.name = \",rna[0-9]..\"\n\n\nigr$start.gene &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid1, rna.name))\nigr$end.gene &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, genename),\n  TRUE ~ stringr::str_extract(igr$Geneid2, rna.name))\nigr$start.bnum &lt;- dplyr::case_when(\n  igr$feature1 == \"CDS\" ~ stringr::str_extract(igr$Geneid1, bnum),\n  TRUE ~ \"none\")\nigr$end.bnum &lt;- dplyr::case_when(\n  igr$feature2 == \"CDS\" ~ stringr::str_extract(igr$Geneid2, bnum),\n  TRUE ~ \"none\")\nigr &lt;- igr |&gt; tidyr::separate(start.gene, into = c(\"comma\", \"start.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma) |&gt; \n  tidyr::separate(end.gene, into = c(\"comma\", \"end.gene\"), sep = \"[,]\") |&gt; \n  dplyr::select(-comma)\nnames(igr)\n\n [1] \"feature\"    \"rest\"       \"Geneid1\"    \"Geneid2\"    \"feature1\"  \n [6] \"feature2\"   \"start.gene\" \"end.gene\"   \"start.bnum\" \"end.bnum\"  \n\nigr\n\n  feature\n1     IGR\n2  AS_IGR\n3     IGR\n                                                                                                                                                                                       rest\n1 (CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220/CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893)\n2                                                         (CDS,b0008,talB,transaldolase_B,cds7,+,953/CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587)\n3                                       (CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910/CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344)\n                                                                                                           Geneid1\n1 IGR:(CDS,b1594,mlc,glucosamine_anaerobic_growth_regulon_transcriptional_repressor3B_autorepressor,cds1581,-,1220\n2                                                                AS_IGR:(CDS,b0008,talB,transaldolase_B,cds7,+,953\n3                                 IGR:(CDS,b1808,yoaA,putative_ATP-dependent_helicase2C_DinG_family,cds1798,-,1910\n                                                                                                   Geneid2\n1           CDS,b1595,ynfL,LysR_family_putative_transcriptional_regulator,cds1582,-,893):+:945:NC_000913.3\n2 CDS,b0009,mog,molybdochelatase_incorporating_molybdenum_into_molybdopterin,cds8,+,587):+:639:NC_000913.3\n3                 CDS,b1809,yoaB,putative_reactive_intermediate_deaminase,cds1799,+,344):+:396:NC_000913.3\n  feature1 feature2 start.gene end.gene start.bnum end.bnum\n1      CDS      CDS        mlc     ynfL      b1594    b1595\n2      CDS      CDS       talB      mog      b0008    b0009\n3      CDS      CDS       yoaA     yoaB      b1808    b1809"
  },
  {
    "objectID": "08-regex.html#lookaround",
    "href": "08-regex.html#lookaround",
    "title": "8  Regular Expressions",
    "section": "\n8.4 Lookaround",
    "text": "8.4 Lookaround\nA lookaround specifies a place in the regular expression that will anchor the string you’d like to match. There are four types of lookarounds: positive lookahead, positive lookbehind, negative lookahead, and negative lookbehind.\n\n“x(?=y)” – positive lookahead (matches ‘x’ when it is followed by ‘y’)\n“x(?!y)” – negative lookahead (matches ‘x’ when it is not followed by ‘y’)\n“(?&lt;=y)x” – positive lookbehind (matches ‘x’ when it is preceded by ‘y’)\n“(?&lt;!y)x” – negative lookbehind (matches ‘x’ when it is not preceded by ‘y’)\n\nNote that the lookaround specifies a place in the string which means it does not return the details of the lookaround. Using lookarounds, you can test strings against patterns without including the lookaround pattern in the resulting match.\n\n\n\n\nFigure 8.1: Image credit: Stefan Judis https://www.stefanjudis.com/blog/a-regular-expression-lookahead-lookbehind-cheat-sheet/"
  },
  {
    "objectID": "08-regex.html#example---taskmaster",
    "href": "08-regex.html#example---taskmaster",
    "title": "8  Regular Expressions",
    "section": "\n8.5 Example - Taskmaster",
    "text": "8.5 Example - Taskmaster\nIn the following example, we will wrangle some data scraped from the wiki site for the TV series, Taskmaster. We won’t cover the html scraping here, but I include the code for completeness.\n\n\n\n\nFigure 8.2: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\n\n8.5.1 Scraping and wrangling Taskmaster\nGoal: to scrape the Taskmaster wiki into a data frame including task, description, episode, episode name, air date, contestant, score, and series.4\n\nresults &lt;- read_html(\"https://taskmaster.fandom.com/wiki/Series_11\") |&gt;\n  html_element(\".tmtable\") |&gt; \n  html_table() |&gt;\n  mutate(episode = ifelse(startsWith(Task, \"Episode\"), Task, NA)) |&gt;\n  fill(episode, .direction = \"down\") |&gt;\n  filter(!startsWith(Task, \"Episode\"), \n         !(Task %in% c(\"Total\", \"Grand Total\"))) |&gt;\n  pivot_longer(cols = -c(Task, Description, episode),\n               names_to = \"contestant\",\n               values_to = \"score\") |&gt;\n  mutate(series = 11)\n\n\nresults |&gt; \n  select(Task, Description, episode, contestant, score, series) |&gt;\n  head(10)\n\n# A tibble: 10 × 6\n  Task  Description                              episode contestant score series\n  &lt;chr&gt; &lt;chr&gt;                                    &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  &lt;dbl&gt;\n1 1     Prize: Best thing you can carry, but on… Episod… Charlotte… 1         11\n2 1     Prize: Best thing you can carry, but on… Episod… Jamali Ma… 2         11\n3 1     Prize: Best thing you can carry, but on… Episod… Lee Mack   4         11\n4 1     Prize: Best thing you can carry, but on… Episod… Mike Wozn… 5         11\n5 1     Prize: Best thing you can carry, but on… Episod… Sarah Ken… 3         11\n6 2     Do the most impressive thing under the … Episod… Charlotte… 2         11\n# ℹ 4 more rows\n\n\nmore succinct results\n\n   Task  Description         episode   contestant score series\n  1     Prize: Best thing…  Episode 1… Charlotte… 1         11\n  1     Prize: Best thing…  Episode 1… Jamali Ma… 2         11\n  1     Prize: Best thing…  Episode 1… Lee Mack   4         11\n  1     Prize: Best thing…  Episode 1… Mike Wozn… 5         11\n  1     Prize: Best thing…  Episode 1… Sarah Ken… 3         11\n  2     Do the most…        Episode 1… Charlotte… 2         11\n  2     Do the most…        Episode 1… Jamali Ma… 3[1]      11\n  2     Do the most…        Episode 1… Lee Mack   3         11\n  2     Do the most…        Episode 1… Mike Wozn… 5         11\n  2     Do the most…        Episode 1… Sarah Ken… 4         11\n\nCurrently, the episode column contains entries like\n\n\"Episode 1: It's not your fault. (18 March 2021)\"\n\n\n8.5.2 Cleaning the score column\n\ntable(results$score)\n\n\n   –    ✔    ✘    0    1    2    3 3[1] 3[2]    4 4[2]    5   DQ \n   7    1    1   11   37   42   48    1    3   50    1   55   13 \n\n\nHow should the scores be stored? What is the cleaning task?\n\n\n\n\nFigure 8.3: Taskmaster Wiki https://taskmaster.fandom.com/wiki/Series_11\n\n\n\nExtracting numeric information\nSuppose we have the following string:\n\n\"3[1]\"\n\nAnd we want to extract just the number “3”:\n\nstr_extract(\"3[1]\", \"3\")\n\n[1] \"3\"\n\n\nWhat if we don’t know which number to extract?\n\nstr_extract(\"3[1]\", \"\\\\d\")\n\n[1] \"3\"\n\n\n\nstr_extract(\"4[1]\", \"\\\\d\")\n\n[1] \"4\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d\")\n\n[1] \"1\"\n\n\n\nstr_extract(\"10[1]\", \"\\\\d+\")\n\n[1] \"10\"\n\n\n\nstr_extract(\"DQ\", \"\\\\d\")\n\n[1] NA\n\n\nstr_extract()\nstr_extract() is an R function in the stringr package which finds regular expressions in strings of text.\n\nstr_extract(\"My cat is 3 years old\", \"cat\")\n\n[1] \"cat\"\n\n\n\nstr_extract(\"My cat is 3 years old\", \"3\")\n\n[1] \"3\"\n\n\nMatching multiple options\nstr_extract() returns the first match; str_extract_all() allows more than one match.\n\nstr_extract(\"My cat is 3 years old\", \"cat|dog\")\n\n[1] \"cat\"\n\nstr_extract(\"My dog is 10 years old\", \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract(\"My dog is 10 years old, my cat is 3 years old\", \n            \"cat|dog\")\n\n[1] \"dog\"\n\nstr_extract_all(\"My dog is 10 years old, my cat is 3 years old\", \n                \"cat|dog\")\n\n[[1]]\n[1] \"dog\" \"cat\"\n\n\nMatching groups of characters\nWhat if I want to extract a number?\n\nstr_extract(\"My cat is 3 years old\", \"\\\\d\")\n\n[1] \"3\"\n\n\nWhat will the result be for the following code?\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d\")\n\n[1] \"1\"\n\n\nThe + symbol in a regular expression means “repeated one or more times”\n\nstr_extract(\"My dog is 10 years old\", \"\\\\d+\")\n\n[1] \"10\"\n\n\nExtracting from multiple strings\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\d+\")\n\n[1] \"3\"  \"10\"\n\n\nWhat if we have multiple instances across multiple strings? We need to be careful working with lists (instead of vectors).\n\nstrings &lt;- c(\"My cat is 3 years old\", \"My dog is 10 years old\")\nstr_extract(strings, \"\\\\w+\")\n\n[1] \"My\" \"My\"\n\nstr_extract_all(strings, \"\\\\w+\")\n\n[[1]]\n[1] \"My\"    \"cat\"   \"is\"    \"3\"     \"years\" \"old\"  \n\n[[2]]\n[1] \"My\"    \"dog\"   \"is\"    \"10\"    \"years\" \"old\""
  },
  {
    "objectID": "08-regex.html#extracting-episode-information",
    "href": "08-regex.html#extracting-episode-information",
    "title": "8  Regular Expressions",
    "section": "\n8.6 Extracting episode information",
    "text": "8.6 Extracting episode information\nCurrently, the episode column contains entries like:\n\n\"Episode 2: The pie whisperer. (4 August 2015)\"\n\nHow would I extract just the episode number?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\d+\")\n\n[1] \"2\"\n\n\nHow would I extract the episode name?\nGoal: find a pattern to match: anything that starts with a :, ends with a .\nLet’s break down that task into pieces.\nHow can we find the period at the end of the sentence? What does each of these lines of code return?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".\")\n\n[1] \"E\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \".+\")\n\n[1] \"Episode 2: The pie whisperer. (4 August 2015)\"\n\n\nWe use an escape character when we actually want to choose a period:\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \"\\\\.\")\n\n[1] \".\"\n\n\nRecall the goal: find a pattern to match: anything that starts with a :, ends with a .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \":.+\\\\.\")\n\n[1] \": The pie whisperer.\""
  },
  {
    "objectID": "08-regex.html#lookaround-again",
    "href": "08-regex.html#lookaround-again",
    "title": "8  Regular Expressions",
    "section": "\n8.7 Lookaround (again)",
    "text": "8.7 Lookaround (again)\n\n8.7.1 Lookbehinds\n(?&lt;=) is a positive lookbehind. It is used to identify expressions which are preceded by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+\")\n\n[1] \"The pie whisperer. (4 August 2015)\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\. ).+\")\n\n[1] \"(4 August 2015)\"\n\n\n\n8.7.2 Lookaheads\n(?=) is a positive lookahead. It is used to identify expressions which are followed by a particular expression.\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=\\\\.)\")\n\n[1] \"Episode 2: The pie whisperer\"\n\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \".+(?=:)\")\n\n[1] \"Episode 2\"\n\n\nExtracting episode information\nGetting everything between the : and the .\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=: ).+(?=\\\\.)\")\n\n[1] \"The pie whisperer\"\n\n\nExtracting air date\nI want to extract just the air date. What pattern do I want to match?\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", ...)\n\n\nstr_extract(\"Episode 2: The pie whisperer. (4 August 2015)\", \n            \"(?&lt;=\\\\().+(?=\\\\))\")\n\n[1] \"4 August 2015\"\n\n\nWrangling the episode info\nCurrently:\n\n\n# A tibble: 270 × 1\n  episode                                        \n  &lt;chr&gt;                                          \n1 Episode 1: It's not your fault. (18 March 2021)\n2 Episode 1: It's not your fault. (18 March 2021)\n3 Episode 1: It's not your fault. (18 March 2021)\n4 Episode 1: It's not your fault. (18 March 2021)\n5 Episode 1: It's not your fault. (18 March 2021)\n6 Episode 1: It's not your fault. (18 March 2021)\n# ℹ 264 more rows\n\n\nOne option:\n\nresults |&gt;\n  select(episode) |&gt;\n  mutate(episode_name = str_extract(episode, \"(?&lt;=: ).+(?=\\\\.)\"),\n         air_date = str_extract(episode, \"(?&lt;=\\\\().+(?=\\\\))\"),\n         episode = str_extract(episode, \"\\\\d+\"))\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows\n\n\nAnother option:\n\nresults |&gt;\n  separate_wider_regex(episode, \n                       patterns = c(\".+ \", \n                                    episode = \"\\\\d+\", \n                                    \": \", \n                                    episode_name = \".+\", \n                                    \"\\\\. \\\\(\", \n                                    air_date = \".+\", \n                                    \"\\\\)\"))\n\n\n\n# A tibble: 270 × 3\n  episode episode_name        air_date     \n  &lt;chr&gt;   &lt;chr&gt;               &lt;chr&gt;        \n1 1       It's not your fault 18 March 2021\n2 1       It's not your fault 18 March 2021\n3 1       It's not your fault 18 March 2021\n4 1       It's not your fault 18 March 2021\n5 1       It's not your fault 18 March 2021\n6 1       It's not your fault 18 March 2021\n# ℹ 264 more rows"
  },
  {
    "objectID": "08-regex.html#regular-expressions-and-sql",
    "href": "08-regex.html#regular-expressions-and-sql",
    "title": "8  Regular Expressions",
    "section": "\n8.8 Regular expressions and SQL\n",
    "text": "8.8 Regular expressions and SQL\n\nBack to the IMDb database…\n\nSELECT production_year, title\n  FROM title\n  WHERE kind_id = 1 AND\n        title REGEXP '(?i)star'\n  LIMIT 0, 20;\n\n\n\n\nDisplaying records 1 - 10\n\n\n\nproduction_year\n\n\ntitle\n\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (I)\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (II)\n\n\n\n\n2005\n\n\n\"Dancing with the Stars\" (III)\n\n\n\n\n2017\n\n\n\"Girl Starter\" (II)\n\n\n\n\n2001\n\n\n\"Popstars\" (I)\n\n\n\n\n2001\n\n\n\"Popstars\" (II)\n\n\n\n\n2002\n\n\n\"Popstars\" (I)\n\n\n\n\n2000\n\n\n\"Popstars\" (I)\n\n\n\n\n1959\n\n\n\"Startime\" (II)\n\n\n\n\n1959\n\n\n\"Startime\" (I)"
  },
  {
    "objectID": "08-regex.html#reflection-questions",
    "href": "08-regex.html#reflection-questions",
    "title": "8  Regular Expressions",
    "section": "\n8.9  Reflection questions",
    "text": "8.9  Reflection questions\n\nWhat is the difference between [a|b] and (a|b)?\nWhat is the main character which needs to be escaped inside [...]? Why?\nWhy do we use lookarounds instead of just putting the location of interest inside our regular expression pattern?\nWhat are the differences between *, +, and ? ? How do the three metacharacters apply to a single character or a group of characters?\nHow can you find a string between two patterns without picking up the bookending patterns themselves?\nWhy does it make sense to write the regular expression pattern that matches the entire string of interest and not just a pattern which evaluates to TRUE?"
  },
  {
    "objectID": "08-regex.html#ethics-considerations",
    "href": "08-regex.html#ethics-considerations",
    "title": "8  Regular Expressions",
    "section": "\n8.10  Ethics considerations",
    "text": "8.10  Ethics considerations\n\nHow can we use regular expressions to check for name mispellings or other similar data cleaning tasks?\nName one thing that you noticed in the course materials (either in class or reading the notes, etc.) where you thought to yourself “Oh, I’ll have to be really careful about that.” Why would you need to be careful?"
  },
  {
    "objectID": "08-regex.html#footnotes",
    "href": "08-regex.html#footnotes",
    "title": "8  Regular Expressions",
    "section": "",
    "text": "\\d\\d.\\d\\d.\\d\\d will work, but it will also match 123456. It is better to replace the dot with the characters of interest: \\d\\d[/.\\-]\\d\\d[/.\\-]\\d\\d. Remember that a dot inside a character class is just a dot. ↩︎\n^(1[012]|[1-9]):[0-5][0-9] (am|pm)$↩︎\nIn the former, the regex will search for \\bMary or Jane or Sue\\b. In the latter, the regex will search for \\bMary\\b or \\bJane\\b or \\bSue\\b. That is, Janet will match the former but not the latter.↩︎\nThanks to Ciaran Evans at Wake Forest University for example and code, https://sta279-f23.github.io/↩︎"
  },
  {
    "objectID": "07-db-etc.html#existing-snl-data",
    "href": "07-db-etc.html#existing-snl-data",
    "title": "7  Other fun database snippets",
    "section": "\n7.1 Existing SNL data",
    "text": "7.1 Existing SNL data\nBuilding on the examples in Chapter 5 and Chapter 6, consider the Saturday Night Live datasets available on the snldb GitHub repo. Data is scraped from http://www.snlarchives.net and http://www.imdb.com/title/tt0072562 by Hendrik Hilleckes and Colin Morris. Notice that there are eleven .csv files available in the output folder.\n\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\n\n\nUSE SNL;\n\n\nDROP TABLE IF EXISTS casts;\n\n\nCREATE TABLE casts (\n  aid VARCHAR(255) NOT NULL DEFAULT '',\n  sid INTEGER NOT NULL DEFAULT 0,\n  featured BOOLEAN NOT NULL DEFAULT 'false',\n  first_epid INTEGER DEFAULT 0,\n  last_epid INTEGER DEFAULT 0,\n  update_anchor BOOLEAN NOT NULL DEFAULT 0,\n  n_episodes INTEGER NOT NULL DEFAULT 0,\n  season_fraction DECIMAL(21,20) NOT NULL DEFAULT 0,\n  PRIMARY KEY (sid, aid)\n);\n\n\nCOPY casts FROM 'data/casts.csv' HEADER;\n\n\nSELECT MAX(sid), MIN(sid) FROM casts LIMIT 10;\n\n\n\n\n1 records\n\n\n\nmax(sid)\n\n\nmin(sid)\n\n\n\n\n46\n\n\n1\n\n\n\n\n\n\nSELECT * FROM casts LIMIT 10;\n\n\n\n\nDisplaying records 1 - 10\n\n\n\naid\n\n\nsid\n\n\nfeatured\n\n\nfirst_epid\n\n\nlast_epid\n\n\nupdate_anchor\n\n\nn_episodes\n\n\nseason_fraction\n\n\n\n\n\nA. Whitney Brown\n\n\n11\n\n\nTRUE\n\n\n19860222\n\n\n\n\nFALSE\n\n\n8\n\n\n0.444\n\n\n\n\nA. Whitney Brown\n\n\n12\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n20\n\n\n1.000\n\n\n\n\nA. Whitney Brown\n\n\n13\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n13\n\n\n1.000\n\n\n\n\nA. Whitney Brown\n\n\n14\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n20\n\n\n1.000\n\n\n\n\nA. Whitney Brown\n\n\n15\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n20\n\n\n1.000\n\n\n\n\nA. Whitney Brown\n\n\n16\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n20\n\n\n1.000\n\n\n\n\nAlan Zweibel\n\n\n5\n\n\nTRUE\n\n\n19800409\n\n\n\n\nFALSE\n\n\n5\n\n\n0.250\n\n\n\n\nSasheer Zamata\n\n\n39\n\n\nTRUE\n\n\n20140118\n\n\n\n\nFALSE\n\n\n11\n\n\n0.524\n\n\n\n\nSasheer Zamata\n\n\n40\n\n\nTRUE\n\n\n\n\n\n\nFALSE\n\n\n21\n\n\n1.000\n\n\n\n\nSasheer Zamata\n\n\n41\n\n\nFALSE\n\n\n\n\n\n\nFALSE\n\n\n21\n\n\n1.000"
  },
  {
    "objectID": "07-db-etc.html#scrape-to-get-more-snl-data",
    "href": "07-db-etc.html#scrape-to-get-more-snl-data",
    "title": "7  Other fun database snippets",
    "section": "\n7.2 Scrape to get more SNL data",
    "text": "7.2 Scrape to get more SNL data\nwhat can i scrape? which variables/tables are easy to update?"
  },
  {
    "objectID": "07-db-etc.html#add-the-scraped-data-to-the-snl-database",
    "href": "07-db-etc.html#add-the-scraped-data-to-the-snl-database",
    "title": "7  Other fun database snippets",
    "section": "\n7.3 Add the scraped data to the SNL database",
    "text": "7.3 Add the scraped data to the SNL database"
  },
  {
    "objectID": "07-db-etc.html#best-practice",
    "href": "07-db-etc.html#best-practice",
    "title": "7  SQL extras",
    "section": "\n7.3 Best practice",
    "text": "7.3 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)"
  },
  {
    "objectID": "07-db-etc.html#reflection-questions",
    "href": "07-db-etc.html#reflection-questions",
    "title": "7  SQL extras",
    "section": "\n7.4  Reflection questions",
    "text": "7.4  Reflection questions\n\nWhat is the main purpose of a KEY? Can a KEY ever be NULL? (Hint: does it depend on what type of KEY it is?)\nWhat is the main purpose of an INDEX? Can an INDEX ever be NULL?\nHow should we approach indexing? That is, what variables are good candidates for choosing to be the index?\nWhy is it harder to calculate the median than the mean?\nWhy are there three functions in R (ifelse(), case_when(), and cut()) to do the job of a single function in SQL (CASE WHEN)?\nWhy won’t dbplyr provide the syntax for the SQL translation of ggplot()?"
  },
  {
    "objectID": "07-db-etc.html#ethics-considerations",
    "href": "07-db-etc.html#ethics-considerations",
    "title": "7  SQL extras",
    "section": "\n7.5  Ethics considerations",
    "text": "7.5  Ethics considerations\n\nWhat types of tasks is R best suited for? What types of tasks is SQL best suited for? Does it matter which one you use if they can both accomplish the same goal?\nWhen setting up a database in SQL why is it important to consider potential users of the database and their potential questions / queries? That is, how is the set-up of the database related to the use of the database?"
  },
  {
    "objectID": "07-db-etc.html#efficiencies",
    "href": "07-db-etc.html#efficiencies",
    "title": "7  SQL extras",
    "section": "\n7.1 Efficiencies",
    "text": "7.1 Efficiencies\nIt is worth pointing out a few aspects to loading data into SQL: keys, indexes, and partitioning.\nBefore we get to the definitions, consider this analogy:\n\nEach library (database) has books (tables). Each book (table) has pages (rows). Each page (row) has a unique page number to identify it (key value); to find a particular page, you sort through the page numbers (key values). But it isn’t immediately obvious where the particular page of interest is, you might have to page through the book a little bit to find the page of interest. It would be easier if you had several bookmarks throughout the book to anchor some of the page numbers. For example, if you want page 1047 and you have a bookmark on page 1050, you only have to turn back three pages. The bookmark is an index, it helps you find the desired rows much more quickly.1\n\n\n7.1.1 Key\nKeys are unique identifiers for each row, used primarily for connecting tables. Keys are generally not helpful for efficiency, but they are important for data integrity and relationships between tables. A key is a pointer that identifies a record. In practice, a key is one or more columns that are earmarked to uniquely identify a record in a table. Keys serve two main purposes:\n\nThey provide constraints on the column such as that it can’t store duplicate or null values.\nThey are also used to generate relationships among different tables.\n\n\n\nPRIMARY KEY is a column or set of columns that uniquely identify each row. Primary keys cannot be NULL. Each table must always have one (and only one) PK. The PK can be made up of one column, but if that isn’t enough to uniquely identify the row, more columns may be added. Sometimes it is easier to designate a numeric column (e.g., row number) to be the PK.\n\nFOREIGN KEY is a column or set of columns that reference a primary key in a different table. The FK links two tables together, and the link is called a relationship.\n\n(There are other keys such as: Super Key, Minimal Super Key, Candidate Key, Unique Key, Alternate Key, Composite Key, Natural Key, Surrogate Key.)\n\n7.1.2 Index\nIndexes are the crux of why SQL is so much more efficient than, say, R. An index is a lookup table that helps SQL keep track of which records contain certain values. By indexing the rows, SQL is able to optimize sorting and joining tables. The index is created in advance (when the table is created) and saved to disk, which can take up substantial space on the disk. Sometimes more than one variable is used to index the table. There are trade-offs to having a lot of indexes (disk space but fast wrangling) versus a few indexes (slow wrangling but less space).\nA table may have more than one index but you shouldn’t add indexes to every column in a table, as these have to be updated for every addition/update/delete to the column. Rather, indexes should be added to columns that are frequently included in queries.\nIndexes may not make much difference for small databases, but, as tables grow in size, queries benefit more from indexes.\nIn MySQL the commands SHOW KEYS and SHOW INDEXES provide information about the keys and indexes for each table (the two operations are synonymous in MySQL). Neither operation is available in DuckDB.\nNotice that the planes table has a single PRIMARY key. That primary key is used to index the table. The flights table has no PRIMARY key, but it does have six different indexes: Year, Date, Origin, Dest, Carrier, and tailNum.\n\nSHOW INDEXES FROM planes;\n\n\n\n\n1 records\n\n\n\nTable\n\n\nNon_unique\n\n\nKey_name\n\n\nSeq_in_index\n\n\nColumn_name\n\n\nCollation\n\n\nCardinality\n\n\nSub_part\n\n\nPacked\n\n\nNull\n\n\nIndex_type\n\n\nComment\n\n\nIndex_comment\n\n\n\n\nplanes\n\n\n0\n\n\nPRIMARY\n\n\n1\n\n\ntailnum\n\n\nA\n\n\n3322\n\n\n\n\n\n\n\n\nBTREE\n\n\n\n\n\n\n\n\n\n\nSHOW INDEXES FROM flights;\n\n\n\n\n8 records\n\n\n\nTable\n\n\nNon_unique\n\n\nKey_name\n\n\nSeq_in_index\n\n\nColumn_name\n\n\nCollation\n\n\nCardinality\n\n\nSub_part\n\n\nPacked\n\n\nNull\n\n\nIndex_type\n\n\nComment\n\n\nIndex_comment\n\n\n\n\n\nflights\n\n\n1\n\n\nYear\n\n\n1\n\n\nyear\n\n\nA\n\n\n7\n\n\n\n\n\n\nYES\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nDate\n\n\n1\n\n\nyear\n\n\nA\n\n\n7\n\n\n\n\n\n\nYES\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nDate\n\n\n2\n\n\nmonth\n\n\nA\n\n\n89\n\n\n\n\n\n\nYES\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nDate\n\n\n3\n\n\nday\n\n\nA\n\n\n2712\n\n\n\n\n\n\nYES\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nOrigin\n\n\n1\n\n\norigin\n\n\nA\n\n\n2267\n\n\n\n\n\n\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nDest\n\n\n1\n\n\ndest\n\n\nA\n\n\n2267\n\n\n\n\n\n\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\nCarrier\n\n\n1\n\n\ncarrier\n\n\nA\n\n\n134\n\n\n\n\n\n\n\n\nBTREE\n\n\n\n\n\n\n\n\nflights\n\n\n1\n\n\ntailNum\n\n\n1\n\n\ntailnum\n\n\nA\n\n\n37862\n\n\n\n\n\n\nYES\n\n\nBTREE\n\n\n\n\n\n\n\n\n\n\nThe values output by SHOW INDEXES are:2\n\nTable: The name of the table.\nNon_unique: 0 if the index cannot contain duplicates, 1 if it can.\nKey_name: The name of the index. If the index is the primary key, the name is always PRIMARY.\nSeq_in_index: The column sequence number in the index, starting with 1.\nColumn_name: The column name. See also the description for the Expression column.\nCollation: How the column is sorted in the index. This can have values A (ascending), D (descending), or NULL (not sorted).\nCardinality: An estimate of the number of unique values in the index.\nCardinality is counted based on statistics stored as integers, so the value is not necessarily exact even for small tables. The higher the cardinality, the greater the chance that MySQL uses the index when doing joins. Indexing with a high cardinality variable will be particularly useful for increased efficiency (if that variable is being queried).\nSub_part: The index prefix. That is, the number of indexed characters if the column is only partly indexed, NULL if the entire column is indexed.\nPacked: Indicates how the key is packed. NULL if it is not.\nNull: Contains YES if the column may contain NULL values and ’’ if not.\nIndex_type: The index method used (BTREE, FULLTEXT, HASH, RTREE).\nComment: Information about the index not described in its own column, such as disabled if the index is disabled.\nIndex_comment: Any comment provided for the index with a COMMENT attribute when the index was created.\n\n7.1.3 Partitioning\nAnother way to speed up query retrievals is to partition the data tables. If, for example, the SNL queries were always done by year, then the episodes table could be partitioned such that they are stored as separate tables (one per year). The partitioning functions as an index on year. The user would not be able to tell the difference between the unpartitioned episodes table and the partitioned one. However, queries done by year would be faster. Queries done grouped in another way would be slower.\n\n7.1.4 Querying\nIndexes are built to accommodate the specific queries that are most likely to be run. However, you might not know which queries are going to be run, so it isn’t always obviously how to index a table.\nFor the flights table, it seems likely that many queries will involve searching for flights from a particular origin, or to a particular destination, or during a particular year (or range of years), or on a specific carrier, and so indexes have been built on each of those columns. There is also a Date index, since it seems likely that people would want to search for flights on a certain date. However, it does not seem so likely that people would search for flights in a specific month across all years, and thus we have not built an index on month alone. The Date index does contain the month column, but the index can only be used if year is also part of the query.\n\nSHOW INDEXES FROM flights;\n\n\n\n\n\nTable 7.1: SQL information about how the query will be run. Because distance is not indexed, all 48 million rows must be searched.\n\nTable\nNon_unique\nKey_name\nSeq_in_index\nColumn_name\nCardinality\n\n\n\nflights\n1\nYear\n1\nyear\n7\n\n\nflights\n1\nDate\n1\nyear\n7\n\n\nflights\n1\nDate\n2\nmonth\n89\n\n\nflights\n1\nDate\n3\nday\n2712\n\n\nflights\n1\nOrigin\n1\norigin\n2267\n\n\nflights\n1\nDest\n1\ndest\n2267\n\n\nflights\n1\nCarrier\n1\ncarrier\n134\n\n\nflights\n1\ntailNum\n1\ntailnum\n37862\n\n\n\n\n\n\n\n\nefficiencies in SELECTing\nMySQL provides information about how it is going to perform a query using the EXPLAIN syntax. The information communicates how onerous the query is, without actually running it—saving you the time of having to wait for it to execute. Table 7.2 provides output that reflects the query plan returned by the MySQL server.\n\nEXPLAIN SELECT * FROM flights WHERE distance &gt; 3000;\n\n\n\n\n\nTable 7.2: SQL information about how the query will be run. Because distance is not indexed, all 48 million rows must be searched.\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n1\nSIMPLE\nflights\np1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24,p25,p26,p27,p28,p29,p30,p31,p32\nALL\n\n\n\n\n47932811\n33.3\nUsing where\n\n\n\n\n\n\n\nIf we were to run a query for long flights using the distance column the server will have to inspect each of the 48 million rows, because distance is not indexed. A query on a non-indexed variable is the slowest possible search and is often called a table scan. The 48 million number that you see in the rows column is an estimate of the number of rows that MySQL will have to consult in order to process your query. In general, more rows mean a slower query.\nOn the other hand, a search for recent flights using the year column, which has an index built on it, considers many fewer rows (about 6.3 million, those flights in 2013).\n\nEXPLAIN SELECT * FROM flights WHERE year = 2013;\n\n\n\n\n\nTable 7.3: SQL information about how the query will be run. Because year is indexed, only 6 million rows (those in 2013) must be searched.\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n1\nSIMPLE\nflights\np27\nALL\nYear,Date\n\n\n\n6369482\n100\nUsing where\n\n\n\n\n\n\n\nIn a search by year and month, SQL uses the Date index. Only 700,000 rows are searched, those in June of 2013.\n\nEXPLAIN SELECT * FROM flights WHERE year = 2013 AND month = 6;\n\n\n\n\n\nTable 7.4: SQL information about how the query will be run. Because year and month are both indexed, only 700,000 rows (those in June of 2013) must be searched.\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n1\nSIMPLE\nflights\np27\nref\nYear,Date\nDate\n6\nconst,const\n714535\n100\n\n\n\n\n\n\n\n\nIf we search for particular months across all years, the indexing does not help at all. The query results in a table scan.\n\nEXPLAIN SELECT * FROM flights WHERE month = 6;\n\n\n\n\n\nTable 7.5: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n1\nSIMPLE\nflights\np1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24,p25,p26,p27,p28,p29,p30,p31,p32\nALL\n\n\n\n\n47932811\n10\nUsing where\n\n\n\n\n\n\n\nAlthough month is part of the Date index, it is the second column in the index, and thus it doesn’t help us when we aren’t filtering on year. Thus, if it were common for our users to search on month without year, it would probably be worth building an index on month. Were we to actually run these queries, there would be a significant difference in computational time.\nefficiencies in JOINing\nUsing indexes is especially important for efficiency when performing JOIN operations on large tables. In the two examples below, both queries use indexes. However, because the cardinality of the index on tailnum is larger that the cardinality of the index on year (see Table 7.1), the number of rows in flights associated with each unique value of tailnum is smaller than for each unique value of year. Thus, the first query runs faster.\n\nEXPLAIN \n  SELECT * FROM planes p \n  LEFT JOIN flights o ON p.tailnum = o.TailNum\n  WHERE manufacturer = 'BOEING';\n\n\n\n\n\nTable 7.6: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\nid\nselect_type\ntable\nrows\nkey\nkey_len\nref\npartitions\ntype\npossible_keys\nfiltered\nExtra\n\n\n\n1\nSIMPLE\np\n3322\n\n\n\n\nALL\n\n10\nUsing where\n\n\n1\nSIMPLE\no\n1266\ntailNum\n9\nairlines.p.tailnum\np1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24,p25,p26,p27,p28,p29,p30,p31,p32\nref\ntailNum\n100\n\n\n\n\n\n\n\n\n\n\nEXPLAIN \n  SELECT * FROM planes p \n  LEFT JOIN flights o ON p.Year = o.Year\n  WHERE manufacturer = 'BOEING';\n\n\n\n\n\nTable 7.7: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\nid\nselect_type\ntable\nrows\nkey\nkey_len\nref\npartitions\ntype\npossible_keys\nfiltered\nExtra\n\n\n\n1\nSIMPLE\np\n3322\n\n\n\n\nALL\n\n10\nUsing where\n\n\n1\nSIMPLE\no\n6450117\nYear\n3\nairlines.p.year\np1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,p12,p13,p14,p15,p16,p17,p18,p19,p20,p21,p22,p23,p24,p25,p26,p27,p28,p29,p30,p31,p32\nref\nYear,Date\n100\nUsing where"
  },
  {
    "objectID": "07-db-etc.html#footnotes",
    "href": "07-db-etc.html#footnotes",
    "title": "7  SQL extras",
    "section": "",
    "text": "Analogy taken from: https://www.quora.com/profile/Lara-Mazilu↩︎\nTaken from: https://dev.mysql.com/doc/refman/8.0/en/show-index.html↩︎\nExample taken from: https://sebhastian.com/mysql-median/↩︎"
  },
  {
    "objectID": "07-db-etc.html#sql-in-dbplyr",
    "href": "07-db-etc.html#sql-in-dbplyr",
    "title": "7  SQL extras",
    "section": "\n7.2 SQL in dbplyr\n",
    "text": "7.2 SQL in dbplyr\n\n\n7.2.1 Median\nLet’s start with an example, calculating the median altitude in the airports table. (Using airports instead of flights just because the airports table is so much smaller.)3\n\nairports &lt;- tbl(con_air, \"airports\")\n\nhead(airports)\n\n# Source:   SQL [6 x 9]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  faa   name                           lat   lon   alt    tz dst   city  country\n  &lt;chr&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1 04G   Lansdowne Airport             41.1 -80.6  1044    -5 A     Youn… United…\n2 06A   Moton Field Municipal Airpo…  32.5 -85.7   264    -6 A     Tusk… United…\n3 06C   Schaumburg Regional           42.0 -88.1   801    -6 A     Scha… United…\n4 06N   Randall Airport               41.4 -74.4   523    -5 A     Midd… United…\n5 09J   Jekyll Island Airport         31.1 -81.4    11    -5 A     Jeky… United…\n6 0A9   Elizabethton Municipal Airp…  36.4 -82.2  1593    -5 A     Eliz… United…\n\n\nIt seems as though the SQL query to calculate the median should be reasonably straightforward.\n\nmedian_query &lt;- airports |&gt;\n  summarize(med_alt = median(alt, na.rm = TRUE))\n\nshow_query(median_query)\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`\n\n\nBut when the SQL query is applied to the airports table, we get an error.\n\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`;\n\nError: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`' at line 1 [1064]\n\n\nAdditionally, the R code itself doesn’t run and gives the same error. Huh, maybe calculating the median is actually harder than it seems (yes, it is).\n\nairports |&gt;\n  summarize(med_alt = median(alt, na.rm = TRUE))\n\nError in `collect()`:\n! Failed to collect lazy table.\nCaused by error:\n! You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`\nLIMIT 7' at line 1 [1064]\n\n\nOne way to calculate the median is to use a row index on the sorted values. Note that attaching a row index to the sorted values requires the values to be sorted (sorting can take a long time).\nBelow is the full code to calculate the median.\n\nSET @row_index := -1;\n\n\nSELECT AVG(subquery.alt) AS median_value\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n\n\n1 records\n\n\n\nmedian_value\n\n\n\n\n476\n\n\n\n\n\nBut let’s break down what the code is doing… First, set the row_index to -1 and iterate through by adding +1 for each row. Then concatenate the row_index information onto our table of interest.\n\nSET @row_index := -1;\n\n\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n  LIMIT 10;\n\n\n\n\nDisplaying records 1 - 10\n\n\n\nrow_index\n\n\nalt\n\n\n\n\n\n0\n\n\n-54\n\n\n\n\n1\n\n\n-42\n\n\n\n\n2\n\n\n0\n\n\n\n\n3\n\n\n0\n\n\n\n\n4\n\n\n0\n\n\n\n\n5\n\n\n0\n\n\n\n\n6\n\n\n0\n\n\n\n\n7\n\n\n0\n\n\n\n\n8\n\n\n0\n\n\n\n\n9\n\n\n0\n\n\n\n\n\n\nNext, filter the data to include only the middle row or two rows.\n\nSET @row_index := -1;\n\n\nSELECT *\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n\n\n2 records\n\n\n\nrow_index\n\n\nalt\n\n\n\n\n\n728\n\n\n474\n\n\n\n\n729\n\n\n477\n\n\n\n\n\n\nThe last step is to average the middle row(s). If only one row is pulled out in the previous query, then only one row will be averaged (which the computer does happily).\n\nSET @row_index := -1;\n\n\nSELECT AVG(subquery.alt) AS median_value\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n\n\n1 records\n\n\n\nmedian_value\n\n\n\n\n476\n\n\n\n\n\nFor a computer to calculate the median: If sorting the numbers and then finding the (average of the) middle numbers, the task will take \\(O(nlog(n))\\) time. That is, a dataset with 1000 records will be \\((1000 \\cdot log(1000)) / (100 \\cdot log(100)) = 10 \\cdot log(900)\\) times slower than a data set with 100 records. There are some caveats: (1) some sorting algorithms are faster, and (2) you don’t need to sort every number to get the median. But generally, sorting is a slow operation!\nFor a computer to calculate the mean: Averaging is just summing, and it happens in linear time, \\(O(n)\\). That means that a dataset with 1000 records will be \\(1000/100 = 10\\) times slower than a dataset with 100 records.\nVerdict: Generally, for a computer, it is easier to calculate a mean than a median, because of the need to sort in finding a median.\n\n7.2.2 CASE WHEN\n\nConsider the various R functions that create new variables based on an original variable. The CASE WHEN function in SQL plays the role of multiple R functions including ifelse(), case_when(), and cut().\nifelse()\nWith ifelse() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = ifelse(alt &gt; 500, \"above sea\", \"near sea\")) |&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… abov…\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… abov…\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… abov…\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\nif_query &lt;- airports |&gt;\n  mutate(sea = ifelse(alt &gt; 500, \"above sea\", \"near sea\"))\n\nshow_query(if_query)\n\n&lt;SQL&gt;\nSELECT\n  *,\n  CASE WHEN (`alt` &gt; 500.0) THEN 'above sea' WHEN NOT (`alt` &gt; 500.0) THEN 'near sea' END AS `sea`\nFROM `airports`\n\n\n\nSELECT *,\nCASE WHEN (`alt` &gt; 500.0) THEN 'above sea' WHEN NOT (`alt` &gt; 500.0) THEN 'near sea' END AS `sea`\nFROM `airports` \nLIMIT 5;\n\n\n\n\n5 records\n\n\n\nfaa\n\n\nname\n\n\nlat\n\n\nlon\n\n\nalt\n\n\ntz\n\n\ndst\n\n\ncity\n\n\ncountry\n\n\nsea\n\n\n\n\n\n04G\n\n\nLansdowne Airport\n\n\n41.1\n\n\n-80.6\n\n\n1044\n\n\n-5\n\n\nA\n\n\nYoungstown\n\n\nUnited States\n\n\nabove sea\n\n\n\n\n06A\n\n\nMoton Field Municipal Airport\n\n\n32.5\n\n\n-85.7\n\n\n264\n\n\n-6\n\n\nA\n\n\nTuskegee\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n06C\n\n\nSchaumburg Regional\n\n\n42.0\n\n\n-88.1\n\n\n801\n\n\n-6\n\n\nA\n\n\nSchaumburg\n\n\nUnited States\n\n\nabove sea\n\n\n\n\n06N\n\n\nRandall Airport\n\n\n41.4\n\n\n-74.4\n\n\n523\n\n\n-5\n\n\nA\n\n\nMiddletown\n\n\nUnited States\n\n\nabove sea\n\n\n\n\n09J\n\n\nJekyll Island Airport\n\n\n31.1\n\n\n-81.4\n\n\n11\n\n\n-5\n\n\nA\n\n\nJekyll Island\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n\n\n\n7.2.2.1 case_when()\n\nWith case_when() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = case_when(\n    alt &lt; 500 ~ \"near sea\",\n    alt &lt; 2000 ~ \"low alt\",\n    alt &lt; 3000 ~ \"mod alt\",\n    alt &lt; 5500 ~ \"high alt\",\n    alt &gt; 5500 ~ \"extreme alt\")) |&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… low …\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… low …\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… low …\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\ncw_query &lt;- airports |&gt;\n  mutate(sea = case_when(\n    alt &lt; 500 ~ \"near sea\",\n    alt &lt; 2000 ~ \"low alt\",\n    alt &lt; 3000 ~ \"mod alt\",\n    alt &lt; 5500 ~ \"high alt\",\n    alt &gt; 5500 ~ \"extreme alt\"))\n\nshow_query(cw_query)\n\n&lt;SQL&gt;\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt; 500.0) THEN 'near sea'\nWHEN (`alt` &lt; 2000.0) THEN 'low alt'\nWHEN (`alt` &lt; 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt; 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\n\n\n\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt; 500.0) THEN 'near sea'\nWHEN (`alt` &lt; 2000.0) THEN 'low alt'\nWHEN (`alt` &lt; 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt; 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\nLIMIT 5;\n\n\n\n\n5 records\n\n\n\nfaa\n\n\nname\n\n\nlat\n\n\nlon\n\n\nalt\n\n\ntz\n\n\ndst\n\n\ncity\n\n\ncountry\n\n\nsea\n\n\n\n\n\n04G\n\n\nLansdowne Airport\n\n\n41.1\n\n\n-80.6\n\n\n1044\n\n\n-5\n\n\nA\n\n\nYoungstown\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n06A\n\n\nMoton Field Municipal Airport\n\n\n32.5\n\n\n-85.7\n\n\n264\n\n\n-6\n\n\nA\n\n\nTuskegee\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n06C\n\n\nSchaumburg Regional\n\n\n42.0\n\n\n-88.1\n\n\n801\n\n\n-6\n\n\nA\n\n\nSchaumburg\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n06N\n\n\nRandall Airport\n\n\n41.4\n\n\n-74.4\n\n\n523\n\n\n-5\n\n\nA\n\n\nMiddletown\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n09J\n\n\nJekyll Island Airport\n\n\n31.1\n\n\n-81.4\n\n\n11\n\n\n-5\n\n\nA\n\n\nJekyll Island\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n\n\ncut()\nWith cut() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = cut(\n    alt,\n    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),\n    labels = c(\"near sea\", \"low alt\", \"mod alt\", \"high alt\", \"extreme alt\")\n  )\n)|&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… low …\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… low …\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… low …\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\ncw_query &lt;- airports |&gt;\n  mutate(sea = cut(\n    alt,\n    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),\n    labels = c(\"near sea\", \"low alt\", \"mod alt\", \"high alt\", \"extreme alt\")\n  )\n)\n\nshow_query(cw_query)\n\n&lt;SQL&gt;\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt;= 500.0) THEN 'near sea'\nWHEN (`alt` &lt;= 2000.0) THEN 'low alt'\nWHEN (`alt` &lt;= 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt;= 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\n\n\n\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt;= 500.0) THEN 'near sea'\nWHEN (`alt` &lt;= 2000.0) THEN 'low alt'\nWHEN (`alt` &lt;= 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt;= 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\nLIMIT 5;\n\n\n\n\n5 records\n\n\n\nfaa\n\n\nname\n\n\nlat\n\n\nlon\n\n\nalt\n\n\ntz\n\n\ndst\n\n\ncity\n\n\ncountry\n\n\nsea\n\n\n\n\n\n04G\n\n\nLansdowne Airport\n\n\n41.1\n\n\n-80.6\n\n\n1044\n\n\n-5\n\n\nA\n\n\nYoungstown\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n06A\n\n\nMoton Field Municipal Airport\n\n\n32.5\n\n\n-85.7\n\n\n264\n\n\n-6\n\n\nA\n\n\nTuskegee\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n06C\n\n\nSchaumburg Regional\n\n\n42.0\n\n\n-88.1\n\n\n801\n\n\n-6\n\n\nA\n\n\nSchaumburg\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n06N\n\n\nRandall Airport\n\n\n41.4\n\n\n-74.4\n\n\n523\n\n\n-5\n\n\nA\n\n\nMiddletown\n\n\nUnited States\n\n\nlow alt\n\n\n\n\n09J\n\n\nJekyll Island Airport\n\n\n31.1\n\n\n-81.4\n\n\n11\n\n\n-5\n\n\nA\n\n\nJekyll Island\n\n\nUnited States\n\n\nnear sea\n\n\n\n\n\n\n\n7.2.3 SELECT DISTINCT\n\nHow many distinct time zones are there in the airports table? distinct() translates to SELECT DISTINCT which runs easily on our tbl.\n\nairports |&gt;\n  select(tz) |&gt;\n  distinct()\n\n# Source:   SQL [?? x 1]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n     tz\n  &lt;int&gt;\n1    -5\n2    -6\n3    -8\n4    -7\n5    -9\n6   -10\n# ℹ more rows\n\n\n\ndist_query &lt;- airports |&gt;\n  select(tz) |&gt;\n  distinct()\n\nshow_query(dist_query)\n\n&lt;SQL&gt;\nSELECT DISTINCT `tz`\nFROM `airports`\n\n\n\nSELECT DISTINCT `tz`\nFROM `airports`;\n\n\n\n\n7 records\n\n\n\ntz\n\n\n\n\n\n-5\n\n\n\n\n-6\n\n\n\n\n-8\n\n\n\n\n-7\n\n\n\n\n-9\n\n\n\n\n-10\n\n\n\n\n8\n\n\n\n\n\n\n\n7.2.4 LIMIT\n\nAs expected, head() translates to LIMIT which we’ve already used many many times.\n\nairports |&gt;\n  head(5)\n\n# Source:   SQL [5 x 9]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:NA/airlines]\n  faa   name                           lat   lon   alt    tz dst   city  country\n  &lt;chr&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1 04G   Lansdowne Airport             41.1 -80.6  1044    -5 A     Youn… United…\n2 06A   Moton Field Municipal Airpo…  32.5 -85.7   264    -6 A     Tusk… United…\n3 06C   Schaumburg Regional           42.0 -88.1   801    -6 A     Scha… United…\n4 06N   Randall Airport               41.4 -74.4   523    -5 A     Midd… United…\n5 09J   Jekyll Island Airport         31.1 -81.4    11    -5 A     Jeky… United…\n\n\n\nhead_query &lt;- airports |&gt;\n  head(5)\n\nshow_query(head_query)\n\n&lt;SQL&gt;\nSELECT *\nFROM `airports`\nLIMIT 5\n\n\n\nSELECT *\nFROM `airports`\nLIMIT 5;\n\n\n\n\n5 records\n\n\n\nfaa\n\n\nname\n\n\nlat\n\n\nlon\n\n\nalt\n\n\ntz\n\n\ndst\n\n\ncity\n\n\ncountry\n\n\n\n\n\n04G\n\n\nLansdowne Airport\n\n\n41.1\n\n\n-80.6\n\n\n1044\n\n\n-5\n\n\nA\n\n\nYoungstown\n\n\nUnited States\n\n\n\n\n06A\n\n\nMoton Field Municipal Airport\n\n\n32.5\n\n\n-85.7\n\n\n264\n\n\n-6\n\n\nA\n\n\nTuskegee\n\n\nUnited States\n\n\n\n\n06C\n\n\nSchaumburg Regional\n\n\n42.0\n\n\n-88.1\n\n\n801\n\n\n-6\n\n\nA\n\n\nSchaumburg\n\n\nUnited States\n\n\n\n\n06N\n\n\nRandall Airport\n\n\n41.4\n\n\n-74.4\n\n\n523\n\n\n-5\n\n\nA\n\n\nMiddletown\n\n\nUnited States\n\n\n\n\n09J\n\n\nJekyll Island Airport\n\n\n31.1\n\n\n-81.4\n\n\n11\n\n\n-5\n\n\nA\n\n\nJekyll Island\n\n\nUnited States\n\n\n\n\n\n\n\n7.2.5 Plotting (!?!?!)\nWhat if we want to plot the values in a tbl? Seems like ggplot() will support using a tbl as input.\n\nairports |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\n\nLet’s ignore the airports outside of the US.\n\nairports |&gt;\n  filter(lon &lt; 0) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\n\nWhat does the ggplot() code translate to in SQL? Of course there is an error! SQL doesn’t have any plotting mechanism, so there couldn’t be any translation into SQL. Which reminds us that different programming languages have different advantages and disadvantages. The more we know about them, the better adept we will be at using the right language at the right time.\n\ngg_query &lt;- airports |&gt;\n  filter(lon &lt; 0) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\nshow_query(gg_query)\n\nError in UseMethod(\"show_query\"): no applicable method for 'show_query' applied to an object of class \"c('gg', 'ggplot')\""
  },
  {
    "objectID": "05-creating-db.html#creating-indexes",
    "href": "05-creating-db.html#creating-indexes",
    "title": "5  Creating databases",
    "section": "\n5.3 Creating INDEXes",
    "text": "5.3 Creating INDEXes\nIndexes can be created on one or more variable. A table does not need to have an INDEX (or a KEY).\n\nCREATE INDEX name_of_index ON table (col1);\n\n\nCREATE INDEX name_of_index ON table (col1, col2);"
  },
  {
    "objectID": "16-db-etc.html",
    "href": "16-db-etc.html",
    "title": "8  SQL extras",
    "section": "",
    "text": "Back to the flights\nThe examples below use the airlines database, including the flights, carriers, airports, and planes tables.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#efficiencies",
    "href": "16-db-etc.html#efficiencies",
    "title": "8  SQL extras",
    "section": "\n8.1 Efficiencies",
    "text": "8.1 Efficiencies\nIt is worth pointing out a few aspects to loading data into SQL: keys, indexes, and partitioning.\nBefore we get to the definitions, consider this analogy:\n\nEach library (database) has books (tables). Each book (table) has pages (rows). Each page (row) has a unique page number to identify it (key value); to find a particular page, you sort through the page numbers (key values). But it isn’t immediately obvious where the particular page of interest is, you might have to page through the book a little bit to find the page of interest. It would be easier if you had several bookmarks throughout the book to anchor some of the page numbers. For example, if you want page 1047 and you have a bookmark on page 1050, you only have to turn back three pages. The bookmark is an index, it helps you find the desired rows much more quickly.1\n\n\n8.1.1 Key\nKeys are unique identifiers for each row, used primarily for connecting tables. Keys are generally not helpful for efficiency, but they are important for data integrity and relationships between tables. A key is a pointer that identifies a record. In practice, a key is one or more columns that are earmarked to uniquely identify a record in a table. Keys serve two main purposes:\n\nThey provide constraints on the column such as that it can’t store duplicate or null values.\nThey are also used to generate relationships among different tables.\n\n\n\nPRIMARY KEY is a column or set of columns that uniquely identify each row. Primary keys cannot be NULL. Each table must always have one (and only one) PK. The PK can be made up of one column, but if that isn’t enough to uniquely identify the row, more columns may be added. Sometimes it is easier to designate a numeric column (e.g., row number) to be the PK.\n\nFOREIGN KEY is a column or set of columns that reference a primary key in a different table. The FK links two tables together, and the link is called a relationship.\n\n(There are other keys such as: Super Key, Minimal Super Key, Candidate Key, Unique Key, Alternate Key, Composite Key, Natural Key, Surrogate Key.)\n\n8.1.2 Index\nIndexes are the crux of why SQL is so much more efficient than, say, R. An index is a lookup table that helps SQL keep track of which records contain certain values. By indexing the rows, SQL is able to optimize sorting and joining tables. The index is created in advance (when the table is created) and saved to disk, which can take up substantial space on the disk. Sometimes more than one variable is used to index the table. There are trade-offs to having a lot of indexes (disk space but fast wrangling) versus a few indexes (slow wrangling but less space).\nA table may have more than one index but you shouldn’t add indexes to every column in a table, as these have to be updated for every addition/update/delete to the column. Rather, indexes should be added to columns that are frequently included in queries.\nIndexes may not make much difference for small databases, but, as tables grow in size, queries benefit more from indexes.\nIn MySQL the commands SHOW KEYS and SHOW INDEXES provide information about the keys and indexes for each table (the two operations are synonymous in MySQL). Neither operation is available in DuckDB.\nNotice that the planes table has a single PRIMARY key. That primary key is used to index the table. The flights table has no PRIMARY key, but it does have six different indexes: Year, Date, Origin, Dest, Carrier, and tailNum.\n\nSHOW INDEXES FROM planes;\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nNon_unique\nKey_name\nSeq_in_index\nColumn_name\nCollation\nCardinality\nSub_part\nPacked\nNull\nIndex_type\nComment\nIndex_comment\nIgnored\n\n\nplanes\n0\nPRIMARY\n1\ntailnum\nA\n3322\n\n\n\nBTREE\n\n\nNO\n\n\n\n\n\nSHOW INDEXES FROM flights;\n\n\n8 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable\nNon_unique\nKey_name\nSeq_in_index\nColumn_name\nCollation\nCardinality\nSub_part\nPacked\nNull\nIndex_type\nComment\nIndex_comment\nIgnored\n\n\n\nflights\n1\nYear\n1\nyear\nA\n2\n\n\nYES\nBTREE\n\n\nNO\n\n\nflights\n1\nDate\n1\nyear\nA\n2\n\n\nYES\nBTREE\n\n\nNO\n\n\nflights\n1\nDate\n2\nmonth\nA\n33\n\n\nYES\nBTREE\n\n\nNO\n\n\nflights\n1\nDate\n3\nday\nA\n1031\n\n\nYES\nBTREE\n\n\nNO\n\n\nflights\n1\nOrigin\n1\norigin\nA\n904\n\n\n\nBTREE\n\n\nNO\n\n\nflights\n1\nDest\n1\ndest\nA\n899\n\n\n\nBTREE\n\n\nNO\n\n\nflights\n1\nCarrier\n1\ncarrier\nA\n45\n\n\n\nBTREE\n\n\nNO\n\n\nflights\n1\ntailNum\n1\ntailnum\nA\n13970\n\n\nYES\nBTREE\n\n\nNO\n\n\n\n\n\nThe values output by SHOW INDEXES are:2\n\nTable: The name of the table.\nNon_unique: 0 if the index cannot contain duplicates, 1 if it can.\nKey_name: The name of the index. If the index is the primary key, the name is always PRIMARY.\nSeq_in_index: The column sequence number in the index, starting with 1.\nColumn_name: The column name. See also the description for the Expression column.\nCollation: How the column is sorted in the index. This can have values A (ascending), D (descending), or NULL (not sorted).\nCardinality: An estimate of the number of unique values in the index.\nCardinality is counted based on statistics stored as integers, so the value is not necessarily exact even for small tables. The higher the cardinality, the greater the chance that MySQL uses the index when doing joins. Indexing with a high cardinality variable will be particularly useful for increased efficiency (if that variable is being queried).\nSub_part: The index prefix. That is, the number of indexed characters if the column is only partly indexed, NULL if the entire column is indexed.\nPacked: Indicates how the key is packed. NULL if it is not.\nNull: Contains YES if the column may contain NULL values and ’’ if not.\nIndex_type: The index method used (BTREE, FULLTEXT, HASH, RTREE).\nComment: Information about the index not described in its own column, such as disabled if the index is disabled.\nIndex_comment: Any comment provided for the index with a COMMENT attribute when the index was created.\n\n8.1.3 Partitioning\nAnother way to speed up query retrievals is to partition the data tables. If, for example, the SNL queries were always done by year, then the episodes table could be partitioned such that they are stored as separate tables (one per year). The partitioning functions as an index on year. The user would not be able to tell the difference between the unpartitioned episodes table and the partitioned one. However, queries done by year would be faster. Queries done grouped in another way would be slower.\n\n8.1.4 Querying\nIndexes are built to accommodate the specific queries that are most likely to be run. However, you might not know which queries are going to be run, so it isn’t always obviously how to index a table.\nFor the flights table, it seems likely that many queries will involve searching for flights from a particular origin, or to a particular destination, or during a particular year (or range of years), or on a specific carrier, and so indexes have been built on each of those columns. There is also a Date index, since it seems likely that people would want to search for flights on a certain date. However, it does not seem so likely that people would search for flights in a specific month across all years, and thus we have not built an index on month alone. The Date index does contain the month column, but the index can only be used if year is also part of the query.\n\nSHOW INDEXES FROM flights;\n\n\n\n\nTable 8.1: SQL information about how the query will be run. Because distance is not indexed, all 48 million rows must be searched.\n\n\n\n\n\nTable\nNon_unique\nKey_name\nSeq_in_index\nColumn_name\nCardinality\n\n\n\nflights\n1\nYear\n1\nyear\n2\n\n\nflights\n1\nDate\n1\nyear\n2\n\n\nflights\n1\nDate\n2\nmonth\n33\n\n\nflights\n1\nDate\n3\nday\n1031\n\n\nflights\n1\nOrigin\n1\norigin\n904\n\n\nflights\n1\nDest\n1\ndest\n899\n\n\nflights\n1\nCarrier\n1\ncarrier\n45\n\n\nflights\n1\ntailNum\n1\ntailnum\n13970\n\n\n\n\n\n\n\n\n\nefficiencies in SELECTing\nMySQL provides information about how it is going to perform a query using the EXPLAIN syntax. The information communicates how onerous the query is, without actually running it—saving you the time of having to wait for it to execute. Table 8.2 provides output that reflects the query plan returned by the MySQL server.\n\nEXPLAIN SELECT * FROM flights WHERE distance &gt; 3000;\n\n\n\n\nTable 8.2: SQL information about how the query will be run. Because distance is not indexed, all 48 million rows must be searched.\n\n\n\n\n\nid\nselect_type\ntable\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nExtra\n\n\n1\nSIMPLE\nflights\nALL\n\n\n\n\n18008372\nUsing where\n\n\n\n\n\n\n\n\nIf we were to run a query for long flights using the distance column the server will have to inspect each of the 48 million rows, because distance is not indexed. A query on a non-indexed variable is the slowest possible search and is often called a table scan. The 48 million number that you see in the rows column is an estimate of the number of rows that MySQL will have to consult in order to process your query. In general, more rows mean a slower query.\nOn the other hand, a search for recent flights using the year column, which has an index built on it, considers many fewer rows (about 6.3 million, those flights in 2013).\n\nEXPLAIN SELECT * FROM flights WHERE year = 2013;\n\n\n\n\nTable 8.3: SQL information about how the query will be run. Because year is indexed, only 6 million rows (those in 2013) must be searched.\n\n\n\n\n\nid\nselect_type\ntable\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nExtra\n\n\n1\nSIMPLE\nflights\nALL\nYear,Date\n\n\n\n6369482\nUsing where\n\n\n\n\n\n\n\n\nIn a search by year and month, SQL uses the Date index. Only 700,000 rows are searched, those in June of 2013.\n\nEXPLAIN SELECT * FROM flights WHERE year = 2013 AND month = 6;\n\n\n\n\nTable 8.4: SQL information about how the query will be run. Because year and month are both indexed, only 700,000 rows (those in June of 2013) must be searched.\n\n\n\n\n\nid\nselect_type\ntable\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nExtra\n\n\n1\nSIMPLE\nflights\nref\nYear,Date\nDate\n6\nconst,const\n547493\n\n\n\n\n\n\n\n\n\nIf we search for particular months across all years, the indexing does not help at all. The query results in a table scan.\n\nEXPLAIN SELECT * FROM flights WHERE month = 6;\n\n\n\n\nTable 8.5: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\n\n\n\n\nid\nselect_type\ntable\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nExtra\n\n\n1\nSIMPLE\nflights\nALL\n\n\n\n\n18008372\nUsing where\n\n\n\n\n\n\n\n\nAlthough month is part of the Date index, it is the second column in the index, and thus it doesn’t help us when we aren’t filtering on year. Thus, if it were common for our users to search on month without year, it would probably be worth building an index on month. Were we to actually run these queries, there would be a significant difference in computational time.\nefficiencies in JOINing\nUsing indexes is especially important for efficiency when performing JOIN operations on large tables. In the two examples below, both queries use indexes. However, because the cardinality of the index on tailnum is larger that the cardinality of the index on year (see Table 8.1), the number of rows in flights associated with each unique value of tailnum is smaller than for each unique value of year. Thus, the first query runs faster.\n\nEXPLAIN \n  SELECT * FROM planes p \n  LEFT JOIN flights o ON p.tailnum = o.TailNum\n  WHERE manufacturer = 'BOEING';\n\n\n\n\nTable 8.6: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\n\n\n\n\nid\nselect_type\ntable\nrows\nkey\nkey_len\nref\ntype\npossible_keys\nExtra\n\n\n\n1\nSIMPLE\np\n3322\n\n\n\nALL\n\nUsing where\n\n\n1\nSIMPLE\no\n1289\ntailNum\n9\nairlines.p.tailnum\nref\ntailNum\n\n\n\n\n\n\n\n\n\n\n\nEXPLAIN \n  SELECT * FROM planes p \n  LEFT JOIN flights o ON p.Year = o.Year\n  WHERE manufacturer = 'BOEING';\n\n\n\n\nTable 8.7: SQL information about how the query will be run. Because month is not indexed on its own, all rows (48 million!) must be searched.\n\n\n\n\n\nid\nselect_type\ntable\nrows\nkey\nkey_len\nref\ntype\npossible_keys\nExtra\n\n\n\n1\nSIMPLE\np\n3322\n\n\n\nALL\n\nUsing where\n\n\n1\nSIMPLE\no\n6369482\nYear\n3\nairlines.p.year\nref\nYear,Date\nUsing where",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#sql-in-dbplyr",
    "href": "16-db-etc.html#sql-in-dbplyr",
    "title": "8  SQL extras",
    "section": "\n8.2 SQL in dbplyr\n",
    "text": "8.2 SQL in dbplyr\n\n\n8.2.1 Median\nLet’s start with an example, calculating the median altitude in the airports table. (Using airports instead of flights just because the airports table is so much smaller.)3\n\nairports &lt;- tbl(con_air, \"airports\")\n\nhead(airports)\n\n# Source:   SQL [6 x 9]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  faa   name                           lat   lon   alt    tz dst   city  country\n  &lt;chr&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1 04G   Lansdowne Airport             41.1 -80.6  1044    -5 A     Youn… United…\n2 06A   Moton Field Municipal Airpo…  32.5 -85.7   264    -6 A     Tusk… United…\n3 06C   Schaumburg Regional           42.0 -88.1   801    -6 A     Scha… United…\n4 06N   Randall Airport               41.4 -74.4   523    -5 A     Midd… United…\n5 09J   Jekyll Island Airport         31.1 -81.4    11    -5 A     Jeky… United…\n6 0A9   Elizabethton Municipal Airp…  36.4 -82.2  1593    -5 A     Eliz… United…\n\n\nIt seems as though the SQL query to calculate the median should be reasonably straightforward.\n\nmedian_query &lt;- airports |&gt;\n  summarize(med_alt = median(alt, na.rm = TRUE))\n\nshow_query(median_query)\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`\n\n\nBut when the SQL query is applied to the airports table, we get an error.\n\nSELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY `alt`) AS `med_alt`\nFROM `airports`;\n\nError: You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'AS `med_alt`\nFROM `airports`' at line 1 [1064]\n\n\nAdditionally, the R code itself doesn’t run and gives the same error. Huh, maybe calculating the median is actually harder than it seems (yes, it is).\n\nairports |&gt;\n  summarize(med_alt = median(alt, na.rm = TRUE))\n\nError in `collect()`:\n! Failed to collect lazy table.\nCaused by error:\n! You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near 'AS `med_alt`\nFROM `airports`\nLIMIT 7' at line 1 [1064]\n\n\nOne way to calculate the median is to use a row index on the sorted values. Note that attaching a row index to the sorted values requires the values to be sorted (sorting can take a long time).\nBelow is the full code to calculate the median.\n\nSET @row_index := -1;\n\n\nSELECT AVG(subquery.alt) AS median_value\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n1 records\n\nmedian_value\n\n\n476\n\n\n\n\nBut let’s break down what the code is doing… First, set the row_index to -1 and iterate through by adding +1 for each row. Then concatenate the row_index information onto our table of interest.\n\nSET @row_index := -1;\n\n\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n  LIMIT 10;\n\n\nDisplaying records 1 - 10\n\nrow_index\nalt\n\n\n\n0\n-54\n\n\n1\n-42\n\n\n2\n0\n\n\n3\n0\n\n\n4\n0\n\n\n5\n0\n\n\n6\n0\n\n\n7\n0\n\n\n8\n0\n\n\n9\n0\n\n\n\n\n\nNext, filter the data to include only the middle row or two rows.\n\nSET @row_index := -1;\n\n\nSELECT *\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n2 records\n\nrow_index\nalt\n\n\n\n728\n474\n\n\n729\n477\n\n\n\n\n\nThe last step is to average the middle row(s). If only one row is pulled out in the previous query, then only one row will be averaged (which the computer does happily).\n\nSET @row_index := -1;\n\n\nSELECT AVG(subquery.alt) AS median_value\nFROM (\nSELECT @row_index:=@row_index + 1 AS row_index, alt\n  FROM airports\n  ORDER BY alt\n) AS subquery\nWHERE subquery.row_index IN (FLOOR(@row_index / 2), CEIL(@row_index / 2));\n\n\n1 records\n\nmedian_value\n\n\n476\n\n\n\n\nFor a computer to calculate the median: If sorting the numbers and then finding the (average of the) middle numbers, the task will take \\(O(nlog(n))\\) time. That is, a dataset with 1000 records will be \\((1000 \\cdot log(1000)) / (100 \\cdot log(100)) = 10 \\cdot log(900)\\) times slower than a data set with 100 records. There are some caveats: (1) some sorting algorithms are faster, and (2) you don’t need to sort every number to get the median. But generally, sorting is a slow operation!\nFor a computer to calculate the mean: Averaging is just summing, and it happens in linear time, \\(O(n)\\). That means that a dataset with 1000 records will be \\(1000/100 = 10\\) times slower than a dataset with 100 records.\nVerdict: Generally, for a computer, it is easier to calculate a mean than a median, because of the need to sort in finding a median.\n\n8.2.2 CASE WHEN\n\nConsider the various R functions that create new variables based on an original variable. The CASE WHEN function in SQL plays the role of multiple R functions including ifelse(), case_when(), and cut().\nifelse()\nWith ifelse() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = ifelse(alt &gt; 500, \"above sea\", \"near sea\")) |&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… abov…\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… abov…\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… abov…\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\nif_query &lt;- airports |&gt;\n  mutate(sea = ifelse(alt &gt; 500, \"above sea\", \"near sea\"))\n\nshow_query(if_query)\n\n&lt;SQL&gt;\nSELECT\n  `airports`.*,\n  CASE WHEN (`alt` &gt; 500.0) THEN 'above sea' WHEN NOT (`alt` &gt; 500.0) THEN 'near sea' END AS `sea`\nFROM `airports`\n\n\n\nSELECT *,\nCASE WHEN (`alt` &gt; 500.0) THEN 'above sea' WHEN NOT (`alt` &gt; 500.0) THEN 'near sea' END AS `sea`\nFROM `airports` \nLIMIT 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nfaa\nname\nlat\nlon\nalt\ntz\ndst\ncity\ncountry\nsea\n\n\n\n04G\nLansdowne Airport\n41.1\n-80.6\n1044\n-5\nA\nYoungstown\nUnited States\nabove sea\n\n\n06A\nMoton Field Municipal Airport\n32.5\n-85.7\n264\n-6\nA\nTuskegee\nUnited States\nnear sea\n\n\n06C\nSchaumburg Regional\n42.0\n-88.1\n801\n-6\nA\nSchaumburg\nUnited States\nabove sea\n\n\n06N\nRandall Airport\n41.4\n-74.4\n523\n-5\nA\nMiddletown\nUnited States\nabove sea\n\n\n09J\nJekyll Island Airport\n31.1\n-81.4\n11\n-5\nA\nJekyll Island\nUnited States\nnear sea\n\n\n\n\n\n\n8.2.2.1 case_when()\n\nWith case_when() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = case_when(\n    alt &lt; 500 ~ \"near sea\",\n    alt &lt; 2000 ~ \"low alt\",\n    alt &lt; 3000 ~ \"mod alt\",\n    alt &lt; 5500 ~ \"high alt\",\n    alt &gt; 5500 ~ \"extreme alt\")) |&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… low …\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… low …\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… low …\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\ncw_query &lt;- airports |&gt;\n  mutate(sea = case_when(\n    alt &lt; 500 ~ \"near sea\",\n    alt &lt; 2000 ~ \"low alt\",\n    alt &lt; 3000 ~ \"mod alt\",\n    alt &lt; 5500 ~ \"high alt\",\n    alt &gt; 5500 ~ \"extreme alt\"))\n\nshow_query(cw_query)\n\n&lt;SQL&gt;\nSELECT\n  `airports`.*,\n  CASE\nWHEN (`alt` &lt; 500.0) THEN 'near sea'\nWHEN (`alt` &lt; 2000.0) THEN 'low alt'\nWHEN (`alt` &lt; 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt; 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\n\n\n\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt; 500.0) THEN 'near sea'\nWHEN (`alt` &lt; 2000.0) THEN 'low alt'\nWHEN (`alt` &lt; 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt; 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\nLIMIT 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nfaa\nname\nlat\nlon\nalt\ntz\ndst\ncity\ncountry\nsea\n\n\n\n04G\nLansdowne Airport\n41.1\n-80.6\n1044\n-5\nA\nYoungstown\nUnited States\nlow alt\n\n\n06A\nMoton Field Municipal Airport\n32.5\n-85.7\n264\n-6\nA\nTuskegee\nUnited States\nnear sea\n\n\n06C\nSchaumburg Regional\n42.0\n-88.1\n801\n-6\nA\nSchaumburg\nUnited States\nlow alt\n\n\n06N\nRandall Airport\n41.4\n-74.4\n523\n-5\nA\nMiddletown\nUnited States\nlow alt\n\n\n09J\nJekyll Island Airport\n31.1\n-81.4\n11\n-5\nA\nJekyll Island\nUnited States\nnear sea\n\n\n\n\n\ncut()\nWith cut() the R translates directly to CASE WHEN in SQL, and the SQL query runs easily.\n\nairports |&gt;\n  mutate(sea = cut(\n    alt,\n    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),\n    labels = c(\"near sea\", \"low alt\", \"mod alt\", \"high alt\", \"extreme alt\")\n  )\n)|&gt;\n  head(5)\n\n# Source:   SQL [5 x 10]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  faa   name                     lat   lon   alt    tz dst   city  country sea  \n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;\n1 04G   Lansdowne Airport       41.1 -80.6  1044    -5 A     Youn… United… low …\n2 06A   Moton Field Municipal…  32.5 -85.7   264    -6 A     Tusk… United… near…\n3 06C   Schaumburg Regional     42.0 -88.1   801    -6 A     Scha… United… low …\n4 06N   Randall Airport         41.4 -74.4   523    -5 A     Midd… United… low …\n5 09J   Jekyll Island Airport   31.1 -81.4    11    -5 A     Jeky… United… near…\n\n\n\ncw_query &lt;- airports |&gt;\n  mutate(sea = cut(\n    alt,\n    breaks = c(-Inf, 500, 2000, 3000, 5500, Inf),\n    labels = c(\"near sea\", \"low alt\", \"mod alt\", \"high alt\", \"extreme alt\")\n  )\n)\n\nshow_query(cw_query)\n\n&lt;SQL&gt;\nSELECT\n  `airports`.*,\n  CASE\nWHEN (`alt` &lt;= 500.0) THEN 'near sea'\nWHEN (`alt` &lt;= 2000.0) THEN 'low alt'\nWHEN (`alt` &lt;= 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt;= 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\n\n\n\nSELECT\n  *,\n  CASE\nWHEN (`alt` &lt;= 500.0) THEN 'near sea'\nWHEN (`alt` &lt;= 2000.0) THEN 'low alt'\nWHEN (`alt` &lt;= 3000.0) THEN 'mod alt'\nWHEN (`alt` &lt;= 5500.0) THEN 'high alt'\nWHEN (`alt` &gt; 5500.0) THEN 'extreme alt'\nEND AS `sea`\nFROM `airports`\nLIMIT 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nfaa\nname\nlat\nlon\nalt\ntz\ndst\ncity\ncountry\nsea\n\n\n\n04G\nLansdowne Airport\n41.1\n-80.6\n1044\n-5\nA\nYoungstown\nUnited States\nlow alt\n\n\n06A\nMoton Field Municipal Airport\n32.5\n-85.7\n264\n-6\nA\nTuskegee\nUnited States\nnear sea\n\n\n06C\nSchaumburg Regional\n42.0\n-88.1\n801\n-6\nA\nSchaumburg\nUnited States\nlow alt\n\n\n06N\nRandall Airport\n41.4\n-74.4\n523\n-5\nA\nMiddletown\nUnited States\nlow alt\n\n\n09J\nJekyll Island Airport\n31.1\n-81.4\n11\n-5\nA\nJekyll Island\nUnited States\nnear sea\n\n\n\n\n\n\n8.2.3 SELECT DISTINCT\n\nHow many distinct time zones are there in the airports table? distinct() translates to SELECT DISTINCT which runs easily on our tbl.\n\nairports |&gt;\n  select(tz) |&gt;\n  distinct()\n\n# Source:   SQL [?? x 1]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n     tz\n  &lt;int&gt;\n1    -5\n2    -6\n3    -8\n4    -7\n5    -9\n6   -10\n# ℹ more rows\n\n\n\ndist_query &lt;- airports |&gt;\n  select(tz) |&gt;\n  distinct()\n\nshow_query(dist_query)\n\n&lt;SQL&gt;\nSELECT DISTINCT `tz`\nFROM `airports`\n\n\n\nSELECT DISTINCT `tz`\nFROM `airports`;\n\n\n7 records\n\ntz\n\n\n\n-5\n\n\n-6\n\n\n-8\n\n\n-7\n\n\n-9\n\n\n-10\n\n\n8\n\n\n\n\n\n\n8.2.4 LIMIT\n\nAs expected, head() translates to LIMIT which we’ve already used many many times.\n\nairports |&gt;\n  head(5)\n\n# Source:   SQL [5 x 9]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  faa   name                           lat   lon   alt    tz dst   city  country\n  &lt;chr&gt; &lt;chr&gt;                        &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  \n1 04G   Lansdowne Airport             41.1 -80.6  1044    -5 A     Youn… United…\n2 06A   Moton Field Municipal Airpo…  32.5 -85.7   264    -6 A     Tusk… United…\n3 06C   Schaumburg Regional           42.0 -88.1   801    -6 A     Scha… United…\n4 06N   Randall Airport               41.4 -74.4   523    -5 A     Midd… United…\n5 09J   Jekyll Island Airport         31.1 -81.4    11    -5 A     Jeky… United…\n\n\n\nhead_query &lt;- airports |&gt;\n  head(5)\n\nshow_query(head_query)\n\n&lt;SQL&gt;\nSELECT `airports`.*\nFROM `airports`\nLIMIT 5\n\n\n\nSELECT *\nFROM `airports`\nLIMIT 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nfaa\nname\nlat\nlon\nalt\ntz\ndst\ncity\ncountry\n\n\n\n04G\nLansdowne Airport\n41.1\n-80.6\n1044\n-5\nA\nYoungstown\nUnited States\n\n\n06A\nMoton Field Municipal Airport\n32.5\n-85.7\n264\n-6\nA\nTuskegee\nUnited States\n\n\n06C\nSchaumburg Regional\n42.0\n-88.1\n801\n-6\nA\nSchaumburg\nUnited States\n\n\n06N\nRandall Airport\n41.4\n-74.4\n523\n-5\nA\nMiddletown\nUnited States\n\n\n09J\nJekyll Island Airport\n31.1\n-81.4\n11\n-5\nA\nJekyll Island\nUnited States\n\n\n\n\n\n\n8.2.5 Plotting (!?!?!)\nWhat if we want to plot the values in a tbl? Seems like ggplot() will support using a tbl as input.\n\nairports |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\n\n\n\n\nLet’s ignore the airports outside of the US.\n\nairports |&gt;\n  filter(lon &lt; 0) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\n\n\n\n\n\n\nWhat does the ggplot() code translate to in SQL? Of course there is an error! SQL doesn’t have any plotting mechanism, so there couldn’t be any translation into SQL. Which reminds us that different programming languages have different advantages and disadvantages. The more we know about them, the better adept we will be at using the right language at the right time.\n\ngg_query &lt;- airports |&gt;\n  filter(lon &lt; 0) |&gt;\n  ggplot(aes(x = lon, y = lat)) +\n  geom_point()\n\nshow_query(gg_query)\n\nError in UseMethod(\"show_query\"): no applicable method for 'show_query' applied to an object of class \"c('gg', 'ggplot')\"",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#best-practice",
    "href": "16-db-etc.html#best-practice",
    "title": "8  SQL extras",
    "section": "\n8.3 Best practice",
    "text": "8.3 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_air, shutdown = TRUE)",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#reflection-questions",
    "href": "16-db-etc.html#reflection-questions",
    "title": "8  SQL extras",
    "section": "\n8.4  Reflection questions",
    "text": "8.4  Reflection questions\n\nWhat is the main purpose of a KEY? Can a KEY ever be NULL? (Hint: does it depend on what type of KEY it is?)\nWhat is the main purpose of an INDEX? Can an INDEX ever be NULL?\nHow should we approach indexing? That is, what variables are good candidates for choosing to be the index?\nWhy is it harder to calculate the median than the mean?\nWhy are there three functions in R (ifelse(), case_when(), and cut()) to do the job of a single function in SQL (CASE WHEN)?\nWhy won’t dbplyr provide the syntax for the SQL translation of ggplot()?",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#ethics-considerations",
    "href": "16-db-etc.html#ethics-considerations",
    "title": "8  SQL extras",
    "section": "\n8.5  Ethics considerations",
    "text": "8.5  Ethics considerations\n\nWhat types of tasks is R best suited for? What types of tasks is SQL best suited for? Does it matter which one you use if they can both accomplish the same goal?\nWhen setting up a database in SQL why is it important to consider potential users of the database and their potential questions / queries? That is, how is the set-up of the database related to the use of the database?",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "16-db-etc.html#footnotes",
    "href": "16-db-etc.html#footnotes",
    "title": "8  SQL extras",
    "section": "",
    "text": "Analogy taken from: https://www.quora.com/profile/Lara-Mazilu↩︎\nTaken from: https://dev.mysql.com/doc/refman/8.0/en/show-index.html↩︎\nExample taken from: https://sebhastian.com/mysql-median/↩︎",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**SQL** extras</span>"
    ]
  },
  {
    "objectID": "15-change-db.html",
    "href": "15-change-db.html",
    "title": "7  Changing databases",
    "section": "",
    "text": "7.1 Changing data\nThe UPDATE function allows you to change a value in a table across all rows that match a certain criteria. The impressions table has a name column indicating the person being impersonated. Let’s say, for whatever reason, that Ivanka Trump decides she doesn’t want to be affiliated with the Trump name and she changes her name to her husband’s name, becoming Ivanka Kushner. You might want to UPDATE the file to indicate the impressions were of Ivanka Kushner instead of Ivanka Trump. (See ?sec-load-duckdb for loading csv files into DuckDB directly.)\nduckdb_read_csv(con = con_duckdb, name = \"impressions\", files = \"data/impressions.csv\")\nSELECT * FROM impressions \n   WHERE name LIKE 'Ivanka%';\nTable 7.1: Finding the impersonations of Ivanka Trump.\n\n\n\n\n\nimpid\naid\nname\n\n\n\n2598\nScarlett Johansson\nIvanka Trump\n\n\n3716\nEmily Blunt\nIvanka Trump\n\n\n3694\nMargot Robbie\nIvanka Trump\n\n\n3679\nVanessa Bayer\nIvanka Trump\n\n\n2340\nMaya Rudolph\nIvanka Trump\nWe can use the UPDATE function to change the value of Ivanka’s name to Ivanka Kushner throughout the database. Note that all rows which match the WHERE clause get updated.\nUPDATE impressions\n   SET name = 'Ivanka Kushner'\n   WHERE name LIKE 'Ivanka%';\nSELECT * FROM impressions \n   WHERE name LIKE 'Ivanka%';\nTable 7.2: Ivanka’s last name has been UPDATEd to Kushner.\n\n\n\n\n\nimpid\naid\nname\n\n\n\n2598\nScarlett Johansson\nIvanka Kushner\n\n\n3716\nEmily Blunt\nIvanka Kushner\n\n\n3694\nMargot Robbie\nIvanka Kushner\n\n\n3679\nVanessa Bayer\nIvanka Kushner\n\n\n2340\nMaya Rudolph\nIvanka Kushner",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#changing-data",
    "href": "15-change-db.html#changing-data",
    "title": "7  Changing databases",
    "section": "",
    "text": "Watch out!\n\n\n\nBe careful with UPDATE. A careless UPDATE could write over all of the data in your table. There is no undo function.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#inserting-data",
    "href": "15-change-db.html#inserting-data",
    "title": "7  Changing databases",
    "section": "\n7.2 Inserting data",
    "text": "7.2 Inserting data\nLet’s say we want to include the more recent hosts in the hosts table. First, we scrape the SNL archives which lists the episode id (the date) and the host. The R package rvest allows us to pull out the appropriate html elements. The epid and aid are joined together in a tibble, and filtered to only include episodes which are not already in the episodes table. (See ?sec-load-duckdb for loading csv files into DuckDB directly.)\n\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\n\nBy searching the SNL archives, we can see that the next host, chronologically was Elon Musk on May 8, 2021.\n\nSELECT * FROM hosts\n    ORDER BY epid DESC\n    LIMIT 10;\n\n\n\n\nTable 7.3: Most recent hosts in the original hosts table.\n\n\n\n\n\nepid\naid\n\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n20201212\nTimothee Chalamet\n\n\n\n\n\n\n\n\n\nINSERT allows us to add the relevant information associated with the episode of SNL that Elon Musk hosted.\n\nINSERT INTO hosts (epid, aid)\n   VALUES ('20210508', 'Elon Musk');\n\n\nSELECT * FROM hosts\n    ORDER BY epid DESC\n    LIMIT 10;\n\n\n\n\nTable 7.4: hosts table including the added observation from May 8, 2021.\n\n\n\n\n\nepid\naid\n\n\n\n20210508\nElon Musk\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n\n\n\n\n\n\n\nIt would be tedious to INSERT all of the most recent host information by hand. Instead, we’ll scrape the SNL archives using the R package rvest, which allows us to pull out the appropriate html elements. The epid and aid are joined together in a tibble, and filtered to only include episodes which are not already in the episodes table.\n\nlibrary(rvest)\n\nrecent_hosts &lt;- read_html(\"http://www.snlarchives.net/Episodes/\") |&gt;\n  html_nodes(\"tr\") |&gt;\n  purrr::map_df( ~ tibble(\n    epid = .x |&gt; html_node(\"a.ms-2.me-2\") |&gt;\n      html_attr(\"href\") |&gt;\n      str_extract(\"\\\\d+\"),\n    aid = .x |&gt; html_node(\"td:nth-child(2)\") |&gt;\n      html_text2() |&gt;\n      str_extract(\"[\\\\w\\\\. \\\\w\\\\.]+(?=/|$)\")\n  )) |&gt;\n  filter(epid &gt; 20210508)\n\n\nwrite_csv(recent_hosts, \"data/recent_hosts.csv\")\n\n\nINSERT INTO hosts\n   SELECT *\n   FROM READ_CSV('data/recent_hosts.csv', AUTO_DETECT = TRUE);\n\n\nSELECT * FROM hosts\n  ORDER BY epid DESC\n  LIMIT 10;\n\n\n\n\nTable 7.5: The full hosts table, updated through December 16, 2023.\n\n\n\n\n\nepid\naid\n\n\n\n20240518\nJake Gyllenhaal\n\n\n20240511\nMaya Rudolph\n\n\n20240504\nDua Lipa\n\n\n20240413\nRyan Gosling\n\n\n20240406\nKristen Wiig\n\n\n20240330\nRamy Youssef\n\n\n20240309\nJosh Brolin\n\n\n20240302\nSydney Sweeney\n\n\n20240224\nShane Gillis\n\n\n20240203\nAyo Edebiri",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#deleting-data",
    "href": "15-change-db.html#deleting-data",
    "title": "7  Changing databases",
    "section": "\n7.3 Deleting data",
    "text": "7.3 Deleting data\nYou might change your mind and decide that you really only want hosts from years up to 2022. The DELETE function deletes any rows specified by the WHERE clause.\n\nDELETE FROM hosts\n   WHERE epid &gt; 20221231\n\n\nSELECT * FROM hosts\n  ORDER BY epid DESC\n  LIMIT 10;\n\n\n\n\nTable 7.6: The hosts table, after 2023 has been DELETEd.\n\n\n\n\n\nepid\naid\n\n\n\n20221217\nAustin Butler\n\n\n20221210\nMartin Short\n\n\n20221203\nKeke Palmer\n\n\n20221112\nDave Chappelle\n\n\n20221105\nAmy Schumer\n\n\n20221029\nJack Harlow\n\n\n20221015\nMegan Thee Stallion\n\n\n20221008\nBrendan Gleeson\n\n\n20221001\nMiles Teller\n\n\n20220521\nNatasha Lyonne",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#altering-the-table",
    "href": "15-change-db.html#altering-the-table",
    "title": "7  Changing databases",
    "section": "\n7.4 Altering the table",
    "text": "7.4 Altering the table\nALTER TABLE changes the structure of a table. For example, you can add or delete columns, create or destroy indexes, change the type of existing columns, or rename columns or the table itself. (Syntax below is for MySQL. Unfortunately, DuckDB is finicky when ALTERing tables, so the commands below may not work on the tables created using DuckDB.)1\nMultiple ADD, ALTER, DROP, and CHANGE clauses are permitted in a single ALTER TABLE statement, separated by commas.\n\nALTER TABLE t1\nDROP COLUMN col1,\nDROP COLUMN col2;\n\nTo alter a column to change both its name and definition, use CHANGE, specifying the old and new names and the new definition. For example, to rename an INT NOT NULL column from a to b and change its definition to use the BIGINT data type while retaining the NOT NULL attribute, do this:\n\nALTER TABLE t1 CHANGE a b BIGINT NOT NULL;\n\nTo change a column definition but not its name, use CHANGE or MODIFY. With CHANGE, the syntax requires two column names, so you must specify the same name twice to leave the name unchanged. For example, to change the definition of column b:\n\nALTER TABLE t1 CHANGE b b INT NOT NULL;\n\nMODIFY is more convenient to change the definition without changing the name because it requires the column name only once:\n\nALTER TABLE t1 MODIFY b INT NOT NULL;\n\nTo change a column name but not its definition, use CHANGE or RENAME COLUMN. With CHANGE, the syntax requires a column definition, so to leave the definition unchanged, you must re-specify the definition the column currently has. For example, to rename an INT NOT NULL column from b to a:\n\nALTER TABLE t1 CHANGE b a INT NOT NULL;\n\nRENAME COLUMN is more convenient to change the name without changing the definition because it requires only the old and new names:\n\nALTER TABLE t1 RENAME COLUMN b TO a;\n\nIn general, you cannot rename a column to a name that already exists in the table. However, this is sometimes not the case, such as when you swap names or move them through a cycle. If a table has columns named a, b, and c, the following are valid operations:\n\n/* swap a and b */\nALTER TABLE t1 RENAME COLUMN a TO b,\n               RENAME COLUMN b TO a;\n\n/* \"rotate\" a, b, c through a cycle */\nALTER TABLE t1 RENAME COLUMN a TO b,\n               RENAME COLUMN b TO c,\n               RENAME COLUMN c TO a;",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#temporary-tables",
    "href": "15-change-db.html#temporary-tables",
    "title": "7  Changing databases",
    "section": "\n7.5 Temporary tables",
    "text": "7.5 Temporary tables\nTemporary tables are used to break down complex queries into smaller, more manageable steps. For example, let’s say we want to JOIN two tables after each has been filtered using different WHERE clauses. The filtered tables can each be saved into their own temporary tables and then the temporary tables can be merged.\nNote that tables in DuckDB are saved (to disk), even when the connection is closed. However, temporary tables are saved in memory (instead of on disk) and are deleted when the connection is closed. Specific configuration of the temporary directory allows for temporary tables to be saved, even when the connection is closed.\nNotice that most of the tables have some kind of ID which allows JOINing across tables.\n\nSELECT * FROM hosts LIMIT 10;\n\n\n\n\nTable 7.7: Note the epid and aid identifiers in the hosts table.\n\n\n\n\n\nepid\naid\n\n\n\n20210410\nCarey Mulligan\n\n\n20210403\nDaniel Kaluuya\n\n\n20210327\nMaya Rudolph\n\n\n20210227\nNick Jonas\n\n\n20210220\nRege-Jean Page\n\n\n20210213\nRegina King\n\n\n20210206\nDan Levy\n\n\n20210130\nJohn Krasinski\n\n\n20201219\nKristen Wiig\n\n\n20201212\nTimothee Chalamet\n\n\n\n\n\n\n\n\n\n\nSELECT * FROM episodes LIMIT 10;\n\n\n\n\nTable 7.8: Note the sid and epid identifiers in the episodes table.\n\n\n\n\n\nsid\nepid\naired\nepno\n\n\n\n46\n20210410\nApril 10, 2021\n17\n\n\n46\n20210403\nApril 3, 2021\n16\n\n\n46\n20210327\nMarch 27, 2021\n15\n\n\n46\n20210227\nFebruary 27, 2021\n14\n\n\n46\n20210220\nFebruary 20, 2021\n13\n\n\n46\n20210213\nFebruary 13, 2021\n12\n\n\n46\n20210206\nFebruary 6, 2021\n11\n\n\n46\n20210130\nJanuary 30, 2021\n10\n\n\n46\n20201219\nDecember 19, 2020\n9\n\n\n46\n20201212\nDecember 12, 2020\n8\n\n\n\n\n\n\n\n\n\n\n7.5.1 Creating a temporary table\nThe episodes table has an aired column which includes the data. Recall that if we create a new variable (e.g., year) using aired, we cannot use year in the WHERE clause (WHERE only works on the original table, not the results set).\nIn MySQL the function STR_TO_DATE allowed us to create a datetime variable from which year could be extracted. However, in DuckDB, it is more complicated to convert the character string of “April 10, 2020” to “2020-04-10”. Don’t worry about the code too much, but note that we wouldn’t want to wrangle the character date string every time we wanted to filter for year.\nWhat does POSITION do?\nIn case you are curious about the date wrangling code… consider SUBSTRING(aired, POSITION(',' IN aired) + 2)\n\nPOSITION(',' IN aired): This part of the expression uses the POSITION function to find the position of the first occurrence of the comma (,) in the string aired. The result is the index (position) of the comma within the string.\nPOSITION(',' IN aired) + 2: This adds 2 to the index of the comma. The + 2 is used to move the starting point of the substring two positions to the right of the comma. This is done to exclude the comma itself and any following spaces.\nSUBSTRING(aired, POSITION(',' IN aired) + 2): This part uses the SUBSTRING function to extract a substring from the string aired. The starting position of the substring is determined by POSITION(',' IN aired) + 2, and it goes until the end of the string. This effectively removes the part of the string that comes before and including the first comma.\n\nIn summary, the entire expression is extracting a substring from the original string aired, starting from two positions to the right of the first comma and continuing until the end of the string. This can be useful in scenarios where you want to remove or isolate part of a string based on the position of a specific character (in this case, the comma).\n\nCREATE TEMP TABLE episodes_date AS\n    SELECT *, CASE\n             WHEN POSITION(',' IN aired) &gt; 0 THEN\n    EXTRACT(YEAR FROM CAST(\n                SUBSTRING(aired, POSITION(',' IN aired) + 2) || '-' ||\n                CASE\n                    WHEN POSITION('January' IN aired) &gt; 0 THEN '01'\n                    WHEN POSITION('February' IN aired) &gt; 0 THEN '02'\n                    WHEN POSITION('March' IN aired) &gt; 0 THEN '03'\n                    WHEN POSITION('April' IN aired) &gt; 0 THEN '04'\n                    WHEN POSITION('May' IN aired) &gt; 0 THEN '05'\n                    WHEN POSITION('June' IN aired) &gt; 0 THEN '06'\n                    WHEN POSITION('July' IN aired) &gt; 0 THEN '07'\n                    WHEN POSITION('August' IN aired) &gt; 0 THEN '08'\n                    WHEN POSITION('September' IN aired) &gt; 0 THEN '09'\n                    WHEN POSITION('October' IN aired) &gt; 0 THEN '10'\n                    WHEN POSITION('November' IN aired) &gt; 0 THEN '11'\n                    WHEN POSITION('December' IN aired) &gt; 0 THEN '12'\n                    ELSE '01' -- Default to January if no month is found\n                END || '-' ||\n                SUBSTRING(aired, POSITION(' ' IN aired) + 1, 2) AS DATE\n            ))\n            END AS year FROM episodes;\n\n\nSELECT * FROM episodes_date LIMIT 10;\n\n\n\n\nTable 7.9: The temporary table called episodes_date that has identifiers of sid, epid, and epno.\n\n\n\n\n\nsid\nepid\naired\nepno\nyear\n\n\n\n46\n20210410\nApril 10, 2021\n17\n2021\n\n\n46\n20210403\nApril 3, 2021\n16\n2021\n\n\n46\n20210327\nMarch 27, 2021\n15\n2021\n\n\n46\n20210227\nFebruary 27, 2021\n14\n2021\n\n\n46\n20210220\nFebruary 20, 2021\n13\n2021\n\n\n46\n20210213\nFebruary 13, 2021\n12\n2021\n\n\n46\n20210206\nFebruary 6, 2021\n11\n2021\n\n\n46\n20210130\nJanuary 30, 2021\n10\n2021\n\n\n46\n20201219\nDecember 19, 2020\n9\n2020\n\n\n46\n20201212\nDecember 12, 2020\n8\n2020\n\n\n\n\n\n\n\n\n\n\n7.5.2 Using a temporary table\nNow that the year variable has been created in the new temporary table called episodes_date, we can use episode_date to query and find, for example, all of the hosts in 2019.\n\nSELECT hosts.aid, ep.aired, ep.year FROM hosts \nJOIN episodes_date AS ep ON hosts.epid = ep.epid\nWHERE year = 2019\nLIMIT 25;\n\n\n\n\nTable 7.10: SNL hosts in 2019.\n\n\n\n\n\naid\naired\nyear\n\n\n\nEddie Murphy\nDecember 21, 2019\n2019\n\n\nScarlett Johansson\nDecember 14, 2019\n2019\n\n\nJennifer Lopez\nDecember 7, 2019\n2019\n\n\nWill Ferrell\nNovember 23, 2019\n2019\n\n\nHarry Styles\nNovember 16, 2019\n2019\n\n\nKristen Stewart\nNovember 2, 2019\n2019\n\n\nChance the Rapper\nOctober 26, 2019\n2019\n\n\nDavid Harbour\nOctober 12, 2019\n2019\n\n\nPhoebe Waller-Bridge\nOctober 5, 2019\n2019\n\n\nWoody Harrelson\nSeptember 28, 2019\n2019\n\n\nPaul Rudd\nMay 18, 2019\n2019\n\n\nEmma Thompson\nMay 11, 2019\n2019\n\n\nAdam Sandler\nMay 4, 2019\n2019\n\n\nEmma Stone\nApril 13, 2019\n2019\n\n\nKit Harington\nApril 6, 2019\n2019\n\n\nSandra Oh\nMarch 30, 2019\n2019\n\n\nIdris Elba\nMarch 9, 2019\n2019\n\n\nJohn Mulaney\nMarch 2, 2019\n2019\n\n\nDon Cheadle\nFebruary 16, 2019\n2019\n\n\nHalsey\nFebruary 9, 2019\n2019\n\n\nJames McAvoy\nJanuary 26, 2019\n2019\n\n\nRachel Brosnahan\nJanuary 19, 2019\n2019",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#best-practice",
    "href": "15-change-db.html#best-practice",
    "title": "7  Changing databases",
    "section": "\n7.6 Best practice",
    "text": "7.6 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#reflection-questions",
    "href": "15-change-db.html#reflection-questions",
    "title": "7  Changing databases",
    "section": "\n7.7  Reflection questions",
    "text": "7.7  Reflection questions\n\nHow can you update the value of a particular variable? What if you want to update a variable for many rows?\nWhy/when would you use CHANGE instead of RENAME COLUMN? Why/when would you use CHANGE instead of MODIFY?\nWhen are temporary tables useful? Can you always create temporary tables if you are working in SQL? Explain the hierarchy of tables, temporary tables, and subqueries.",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#ethics-considerations",
    "href": "15-change-db.html#ethics-considerations",
    "title": "7  Changing databases",
    "section": "\n7.8  Ethics considerations",
    "text": "7.8  Ethics considerations\n\nWho should have the ability / access to insert, delete, or update tables? Should everyone who accesses a table also have the ability to edit the table? Why or why not?\nWhat can you do if you accidentally DELETE the wrong rows or DROP the wrong the columns?",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "15-change-db.html#footnotes",
    "href": "15-change-db.html#footnotes",
    "title": "7  Changing databases",
    "section": "",
    "text": "Information and examples in this section taken from https://dev.mysql.com/doc/refman/8.0/en/alter-table.html#alter-table-add-drop-column↩︎",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Changing databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html",
    "href": "14-creating-db.html",
    "title": "6  Creating databases",
    "section": "",
    "text": "6.1 Preparing to load data\nThe duckdb database is currently empty, so we need to load in some data. The duckdb_read_csv() function in the duckdb R package allows us to load the .csv file (available on GitHub) directly into the database without being loaded as an R object first. The function, duckdb_read_csv() does some of the work for us to find data types. However, we will first learn what data types are and how to use them, and most dialects of SQL require you to specify the data types before loading in data (usually done using LOAD DATA but using COPY in Duckdb).\nRecall that in ?tbl-select-describe we used DESCRIBE to display the variable types of the database table(s). The list includes the variable name (Field), its Type, whether there are NULL values allowed, and whether there are keys or indexes defined on the variable. See Table 6.2 for the DESCRIBE output on the table we are about to import.\nUnlike R, when creating a new data table, SQL requires that you communicate each future variable (column) and that variable’s type. Variable types are not automatically generated!\nAs an example, consider the Saturday Night Live datasets available on the snldb GitHub repo. Data is scraped from http://www.snlarchives.net and http://www.imdb.com/title/tt0072562 by Hendrik Hilleckes and Colin Morris. Notice that there are eleven .csv files available in the output folder.\nSpecifically, let’s consider the casts.csv file.\nBefore we get into loading data into a SQL database, let’s look at the casts file in R, so that we understand the data we want to load. glimpse() provides the variable names and the variables types. The variables types are a mix of character strings, numeric, and logical. Variable types are very important for inputting data into a SQL server.\ncasts &lt;- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\nglimpse(casts)\n\nRows: 614\nColumns: 8\n$ aid             &lt;chr&gt; \"A. Whitney Brown\", \"A. Whitney Brown\", \"A. Whitney Br…\n$ sid             &lt;dbl&gt; 11, 12, 13, 14, 15, 16, 5, 39, 40, 41, 42, 45, 46, 21,…\n$ featured        &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, …\n$ first_epid      &lt;dbl&gt; 19860222, NA, NA, NA, NA, NA, 19800409, 20140118, NA, …\n$ last_epid       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ update_anchor   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ n_episodes      &lt;dbl&gt; 8, 20, 13, 20, 20, 20, 5, 11, 21, 21, 21, 18, 17, 20, …\n$ season_fraction &lt;dbl&gt; 0.444, 1.000, 1.000, 1.000, 1.000, 1.000, 0.250, 0.524…",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html#preparing-to-load-data",
    "href": "14-creating-db.html#preparing-to-load-data",
    "title": "6  Creating databases",
    "section": "",
    "text": "6.1.1 Variable types\nThe glance() function indicates that there are different variables types, here chr (character string), dbl (numerical value with double precision), and lgl (logical value). What are the data types in SQL?1\nNumbers\n\nexact numeric data types (INTEGER, SMALLINT, DECIMAL, and NUMERIC)2\n\napproximate numeric data types (FLOAT, REAL, and DOUBLE PRECISION)3\n\ninteger types INTEGER (or INT) and SMALLINT, TINYINT, MEDIUMINT, and BIGINT\n\n\nDECIMAL(precision, scale). For example, in DECIMAL(5,2) 5 is the precision and 2 is the scale. The precision represents the number of significant digits that are stored for values, and the scale represents the number of digits that can be stored following the decimal point. DECIMAL(5,2) must store any value with five digits and two decimals, so values that can be stored in the salary column range from -999.99 to 999.99. DECIMAL(M) is equivalent to DECIMAL(M,0). Similarly, the syntax DECIMAL is equivalent to DECIMAL(M,0), where the default value of M is 10. If the scale is 0, DECIMAL values contain no decimal point or fractional part.\nStrings\n\nstring data types are CHAR, VARCHAR, BINARY, VARBINARY, BLOB, TEXT, ENUM, and SET\n\n\nCHAR and VARCHAR types are similar, but differ in the way they are stored and retrieved. They also differ in maximum length and in whether trailing spaces are retained.\n\nCHAR and VARCHAR types are declared with a length that indicates the maximum number of characters you want to store. For example, CHAR(30) can hold up to 30 characters.\nThe length of a CHAR column is fixed to the length that you declare when you create the table.\nValues in VARCHAR columns are variable-length strings. The length can be specified as a value from 0 to 65,535.\n\nBINARY and VARBINARY types are similar to CHAR and VARCHAR, except that they store binary strings rather than nonbinary strings. That is, they store byte strings rather than character strings.\nA BLOB is a binary large object that can hold a variable amount of data. The four BLOB types are TINYBLOB, BLOB, MEDIUMBLOB, and LONGBLOB. These differ only in the maximum length of the values they can hold. The four TEXT types are TINYTEXT, TEXT, MEDIUMTEXT, and LONGTEXT. These correspond to the four BLOB types and have the same maximum lengths and storage requirements.\nDate\n\ndate and time data types for representing temporal values are DATE, TIME, DATETIME, TIMESTAMP, and YEAR\n\n\nDATE type is used for values with a date part but no time part. MySQL retrieves and displays DATE values in ‘YYYY-MM-DD’ format. The supported range is ‘1000-01-01’ to ‘9999-12-31’.\n\nDATETIME type is used for values that contain both date and time parts. MySQL retrieves and displays DATETIME values in ‘YYYY-MM-DD hh:mm:ss’ format. The supported range is ‘1000-01-01 00:00:00’ to ‘9999-12-31 23:59:59’.\n\nTIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of ‘1970-01-01 00:00:01’ UTC to ‘2038-01-19 03:14:07’ UTC.\n\nTIME values in ‘hh:mm:ss’ format (or ‘hhh:mm:ss’ format for large hours values). TIME values may range from ‘-838:59:59’ to ‘838:59:59’. The hours part may be so large because the TIME type can be used not only to represent a time of day (which must be less than 24 hours), but also elapsed time or a time interval between two events (which may be much greater than 24 hours, or even negative).\n\nYEAR type is a 1-byte type used to represent year values with a display width of four characters.\n\n6.1.2 CHECK constraints\nWhile implementing CREATE TABLE, constraints can be added either to individual variables (CountryPopulation &gt; 0) or to the table as a whole (LastCensus &lt; NextCensus).4\nIf an attempt is made to load data that violate the CHECK constraints, an error will be given.\n\nCREATE TABLE CountryListCensus (\n    Id INT,\n    CountryName VARCHAR(255) NOT NULL,\n    CountryPopulation INT CHECK(CountryPopulation &gt; 0),\n    LastCensus DATE,\n    NextCensus DATE,\n    CHECK(LastCensus&lt;NextCensus),\n    PRIMARY KEY (Id)\n);\n\n\n6.1.3 Creating KEYs\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY col1,\n  FOREIGN KEY col2 REFERENCES table2(table2col1)\n);\n\nEither or both of the KEYs could be multiple columns.\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY (col1, col3),\n  FOREIGN KEY (col1, col2) REFERENCES table2(table2col1, table2col4)\n);\n\n\n6.1.4 Creating INDEXes\nIndexes can be created on one or more variable. A table does not need to have an INDEX (or a KEY).\n\nCREATE INDEX name_of_index ON table (col1);\n\n\nCREATE INDEX name_of_index ON table (col1, col2);\n\n\n6.1.5 Loading data\nOnce the database is set up, you will be ready to import .csv files into the database as tables. Importing .csv files as tables requires a series of steps:5\n\na USE statement that ensures we are in the right schema/database.\na series of DROP TABLE statements that drop any old tables with the same names as the ones we are going to create.\na series of CREATE TABLE statements that specify the table structures.\na series of COPY statements that read the data from the .csv files into the appropriate tables.\n\n\n\n\n\n\n\n Watch out!\n\n\n\nDuckDB has its own dialect of SQL. To load data into a MySQL server, the final statement would be LOAD DATA instead of COPY. See MDSR for more information on loading data into a remote MySQL server.\n\n\nLoading step 1\nUse the local database that we’ve called duck_datab.\n\nUSE duck_datab;\n\nLoading step 2\nMake sure to “refresh” the table, in case it already exists. However, be very careful with the DROP TABLE statement, as it will remove the casts table.\n\nDROP TABLE IF EXISTS casts;\n\nLoading step 3\nCarefully define the variable types, whether or not they allow missing values, and what a default value is for that variable. Additionally, identify the key for accessing information.\nMySQL doesn’t actually have a BOOLEAN datatype (you would use TINYINT(1) instead). But DuckDB does have a BOOLEAN datatype!\n\nCREATE TABLE casts (\n  aid VARCHAR(255) NOT NULL DEFAULT ' ',\n  sid INTEGER NOT NULL DEFAULT 0,\n  featured BOOLEAN NOT NULL DEFAULT 'false',\n  first_epid INTEGER DEFAULT 0,\n  last_epid INTEGER DEFAULT 0,\n  update_anchor BOOLEAN NOT NULL DEFAULT 0,\n  n_episodes INTEGER NOT NULL DEFAULT 0,\n  season_fraction DECIMAL(21,20) NOT NULL DEFAULT 0,\n  PRIMARY KEY (sid, aid)\n);\n\nLoading step 4\nThe .csv file lives on my computer, so I load it in directly. Note that the statement to load in data is slightly different in MySQL.\n\nCOPY casts FROM 'data/casts.csv' HEADER;\n\nChecking the loading\n\nSELECT * FROM casts LIMIT 8;\n\n\n\n\nTable 6.1: After CREATE TABLE where variable types are set, the COPY command pulls the data into the table. SELECT shows us that the table is as expected.\n\n\n\n\n\naid\nsid\nfeatured\nfirst_epid\nlast_epid\nupdate_anchor\nn_episodes\nseason_fraction\n\n\n\nA. Whitney Brown\n11\nTRUE\n19860222\n\nFALSE\n8\n0.444\n\n\nA. Whitney Brown\n12\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n13\nTRUE\n\n\nFALSE\n13\n1.000\n\n\nA. Whitney Brown\n14\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n15\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nA. Whitney Brown\n16\nTRUE\n\n\nFALSE\n20\n1.000\n\n\nAlan Zweibel\n5\nTRUE\n19800409\n\nFALSE\n5\n0.250\n\n\nSasheer Zamata\n39\nTRUE\n20140118\n\nFALSE\n11\n0.524\n\n\n\n\n\n\n\n\n\nCheck\nLet’s make sure that the database exists and that the table in the database exists.\n\nSHOW DATABASES;\n\n\n1 records\n\ndatabase_name\n\n\nduck_datab\n\n\n\n\n\nSHOW TABLES;\n\n\n1 records\n\nname\n\n\ncasts\n\n\n\n\n\nDESCRIBE casts;\n\n\n\n\nTable 6.2: DESCRIBE variables in the casts table.\n\n\n\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\naid\nVARCHAR\nNO\nPRI\n' '\n\n\n\nsid\nINTEGER\nNO\nPRI\n0\n\n\n\nfeatured\nBOOLEAN\nNO\n\n'false'\n\n\n\nfirst_epid\nINTEGER\nYES\n\n0\n\n\n\nlast_epid\nINTEGER\nYES\n\n0\n\n\n\nupdate_anchor\nBOOLEAN\nNO\n\n0\n\n\n\nn_episodes\nINTEGER\nNO\n\n0\n\n\n\nseason_fraction\nDECIMAL(21,20)\nNO\n\n0\n\n\n\n\n\n\n\n\n\n\n\n6.1.6 Using DuckDB for loading data\nThe steps given in Section 6.1.5 are general to many SQL dialects and are important when working with most SQL clients. It is important to have control over the variables configurations as they make up the SQL database. However, using the duckdb package in R allows for shorthand entry of data from .csv files into the DuckDB database. Here, we take advantage of working with the DuckDB functionality in R.\n\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"episodes\", files = \"data/episodes.csv\")\n\nChecking the loading\n\nSHOW TABLES;\n\n\n3 records\n\nname\n\n\n\ncasts\n\n\nepisodes\n\n\nhosts",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html#best-practice",
    "href": "14-creating-db.html#best-practice",
    "title": "6  Creating databases",
    "section": "\n6.2 Best practice",
    "text": "6.2 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html#reflection-questions",
    "href": "14-creating-db.html#reflection-questions",
    "title": "6  Creating databases",
    "section": "\n6.3  Reflection questions",
    "text": "6.3  Reflection questions\n\nWhat is the difference between R and SQL in terms of communicating the different data types?\nWhy does it matter if the variable type is specified correctly? For example, why would it be better for a date column to be specified as DATETIME instead of VARCHAR?\nIf you are Googling some SQL syntax, why do you need to specify the dialect, for example, DuckDB or MySQL?\nWhy do we often include the DROP TABLE operation before the CREATE TABLE operation?",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html#ethics-considerations",
    "href": "14-creating-db.html#ethics-considerations",
    "title": "6  Creating databases",
    "section": "\n6.4  Ethics considerations",
    "text": "6.4  Ethics considerations\n\nWhen creating a database why should we worry about the provenance (origin) of the data?\nHow does a SQL database automatically hold extra information about the database (e.g., provenance)? Spoiler: it doesn’t. So what can we do?",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "14-creating-db.html#footnotes",
    "href": "14-creating-db.html#footnotes",
    "title": "6  Creating databases",
    "section": "",
    "text": "Taken from https://dev.mysql.com/doc/refman/8.0/en/data-types.html↩︎\nThe keyword INT is a synonym for INTEGER, and the keywords DEC and FIXED are synonyms for DECIMAL↩︎\n MySQL treats DOUBLE as a synonym for DOUBLE PRECISION. MySQL also treats REAL as a synonym for DOUBLE PRECISION.↩︎\nExample from https://www.sqlshack.com/how-to-use-sql-check-constraints/↩︎\ntaken from MDSR.↩︎",
    "crumbs": [
      "Building databases",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creating databases</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html",
    "href": "13-sql-joins.html",
    "title": "5  Combining tables in SQL",
    "section": "",
    "text": "5.1 Subqueries\nA SQL subquery is a query used as a data source in the FROM clause, instead of the usual table. There was a subquery in ?tbl-length-time2 when the task required a function of the results set within the SELECT clause.\nWe could do something similar if we wanted to transform the variables in the select column. The example is a little bit forced, and there are other ways to obtain the same results. But hopefully the idea of a subquery is becoming more clear. Again, a subquery is just a query that becomes the data source for FROM.\n?sec-reg-expr will cover regular expressions in some detail. Here we use the function REGEXP_REPLACE to remove any characters which are not letters, comma, or space. The function LOWER converts any upper case letters to lower case.\nSELECT name,\n       name_clean,\n       SUBSTRING_INDEX(name_clean, ',', 1) AS last_name,\n       SUBSTRING_INDEX(name_clean, ',', -1) AS first_name\nFROM (\nSELECT LOWER(REGEXP_REPLACE(name, '[^a-z,. ]', '')) AS name_clean,\n       name,\n       id, person_id\nFROM aka_name) AS temp_subquery\nLIMIT 0, 30;\nTable 5.3: A subquery is used so that the variable in the subquery can be used and transformed in the SELECT clause.\n\n\n\n\n\nname\nname_clean\nlast_name\nfirst_name\n\n\n\nSmith, Jessica Noel\nsmith, jessica noel\nsmith\njessica noel\n\n\nPain, L. $ham\npain, l. ham\npain\nl. ham\n\n\nBoy, $hutter\nboy, hutter\nboy\nhutter\n\n\nDollasign, Ty\ndollasign, ty\ndollasign\nty\n\n\nSign, Ty Dolla\nsign, ty dolla\nsign\nty dolla\n\n\nMoore, Brandon\nmoore, brandon\nmoore\nbrandon\n\n\n$torm, Country\ntorm, country\ntorm\ncountry\n\n\n'Hooper', Simon P.J. Kelly\nhooper, simon p.j. kelly\nhooper\nsimon p.j. kelly\n\n\nHooper\nhooper\nhooper\nhooper\n\n\nKelly, Simon P.J.\nkelly, simon p.j.\nkelly\nsimon p.j.\n\n\nAbdul-Hamid, Jaffar\nabdulhamid, jaffar\nabdulhamid\njaffar\n\n\nAl-Hamid, Jaffar Abd\nalhamid, jaffar abd\nalhamid\njaffar abd\n\n\nSvensson, Acke\nsvensson, acke\nsvensson\nacke\n\n\nViera, Michael 'Power'\nviera, michael power\nviera\nmichael power\n\n\nBuguelo\nbuguelo\nbuguelo\nbuguelo\n\n\n'El Burro' Rankin', Jorge Van\nel burro rankin, jorge van\nel burro rankin\njorge van\n\n\nBurro, El\nburro, el\nburro\nel\n\n\nVan Rankin, Jorge 'Burro'\nvan rankin, jorge burro\nvan rankin\njorge burro\n\n\nVan Rankin, Jorge\nvan rankin, jorge\nvan rankin\njorge\n\n\nvan Rankin, Jorge 'El Burro'\nvan rankin, jorge el burro\nvan rankin\njorge el burro\n\n\nSeigal, Jason\nseigal, jason\nseigal\njason\n\n\nKaufman, Murray\nkaufman, murray\nkaufman\nmurray\n\n\n'Knoccout'Madison, Kareim\nknoccoutmadison, kareim\nknoccoutmadison\nkareim\n\n\nStarks, Johnny\nstarks, johnny\nstarks\njohnny\n\n\nKraemer, 'Logan' Howard\nkraemer, logan howard\nkraemer\nlogan howard\n\n\nGee, Emm\ngee, emm\ngee\nemm\n\n\nCusick, Maura\ncusick, maura\ncusick\nmaura\n\n\nMaura, Maude Cusick\nmaura, maude cusick\nmaura\nmaude cusick\n\n\nWheeler, Mackenzie\nwheeler, mackenzie\nwheeler\nmackenzie\n\n\nMonkey\nmonkey\nmonkey\nmonkey",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html#all-the-joins",
    "href": "13-sql-joins.html#all-the-joins",
    "title": "5  Combining tables in SQL",
    "section": "\n5.2 All the JOINs",
    "text": "5.2 All the JOINs\nRecall that SQL is a query language that works on relational databases. One of its major strengths is being able to efficiently store information in separate tables that can be easily connected as needed. The syntax for tying together information from multiple tables is done with a JOIN clause.\nEach JOIN clause needs four specific pieces of information:\n\nThe name of the first table you want to JOIN.\nThe type of JOIN being used.\nThe name of the second table you want to JOIN.\nThe condition(s) under which you want the records in the first table to match records in the second table.\n\nSome types of JOINs available in MySQL include the following, which are represented as Venn diagrams in Figure 5.1.\n\n\nJOIN: include all of the rows that exist in both tables (similar to inner_join() in R, the intersection of the two tables). INNER JOIN is alternative, and identical, function to JOIN.\n\nLEFT JOIN: include all of the rows in the first table. Connect them, as much as possible, to the rows in the second table. Rows that have no match in the second table will have a value of NULL for the new “second table” variables.\n\nRIGHT JOIN: include all of the rows in the second table. Connect them, as much as possible, to the rows in the first table. Rows that have no match in the first table will have a value of NULL for the new “first table” variables. A RIGHT JOIN with the tables in the opposite order is the same as a LEFT JOIN with the tables in the original order.\n\nFULL OUTER JOIN: include all rows in either table. Rows that have no match in the other table will have a value of NULL for the other table variables. (similar to full_join() in R, the union of the two tables). The functionality doesn’t exist in MySQL but can be created using joins and UNION.\n\nCROSS JOIN: match each row of the first table with each row in the second table.\n\nFigure 5.1 shows Venn diagrams of the different types of joins. Figure 5.2 shows four of the JOIN functions with mini data tables. Note that in SQL the missing values will be labeled as NULL (not NA).\n\n\n\n\n\n\n\nFigure 5.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\n\n\n\n\n\n5.2.1 A toy example\nWe will head to R for just a minute so as to understand joins using a small toy dataset on rock bands from the 60s, The Beatles and The Rolling Stones. The function sqldf() in the sqldf R package allows for SQL commands on R objects.\nConsider the following datasets which are available in the dplyr package.\n\nband_members\n\n# A tibble: 3 × 2\n  name  band   \n  &lt;chr&gt; &lt;chr&gt;  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nband_instruments\n\n# A tibble: 3 × 2\n  name  plays \n  &lt;chr&gt; &lt;chr&gt; \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\n\nInner join\nAn inner join combines two datasets returning only the observations that exist in both of the original datasets.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                     inst.plays\n              FROM band_members AS star\n              JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 John Beatles guitar\n2 Paul Beatles   bass\n\n\nFull join\nA full join combines two datasets returning every observation that exists in either one of the original datasets. Note that in the results, Mick’s instrument is missing, and Keith’s band is missing.\nThe full_join() function does not have an equivalent in MySQL. See Section 5.3.1.1 for using JOINs and UNIONs to produce a full join.\n\nband_members |&gt;\n  full_join(band_instruments)\n\n# A tibble: 4 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith &lt;NA&gt;    guitar\n\n\nLeft join\nA left join combines two datasets returning every observation that exists in the left (or first) original dataset. Note that in the results, Mick’s instrument is missing.\n\nsqldf::sqldf(\"SELECT star.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              LEFT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\n\nRight join\nA right join combines two datasets returning every observation that exists in the right (or second) original dataset. Note that in the results, Keith’s band is missing.\n\nsqldf::sqldf(\"SELECT inst.name,\n                     star.band,\n                      inst.plays\n              FROM band_members AS star\n              RIGHT JOIN band_instruments AS inst \n              ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\n\n\n5.2.2 JOIN\n\nIn the imdb database, the title table includes information about the 4,626,322 titles in the database, including the id, title, kind_id (indicator for the kind of ID it is), and production_year. It does not, however, include the review of the title. See Table 5.4.\n\nSELECT * FROM title LIMIT 0, 10;\n\n\n\n\nTable 5.4: SELECT to glance at the title table in the imdb database.\n\n\n\n\n\nid\ntitle\nimdb_index\nkind_id\nproduction_year\nimdb_id\nphonetic_code\nepisode_of_id\nseason_nr\nepisode_nr\nseries_years\nmd5sum\n\n\n\n78460\nAdults Recat to the Simpsons (30th Anniversary)\n\n7\n2017\n\nA3432\n78406\n\n\n\n2ae09eed7d576cc2c24774fed5b18168\n\n\n70273\n(2016-05-18)\n\n7\n2016\n\n\n68058\n\n\n\n511dfc14cfff7589d29a95abb30cd66a\n\n\n60105\n(2014-04-11)\n\n7\n2014\n\n\n59138\n\n\n\nc6cdce7e667e07713e431805c407feed\n\n\n32120\n(2008-05-01)\n\n7\n2008\n\n\n32060\n\n\n\n100df65742caf5afd092b2e0ead67d8e\n\n\n97554\nSchmÃ¶lders Traum\n\n7\n2001\n\nS2543\n97302\n10\n1\n\n46862a2f96f9fb2d59e8c9a11ecfdd28\n\n\n57966\n(#1.1)\n\n7\n2013\n\n\n57965\n1\n1\n\n409c37703766c4b24f8a86162fd9cf85\n\n\n76391\nAnniversary\n\n7\n1971\n\nA5162\n76385\n4\n9\n\n5e12ce73fac1d1dcf94136b6e9acd8f8\n\n\n11952\nAngus Black/Lester Barrie/DC Curry\n\n7\n2009\n\nA5214\n11937\n4\n7\n\n9c38b9e5601dc154444b73b518034aa1\n\n\n1554\nNew Orleans\n\n7\n2003\n\nN6452\n1508\n2\n11\n\n621bea735740a547e862e4a3226f35d2\n\n\n58442\nKiss Me Kate\n\n7\n2011\n\nK2523\n58436\n1\n10\n\n293e8c75c7f35a4035abf617962be5a9\n\n\n\n\n\n\n\n\n\nThe movie_info_idx table does not contain much information about each particular film. It does, however, have an indicator for the movie ID (given by movie_id) as well as the number of votes (given by info where type_id = 100). See Table 5.5.\n\nSELECT * FROM movie_info_idx LIMIT 0, 6;\n\n\n\n\nTable 5.5: SELECT to glance at the movie_info_idx table in the imdb database.\n\n\n\n\n\nid\nmovie_id\ninfo_type_id\ninfo\nnote\n\n\n\n1\n1\n99\n31.2.1..2.\n\n\n\n2\n1\n100\n9\n\n\n\n3\n1\n101\n4.1\n\n\n\n4\n2\n99\n1000000102\n\n\n\n5\n2\n100\n61\n\n\n\n6\n2\n101\n6.4\n\n\n\n\n\n\n\n\n\n\nLet’s say we want to combine the titles with the number of votes so that each title with user votes is included. That is, only keep the titles that have a corresponding votes. And also, only keep the votes if there is an associated title (which means we use INNER JOIN or just plain JOIN).\nRemember that WHERE will work on the raw variables, and HAVING works on the results set.\nSome aspects of the query are worth pointing out:\n* The variables in the output are given in the SELECT clause. The id and title (both from the title table) and the info from the movie_info_idx which represents the number of IMDb votes. * The variables are preceded by the table from which they came. While not always necessary, it is good practice so as to avoid confusion. * The JOIN happens by linking the id variable in the title table with the movie_id variable in the movie_info_idx table. * The LIMIT wasn’t necessary (there are only 12 observations), but it’s good practice so that we don’t end up with unwieldy query results. * The WHERE clause happens before the JOIN action, despite being written after. * In the WHERE clause, we keep only movies, only 2015 production year, and only at least 150,000 votes.\n\nSELECT title.id,\n       title.title,\n       movie_info_idx.info\nFROM title\nJOIN movie_info_idx ON title.id = movie_info_idx.movie_id \nWHERE title.production_year = 2015 \n    AND title.kind_id = 1                  # movies only\n    AND movie_info_idx.info_type_id = 100  # info_type is votes\n    AND movie_info_idx.info &gt; 150000       # at least 150,000 votes\nORDER BY movie_info_idx.info DESC\nLIMIT 0, 20;\n\n\n\n\nTable 5.6: Movies from 2015 that have at least 150,000 votes in the imdb database.\n\n\n\n\n\nid\ntitle\ninfo\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n\n\n3915213\nMad Max: Fury Road\n666484\n\n\n4389619\nThe Martian\n583987\n\n\n3313672\nAvengers: Age of Ultron\n540606\n\n\n4414139\nThe Revenant\n526189\n\n\n3787790\nJurassic World\n471237\n\n\n3752999\nInside Out\n443051\n\n\n3292159\nAnt-Man\n390965\n\n\n4364483\nThe Hateful Eight\n363199\n\n\n4251736\nSpectre\n319875\n\n\n3630368\nFurious Seven\n310970\n\n\n4255450\nSpotlight\n290362\n\n\n3961438\nMission: Impossible - Rogue Nation\n266759\n\n\n4321769\nThe Big Short\n262598\n\n\n4221220\nSicario\n260996\n\n\n3600120\nFifty Shades of Grey\n250962\n\n\n4164324\nRoom\n244210\n\n\n3379559\nBridge of Spies\n229440\n\n\n4368666\nThe Hunger Games: Mockingjay - Part 2\n214569\n\n\n4387967\nThe Man from U.N.C.L.E.\n213754\n\n\n\n\n\n\n\n\n\nLet’s say we also want to obtain information about the actors and actresses in each of the movies. In the cast_info table, there is a person_id, a movie_id, and person_role_id is 1 if actor and 2 if actress.\n\nSELECT * FROM cast_info LIMIT 0, 10;\n\n\n\n\nTable 5.7: SELECT to glance at the cast_info table in the imdb database.\n\n\n\n\n\nid\nperson_id\nmovie_id\nperson_role_id\nnote\nnr_order\nrole_id\n\n\n\n1\n1\n3432997\n1\n\n31\n1\n\n\n2\n2\n1901690\n2\n\n\n1\n\n\n3\n3\n4027567\n2\n\n25\n1\n\n\n4\n3\n4282876\n3\n\n22\n1\n\n\n5\n4\n3542672\n\n\n12\n1\n\n\n6\n5\n3331520\n4\n(as $hutter Boy)\n10\n1\n\n\n7\n5\n4027191\n2\n(as $hutter Boy)\n1\n1\n\n\n8\n5\n4195731\n5\n(uncredited)\n\n1\n\n\n9\n5\n4263956\n6\n(uncredited)\n\n1\n\n\n10\n5\n4267787\n7\n(uncredited)\n\n1\n\n\n\n\n\n\n\n\n\nWe also want the name of the actress which is in the table aka_name. Note that there is no movie information in the aka_name table!\n\nSELECT * FROM aka_name LIMIT 0, 10;\n\n\n\n\nTable 5.8: SELECT to glance at the aka_name table in the imdb database.\n\n\n\n\n\nid\nperson_id\nname\nimdb_index\nname_pcode_cf\nname_pcode_nf\nsurname_pcode\nmd5sum\n\n\n\n1\n6188450\nSmith, Jessica Noel\n\nS5325\nJ2542\nS53\n25c9d464e3ff2957533546aa92b397ed\n\n\n2\n5125059\nPain, L. $ham\n\nP545\nL515\nP5\n569b1e885ccb51211c01753f0dad9b2c\n\n\n3\n5\nBoy, $hutter\n\nB36\nH361\nB\n35092b5604ce378fc48c8a6fc0038a49\n\n\n4\n4152053\nDollasign, Ty\n\nD4253\nT3425\nD425\n0f565a2d8027cfb8ed6c5f4bba719fcd\n\n\n5\n4152053\nSign, Ty Dolla\n\nS2534\nT3425\nS25\n2eded1b021b96333b4b74e0fec959650\n\n\n6\n6\nMoore, Brandon\n\nM6165\nB6535\nM6\n193a6f5adf4756320f622162d2475608\n\n\n7\n8\n$torm, Country\n\nT6525\nC5363\nT65\n1654400b707d34323ea392b87060e6cc\n\n\n8\n19\n'Hooper', Simon P.J. Kelly\n\nH1625\nS5124\nH16\n3fd8885372c23f8c74e583da91d1fd05\n\n\n9\n19\nHooper\n\nH16\n\n\n24ddc68ab605ee95857ad45b65ffa2d8\n\n\n10\n19\nKelly, Simon P.J.\n\nK4251\nS5124\nK4\n33d976f22e276b73c61513bc5f6e72a6\n\n\n\n\n\n\n\n\n\nConnecting the most popular movies of 2015 with the actresses in those movies requires a series of JOINs. Note that to make the code less onerous, the title table has been aliased by t, the movie_info_idx table has been aliased by idx, the cast_info table has been aliased by a, and the aka_name table has been aliased by n.\nThere is a lot of data cleaning to do as some of the person_id values are one to many!! That is, the person_id matches multiple names in the aka_name database.\n\nSELECT t.title,\n       idx.info,\n       a.person_id,\n       n.name\nFROM title AS t\nJOIN movie_info_idx AS idx ON t.id = idx.movie_id \nJOIN cast_info AS a ON idx.movie_id = a.movie_id\nJOIN aka_name AS n ON a.person_id = n.person_id\nWHERE t.production_year = 2015 \n    AND t.kind_id = 1           # movies only\n    AND idx.info_type_id = 100  # info_type is votes\n    AND idx.info &gt; 150000       # at least 150,000 votes\n    AND a.role_id = 2           # actresses only\nORDER BY idx.info DESC\nLIMIT 0, 50;\n\n\n\n\nTable 5.9: Movies from 2015 that have at least 150,000 votes in the imdb database with the actress name joined.\n\n\n\n\n\ntitle\ninfo\nperson_id\nname\n\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2698188\nSam\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2806101\nGillespie, Hilary Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n2959609\nCuzner, Natalie\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3089483\nFisher, Carrie Frances\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nClass, Clare\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3150880\nGlass, Claire\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3231758\nHenwick, Jessica Yu Li\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3265686\nHui, Karen\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3305561\nKamen, Hannah John\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nBilly\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3462940\nLourd, Billie Catherine\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3569409\nFran\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyongo, Lupita\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3649948\nNyong'o, Lupita Amondi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3785240\nRidley, Daisy Jazz Isobel\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nGiagrande, Meredith J.\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalinger, Meredith\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3835377\nSalenger, Meredith Dawn\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3850834\nPhi\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3875581\nFox, Claudia\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3879039\nArti\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSlade, Sandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3907812\nSandy Slade\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3938795\nRyan, Karol Lesley\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nStevens, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTaber, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nTabor, Cat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n3970637\nCat\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Dame Harriet\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4073883\nWalter, Harriet Mary\n\n\nStar Wars: Episode VII - The Force Awakens\n691691\n4094732\nWhite, Kelsey Marie\n\n\nMad Max: Fury Road\n666484\n2681098\nMichelle, Debra\n\n\nMad Max: Fury Road\n666484\n2782138\nAli\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helena\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, Helene\n\n\nMad Max: Fury Road\n666484\n2873752\nCardona, HÃ©lÃ¨ne Vania\n\n\nMad Max: Fury Road\n666484\n2957052\nCunico, Lillie\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3087531\nFinlay, Sandi 'Hotrod'\n\n\nMad Max: Fury Road\n666484\n3146859\nGilles, Coco Jack\n\n\nMad Max: Fury Road\n666484\n3146859\nGillies, Coco\n\n\nMad Max: Fury Road\n666484\n3268456\nRose\n\n\nMad Max: Fury Road\n666484\n3268456\nHuntington-Whiteley, Rosie Alice\n\n\nMad Max: Fury Road\n666484\n3343489\nKellerman, Antoinette\n\n\nMad Max: Fury Road\n666484\n3348513\nRiley\n\n\n\n\n\n\n\n\n\n\n5.2.3 Other JOINs\nConsider the following two tables. The first has seven movies in it (from 2015 with at least 400,000 IMDb votes). The second consists of almost 3 million actresses (person_role_id = 2). In order to find a subset of actresses, the person_id &gt; 3900000 was set arbitrarily (in order to have a smaller group with which to work).\nmovies:\n\nSELECT t.id,\n       t.title,\n       idx.info,\n       (SELECT COUNT(*)\n       FROM title AS t\n       JOIN movie_info_idx AS idx ON idx.movie_id = t.id\n       WHERE t.production_year = 2015  \n             AND t.kind_id = 1\n             AND idx.info_type_id = 100\n             AND idx.info &gt; 400000) AS row_count\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1             # movies only\n    AND idx.info_type_id = 100    # info_type is votes\n    AND idx.info &gt; 400000         # at least 400,000 votes\nORDER BY idx.info DESC\n\n\n\n\nTable 5.10: Movies from 2015 that have at least 400,000 votes in the imdb database.\n\n\n\n\n\nid\ntitle\ninfo\nrow_count\n\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n691691\n7\n\n\n3915213\nMad Max: Fury Road\n666484\n7\n\n\n4389619\nThe Martian\n583987\n7\n\n\n3313672\nAvengers: Age of Ultron\n540606\n7\n\n\n4414139\nThe Revenant\n526189\n7\n\n\n3787790\nJurassic World\n471237\n7\n\n\n3752999\nInside Out\n443051\n7\n\n\n\n\n\n\n\n\n\nactresses:\n\nSELECT a.person_id,\n       a.movie_id,\n       n.name,\n       (SELECT COUNT(*)\n       FROM cast_info AS a\n       JOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 390000) AS row_count\nFROM cast_info AS a\nJOIN aka_name AS n ON a.person_id = n.person_id\n       WHERE a.person_role_id = 2  \n             AND a.person_id &gt; 3900000\nLIMIT 0, 20;\n\n\n\n\nTable 5.11: Actresses whose person_id is greater than 400000. Note that some actresses have different spelling or phrasing of their names.\n\n\n\n\n\nperson_id\nmovie_id\nname\nrow_count\n\n\n\n3900141\n759802\nSimons, Rita Joanne\n2904759\n\n\n3902258\n4365829\nSinger, Rabbi Tovia\n2904759\n\n\n3902699\n3109788\nSingh, Sabine Erika\n2904759\n\n\n3903035\n3215866\nVal\n2904759\n\n\n3904831\n2468067\nMasha\n2904759\n\n\n3904928\n3654347\nFei, Siu Yin\n2904759\n\n\n3904928\n3654347\nHsiao, Yen-fei\n2904759\n\n\n3904928\n3654347\nSiu, Yinfei\n2904759\n\n\n3904928\n3654347\nXiao, Yanfei\n2904759\n\n\n3904928\n3654347\nYin-Fai, Siu\n2904759\n\n\n3905289\n115191\nCoso, Cosondra\n2904759\n\n\n3905289\n115191\nSjostrom, Cossondra\n2904759\n\n\n3905289\n115191\nCoso\n2904759\n\n\n3909355\n2939100\nSlovÃ¡ckovÃ¡, Anna Julie\n2904759\n\n\n3911826\n4379610\nMeador, Constance June\n2904759\n\n\n3912134\n2675144\nDJ\n2904759\n\n\n3912134\n2675144\nSmith, DJ\n2904759\n\n\n3912134\n2675144\nSmith, Dujonette\n2904759\n\n\n3912134\n2675144\nDJ Smith\n2904759\n\n\n3913519\n1678444\nKeely, Dorothy Jacqueline\n2904759\n\n\n\n\n\n\n\n\n\nUsing subqueries, we can JOIN the two datasets using different JOIN techniques.\nInner JOIN\n\nWith an inner JOIN, there are 32 rows corresponding to all the actresses in the seven 2015 films with the most votes. Because the JOIN is an intersection of the two tables, only the actresses with person_id above 3900000 are included.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nINNER JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 5.12: Inner JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nRIGHT JOIN\nWith a RIGHT JOIN, there are more than 300 rows (the LIMIT clause keeps us from knowing how many rows, but there are a LOT!) corresponding to all the actresses whose person_id above 3900000 are included. Those actresses who acted in one of the seven top 2015 films are also included in the full results table, but they don’t happen to be in the truncated output here.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nRIGHT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 5.13: RIGHT JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nLEFT JOIN\nWith a LEFT JOIN, there are 33 rows corresponding to the actresses in the seven top 2015 movies. Only The Revenant did not have any actresses whose person_id is greater than 3900000.\n\n\nSELECT * FROM\n(SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes     \nLEFT JOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nLIMIT 0, 300;\n\n\n\n\nTable 5.14: LEFT JOIN of movies and actresses.\n\n\n\n\n\n\n\n\n\nCounting repeat actresses\nWe might, for example, want to know how many names / spellings of a name with a specific person_id (above 3900000) exist for each person_id in each of the top voted seven films of 2015.\nIn Table 5.15 why isn’t there a column indicating the name of the actress? (There can’t be such a column. Why not?)\n\nSELECT acts.person_id, \n       COUNT(*) AS num_repeat_names\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY acts.person_id;\n\n\n\n\nTable 5.15: For each person_id (&gt; 3900000) in the seven top voted 2015 films, how many names / spellings are associated with the person_id?\n\n\n\n\n\nperson_id\nnum_repeat_names\n\n\n\n3916648\n1\n\n\n4122876\n1\n\n\n3938423\n2\n\n\n3950111\n1\n\n\n4079047\n2\n\n\n4084626\n3\n\n\n4099458\n1\n\n\n3958614\n1\n\n\n3990819\n2\n\n\n4081131\n2\n\n\n3907812\n3\n\n\n3938795\n1\n\n\n3970637\n8\n\n\n4073883\n2\n\n\n4094732\n1\n\n\n4098918\n1\n\n\n\n\n\n\n\n\n\nCounting number of actresses per film\nWe might, for example, want to know how many actresses with a specific person_id (above 3900000) are in each of the top voted seven films of 2015.\n\nSELECT movs.id, \n       movs.title,\n       COUNT(*) AS num_actress\nFROM (SELECT t.id,\n       t.title\nFROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               # movies only\n    AND idx.info_type_id = 100      # info_type is votes\n    AND idx.info &gt; 400000) AS movs  # at least 400,000 votes\nJOIN (SELECT a.person_id,\n       a.movie_id,\n       n.name\n    FROM cast_info AS a\n    JOIN aka_name AS n ON a.person_id = n.person_id\n    WHERE a.role_id = 2             # acresses only\n        AND a.person_id &gt; 3900000) AS acts ON acts.movie_id = movs.id\nGROUP BY movs.id;\n\n\n\n\nTable 5.16: Number of actresses (with person_id &gt; 3900000) in each of the seven top voted films of 2015. Recall that The Revenant had no actresses with person_id &gt; 3900000, so there are only six movies listed.\n\n\n\n\n\nid\ntitle\nnum_actress\n\n\n\n3313672\nAvengers: Age of Ultron\n1\n\n\n3752999\nInside Out\n1\n\n\n3787790\nJurassic World\n9\n\n\n3915213\nMad Max: Fury Road\n5\n\n\n4260166\nStar Wars: Episode VII - The Force Awakens\n15\n\n\n4389619\nThe Martian\n1",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html#unioning",
    "href": "13-sql-joins.html#unioning",
    "title": "5  Combining tables in SQL",
    "section": "\n5.3 UNIONing",
    "text": "5.3 UNIONing\nIn SQL a UNION clause combines two different tables by their rows (whereas JOIN combines two tables by columns). Think about UNION similarly to the bind_rows() command in R.\n\n\n\n\n\n\n\nFigure 5.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html\n\n\n\n\n\n5.3.1 UNIONs\nUNION does not check the names of the columns to make sure they match. UNION requires that the number of columns be the same and that the variable type be the same for all columns in the two tables being combined.\nTable 5.17 contains a silly example. The first table has 1 as bar and the second table has 20 as bar. But when the tables are UNIONed, the bar column contains c(1, 10). SQL took the column names from the first table and appended the second table without considering the variable names.\n\nSELECT \n    1 AS bar,\n    2 AS foo\n\nUNION\n\nSELECT \n    10 AS foo,\n    20 AS bar;\n\n\n\n\nTable 5.17: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\n\n\n\n\nbar\nfoo\n\n\n\n1\n2\n\n\n10\n20\n\n\n\n\n\n\n\n\n\nUNION is specifically designed to bind rows from two different SELECT queries where the variables have been selected in the same order. If the two SELECT clauses are done from the same table with the same order of variables, you do not need to worry about the order of the variables matching up in the UNION. If you are UNIONing two very different subqueries, you do need to worry about the variables and their order.\nUNION\nLet’s say we want to combine the top voted movies from 2015 with the top voted movies from 2019. However, to account for time, we require the movies from 2015 to have more votes (400,000) than the movies from 2017 (200,000). That is, the WHERE clause is different for the two subqueries.\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2017  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 200000)\nLIMIT 0, 100;\n   \n\n\n\n\nTable 5.18: The variable names are chosen from the first table. The names and order of the variables in the second table are ignored when using UNION.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nThe Martian\n2015\n583987\n\n\nThe Revenant\n2015\n526189\n\n\nDunkirk\n2017\n229089\n\n\nGuardians of the Galaxy Vol. 2\n2017\n281845\n\n\nLogan\n2017\n397056\n\n\nSpider-Man: Homecoming\n2017\n209930\n\n\nWonder Woman\n2017\n306611\n\n\n\n\n\n\n\n\n\nUNION ALL\nUNION does check, however, to see if any of the rows in the two tables are identical. If the goal is to include duplicates across two tables, use UNION ALL instead of UNION.\nLet’s say that the first table is all movies with production year after 2012 and number of votes greater than 500,000. The second table is movies with production year equal to 2015 and number of votes greater than 400,000. Even though the Martian would have been in both tables, the results table lists The Marian only once in Table 5.19.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\nTable 5.19: Using UNION to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\n\nWhen UNION ALL is applied in the same context, The Martian is listed twice in the results table given in Table 5.20.\n\n(SELECT t.title,\n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year &gt; 2012  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 500000)\n\nUNION ALL\n\n(SELECT t.title, \n        t.production_year, \n        idx.info AS num_votes\n    FROM title AS t\nJOIN movie_info_idx AS idx ON idx.movie_id = t.id\nWHERE t.production_year = 2015  \n    AND t.kind_id = 1               \n    AND idx.info_type_id = 100      \n    AND idx.info &gt; 400000)\nORDER BY production_year DESC, num_votes;\n\n\n\n\nTable 5.20: Using UNION ALL to combine movies from table 1: later than 2012 and at least 500,000 votes with movies from table 2: 2015 and at least 400,000 votes.\n\n\n\n\n\ntitle\nproduction_year\nnum_votes\n\n\n\nBatman v Superman: Dawn of Justice\n2016\n500037\n\n\nDeadpool\n2016\n673887\n\n\nInside Out\n2015\n443051\n\n\nJurassic World\n2015\n471237\n\n\nThe Revenant\n2015\n526189\n\n\nThe Revenant\n2015\n526189\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nAvengers: Age of Ultron\n2015\n540606\n\n\nThe Martian\n2015\n583987\n\n\nThe Martian\n2015\n583987\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nMad Max: Fury Road\n2015\n666484\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nStar Wars: Episode VII - The Force Awakens\n2015\n691691\n\n\nInterstellar\n2014\n1102826\n\n\nWhiplash\n2014\n507827\n\n\nThe Imitation Game\n2014\n550521\n\n\nThe Grand Budapest Hotel\n2014\n553558\n\n\nCaptain America: The Winter Soldier\n2014\n562419\n\n\nX-Men: Days of Future Past\n2014\n567780\n\n\nGone Girl\n2014\n664035\n\n\nGuardians of the Galaxy\n2014\n795151\n\n\n12 Years a Slave\n2013\n506640\n\n\nNow You See Me\n2013\n507519\n\n\nWorld War Z\n2013\n509285\n\n\nThe Hobbit: The Desolation of Smaug\n2013\n526001\n\n\nThe Hunger Games: Catching Fire\n2013\n537678\n\n\nMan of Steel\n2013\n592427\n\n\nIron Man Three\n2013\n607323\n\n\nGravity\n2013\n640900\n\n\nThe Wolf of Wall Street\n2013\n900450\n\n\n\n\n\n\n\n\n\n\n5.3.1.1 FULL OUTER JOIN via UNION\n\nMySQL doesn’t have a FULL OUTER JOIN (although other implementations of SQL do have full join functionality). However, we can mimic a full join using right and left joins with UNION.\nRecall the ideas of RIGHT JOIN (which keeps all observations in the right table) and LEFT JOIN (which keeps all observations in the left table). By UNIONing the right and left joins, all of the observations are obtained (i.e., a full join). Using the function sqldf() in the sqldf R package, the full join will be demonstrated using the 1960s rock bands.\nNotice that in the RIGHT JOIN the name column must come from the right table (not the left table).\nAlso notice that UNION ALL keeps the duplicate rows which is probably not what we want.\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n  name    band  plays\n1 Mick  Stones   &lt;NA&gt;\n2 John Beatles guitar\n3 Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name\")\n\n   name    band  plays\n1  John Beatles guitar\n2  Paul Beatles   bass\n3 Keith    &lt;NA&gt; guitar\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  John Beatles guitar\n2 Keith    &lt;NA&gt; guitar\n3  Mick  Stones   &lt;NA&gt;\n4  Paul Beatles   bass\n\nsqldf::sqldf(\"SELECT star.name, star.band, inst.plays \n      FROM band_members AS star\n      LEFT JOIN band_instruments AS inst ON star.name = inst.name\nUNION ALL\n      SELECT inst.name, star.band, inst.plays \n      FROM band_members AS star\n      RIGHT JOIN band_instruments AS inst ON star.name = inst.name \")\n\n   name    band  plays\n1  Mick  Stones   &lt;NA&gt;\n2  John Beatles guitar\n3  Paul Beatles   bass\n4  John Beatles guitar\n5  Paul Beatles   bass\n6 Keith    &lt;NA&gt; guitar",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html#best-practice",
    "href": "13-sql-joins.html#best-practice",
    "title": "5  Combining tables in SQL",
    "section": "\n5.4 Best practice",
    "text": "5.4 Best practice\nIt is always a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_imdb, shutdown = TRUE)",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html#reflection-questions",
    "href": "13-sql-joins.html#reflection-questions",
    "title": "5  Combining tables in SQL",
    "section": "\n5.5  Reflection questions",
    "text": "5.5  Reflection questions\n\nWhat are the different types of joins? Which data from which table gets kept and which gets removed for each type of join?\nWhat is the difference between a join and a union?\nWhen working with multiple tables, how (and why) is a variable linked to its table?\nConsider a RIGHT JOIN. If there are records in the right table that are not in the left table, what will the value of the left table variable be for those records?",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "13-sql-joins.html#ethics-considerations",
    "href": "13-sql-joins.html#ethics-considerations",
    "title": "5  Combining tables in SQL",
    "section": "\n5.6  Ethics considerations",
    "text": "5.6  Ethics considerations\n\nWhat can happen if a UNION is done without carefully matching up the columns of the two tables being UNIONed?\nHow will you know if JOINing removed some records? What if the JOIN produced missing values for some of the variables? How should we deal with missing data or arbitrarily removed records?\n\n\n\n\nFigure 5.1: Venn diagrams describing different JOINs, image credit: phoenixNAP https://phoenixnap.com/kb/mysql-join\nFigure 5.2: Mini data tables describing different JOINs, image credit: Statistics Globe blog, https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nFigure 5.3: UNION binds rows while JOIN appends columns, image credit: Jane Williams https://blog.devart.com/mysql-union-tutorial-html.html",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Combining tables in SQL</span>"
    ]
  },
  {
    "objectID": "10-db.html",
    "href": "10-db.html",
    "title": "4  Databases",
    "section": "",
    "text": "4.1 What is a database?\nA database is a structured collection of data that is organized in such a way that facilitates efficient storage, retrieval, and management of information. They are particularly important for industries with exceptionally large amounts of data which can be partitioned into different tables. Additionally, databases allow for multiple users to access the data simultaneously.\nYou are likely already familiar with the concept of tidy data. (If you have never encountered tidy data, see Chapter 5 Data tidying in R for Data Science.) Tidy data typically live in data frames or tables; where, importantly, they consist of columns of variables (where every column is the same type!) and the rows of observational units.\nConceptually, a table in a database is no different from the data frames which are used in R. They will always be rectangles with the same row and column structure.\nThere are two big differences between a stand alone data frame (e.g., in R) and a table which lives in a database (e.g., in SQL).",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#sec-what-db",
    "href": "10-db.html#sec-what-db",
    "title": "4  Databases",
    "section": "",
    "text": "Most importantly, tables in databases can be arbitrarily large, primarily due to being stored on disk. Indeed, they are typically not stored on your computer’s disk, they are stored remotely on a hard drive outside of your own computer and work space. Data frames are stored in memory (on your computer) and can be quite limited in size.\n\nMemory (RAM) is the amount of data that a computer can work on simultaneously. My computer has 32 GB of RAM. The important thing about memory is that the computer has easy access to its own memory and will access it quickly (tens of GBs per second).\nHard Disk is the amount of data that a computer can store permanently; it is your storage space. My computer has 2 TB of storage. Accessing the disk is much slower (hundreds of MBs per second) than accessing the memory. Accessing disk space is even slower if the storage lives on a different computer and is accessed virtually (via WiFi).\n\n\nThe tables in a database are usually linked with a key. We will cover join() functions in Chapter 5, but keep in mind that in order to get information from one table to connect to information in another table, we need to somehow relate the rows of the first table to the rows of the second table.",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#the-airlines-database",
    "href": "10-db.html#the-airlines-database",
    "title": "4  Databases",
    "section": "\n4.2 The airlines database",
    "text": "4.2 The airlines database\nTo demonstrate the difference between data frames in memory and tables in storage, we will consider the airlines data consisting of millions of individual flights between 2010 and 2017. The flights are downloaded from the Bureau of Transportation Statistics, US Department of Transportation. The database is a superset of the nycflights13 R package that tracks only flights in and out of airports serving New York City in 2013.\nThe full data set occupies almost 20GB when they are saved as CSV (comma separated value) files, a common way to hold data that can be represented as columns of text which are separated by commas.",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#tables-in-sql",
    "href": "10-db.html#tables-in-sql",
    "title": "4  Databases",
    "section": "\n4.3 Tables in SQL\n",
    "text": "4.3 Tables in SQL\n\nTo look at the airlines data tables, we first need to connect to the database remotely. The function dbConnect_scidb() in the mdsr package allows us to connect to the databases which are stored for use with the text Modern Data Science with R.\nThe tbl() function (in the dplyr package) maps the tables called flights and carriers from the database to an object in R.\n\nlibrary(tidyverse)\nlibrary(mdsr)\ncon_air &lt;- mdsr::dbConnect_scidb(\"airlines\")\nflights &lt;- tbl(con_air, \"flights\")\ncarriers &lt;- tbl(con_air, \"carriers\")\n\nAlternatively, the SQL connection can be set up directly using information specific to the mdsr R package database.\n\ncon_air &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"airlines\",\n  host = Sys.getenv(\"MDSR_HOST\"),\n  user = Sys.getenv(\"MDSR_USER\"),\n  password = Sys.getenv(\"MDSR_PWD\")\n)\n\nWe can now use the objects flights and carriers as if they were data frames, but they are not actually the R version of data.frame. Instead, they exist as a tbl which is a special object that behaves similarly to a data.frame.\nThe carriers data represents the name of each airline and the associated carrier code. Note that when the data are printed to screen, the number of rows is given by ??, indicating that they are unknown. Indeed, R just needed the first few rows to print to screen, no need to spend precious computing resources looking at the entire table.\n\ncarriers \n\n# Source:   table&lt;`carriers`&gt; [?? x 2]\n# Database: mysql  [mdsr_public@mdsr.cdc7tgkkqd0n.us-east-1.rds.amazonaws.com:3306/airlines]\n  carrier name                         \n  &lt;chr&gt;   &lt;chr&gt;                        \n1 02Q     Titan Airways                \n2 04Q     Tradewind Aviation           \n3 05Q     Comlux Aviation, AG          \n4 06Q     Master Top Linhas Aereas Ltd.\n5 07Q     Flair Airlines Ltd.          \n6 09Q     Swift Air, LLC               \n# ℹ more rows\n\n\nWith the carriers data, it is possible, however, to load the entire object into R using collect(). Now, when the data are printed to screen, the object is a tibble and R knows that it has 1,610 rows.\n\ncarriers |&gt;\n  collect() \n\n# A tibble: 1,610 × 2\n  carrier name                         \n  &lt;chr&gt;   &lt;chr&gt;                        \n1 02Q     Titan Airways                \n2 04Q     Tradewind Aviation           \n3 05Q     Comlux Aviation, AG          \n4 06Q     Master Top Linhas Aereas Ltd.\n5 07Q     Flair Airlines Ltd.          \n6 09Q     Swift Air, LLC               \n# ℹ 1,604 more rows\n\n\nAs already mentioned, working in R versus working remotely has trade-offs. Remember that the object takes up much more space in your memory if you load it into R. Consider the following which demonstrates how much more memory intensive it is to hold an object in R.\n\n# carriers lives in the SQL database and is linked remotely\ncarriers |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n5.3 Kb\n\n\n\n# carriers lives in R\ncarriers |&gt;\n  collect() |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n234.8 Kb\n\n\nIndeed, the flights data set contains all of the flights and is much larger than the carriers data set. When pulled into R it takes up almost 5 GB of memory (!), but when it exists as only a connection to the SQL database is uses just a few Kb of memory.\n\n# flights lives in the database and is linked remotely\nflights |&gt;\n  object.size() |&gt;\n  print(units = \"Kb\")\n\n6.5 Kb\n\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\nlibrary(DBI)\ndbDisconnect(con_air, shutdown = TRUE)",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#engaging-with-a-database",
    "href": "10-db.html#engaging-with-a-database",
    "title": "4  Databases",
    "section": "\n4.4 Engaging with a database",
    "text": "4.4 Engaging with a database\n\n4.4.1 Many SQL implementations\nSQL (Structured Query Language) is a query language for working with relational databases. (The relational part means that the data sets are connected in a meaningful way, the database part means that we have lots of tables living somewhere remotely on a hard drive.) SQL has been around since the 1970s and is extremely powerful for data wrangling tasks.\nAlthough SQL has been a standard for the American National Standards Institute (ANSI) since 1986, there exist many dialects of SQL. Translating between the dialects is not always easy although once you learn how to program in one dialect, you will be able to pick up any of the other SQL dialects. We will use MySQL in this class. MySQL is among the most popular implementations of SQL and it is open source.\n\n4.4.2 SQL interfaces\nMySQL is based on a client-server model. The data live on a powerful computer (the server) and you connect to the data from your own computer (the client). In this class:\n\n\nSQL code will be written in the MySQL dialect\nusing both RStudio and DBeaver as the interface to\nconnect to many different remote servers.\n\n4.4.3 SQL in-process\nAnother approach to engaging with SQL is where the client and the server are both on a single computer (called in-process). You may want to set up a database on your own computer to try things out and avoid monthly charges associated with buying server space in the cloud. Indeed, for the end of the semester project, if you choose to set up a database on your own computer, a good free database management system is Duckdb",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#connecting-to-a-database",
    "href": "10-db.html#connecting-to-a-database",
    "title": "4  Databases",
    "section": "\n4.5 Connecting to a database",
    "text": "4.5 Connecting to a database\nWhen using R to connect to a database, we need two R packages. DBI is a low-level interface that connects to databases and executes SQL; RMariaDB is a package specifically tailored for MySQL which translates generic DBI commands into the specific syntax needed for MySQL.\nA third R package, dbplyr, is a high-level interface that translates dplyr code (i.e., R code) to SQL queries then executes them with DBI.\n\n4.5.1 Creating a database connection\nBefore we can do anything, we need to set up a connection to a MySQL database. We will call the connection con, and the syntax will look something like what is written below. However, the code below won’t run because values must be set for dbname, host, user, and password.\n\ncon &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"  \",\n  host = \"  \",\n  user = \"  \",\n  password = \"  \"\n)\n\nNote that the function dbConnect_scidb() in the mdsr package is just a wrapper of the dbConnect() function where all the arguments are filled in to connect to the mdsr database.\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con, shutdown = TRUE)\n\n\n4.5.2 Connecting to Duckdb\nConnecting to Duckdb is reasonably straightforward because the default values of the duckdb() function in the duckdb package create a temporary database that is deleted when you quit R.\n\ncon_duckdb &lt;- DBI::dbConnect(duckdb::duckdb())\n\nAlways a good idea to terminate the SQL connection when you are done with it.\n\ndbDisconnect(con_duckdb, shutdown = TRUE)",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#reflection-questions",
    "href": "10-db.html#reflection-questions",
    "title": "4  Databases",
    "section": "\n4.6  Reflection questions",
    "text": "4.6  Reflection questions\n\nWhy is SQL such an important tool for data scientists? That is, what are the characteristics that make it useful?\nWhat is a relational database?\n(Maybe best to answer after learning a few more concepts.) What are the main differences between working with a data frame in R and a table in SQL?\nHow does one connect to a SQL database? Using R? Using DuckDB?",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "10-db.html#ethics-considerations",
    "href": "10-db.html#ethics-considerations",
    "title": "4  Databases",
    "section": "\n4.7  Ethics considerations",
    "text": "4.7  Ethics considerations\n\nWhy should you need a password to access a SQL server?\nWhat other skills for working with databases are important, beyond accessing, wrangling, and creating databases? (E.g., provenance of the data, purpose of the data’s use, protection of data, privacy of data… etc. Why / how are those important?)",
    "crumbs": [
      "Using SQL",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Databases</span>"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Course Logistics\nWhat is Statistics? Generally, statistics is the academic discipline which uses data to make claims and predictions about larger populations of interest. It is the science of collecting, wrangling, visualizing, and analyzing data as a representation of a larger whole. It is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nProbability vs. Statistics\nWhat is the content of Math 154? This class will be an introduction to statistical methods that rely heavily on the use of computers. The course will generally have three parts. The first section will include communicating and working with data in a modern era. This includes data wrangling, data visualization, data ethics, and collaborative research (via GitHub). The second part of the course will focus on traditional statistical inference done through computational methods (e.g., permutation tests, bootstrapping, and regression smoothers). The last part of the course will focus on machine learning ideas such as classification, clustering, and dimension reduction techniques. Some of the methods were invented before the ubiquitous use of personal computers, but only because the calculus used to solve the problem was relatively straightforward (or because the method wasn’t actually every used). Some of the methods have been developed within the last few years.\nWho should take Math 154? Computational Statistics will cover many of the concepts and tools for modern data analysis, and therefore the ideas are important for people who would like to do modern data analysis. Some individuals may want to go to graduate school in statistics or data science, some may hope to become data scientists without additional graduate work, and some may hope to use modern techniques in other disciplines (e.g., computational biology, environmental analysis, or political science). All of these groups of individuals will get a lot out of Computational Statistics as they turn to analyzing their own data. Computational Statistics is not, however, a course which is necessary for entry into graduate school in statistics, mathematics, data science, or computer science.\nWhat are the prerequisites for Math 154? Computational Statistics requires a strong background in both statistics as well as algorithmic thinking. The formal prerequisite is any introductory statistics course, but if you have had only AP Statistics, you may find yourself working very hard in the first few weeks of the class to catch up. If you have taken a lot of mathematics, there are parts of the course that will come easily to you. However, a mathematics degree is not a substitute for introductory statistics, and if you have not taken introductory statistics, the majority of the course work will not be intuitive for you. You must have taken a prior statistics course as a pre-requisite to Math 154; a computer science course is also recommended.\nIt is worth noting that probability represents the majority of mathematical tools used in statistics, but probability as a discipline does not work with data. Having taken a probability class may help you with some of the mathematics covered in the course, but it is not a substitute for understanding the basics of introductory statistics.\nIs there overlap with other classes? There are many machine learning and data science courses at the 5Cs which overlap with Math 154. Those courses continue to be developed and change, so I cannot speak to all of them. Generally, the Data Science courses taught in other 5C math departments focus slightly more on the mathematics of the tools (e.g., mathematically breaking down sparse matrices) and the Machine Learning courses taught in 5C CS departments focus on the programming aspects of the tools (e.g., how to code a Random Forest). Our focus will be on the inferential aspect of the tools, that is, what do the results say about the larger problem which we are trying to solve? How can we know the results are accurate? What are the sources of variability?\nWhen should I take Math 154? While the prerequisite for Computational Statistics is Introduction to Statistics, the course moves very quickly and covers a tremendous amount of material. It is not ideally suited for a first year student coming straight out of AP Statistics. Instead, that student should focus on taking more mathematics, CS, interdisciplinary science, or other statistics courses. Most students taking Computational Statistics are juniors and seniors.\nWhat is the workload for Math 154? There is one homework assignment per week, two in-class midterm exams, two take-home midterm exams, and a final end of the semester project. Many students report working about 8-10 hours per week on this class.\nWhat software will we use? Will there be any real world applications? Will there be any mathematics? Will there be any CS? All of the work will be done in R (using RStudio as a front end, called an integrated development environment, IDE). You will need to either download R and RStudio (both are free) onto your own computer or use them on Pomona’s server. All assignments will be posted to private repositories on GitHub. The class is a mix of many real world applications and case studies, some higher level math, programming, and communication skills. The final project requires your own analysis of a dataset of your choosing.\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-logistics",
    "href": "01-intro.html#course-logistics",
    "title": "1  Introduction",
    "section": "",
    "text": "descriptive statistics describe the sample at hand with no intent on making generalizations.\n\ninferential statistics use a sample to make claims about a population\n\n\n\n\n\n\n\n\n\n\nYou may use R on the Pomona server: https://rstudio.pomona.edu/ (All Pomona students will be able to log in immediately. Non-Pomona students need to go to ITS at Pomona to get Pomona login information.)\nIf you want to use R on your own machine, you may. Please make sure all components are updated: R is freely available at http://www.r-project.org/ and is already installed on college computers. Additionally, installing R Studio is required http://rstudio.org/.\nAll assignments should be turned in using R Markdown compiled to pdf.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#course-content",
    "href": "01-intro.html#course-content",
    "title": "1  Introduction",
    "section": "\n1.2 Course Content",
    "text": "1.2 Course Content\n\n1.2.1 Topics\nComputational Statistics can be a very large umbrella for many ideas. Indeed, sometimes the topics can seem somewhat disjointed. Below, I’ve categorized the topics we will cover into four groups. The four different broad topics all play different roles and can be more or less important depending on the problem at hand. None of the topics should exist on their own, because only with the bigger focus on all topics will any sort of data analysis / interpretation be accurate and compelling.\nLetting the computer help: R, RStudio, Git, GitHub, Reproducibility, Data Viz, Data Wrangling\nStatistics: Simulating, Randomization / Permutation Tests, Bootstrapping, Ethics\nMachine Learning: Classification, Clustering, Regular Expressions\nMathematics: Support Vector Machines\n\n1.2.2 Vocabulary\n\nA statistic is a numerical measurement we get from the sample, a function of the data.\nA parameter is a numerical measurement of the population. We never know the true value of the parameter.\nAn estimator is a function of the unobserved data that tries to approximate the unknown parameter value.\nAn estimate is the value of the estimator for a given set of data. [Estimate and statistic can be used interchangeably.]\n\n\nOne of my goals for this course was to convince students that there are two major kinds of skills one must have in order to be a successful data scientist: technical skills to actually do the analyses; and communication skills in order to present one’s findings to a presumably non-technical audience. (Baumer 2015)\n\nWith thanks to Ben Baumer for perspective and sharing course materials.\n\n1.2.3 The Workflow\n\n\n\n\n\n\n\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\n\n\n\n\n\n\n\n\n\n\nStitch Fix Algorithms Tour\n\n\n\n\n1.2.4 Principles for the Data Science Process tl;dr\n(Below are some very good thoughts on the DS Process, but you are not responsible for any of the content in this section.)\nDuncan Temple Lang, University of California, Davis\nDuncan Temple-Lang is a leader in the area of combining computer science research concepts within the context of statistics and science more generally. Recently, he was invited to participate in a workshop, Training Students to Extract Value from Big Data. The workshop was subsequently summarized in a manuscript of the same name and has been provided free of charge. http://www.nap.edu/catalog.php?record_id=18981 [National Research Council. Training Students to Extract Value from Big Data: Summary of a Workshop. Washington, DC: The National Academies Press, 2014.]\nDuncan Temple Lang began by listing the core concepts of data science - items that will need to be taught: statistics and machine learning, computing and technologies, and domain knowledge of each problem. He stressed the importance of interpretation and reasoning - not only methods - in addressing data. Students who work in data science will have to have a broad set of skills - including knowledge of randomness and uncertainty, statistical methods, programming, and technology - and practical experience in them. Students tend to have had few computing and statistics classes on entering graduate school in a domain science.\nTemple Lang then described the data analysis pipeline, outlining the steps in one example of a data analysis and exploration process:\n\nAsking a general question.\nRefining the question, identifying data, and understanding data and metadata. Temple Lang noted that the data used are usually not collected for the specific question at hand, so the original experiment and data set should be understood.\nAccess to data. This is unrelated to the science but does require computational skill.\nTransforming to data structures.\nExploratory data analyses to understand the data and determine whether the results will scale.\n\nThis is a critical step; Temple Lang noted that 80 percent of a data scientist’s time can be spent in cleaning and preparing the data. 6. Dimension reduction. Temple Lang stressed that it can be difficult or impossible to automate this step. 7. Modeling and estimation. Temple Lang noted that computer and machine learning scientists tend to focus more on predictive models than on modeling of physical behavior or characteristics. 8. Diagnostics. This helps to understand how well the model fits the data and identifies anomalies and aspects for further study. This step has similarities to exploratory data analysis. 9. Quantifying uncertainty. Temple Lang indicated that quantifying uncertainty with statistical techniques is important for understanding and interpreting models and results. 10. Conveying results.\nTemple Lang stressed that the data analysis process is highly interactive and iterative and requires the presence of a human in the loop. The next step in data processing is often not clear until the results of the current step are clear, and often something unexpected is uncovered. He also emphasized the importance of abstract skills and concepts and said that people need to be exposed to authentic data analyses, not only to the methods used. Data scientists also need to have a statistical understanding, and Temple Lang described the statistical concepts that should be taught to a student:\n\nMapping the general question to a statistical framework.\nUnderstanding the scope of inference, sampling, biases, and limitations.\nExploratory data analyses, including missing values, data quality, cleaning, matching, and fusing.\nUnderstanding randomness, variability, and uncertainty. Temple Lang noted that many students do not understand sampling variability.\nConditional dependence and heterogeneity.\nDimension reduction, variable selection, and sparsity.\nSpurious relationships and multiple testing.\nParameter estimation versus “black box” prediction and classification.\nDiagnostics, residuals, and comparing models.\nQuantifying the uncertainty of a model.\nSampling structure and dependence for data reduction. Temple Lang noted that modeling of data becomes complicated when variables are not independent, identically distributed.\nStatistical accuracy versus computational complexity and efficiency.\n\nTemple Lang then briefly discussed some of the practical aspects of computing, including the following:\n\nAccessing data.\nManipulating raw data.\nData structures and storage, including correlated data.\nVisualization at all stages (particularly in exploratory data analyses and conveying the results).\nParallel computing, which can be challenging for a new student.\nTranslating high-level descriptions to optimal programs.\n\nDuring the discussion, Temple Lang proposed computing statistics on visualizations to examine data rigorously in a statistical and automated way. He explained that “scagnostics” (from scatter plot diagnostics) is a data analysis technique for graphically exploring the relationships among variables. A small set of statistical measures can characterize scatter plots, and exploratory data analysis can be conducted on the residuals. [More information about scagnostics can be found in (Wilkinson et al., 2005, 2006).]\nA workshop participant noted the difference between a data error and a data blunder. A blunder is a large, easily noticeable mistake. The participant gave the example of shipboard observations of cloud cover; blunders, in that case, occur when the location of the ship observation is given to be on land rather than at sea. Another blunder would be a case of a ship’s changing location too quickly. The participant speculated that such blunders could be generalized to detect problematic observations, although the tools would need to be scalable to be applied to large data sets.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#repro",
    "href": "01-intro.html#repro",
    "title": "1  Introduction",
    "section": "\n1.3 Reproducibility",
    "text": "1.3 Reproducibility\nReproducibility has long been considered an important topic for consideration in any research project. However, recently there has been increased press and available examples for understanding the impact that non-reproducible science can have.\nKitzes, Turek, and Deniz (2018) provide a full textbook on the structure of reproducible research as well as dozens of case studies to help hone skills and consider different aspects of the reproducible pipeline. Below are a handful of examples to get us started.\n\n1.3.1 Need for Reproducibility\n\n\n\n\n\n\n\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\n\n\n\nExample 1\nScience retracts gay marriage paper without agreement of lead author LaCour\n\n\n\n\n\n\n\n\n\nIn May 2015 Science retracted a study of how canvassers can sway people’s opinions about gay marriage published just 5 months prior.\nScience Editor-in-Chief Marcia McNutt:\n\nOriginal survey data not made available for independent reproduction of results.\nSurvey incentives misrepresented.\nSponsorship statement false.\n\n\nTwo Berkeley grad students who attempted to replicate the study quickly discovered that the data must have been faked.\nMethods we’ll discuss can’t prevent this, but they can make it easier to discover issues.\nSource: http://news.sciencemag.org/policy/2015/05/science-retracts-gay-marriage-paper-without-lead-author-s-consent\nExample 2\nSeizure study retracted after authors realize data got “terribly mixed”\n\n\n\n\n\n\n\n\n\nFrom the authors of Low Dose Lidocaine for Refractory Seizures in Preterm Neonates:\n\n\nThe article has been retracted at the request of the authors. After carefully re-examining the data presented in the article, they identified that data of two different hospitals got terribly mixed. The published results cannot be reproduced in accordance with scientific and clinical correctness.\n\n\nSource: http://retractionwatch.com/2013/02/01/seizure-study-retracted-after-authors-realize-data-got-terribly-mixed/\nExample 3\nBad spreadsheet merge kills depression paper, quick fix resurrects it\n\n\n\n\n\n\n\n\n\nThe authors informed the journal that the merge of lab results and other survey data used in the paper resulted in an error regarding the identification codes. Results of the analyses were based on the data set in which this error occurred. Further analyses established the results reported in this manuscript and interpretation of the data are not correct.\n\n\nOriginal conclusion: Lower levels of CSF IL-6 were associated with current depression and with future depression …\n\n\nRevised conclusion: Higher levels of CSF IL-6 and IL-8 were associated with current depression …\n\n\nSource: http://retractionwatch.com/2014/07/01/bad-spreadsheet-merge-kills-depression-paper-quick-fix-resurrects-it/\nExample 4\nPNAS paper retracted due to problems with figure and reproducibility (April 2016): http://cardiobrief.org/2016/04/06/pnas-paper-by-prominent-cardiologist-and-dean-retracted/\n\n\n\n\n\n\n\n\n\n1.3.2 The reproducible data analysis process\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming \\(\\rightarrow\\) R Markdown\nVersion control \\(\\rightarrow\\) Git / GitHub\n\nScripting and literate programming\nDonald Knuth “Literate Programming” (1983)\n\nLet us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer- what to do, let us concentrate rather on explaining to human beings- what we want a computer to do.\n\n\nThe ideas of literate programming have been around for many years!\nand tools for putting them to practice have also been around\nbut they have never been as accessible as the current tools\nReproducibility checklist\n\nAre the tables and figures reproducible from the code and data?\nDoes the code actually do what you think it does?\nIn addition to what was done, is it clear why it was done? (e.g., how were parameter settings chosen?)\nCan the code be used for other data?\nCan you extend the code to do other things?\nTools: R & R Studio\nSee this great video (less than 2 min) on a reproducible workflow: https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.be\n\nYou must use both R and RStudio software programs\nR does the programming\nR Studio brings everything together\nYou may use Pomona’s server: https://rstudio.pomona.edu/\nSee course website for getting started: http://research.pomona.edu/johardin/math154f19/\n\n\n\n\n\n\n\n\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\n\n\n\n\n\n\n\n\n\n\nJessica Ward, PhD student at Newcastle University\n\n\n\nTools: Git & GitHub\n\nYou must submit your assignments via GitHub\nFollow Jenny Bryan’s advice on how to get set-up: http://happygitwithr.com/\nClass specific instructions at https://m154-comp-stats.netlify.app/github.html\n\nAdmittedly, there is a steep learning curve with Git. However, it is among the tools which you are most likely to use in your future endeavors, so spending a little time focusing on the concepts now may pay off big time in the future. Beyond practicing and working through http://happygitwithr.com/, you may want to read a little bit about waht Git is doing behind the scences. This reference: Learn git concepts, not commands is very good and accessible.\nTools: a GitHub merge conflict (demo)\n\nOn GitHub (on the web) edit the README document and Commit it with a message describing what you did.\nThen, in RStudio also edit the README document with a different change.\n\nCommit your changes\nTry to push \\(\\rightarrow\\) you’ll get an error!\nTry pulling\nResolve the merge conflict and then commit and push\n\n\nAs you work in teams you will run into merge conflicts, learning how to resolve them properly will be very important.\n\n\n\n\n\n\n\n\nhttps://xkcd.com/1597/\n\n\n\nSteps for weekly homework\n\nYou will get a link to the new assignment (clicking on the link will create a new private repo)\n\nUse R (within R Studio)\n\nNew Project, version control, Git\n\nClone the repo using SSH\n\n\n\nIf it exists, rename the Rmd file to ma154-hw#-lname-fname.Rmd\n\nDo the assignment\n\n\ncommit and push after every problem\n\n\n\nAll necessary files must be in the same folder (e.g., data)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-examples",
    "href": "01-intro.html#data-examples",
    "title": "1  Introduction",
    "section": "\n1.4 Data Examples",
    "text": "1.4 Data Examples\nWhat can/can’t Data Science Do?\n\nCan model the data at hand!\nCan find patterns & visualizations in large datasets.\nCan’t establish causation.\nCan’t represent data if it isn’t there.\nStats / Data Science / Math are not apolitical/agnostic\n\n“Inner city crime is reaching record levels” (Donald Trump, 8/30/16)\n“The unemployment rate for African-American youth is 59 percent” (Donald Trump 6/20/16)\n“Two million more Latinos are in poverty today than when President Obama took his oath of office less than eight years ago” (Donald Trump 8/25/16)\n“We are now, for the first time ever, energy independent” (Hillary Clinton 8/10/16)\n“If you look worldwide, the number of terrorist incidents have not substantially increased” (Barack Obama 10/13/16)\n“Illegal immigration is lower than it’s been in 40 years” (Barack Obama, 3/17/16)\n\nSource: http://www.politifact.com/truth-o-meter/statements/\n\n1.4.1 College Rankings Systems\nCheating\n\nBucknell University lied about SAT averages from 2006 to 2012, and Emory University sent in biased SAT scores and class ranks for at least 11 years, starting in 2000. Iona College admitted to fudging SAT scores, graduation rates, retention rates, acceptance rates, and student-to-faculty ratios in order to move from 50th place to 30th for nine years before it was discovered. ( Weapons of Math Destruction, O’Neil, https://weaponsofmathdestructionbook.com/ and http://www.slate.com/articles/business/moneybox/2016/09/how_big_data_made_applying_to_college_tougher_crueler_and_more_expensive.html)\n\nGaming the system\n\nPoint by point, senior staff members tackled different criteria, always with an eye to U.S. News’s methodology. Freeland added faculty, for instance, to reduce class size. “We did play other kinds of games,” he says. “You get credit for the number of classes you have under 20 [students], so we lowered our caps on a lot of our classes to 19 just to make sure.” From 1996 to the 2003 edition (released in 2002), Northeastern rose 20 spots. ( 14 Reasons Why US News College Rankings are Meaningless http://www.liberalartscolleges.com/us-news-college-rankings-meaningless/)\n\nNo way to measure “quality of education”\nWhat is “best”? A big part of the ranking system has to do with peer-assessed reputation (feedback loop!).\n\n1.4.2 Trump and Twitter\nAnalysis of Trump’s tweets with evidence that someone else tweets from his account using an iPhone.\n\nAug 9, 2016 http://varianceexplained.org/r/trump-tweets/\n\n\nMy analysis, shown below, concludes that the Android and iPhone tweets are clearly from different people, posting during different times of day and using hashtags, links, and retweets in distinct ways. What’s more, we can see that the Android tweets are angrier and more negative, while the iPhone tweets tend to be benign announcements and pictures.\n\n\nAug 9, 2017 http://varianceexplained.org/r/trump-followup/\n\n\nThere is a year of new data, with over 2700 more tweets. And quite notably, Trump stopped using the Android in March 2017. This is why machine learning approaches like http://didtrumptweetit.com/ are useful, since they can still distinguish Trump’s tweets from his campaign’s by training on the kinds of features I used in my original post.\n\n\nI’ve found a better dataset: in my original analysis, I was working quickly and used the twitteR package (https://cran.r-project.org/web/packages/twitteR/) to query Trump’s tweets. I since learned there’s a bug in the package that caused it to retrieve only about half the tweets that could have been retrieved, and in any case I was able to go back only to January 2016. I’ve since found the truly excellent Trump Twitter Archive (http://www.trumptwitterarchive.com/), which contains all of Trump’s tweets going back to 2009. Below I show some R code for querying it.\n\n\nI’ve heard some interesting questions that I wanted to follow up on: These come from the comments on the original post and other conversations I’ve had since. Two questions included what device Trump tended to use before the campaign, and what types of tweets tended to lead to high engagement.\n\n\n1.4.3 Can Twitter Predict Election Results?\n\nIn 2013, DiGrazia et al. (2013) published a provocative paper suggesting that polling could now be replaced by analyzing social media data. They analyzed 406 competitive US congressional races using over 3.5 billion tweets. In an article in The Washington Post one of the co-authors, Rojas, writes: “Anyone with programming skills can write a program that will harvest tweets, sort them for content and analyze the results. This can be done with nothing more than a laptop computer.” (Rojas 2013)\nWhat makes using Tweets to predict elections relevant to our class? (See Baumer (2015).)\n\nThe data come from neither an experiment nor a random sample - there must be careful thought applied to the question of to whom the analysis can be generalized. The data were also scraped from the internet.\nThe analysis was done combining domain knowledge (about congressional races) with a data source that seems completely irrelevant at the outset (tweets).\nThe dataset was quite large! 3.5 billion tweets were collected and a random sample of 500,000 tweets were analyzed.\nThe researchers were from sociology and computer science - a truly collaborative endeavor, and one that is often quite efficient at producing high quality analyses.\n\nActivity\nSpend a few minutes reading the Rojas editorial and skimming the actual paper. Be sure to consider Figure 1 and Table 1 carefully, and address the following questions.\n\nworking paper: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2235423\npublished in PLoS ONE: http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0079449 DiGrazia J, McKelvey K, Bollen J, Rojas F (2013) More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior. PLoS ONE 8 (11): e79449.\neditorial in The Washington Post by Rojas: http://www.washingtonpost.com/opinions/how-twitter-can-predict-an-election/2013/08/11/35ef885a-0108-11e3-96a8-d3b921c0924a_story.html\neditorial in the Huffington Post by Linkins: http://www.huffingtonpost.com/2013/08/14/twitter-predict-elections_n_3755326.html\neditorial blog by Gelman: http://andrewgelman.com/2013/04/24/the-tweets-votes-curve/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Hat\n\nWrite a sentence summarizing the findings of the paper.\nDiscuss Figure 1 with your neighbor. What is its purpose? What does it convey? Think critically about this data visualization. What would you do differently?\n\nshould be proportion for the response variable. The bizarre scaling could dramatically change the results\ndots could then be scaled in proportion to the number of tweets\nlinear fit may be questionable.\nHow would you improve the plot? I.e., annotate it to make it more convincing / communicative? Does it need enhancement?\n\n\nInterpret the coefficient of Republican Tweet Share in both models shown in Table 1. Be sure to include units.\nDiscuss with your neighbor the differences between the Bivariate model and the Full Model. Which one do you think does a better job of predicting the outcome of an election? Which one do you think best addresses the influence of tweets on an election?\n\n\n\\(R^2\\) is way higher after control variables are included, but duh!\nthe full model will likely do a better job of predicting\n\n\nWhy do you suppose that the coefficient of Republican Tweet Share is so much larger in the Bivariate model? How does this reflect on the influence of tweets in an election?\n\nAfter controlling for how many Republicans are in the district, most of the effect disappears\nWhile the coefficient of the main term is still statistically significant, the size of the coefficient\n(155 +/- 43 votes) is of little practical significance\n\n\nDo you think the study holds water? Why or why not? What are the shortcomings of this study?\n\nNot really. First of all, how many of these races are actually competitive? It’s not 406, it’s probably fewer than 100. If you redid the study on that sample, would the tweet share still be statistically significant in the full model?\n\n\nData Scientist Hat\nImagine that your boss, who does not have advanced technical skills or knowledge, asked you to reproduce the study you just read. Discuss the following with your neighbor.\n\nWhat steps are necessary to reproduce this study? Be as specific as you can! Try to list the subtasks that you would have to perform.\nWhat computational tools would you use for each task? Identify all the steps necessary to conduct the study. Could you do it given your current abilities & knowledge? What about the practical considerations? (1) How do you download from Twitter? (2) What is an API (Application Programming Interface), and how does R interface with APIs? (3) How hard is it to store 3.5 billion tweets? (4) How big is a tweet? (5) How do you know which congressional district the person who tweeted was in?\n\nHow much storage does it take to download 3.5 billion tweets? = 2000+ Gb = 2+ Tb (your hard drive is likely 1Tb, unless you have a small computer). Can you explain the billions of tweets stored at Indiana University? How would you randomly sample from the database? One tweet is about 2/3 of a Kb.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvantages\n\nCheap\nCan measure any political race (not just the wealthy ones).\nDisadvantages\n\nIs it really reflective of the voting populace? Who would it bias toward?\nDoes simple mention of a candidate always reflect voting patterns? When wouldn’t it?\nMargin of error of 2.7%. How is that number typically calculated in a poll? Note: \\(2 \\cdot \\sqrt{(1/2)(1/2)/1000} = 0.0316\\).\nTweets feel more free in terms of what you are able to say - is that a good thing or a bad thing with respect to polling?\nCan’t measure any demographic information.\nWhat could be done differently?\n\nGelman: look only at close races\nGelman: “It might make sense to flip it around and predict twitter mentions given candidate popularity. That is, rotate the graph 90 degrees, and see how much variation there is in tweet shares for elections of different degrees of closeness.”\nGelman: “And scale the size of each dot to the total number of tweets for the two candidates in the election.”\nGelman: Make the data publicly available so that others can try to reproduce the results\nTweeting and R\nThe twitter analysis requires a twitter password, and sorry, I won’t give you mine. If you want to download tweets, follow the instructions at http://stats.seandolinar.com/collecting-twitter-data-introduction/ or maybe one of these: https://www.credera.com/blog/business-intelligence/twitter-analytics-using-r-part-1-extract-tweets/ and http://davetang.org/muse/2013/04/06/using-the-r_twitter-package/ and ask me if you have any questions.\n\n\n\nProbability vs. Statistics\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nA schematic of the typical workflow used in data analysis. Most statistics classes focus only on the left side. We will work to address all aspects (including those on the right side). (Baumer 2015)\nStitch Fix Algorithms Tour\nslide taken from Kellie Ottoboni https://github.com/kellieotto/useR2016\nTaken from Modern Drive: An introduction to statistical and data sciences via R, by Ismay and Kim\nJessica Ward, PhD student at Newcastle University\nhttps://xkcd.com/1597/\n\n\n\nBaumer, Ben. 2015. “A Data Science Course for Undergraduates: Thinking with Data.” The American Statistician.\n\n\nDiGrazia, Joseph, Karissa McKelvey, Johan Bollen, and Fabio Rojas. 2013. “More Tweets, More Votes: Social Media as a Quantitative Indicator of Political Behavior.” PLoS ONE 8 (11): e79449.\n\n\nKitzes, Justin, Daniel Turek, and Fatma Deniz, eds. 2018. In The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. University of California Press.\n\n\nRojas, Fabio. 2013. “How Twitter Can Predict and Election.” The Washington Post.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-viz.html",
    "href": "02-viz.html",
    "title": "\n2  Visualization\n",
    "section": "",
    "text": "2.1 Examples\nThe first two examples are taken from a book by Edward Tufte who is arguably the master at visualizations. The book is Visual and Statistical Thinking: Displays of Evidence for Making decisions. The book can be purchased at http://www.edwardtufte.com/tufte/books_textb, though there may be online versions of it that you can download.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#examples",
    "href": "02-viz.html#examples",
    "title": "\n2  Visualization\n",
    "section": "",
    "text": "An aside\nGenerally, the better your graphics are, the better able you will be to communicate ideas broadly (that’s how you become rich and famous). By graphics I mean not only figures associated with analyses, but also power point presentations, posters, and information on your website provided for other scientists who might be interested in your work. Tufte is a master at understanding how to convey information visually, and I strongly recommend you look at his work. Start with Wikipedia where some of his main ideas are provided (e.g., “data-ink ratio”) and then check out his incredible texts. I have many of them in my office and am happy to let you peruse them. http://www.edwardtufte.com/tufte/books_vdqi\nAs mentioned in the booklet we are using, there are two main motivational steps to working with graphics as part of an argument (Tufte 1997).\n\n“An essential analytic task in making decisions based on evidence is to understand how things work.”\nMaking decisions based on evidence requires the appropriate display of that evidence.”\n\nBack to the examples…\n\n2.1.1 Cholera via Tufte\nIn September 1854, the worst outbreak of cholera in London occurred in a few block radius - within 10 days, there were more than 500 fatalities. John Snow recognized the clumping of deaths, and hypothesized that they were due to contamination of the Broad Street water pump. Despite testing the water from the pump and finding no suspicious impurities, he did notice that the water quality varies from data to day. More importantly, there seemed to be no other possible causal mechanism for the outbreak. Eight days after the outbreak began, Snow described his findings to the authorities, and the Board of Guardians of St. James’s Parish ordered the Broad Street pump handle removed. The epidemic ended soon after.\nWhy was John Snow successful at solving the problem? Some thoughts to consider (as reported in Tufte (1997)):\n\nThe bacterium Vibrio cholerae was not discovered until 1886, however Snow had myriad experience both as a medical doctor and in looking at patterns of of other outbreaks. He was the first to realized that cholera was transmitted through water instead of by air or other means.\nData in Context Snow thought carefully about how to present the data. Instead of simply looking at the data as counts or frequencies, he looked at the death spatially - on a map of the area.\nComparisons In order to isolate the pump as the cause of the outbreak, Snow needed to understand how the individuals who had died were different than the individuals who had survived. Snow found two other groups of individuals (brewers who drank only beer, and employees at a work house who had an on-site pump) who had not succumbed to the disease.\nAlternatives Whenever a theory is present, it is vitally important to contrast the theory against all possible alternative possibilities. In Snow’s case, he needed to consider all individuals who did not regularly use the Broad Street pump - he was able to understand the exceptions in every case.\nDid removing the pump handle really cause the outbreak to cease? Wasn’t it already on the decline?\nAssessment of the Graphic Did the individuals die at the place on the map? Live at the place on the map? Which (types of) individuals were missing from the graph? Missing at random? What decisions did he make in creating the graph (axes, binning of histogram bars, time over which data are plotted, etc.) that change the story needing to be told?\n\n2.1.2 Challenger via Tufte\nJohn Snow’s story of the successful graphical intervention in the cholera outbreak is contrasted with the fateful poor-graphical non-intervention of the Challenger disaster. On January 28, 1986, the space shuttle Challenger took off from Cape Canaveral, FL and immediately exploded, killing all seven astronauts aboard. We now know that the reason for the explosion was due to the failure of two rubber O-rings which malfunctioned due to the cold temperature of the day (\\(\\sim 29^\\circ\\) F).\nUnlike the cholera epidemic, those who understood the liability of a shuttle launch under cold conditions were unable to convince the powers that be to postpone the launch (there was much political momentum going forward to get the shuttle off the ground, including the first teacher in space, Christa McAuliffe). As seen in the Tufte chapter, the evidence was clear but not communicated!\nThe biggest problem (existing in many of the bullet points below) is that the engineers failed to as the important question about the data: in relation to what??\n\n\nThe engineers who understood the problem created tables and engineering graphs which were\n\nNot visually appealing.\nNot decipherable to the layman (e.g., “At about \\(50^\\circ\\) F blow-by could be experienced in case joints”)\nThere was also no authorship (reproducibility!). Figures should always have both accountability and reproducibility.\n\n\n\nThe information provided included very relevant points (about temperature) and superfluous information unrelated to temperature. The univariate analysis was insufficient because the story the data were trying to tell was about the bivariate relationship between temperature and o-ring failure.\nMissing data created an illusion of lack of evidence, when in fact, the true story was quite strong given the full set of information. (92% of the temperature data was missing from some of the most vital tables.)\nAnecdotal evidence was misconstrued: SRM-15 at 57F had the most damage, but SRM-22 at 75F had the second most damage.\nIn the end, the shuttle launched on a day which was an extrapolation from the model suggested by the data. They had never launched a shuttle at temperatures of \\(26^\\circ-29^\\circ\\)F.\nTufte goes on to describe many ways which the final presentation by the engineers to the administrators was inadequate: disappearing legend (labels), chartjunk, lack of clarity depicting cause and effect, and wrong order.\n\nAs with the cholera outbreak, a persuasive argument could have been made if the visualizations had\n\nbeen in context plot data versus temperature not time!,\nused appropriate comparisons: as compared with what?,\nconsider alternative scenarios when else did O-rings fail? What is the science behind O-ring failure?, and\nthe graphics had been assessed what is all of the extra noise? are the words being used accessible to non-engineers?.\n\nTufte (Tufte 1997) created the graphic below which should have been used before the launch to convince others to postpone. As you can see, the graphic is extremely convincing. An aside: the O-ring data are well suited for an analysis using logistic regression. Today, most scientists believe that the temperature caused the O-ring failure, however, the data do not speak to the causal relationship because they were not collected using a randomized experiment. That is, there could have been other confounding variables (e.g., humidity) which were possible causal mechanisms.\n\n\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#thoughts",
    "href": "02-viz.html#thoughts",
    "title": "\n2  Visualization\n",
    "section": "\n2.2 Thoughts on Plotting",
    "text": "2.2 Thoughts on Plotting\n\n2.2.1 Advice\n\nBasic plotting\n\nAvoid having other graph elements interfere with data\nUse visually prominent symbols\nAvoid over-plotting (One way to avoid over plotting: Jitter the values)\nDifferent values of data may obscure each other\nInclude all or nearly all of the data\nFill data region\n\n\nEliminate superfluous material\n\nChart junk & stuff that adds no meaning, e.g. butterflies on top of barplots, background images\nExtra tick marks and grid lines\nUnnecessary text and arrows\nDecimal places beyond the measurement error or the level of difference\n\n\nFacilitate Comparisons\n\nPut juxtaposed plots on same scale\nMake it easy to distinguish elements of superposed plots (e.g. color)\nEmphasizes the important difference\nComparison: volume, area, height (be careful, volume can seem bigger than you mean it to)\n\n\nChoosing the Scale (n.b., some of the principles may go counter to one another, use your judgment.)\n\nKeep scales on x and y axes the same for both plots to facilitate the comparison\nZoom in to focus on the region that contains the bulk of the data\nKeep the scale the same throughout the plot (i.e. don’t change it mid-axis)\nOrigin need not be on the scale\nChoose a scale that improves resolution\nAvoid jiggling the baseline\n\n\nHow to make a plot information rich\n\nDescribe what you see in the caption\nAdd context with reference markers (lines and points) including text\nAdd legends and labels\nUse color and plotting symbols to add more information\nPlot the same thing more than once in different ways/scales\nReduce clutter\n\n\nCaptions should\n\nBe comprehensive\nSelf-contained\nDescribe what has been graphed\nDraw attention to important features\nDescribe conclusions drawn from graph\n\n\nGood Plot Making Practice\n\nPut major conclusions in graphical form\nProvide reference information\nProof read for clarity and consistency\nGraphing is an iterative process\nMultiplicity is OK, i.e. two plots of the same variable may provide different messages\nMake plots data rich\n\n\n\nCreating a statistical graphic is an iterative process of discovery and fine tuning. We try to model the process of creating visualizations in the course by dedicating class time to an iterative creation of a plot. We begin either with a plot that screams for correction, and we transform it step-by-step, always thinking about the goal of a graph that is data rich and presents a clear vision of the important features of the data.\n\n2.2.2 An example from Information is Beautiful\n(See HW2 for details on R code)\nConsider the plot at http://www.informationisbeautiful.net/visualizations/caffeine-and-calories/. Note that the origin is at the point (150,150). While we can get over the hurdle, it is not what is expected when looking at a graph.\n\n\n\n\n\n\n\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\n\n\n\nI have removed the vertical and horizontal lines which detracted from the idea of an origin. I have also added additional information (color) to describe the chain from which the drink comes from. Notice that an additional difference between my plot and the original plot is that I have many more observations.\n\n\n\n\n\n\n\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nData retrieved from: https://docs.google.com/spreadsheets/d/1KYMUjrCulPtpUHwep9bVvsBvmVsDEbucdyRZ5uHCDxw/edit?hl=en_GB#gid=0\n\n2.2.2.1 Fonts Matter\nAt RStudio::conf 2020, The Glamour of Graphics, Will Chase makes some very important points about how and why making good graphics matters. The talk might be summarized by the plot below: fonts matter.\n\n\n\n\n\n\n\n\n\n2.2.3 Assessing Graphics (and Other Analyses)\n\n\n\n\n\n\n\n\nCritical Task\nNeeds Improvement\nBasic\nSurpassed\n\n\n\n\nComputation Perform computations\nComputations contain errors and extraneous code\nComputations are correct but contain extraneous / unnecessary computations\nComputations are correct and properly identified and labeled\n\n\n\nAnalysis Choose and carry out analysis appropriate for data and content(s)\nChoice of analysis is overly simplistic, irrelevant, or missing key component\nAnalysis appropriate, but incomplete, or not important features and assumptions not made explicit\nAnalysis appropriate, complete, advanced, relevant, and informative\n\n\n\nSynthesis Identify key features of the analysis, and interpret results (including context)\nConclusions are missing, incorrect, or not made based on results of analysis\nConclusions reasonable, but is partially correct or partially complete\nMake relevant conclusions explicitly connect to analysis and to context\n\n\n\nVisual presentation Communicate findings graphically clearly, precisely, and concisely\nInappropriate choice of plots; poorly labeled plots; plots missing\nPlots convey information correctly but lack context for interpretation\nPlots convey information correctly with adequate / appropriate reference information\n\n\n\nWritten Communicate findings clearly, precisely, and concisely\nExplanation is illogical, incorrect, or incoherent\nExplanation is partially correct but incomplete or unconvincing\nExplanation is correct, complete, and convincing\n\n\n\nA rubric for assessing analysis and corresponding visualization. Note that there can be a large amount of information gained in moving from basic competency to surpassed competency. Table taken from Nolan and Perrett (2016).",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "02-viz.html#deconstruct",
    "href": "02-viz.html#deconstruct",
    "title": "\n2  Visualization\n",
    "section": "\n2.3 Deconstructing a graph",
    "text": "2.3 Deconstructing a graph\n\n2.3.1 The Grammar of Graphics (gg)\nYau (2013) and Wickham (2014) have come up with a taxonomy and a grammar for thinking about the parts of a figure just like we conceptualize the parts of a body or the parts of a sentence.\nOne great way of thinking of the new process: it is not longer necessary to talk about the name of the graph (e.g., boxplot). Instead we now think in glyphs (geoms), and so we can put whatever we want on the plot. Note also that the transition leads you from a passive consumer (I need to make plot XXX because everyone else does, so I just plug in the data) into an active participant (what do I want my data to say? and how can I put that information onto my graphic?)\nThe most important questions you can ask with respect to creating figures are:\n\nWhat do we want R to do? (What is the goal?)\nWhat does R need to know?\n\nYau (2013) gives us nine visual cues, and Wickham (2014) translates them into a language using ggplot2. (The items below are from Baumer, Kaplan, and Horton (2021), chapter 2.)\n\nVisual Cues: the aspects of the figure where we should focus.Position (numerical) where in relation to other things?Length (numerical) how big (in one dimension)?Angle (numerical) how wide? parallel to something else?Direction (numerical) at what slope? In a time series, going up or down?Shape (categorical) belonging to what group?Area (numerical) how big (in two dimensions)? Beware of improper scaling!Volume (numerical) how big (in three dimensions)? Beware of improper scaling!Shade (either) to what extent? how severely?Color (either) to what extent? how severely? Beware of red/green color blindness.\nCoordinate System: rectangular, polar, geographic, etc.\nScale: numeric (linear? logarithmic?), categorical (ordered?), time\nContext: in comparison to what (think back to ideas from Tufte)\n\n\n\n\n\n\n\n\n\nOrder Matters\n\n\n\n\n\n\n\n\nCues Together\n\n\n\n\n\n\n\n\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\nWhat are the visual cues on the plot?\n\n\n\n\n\n\n\n\n\nposition?\nlength?\nshape?\narea/volume?\nshade/color?\ncoordinate System?\n\nscale?\n\n2.3.1.1 The grammar of graphics in ggplot2\n\ngeom: the geometric “shape” used to display data\n\nbar, point, line, ribbon, text, etc.\n\naesthetic: an attribute controlling how geom is displayed with respect to variables\n\nx position, y position, color, fill, shape, size, etc.\n\nscale: adjust information in the aesthetic to map onto the plot\n\n\nparticular assignment of colors, shapes, sizes, etc.; making axes continuous or constrained to a particular range of values.\n\nguide: helps user convert visual data back into raw data (legends, axes)\nstat: a transformation applied to data before geom gets it\n\nexample: histograms work on binned data\n\n2.3.2 ggplot2\n\nIn ggplot2, an aesthetic refers to a mapping between a variable and the information it conveys on the plot. Further information about plotting and visualizing information is given in chapter 2 (Data visualization) of Baumer, Kaplan, and Horton (2021). Much of the data in the presentation represents all births from 1978 in the US: the date, the day of the year, and the number of births.\n\nGoals\nWhat I will try to do\n\ngive a tour of ggplot2\nexplain how to think about plots the ggplot2 way\nprepare/encourage you to learn more later\n\nWhat I can’t do in one session\n\nshow every bell and whistle\nmake you an expert at using ggplot2\nGetting help\n\nOne of the best ways to get started with ggplot is to google what you want to do with the word ggplot. Then look through the images that come up. More often than not, the associated code is there. There are also ggplot galleries of images, one of them is here: https://plot.ly/ggplot2/\nggplot2 cheat sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf\nLook at the end of the presentation. More help options there.\n\n\n\n\n\n\n\n\n\n\nrequire(mosaic)\nrequire(lubridate) # package for working with dates\ndata(Births78)     # restore fresh version of Births78\nhead(Births78, 3)\n\n        date births wday year month day_of_year day_of_month day_of_week\n1 1978-01-01   7701  Sun 1978     1           1            1           1\n2 1978-01-02   7527  Mon 1978     1           2            2           2\n3 1978-01-03   8825  Tue 1978     1           3            3           3\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nTwo Questions:\n\nWhat do we want R to do? (What is the goal?)\n\nWhat does R need to know?\n\ndata source: Births78\n\naesthetics:\n\ndate -&gt; x\nbirths -&gt; y\npoints (!)\n\n\n\n\n\nGoal: scatterplot = a plot with points\n\nggplot() + geom_point()\n\n\n\nWhat does R need to know?\n\ndata source: data = Births78\naesthetics: aes(x = date, y = births)\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nWhat has changed?\n\nnew aesthetic: mapping color to day of week\nAdding day of week to the data set\nThe wday() function in the lubridate package computes the day of the week from a date.\n\nBirths78 &lt;-  \n  Births78 %&gt;% \n  mutate(wday = lubridate::wday(date, label=TRUE))\n\n\nggplot(data=Births78) +\n  geom_point(aes(x=date, y=births, color=wday))+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we use lines instead of dots\n\nggplot(data=Births78) +\n  geom_line(aes(x=date, y=births, color=wday)) +\n  ggtitle(\"US Births in 1978\")\n\nHow can we make the plot?\n\n\n\n\n\n\n\n\nNow we have two layers, one with points and one with lines\n\nggplot(data=Births78, \n       aes(x=date, y=births, color=wday)) + \n  geom_point() +  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\nThe layers are placed one on top of the other: the points are below and the lines are above.\ndata and aes specified in ggplot() affect all geoms\nAlternative Syntax\n\nBirths78 %&gt;% \n  ggplot(aes(x=date, y=births, color=wday)) + \n  geom_point() + \n  geom_line()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nWhat does adding the color argument do?\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, color=\"navy\")) + \n  geom_point()  +\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\n\nBecause there is no variable, we have mapped the color aesthetic to a new variable with only one value (“navy”). So all the dots get set to the same color, but it’s not navy.\nSetting vs. Mapping\nIf we want to set the color to be navy for all of the dots, we do it outside the aesthetic, without a dataset variable:\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) +   # map x & y \n  geom_point(color = \"navy\")   +     # set color\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\n\nNote that color = \"navy\" is now outside of the aesthetics list. That’s how ggplot2 distinguishes between mapping and setting.\nHow can we make the plot?\n\n\n\n\n\n\n\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births)) + \n  geom_line(aes(color=wday)) +       # map color here\n  geom_point(color=\"navy\") +          # set color here\n  ggtitle(\"US Births in 1978\")\n\n\nggplot() establishes the default data and aesthetics for the geoms, but each geom may change the defaults.\ngood practice: put into ggplot() the things that affect all (or most) of the layers; rest in geom_blah()\nSetting vs. Mapping (again)\nInformation gets passed to the plot via:\n\nmap the variable information inside the aes (aesthetic) command\nset the non-variable information outside the aes (aesthetic) command\nOther geoms\n\napropos(\"^geom_\")\n\n [1] \"geom_abline\"            \"geom_area\"              \"geom_ash\"              \n [4] \"geom_bar\"               \"geom_bin_2d\"            \"geom_bin2d\"            \n [7] \"geom_blank\"             \"geom_boxplot\"           \"geom_col\"              \n[10] \"geom_contour\"           \"geom_contour_filled\"    \"geom_count\"            \n[13] \"geom_crossbar\"          \"geom_curve\"             \"geom_density\"          \n[16] \"geom_density_2d\"        \"geom_density_2d_filled\" \"geom_density2d\"        \n[19] \"geom_density2d_filled\"  \"geom_dotplot\"           \"geom_errorbar\"         \n[22] \"geom_errorbarh\"         \"geom_freqpoly\"          \"geom_function\"         \n[25] \"geom_hex\"               \"geom_histogram\"         \"geom_hline\"            \n[28] \"geom_jitter\"            \"geom_label\"             \"geom_line\"             \n[31] \"geom_linerange\"         \"geom_lm\"                \"geom_map\"              \n[34] \"geom_path\"              \"geom_point\"             \"geom_pointrange\"       \n[37] \"geom_polygon\"           \"geom_qq\"                \"geom_qq_line\"          \n[40] \"geom_quantile\"          \"geom_raster\"            \"geom_rect\"             \n[43] \"geom_ribbon\"            \"geom_rug\"               \"geom_segment\"          \n[46] \"geom_sf\"                \"geom_sf_label\"          \"geom_sf_text\"          \n[49] \"geom_smooth\"            \"geom_spline\"            \"geom_spoke\"            \n[52] \"geom_step\"              \"geom_text\"              \"geom_tile\"             \n[55] \"geom_violin\"            \"geom_vline\"            \n\n\nhelp pages will tell you their aesthetics, default stats, etc.\n\n?geom_area             # for example\n\nLet’s try geom_area\n\n\nBirths78 %&gt;%\n  ggplot(aes(x=date, y=births, fill=wday)) + \n  geom_area()+\n  ggtitle(\"US Births in 1978\")\n\n\n\n\n\n\n\nUsing area does not produce a good plot\n\nover plotting is hiding much of the data\nextending y-axis to 0 may or may not be desirable.\nSide note: what makes a plot good?\nMost (all?) graphics are intended to help us make comparisons\n\nHow does something change over time?\nDo my treatments matter? How much?\nDo men and women respond the same way?\n\nKey plot metric: Does my plot make the comparisons I am interested in\n\neasily, and\naccurately?\nTime for some different data\nHELPrct: Health Evaluation and Linkage to Primary care randomized clinical trial\n\nhead(HELPrct)\n\n  age anysubstatus anysub cesd d1 daysanysub dayslink drugrisk e2b female\n1  37            1    yes   49  3        177      225        0  NA      0\n2  37            1    yes   30 22          2       NA        0  NA      0\n3  26            1    yes   39  0          3      365       20  NA      0\n4  39            1    yes   15  2        189      343        0   1      1\n5  32            1    yes   39 12          2       57        0   1      0\n6  47            1    yes    6  1         31      365        0  NA      1\n     sex g1b homeless i1 i2 id indtot linkstatus link       mcs      pcs pss_fr\n1   male yes   housed 13 26  1     39          1  yes 25.111990 58.41369      0\n2   male yes homeless 56 62  2     43         NA &lt;NA&gt; 26.670307 36.03694      1\n3   male  no   housed  0  0  3     41          0   no  6.762923 74.80633     13\n4 female  no   housed  5  5  4     28          0   no 43.967880 61.93168     11\n5   male  no homeless 10 13  5     38          1  yes 21.675755 37.34558     10\n6 female  no   housed  4  4  6     29          0   no 55.508991 46.47521      5\n  racegrp satreat sexrisk substance treat avg_drinks max_drinks\n1   black      no       4   cocaine   yes         13         26\n2   white      no       7   alcohol   yes         56         62\n3   black      no       2    heroin    no          0          0\n4   white     yes       4    heroin    no          5          5\n5   black      no       6   cocaine    no         10         13\n6   black      no       5   cocaine   yes          4          4\n  hospitalizations\n1                3\n2               22\n3                0\n4                2\n5               12\n6                1\n\n\nSubjects admitted for treatment for addiction to one of three substances.\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\nHmm. What’s up with y?\n\n\nstat_bin() is being applied to the data before the geom_bar() gets to do its thing. Binning creates the y values.\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nlibrary(scales)\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar() +\n  scale_y_continuous(labels = percent)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWho are the people in the study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, fill=sex)) + \n  geom_bar(position=\"fill\") +\n  scale_y_continuous(\"actually, percent\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study?\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nNotice the messages\n\nstat_bin: Histograms are not mapping the raw data but binned data.stat_bin() performs the data transformation.\nbinwidth: a default binwidth has been selected, but we should really choose our own.\nSetting the binwidth manually\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_histogram(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow old are people in the HELP study? – Other geoms\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_freqpoly(binwidth=2)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery geom comes with a default stat\n\nfor simple cases, the stat is stat_identity() which does nothing\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nSelecting stat and geom manually\nEvery stat comes with a default geom, every geom with a default stat\n\nwe can specify stats instead of geom, if we prefer\nwe can mix and match geoms and stats however we like\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  stat_density( geom=\"line\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMore combinations\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3) + \n  geom_line(stat=\"bin\", binwidth=3)  +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_area(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\") \n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=age)) + \n  geom_point(stat=\"bin\", binwidth=3, aes(size=..count..)) +\n  geom_line(stat=\"bin\", binwidth=3) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHow much do they drink? (i1)\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_histogram()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_density()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1)) + geom_area(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nCovariates: Adding in more variables\nUsing color and linetype:\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance, linetype=sex)) + \n  geom_line(stat=\"density\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nUsing color and facets\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( . ~ sex )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\nHELPrct %&gt;% \n  ggplot(aes(x=i1, color=substance)) + \n  geom_line(stat=\"density\") + facet_grid( sex ~ . )+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nBoxplots\nBoxplots use stat_quantile() which computes a five-number summary (roughly the five quartiles of the data) and uses them to define a “box” and “whiskers”.\nThe quantitative variable must be y, and there must be an additional x variable.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nHorizontal boxplots\nHorizontal boxplots are obtained by flipping the coordinate system:\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\n\n\ncoord_flip() may be used with other plots as well to reverse the roles of x and y on the plot.\nAxes scaling with boxplots\nWe can scale the continuous axis\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot() +\n  coord_trans(y=\"log\")+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nGive me some space\nWe’ve triggered a new feature: dodge (for dodging things left/right). We can control how much if we set the dodge manually.\n\nHELPrct %&gt;% \n  ggplot(aes(x=substance, y=age, color=sex)) + \n  geom_boxplot(position=position_dodge(width=1)) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nIssues with bigger data\n\nrequire(NHANES)\ndim(NHANES)\n\n[1] 10000    76\n\nNHANES %&gt;%  ggplot(aes(x=Height, y=Weight)) +\n  geom_point() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\n\nAlthough we can see a generally positive association (as we would expect), the over plotting may be hiding information.\nUsing alpha (opacity)\nOne way to deal with over plotting is to set the opacity low.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_point(alpha=0.01) + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\ngeom_density2d\nAlternatively (or simultaneously) we might prefer a different geom altogether.\n\nNHANES %&gt;% \n  ggplot(aes(x=Height, y=Weight)) +\n  geom_density2d() + facet_grid( Gender ~ PregnantNow ) +\n  ggtitle(\"National Health and Nutrition Examination Survey\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_jitter(alpha=.6) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nMultiple layers\n\nggplot( data=HELPrct, aes(x=sex, y=age)) +\n  geom_boxplot(outlier.size=0) +\n  geom_point(alpha=.6, position=position_jitter(width=.1, height=0)) +\n  coord_flip()+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nThings I haven’t mentioned (much)\n\ncoords (coord_flip() is good to know about)\nthemes (for customizing appearance)\nposition (position_dodge(), position_jitterdodge(), position_stack(), etc.)\ntransforming axes\n\n\nrequire(ggthemes)\nggplot(Births78, aes(x=date, y=births)) + geom_point() + \n          theme_wsj()\n\n\n\n\n\n\n\n\nggplot(data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge()) +\n  geom_point(aes(color=sex, fill=sex), position=position_jitterdodge()) +\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nA little bit of everything\n\nggplot( data=HELPrct, aes(x=substance, y=age, color=sex)) +\n  geom_boxplot(coef = 10, position=position_dodge(width=1)) +\n  geom_point(aes(fill=sex), alpha=.5, \n             position=position_jitterdodge(dodge.width=1)) + \n  facet_wrap(~homeless)+\n  ggtitle(\"HELP clinical trial at detoxification unit\")\n\n\n\n\n\n\n\nWant to learn more?\n\ndocs.ggplot2.org/\nWinston Chang’s: R Graphics Cookbook\n\n\n\n\n\n\n\n\n\nWhat else can we do?\nshiny\n\ninteractive graphics / modeling\nhttps://shiny.rstudio.com/\n\nplotly\n\nPlotly is an R package for creating interactive web-based graphs via plotly’s JavaScript graphing library, plotly.js. The plotly R library contains the ggplotly function , which will convert ggplot2 figures into a Plotly object. Furthermore, you have the option of manipulating the Plotly object with the style function.\n\n\nhttps://plot.ly/ggplot2/getting-started/\n\nDynamic documents\n\ncombination of RMarkdown, ggvis, and shiny\n\n\n\n\n\nThe graphic the engineers should have led with in trying to persuade the administrators not to launch. It is evident that the number of O-ring failures is quite highly associated with the ambient temperature. Note the vital information on the x-axis associated with the large number of launches at warm temperatures that had zero O-ring failures. (Tufte 1997)\nhttp://infobeautiful3.s3.amazonaws.com/2013/01/1276_buzz_v_bulge.png\nCalories and Caffeine for drinks from various drinks and other items. Data source is: World Cancer Research Fund, Starbucks Beverage Nutrition Guide, Calorie Counter Database. Seemingly, the observational units (rows) are not a random sample of anything. As such, we should be careful of summarizing the data in any way - what would the ‘average’ calories even mean? Note, from the entire dataset give, the average calories is 179.8 and the average caffeine is 134.43. How do those numbers compare to the original plot?\n\n\n\nBaumer, Ben, Daniel Kaplan, and Nicholas Horton. 2021. Modern Data Science with r. CRC Press. https://mdsr-book.github.io/mdsr2e/.\n\n\nGelman, Andrew. 2011. “Rejoinder.” Journal of Computational and Graphical Statistics 20: 36–40. http://arxiv.org/abs/1503.00781.\n\n\nNolan, Deborah, and Jamis Perrett. 2016. “Teaching and Learning Data Visualization: Ideas and Assignments.” The American Statistician.\n\n\nTufte, Edward. 1997. “Visual Explanations: Images and Quantities, Evidence and Narrative.” In, 27–54. Graphics Press, LLC. www.edwardtufte.com.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.\n\n\nYau, Nathan. 2013. Data Points: Visualization That Means Something. Wiley.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html",
    "href": "03-wrangling.html",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "3.1 Structure of Data\nFor plotting, analyses, model building, etc., it’s important that the data be structured in a very particular way. Hadley Wickham provides a thorough discussion and advice for cleaning up the data in Wickham (2014).\nThe Active Duty data are not tidy! What are the cases? How are the data not tidy? What might the data look like in tidy form? Suppose that the case was “an individual in the armed forces.” What variables would you use to capture the information in the following table?\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1811988794\nProblem: totals and different sheets\nBetter for R: longer format with columns - grade, gender, status, service, count (case is still the total pay grade)\nCase is individual (?): grade, gender, status, service (no count because each row does the counting)",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#datastruc",
    "href": "03-wrangling.html#datastruc",
    "title": "3  Data Wrangling",
    "section": "",
    "text": "Tidy Data: rows (cases/observational units) and columns (variables). The key is that every row is a case and *every} column is a variable. No exceptions.\nCreating tidy data is not trivial. We work with objects (often data tables), functions, and arguments (often variables).\n\n\n\n\n\n\n\n\n3.1.1 Building Tidy Data\nWithin R (really within any type of computing language, Python, SQL, Java, etc.), we need to understand how to build data using the patterns of the language. Some things to consider:\n\n\nobject_name = function_name(arguments) is a way of using a function to create a new object.\n\nobject_name = data_table %&gt;% function_name(arguments) uses chaining syntax as an extension of the ideas of functions. In chaining, the value on the left side of %&gt;% becomes the first argument to the function on the right side.\n\nobject_name = data_table %&gt;%\nfunction_name(arguments) %&gt;% \nfunction_name(arguments)\nis extended chaining. %&gt;% is never at the front of the line, it is always connecting one idea with the continuation of that idea on the next line. * In R, all functions take arguments in round parentheses (as opposed to subsetting observations or variables from data objects which happen with square parentheses). Additionally, the spot to the left of %&gt;% is always a data table. * The pipe syntax should be read as then, %&gt;%.\n\n3.1.2 Examples of Chaining\nThe pipe syntax (%&gt;%) takes a data frame (or data table) and sends it to the argument of a function. The mapping goes to the first available argument in the function. For example:\nx %&gt;% f(y) is the same as f(x, y)\ny %&gt;% f(x, ., z) is the same as f(x,y,z)\n\n3.1.2.1 Little Bunny Foo Foo\nFrom Hadley Wickham, how to think about tidy data.\n\nLittle bunny Foo Foo Went hopping through the forest Scooping up the field mice And bopping them on the head\n\nThe nursery rhyme could be created by a series of steps where the output from each step is saved as an object along the way.\nfoo_foo &lt;- little_bunny()\nfoo_foo_1 &lt;- hop(foo_foo, through = forest)\nfoo_foo_2 &lt;- scoop(foo_foo_2, up = field_mice)\nfoo_foo_3 &lt;- bop(foo_foo_2, on = head)\nAnother approach is to concatenate the functions so that there is only one output.\nbop(\n   scoop(\n      hop(foo_foo, through = forest),\n      up = field_mice),\n   on = head)\nOr even worse, as one line:\nbop(scoop(hop(foo_foo, through = forest), up = field_mice), on = head)))\nInstead, the code can be written using the pipe in the order in which the function is evaluated:\nfoo_foo %&gt;%\n   hop(through = forest) %&gt;%\n       scoop(up = field_mice) %&gt;%\n           bop(on = head)\nbabynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2014, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys.\nThe babynames data table in the babynames package comes from the Social Security Administration’s listing of the names givens to babies in each year, and the number of babies of each sex given that name. (Only names with 5 or more babies are published by the SSA.)\n\n3.1.3 Data Verbs (on single data frames)\n\nSuper important resource: The RStudio dplyr cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf\n\nData verbs take data tables as input and give data tables as output (that’s how we can use the chaining syntax!). We will use the R package dplyr to do much of our data wrangling. Below is a list of verbs which will be helpful in wrangling many different types of data. See the Data Wrangling cheat sheet from RStudio for additional help. https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf\n\nsample_n() take a random row(s)\nhead() grab the first few rows\ntail() grab the last few rows\nfilter() removes unwanted cases\narrange() reorders the cases\nselect() removes unwanted variables (and rename() )\ndistinct() returns the unique values in a table\nmutate() transforms the variable (and transmute() like mutate, returns only new variables)\ngroup_by() tells R that SUCCESSIVE functions keep in mind that there are groups of items. So group_by() only makes sense with verbs later on (like summarize()).\n\nsummarize() collapses a data frame to a single row. Some functions that are used within summarize() include:\n\n\nmin(), max(), mean(), sum(), sd(), median(), and IQR()\n\n\nn(): number of observations in the current group\n\nn_distinct(x): count the number of unique values in x\n\n\nfirst_value(x), last_value(x) and nth_value(x, n): work similarly to x[1], x[length(x)], and x[n]",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#r-examples-basic-verbs",
    "href": "03-wrangling.html#r-examples-basic-verbs",
    "title": "3  Data Wrangling",
    "section": "\n3.2 R examples, basic verbs",
    "text": "3.2 R examples, basic verbs\n\n3.2.1 Datasets\nstarwars is from dplyr , although originally from SWAPI, the Star Wars API, http://swapi.co/.\nNHANES From ?NHANES: NHANES is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC).\nbabynames Each year, the US Social Security Administration publishes a list of the most popular names given to babies. In 2018, http://www.ssa.gov/oact/babynames/#ht=2 shows Emma and Olivia leading for girls, Noah and Liam for boys. (Only names with 5 or more babies are published by the SSA.)\n\n3.2.2 Examples of Chaining\n\nlibrary(babynames)\nbabynames %&gt;% nrow()\n\n[1] 1924665\n\nbabynames %&gt;% names()\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\nbabynames %&gt;% glimpse()\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\nbabynames %&gt;% head()\n\n# A tibble: 6 × 5\n   year sex   name          n   prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;\n1  1880 F     Mary       7065 0.0724\n2  1880 F     Anna       2604 0.0267\n3  1880 F     Emma       2003 0.0205\n4  1880 F     Elizabeth  1939 0.0199\n5  1880 F     Minnie     1746 0.0179\n6  1880 F     Margaret   1578 0.0162\n\nbabynames %&gt;% tail()\n\n# A tibble: 6 × 5\n   year sex   name       n       prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt;\n1  2017 M     Zyhier     5 0.00000255\n2  2017 M     Zykai      5 0.00000255\n3  2017 M     Zykeem     5 0.00000255\n4  2017 M     Zylin      5 0.00000255\n5  2017 M     Zylis      5 0.00000255\n6  2017 M     Zyrie      5 0.00000255\n\nbabynames %&gt;% sample_n(size=5)\n\n# A tibble: 5 × 5\n   year sex   name        n       prop\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1  2002 F     Sueann     11 0.00000557\n2  1949 F     Cleda      12 0.00000684\n3  1958 M     Pinkney     6 0.00000279\n4  1981 M     Denard     26 0.0000140 \n5  1994 M     Bodie      28 0.0000137 \n\nbabynames %&gt;% mosaic::favstats(n ~ sex, data = .)\n\n  sex min Q1 median Q3   max     mean       sd       n missing\n1   F   5  7     11 31 99686 151.4294 1180.557 1138293       0\n2   M   5  7     12 33 94756 223.4940 1932.338  786372       0\n\n\n\n3.2.3 Data Verbs\nTaken from the dplyr tutorial: http://dplyr.tidyverse.org/\n\n3.2.3.1 Starwars\n\nlibrary(dplyr)\n\nstarwars %&gt;% dim()\n\n[1] 87 14\n\nstarwars %&gt;% names()\n\n [1] \"name\"       \"height\"     \"mass\"       \"hair_color\" \"skin_color\"\n [6] \"eye_color\"  \"birth_year\" \"sex\"        \"gender\"     \"homeworld\" \n[11] \"species\"    \"films\"      \"vehicles\"   \"starships\" \n\nstarwars %&gt;% head()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n4 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n5 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n6 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;%\n  mosaic::favstats(mass~gender, data = .)\n\n     gender min   Q1 median   Q3  max      mean         sd  n missing\n1  feminine  45 50.0     55 56.2   75  54.68889   8.591921  9       8\n2 masculine  15 74.5     80 87.5 1358 106.51489 188.924092 47      19\n\nstarwars %&gt;% \n  dplyr::filter(species == \"Droid\")\n\n# A tibble: 6 × 14\n  name   height  mass hair_color skin_color  eye_color birth_year sex   gender  \n  &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   \n1 C-3PO     167    75 &lt;NA&gt;       gold        yellow           112 none  masculi…\n2 R2-D2      96    32 &lt;NA&gt;       white, blue red               33 none  masculi…\n3 R5-D4      97    32 &lt;NA&gt;       white, red  red               NA none  masculi…\n4 IG-88     200   140 none       metal       red               15 none  masculi…\n5 R4-P17     96    NA none       silver, red red, blue         NA none  feminine\n6 BB8        NA    NA none       none        black             NA none  masculi…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;% \n  dplyr::filter(species != \"Droid\") %&gt;%\n  mosaic::favstats(mass~gender, data = .)\n\n     gender min Q1 median   Q3  max      mean         sd  n missing\n1  feminine  45 50     55 56.2   75  54.68889   8.591921  9       7\n2 masculine  15 76     80 87.5 1358 109.93488 196.887934 43      18\n\nstarwars %&gt;% \n  dplyr::select(name, ends_with(\"color\"))\n\n# A tibble: 87 × 4\n   name               hair_color    skin_color  eye_color\n   &lt;chr&gt;              &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 Luke Skywalker     blond         fair        blue     \n 2 C-3PO              &lt;NA&gt;          gold        yellow   \n 3 R2-D2              &lt;NA&gt;          white, blue red      \n 4 Darth Vader        none          white       yellow   \n 5 Leia Organa        brown         light       brown    \n 6 Owen Lars          brown, grey   light       blue     \n 7 Beru Whitesun Lars brown         light       blue     \n 8 R5-D4              &lt;NA&gt;          white, red  red      \n 9 Biggs Darklighter  black         light       brown    \n10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n# ℹ 77 more rows\n\nstarwars %&gt;% \n  dplyr::mutate(name, bmi = mass / ((height / 100)  ^ 2)) %&gt;%\n  dplyr::select(name:mass, bmi)\n\n# A tibble: 87 × 4\n   name               height  mass   bmi\n   &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Luke Skywalker        172    77  26.0\n 2 C-3PO                 167    75  26.9\n 3 R2-D2                  96    32  34.7\n 4 Darth Vader           202   136  33.3\n 5 Leia Organa           150    49  21.8\n 6 Owen Lars             178   120  37.9\n 7 Beru Whitesun Lars    165    75  27.5\n 8 R5-D4                  97    32  34.0\n 9 Biggs Darklighter     183    84  25.1\n10 Obi-Wan Kenobi        182    77  23.2\n# ℹ 77 more rows\n\nstarwars %&gt;% \n  dplyr::arrange(desc(mass))\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Jabba D…    175  1358 &lt;NA&gt;       green-tan… orange         600   herm… mascu…\n 2 Grievous    216   159 none       brown, wh… green, y…       NA   male  mascu…\n 3 IG-88       200   140 none       metal      red             15   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Tarfful     234   136 brown      brown      blue            NA   male  mascu…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Bossk       190   113 none       green      red             53   male  mascu…\n 8 Chewbac…    228   112 brown      unknown    blue           200   male  mascu…\n 9 Jek Ton…    180   110 brown      fair       blue            NA   &lt;NA&gt;  &lt;NA&gt;  \n10 Dexter …    198   102 none       brown      yellow          NA   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\nstarwars %&gt;%\n  dplyr::group_by(species) %&gt;%\n  dplyr::summarize(\n    num = n(),\n    mass = mean(mass, na.rm = TRUE)\n  ) %&gt;%\n  dplyr::filter(num &gt; 1)\n\n# A tibble: 9 × 3\n  species    num  mass\n  &lt;chr&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Droid        6  69.8\n2 Gungan       3  74  \n3 Human       35  81.3\n4 Kaminoan     2  88  \n5 Mirialan     2  53.1\n6 Twi'lek      2  55  \n7 Wookiee      2 124  \n8 Zabrak       2  80  \n9 &lt;NA&gt;         4  81  \n\n\n\n3.2.3.2 NHANES\n\nrequire(NHANES)\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n# find the sleep variables\nNHANESsleep &lt;- NHANES %&gt;% select(Gender, Age, Weight, Race1, Race3, \n                                 Education, SleepTrouble, SleepHrsNight, \n                                 TVHrsDay, TVHrsDayChild, PhysActive)\nnames(NHANESsleep)\n\n [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"   \n\ndim(NHANESsleep)\n\n[1] 10000    11\n\n# subset for college students\nNHANESsleep &lt;- NHANESsleep %&gt;% filter(Age %in% c(18:22)) %&gt;% \n  mutate(Weightlb = Weight*2.2)\n\nnames(NHANESsleep)\n\n [1] \"Gender\"        \"Age\"           \"Weight\"        \"Race1\"        \n [5] \"Race3\"         \"Education\"     \"SleepTrouble\"  \"SleepHrsNight\"\n [9] \"TVHrsDay\"      \"TVHrsDayChild\" \"PhysActive\"    \"Weightlb\"     \n\ndim(NHANESsleep)\n\n[1] 655  12\n\nNHANESsleep %&gt;% ggplot(aes(x=Age, y=SleepHrsNight, color=Gender)) + \n  geom_point(position=position_jitter(width=.25, height=0) ) + \n  facet_grid(SleepTrouble ~ TVHrsDay) \n\n\n\n\n\n\n\n\n3.2.4 summarize and group_by\n\n\n# number of people (cases) in NHANES\nNHANES %&gt;% summarize(n())\n\n# A tibble: 1 × 1\n  `n()`\n  &lt;int&gt;\n1 10000\n\n# total weight of all the people in NHANES (silly)\nNHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(sum(Weightlb, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `sum(Weightlb, na.rm = TRUE)`\n                          &lt;dbl&gt;\n1                      1549419.\n\n# mean weight of all the people in NHANES\nNHANES %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 1 × 1\n  `mean(Weightlb, na.rm = TRUE)`\n                           &lt;dbl&gt;\n1                           156.\n\n# repeat the above but for groups\n\n# males versus females\nNHANES %&gt;% group_by(Gender) %&gt;% summarize(n())\n\n# A tibble: 2 × 2\n  Gender `n()`\n  &lt;fct&gt;  &lt;int&gt;\n1 female  5020\n2 male    4980\n\nNHANES %&gt;% group_by(Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 2 × 2\n  Gender `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                           &lt;dbl&gt;\n1 female                           146.\n2 male                             167.\n\n# smokers and non-smokers\nNHANES %&gt;% group_by(SmokeNow) %&gt;% summarize(n())\n\n# A tibble: 3 × 2\n  SmokeNow `n()`\n  &lt;fct&gt;    &lt;int&gt;\n1 No        1745\n2 Yes       1466\n3 &lt;NA&gt;      6789\n\nNHANES %&gt;% group_by(SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  SmokeNow `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 No                                 186.\n2 Yes                                177.\n3 &lt;NA&gt;                               144.\n\n# people with and without diabetes\nNHANES %&gt;% group_by(Diabetes) %&gt;% summarize(n())\n\n# A tibble: 3 × 2\n  Diabetes `n()`\n  &lt;fct&gt;    &lt;int&gt;\n1 No        9098\n2 Yes        760\n3 &lt;NA&gt;       142\n\nNHANES %&gt;% group_by(Diabetes) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 3 × 2\n  Diabetes `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;                             &lt;dbl&gt;\n1 No                                155. \n2 Yes                               202. \n3 &lt;NA&gt;                               21.6\n\n# break down the smokers versus non-smokers further, by sex\nNHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% summarize(n())\n\n# A tibble: 6 × 3\n# Groups:   SmokeNow [3]\n  SmokeNow Gender `n()`\n  &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt;\n1 No       female   764\n2 No       male     981\n3 Yes      female   638\n4 Yes      male     828\n5 &lt;NA&gt;     female  3618\n6 &lt;NA&gt;     male    3171\n\nNHANES %&gt;% group_by(SmokeNow, Gender) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 6 × 3\n# Groups:   SmokeNow [3]\n  SmokeNow Gender `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;    &lt;fct&gt;                           &lt;dbl&gt;\n1 No       female                           167.\n2 No       male                             201.\n3 Yes      female                           167.\n4 Yes      male                             185.\n5 &lt;NA&gt;     female                           138.\n6 &lt;NA&gt;     male                             151.\n\n# break down the people with diabetes further, by smoking\nNHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% summarize(n())\n\n# A tibble: 8 × 3\n# Groups:   Diabetes [3]\n  Diabetes SmokeNow `n()`\n  &lt;fct&gt;    &lt;fct&gt;    &lt;int&gt;\n1 No       No        1476\n2 No       Yes       1360\n3 No       &lt;NA&gt;      6262\n4 Yes      No         267\n5 Yes      Yes        106\n6 Yes      &lt;NA&gt;       387\n7 &lt;NA&gt;     No           2\n8 &lt;NA&gt;     &lt;NA&gt;       140\n\nNHANES %&gt;% group_by(Diabetes, SmokeNow) %&gt;% mutate(Weightlb = Weight*2.2) %&gt;% \n  summarize(mean(Weightlb, na.rm=TRUE))\n\n# A tibble: 8 × 3\n# Groups:   Diabetes [3]\n  Diabetes SmokeNow `mean(Weightlb, na.rm = TRUE)`\n  &lt;fct&gt;    &lt;fct&gt;                             &lt;dbl&gt;\n1 No       No                                183. \n2 No       Yes                               175. \n3 No       &lt;NA&gt;                              143. \n4 Yes      No                                204. \n5 Yes      Yes                               204. \n6 Yes      &lt;NA&gt;                              199. \n7 &lt;NA&gt;     No                                193. \n8 &lt;NA&gt;     &lt;NA&gt;                               19.1\n\n\n\n3.2.5 babynames\n\nbabynames %&gt;% group_by(sex) %&gt;%\n  summarize(total=sum(n))\n\n# A tibble: 2 × 2\n  sex       total\n  &lt;chr&gt;     &lt;int&gt;\n1 F     172371079\n2 M     175749438\n\nbabynames %&gt;% group_by(year, sex) %&gt;%\n  summarize(name_count = n_distinct(name)) %&gt;% head()\n\n# A tibble: 6 × 3\n# Groups:   year [3]\n   year sex   name_count\n  &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;\n1  1880 F            942\n2  1880 M           1058\n3  1881 F            938\n4  1881 M            997\n5  1882 F           1028\n6  1882 M           1099\n\nbabynames %&gt;% group_by(year, sex) %&gt;%\n  summarize(name_count = n_distinct(name)) %&gt;% tail()\n\n# A tibble: 6 × 3\n# Groups:   year [3]\n   year sex   name_count\n  &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt;\n1  2015 F          19074\n2  2015 M          14024\n3  2016 F          18817\n4  2016 M          14162\n5  2017 F          18309\n6  2017 M          14160\n\nbabysamp &lt;- babynames %&gt;% sample_n(size=50)\nbabysamp %&gt;% select(year) %&gt;% distinct() %&gt;% table()\n\nyear\n1882 1896 1916 1917 1922 1923 1924 1927 1928 1931 1943 1960 1961 1962 1967 1968 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n1969 1974 1977 1979 1981 1982 1985 1987 1989 1991 1992 1993 1994 1997 1998 2001 \n   1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1 \n2002 2004 2006 2011 2013 2015 2016 \n   1    1    1    1    1    1    1 \n\nbabysamp %&gt;% distinct() %&gt;% select(year) %&gt;% table()\n\nyear\n1882 1896 1916 1917 1922 1923 1924 1927 1928 1931 1943 1960 1961 1962 1967 1968 \n   1    1    1    1    1    1    1    1    1    1    3    2    2    1    1    1 \n1969 1974 1977 1979 1981 1982 1985 1987 1989 1991 1992 1993 1994 1997 1998 2001 \n   1    1    1    1    2    2    1    1    1    2    1    2    2    1    1    2 \n2002 2004 2006 2011 2013 2015 2016 \n   1    1    1    1    1    2    1 \n\nFrances &lt;- babynames %&gt;%\n  filter(name== \"Frances\") %&gt;%\n  group_by(year, sex) %&gt;%\n  summarize(yrTot = sum(n))\n\nFrances %&gt;% ggplot(aes(x=year, y=yrTot)) +\n  geom_point(aes(color=sex)) + \n  geom_vline(xintercept=2006) + scale_y_log10() +\n  ylab(\"Yearly total on log10 scale\")",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#highverb",
    "href": "03-wrangling.html#highverb",
    "title": "3  Data Wrangling",
    "section": "\n3.3 Higher Level Data Verbs",
    "text": "3.3 Higher Level Data Verbs\nThere are more complicated verbs which may be important for more sophisticated analyses. See the RStudio dplyr cheat sheet, https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}.\n\n\npivot_longer makes many columns into 2 columns: pivot_longer(data, cols,  names_to = , value_to = )\n\n\npivot_wider makes one column into multiple columns: pivot_wider(data, names_from = , values_from = )\n\n\nleft_join returns all rows from the left table, and any rows with matching keys from the right table.\n\ninner_join returns only the rows in which the left table have matching keys in the right table (i.e., matching rows in both sets).\n\nfull_join returns all rows from both tables, join records from the left which have matching keys in the right table.\n\nGood practice: always specify the by argument when joining data frames.\n\n\nIf you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#r-examples-higher-level-verbs",
    "href": "03-wrangling.html#r-examples-higher-level-verbs",
    "title": "3  Data Wrangling",
    "section": "\n3.4 R examples, higher level verbs",
    "text": "3.4 R examples, higher level verbs\n\ntidyr 1.0.0 has just been released! The new release means that you need to update tidyr. You will know if you have the latest version if the following command works in the console (window below):\n?tidyr::pivot_longer\nIf you are familiar with spread and gather, you should acquaint yourself with pivot_longer() and pivot_wider(). The idea is to go from very wide dataframes to very long dataframes and vice versa.\n\n3.4.1 pivot_longer()\n\npivot the military pay grade to become longer?\n\n\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794\n\n\nlibrary(googlesheets4)\ngs4_deauth()\n\nnavy_gs = read_sheet(\"https://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit#gid=1877566408\", \n                     col_types = \"ccnnnnnnnnnnnnnnn\")\n\n\nglimpse(navy_gs)\n\nRows: 38\nColumns: 17\n$ ...1                 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ `Active Duty Family` &lt;chr&gt; NA, \"Marital Status Report\", NA, \"Data Reflect Se…\n$ ...3                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 31229, 53094, 131…\n$ ...4                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5717, 8388, 21019…\n$ ...5                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36946, 61482, 152…\n$ ...6                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 563, 1457, 4264, …\n$ ...7                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 122, 275, 1920, 4…\n$ ...8                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 685, 1732, 6184, …\n$ ...9                 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 139, 438, 3579, 8…\n$ ...10                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 141, 579, 4902, 9…\n$ ...11                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 280, 1017, 8481, …\n$ ...12                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5060, 12483, 5479…\n$ ...13                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 719, 1682, 6641, …\n$ ...14                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5779, 14165, 6143…\n$ ...15                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 36991, 67472, 193…\n$ ...16                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 6699, 10924, 3448…\n$ ...17                &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 43690, 78396, 228…\n\nnames(navy_gs) = c(\"X\",\"pay.grade\", \"male.sing.wo\", \"female.sing.wo\",\n                   \"tot.sing.wo\", \"male.sing.w\", \"female.sing.w\", \n                   \"tot.sing.w\", \"male.joint.NA\", \"female.joint.NA\",\n                   \"tot.joint.NA\", \"male.civ.NA\", \"female.civ.NA\",\n                   \"tot.civ.NA\", \"male.tot.NA\", \"female.tot.NA\", \n                   \"tot.tot.NA\")\nnavy = navy_gs[-c(1:8), -1]\ndplyr::glimpse(navy)\n\nRows: 30\nColumns: 16\n$ pay.grade       &lt;chr&gt; \"E-1\", \"E-2\", \"E-3\", \"E-4\", \"E-5\", \"E-6\", \"E-7\", \"E-8\"…\n$ male.sing.wo    &lt;dbl&gt; 31229, 53094, 131091, 112710, 57989, 19125, 5446, 1009…\n$ female.sing.wo  &lt;dbl&gt; 5717, 8388, 21019, 16381, 11021, 4654, 1913, 438, 202,…\n$ tot.sing.wo     &lt;dbl&gt; 36946, 61482, 152110, 129091, 69010, 23779, 7359, 1447…\n$ male.sing.w     &lt;dbl&gt; 563, 1457, 4264, 9491, 10937, 10369, 6530, 1786, 579, …\n$ female.sing.w   &lt;dbl&gt; 122, 275, 1920, 4662, 6576, 4962, 2585, 513, 144, 2175…\n$ tot.sing.w      &lt;dbl&gt; 685, 1732, 6184, 14153, 17513, 15331, 9115, 2299, 723,…\n$ male.joint.NA   &lt;dbl&gt; 139, 438, 3579, 8661, 12459, 8474, 5065, 1423, 458, 40…\n$ female.joint.NA &lt;dbl&gt; 141, 579, 4902, 9778, 11117, 6961, 3291, 651, 150, 375…\n$ tot.joint.NA    &lt;dbl&gt; 280, 1017, 8481, 18439, 23576, 15435, 8356, 2074, 608,…\n$ male.civ.NA     &lt;dbl&gt; 5060, 12483, 54795, 105556, 130944, 110322, 70001, 210…\n$ female.civ.NA   &lt;dbl&gt; 719, 1682, 6641, 9961, 8592, 5827, 3206, 820, 291, 377…\n$ tot.civ.NA      &lt;dbl&gt; 5779, 14165, 61436, 115517, 139536, 116149, 73207, 218…\n$ male.tot.NA     &lt;dbl&gt; 36991, 67472, 193729, 236418, 212329, 148290, 87042, 2…\n$ female.tot.NA   &lt;dbl&gt; 6699, 10924, 34482, 40782, 37306, 22404, 10995, 2422, …\n$ tot.tot.NA      &lt;dbl&gt; 43690, 78396, 228211, 277200, 249635, 170694, 98037, 2…\n\n# get rid of total columns & rows:\n\nnavyWR = navy %&gt;% select(-contains(\"tot\")) %&gt;%\n   filter(substr(pay.grade, 1, 5) != \"TOTAL\" & \n                   substr(pay.grade, 1, 5) != \"GRAND\" ) %&gt;%\n   pivot_longer(-pay.grade, \n                       values_to = \"numPeople\", \n                       names_to = \"status\") %&gt;%\n   separate(status, into = c(\"sex\", \"marital\", \"kids\"))\n\nnavyWR %&gt;% head()\n\n# A tibble: 6 × 5\n  pay.grade sex    marital kids  numPeople\n  &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 E-1       male   sing    wo        31229\n2 E-1       female sing    wo         5717\n3 E-1       male   sing    w           563\n4 E-1       female sing    w           122\n5 E-1       male   joint   NA          139\n6 E-1       female joint   NA          141\n\n\nDoes a graph tell us if we did it right? what if we had done it wrong…?\n\nnavyWR %&gt;% ggplot(aes(x=pay.grade, y=numPeople, color=sex)) + \n  geom_point()  + \n  facet_grid(kids ~ marital) +\n  theme_minimal() +\n  scale_color_viridis_d() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, \n                                   hjust = 1, size = rel(.5)))\n\n\n\n\n\n\n\n\n3.4.2 pivot_wider\n\n\nlibrary(babynames)\nbabynames %&gt;% dplyr::select(-prop) %&gt;%\n   tidyr::pivot_wider(names_from = sex, values_from = n) \n\n# A tibble: 1,756,284 × 4\n    year name          F     M\n   &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt;\n 1  1880 Mary       7065    27\n 2  1880 Anna       2604    12\n 3  1880 Emma       2003    10\n 4  1880 Elizabeth  1939     9\n 5  1880 Minnie     1746     9\n 6  1880 Margaret   1578    NA\n 7  1880 Ida        1472     8\n 8  1880 Alice      1414    NA\n 9  1880 Bertha     1320    NA\n10  1880 Sarah      1288    NA\n# ℹ 1,756,274 more rows\n\n\n\nbabynames %&gt;% \n  select(-prop) %&gt;% \n  pivot_wider(names_from = sex, values_from = n) %&gt;%\n  filter(!is.na(F) & !is.na(M)) %&gt;%\n  arrange(desc(year), desc(M))\n\n# A tibble: 168,381 × 4\n    year name         F     M\n   &lt;dbl&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt;\n 1  2017 Liam        36 18728\n 2  2017 Noah       170 18326\n 3  2017 William     18 14904\n 4  2017 James       77 14232\n 5  2017 Logan     1103 13974\n 6  2017 Benjamin     8 13733\n 7  2017 Mason       58 13502\n 8  2017 Elijah      26 13268\n 9  2017 Oliver      15 13141\n10  2017 Jacob       16 13106\n# ℹ 168,371 more rows\n\n\n\nbabynames %&gt;% \n  pivot_wider(names_from = sex, values_from = n) %&gt;%\n  filter(!is.na(F) & !is.na(M)) %&gt;%\n  arrange(desc(prop))\n\n# A tibble: 12 × 5\n    year name            prop     F     M\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n 1  1986 Marquette 0.0000130     24    25\n 2  1996 Dariel    0.0000115     22    23\n 3  2014 Laramie   0.0000108     21    22\n 4  1939 Earnie    0.00000882    10    10\n 5  1939 Vertis    0.00000882    10    10\n 6  1921 Vernis    0.00000703     9     8\n 7  1939 Alvia     0.00000529     6     6\n 8  1939 Eudell    0.00000529     6     6\n 9  1939 Ladell    0.00000529     6     6\n10  1939 Lory      0.00000529     6     6\n11  1939 Maitland  0.00000529     6     6\n12  1939 Delaney   0.00000441     5     5\n\n\n\n3.4.3 join (use join to merge two datasets)\n\n3.4.3.1 First get the data (GapMinder)\nBoth of the following datasets come from GapMinder. The first represents country, year, and female literacy rate. The second represents country, year, and GDP (in fixed 2000 US$).\n\ngs4_deauth()\nlitF = read_sheet(\"https://docs.google.com/spreadsheets/d/1hDinTIRHQIaZg1RUn6Z_6mo12PtKwEPFIz_mJVF6P5I/pub?gid=0\")\n\nlitF = litF %&gt;% select(country=starts_with(\"Adult\"), \n                              starts_with(\"1\"), starts_with(\"2\")) %&gt;%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"litRateF\") %&gt;%\n  filter(!is.na(litRateF))\n\n\ngs4_deauth()\nGDP = read_sheet(\"https://docs.google.com/spreadsheets/d/1RctTQmKB0hzbm1E8rGcufYdMshRdhmYdeL29nXqmvsc/pub?gid=0\")\n\nGDP = GDP %&gt;% select(country = starts_with(\"Income\"), \n                            starts_with(\"1\"), starts_with(\"2\")) %&gt;%\n  pivot_longer(-country, \n                      names_to = \"year\", \n                      values_to = \"gdp\") %&gt;%\n  filter(!is.na(gdp))\n\n\nhead(litF)\n\n# A tibble: 6 × 3\n  country     year  litRateF\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;\n1 Afghanistan 1979      4.99\n2 Afghanistan 2011     13   \n3 Albania     2001     98.3 \n4 Albania     2008     94.7 \n5 Albania     2011     95.7 \n6 Algeria     1987     35.8 \n\nhead(GDP)\n\n# A tibble: 6 × 3\n  country year    gdp\n  &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n1 Albania 1980  1061.\n2 Albania 1981  1100.\n3 Albania 1982  1111.\n4 Albania 1983  1101.\n5 Albania 1984  1065.\n6 Albania 1985  1060.\n\n# left\nlitGDPleft = left_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPleft)\n\n[1] 571   4\n\nsum(is.na(litGDPleft$gdp))\n\n[1] 66\n\nhead(litGDPleft)\n\n# A tibble: 6 × 4\n  country     year  litRateF   gdp\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan 1979      4.99   NA \n2 Afghanistan 2011     13      NA \n3 Albania     2001     98.3  1282.\n4 Albania     2008     94.7  1804.\n5 Albania     2011     95.7  1966.\n6 Algeria     1987     35.8  1902.\n\n# right\nlitGDPright = right_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPright)\n\n[1] 7988    4\n\nsum(is.na(litGDPright$gdp))\n\n[1] 0\n\nhead(litGDPright)\n\n# A tibble: 6 × 4\n  country year  litRateF   gdp\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Albania 2001      98.3 1282.\n2 Albania 2008      94.7 1804.\n3 Albania 2011      95.7 1966.\n4 Algeria 1987      35.8 1902.\n5 Algeria 2002      60.1 1872.\n6 Algeria 2006      63.9 2125.\n\n# inner\nlitGDPinner = inner_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPinner)\n\n[1] 505   4\n\nsum(is.na(litGDPinner$gdp))\n\n[1] 0\n\nhead(litGDPinner)\n\n# A tibble: 6 × 4\n  country year  litRateF   gdp\n  &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Albania 2001      98.3 1282.\n2 Albania 2008      94.7 1804.\n3 Albania 2011      95.7 1966.\n4 Algeria 1987      35.8 1902.\n5 Algeria 2002      60.1 1872.\n6 Algeria 2006      63.9 2125.\n\n# full\nlitGDPfull = full_join(litF, GDP, by=c(\"country\", \"year\"))\ndim(litGDPfull)\n\n[1] 8054    4\n\nsum(is.na(litGDPfull$gdp))\n\n[1] 66\n\nhead(litGDPfull)\n\n# A tibble: 6 × 4\n  country     year  litRateF   gdp\n  &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Afghanistan 1979      4.99   NA \n2 Afghanistan 2011     13      NA \n3 Albania     2001     98.3  1282.\n4 Albania     2008     94.7  1804.\n5 Albania     2011     95.7  1966.\n6 Algeria     1987     35.8  1902.\n\n\n\n3.4.4 lubridate\n\nlubridate is a another R package meant for data wrangling (Grolemund and Wickham 2011). In particular, lubridate makes it very easy to work with days, times, and dates. The base idea is to start with dates in a ymd (year month day) format and transform the information into whatever you want. The linked table is from the original paper and provides many of the basic lubridate commands: http://blog.yhathq.com/static/pdf/R_date_cheat_sheet.pdf}.\nExample from https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html\n\n3.4.4.1 If anyone drove a time machine, they would crash\nThe length of months and years change so often that doing arithmetic with them can be unintuitive. Consider a simple operation, January 31st + one month. Should the answer be:\n\nFebruary 31st (which doesn’t exist)\nMarch 4th (31 days after January 31), or\nFebruary 28th (assuming its not a leap year)\n\nA basic property of arithmetic is that a + b - b = a. Only solution 1 obeys the mathematical property, but it is an invalid date. Wickham wants to make lubridate as consistent as possible by invoking the following rule: if adding or subtracting a month or a year creates an invalid date, lubridate will return an NA.\nIf you thought solution 2 or 3 was more useful, no problem. You can still get those results with clever arithmetic, or by using the special %m+% and %m-% operators. %m+% and %m-% automatically roll dates back to the last day of the month, should that be necessary.\n\n3.4.4.2 R examples, lubridate()\n\nSome basics in lubridate\n\nrequire(lubridate)\nrightnow &lt;- now()\n\nday(rightnow)\n\n[1] 1\n\nweek(rightnow)\n\n[1] 31\n\nmonth(rightnow, label=FALSE)\n\n[1] 8\n\nmonth(rightnow, label=TRUE)\n\n[1] Aug\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nyear(rightnow)\n\n[1] 2024\n\nminute(rightnow)\n\n[1] 19\n\nhour(rightnow)\n\n[1] 14\n\nyday(rightnow)\n\n[1] 214\n\nmday(rightnow)\n\n[1] 1\n\nwday(rightnow, label=FALSE)\n\n[1] 5\n\nwday(rightnow, label=TRUE)\n\n[1] Thu\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\nBut how do I create a date object?\n\njan31 &lt;- ymd(\"2021-01-31\")\njan31 + months(0:11)\n\n [1] \"2021-01-31\" NA           \"2021-03-31\" NA           \"2021-05-31\"\n [6] NA           \"2021-07-31\" \"2021-08-31\" NA           \"2021-10-31\"\n[11] NA           \"2021-12-31\"\n\nfloor_date(jan31, \"month\") + months(0:11) + days(31)\n\n [1] \"2021-02-01\" \"2021-03-04\" \"2021-04-01\" \"2021-05-02\" \"2021-06-01\"\n [6] \"2021-07-02\" \"2021-08-01\" \"2021-09-01\" \"2021-10-02\" \"2021-11-01\"\n[11] \"2021-12-02\" \"2022-01-01\"\n\njan31 + months(0:11) + days(31)\n\n [1] \"2021-03-03\" NA           \"2021-05-01\" NA           \"2021-07-01\"\n [6] NA           \"2021-08-31\" \"2021-10-01\" NA           \"2021-12-01\"\n[11] NA           \"2022-01-31\"\n\njan31 %m+% months(0:11)\n\n [1] \"2021-01-31\" \"2021-02-28\" \"2021-03-31\" \"2021-04-30\" \"2021-05-31\"\n [6] \"2021-06-30\" \"2021-07-31\" \"2021-08-31\" \"2021-09-30\" \"2021-10-31\"\n[11] \"2021-11-30\" \"2021-12-31\"\n\n\nNYC flights\n\nlibrary(nycflights13)\nnames(flights)\n\n [1] \"year\"           \"month\"          \"day\"            \"dep_time\"      \n [5] \"sched_dep_time\" \"dep_delay\"      \"arr_time\"       \"sched_arr_time\"\n [9] \"arr_delay\"      \"carrier\"        \"flight\"         \"tailnum\"       \n[13] \"origin\"         \"dest\"           \"air_time\"       \"distance\"      \n[17] \"hour\"           \"minute\"         \"time_hour\"     \n\nflightsWK &lt;- flights %&gt;% \n   mutate(ymdday = ymd(paste(year, month,day, sep=\"-\"))) %&gt;%\n   mutate(weekdy = wday(ymdday, label=TRUE), \n          whichweek = week(ymdday))\n\nhead(flightsWK)\n\n# A tibble: 6 × 22\n   year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n1  2013     1     1      517            515         2      830            819\n2  2013     1     1      533            529         4      850            830\n3  2013     1     1      542            540         2      923            850\n4  2013     1     1      544            545        -1     1004           1022\n5  2013     1     1      554            600        -6      812            837\n6  2013     1     1      554            558        -4      740            728\n# ℹ 14 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, ymdday &lt;date&gt;, weekdy &lt;ord&gt;,\n#   whichweek &lt;dbl&gt;\n\nflightsWK &lt;- flights %&gt;% \n   mutate(ymdday = ymd(paste(year,\"-\", month,\"-\",day))) %&gt;%\n   mutate(weekdy = wday(ymdday, label=TRUE), whichweek = week(ymdday))\n\nflightsWK %&gt;% select(year, month, day, ymdday, weekdy, whichweek, dep_time, \n                     arr_time, air_time) %&gt;%  \n   head()\n\n# A tibble: 6 × 9\n   year month   day ymdday     weekdy whichweek dep_time arr_time air_time\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;date&gt;     &lt;ord&gt;      &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;    &lt;dbl&gt;\n1  2013     1     1 2013-01-01 Tue            1      517      830      227\n2  2013     1     1 2013-01-01 Tue            1      533      850      227\n3  2013     1     1 2013-01-01 Tue            1      542      923      160\n4  2013     1     1 2013-01-01 Tue            1      544     1004      183\n5  2013     1     1 2013-01-01 Tue            1      554      812      116\n6  2013     1     1 2013-01-01 Tue            1      554      740      150",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#purrr-for-functional-programming",
    "href": "03-wrangling.html#purrr-for-functional-programming",
    "title": "3  Data Wrangling",
    "section": "\n3.5 purrr for functional programming",
    "text": "3.5 purrr for functional programming\nWe will see the R package purrr in greater detail as we go, but for now, let’s get a hint for how it works.\nWe are going to focus on the map family of functions which will just get us started. Lots of other good purrr functions like pluck() and accumulate().\nMuch of below is taken from a tutorial by Rebecca Barter.\nThe map functions are named by the output the produce. For example:\n\nmap(.x, .f) is the main mapping function and returns a list\nmap_df(.x, .f) returns a data frame\nmap_dbl(.x, .f) returns a numeric (double) vector\nmap_chr(.x, .f) returns a character vector\nmap_lgl(.x, .f) returns a logical vector\n\nNote that the first argument is always the data object and the second object is always the function you want to iteratively apply to each element in the input object.\nThe input to a map function is always either a vector (like a column), a list (which can be non-rectangular), or a dataframe (like a rectangle).\nA list is a way to hold things which might be very different in shape:\n\na_list &lt;- list(a_number = 5,\n                      a_vector = c(\"a\", \"b\", \"c\"),\n                      a_dataframe = data.frame(a = 1:3, \n                                               b = c(\"q\", \"b\", \"z\"), \n                                               c = c(\"bananas\", \"are\", \"so very great\")))\n\nprint(a_list)\n\n$a_number\n[1] 5\n\n$a_vector\n[1] \"a\" \"b\" \"c\"\n\n$a_dataframe\n  a b             c\n1 1 q       bananas\n2 2 b           are\n3 3 z so very great\n\n\nConsider the following function:\n\nadd_ten &lt;- function(x) {\n  return(x + 10)\n  }\n\nWe can map() the add_ten() function across a vector. Note that the output is a list (the default).\n\nlibrary(tidyverse)\nmap(.x = c(2, 5, 10),\n    .f = add_ten)\n\n[[1]]\n[1] 12\n\n[[2]]\n[1] 15\n\n[[3]]\n[1] 20\n\n\nWhat if we use a different type of input? The default behavior is to still return a list!\n\ndata.frame(a = 2, b = 5, c = 10) %&gt;%\n  map(add_ten)\n\n$a\n[1] 12\n\n$b\n[1] 15\n\n$c\n[1] 20\n\n\nWhat if we want a different type of output? We use a different map() function, map_df(), for example.\n\ndata.frame(a = 2, b = 5, c = 10) %&gt;%\n  map_df(add_ten)\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nShorthand lets us get away from pre-defining the function (which will be useful). Use the tilde ~ to indicate that you have a function:\n\ndata.frame(a = 2, b = 5, c = 10) %&gt;%\n  map_df(~{.x + 10})\n\n# A tibble: 1 × 3\n      a     b     c\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    12    15    20\n\n\nMostly, the tilde will be used for functions we already know:\n\nlibrary(palmerpenguins)\nlibrary(broom)\n\npenguins %&gt;%\n  split(.$species) %&gt;%\n  map(~ lm(body_mass_g ~ flipper_length_mm, data = .x)) %&gt;%\n  map_df(tidy)  # map(tidy)\n\n# A tibble: 6 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 9.48e- 3\n2 flipper_length_mm     32.8      5.08      6.47 1.34e- 9\n3 (Intercept)        -3037.     997.       -3.05 3.33e- 3\n4 flipper_length_mm     34.6      5.09      6.79 3.75e- 9\n5 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n6 flipper_length_mm     54.6      5.03     10.9  1.33e-19\n\npenguins %&gt;%\n  group_by(species) %&gt;%\n  group_map(~lm(body_mass_g ~ flipper_length_mm, data = .x)) %&gt;%\n  map(tidy)  # map_df(tidy)\n\n[[1]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -2536.     965.       -2.63 0.00948      \n2 flipper_length_mm     32.8      5.08      6.47 0.00000000134\n\n[[2]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic       p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 (Intercept)        -3037.     997.       -3.05 0.00333      \n2 flipper_length_mm     34.6      5.09      6.79 0.00000000375\n\n[[3]]\n# A tibble: 2 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -6787.    1093.       -6.21 7.65e- 9\n2 flipper_length_mm     54.6      5.03     10.9  1.33e-19",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "03-wrangling.html#reprex",
    "href": "03-wrangling.html#reprex",
    "title": "3  Data Wrangling",
    "section": "\n3.6 reprex()\n",
    "text": "3.6 reprex()\n\n\nHelp me help you\n\nIn order to create a reproducible example …\nStep 1. Copy code onto the clipboard\nStep 2. Type reprex() into the Console\nStep 3. Look at the Viewer to the right. Copy the Viewer output into GitHub, Piazza, an email, stackexchange, etc.\nSome places to learn more about reprex include\n\nA blog about it: https://teachdatascience.com/reprex/\nThe reprex vignette: https://reprex.tidyverse.org/index.html\n\nreprex dos and donts: https://reprex.tidyverse.org/articles/reprex-dos-and-donts.html\nJenny Bryan webinar on reprex: “Help me help you. Creating reproducible examples” https://resources.rstudio.com/webinars/help-me-help-you-creating-reproducible-examples-jenny-bryan\nSome advice: https://stackoverflow.com/help/minimal-reproducible-example\n\n\n3.6.0.1 reprex demo\nreprex(\n  jan31 + months(0:11) + days(31)\n)\nmultiple lines of code:\nreprex({\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})\nreprex({\n  library(lubridate)\n  jan31 &lt;- ymd(\"2021-01-31\")\n  jan31 + months(0:11) + days(31)\n})\n\n\n\nIf you ever need to understand which join is the right join for you, try to find an image that will lay out what the function is doing. I found this one that is quite good and is taken from Statistics Globe blog: https://statisticsglobe.com/r-dplyr-join-inner-left-right-full-semi-anti\nhttps://docs.google.com/spreadsheets/d/1Ow6Cm4z-Z1Yybk3i352msulYCEDOUaOghmo9ALajyHo/edit# gid=1811988794\n\n\n\nGrolemund, G., and H. Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3). http://www.jstatsoft.org/v40/i03/paper.\n\n\nKaplan, Daniel. 2015. Data Computing: An Introduction to Wrangling and Visualization with r. Project Mosaic Books.\n\n\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/paper.",
    "crumbs": [
      "Introduction to databases and SQL",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  }
]