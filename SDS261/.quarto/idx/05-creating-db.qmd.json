{"title":"Creating databases","markdown":{"headingText":"Creating databases","headingAttr":{"id":"sec-create-db","classes":[],"keyvalue":[]},"containsRefs":false,"markdown":"\n\n```{r}\n#| include: false\n\nsource(\"_common.R\")\nfontawesome::fa_html_dependency()\n```\n\n\nIn order to practice creating and editing databases, you will need to install <a href = \"https://duckdb.org/\" target = \"_blank\">DuckDB</a> onto your own computer. Fortunately, you can do the installation using **R**! **DuckDB** is an in-process database management system that runs entirely on your own computer. Setting up a database on your own computer is a great way to work with large datasets where you are the only analyst.  The data then lives in your storage (instead of your memory), and you don't have to transfer queries or results over the internet.\n\nUsing **DuckDB** suits our purposes because it allows us to create a local database that we can edit.  However, the **SQL** dialect used in **DuckDB** is slightly different from **MySQL**, which is important to recognize.  For example, we write `SELECT * FROM table 10;` instead of `SELECT * FROM table 0, 10;`.  In your future as a data scientist, you will find different dialects, depending on the **SQL** server.  Always be aware of the dialect you are using.\n\n```{r}\n#| eval: false\n\ninstall.packages(\"duckdb\")  # only once, in the Console, not in the .qmd or .Rmd file\nlibrary(duckdb)             # at the top of the .qmd or .Rmd file\n\nlibrary(DBI)                # we also still need the DBI package\n```\n\nIn order to create a database (on our own computer), we'll start by creating a connection to **DuckDB**. Note that the database has been stored to a database directory called `duck_datab` which lives in the current **R** project.  You won't be able to open it like a standard folder, but it is where **DuckDB** stores the database files.\n\n```{r}\ncon_duckdb <- DBI::dbConnect(duckdb::duckdb(),\n                             dbdir = \"duck_datab\")\n```\n\n \n## Preparing to load data \n\nThe `duckdb` database is currently empty, so we need to load in some data.  The `duckdb_read_csv()` function in the **duckdb** R package allows us to load the .csv file (available on GitHub) directly into the database without being loaded as an **R** object first.  The function, `duckdb_read_csv()` does some of the work for us to find data types.  However, we will first learn what data types are and how to use them, and most dialects of **SQL** require you to specify the data types before loading in data (usually done using `LOAD DATA` but using `COPY` in **Duckdb**).\n \n\nRecall that in @tbl-select-describe we used `DESCRIBE` to display the variable types of the database table(s).  The list includes the variable name (`Field`), its `Type`, whether there are `NULL` values allowed, and whether there are keys or indexes defined on the variable.  See @tbl-casts-describe for the `DESCRIBE` output on the table we are about to import.\n\nUnlike **R**, when creating a new data table, **SQL** requires that you communicate each future variable (column) and that variable's type. Variable types are **not** automatically generated!   \n\nAs an example, consider the Saturday Night Live datasets available on the <a href = \"https://github.com/hhllcks/snldb/\" target = \"_blank\">snldb GitHub repo</a>. Data is scraped from <a href = \"http://www.snlarchives.net\" target = \"_blank\">http://www.snlarchives.net</a> and <a href = \"http://www.imdb.com/title/tt0072562\" target = \"_blank\">http://www.imdb.com/title/tt0072562</a> by <a href = \"https://github.com/hhllcks\" target = \"_blank\">Hendrik Hilleckes</a> and <a href = \"https://github.com/colinmorris\" target = \"_blank\">Colin Morris</a>.  Notice that there are eleven .csv files available in the <a href = \"https://github.com/hhllcks/snldb/tree/master/output\" target = \"_blank\">output folder</a>.\n\nSpecifically, let's consider the <a href = \"https://raw.githubusercontent.com/hhllcks/snldb/master/output/actors.csv\" target = \"_blank\">casts.csv</a> file.\n\nBefore we get into loading data into a **SQL** database, let's look at the casts file in **R**, so that we understand the data we want to load. `glimpse()` provides the variable names and the variables types.  The variables types are a mix of character strings, numeric, and logical.  Variable types are very important for inputting data into a **SQL** server.\n\n```{r}\n#| echo: true\ncasts <- readr::read_csv(\"https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv\")\nglimpse(casts)\n```\n\n\n### Variable types\n\nThe `glance()` function indicates that there are different variables types, here `chr` (character string), `dbl` (numerical value with double precision), and `lgl` (logical value).  What are the data types in **SQL**?^[Taken from https://dev.mysql.com/doc/refman/8.0/en/data-types.html]\n\n#### Numbers {-}\n* exact numeric data types (`INTEGER`, `SMALLINT`, `DECIMAL`, and `NUMERIC`)^[The keyword `INT` is a synonym for `INTEGER`, and the keywords `DEC` and `FIXED` are synonyms for `DECIMAL`] \n* approximate numeric data types (`FLOAT`, `REAL`, and `DOUBLE PRECISION`)^[ **MySQL** treats `DOUBLE` as a synonym for `DOUBLE PRECISION`. **MySQL** also treats `REAL` as a synonym for `DOUBLE PRECISION`.]\n* integer types `INTEGER` (or `INT`) and `SMALLINT`, `TINYINT`, `MEDIUMINT`, and `BIGINT`\n* `DECIMAL(precision, scale)`.  For example, in `DECIMAL(5,2)` 5 is the precision and 2 is the scale. The precision represents the number of significant digits that are stored for values, and the scale represents the number of digits that can be stored following the decimal point. `DECIMAL(5,2)` must store any value with five digits and two decimals, so values that can be stored in the salary column range from -999.99 to 999.99. `DECIMAL(M)` is equivalent to `DECIMAL(M,0)`. Similarly, the syntax `DECIMAL` is equivalent to `DECIMAL(M,0)`, where the default value of `M` is 10. If the scale is 0, `DECIMAL` values contain no decimal point or fractional part.\n\n#### Strings {-}\n\n* string data types are `CHAR`, `VARCHAR`, `BINARY`, `VARBINARY`, `BLOB`, `TEXT`, `ENUM`, and `SET`\n* `CHAR` and `VARCHAR` types are similar, but differ in the way they are stored and retrieved. They also differ in maximum length and in whether trailing spaces are retained.\n* `CHAR` and `VARCHAR` types are declared with a length that indicates the maximum number of characters you want to store. For example, `CHAR(30)` can hold up to 30 characters.\n* The length of a `CHAR` column is fixed to the length that you declare when you create the table. \n* Values in `VARCHAR` columns are variable-length strings. The length can be specified as a value from 0 to 65,535. \n* `BINARY` and `VARBINARY` types are similar to `CHAR` and `VARCHAR`, except that they store binary strings rather than nonbinary strings. That is, they store byte strings rather than character strings.\n* A `BLOB` is a binary large object that can hold a variable amount of data. The four `BLOB` types are `TINYBLOB`, `BLOB`, `MEDIUMBLOB`, and `LONGBLOB`. These differ only in the maximum length of the values they can hold. The four `TEXT` types are `TINYTEXT`, `TEXT`, `MEDIUMTEXT`, and `LONGTEXT`. These correspond to the four `BLOB` types and have the same maximum lengths and storage requirements. \n\n\n#### Date {-}\n* date and time data types for representing temporal values are `DATE`, `TIME`, `DATETIME`, `TIMESTAMP`, and `YEAR`\n* `DATE` type is used for values with a date part but no time part. **MySQL** retrieves and displays DATE values in 'YYYY-MM-DD' format. The supported range is '1000-01-01' to '9999-12-31'.\n* `DATETIME` type is used for values that contain both date and time parts. **MySQL** retrieves and displays `DATETIME` values in 'YYYY-MM-DD hh:mm:ss' format. The supported range is '1000-01-01 00:00:00' to '9999-12-31 23:59:59'.\n* `TIMESTAMP` data type is used for values that contain both date and time parts. `TIMESTAMP` has a range of '1970-01-01 00:00:01' UTC to '2038-01-19 03:14:07' UTC.\n* `TIME` values in 'hh:mm:ss' format (or 'hhh:mm:ss' format for large hours values). `TIME` values may range from '-838:59:59' to '838:59:59'. The hours part may be so large because the TIME type can be used not only to represent a time of day (which must be less than 24 hours), but also elapsed time or a time interval between two events (which may be much greater than 24 hours, or even negative).\n* `YEAR` type is a 1-byte type used to represent year values with a display width of four characters.\n\n\n\n### `CHECK` constraints\n\nWhile implementing `CREATE TABLE`, constraints can be added either to individual variables (`CountryPopulation > 0`) or to the table as a whole (`LastCensus < NextCensus`).^[Example from https://www.sqlshack.com/how-to-use-sql-check-constraints/]\n\nIf an attempt is made to load data that violate the `CHECK` constraints, an error will be given.\n\n```{sql}\n#| eval: false\n\nCREATE TABLE CountryListCensus (\n    Id INT,\n    CountryName VARCHAR(255) NOT NULL,\n    CountryPopulation INT CHECK(CountryPopulation > 0),\n    LastCensus DATE,\n    NextCensus DATE,\n    CHECK(LastCensus<NextCensus),\n    PRIMARY KEY (Id)\n);\n```\n\n\n\n\n### Creating `KEY`s\n\n```{sql}\n#| eval: false\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY col1,\n  FOREIGN KEY col2 REFERENCES table2(table2col1)\n);\n```\n\nEither or both of the `KEY`s could be multiple columns.\n\n```{sql}\n#| eval: false\n\nCREATE TABLE table1 (\n  col1 ...,\n  col2 ...,\n  col3 ...,\n  PRIMARY KEY (col1, col3),\n  FOREIGN KEY (col1, col2) REFERENCES table2(table2col1, table2col4)\n);\n```\n\n### Creating `INDEX`es\n\nIndexes can be created on one or more variable.  A table does not need to have an `INDEX` (or a `KEY`).\n\n```{sql}\n#| eval: false\n\nCREATE INDEX name_of_index ON table (col1);\n```\n\n```{sql}\n#| eval: false\n\nCREATE INDEX name_of_index ON table (col1, col2);\n```\n\n\n### Loading data {#sec-load-data}\n\nOnce the database is set up, you will be ready to import .csv files into the database as tables.  Importing .csv files as tables requires a series of steps:^[taken from <a href = \"https://mdsr-book.github.io/mdsr3e/16-sqlII.html#load-into-mysql-database\" target = \"_blank\">MDSR</a>.]\n\n1. a `USE` statement that ensures we are in the right schema/database.\n2. a series of `DROP TABLE` statements that drop any old tables with the same names as the ones we are going to create.\n3. a series of `CREATE TABLE` statements that specify the table structures.\n4. a series of `COPY` statements that read the data from the .csv files into the appropriate tables.\n\n::: {.callout-tip icon=false}\n\n## <i class=\"fas fa-triangle-exclamation\"></i> Watch out!\n\n**DuckDB** has its own dialect of **SQL**.  To load data into a **MySQL** server, the final statement would be `LOAD DATA` instead of `COPY`.  See <a href = \"https://mdsr-book.github.io/mdsr3e/16-sqlII.html#load-into-mysql-database\" target = \"_blank\">MDSR</a> for more information on loading data into a remote **MySQL** server.\n:::\n\n#### Loading step 1 {-}\n\nUse the local database that we've called `duck_datab`.\n\n```{sql}\n#| connection: con_duckdb\n\nUSE duck_datab;\n```\n\n#### Loading step 2 {-}\n\nMake sure to \"refresh\" the table, in case it already exists.  However, be very careful with the `DROP TABLE` statement, as it will **remove** the `casts` table.\n\n```{sql}\n#| connection: con_duckdb\n\nDROP TABLE IF EXISTS casts;\n```\n\n\n#### Loading step 3 {-}\n\nCarefully define the variable types, whether or not they allow missing values, and what a default value is for that variable.  Additionally, identify the key for accessing information.\n\n**MySQL** doesn't actually have a `BOOLEAN` datatype (you would use `TINYINT(1)` instead).  But **DuckDB** *does* have a `BOOLEAN` datatype!\n\n```{sql}\n#| connection: con_duckdb\n\nCREATE TABLE casts (\n  aid VARCHAR(255) NOT NULL DEFAULT ' ',\n  sid INTEGER NOT NULL DEFAULT 0,\n  featured BOOLEAN NOT NULL DEFAULT 'false',\n  first_epid INTEGER DEFAULT 0,\n  last_epid INTEGER DEFAULT 0,\n  update_anchor BOOLEAN NOT NULL DEFAULT 0,\n  n_episodes INTEGER NOT NULL DEFAULT 0,\n  season_fraction DECIMAL(21,20) NOT NULL DEFAULT 0,\n  PRIMARY KEY (sid, aid)\n);\n```\n\n\n#### Loading step 4 {-}\n\nThe .csv file lives on my computer, so I load it in directly.  Note that the statement to load in data is slightly different in **MySQL**.\n\n```{sql}\n#| connection: con_duckdb\n\nCOPY casts FROM 'data/casts.csv' HEADER;\n```\n\n#### Checking the loading {-}\n\n```{sql}\n#| connection: con_duckdb\n#| label: select-casts\n#| output.var: \"select_casts\"\n\nSELECT * FROM casts LIMIT 8;\n```\n\n```{r}\n#| label: tbl-select-casts\n#| echo: false\n#| tbl-cap: \"After `CREATE TABLE` where variable types are set, the `COPY` command pulls the data into the table.  `SELECT` shows us that the table is as expected.\"\n\nselect_casts |>\n  kbl(linesep = \"\", booktabs = TRUE) |>\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\"), \n                latex_options = c(\"striped\", \"hold_position\"),\n                full_width = FALSE) \n\n```\n\n\n#### Check {-}\n\nLet's make sure that the database exists and that the table in the database exists.\n\n\n```{sql}\n#| connection: con_duckdb\n\nSHOW DATABASES;\n```\n\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS actors;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS seasons;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS titles;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS hosts;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS episodes;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS impressions;\n```\n\n```{sql}\n#| connection: con_duckdb\n\nSHOW TABLES;\n```\n```{sql}\n#| connection: con_duckdb\n#| label: casts-describe\n#| output.var: \"casts_describe\"\n\nDESCRIBE casts;\n```\n\n```{r}\n#| label: tbl-casts-describe\n#| echo: false\n#| tbl-cap: \"DESCRIBE variables in the casts table.\"\n\ncasts_describe |>\n  kbl(linesep = \"\", booktabs = TRUE) |>\n  kable_styling(bootstrap_options = c(\"striped\", \"condensed\"), \n                latex_options = c(\"striped\", \"hold_position\"),\n                full_width = FALSE) \n\n```\n\n###  Using **DuckDB** for loading data {#sec-load-duckdb}\n\nThe steps given in @sec-load-data are general to many **SQL** dialects and are important when working with most **SQL** clients.  It is important to have control over the variables configurations as they make up the **SQL** database.  However, using the **duckdb** package in **R** allows for shorthand entry of data from .csv files into the **DuckDB** database.  Here, we take advantage of working with the **DuckDB** functionality in R.\n\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS hosts;\n```\n\n```{sql}\n#| connection: con_duckdb\n#| echo: false\n#| include: false\n\nDROP TABLE IF EXISTS episodes;\n```\n\n```{r}\nduckdb_read_csv(con = con_duckdb, name = \"hosts\", files = \"data/hosts.csv\")\nduckdb_read_csv(con = con_duckdb, name = \"episodes\", files = \"data/episodes.csv\")\n```\n\n#### Checking the loading {-}\n\n```{sql}\n#| connection: con_duckdb\n\nSHOW TABLES;\n```\n\n\n## Best practice\n\nIt is always a good idea to terminate the **SQL** connection when you are done with it.\n\n```{r}\ndbDisconnect(con_duckdb, shutdown = TRUE)\n```\n\n## <i class=\"fas fa-lightbulb\"></i> Reflection questions  \n\n1. What is the difference between **R** and **SQL** in terms of communicating the different *data types*?\n\n2. Why does it matter if the variable type is specified correctly?  For example, why would it be better for a date column to be specified as `DATETIME` instead of `VARCHAR`?\n\n3. If you are Googling some **SQL** syntax, why do you need to specify the dialect, for example, **DuckDB** or **MySQL**?\n\n4. Why do we often include the `DROP TABLE` operation before the `CREATE TABLE` operation?\n\n\n## <i class=\"fas fa-balance-scale\"></i> Ethics considerations \n\n1. When creating a database why should we worry about the provenance (origin) of the data?\n\n2. How does a **SQL** database automatically hold extra information about the database (e.g., provenance)?  Spoiler: it doesn't.  So what can we do?\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":6,"fig-height":5,"fig-format":"retina","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":false,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"fig-show":"hold","fig-align":"center","callout-icon":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","include-in-header":["js/mathjax-popover.html"],"output-file":"05-creating-db.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["book.bib","packages.bib"],"editor":"source","knitr":{"opts_chunk":{"dev":"ragg_png","fig-asp":0.618}},"theme":["scss/colors.scss"],"header-includes":["<link rel=\"stylesheet\" media=\"screen\" href=\"https://fonts.googleapis.com/css2?family=Quicksand\" type=\"text/css\"/>\n"],"mainfont":"Quicksand","monofont":"Quicksand","author-meta":"Jo Hardin","lightbox":{"match":"auto","loop":false}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}