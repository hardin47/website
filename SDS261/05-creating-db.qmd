# Creating databases {#sec-create-db}


```{r}
#| include: false

source("_common.R")
fontawesome::fa_html_dependency()
```


In order to practice creating and editing databases, you will need to install <a href = "https://duckdb.org/" target = "_blank">DuckDB</a> onto your own computer. Fortunately, you can do the installation using **R**! **DuckDB** is an in-process database management system that runs entirely on your own computer. Setting up a database on your own computer is a great way to work with large datasets where you are the only analyst.  The data then lives in your storage (instead of your memory), and you don't have to transfer queries or results over the internet.

Using **DuckDB** suits our purposes because it allows us to create a local database that we can edit.  However, the **SQL** dialect used in **DuckDB** is slightly different from **MySQL**, which is important to recognize.  For example, we write `SELECT * FROM table 10;` instead of `SELECT * FROM table 0, 10;`.  In your future as a data scientist, you will find different dialects, depending on the **SQL** server.  Always be aware of the dialect you are using.

```{r}
#| eval: false

install.packages("duckdb")  # only once, in the Console, not in the .qmd or .Rmd file
library(duckdb)             # at the top of the .qmd or .Rmd file

library(DBI)                # we also still need the DBI package
```

In order to create a database (on our own computer), we'll start by creating a connection to **DuckDB**. Note that the database has been stored to a database directory called `duck_datab` which lives in the current **R** project.  You won't be able to open it like a standard folder, but it is where **DuckDB** stores the database files.

```{r}
con_duckdb <- DBI::dbConnect(duckdb::duckdb(),
                             dbdir = "duck_datab")
```

 
## Preparing to load data 

The `duckdb` database is currently empty, so we need to load in some data.  The `duckdb_read_csv()` function in the **duckdb** R package allows us to load the .csv file (available on GitHub) directly into the database without being loaded as an **R** object first.
 

Recall that in @tbl-select-describe we used `DESCRIBE` to display the variable types of the database table(s).  The list includes the variable name (`Field`), its `Type`, whether there are `NULL` values allowed, and whether there are keys or indexes defined on the variable.  See @tbl-casts-describe for the `DESCRIBE` output on the table we are about to import.

Unlike **R**, when creating a new data table, **SQL** requires that you communicate each future variable (column) and that variable's type. Variable types are **not** automatically generated!   

As an example, consider the Saturday Night Live datasets available on the <a href = "https://github.com/hhllcks/snldb/" target = "_blank">snldb GitHub repo</a>. Data is scraped from <a href = "http://www.snlarchives.net" target = "_blank">http://www.snlarchives.net</a> and <a href = "http://www.imdb.com/title/tt0072562" target = "_blank">http://www.imdb.com/title/tt0072562</a> by <a href = "https://github.com/hhllcks" target = "_blank">Hendrik Hilleckes</a> and <a href = "https://github.com/colinmorris" target = "_blank">Colin Morris</a>.  Notice that there are eleven .csv files available in the <a href = "https://github.com/hhllcks/snldb/tree/master/output" target = "_blank">output folder</a>.

Specifically, let's consider the <a href = "https://raw.githubusercontent.com/hhllcks/snldb/master/output/actors.csv" target = "_blank">casts.csv</a> file.

Before we get into loading data into a **SQL** database, let's look at the casts file in **R**, so that we understand the data we want to load. `glimpse()` provides the variable names and the variables types.  The variables types are a mix of character strings, numeric, and logical.  Variable types are very important for inputting data into a **SQL** server.

```{r}
#| echo: true
casts <- readr::read_csv("https://raw.githubusercontent.com/hhllcks/snldb/master/output/casts.csv")
glimpse(casts)
```


### Loading data {#sec-load-data}

Once the database is set up, you will be ready to import .csv files into the database as tables.  Importing .csv files as tables requires a series of steps:^[taken from <a href = "https://mdsr-book.github.io/mdsr3e/16-sqlII.html#load-into-mysql-database" target = "_blank">MDSR</a>.]

1. a `USE` statement that ensures we are in the right schema/database.
2. a series of `DROP TABLE` statements that drop any old tables with the same names as the ones we are going to create.
3. a series of `CREATE TABLE` statements that specify the table structures.
4. a series of `COPY` statements that read the data from the .csv files into the appropriate tables.

::: {.callout-tip icon=false}

## <i class="fas fa-triangle-exclamation"></i> Watch out!

**DuckDB** has its own dialect of **SQL**.  To load data into a **MySQL** server, the final statement would be `LOAD DATA` instead of `COPY`.  See <a href = "https://mdsr-book.github.io/mdsr3e/16-sqlII.html#load-into-mysql-database" target = "_blank">MDSR</a> for more information on loading data into a remote **MySQL** server.
:::

#### Loading step 1 {-}

Use the local database that we've called `duck_datab`.

```{sql}
#| connection: con_duckdb

USE duck_datab;
```

#### Loading step 2 {-}

Make sure to "refresh" the table, in case it already exists.  However, be very careful with the `DROP TABLE` statement, as it will **remove** the `casts` table.

```{sql}
#| connection: con_duckdb

DROP TABLE IF EXISTS casts;
```


#### Loading step 3 {-}

Carefully define the variable types, whether or not they allow missing values, and what a default value is for that variable.  Additionally, identify the key for accessing information.

```{sql}
#| connection: con_duckdb

CREATE TABLE casts (
  aid VARCHAR(255) NOT NULL DEFAULT '',
  sid INTEGER NOT NULL DEFAULT 0,
  featured BOOLEAN NOT NULL DEFAULT 'false',
  first_epid INTEGER DEFAULT 0,
  last_epid INTEGER DEFAULT 0,
  update_anchor BOOLEAN NOT NULL DEFAULT 0,
  n_episodes INTEGER NOT NULL DEFAULT 0,
  season_fraction DECIMAL(21,20) NOT NULL DEFAULT 0,
  PRIMARY KEY (sid, aid)
);
```


#### Loading step 4 {-}

The .csv file lives on my computer, so I load it in directly.  Note that the statement to load in data is slightly different in **MySQL**.

```{sql}
#| connection: con_duckdb

COPY casts FROM 'data/casts.csv' HEADER;
```

#### Checking the loading {-}

```{sql}
#| connection: con_duckdb
#| label: select-casts
#| output.var: "select_casts"

SELECT * FROM casts LIMIT 8;
```

```{r}
#| label: tbl-select-casts
#| echo: false
#| tbl-cap: "After `CREATE TABLE` where variable types are set, the `COPY` command pulls the data into the table.  `SELECT` shows us that the table is as expected."

select_casts |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```


#### Check {-}

Let's make sure that the database exists and that the table in the database exists.


```{sql}
#| connection: con_duckdb

SHOW DATABASES;
```


```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS actors;
```

```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS seasons;
```

```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS titles;
```

```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS hosts;
```

```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS episodes;
```

```{sql}
#| connection: con_duckdb
#| echo: false
#| include: false

DROP TABLE IF EXISTS impressions;
```

```{sql}
#| connection: con_duckdb

SHOW TABLES;
```
```{sql}
#| connection: con_duckdb
#| label: casts-describe
#| output.var: "casts_describe"

DESCRIBE casts;
```

```{r}
#| label: tbl-casts-describe
#| echo: false
#| tbl-cap: "DESCRIBE variables in the casts table."

casts_describe |>
  kbl(linesep = "", booktabs = TRUE) |>
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = c("striped", "hold_position"),
                full_width = FALSE) 

```


## Efficiencies

It is worth pointing out a few aspects to loading data into **SQL**: keys, indexes, and partitioning.

Before we get to the definitions, consider this analogy:

> Each library (`database`) has books (`table`s). Each book (`table`) has pages (rows). Each page (row) has a unique page number to identify it (`key` value); to find a particular page, you sort through the page numbers (`key` values). But it isn't immediately obvious where the particular page of interest is, you might have to page through the book a little bit to find the page of interest.  It would be easier if you had several bookmarks throughout the book to anchor some of the page numbers.  For example, if you want page 1047 and you have a bookmark on page 1050, you only have to turn back three pages.  The bookmark is an `index`, it helps you find the desired rows much more quickly.^[Analogy taken from: https://www.quora.com/profile/Lara-Mazilu]


### Key

Keys are unique identifiers for each row, used primarily for connecting tables. Keys are generally not helpful for efficiency, but they are important for data integrity and relationships between tables. A key is a pointer that identifies a record. In practice, a key is one or more columns that are earmarked to uniquely identify a record in a table. Keys serve two main purposes:

1. They provide constraints on the column such as that it can't store duplicate or null values.
2. They are also used to generate relationships among different tables.

* `PRIMARY KEY` is a column or set of columns that uniquely identify each row.  Primary keys cannot be `NULL`. Each table must always have one (and only one) PK. The PK can be made up of one column, but if that isn't enough to uniquely identify the row, more columns may be added. Sometimes it is easier to designate a numeric column (e.g., row number) to be the PK.
* `FOREIGN KEY` is a column or set of columns that reference a primary key in a different table.  The FK linkes two tables together, and the link is called a relationship.

(There are other keys such as: Super Key, Minimal Super Key, Candidate Key, Unique Key, Alternate Key, Composite Key, Natural Key, Surrogate Key.)



### Index

Indexes are the crux of why **SQL** is so much more efficient than, say, **R**.  An index is a lookup table that helps **SQL** keep track of which records contain certain values.  By indexing the rows, **SQL** is able to optimize sorting and joining tables.  The index is created in advance (when the table is created) and saved to disk, which can take up substantial space on the disk.  Sometimes more than one variable is used to index the table. There are trade-offs to having a lot of indexes (disk space but fast wrangling) versus a few indexes (slow wrangling but less space).

A table may have more than one index but you shouldn't add indexes to every column in a table, as these have to be updated for every addition/update/delete to the column. Rather, indexes should be added to columns that are frequently included in queries.

Indexes may not make much difference for small databases, but, as tables grow in size, queries benefit more from indexes.

In **MySQL** the commands `SHOW KEYS` and `SHOW INDEXES` provide information about the keys and indexes for each table.  (Neither operation is available in **DuckDB**.)


### Partitioning

Another way to speed up query retrievals is to partition the data tables.  If, for example, the SNL queries were always done by year, then the `episodes` table could be partitioned such that they are stored as separate tables (one per `year`).  The partitioning functions as an index on `year`.  The user would not be able to tell the difference between the unpartitioned `episodes` table and the partitioned one.  However, queries done by `year` would be faster.  Queries done grouped in another way would be slower.


## Best practice

It is always a good idea to terminate the **SQL** connection when you are done with it.

```{r}
dbDisconnect(con_duckdb, shutdown = TRUE)
```



## <i class="fas fa-lightbulb"></i> Reflection questions  

## <i class="fas fa-balance-scale"></i> Ethics considerations 



